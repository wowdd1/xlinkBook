arxiv-9000-1 | Fast and Memory-Efficient Significant Pattern Mining via Permutation Testing | http://arxiv.org/pdf/1502.04315v1.pdf | author:Felipe Llinares López, Mahito Sugiyama, Laetitia Papaxanthos, Karsten M. Borgwardt category:stat.ML published:2015-02-15 summary:We present a novel algorithm, Westfall-Young light, for detecting patterns,such as itemsets and subgraphs, which are statistically significantly enrichedin one of two classes. Our method corrects rigorously for multiple hypothesistesting and correlations between patterns through the Westfall-Youngpermutation procedure, which empirically estimates the null distribution ofpattern frequencies in each class via permutations. In our experiments,Westfall-Young light dramatically outperforms the current state-of-the-artapproach in terms of both runtime and memory efficiency on popular real-worldbenchmark datasets for pattern mining. The key to this efficiency is thatunlike all existing methods, our algorithm neither needs to solve theunderlying frequent itemset mining problem anew for each permutation nor needsto store the occurrence list of all frequent patterns. Westfall-Young lightopens the door to significant pattern mining on large datasets that previouslyled to prohibitive runtime or memory costs.
arxiv-9000-2 | Cost-Sensitive Support Vector Machines | http://arxiv.org/pdf/1212.0975v2.pdf | author:Hamed Masnadi-Shirazi, Nuno Vasconcelos, Arya Iranmehr category:cs.LG stat.ML published:2012-12-05 summary:A new procedure for learning cost-sensitive SVM(CS-SVM) classifiers isproposed. The SVM hinge loss is extended to the cost sensitive setting, and theCS-SVM is derived as the minimizer of the associated risk. The extension of thehinge loss draws on recent connections between risk minimization andprobability elicitation. These connections are generalized to cost-sensitiveclassification, in a manner that guarantees consistency with the cost-sensitiveBayes risk, and associated Bayes decision rule. This ensures that optimaldecision rules, under the new hinge loss, implement the Bayes-optimalcost-sensitive classification boundary. Minimization of the new hinge loss isshown to be a generalization of the classic SVM optimization problem, and canbe solved by identical procedures. The dual problem of CS-SVM is carefullyscrutinized by means of regularization theory and sensitivity analysis and theCS-SVM algorithm is substantiated. The proposed algorithm is also extended tocost-sensitive learning with example dependent costs. The minimum costsensitive risk is proposed as the performance measure and is connected to ROCanalysis through vector optimization. The resulting algorithm avoids theshortcomings of previous approaches to cost-sensitive SVM design, and is shownto have superior experimental performance on a large number of cost sensitiveand imbalanced datasets.
arxiv-9000-3 | Accurate automatic segmentation of retina layers with emphasis on first layer | http://arxiv.org/pdf/1501.06114v2.pdf | author:Mahdi Salarian category:cs.CV published:2015-01-25 summary:Quantification of intra-retinal boundaries in optical coherence tomography(OCT) is a crucial task for studying and diagnosing neurological and oculardiseases. Since manual segmentation of layers is usually a time consuming taskand relay on user, a lot of attempts done to do it automatically and withoutinterference of user. Although for extracting all layers usually same procedureis applied but finding the first layer is usually more difficult due tovanishing it in some region specially close to Fobia. To have a generalsoftware, beside using common methods like applying shortest path algorithm onglobal gradient of image, some extra steps are used here to confine search areafor Dijstra algorithm especially for the second layer. Results demonstrateshigh accuracy in segmenting all present layers, especially the first one thatis important for diagnosing issue.
arxiv-9000-4 | segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection | http://arxiv.org/pdf/1502.04275v1.pdf | author:Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, Sanja Fidler category:cs.CV published:2015-02-15 summary:In this paper, we propose an approach that exploits object segmentation inorder to improve the accuracy of object detection. We frame the problem asinference in a Markov Random Field, in which each detection hypothesis scoresobject appearance as well as contextual information using Convolutional NeuralNetworks, and allows the hypothesis to choose and score a segment out of alarge pool of accurate object segmentation proposals. This enables the detectorto incorporate additional evidence when it is available and thus results inmore accurate detections. Our experiments show an improvement of 4.1% in mAPover the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the currentstate-of-the-art, demonstrating the power of our approach.
arxiv-9000-5 | Spatial Stimuli Gradient Sketch Model | http://arxiv.org/pdf/1502.04272v1.pdf | author:Joshin John Mathew, Alex Pappachen James category:cs.CV published:2015-02-15 summary:The inability of automated edge detection methods inspired from primal sketchmodels to accurately calculate object edges under the influence of pixel noiseis an open problem. Extending the principles of image perception i.e.Weber-Fechner law, and Sheperd similarity law, we propose a new edge detectionmethod and formulation that use perceived brightness and neighbourhoodsimilarity calculations in the determination of robust object edges. Therobustness of the detected edges is benchmark against Sobel, SIS, Kirsch, andPrewitt edge detection methods in an example face recognition problem showingstatistically significant improvement in recognition accuracy and pixel noisetolerance.
arxiv-9000-6 | Cardiac MR Image Segmentation Techniques: an overview | http://arxiv.org/pdf/1502.04252v1.pdf | author:Tizita Nesibu Shewaye category:cs.CV published:2015-02-14 summary:Broadly speaking, the objective in cardiac image segmentation is to delineatethe outer and inner walls of the heart to segment out either the entire orparts of the organ boundaries. This paper will focus on MR images as they arethe most widely used in cardiac segmentation -- as a result of the accuratemorphological information and better soft tissue contrast they provide. Thiscardiac segmentation information is very useful as it eases physicalmeasurements that provides useful metrics for cardiac diagnosis such asinfracted volumes, ventricular volumes, ejection fraction, myocardial mass,cardiac movement, and the like. But, this task is difficult due to theintensity and texture similarities amongst the different cardiac and backgroundstructures on top of some noisy artifacts present in MR images. Thus far,various researchers have proposed different techniques to solve some of thepressing issues. This seminar paper presents an overview of representativemedical image segmentation techniques. The paper also highlights preferredapproaches for segmentation of the four cardiac chambers: the left ventricle(LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on shortaxis image planes.
arxiv-9000-7 | Asymptotic Justification of Bandlimited Interpolation of Graph signals for Semi-Supervised Learning | http://arxiv.org/pdf/1502.04248v1.pdf | author:Aamir Anis, Aly El Gamal, A. Salman Avestimehr, Antonio Ortega category:cs.LG cs.IT math.IT published:2015-02-14 summary:Graph-based methods play an important role in unsupervised andsemi-supervised learning tasks by taking into account the underlying geometryof the data set. In this paper, we consider a statistical setting forsemi-supervised learning and provide a formal justification of the recentlyintroduced framework of bandlimited interpolation of graph signals. Ouranalysis leads to the interpretation that, given enough labeled data, thismethod is very closely related to a constrained low density separation problemas the number of data points tends to infinity. We demonstrate the practicalutility of our results through simple experiments.
arxiv-9000-8 | Analysis and Understanding of Various Models for Efficient Representation and Accurate Recognition of Human Faces | http://arxiv.org/pdf/1312.3787v2.pdf | author:Dharini S., Guru Prasad M., Hari haran. V., Kiran Tej J. L., Kunal Ghosh category:cs.CV published:2013-12-13 summary:In this paper we have tried to compare the various face recognition modelsagainst their classical problems. We look at the methods followed by theseapproaches and evaluate to what extent they are able to solve the problems. Allmethods proposed have some drawbacks under certain conditions. To overcomethese drawbacks we propose a multi-model approach
arxiv-9000-9 | Probabilistic Models for High-Order Projective Dependency Parsing | http://arxiv.org/pdf/1502.04174v1.pdf | author:Xuezhe Ma, Hai Zhao category:cs.CL published:2015-02-14 summary:This paper presents generalized probabilistic models for high-orderprojective dependency parsing and an algorithmic framework for learning thesestatistical models involving dependency trees. Partition functions andmarginals for high-order dependency trees can be computed efficiently, byadapting our algorithms which extend the inside-outside algorithm tohigher-order cases. To show the effectiveness of our algorithms, we performexperiments on three languages---English, Chinese and Czech, using maximumconditional likelihood estimation for model training and L-BFGS for parameterestimation. Our methods achieve competitive performance for English, andoutperform all previously reported dependency parsers for Chinese and Czech.
arxiv-9000-10 | A Distributional Representation Model For Collaborative Filtering | http://arxiv.org/pdf/1502.04163v1.pdf | author:Zhang Junlin, Cai Heng, Huang Tongwen, Xue Huiping category:cs.IR cs.NE published:2015-02-14 summary:In this paper, we propose a very concise deep learning approach forcollaborative filtering that jointly models distributional representation forusers and items. The proposed framework obtains better performance whencompared against current state-of-art algorithms and that made thedistributional representation model a promising direction for further researchin the collaborative filtering.
arxiv-9000-11 | Geometry-based Adaptive Symbolic Approximation for Fast Sequence Matching on Manifolds | http://arxiv.org/pdf/1403.0820v2.pdf | author:Rushil Anirudh, Pavan Turaga category:cs.CV math.DG published:2014-03-04 summary:In this paper, we consider the problem of fast and efficient indexingtechniques for sequences evolving in non-Euclidean spaces. This problem hasseveral applications in the areas of human activity analysis, where there is aneed to perform fast search, and recognition in very high dimensional spaces.The problem is made more challenging when representations such as landmarks,contours, and human skeletons etc. are naturally studied in a non-Euclideansetting where even simple operations are much more computationally intensivethan their Euclidean counterparts. We propose a geometry and data adaptivesymbolic framework that is shown to enable the deployment of fast and accuratealgorithms for activity recognition, dynamic texture recognition, motifdiscovery. Toward this end, we present generalizations of key concepts ofpiece-wise aggregation and symbolic approximation for the case of non-Euclideanmanifolds. We show that one can replace expensive geodesic computations withmuch faster symbolic computations with little loss of accuracy in activityrecognition and discovery applications. The framework is general enough to workacross both Euclidean and non-Euclidean spaces, depending on appropriatefeature representations without compromising on the ultra-low bandwidth, highspeed and high accuracy. The proposed methods are ideally suited for real-timesystems and low complexity scenarios.
arxiv-9000-12 | Non-Adaptive Learning a Hidden Hipergraph | http://arxiv.org/pdf/1502.04137v1.pdf | author:Hasan Abasi, Nader H. Bshouty, Hanna Mazzawi category:cs.LG published:2015-02-13 summary:We give a new deterministic algorithm that non-adaptively learns a hiddenhypergraph from edge-detecting queries. All previous non-adaptive algorithmseither run in exponential time or have non-optimal query complexity. We givethe first polynomial time non-adaptive learning algorithm for learninghypergraph that asks almost optimal number of queries.
arxiv-9000-13 | Long-short Term Motion Feature for Action Classification and Retrieval | http://arxiv.org/pdf/1502.04132v1.pdf | author:Zhenzhong Lan, Xuanchong Li, Ming Lin, Alexander G. Hauptmann category:cs.CV published:2015-02-13 summary:We propose a method for representing motion information for videoclassification and retrieval. We improve upon local descriptor based methodsthat have been among the most popular and successful models for representingvideos. The desired local descriptors need to satisfy two requirements: 1) tobe representative, 2) to be discriminative. Therefore, they need to occurfrequently enough in the videos and to be be able to tell the difference amongdifferent types of motions. To generate such local descriptors, the videoblocks they are based on must contain just the right amount of motioninformation. However, current state-of-the-art local descriptor methods usevideo blocks with a single fixed size, which is insufficient for coveringactions with varying speeds. In this paper, we introduce a long-short termmotion feature that generates descriptors from video blocks with multiplelengths, thus covering motions with large speed variance. Experimental resultsshow that, albeit simple, our model achieves state-of-the-arts results onseveral benchmark datasets.
arxiv-9000-14 | Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures | http://arxiv.org/pdf/1312.7857v2.pdf | author:Peter Orbanz, Daniel M. Roy category:math.ST stat.ML stat.TH published:2013-12-30 summary:The natural habitat of most Bayesian methods is data represented byexchangeable sequences of observations, for which de Finetti's theorem providesthe theoretical foundation. Dirichlet process clustering, Gaussian processregression, and many other parametric and nonparametric Bayesian models fallwithin the remit of this framework; many problems arising in modern dataanalysis do not. This article provides an introduction to Bayesian models ofgraphs, matrices, and other data that can be modeled by random structures. Wedescribe results in probability theory that generalize de Finetti's theorem tosuch data and discuss their relevance to nonparametric Bayesian modeling. Withthe basic ideas in place, we survey example models available in the literature;applications of such models include collaborative filtering, link prediction,and graph and network analysis. We also highlight connections to recentdevelopments in graph theory and probability, and sketch the more generalmathematical foundation of Bayesian methods for other types of data beyondsequences and arrays.
arxiv-9000-15 | How essential are unstructured clinical narratives and information fusion to clinical trial recruitment? | http://arxiv.org/pdf/1502.04049v1.pdf | author:Preethi Raghavan, James L. Chen, Eric Fosler-Lussier, Albert M. Lai category:cs.CY cs.AI cs.CL published:2015-02-13 summary:Electronic health records capture patient information using structuredcontrolled vocabularies and unstructured narrative text. While structured datatypically encodes lab values, encounters and medication lists, unstructureddata captures the physician's interpretation of the patient's condition,prognosis, and response to therapeutic intervention. In this paper, wedemonstrate that information extraction from unstructured clinical narrativesis essential to most clinical applications. We perform an empirical study tovalidate the argument and show that structured data alone is insufficient inresolving eligibility criteria for recruiting patients onto clinical trials forchronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data isessential to solving 59% of the CLL trial criteria and 77% of the prostatecancer trial criteria. More specifically, for resolving eligibility criteriawith temporal constraints, we show the need for temporal reasoning andinformation integration with medical events within and across unstructuredclinical narratives and structured data.
arxiv-9000-16 | Abstract Learning via Demodulation in a Deep Neural Network | http://arxiv.org/pdf/1502.04042v1.pdf | author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx published:2015-02-13 summary:Inspired by the brain, deep neural networks (DNN) are thought to learnabstract representations through their hierarchical architecture. However, atpresent, how this happens is not well understood. Here, we demonstrate that DNNlearn abstract representations by a process of demodulation. We introduce abiased sigmoid activation function and use it to show that DNN learn andperform better when optimized for demodulation. Our findings constitute thefirst unambiguous evidence that DNN perform abstract learning in practical use.Our findings may also explain abstract learning in the human brain.
arxiv-9000-17 | Incoherence-Optimal Matrix Completion | http://arxiv.org/pdf/1310.0154v4.pdf | author:Yudong Chen category:cs.IT cs.LG math.IT stat.ML published:2013-10-01 summary:This paper considers the matrix completion problem. We show that it is notnecessary to assume joint incoherence, which is a standard but unintuitive andrestrictive condition that is imposed by previous studies. This leads to asample complexity bound that is order-wise optimal with respect to theincoherence parameter (as well as to the rank $r$ and the matrix dimension $n$up to a log factor). As a consequence, we improve the sample complexity ofrecovering a semidefinite matrix from $O(nr^{2}\log^{2}n)$ to $O(nr\log^{2}n)$,and the highest allowable rank from $\Theta(\sqrt{n}/\log n)$ to$\Theta(n/\log^{2}n)$. The key step in proof is to obtain new bounds on the$\ell_{\infty,2}$-norm, defined as the maximum of the row and column norms of amatrix. To illustrate the applicability of our techniques, we discussextensions to SVD projection, structured matrix completion and semi-supervisedclustering, for which we provide order-wise improvements over existing results.Finally, we turn to the closely-related problem of low-rank-plus-sparse matrixdecomposition. We show that the joint incoherence condition is unavoidable herefor polynomial-time algorithms conditioned on the Planted Clique conjecture.This means it is intractable in general to separate a rank-$\omega(\sqrt{n})$positive semidefinite matrix and a sparse matrix. Interestingly, our resultsshow that the standard and joint incoherence conditions are associatedrespectively with the information (statistical) and computational aspects ofthe matrix decomposition problem.
arxiv-9000-18 | Polynomial-Chaos-based Kriging | http://arxiv.org/pdf/1502.03939v1.pdf | author:R. Schoebi, B. Sudret, J. Wiart category:stat.CO stat.ME stat.ML published:2015-02-13 summary:Computer simulation has become the standard tool in many engineering fieldsfor designing and optimizing systems, as well as for assessing theirreliability. To cope with demanding analysis such as optimization andreliability, surrogate models (a.k.a meta-models) have been increasinglyinvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Krigingare two popular non-intrusive meta-modelling techniques. PCE surrogates thecomputational model with a series of orthonormal polynomials in the inputvariables where polynomials are chosen in coherency with the probabilitydistributions of those input variables. On the other hand, Kriging assumes thatthe computer model behaves as a realization of a Gaussian random process whoseparameters are estimated from the available computer runs, i.e. input vectorsand response values. These two techniques have been developed more or less inparallel so far with little interaction between the researchers in the twofields. In this paper, PC-Kriging is derived as a new non-intrusivemeta-modeling approach combining PCE and Kriging. A sparse set of orthonormalpolynomials (PCE) approximates the global behavior of the computational modelwhereas Kriging manages the local variability of the model output. An adaptivealgorithm similar to the least angle regression algorithm determines theoptimal sparse set of polynomials. PC-Kriging is validated on various benchmarkanalytical functions which are easy to sample for reference results. From thenumerical investigations it is concluded that PC-Kriging performs better thanor at least as good as the two distinct meta-modeling techniques. A larger gainin accuracy is obtained when the experimental design has a limited size, whichis an asset when dealing with demanding computational models.
arxiv-9000-19 | A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates | http://arxiv.org/pdf/1312.7006v2.pdf | author:Yudong Chen, Xinyang Yi, Constantine Caramanis category:stat.ML cs.IT cs.LG math.IT published:2013-12-25 summary:We consider the mixed regression problem with two components, underadversarial and stochastic noise. We give a convex optimization formulationthat provably recovers the true solution, and provide upper bounds on therecovery errors for both arbitrary noise and stochastic noise settings. We alsogive matching minimax lower bounds (up to log factors), showing that undercertain assumptions, our algorithm is information-theoretically optimal. Ourresults represent the first tractable algorithm guaranteeing successfulrecovery with tight bounds on recovery errors and sample complexity.
arxiv-9000-20 | Gradient Difference based approach for Text Localization in Compressed domain | http://arxiv.org/pdf/1502.03918v1.pdf | author:B. H. Shekar, Smitha M. L category:cs.CV published:2015-02-13 summary:In this paper, we propose a gradient difference based approach to textlocalization in videos and scene images. The input video frame/ image is firstcompressed using multilevel 2-D wavelet transform. The edge information of thereconstructed image is found which is further used for finding the maximumgradient difference between the pixels and then the boundaries of the detectedtext blocks are computed using zero crossing technique. We perform logical ANDoperation of the text blocks obtained by gradient difference and the zerocrossing technique followed by connected component analysis to eliminate thefalse positives. Finally, the morphological dilation operation is employed onthe detected text blocks for scene text localization. The experimental resultsobtained on publicly available standard datasets illustrate that the proposedmethod can detect and localize the texts of various sizes, fonts and colors.
arxiv-9000-21 | Skeleton Matching based approach for Text Localization in Scene Images | http://arxiv.org/pdf/1502.03913v1.pdf | author:B. H. Shekar, Smitha M. L category:cs.CV published:2015-02-13 summary:In this paper, we propose a skeleton matching based approach which aids intext localization in scene images. The input image is preprocessed andsegmented into blocks using connected component analysis. We obtain theskeleton of the segmented block using morphology based approach. Theskeletonized images are compared with the trained templates in the database tocategorize into text and non-text blocks. Further, the newly designedgeometrical rules and morphological operations are employed on the detectedtext blocks for scene text localization. The experimental results obtained onpublicly available standard datasets illustrate that the proposed method candetect and localize the texts of various sizes, fonts and colors.
arxiv-9000-22 | Semi-supervised Data Representation via Affinity Graph Learning | http://arxiv.org/pdf/1502.03879v1.pdf | author:Weiya Ren category:cs.LG cs.CV 68T10 I.4.2; I.4.7 published:2015-02-13 summary:We consider the general problem of utilizing both labeled and unlabeled datato improve data representation performance. A new semi-supervised learningframework is proposed by combing manifold regularization and datarepresentation methods such as Non negative matrix factorization and sparsecoding. We adopt unsupervised data representation methods as the learningmachines because they do not depend on the labeled data, which can improvemachine's generation ability as much as possible. The proposed framework formsthe Laplacian regularizer through learning the affinity graph. We incorporatethe new Laplacian regularizer into the unsupervised data representation tosmooth the low dimensional representation of data and make use of labelinformation. Experimental results on several real benchmark datasets indicatethat our semi-supervised learning framework achieves encouraging resultscompared with state-of-art methods.
arxiv-9000-23 | Far-Field Compression for Fast Kernel Summation Methods in High Dimensions | http://arxiv.org/pdf/1409.2802v2.pdf | author:William B. March, George Biros category:cs.LG stat.ML published:2014-09-09 summary:We consider fast kernel summations in high dimensions: given a large set ofpoints in $d$ dimensions (with $d \gg 3$) and a pair-potential function (the{\em kernel} function), we compute a weighted sum of all pairwise kernelinteractions for each point in the set. Direct summation is equivalent to a(dense) matrix-vector multiplication and scales quadratically with the numberof points. Fast kernel summation algorithms reduce this cost to log-linear orlinear complexity. Treecodes and Fast Multipole Methods (FMMs) deliver tremendous speedups byconstructing approximate representations of interactions of points that are farfrom each other. In algebraic terms, these representations correspond tolow-rank approximations of blocks of the overall interaction matrix. Existingapproaches require an excessive number of kernel evaluations with increasing$d$ and number of points in the dataset. To address this issue, we use a randomized algebraic approach in which wefirst sample the rows of a block and then construct its approximate, low-rankinterpolative decomposition. We examine the feasibility of this approachtheoretically and experimentally. We provide a new theoretical result showing atighter bound on the reconstruction error from uniformly sampling rows than theexisting state-of-the-art. We demonstrate that our sampling approach iscompetitive with existing (but prohibitively expensive) methods from theliterature. We also construct kernel matrices for the Laplacian, Gaussian, andpolynomial kernels -- all commonly used in physics and data analysis. Weexplore the numerical properties of blocks of these matrices, and show thatthey are amenable to our approach. Depending on the data set, our randomizedalgorithm can successfully compute low rank approximations in high dimensions.We report results for data sets with ambient dimensions from four to 1,000.
arxiv-9000-24 | Discovering Human Interactions in Videos with Limited Data Labeling | http://arxiv.org/pdf/1502.03851v1.pdf | author:Mehran Khodabandeh, Arash Vahdat, Guang-Tong Zhou, Hossein Hajimirsadeghi, Mehrsan Javan Roshtkhari, Greg Mori, Stephen Se category:cs.CV published:2015-02-12 summary:We present a novel approach for discovering human interactions in videos.Activity understanding techniques usually require a large number of labeledexamples, which are not available in many practical cases. Here, we focus onrecovering semantically meaningful clusters of human-human and human-objectinteraction in an unsupervised fashion. A new iterative solution is introducedbased on Maximum Margin Clustering (MMC), which also accepts user feedback torefine clusters. This is achieved by formulating the whole process as a unifiedconstrained latent max-margin clustering problem. Extensive experiments havebeen carried out over three challenging datasets, Collective Activity, VIRAT,and UT-interaction. Empirical results demonstrate that the proposed algorithmcan efficiently discover perfect semantic clusters of human interactions withonly a small amount of labeling effort.
arxiv-9000-25 | A new hybrid metric for verifying parallel corpora of Arabic-English | http://arxiv.org/pdf/1502.03752v1.pdf | author:Saad Alkahtani, Wei Liu, William J. Teahan category:cs.CL published:2015-02-12 summary:This paper discusses a new metric that has been applied to verify the qualityin translation between sentence pairs in parallel corpora of Arabic-English.This metric combines two techniques, one based on sentence length and the otherbased on compression code length. Experiments on sample test parallelArabic-English corpora indicate the combination of these two techniquesimproves accuracy of the identification of satisfactory and unsatisfactorysentence pairs compared to sentence length and compression code length alone.The new method proposed in this research is effective at filtering noise andreducing mis-translations resulting in greatly improved quality.
arxiv-9000-26 | Simulation of Color Blindness and a Proposal for Using Google Glass as Color-correcting Tool | http://arxiv.org/pdf/1502.03723v1.pdf | author:H. M. de Oliveira, J. Ranhel, R. B. A. Alves category:cs.HC cs.CV published:2015-02-12 summary:The human visual color response is driven by specialized cells called cones,which exist in three types, viz. R, G, and B. Software is developed to simulatehow color images are displayed for different types of color blindness.Specified the default color deficiency associated with a user, it generates apreview of the rainbow (in the visible range, from red to violet) and shows up,side by side with a colorful image provided as input, the display correspondentcolorblind. The idea is to provide an image processing after image acquisitionto enable a better perception ofcolors by the color blind. Examples ofpseudo-correction are shown for the case of Protanopia (red blindness). Thesystem is adapted into a screen of an i-pad or a cellphone in which thecolorblind observe the camera, the image processed with color detail previouslyimperceptible by his naked eye. As prospecting, wearable computer glasses couldbe manufactured to provide a corrected image playback. The approach can alsoprovide augmented reality for human vision by adding the UV or IR responses asa new feature of Google Glass.
arxiv-9000-27 | Generalized Non-orthogonal Joint Diagonalization with LU Decomposition and Successive Rotations | http://arxiv.org/pdf/1312.0712v3.pdf | author:Xiao-Feng Gong, Xiu-Lin Wang, Qiu-Hua Lin category:stat.ML published:2013-12-03 summary:Non-orthogonal joint diagonalization (NJD) free of prewhitening has beenwidely studied in the context of blind source separation (BSS) and array signalprocessing, etc. However, NJD is used to retrieve the jointly diagonalizablestructure for a single set of target matrices which are mostly formulized witha single dataset, and thus is insufficient to handle multiple datasets withinter-set dependences, a scenario often encountered in joint BSS (J-BSS)applications. As such, we present a generalized NJD (GNJD) algorithm tosimultaneously perform asymmetric NJD upon multiple sets of target matriceswith mutually linked loading matrices, by using LU decomposition and successiverotations, to enable J-BSS over multiple datasets with indication/exploitationof their mutual dependences. Experiments with synthetic and real-world datasetsare provided to illustrate the performance of the proposed algorithm.
arxiv-9000-28 | Analysis of Solution Quality of a Multiobjective Optimization-based Evolutionary Algorithm for Knapsack Problem | http://arxiv.org/pdf/1502.03699v1.pdf | author:Jun He, Yong Wang, Yuren Zhou category:cs.NE published:2015-02-12 summary:Multi-objective optimisation is regarded as one of the most promising waysfor dealing with constrained optimisation problems in evolutionaryoptimisation. This paper presents a theoretical investigation of amulti-objective optimisation evolutionary algorithm for solving the 0-1knapsack problem. Two initialisation methods are considered in the algorithm:local search initialisation and greedy search initialisation. Then the solutionquality of the algorithm is analysed in terms of the approximation ratio.
arxiv-9000-29 | Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation | http://arxiv.org/pdf/1502.03682v1.pdf | author:Jose Antonio Miñarro-Giménez, Oscar Marín-Alonso, Matthias Samwald category:cs.CL cs.IR cs.LG cs.NE published:2015-02-12 summary:BACKGROUND: The amount of biomedical literature is rapidly growing and it isbecoming increasingly difficult to keep manually curated knowledge bases andontologies up-to-date. In this study we applied the word2vec deep learningtoolkit to medical corpora to test its potential for identifying relationshipsfrom unstructured text. We evaluated the efficiency of word2vec in identifyingproperties of pharmaceuticals based on mid-sized, unstructured medical textcorpora available on the web. Properties included relationships to diseases('may treat') or physiological processes ('has physiological effect'). Wecompared the relationships identified by word2vec with manually curatedinformation from the National Drug File - Reference Terminology (NDF-RT)ontology as a gold standard. RESULTS: Our results revealed a maximum accuracyof 49.28% which suggests a limited ability of word2vec to capture linguisticregularities on the collected medical corpora compared with other publishedresults. We were able to document the influence of different parameter settingson result accuracy and found and unexpected trade-off between ranking qualityand accuracy. Pre-processing corpora to reduce syntactic variability proved tobe a good strategy for increasing the utility of the trained vector models.CONCLUSIONS: Word2vec is a very efficient implementation for computing vectorrepresentations and for its ability to identify relationships in textual datawithout any prior domain knowledge. We found that the ranking and retrievedresults generated by word2vec were not of sufficient quality for automaticpopulation of knowledge bases and ontologies, but could serve as a startingpoint for further manual curation.
arxiv-9000-30 | Over-Sampling in a Deep Neural Network | http://arxiv.org/pdf/1502.03648v1.pdf | author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx published:2015-02-12 summary:Deep neural networks (DNN) are the state of the art on many engineeringproblems such as computer vision and audition. A key factor in the success ofthe DNN is scalability - bigger networks work better. However, the reason forthis scalability is not yet well understood. Here, we interpret the DNN as adiscrete system, of linear filters followed by nonlinear activations, that issubject to the laws of sampling theory. In this context, we demonstrate thatover-sampled networks are more selective, learn faster and learn more robustly.Our findings may ultimately generalize to the human brain.
arxiv-9000-31 | Ordering-sensitive and Semantic-aware Topic Modeling | http://arxiv.org/pdf/1502.03630v1.pdf | author:Min Yang, Tianyi Cui, Wenting Tu category:cs.LG cs.CL cs.IR published:2015-02-12 summary:Topic modeling of textual corpora is an important and challenging problem. Inmost previous work, the "bag-of-words" assumption is usually made which ignoresthe ordering of words. This assumption simplifies the computation, but itunrealistically loses the ordering information and the semantic of words in thecontext. In this paper, we present a Gaussian Mixture Neural Topic Model(GMNTM) which incorporates both the ordering of words and the semantic meaningof sentences into topic modeling. Specifically, we represent each topic as acluster of multi-dimensional vectors and embed the corpus into a collection ofvectors generated by the Gaussian mixture model. Each word is affected not onlyby its topic, but also by the embedding vector of its surrounding words and thecontext. The Gaussian mixture components and the topic of documents, sentencesand words can be learnt jointly. Extensive experiments show that our model canlearn better topics and more accurate word distributions for each topic.Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTMobtains significantly better performance in terms of perplexity, retrievalaccuracy and classification accuracy.
arxiv-9000-32 | A Predictive System for detection of Bankruptcy using Machine Learning techniques | http://arxiv.org/pdf/1502.03601v1.pdf | author:Kalyan Nagaraj, Amulyashree Sridhar category:cs.LG published:2015-02-12 summary:Bankruptcy is a legal procedure that claims a person or organization as adebtor. It is essential to ascertain the risk of bankruptcy at initial stagesto prevent financial losses. In this perspective, different soft computingtechniques can be employed to ascertain bankruptcy. This study proposes abankruptcy prediction system to categorize the companies based on extent ofrisk. The prediction system acts as a decision support tool for detection ofbankruptcy Keywords: Bankruptcy, soft computing, decision support tool
arxiv-9000-33 | Towards zero-configuration condition monitoring based on dictionary learning | http://arxiv.org/pdf/1502.03596v1.pdf | author:Sergio Martin-del-Campo, Fredrik Sandin category:cs.CV published:2015-02-12 summary:Condition-based predictive maintenance can significantly improve overallequipment effectiveness provided that appropriate monitoring methods are used.Online condition monitoring systems are customized to each type of machine andneed to be reconfigured when conditions change, which is costly and requiresexpert knowledge. Basic feature extraction methods limited to signaldistribution functions and spectra are commonly used, making it difficult toautomatically analyze and compare machine conditions. In this paper, weinvestigate the possibility to automate the condition monitoring process bycontinuously learning a dictionary of optimized shift-invariant feature vectorsusing a well-known sparse approximation method. We study how the featurevectors learned from a vibration signal evolve over time when a fault developswithin a ball bearing of a rotating machine. We quantify the adaptation rate oflearned features and find that this quantity changes significantly in thetransitions between normal and faulty states of operation of the ball bearing.
arxiv-9000-34 | On the Easiest and Hardest Fitness Functions | http://arxiv.org/pdf/1203.6286v5.pdf | author:Jun He, Tianshi Chen, Xin Yao category:cs.NE published:2012-03-28 summary:The hardness of fitness functions is an important research topic in the fieldof evolutionary computation. In theory, the study can help understanding theability of evolutionary algorithms. In practice, the study may provide aguideline to the design of benchmarks. The aim of this paper is to answer thefollowing research questions: Given a fitness function class, which functionsare the easiest with respect to an evolutionary algorithm? Which are thehardest? How are these functions constructed? The paper provides theoreticalanswers to these questions. The easiest and hardest fitness functions areconstructed for an elitist (1+1) evolutionary algorithm to maximise a class offitness functions with the same optima. It is demonstrated that the unimodalfunctions are the easiest and deceptive functions are the hardest in terms ofthe time-fitness landscape. The paper also reveals that the easiest fitnessfunction to one algorithm may become the hardest to another algorithm, and viceversa.
arxiv-9000-35 | Web spam classification using supervised artificial neural network algorithms | http://arxiv.org/pdf/1502.03581v1.pdf | author:Ashish Chandra, Mohammad Suaib, Dr. Rizwan Beg category:cs.NE cs.LG published:2015-02-12 summary:Due to the rapid growth in technology employed by the spammers, there is aneed of classifiers that are more efficient, generic and highly adaptive.Neural Network based technologies have high ability of adaption as well asgeneralization. As per our knowledge, very little work has been done in thisfield using neural network. We present this paper to fill this gap. This paperevaluates performance of three supervised learning algorithms of artificialneural network by creating classifiers for the complex problem of latest webspam pattern classification. These algorithms are Conjugate Gradient algorithm,Resilient Backpropagation learning, and Levenberg-Marquardt algorithm.
arxiv-9000-36 | Convergence of gradient based pre-training in Denoising autoencoders | http://arxiv.org/pdf/1502.03537v1.pdf | author:Vamsi K Ithapu, Sathya Ravi, Vikas Singh category:cs.LG cs.CV math.OC published:2015-02-12 summary:The success of deep architectures is at least in part attributed to thelayer-by-layer unsupervised pre-training that initializes the network. Variouspapers have reported extensive empirical analysis focusing on the design andimplementation of good pre-training procedures. However, an understandingpertaining to the consistency of parameter estimates, the convergence oflearning procedures and the sample size estimates is still unavailable in theliterature. In this work, we study pre-training in classical and distributeddenoising autoencoders with these goals in mind. We show that the gradientconverges at the rate of $\frac{1}{\sqrt{N}}$ and has a sub-linear dependenceon the size of the autoencoder network. In a distributed setting where disjointsections of the whole network are pre-trained synchronously, we show that theconvergence improves by at least $\tau^{3/4}$, where $\tau$ corresponds to thesize of the sections. We provide a broad set of experiments to empiricallyevaluate the suggested behavior.
arxiv-9000-37 | Speeding up Permutation Testing in Neuroimaging | http://arxiv.org/pdf/1502.03536v1.pdf | author:Chris Hinrichs, Vamsi K Ithapu, Qinyuan Sun, Sterling C Johnson, Vikas Singh category:stat.CO cs.AI stat.ML published:2015-02-12 summary:Multiple hypothesis testing is a significant problem in nearly allneuroimaging studies. In order to correct for this phenomena, we require areliable estimate of the Family-Wise Error Rate (FWER). The well knownBonferroni correction method, while simple to implement, is quite conservative,and can substantially under-power a study because it ignores dependenciesbetween test statistics. Permutation testing, on the other hand, is an exact,non-parametric method of estimating the FWER for a given $\alpha$-threshold,but for acceptably low thresholds the computational burden can be prohibitive.In this paper, we show that permutation testing in fact amounts to populatingthe columns of a very large matrix ${\bf P}$. By analyzing the spectrum of thismatrix, under certain conditions, we see that ${\bf P}$ has a low-rank plus alow-variance residual decomposition which makes it suitable for highlysub--sampled --- on the order of $0.5\%$ --- matrix completion methods. Basedon this observation, we propose a novel permutation testing methodology whichoffers a large speedup, without sacrificing the fidelity of the estimated FWER.Our evaluations on four different neuroimaging datasets show that acomputational speedup factor of roughly $50\times$ can be achieved whilerecovering the FWER distribution up to very high accuracy. Further, we showthat the estimated $\alpha$-threshold is also recovered faithfully, and isstable.
arxiv-9000-38 | Dependent Nonparametric Bayesian Group Dictionary Learning for online reconstruction of Dynamic MR images | http://arxiv.org/pdf/1408.5667v3.pdf | author:Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim category:cs.CV published:2014-08-25 summary:In this paper, we introduce a dictionary learning based approach applied tothe problem of real-time reconstruction of MR image sequences that are highlyundersampled in k-space. Unlike traditional dictionary learning, our methodintegrates both global and patch-wise (local) sparsity information andincorporates some priori information into the reconstruction process. Moreover,we use a Dependent Hierarchical Beta-process as the prior for the group-baseddictionary learning, which adaptively infers the dictionary size and thesparsity of each patch; and also ensures that similar patches are manifested interms of similar dictionary atoms. An efficient numerical algorithm based onthe alternating direction method of multipliers (ADMM) is also presented.Through extensive experimental results we show that our proposed methodachieves superior reconstruction quality, compared to the other state-of-the-art DL-based methods.
arxiv-9000-39 | An equalised global graphical model-based approach for multi-camera object tracking | http://arxiv.org/pdf/1502.03532v1.pdf | author:Lijun Cao, Weihua Chen, Xiaotang Chen, Shuai Zheng, Kaiqi Huang category:cs.CV published:2015-02-12 summary:Multi-camera non-overlapping visual object tracking system typically consistsof two tasks: single camera object tracking and inter-camera object tracking.Since the state-of-the-art approaches are yet not perform perfectly in realscenes, the errors in single camera object tracking module would propagate intothe module of inter-camera object tracking, resulting much lower overallperformance. In order to address this problem, we develop an approach thatjointly optimise the single camera object tracking and inter-camera objecttracking in an equalised global graphical model. Such an approach has theadvantage of guaranteeing a good overall tracking performance even when thereare limited amount of false tracking in single camera object tracking. Besides,the similarity metrics used in our approach improve the compatibility of themetrics used in the two different tasks. Results show that our approach achievethe state-of-the-art results in multi-camera non-overlapping tracking datasets.
arxiv-9000-40 | Multispectral Palmprint Recognition Using Textural Features | http://arxiv.org/pdf/1408.6615v3.pdf | author:Shervin Minaee, AmirAli Abdolrashidi category:cs.CV published:2014-08-28 summary:In order to utilize identification to the best extent, we need robust andfast algorithms and systems to process the data. Having palmprint as a reliableand unique characteristic of every person, we extract and use its featuresbased on its geometry, lines and angles. There are countless ways to definemeasures for the recognition task. To analyze a new point of view, we extractedtextural features and used them for palmprint recognition. Co-occurrence matrixcan be used for textural feature extraction. As classifiers, we have used theminimum distance classifier (MDC) and the weighted majority voting system(WMV). The proposed method is tested on a well-known multispectral palmprintdataset of 6000 samples and an accuracy rate of 99.96-100% is obtained for mostscenarios which outperforms all previous works in multispectral palmprintrecognition.
arxiv-9000-41 | Depth Reconstruction from Sparse Samples: Representation, Algorithm, and Sampling | http://arxiv.org/pdf/1407.3840v4.pdf | author:Lee-Kang Liu, Stanley H. Chan, Truong Q. Nguyen category:cs.CV published:2014-07-14 summary:The rapid development of 3D technology and computer vision applications havemotivated a thrust of methodologies for depth acquisition and estimation.However, most existing hardware and software methods have limited performancedue to poor depth precision, low resolution and high computational cost. Inthis paper, we present a computationally efficient method to recover densedepth maps from sparse measurements. We make three contributions. First, weprovide empirical evidence that depth maps can be encoded much more sparselythan natural images by using common dictionaries such as wavelets andcontourlets. We also show that a combined wavelet-contourlet dictionaryachieves better performance than using either dictionary alone. Second, wepropose an alternating direction method of multipliers (ADMM) to achieve fastreconstruction. A multi-scale warm start procedure is proposed to speed up theconvergence. Third, we propose a two-stage randomized sampling scheme tooptimally choose the sampling locations, thus maximizing the reconstructionperformance for any given sampling budget. Experimental results show that theproposed method produces high quality dense depth estimates, and is robust tonoisy measurements. Applications to real data in stereo matching aredemonstrated.
arxiv-9000-42 | Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices | http://arxiv.org/pdf/1502.03505v1.pdf | author:Florian Yger, Masashi Sugiyama category:cs.LG published:2015-02-12 summary:Metric learning has been shown to be highly effective to improve theperformance of nearest neighbor classification. In this paper, we address theproblem of metric learning for Symmetric Positive Definite (SPD) matrices suchas covariance matrices, which arise in many real-world applications. Naivelyusing standard Mahalanobis metric learning methods under the Euclidean geometryfor SPD matrices is not appropriate, because the difference of SPD matrices canbe a non-SPD matrix and thus the obtained solution can be uninterpretable. Tocope with this problem, we propose to use a properly parameterized LogEuclideandistance and optimize the metric with respect to kernel-target alignment, whichis a supervised criterion for kernel learning. Then the resulting non-trivialoptimization problem is solved by utilizing the Riemannian geometry. Finally,we experimentally demonstrate the usefulness of our LogEuclidean metriclearning algorithm on real-world classification tasks for EEG signals andtexture patches.
arxiv-9000-43 | Spectral Sparsification of Random-Walk Matrix Polynomials | http://arxiv.org/pdf/1502.03496v1.pdf | author:Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng category:cs.DS cs.DM cs.LG cs.SI stat.ML published:2015-02-12 summary:We consider a fundamental algorithmic question in spectral graph theory:Compute a spectral sparsifier of random-walk matrix-polynomial$$L_\alpha(G)=D-\sum_{r=1}^d\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacencymatrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighteddegrees, and $\alpha=(\alpha_1...\alpha_d)$ are nonnegative coefficients with$\sum_{r=1}^d\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix ofrandom walks on the graph. The sparsification of $L_\alpha(G)$ appears to bealgorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by allpaths of length $r$, whose precise calculation would be prohibitivelyexpensive. In this paper, we develop the first nearly linear time algorithm for thissparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$coefficients $\alpha$, and $\epsilon > 0$, our algorithm runs in time$O(d^2m\log^2n/\epsilon^{2})$ to construct a Laplacian matrix$\tilde{L}=D-\tilde{A}$ with $O(n\log n/\epsilon^{2})$ non-zeros such that$\tilde{L}\approx_{\epsilon}L_\alpha(G)$. Matrix polynomials arise in mathematical analysis of matrix functions as wellas numerical solutions of matrix equations. Our work is particularly motivatedby the algorithmic problems for speeding up the classic Newton's method inapplications such as computing the inverse square-root of the precision matrixof a Gaussian random field, as well as computing the $q$th-root transition (for$q\geq1$) in a time-reversible Markov model. The key algorithmic step for bothapplications is the construction of a spectral sparsifier of a constant degreerandom-walk matrix-polynomials introduced by Newton's method. Our algorithm canalso be used to build efficient data structures for effective resistances formulti-step time-reversible Markov models, and we anticipate that it could beuseful for other tasks in network analysis.
arxiv-9000-44 | How to show a probabilistic model is better | http://arxiv.org/pdf/1502.03491v1.pdf | author:Mithun Chakraborty, Sanmay Das, Allen Lavoie category:stat.ML cs.LG published:2015-02-11 summary:We present a simple theoretical framework, and corresponding practicalprocedures, for comparing probabilistic models on real data in a traditionalmachine learning setting. This framework is based on the theory of properscoring rules, but requires only basic algebra and probability theory tounderstand and verify. The theoretical concepts presented are well-studied,primarily in the statistics literature. The goal of this paper is to advocatetheir wider adoption for performance evaluation in empirical machine learning.
arxiv-9000-45 | On Learning from Label Proportions | http://arxiv.org/pdf/1402.5902v2.pdf | author:Felix X. Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang category:stat.ML cs.LG published:2014-02-24 summary:Learning from Label Proportions (LLP) is a learning setting, where thetraining data is provided in groups, or "bags", and only the proportion of eachclass in each bag is known. The task is to learn a model to predict the classlabels of the individual instances. LLP has broad applications in politicalscience, marketing, healthcare, and computer vision. This work answers thefundamental question, when and why LLP is possible, by introducing a generalframework, Empirical Proportion Risk Minimization (EPRM). EPRM learns aninstance label classifier to match the given label proportions on the trainingdata. Our result is based on a two-step analysis. First, we provide a VC boundon the generalization error of the bag proportions. We show that the bag samplecomplexity is only mildly sensitive to the bag size. Second, we show that undersome mild assumptions, good bag proportion prediction guarantees good instancelabel prediction. The results together provide a formal guarantee that theindividual labels can indeed be learned in the LLP setting. We discussapplications of the analysis, including justification of LLP algorithms,learning with population proportions, and a paradigm for learning algorithmswith privacy guarantees. We also demonstrate the feasibility of LLP based on acase study in real-world setting: predicting income based on census data.
arxiv-9000-46 | Dependent Matérn Processes for Multivariate Time Series | http://arxiv.org/pdf/1502.03466v1.pdf | author:Alexander Vandenberg-Rodes, Babak Shahbaba category:stat.ML published:2015-02-11 summary:For the challenging task of modeling multivariate time series, we propose anew class of models that use dependent Mat\'ern processes to capture theunderlying structure of data, explain their interdependencies, and predicttheir unknown values. Although similar models have been proposed in theeconometric, statistics, and machine learning literature, our approach hasseveral advantages that distinguish it from existing methods: 1) it is flexibleto provide high prediction accuracy, yet its complexity is controlled to avoidoverfitting; 2) its interpretability separates it from black-box methods; 3)finally, its computational efficiency makes it scalable for high-dimensionaltime series. In this paper, we use several simulated and real data sets toillustrate these advantages. We will also briefly discuss some extensions ofour model.
arxiv-9000-47 | Variable and Fixed Interval Exponential Smoothing | http://arxiv.org/pdf/1502.03465v1.pdf | author:Javier R. Movellan category:stat.ML math.OC published:2015-02-11 summary:Exponential smoothers are a simple and memory efficient way to computerunning averages of time series. Here we define and describe practicalproperties of exponential smoothers for signals observed at constant andvariable intervals.
arxiv-9000-48 | Large-Scale Deep Learning on the YFCC100M Dataset | http://arxiv.org/pdf/1502.03409v1.pdf | author:Karl Ni, Roger Pearce, Kofi Boakye, Brian Van Essen, Damian Borth, Barry Chen, Eric Wang category:cs.LG cs.CV published:2015-02-11 summary:We present a work-in-progress snapshot of learning with a 15 billionparameter deep learning network on HPC architectures applied to the largestpublicly available natural image and video dataset released to-date. Recentadvancements in unsupervised deep neural networks suggest that scaling up suchnetworks in both model and training dataset size can yield significantimprovements in the learning of concepts at the highest layers. We train ourthree-layer deep neural network on the Yahoo! Flickr Creative Commons 100Mdataset. The dataset comprises approximately 99.2 million images and 800,000user-created videos from Yahoo's Flickr image and video sharing platform.Training of our network takes eight days on 98 GPU nodes at the HighPerformance Computing Center at Lawrence Livermore National Laboratory.Encouraging preliminary results and future research directions are presentedand discussed.
arxiv-9000-49 | Covariance estimation using conjugate gradient for 3D classification in Cryo-EM | http://arxiv.org/pdf/1412.0985v3.pdf | author:Joakim Andén, Eugene Katsevich, Amit Singer category:cs.CV published:2014-12-02 summary:Classifying structural variability in noisy projections of biologicalmacromolecules is a central problem in Cryo-EM. In this work, we build on aprevious method for estimating the covariance matrix of the three-dimensionalstructure present in the molecules being imaged. Our proposed method allows forincorporation of contrast transfer function and non-uniform distribution ofviewing angles, making it more suitable for real-world data. We evaluate itsperformance on a synthetic dataset and an experimental dataset obtained byimaging a 70S ribosome complex.
arxiv-9000-50 | Reconstruction in the Labeled Stochastic Block Model | http://arxiv.org/pdf/1502.03365v1.pdf | author:Marc Lelarge, Laurent Massoulié, Jiaming Xu category:stat.ML published:2015-02-11 summary:The labeled stochastic block model is a random graph model representingnetworks with community structure and interactions of multiple types. In itssimplest form, it consists of two communities of approximately equal size, andthe edges are drawn and labeled at random with probability depending on whethertheir two endpoints belong to the same community or not. It has been conjectured in \cite{Heimlicher12} that correlated reconstruction(i.e.\ identification of a partition correlated with the true partition intothe underlying communities) would be feasible if and only if a model parameterexceeds a threshold. We prove one half of this conjecture, i.e., reconstructionis impossible when below the threshold. In the positive direction, we introducea weighted graph to exploit the label information. With a suitable choice ofweight function, we show that when above the threshold by a specific constant,reconstruction is achieved by (1) minimum bisection, (2) a semidefiniterelaxation of minimum bisection, and (3) a spectral method combined withremoval of edges incident to vertices of high degree. Furthermore, we show thathypothesis testing between the labeled stochastic block model and the labeledErd\H{o}s-R\'enyi random graph model exhibits a phase transition at theconjectured reconstruction threshold.
arxiv-9000-51 | Limits on Support Recovery with Probabilistic Models: An Information-Theoretic Framework | http://arxiv.org/pdf/1501.07440v2.pdf | author:Jonathan Scarlett, Volkan Cevher category:cs.IT math.IT math.ST stat.ML stat.TH published:2015-01-29 summary:The support recovery problem consists of determining a sparse subset of a setof variables that is relevant in generating a set of observations, and arisesin a diverse range of settings such as group testing, compressive sensing, andsubset selection in regression. In this paper, we take a unified approach tosupport recovery problems, considering general probabilistic observation modelsrelating a sparse data vector to an observation vector. We study theinformation-theoretic limits of both exact and partial support recovery, takinga novel approach motivated by thresholding techniques in channel coding. Weprovide general achievability and converse bounds characterizing the trade-offbetween the error probability and number of measurements, and we specializethese bounds to variants of models from group testing, linear regression, and1-bit compressive sensing. In several cases, our bounds not only providematching scaling laws in the necessary and sufficient number of measurements,but also sharp thresholds with matching constant factors. Our approach hasseveral advantages over previous approaches: For the achievability part, weobtain sharp thresholds under broader scalings of the sparsity level and otherparameters (e.g. signal-to-noise ratio) compared to several previous works, andfor the converse part, we not only provide conditions under which the errorprobability fails to vanish, but also conditions under which it tends to one.
arxiv-9000-52 | Boost Phrase-level Polarity Labelling with Review-level Sentiment Classification | http://arxiv.org/pdf/1502.03322v1.pdf | author:Yongfeng Zhang, Min Zhang, Yiqun Liu, Shaoping Ma category:cs.CL cs.AI published:2015-02-11 summary:Sentiment analysis on user reviews helps to keep track of user reactionstowards products, and make advices to users about what to buy. State-of-the-artreview-level sentiment classification techniques could give pretty goodprecisions of above 90%. However, current phrase-level sentiment analysisapproaches might only give sentiment polarity labelling precisions of around70%~80%, which is far from satisfaction and restricts its application in manypractical tasks. In this paper, we focus on the problem of phrase-levelsentiment polarity labelling and attempt to bridge the gap between phrase-leveland review-level sentiment analysis. We investigate the inconsistency betweenthe numerical star ratings and the sentiment orientation of textual userreviews. Although they have long been treated as identical, which serves as abasic assumption in previous work, we find that this assumption is notnecessarily true. We further propose to leverage the results of review-levelsentiment classification to boost the performance of phrase-level polaritylabelling using a novel constrained convex optimization framework. Besides, theframework is capable of integrating various kinds of information sources andheuristics, while giving the global optimal solution due to its convexity.Experimental results on both English and Chinese reviews show that ourframework achieves high labelling precisions of up to 89%, which is asignificant improvement from current approaches.
arxiv-9000-53 | Statistical laws in linguistics | http://arxiv.org/pdf/1502.03296v1.pdf | author:Eduardo G. Altmann, Martin Gerlach category:physics.soc-ph cs.LG published:2015-02-11 summary:Zipf's law is just one out of many universal laws proposed to describestatistical regularities in language. Here we review and critically discuss howthese laws can be statistically interpreted, fitted, and tested (falsified).The modern availability of large databases of written text allows for testswith an unprecedent statistical accuracy and also a characterization of thefluctuations around the typical behavior. We find that fluctuations are usuallymuch larger than expected based on simplifying statistical assumptions (e.g.,independence and lack of correlations between observations).Thesesimplifications appear also in usual statistical tests so that the largefluctuations can be erroneously interpreted as a falsification of the law.Instead, here we argue that linguistic laws are only meaningful (falsifiable)if accompanied by a model for which the fluctuations can be computed (e.g., agenerative model of the text). The large fluctuations we report show that theconstraints imposed by linguistic laws on the creativity process of textgeneration are not as tight as one could expect.
arxiv-9000-54 | Off-policy evaluation for MDPs with unknown structure | http://arxiv.org/pdf/1502.03255v1.pdf | author:Assaf Hallak, François Schnitzler, Timothy Mann, Shie Mannor category:stat.ML cs.LG published:2015-02-11 summary:Off-policy learning in dynamic decision problems is essential for providingstrong evidence that a new policy is better than the one in use. But how can weprove superiority without testing the new policy? To answer this question, weintroduce the G-SCOPE algorithm that evaluates a new policy based on datagenerated by the existing policy. Our algorithm is both computationally andsample efficient because it greedily learns to exploit factored structure inthe dynamics of the environment. We present a finite sample analysis of ourapproach and show through experiments that the algorithm scales well onhigh-dimensional problems with few samples.
arxiv-9000-55 | A Hybrid Approach for Improved Content-based Image Retrieval using Segmentation | http://arxiv.org/pdf/1502.03215v1.pdf | author:Smarajit Bose, Amita Pal, Jhimli Mallick, Sunil Kumar, Pratyaydipta Rudra category:cs.IR cs.CV stat.ME published:2015-02-11 summary:The objective of Content-Based Image Retrieval (CBIR) methods is essentiallyto extract, from large (image) databases, a specified number of images similarin visual and semantic content to a so-called query image. To bridge thesemantic gap that exists between the representation of an image by low-levelfeatures (namely, colour, shape, texture) and its high-level semantic contentas perceived by humans, CBIR systems typically make use of the relevancefeedback (RF) mechanism. RF iteratively incorporates user-given inputsregarding the relevance of retrieved images, to improve retrieval efficiency.One approach is to vary the weights of the features dynamically via featurereweighting. In this work, an attempt has been made to improve retrievalaccuracy by enhancing a CBIR system based on color features alone, throughimplicit incorporation of shape information obtained through prior segmentationof the images. Novel schemes for feature reweighting as well as forinitialization of the relevant set for improved relevance feedback, have alsobeen proposed for boosting performance of RF- based CBIR. At the same time, newmeasures for evaluation of retrieval accuracy have been suggested, to overcomethe limitations of existing measures in the RF context. Results of extensiveexperiments have been presented to illustrate the effectiveness of the proposedapproaches.
arxiv-9000-56 | An Extreme-Value Approach for Testing the Equality of Large U-Statistic Based Correlation Matrices | http://arxiv.org/pdf/1502.03211v1.pdf | author:Cheng Zhou, Fang Han, Xinsheng Zhang, Han Liu category:math.ST stat.ML stat.TH published:2015-02-11 summary:There has been an increasing interest in testing the equality of largePearson's correlation matrices. However, in many applications it is moreimportant to test the equality of large rank-based correlation matrices sincethey are more robust to outliers and nonlinearity. Unlike the Pearson's case,testing the equality of large rank-based statistics has not been well exploredand requires us to develop new methods and theory. In this paper, we provide aframework for testing the equality of two large U-statistic based correlationmatrices, which include the rank-based correlation matrices as special cases.Our approach exploits extreme value statistics and the Jackknife estimator foruncertainty assessment and is valid under a fully nonparametric model.Theoretically, we develop a theory for testing the equality of U-statisticbased correlation matrices. We then apply this theory to study the problem oftesting large Kendall's tau correlation matrices and demonstrate itsoptimality. For proving this optimality, a novel construction of leastfavourable distributions is developed for the correlation matrix comparison.
arxiv-9000-57 | Hierarchical Dirichlet Scaling Process | http://arxiv.org/pdf/1404.1282v3.pdf | author:Dongwoo Kim, Alice Oh category:cs.LG published:2014-03-22 summary:We present the \textit{hierarchical Dirichlet scaling process} (HDSP), aBayesian nonparametric mixed membership model. The HDSP generalizes thehierarchical Dirichlet process (HDP) to model the correlation structure betweenmetadata in the corpus and mixture components. We construct the HDSP based onthe normalized gamma representation of the Dirichlet process, and thisconstruction allows incorporating a scaling function that controls themembership probabilities of the mixture components. We develop two scalingmethods to demonstrate that different modeling assumptions can be expressed inthe HDSP. We also derive the corresponding approximate posterior inferencealgorithms using variational Bayes. Through experiments on datasets ofnewswire, medical journal articles, conference proceedings, and productreviews, we show that the HDSP results in a better predictive performance thanlabeled LDA, partially labeled LDA, and author topic model and a betternegative review classification performance than the supervised topic model andSVM.
arxiv-9000-58 | A HMAX with LLC for visual recognition | http://arxiv.org/pdf/1502.02772v2.pdf | author:Kean Hong Lau, Yong Haur Tay, Fook Loong Lo category:cs.CV published:2015-02-10 summary:Today's high performance deep artificial neural networks (ANNs) rely heavilyon parameter optimization, which is sequential in nature and even with apowerful GPU, would have taken weeks to train them up for solving challengingtasks [22]. HMAX [17] has demonstrated that a simple high performing networkcould be obtained without heavy optimization. In this paper, we had improved onthe existing best HMAX neural network [12] in terms of structural simplicityand performance. Our design replaces the L1 minimization sparse coding (SC)with a locality-constrained linear coding (LLC) [20] which has a lowercomputational demand. We also put the simple orientation filter bank back intothe front layer of the network replacing PCA. Our system's performance hasimproved over the existing architecture and reached 79.0% on the challengingCaltech-101 [7] dataset, which is state-of-the-art for ANNs (without transferlearning). From our empirical data, the main contributors to our system'sperformance include an introduction of partial signal whitening, a spotdetector, and a spatial pyramid matching (SPM) [14] layer.
arxiv-9000-59 | An Aggregation Method for Sparse Logistic Regression | http://arxiv.org/pdf/1410.6959v2.pdf | author:Zhe Liu category:stat.ML published:2014-10-25 summary:$L_1$ regularized logistic regression has now become a workhorse of datamining and bioinformatics: it is widely used for many classification problems,particularly ones with many features. However, $L_1$ regularization typicallyselects too many features and that so-called false positives are unavoidable.In this paper, we demonstrate and analyze an aggregation method for sparselogistic regression in high dimensions. This approach linearly combines theestimators from a suitable set of logistic models with different underlyingsparsity patterns and can balance the predictive ability and modelinterpretability. Numerical performance of our proposed aggregation method isthen investigated using simulation studies. We also analyze a publishedgenome-wide case-control dataset to further evaluate the usefulness of theaggregation method in multilocus association mapping.
arxiv-9000-60 | Gaussian Process Models for HRTF based Sound-Source Localization and Active-Learning | http://arxiv.org/pdf/1502.03163v1.pdf | author:Yuancheng Luo, Dmitry N. Zotkin, Ramani Duraiswami category:cs.SD cs.LG stat.ML 60G15 published:2015-02-11 summary:From a machine learning perspective, the human ability localize sounds can bemodeled as a non-parametric and non-linear regression problem between binauralspectral features of sound received at the ears (input) and their sound-sourcedirections (output). The input features can be summarized in terms of theindividual's head-related transfer functions (HRTFs) which measure the spectralresponse between the listener's eardrum and an external point in $3$D. Based onthese viewpoints, two related problems are considered: how can one achieve anoptimal sampling of measurements for training sound-source localization (SSL)models, and how can SSL models be used to infer the subject's HRTFs inlistening tests. First, we develop a class of binaural SSL models based onGaussian process regression and solve a \emph{forward selection} problem thatfinds a subset of input-output samples that best generalize to all SSLdirections. Second, we use an \emph{active-learning} approach that updates anonline SSL model for inferring the subject's SSL errors via headphones and agraphical user interface. Experiments show that only a small fraction of HRTFsare required for $5^{\circ}$ localization accuracy and that the learned HRTFsare localized closer to their intended directions than non-individualizedHRTFs.
arxiv-9000-61 | Kernel Task-Driven Dictionary Learning for Hyperspectral Image Classification | http://arxiv.org/pdf/1502.03126v1.pdf | author:Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, Kenneth W. Jenkins category:stat.ML cs.CV cs.LG published:2015-02-10 summary:Dictionary learning algorithms have been successfully used in bothreconstructive and discriminative tasks, where the input signal is representedby a linear combination of a few dictionary atoms. While these methods areusually developed under $\ell_1$ sparsity constrain (prior) in the inputdomain, recent studies have demonstrated the advantages of sparserepresentation using structured sparsity priors in the kernel domain. In thispaper, we propose a supervised dictionary learning algorithm in the kerneldomain for hyperspectral image classification. In the proposed formulation, thedictionary and classifier are obtained jointly for optimal classificationperformance. The supervised formulation is task-driven and provides learnedfeatures from the hyperspectral data that are well suited for theclassification task. Moreover, the proposed algorithm uses a joint($\ell_{12}$) sparsity prior to enforce collaboration among the neighboringpixels. The simulation results illustrate the efficiency of the proposeddictionary learning algorithm.
arxiv-9000-62 | Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation | http://arxiv.org/pdf/1502.03121v1.pdf | author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV published:2015-02-10 summary:This paper proposes a fast multi-band image fusion algorithm, which combinesa high-spatial low-spectral resolution image and a low-spatial high-spectralresolution image. The well admitted forward model is explored to form thelikelihoods of the observations. Maximizing the likelihoods leads to solving aSylvester equation. By exploiting the properties of the circulant anddownsampling matrices associated with the fusion problem, a closed-formsolution for the corresponding Sylvester equation is obtained explicitly,getting rid of any iterative update step. Coupled with the alternatingdirection method of multipliers and the block coordinate descent method, theproposed algorithm can be easily generalized to incorporate prior informationfor the fusion problem, allowing a Bayesian estimator. Simulation results showthat the proposed algorithm achieves the same performance as existingalgorithms with the advantage of significantly decreasing the computationalcomplexity of these algorithms.
arxiv-9000-63 | Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems | http://arxiv.org/pdf/1411.1134v3.pdf | author:Christopher De Sa, Kunle Olukotun, Christopher Ré category:cs.LG math.OC stat.ML published:2014-11-05 summary:Stochastic gradient descent (SGD) on a low-rank factorization is commonlyemployed to speed up matrix problems including matrix completion, subspacetracking, and SDP relaxation. In this paper, we exhibit a step size scheme forSGD on a low-rank least-squares problem, and we prove that, under broadsampling conditions, our method converges globally from a random starting pointwithin $O(\epsilon^{-1} n \log n)$ steps with constant probability forconstant-rank problems. Our modification of SGD relates it to stochastic poweriteration. We also show experiments to illustrate the runtime and convergenceof the algorithm.
arxiv-9000-64 | Empirically Estimable Classification Bounds Based on a New Divergence Measure | http://arxiv.org/pdf/1412.6534v2.pdf | author:Visar Berisha, Alan Wisler, Alfred O. Hero, Andreas Spanias category:cs.IT math.IT stat.ML published:2014-12-19 summary:Information divergence functions play a critical role in statistics andinformation theory. In this paper we show that a non-parametric f-divergencemeasure can be used to provide improved bounds on the minimum binaryclassification probability of error for the case when the training and testdata are drawn from the same distribution and for the case where there existssome mismatch between training and test distributions. We confirm thetheoretical results by designing feature selection algorithms using thecriteria from these bounds and by evaluating the algorithms on a series ofpathological speech classification tasks.
arxiv-9000-65 | Video Primal Sketch: A Unified Middle-Level Representation for Video | http://arxiv.org/pdf/1502.02965v1.pdf | author:Zhi Han, Zongben Xu, Song-Chun Zhu category:cs.CV published:2015-02-10 summary:This paper presents a middle-level video representation named Video PrimalSketch (VPS), which integrates two regimes of models: i) sparse coding modelusing static or moving primitives to explicitly represent moving corners,lines, feature points, etc., ii) FRAME /MRF model reproducing featurestatistics extracted from input video to implicitly represent textured motion,such as water and fire. The feature statistics include histograms ofspatio-temporal filters and velocity distributions. This paper makes threecontributions to the literature: i) Learning a dictionary of video primitivesusing parametric generative models; ii) Proposing the Spatio-Temporal FRAME(ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling andsynthesizing textured motion; and iii) Developing a parsimonious hybrid modelfor generic video representation. Given an input video, VPS selects the propermodels automatically for different motion patterns and is compatible withhigh-level action representations. In the experiments, we synthesize a numberof textured motion; reconstruct real videos using the VPS; report a series ofhuman perception experiments to verify the quality of reconstructed videos;demonstrate how the VPS changes over the scale transition in videos; andpresent the close connection between VPS and high-level action models.
arxiv-9000-66 | Characterization and Compensation of Network-Level Anomalies in Mixed-Signal Neuromorphic Modeling Platforms | http://arxiv.org/pdf/1404.7514v2.pdf | author:Mihai A. Petrovici, Bernhard Vogginger, Paul Müller, Oliver Breitwieser, Mikael Lundqvist, Lyle Muller, Matthias Ehrlich, Alain Destexhe, Anders Lansner, René Schüffny, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE published:2014-04-29 summary:Advancing the size and complexity of neural network models leads to an everincreasing demand for computational resources for their simulation.Neuromorphic devices offer a number of advantages over conventional computingarchitectures, such as high emulation speed or low power consumption, but thisusually comes at the price of reduced configurability and precision. In thisarticle, we investigate the consequences of several such factors that arecommon to neuromorphic devices, more specifically limited hardware resources,limited parameter configurability and parameter variations. Our final aim is toprovide an array of methods for coping with such inevitable distortionmechanisms. As a platform for testing our proposed strategies, we use anexecutable system specification (ESS) of the BrainScaleS neuromorphic system,which has been designed as a universal emulation back-end for neuroscientificmodeling. We address the most essential limitations of this device in detailand study their effects on three prototypical benchmark network models within awell-defined, systematic workflow. For each network model, we start by definingquantifiable functionality measures by which we then assess the effects oftypical hardware-specific distortion mechanisms, both in idealized softwaresimulations and on the ESS. For those effects that cause unacceptabledeviations from the original network dynamics, we suggest generic compensationmechanisms and demonstrate their effectiveness. Both the suggested workflow andthe investigated compensation mechanisms are largely back-end independent anddo not require additional hardware configurability beyond the one required toemulate the benchmark networks in the first place. We hereby provide a genericmethodological environment for configurable neuromorphic devices that aretargeted at emulating large-scale, functional neural networks.
arxiv-9000-67 | Real Time Implementation of Spatial Filtering On FPGA | http://arxiv.org/pdf/1502.02905v1.pdf | author:Chaitannya Supe category:cs.CV published:2015-02-10 summary:Field Programmable Gate Array (FPGA) technology has gained vital importancemainly because of its parallel processing hardware which makes it ideal forimage and video processing. In this paper, a step by step approach to apply alinear spatial filter on real time video frame sent by Omnivision OV7670 camerausing Zynq Evaluation and Development board based on Xilinx XC7Z020 has beendiscussed. Face detection application was chosen to explain above procedure.This procedure is applicable to most of the complex image processing algorithmswhich needs to be implemented using FPGA.
arxiv-9000-68 | Adjusting Leverage Scores by Row Weighting: A Practical Approach to Coherent Matrix Completion | http://arxiv.org/pdf/1412.7938v2.pdf | author:Shusen Wang, Tong Zhang, Zhihua Zhang category:cs.LG stat.ML published:2014-12-26 summary:Low-rank matrix completion is an important problem with extensive real-worldapplications. When observations are uniformly sampled from the underlyingmatrix entries, existing methods all require the matrix to be incoherent. Thispaper provides the first working method for coherent matrix completion underthe standard uniform sampling model. Our approach is based on the weightednuclear norm minimization idea proposed in several recent work, and our keycontribution is a practical method to compute the weighting matrices so thatthe leverage scores become more uniform after weighting. Under suitableconditions, we are able to derive theoretical results, showing theeffectiveness of our approach. Experiments on synthetic data show that ourapproach recovers highly coherent matrices with high precision, whereas thestandard unweighted method fails even on noise-free data.
arxiv-9000-69 | Talk to the Hand: Generating a 3D Print from Photographs | http://arxiv.org/pdf/1502.02871v1.pdf | author:Edward Aboufadel, Sylvanna V. Krawczyk, Melissa Sherman-Bennett category:math.HO cs.CV published:2015-02-10 summary:This manuscript presents a linear algebra-based technique that only requirestwo unique photographs from a digital camera to mathematically construct a 3Dsurface representation which can then be 3D printed. Basic computer visiontheory and manufacturing principles are also briefly discussed.
arxiv-9000-70 | Gaussian Processes for Data-Efficient Learning in Robotics and Control | http://arxiv.org/pdf/1502.02860v1.pdf | author:Marc Peter Deisenroth, Dieter Fox, Carl Edward Rasmussen category:stat.ML cs.LG cs.RO cs.SY published:2015-02-10 summary:Autonomous learning has been a promising direction in control and roboticsfor more than a decade since data-driven learning allows to reduce the amountof engineering knowledge, which is otherwise required. However, autonomousreinforcement learning (RL) approaches typically require many interactions withthe system to learn controllers, which is a practical limitation in realsystems, such as robots, where many interactions can be impractical and timeconsuming. To address this problem, current learning approaches typicallyrequire task-specific knowledge in form of expert demonstrations, realisticsimulators, pre-shaped policies, or specific knowledge about the underlyingdynamics. In this article, we follow a different approach and speed up learningby extracting more information from data. In particular, we learn aprobabilistic, non-parametric Gaussian process transition model of the system.By explicitly incorporating model uncertainty into long-term planning andcontroller learning our approach reduces the effects of model errors, a keyproblem in model-based learning. Compared to state-of-the art RL ourmodel-based policy search method achieves an unprecedented speed of learning.We demonstrate its applicability to autonomous learning in real robot andcontrol tasks.
arxiv-9000-71 | A Global Approach for Solving Edge-Matching Puzzles | http://arxiv.org/pdf/1409.5957v2.pdf | author:Shahar Z. Kovalsky, Daniel Glasner, Ronen Basri category:cs.CV published:2014-09-21 summary:We consider apictorial edge-matching puzzles, in which the goal is to arrangea collection of puzzle pieces with colored edges so that the colors match alongthe edges of adjacent pieces. We devise an algebraic representation for thisproblem and provide conditions under which it exactly characterizes a puzzle.Using the new representation, we recast the combinatorial, discrete problem ofsolving puzzles as a global, polynomial system of equations with continuousvariables. We further propose new algorithms for generating approximatesolutions to the continuous problem by solving a sequence of convexrelaxations.
arxiv-9000-72 | Sequential Kernel Herding: Frank-Wolfe Optimization for Particle Filtering | http://arxiv.org/pdf/1501.02056v2.pdf | author:Simon Lacoste-Julien, Fredrik Lindsten, Francis Bach category:stat.ML cs.LG published:2015-01-09 summary:Recently, the Frank-Wolfe optimization algorithm was suggested as a procedureto obtain adaptive quadrature rules for integrals of functions in a reproducingkernel Hilbert space (RKHS) with a potentially faster rate of convergence thanMonte Carlo integration (and "kernel herding" was shown to be a special case ofthis procedure). In this paper, we propose to replace the random sampling stepin a particle filter by Frank-Wolfe optimization. By optimizing the position ofthe particles, we can obtain better accuracy than random or quasi-Monte Carlosampling. In applications where the evaluation of the emission probabilities isexpensive (such as in robot localization), the additional computational cost togenerate the particles through optimization can be justified. Experiments onstandard synthetic examples as well as on a robot localization task indicateindeed an improvement of accuracy over random and quasi-Monte Carlo sampling.
arxiv-9000-73 | The Benefit of Sex in Noisy Evolutionary Search | http://arxiv.org/pdf/1502.02793v1.pdf | author:Tobias Friedrich, Timo Kötzing, Martin Krejca, Andrew M. Sutton category:cs.NE published:2015-02-10 summary:The benefit of sexual recombination is one of the most fundamental questionsboth in population genetics and evolutionary computation. It is widely believedthat recombination helps solving difficult optimization problems. We presentthe first result, which rigorously proves that it is beneficial to use sexualrecombination in an uncertain environment with a noisy fitness function. Forthis, we model sexual recombination with a simple estimation of distributionalgorithm called the Compact Genetic Algorithm (cGA), which we compare with theclassical $\mu+1$ EA. For a simple noisy fitness function with additiveGaussian posterior noise $\mathcal{N}(0,\sigma^2)$, we prove that themutation-only $\mu+1$ EA typically cannot handle noise in polynomial time for$\sigma^2$ large enough while the cGA runs in polynomial time as long as thepopulation size is not too small. This shows that in this uncertain environmentsexual recombination is provably beneficial. We observe the same behavior in asmall empirical study.
arxiv-9000-74 | Generative Moment Matching Networks | http://arxiv.org/pdf/1502.02761v1.pdf | author:Yujia Li, Kevin Swersky, Richard Zemel category:cs.LG cs.AI stat.ML published:2015-02-10 summary:We consider the problem of learning deep generative models from data. Weformulate a method that generates an independent sample via a singlefeedforward pass through a multilayer perceptron, as in the recently proposedgenerative adversarial networks (Goodfellow et al., 2014). Training agenerative adversarial network, however, requires careful optimization of adifficult minimax program. Instead, we utilize a technique from statisticalhypothesis testing known as maximum mean discrepancy (MMD), which leads to asimple objective that can be interpreted as matching all orders of statisticsbetween a dataset and samples from the model, and can be trained bybackpropagation. We further boost the performance of this approach by combiningour generative network with an auto-encoder network, using MMD to learn togenerate codes that can then be decoded to produce samples. We show that thecombination of these techniques yields excellent generative models compared tobaseline approaches as measured on MNIST and the Toronto Face Database.
arxiv-9000-75 | Particle Gibbs with Ancestor Sampling for Probabilistic Programs | http://arxiv.org/pdf/1501.06769v5.pdf | author:Jan-Willem van de Meent, Hongseok Yang, Vikash Mansinghka, Frank Wood category:stat.ML cs.AI cs.PL published:2015-01-27 summary:Particle Markov chain Monte Carlo techniques rank among currentstate-of-the-art methods for probabilistic program inference. A drawback ofthese techniques is that they rely on importance resampling, which results indegenerate particle trajectories and a low effective sample size for variablessampled early in a program. We here develop a formalism to adapt ancestorresampling, a technique that mitigates particle degeneracy, to theprobabilistic programming setting. We present empirical results thatdemonstrate nontrivial performance gains.
arxiv-9000-76 | Learning Reductions that Really Work | http://arxiv.org/pdf/1502.02704v1.pdf | author:Alina Beygelzimer, Hal Daumé III, John Langford, Paul Mineiro category:cs.LG published:2015-02-09 summary:We provide a summary of the mathematical and computational techniques thathave enabled learning reductions to effectively address a wide class ofproblems, and show that this approach to solving machine learning problems canbe broadly useful.
arxiv-9000-77 | Optimal and Adaptive Algorithms for Online Boosting | http://arxiv.org/pdf/1502.02651v1.pdf | author:Alina Beygelzimer, Satyen Kale, Haipeng Luo category:cs.LG published:2015-02-09 summary:We study online boosting, the task of converting any weak online learner intoa strong online learner. Based on a novel and natural definition of weak onlinelearnability, we develop two online boosting algorithms. The first algorithm isan online version of boost-by-majority. By proving a matching lower bound, weshow that this algorithm is essentially optimal in terms of the number of weaklearners and the sample complexity needed to achieve a specified accuracy. Thisoptimal algorithm is not adaptive however. Using tools from online lossminimization, we derive an adaptive online boosting algorithm that is alsoparameter-free, but not optimal. Both algorithms work with base learners thatcan handle example importance weights directly, as well as by rejectionsampling examples with probability defined by the booster. Results arecomplemented with an extensive experimental study.
arxiv-9000-78 | Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions | http://arxiv.org/pdf/1502.02643v1.pdf | author:Alina Ene, Huy L. Nguyen category:cs.LG cs.AI published:2015-02-09 summary:Submodular function minimization is a fundamental optimization problem thatarises in several applications in machine learning and computer vision. Theproblem is known to be solvable in polynomial time, but general purposealgorithms have high running times and are unsuitable for large-scale problems.Recent work have used convex optimization techniques to obtain very practicalalgorithms for minimizing functions that are sums of ``simple" functions. Inthis paper, we use random coordinate descent methods to obtain algorithms withfaster linear convergence rates and cheaper iteration costs. Compared toalternating projection methods, our algorithms do not rely on full-dimensionalvector operations and they converge in significantly fewer iterations.
arxiv-9000-79 | From Predictive to Prescriptive Analytics | http://arxiv.org/pdf/1402.5481v3.pdf | author:Dimitris Bertsimas, Nathan Kallus category:stat.ML cs.LG math.OC published:2014-02-22 summary:In this paper, we combine ideas from machine learning (ML) and operationsresearch and management science (OR/MS) in developing a framework, along withspecific methods, for using data to prescribe decisions in OR/MS problems. In adeparture from other work on data-driven optimization and reflecting ourpractical experience with the data available in applications of OR/MS, weconsider data consisting, not only of observations of quantities with directeffect on costs/revenues, such as demand or returns, but predominantly ofobservations of associated auxiliary quantities. The main problem of interestis a conditional stochastic optimization problem, given imperfect observations,where the joint probability distributions that specify the problem are unknown.We demonstrate that our proposed solution methods are generally applicable to awide range of decision problems. We prove that they are computationallytractable and asymptotically optimal under mild conditions even when data isnot independent and identically distributed (iid) and even for censoredobservations. As an analogue to the coefficient of determination $R^2$, wedevelop a metric $P$ termed the coefficient of prescriptiveness to measure theprescriptive content of data and the efficacy of a policy from an operationsperspective. To demonstrate the power of our approach in a real-world settingwe study an inventory management problem faced by the distribution arm of aninternational media conglomerate, which ships an average of 1 billion units peryear. We leverage both internal data and public online data harvested fromIMDb, Rotten Tomatoes, and Google to prescribe operational decisions thatoutperform baseline measures. Specifically, the data we collect, leveraged byour methods, accounts for an 88% improvement as measured by our coefficient ofprescriptiveness.
arxiv-9000-80 | Complex-Valued Hough Transforms for Circles | http://arxiv.org/pdf/1502.00558v2.pdf | author:Marcelo Cicconet, Davi Geiger, Michael Werman category:cs.CV published:2015-02-02 summary:This paper advocates the use of complex variables to represent votes in theHough transform for circle detection. Replacing the positive numbersclassically used in the parameter space of the Hough transforms by complexnumbers allows cancellation effects when adding up the votes. Cancellation andthe computation of shape likelihood via a complex number's magnitude squarelead to more robust solutions than the "classic" algorithms, as shown bycomputational experiments on synthetic and real datasets.
arxiv-9000-81 | Quantum Pairwise Symmetry: Applications in 2D Shape Analysis | http://arxiv.org/pdf/1502.00561v2.pdf | author:Marcelo Cicconet, Davi Geiger, Michael Werman category:cs.CV published:2015-02-02 summary:A pair of rooted tangents -- defining a quantum triangle -- with anassociated quantum wave of spin 1/2 is proposed as the primitive to representand compute symmetry. Measures of the spin characterize how "isosceles" or how"degenerate" these triangles are -- which corresponds to their mirror orparallel symmetry. We also introduce a complex-valued kernel to modelprobability errors in the parameter space, which is more robust to noise andclutter than the classical model.
arxiv-9000-82 | Efficient model-based reinforcement learning for approximate online optimal | http://arxiv.org/pdf/1502.02609v1.pdf | author:Rushikesh Kamalapurkar, Joel A. Rosenfeld, Warren E. Dixon category:cs.SY cs.LG math.OC published:2015-02-09 summary:In this paper the infinite horizon optimal regulation problem is solvedonline for a deterministic control-affine nonlinear dynamical system using thestate following (StaF) kernel method to approximate the value function. Unliketraditional methods that aim to approximate a function over a large compactset, the StaF kernel method aims to approximate a function in a smallneighborhood of a state that travels within a compact set. Simulation resultsdemonstrate that stability and approximate optimality of the control system canbe achieved with significantly fewer basis functions than may be required forglobal approximation methods.
arxiv-9000-83 | Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction | http://arxiv.org/pdf/1502.02599v1.pdf | author:Mohamed Elshrif, Ernest Fokoue category:cs.LG published:2015-02-09 summary:We present a novel adaptive random subspace learning algorithm (RSSL) forprediction purpose. This new framework is flexible where it can be adapted withany learning technique. In this paper, we tested the algorithm for regressionand classification problems. In addition, we provide a variety of weightingschemes to increase the robustness of the developed algorithm. These differentwighting flavors were evaluated on simulated as well as on real-world data setsconsidering the cases where the ratio between features (attributes) andinstances (samples) is large and vice versa. The framework of the new algorithmconsists of many stages: first, calculate the weights of all features on thedata set using the correlation coefficient and F-statistic statisticalmeasurements. Second, randomly draw n samples with replacement from the dataset. Third, perform regular bootstrap sampling (bagging). Fourth, draw withoutreplacement the indices of the chosen variables. The decision was taken basedon the heuristic subspacing scheme. Fifth, call base learners and build themodel. Sixth, use the model for prediction purpose on test set of the data. Theresults show the advancement of the adaptive RSSL algorithm in most of thecases compared with the synonym (conventional) machine learning algorithms.
arxiv-9000-84 | Domain-Adversarial Neural Networks | http://arxiv.org/pdf/1412.4446v2.pdf | author:Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand category:stat.ML cs.LG cs.NE published:2014-12-15 summary:We introduce a new representation learning algorithm suited to the context ofdomain adaptation, in which data at training and test time come from similarbut different distributions. Our algorithm is directly inspired by theory ondomain adaptation suggesting that, for effective domain transfer to beachieved, predictions must be made based on a data representation that cannotdiscriminate between the training (source) and test (target) domains. Wepropose a training objective that implements this idea in the context of aneural network, whose hidden layer is trained to be predictive of theclassification task, but uninformative as to the domain of the input. Ourexperiments on a sentiment analysis classification benchmark, where the targetdomain data available at training time is unlabeled, show that our neuralnetwork for domain adaption algorithm has better performance than either astandard neural network or an SVM, even if trained on input features extractedwith the state-of-the-art marginalized stacked denoising autoencoders of Chenet al. (2012).
arxiv-9000-85 | Deep Learning with Limited Numerical Precision | http://arxiv.org/pdf/1502.02551v1.pdf | author:Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan category:cs.LG cs.NE stat.ML published:2015-02-09 summary:Training of large-scale deep neural networks is often constrained by theavailable computational resources. We study the effect of limited precisiondata representation and computation on neural network training. Within thecontext of low-precision fixed-point computations, we observe the roundingscheme to play a crucial role in determining the network's behavior duringtraining. Our results show that deep networks can be trained using only 16-bitwide fixed-point number representation when using stochastic rounding, andincur little to no degradation in the classification accuracy. We alsodemonstrate an energy-efficient hardware accelerator that implementslow-precision fixed-point arithmetic with stochastic rounding.
arxiv-9000-86 | Unbiased Bayes for Big Data: Paths of Partial Posteriors | http://arxiv.org/pdf/1501.03326v2.pdf | author:Heiko Strathmann, Dino Sejdinovic, Mark Girolami category:stat.ML cs.LG stat.ME published:2015-01-14 summary:A key quantity of interest in Bayesian inference are expectations offunctions with respect to a posterior distribution. Markov Chain Monte Carlo isa fundamental tool to consistently compute these expectations via averagingsamples drawn from an approximate posterior. However, its feasibility is beingchallenged in the era of so called Big Data as all data needs to be processedin every iteration. Realising that such simulation is an unnecessarily hardproblem if the goal is estimation, we construct a computationally scalablemethodology that allows unbiased estimation of the required expectations --without explicit simulation from the full posterior. The scheme's variance isfinite by construction and straightforward to control, leading to algorithmsthat are provably unbiased and naturally arrive at a desired error tolerance.This is achieved at an average computational complexity that is sub-linear inthe size of the dataset and its free parameters are easy to tune. Wedemonstrate the utility and generality of the methodology on a range of commonstatistical models applied to large-scale benchmark and real-world datasets.
arxiv-9000-87 | Evaluation of modelling approaches for predicting the spatial distribution of soil organic carbon stocks at the national scale | http://arxiv.org/pdf/1502.02513v1.pdf | author:M. P. Martin, T. G. Orton, E. Lacarce, J. Meersmans, N. P. A. Saby, J. B. Paroissien, C. Jolivet, L. Boulonne, D. Arrouays category:stat.AP stat.ML published:2015-02-09 summary:Soil organic carbon (SOC) plays a major role in the global carbon budget. Itcan act as a source or a sink of atmospheric carbon, thereby possiblyinfluencing the course of climate change. Improving the tools that model thespatial distributions of SOC stocks at national scales is a priority, both formonitoring changes in SOC and as an input for global carbon cycles studies. Inthis paper, we compare and evaluate two recent and promising modellingapproaches. First, we considered several increasingly complex boostedregression trees (BRT), a convenient and efficient multiple regression modelfrom the statistical learning field. Further, we considered a robustgeostatistical approach coupled to the BRT models. Testing the differentapproaches was performed on the dataset from the French Soil MonitoringNetwork, with a consistent cross-validation procedure. We showed that when alimited number of predictors were included in the BRT model, the standalone BRTpredictions were significantly improved by robust geostatistical modelling ofthe residuals. However, when data for several SOC drivers were included, thestandalone BRT model predictions were not significantly improved bygeostatistical modelling. Therefore, in this latter situation, the BRTpredictions might be considered adequate without the need for geostatisticalmodelling, provided that i) care is exercised in model fitting and validating,and ii) the dataset does not allow for modelling of local spatialautocorrelations, as is the case for many national systematic sampling schemes.
arxiv-9000-88 | The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster Technique | http://arxiv.org/pdf/1502.02512v1.pdf | author:H. M. de Oliveira category:stat.ME cs.LG stat.AP published:2015-02-09 summary:In this paper a variant of the classical hierarchical cluster analysis isreported. This agglomerative (bottom-up) cluster technique is referred to asthe Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkagealgorithm where the value of the threshold is conveniently up-dated at eachinteraction. The superiority of the adaptive clustering with respect to theaverage-linkage algorithm follows because it achieves a good compromise onthreshold values: Thresholds based on the cut-off distance are sufficientlysmall to assure the homogeneity and also large enough to guarantee at least apair of merging sets. This approach is applied to a set of possiblesubstituents in a chemical series.
arxiv-9000-89 | Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks | http://arxiv.org/pdf/1502.02506v1.pdf | author:Adrien Payan, Giovanni Montana category:cs.CV cs.LG stat.AP stat.ML published:2015-02-09 summary:Pattern recognition methods using neuroimaging data for the diagnosis ofAlzheimer's disease have been the subject of extensive research in recentyears. In this paper, we use deep learning methods, and in particular sparseautoencoders and 3D convolutional neural networks, to build an algorithm thatcan predict the disease status of a patient, based on an MRI scan of the brain.We report on experiments using the ADNI data set involving 2,265 historicalscans. We demonstrate that 3D convolutional neural networks outperform severalother classifiers reported in the literature and produce state-of-art results.
arxiv-9000-90 | Efficient batchwise dropout training using submatrices | http://arxiv.org/pdf/1502.02478v1.pdf | author:Ben Graham, Jeremy Reizenstein, Leigh Robinson category:cs.NE cs.CV published:2015-02-09 summary:Dropout is a popular technique for regularizing artificial neural networks.Dropout networks are generally trained by minibatch gradient descent with adropout mask turning off some of the units---a different pattern of dropout isapplied to every sample in the minibatch. We explore a very simple alternativeto the dropout mask. Instead of masking dropped out units by setting them tozero, we perform matrix multiplication using a submatrix of the weightmatrix---unneeded hidden units are never calculated. Performing dropoutbatchwise, so that one pattern of dropout is used for each sample in aminibatch, we can substantially reduce training times. Batchwise dropout can beused with fully-connected and convolutional neural networks.
arxiv-9000-91 | Cognitive Learning of Statistical Primary Patterns via Bayesian Network | http://arxiv.org/pdf/1409.7930v5.pdf | author:Weijia Han, Huiyan Sang, Min Sheng, Jiandong Li, Shuguang Cui category:cs.LG published:2014-09-28 summary:In cognitive radio (CR) technology, the trend of sensing is no longer to onlydetect the presence of active primary users. A large number of applicationsdemand for more comprehensive knowledge on primary user behaviors in spatial,temporal, and frequency domains. To satisfy such requirements, we study thestatistical relationship among primary users by introducing a Bayesian network(BN) based framework. How to learn such a BN structure is a long standingissue, not fully understood even in the statistical learning community.Besides, another key problem in this learning scenario is that the CR has toidentify how many variables are in the BN, which is usually considered as priorknowledge in statistical learning applications. To solve such two issuessimultaneously, this paper proposes a BN structure learning scheme consistingof an efficient structure learning algorithm and a blind variableidentification scheme. The proposed approach incurs significantly lowercomputational complexity compared with previous ones, and is capable ofdetermining the structure without assuming much prior knowledge aboutvariables. With this result, cognitive users could efficiently understand thestatistical pattern of primary networks, such that more efficient cognitiveprotocols could be designed across different network layers.
arxiv-9000-92 | On the Dynamics of a Recurrent Hopfield Network | http://arxiv.org/pdf/1502.02444v1.pdf | author:Rama Garimella, Berkay Kicanaoglu, Moncef Gabbouj category:cs.NE published:2015-02-09 summary:In this research paper novel real/complex valued recurrent Hopfield NeuralNetwork (RHNN) is proposed. The method of synthesizing the energy landscape ofsuch a network and the experimental investigation of dynamics of RecurrentHopfield Network is discussed. Parallel modes of operation (other than fullyparallel mode) in layered RHNN is proposed. Also, certain potentialapplications are proposed.
arxiv-9000-93 | Out-of-sample generalizations for supervised manifold learning for classification | http://arxiv.org/pdf/1502.02410v1.pdf | author:Elif Vural, Christine Guillemot category:cs.CV cs.LG published:2015-02-09 summary:Supervised manifold learning methods for data classification map data samplesresiding in a high-dimensional ambient space to a lower-dimensional domain in astructure-preserving way, while enhancing the separation between differentclasses in the learned embedding. Most nonlinear supervised manifold learningmethods compute the embedding of the manifolds only at the initially availabletraining points, while the generalization of the embedding to novel points,known as the out-of-sample extension problem in manifold learning, becomesespecially important in classification applications. In this work, we propose asemi-supervised method for building an interpolation function that provides anout-of-sample extension for general supervised manifold learning algorithmsstudied in the context of classification. The proposed algorithm computes aradial basis function (RBF) interpolator that minimizes an objective functionconsisting of the total embedding error of unlabeled test samples, defined astheir distance to the embeddings of the manifolds of their own class, as wellas a regularization term that controls the smoothness of the interpolationfunction in a direction-dependent way. The class labels of test data and theinterpolation function parameters are estimated jointly with a progressiveprocedure. Experimental results on face and object images demonstrate thepotential of the proposed out-of-sample extension algorithm for theclassification of manifold-modeled data sets.
arxiv-9000-94 | A Social Spider Algorithm for Global Optimization | http://arxiv.org/pdf/1502.02407v1.pdf | author:James J. Q. Yu, Victor O. K. Li category:cs.NE published:2015-02-09 summary:The growing complexity of real-world problems has motivated computerscientists to search for efficient problem-solving methods. Metaheuristicsbased on evolutionary computation and swarm intelligence are outstandingexamples of nature-inspired solution techniques. Inspired by the socialspiders, we propose a novel Social Spider Algorithm to solve globaloptimization problems. This algorithm is mainly based on the foraging strategyof social spiders, utilizing the vibrations on the spider web to determine thepositions of preys. Different from the previously proposed swarm intelligencealgorithms, we introduce a new social animal foraging strategy model to solveoptimization problems. In addition, we perform preliminary parametersensitivity analysis for our proposed algorithm, developing guidelines forchoosing the parameter values. The Social Spider Algorithm is evaluated by aseries of widely-used benchmark functions, and our proposed algorithm hassuperior performance compared with other state-of-the-art metaheuristics.
arxiv-9000-95 | Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction | http://arxiv.org/pdf/1502.02330v1.pdf | author:Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu category:stat.ML cs.CV cs.LG published:2015-02-09 summary:Canonical correlation analysis (CCA) has proven an effective tool fortwo-view dimension reduction due to its profound theoretical foundation andsuccess in practical applications. In respect of multi-view learning, however,it is limited by its capability of only handling data represented by two-viewfeatures, while in many real-world applications, the number of views isfrequently many more. Although the ad hoc way of simultaneously exploring allpossible pairs of features can numerically deal with multi-view data, itignores the high order statistics (correlation information) which can only bediscovered by simultaneously exploring all features. Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardlyyet naturally generalizes CCA to handle the data of an arbitrary number ofviews by analyzing the covariance tensor of the different views. TCCA aims todirectly maximize the canonical correlation of multiple (more than two) views.Crucially, we prove that the multi-view canonical correlation maximizationproblem is equivalent to finding the best rank-1 approximation of the datacovariance tensor, which can be solved efficiently using the well-knownalternating least squares (ALS) algorithm. As a consequence, the high ordercorrelation information contained in the different views is explored and thus amore reliable common subspace shared by all features can be obtained. Inaddition, a non-linear extension of TCCA is presented. Experiments on variouschallenge tasks, including large scale biometric structure prediction, internetadvertisement classification and web image annotation, demonstrate theeffectiveness of the proposed method.
arxiv-9000-96 | Sparse And Low Rank Decomposition Based Batch Image Alignment for Speckle Reduction of retinal OCT Images | http://arxiv.org/pdf/1411.4033v3.pdf | author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV published:2014-11-14 summary:Optical Coherence Tomography (OCT) is an emerging technique in the field ofbiomedical imaging, with applications in ophthalmology, dermatology, coronaryimaging etc. Due to the underlying physics, OCT images usually suffer from agranular pattern, called speckle noise, which restricts the process ofinterpretation. Here, a sparse and low rank decomposition based method is usedfor speckle reduction in retinal OCT images. This technique works on input datathat consists of several B-scans of the same location. The next step is thebatch alignment of the images using a sparse and low-rank decomposition basedtechnique. Finally the denoised image is created by median filtering of thelow-rank component of the processed data. Simultaneous decomposition andalignment of the images result in better performance in comparison to simpleregistration-based methods that are used in the literature for noise reductionof OCT images.
arxiv-9000-97 | Supervised Hashing Using Graph Cuts and Boosted Decision Trees | http://arxiv.org/pdf/1408.5574v2.pdf | author:Guosheng Lin, Chunhua Shen, Anton van den Hengel category:cs.LG cs.CV published:2014-08-24 summary:Embedding image features into a binary Hamming space can improve both thespeed and accuracy of large-scale query-by-example image retrieval systems.Supervised hashing aims to map the original features to compact binary codes ina manner which preserves the label-based similarities of the original data.Most existing approaches apply a single form of hash function, and anoptimization process which is typically deeply coupled to this specific form.This tight coupling restricts the flexibility of those methods, and can resultin complex optimization problems that are difficult to solve. In this work weproffer a flexible yet simple framework that is able to accommodate differenttypes of loss functions and hash functions. The proposed framework allows anumber of existing approaches to hashing to be placed in context, andsimplifies the development of new problem-specific hashing methods. Ourframework decomposes the into two steps: binary code (hash bits) learning, andhash function learning. The first step can typically be formulated as a binaryquadratic problem, and the second step can be accomplished by training standardbinary classifiers. For solving large-scale binary code inference, we show howto ensure that the binary quadratic problems are submodular such that anefficient graph cut approach can be used. To achieve efficiency as well asefficacy on large-scale high-dimensional data, we propose to use boosteddecision trees as the hash functions, which are nonlinear, highly descriptive,and very fast to train and evaluate. Experiments demonstrate that our proposedmethod significantly outperforms most state-of-the-art methods, especially onhigh-dimensional data.
arxiv-9000-98 | Measuring the functional connectome "on-the-fly": towards a new control signal for fMRI-based brain-computer interfaces | http://arxiv.org/pdf/1502.02309v1.pdf | author:Ricardo Pio Monti, Romy Lorenz, Christoforos Anagnostopoulos, Robert Leech, Giovanni Montana category:stat.ML published:2015-02-08 summary:There has been an explosion of interest in functional Magnetic ResonanceImaging (MRI) during the past two decades. Naturally, this has been accompaniedby many major advances in the understanding of the human connectome. Theseadvances have served to pose novel challenges as well as open new avenues forresearch. One of the most promising and exciting of such avenues is the studyof functional MRI in real-time. Such studies have recently gained momentum andhave been applied in a wide variety of settings; ranging from training ofhealthy subjects to self-regulate neuronal activity to being suggested aspotential treatments for clinical populations. To date, the vast majority ofthese studies have focused on a single region at a time. This is due in part tothe many challenges faced when estimating dynamic functional connectivitynetworks in real-time. In this work we propose a novel methodology with whichto accurately track changes in functional connectivity networks in real-time.We adapt the recently proposed SINGLE algorithm for estimating sparse andtemporally homo- geneous dynamic networks to be applicable in real-time. Theproposed method is applied to motor task data from the Human Connectome Projectas well as to real-time data ob- tained while exploring a virtual environment.We show that the algorithm is able to estimate signi?cant task-related changesin network structure quickly enough to be useful in future brain-computerinterface applications.
arxiv-9000-99 | Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches | http://arxiv.org/pdf/1502.02277v1.pdf | author:Seung-Hoon Na, In-Su Kang, Jong-Hyeok Lee category:cs.IR cs.CL H.3.3 published:2015-02-08 summary:Term frequency normalization is a serious issue since lengths of documentsare various. Generally, documents become long due to two different reasons -verbosity and multi-topicality. First, verbosity means that the same topic isrepeatedly mentioned by terms related to the topic, so that term frequency ismore increased than the well-summarized one. Second, multi-topicality indicatesthat a document has a broad discussion of multi-topics, rather than singletopic. Although these document characteristics should be differently handled,all previous methods of term frequency normalization have ignored thesedifferences and have used a simplified length-driven approach which decreasesthe term frequency by only the length of a document, causing an unreasonablepenalization. To attack this problem, we propose a novel TF normalizationmethod which is a type of partially-axiomatic approach. We first formulate twoformal constraints that the retrieval model should satisfy for documents havingverbose and multi-topicality characteristic, respectively. Then, we modifylanguage modeling approaches to better satisfy these two constraints, andderive novel smoothing methods. Experimental results show that the proposedmethod increases significantly the precision for keyword queries, andsubstantially improves MAP (Mean Average Precision) for verbose queries.
arxiv-9000-100 | SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization | http://arxiv.org/pdf/1502.02268v1.pdf | author:Zheng Qu, Peter Richtárik, Martin Takáč, Olivier Fercoq category:cs.LG published:2015-02-08 summary:We propose a new algorithm for minimizing regularized empirical loss:Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in eachiteration we update a random subset of the dual variables. However, unlikeexisting methods such as stochastic dual coordinate ascent, SDNA is capable ofutilizing all curvature information contained in the examples, which leads tostriking improvements in both theory and practice - sometimes by orders ofmagnitude. In the special case when an L2-regularizer is used in the primal,the dual problem is a concave quadratic maximization problem plus a separableterm. In this regime, SDNA in each step solves a proximal subproblem involvinga random principal submatrix of the Hessian of the quadratic function; whencethe name of the method. If, in addition, the loss functions are quadratic, ourmethod can be interpreted as a novel variant of the recently introducedIterative Hessian Sketch.
arxiv-9000-101 | Contextual Markov Decision Processes | http://arxiv.org/pdf/1502.02259v1.pdf | author:Assaf Hallak, Dotan Di Castro, Shie Mannor category:stat.ML cs.LG published:2015-02-08 summary:We consider a planning problem where the dynamics and rewards of theenvironment depend on a hidden static parameter referred to as the context. Theobjective is to learn a strategy that maximizes the accumulated reward acrossall contexts. The new model, called Contextual Markov Decision Process (CMDP),can model a customer's behavior when interacting with a website (the learner).The customer's behavior depends on gender, age, location, device, etc. Based onthat behavior, the website objective is to determine customer characteristics,and to optimize the interaction between them. Our work focuses on one basicscenario--finite horizon with a small known number of possible contexts. Wesuggest a family of algorithms with provable guarantees that learn theunderlying models and the latent contexts, and optimize the CMDPs. Bounds areobtained for specific naive implementations, and extensions of the frameworkare discussed, laying the ground for future research.
arxiv-9000-102 | Hierarchical Dirichlet process for tracking complex topical structure evolution and its application to autism research literature | http://arxiv.org/pdf/1502.02233v1.pdf | author:Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh category:cs.IR cs.CL published:2015-02-08 summary:In this paper we describe a novel framework for the discovery of the topicalcontent of a data corpus, and the tracking of its complex structural changesacross the temporal dimension. In contrast to previous work our model does notimpose a prior on the rate at which documents are added to the corpus nor doesit adopt the Markovian assumption which overly restricts the type of changesthat the model can capture. Our key technical contribution is a framework basedon (i) discretization of time into epochs, (ii) epoch-wise topic discoveryusing a hierarchical Dirichlet process-based model, and (iii) a temporalsimilarity graph which allows for the modelling of complex topic changes:emergence and disappearance, evolution, and splitting and merging. The power ofthe proposed framework is demonstrated on the medical literature corpusconcerned with the autism spectrum disorder (ASD) - an increasingly importantresearch subject of significant social and healthcare importance. In additionto the collected ASD literature corpus which we will make freely available, ourcontributions also include two free online tools we built as aids to ASDresearchers. These can be used for semantically meaningful navigation andsearching, as well as knowledge discovery from this large and rapidly growingcorpus of literature.
arxiv-9000-103 | A new variational principle for the Euclidean distance function: Linear approach to the non-linear eikonal problem | http://arxiv.org/pdf/1112.3010v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan category:cs.CV math.NA 65D18, 65M80 published:2011-12-13 summary:We present a fast convolution-based technique for computing an approximate,signed Euclidean distance function $S$ on a set of 2D and 3D grid locations.Instead of solving the non-linear, static Hamilton-Jacobi equation ($\\nablaS\=1$), our solution stems from first solving for a scalar field $\phi$ in alinear differential equation and then deriving the solution for $S$ by takingthe negative logarithm. In other words, when $S$ and $\phi$ are related by$\phi = \exp \left(-\frac{S}{\tau} \right)$ and $\phi$ satisfies a specificlinear differential equation corresponding to the extremum of a variationalproblem, we obtain the approximate Euclidean distance function $S = -\tau\log(\phi)$ which converges to the true solution in the limit as $\tau\rightarrow 0$. This is in sharp contrast to techniques like the fast marchingand fast sweeping methods which directly solve the Hamilton-Jacobi equation bythe Godunov upwind discretization scheme. Our linear formulation results in aclosed-form solution to the approximate Euclidean distance function expressibleas a discrete convolution, and hence efficiently computable using the fastFourier transform (FFT). Our solution also circumvents the need for spatialdiscretization of the derivative operator. As $\tau\rightarrow0$ we show theconvergence of our results to the true solution and also bound the error for agiven value of $\tau$. The differentiability of our solution allows us tocompute---using a set of convolutions---the first and second derivatives of theapproximate distance function. In order to determine the sign of the distancefunction (defined to be positive inside a closed region and negative outside),we compute the winding number in 2D and the topological degree in 3D, whosecomputations can also be performed via fast convolutions. We demonstrate theefficacy of our method through a set of experimental results.
arxiv-9000-104 | A fast eikonal equation solver using the Schrodinger wave equation | http://arxiv.org/pdf/1403.1937v2.pdf | author:Karthik S. Gurumoorthy, Adrian M. Peter, Birmingham Hang Guan, Anand Rangarajan category:math.NA cs.CV cs.NA published:2014-03-08 summary:We use a Schr\"odinger wave equation formalism to solve the eikonal equation.In our framework, a solution to the eikonal equation is obtained in the limitas Planck's constant $\hbar$ (treated as a free parameter) tends to zero of thesolution to the corresponding linear Schr\"odinger equation. The Schr\"odingerequation corresponding to the eikonal turns out to be a \emph{generalized,screened Poisson equation}. Despite being linear, it does not have aclosed-form solution for arbitrary forcing functions. We present two differenttechniques to solve the screened Poisson equation. In the first approach we usea standard perturbation analysis approach to derive a new algorithm which isguaranteed to converge provided the forcing function is bounded and positive.The perturbation technique requires a sequence of discrete convolutions whichcan be performed in $O(N\log N)$ using the Fast Fourier Transform (FFT) where$N$ is the number of grid points. In the second method we discretize the linearLaplacian operator by the finite difference method leading to a sparse linearsystem of equations which can be solved using the plethora of sparse solvers.The eikonal solution is recovered from the exponent of the resultant scalarfield. Our approach eliminates the need to explicitly construct viscositysolutions as customary with direct solutions to the eikonal. Since the linearequation is computed for a small but non-zero $\hbar$, the obtained solution isan approximation. Though our solution framework is applicable to the generalclass of eikonal problems, we detail specifics for the popular visionapplications of shape-from-shading, vessel segmentation, and path planning.
arxiv-9000-105 | Real World Applications of Machine Learning Techniques over Large Mobile Subscriber Datasets | http://arxiv.org/pdf/1502.02215v1.pdf | author:Jobin Wilson, Chitharanj Kachappilly, Rakesh Mohan, Prateek Kapadia, Arun Soman, Santanu Chaudhury category:cs.LG cs.CY cs.SE published:2015-02-08 summary:Communication Service Providers (CSPs) are in a unique position to utilizetheir vast transactional data assets generated from interactions of subscriberswith network elements as well as with other subscribers. CSPs could leverageits data assets for a gamut of applications such as service personalization,predictive offer management, loyalty management, revenue forecasting, networkcapacity planning, product bundle optimization and churn management to gainsignificant competitive advantage. However, due to the sheer data volume,variety, velocity and veracity of mobile subscriber datasets, sophisticateddata analytics techniques and frameworks are necessary to derive actionableinsights in a useable timeframe. In this paper, we describe our journey from arelational database management system (RDBMS) based campaign managementsolution which allowed data scientists and marketers to use hand-written rulesfor service personalization and targeted promotions to a distributed Big DataAnalytics platform, capable of performing large scale machine learning and datamining to deliver real time service personalization, predictive modelling andproduct optimization. Our work involves a careful blend of technology,processes and best practices, which facilitate man-machine collaboration andcontinuous experimentation to derive measurable economic value from data. Ourplatform has a reach of more than 500 million mobile subscribers worldwide,delivering over 1 billion personalized recommendations annually, processing atotal data volume of 64 Petabytes, corresponding to 8.5 trillion events.
arxiv-9000-106 | Learning Rank Functionals: An Empirical Study | http://arxiv.org/pdf/1407.6089v2.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.IR cs.LG stat.ML published:2014-07-23 summary:Ranking is a key aspect of many applications, such as information retrieval,question answering, ad placement and recommender systems. Learning to rank hasthe goal of estimating a ranking model automatically from training data. Inpractical settings, the task often reduces to estimating a rank functional ofan object with respect to a query. In this paper, we investigate key issues indesigning an effective learning to rank algorithm. These include datarepresentation, the choice of rank functionals, the design of the loss functionso that it is correlated with the rank metrics used in evaluation. For the lossfunction, we study three techniques: approximating the rank metric by a smoothfunction, decomposition of the loss into a weighted sum of element-wise lossesand into a weighted sum of pairwise losses. We then present derivations ofpiecewise losses using the theory of high-order Markov chains and Markov randomfields. In experiments, we evaluate these design aspects on two tasks: answerranking in a Social Question Answering site, and Web Information Retrieval.
arxiv-9000-107 | An investigation into language complexity of World-of-Warcraft game-external texts | http://arxiv.org/pdf/1502.02655v1.pdf | author:Simon Šuster category:cs.CL published:2015-02-07 summary:We present a language complexity analysis of World of Warcraft (WoW)community texts, which we compare to texts from a general corpus of webEnglish. Results from several complexity types are presented, including lexicaldiversity, density, readability and syntactic complexity. The language of WoWtexts is found to be comparable to the general corpus on some complexitymeasures, yet more specialized on other measures. Our findings can be used byeducators willing to include game-related activities into school curricula.
arxiv-9000-108 | Minimax Optimal Convergence Rates for Estimating Ground Truth from Crowdsourced Labels | http://arxiv.org/pdf/1310.5764v5.pdf | author:Chao Gao, Dengyong Zhou category:stat.ML math.ST stat.TH published:2013-10-22 summary:Crowdsourcing has become a primary means for label collection in manyreal-world machine learning applications. A classical method for inferring thetrue labels from the noisy labels provided by crowdsourcing workers isDawid-Skene estimator. In this paper, we prove convergence rates of globaloptimizers of Dawid-Skene estimator. The revealed exponent in the rate ofconvergence is shown to be optimal via a lower bound argument. A projected EMalgorithm is analyzed and is shown to achieve nearly the same exponent as thatof the global optimizers. Our work resolves the long standing issue of whetherDawid-Skene estimator has sound theoretical guarantees besides its goodperformance observed in practice. In addition, a comparative study withmajority voting illustrates both advantages and pitfalls of Dawid-Skeneestimator.
arxiv-9000-109 | Person Re-identification Meets Image Search | http://arxiv.org/pdf/1502.02171v1.pdf | author:Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jiahao Bu, Qi Tian category:cs.CV published:2015-02-07 summary:For long time, person re-identification and image search are two separatelystudied tasks. However, for person re-identification, the effectiveness oflocal features and the "query-search" mode make it well posed for image searchtechniques. In the light of recent advances in image search, this paper proposes to treatperson re-identification as an image search problem. Specifically, this paperclaims two major contributions. 1) By designing an unsupervised Bag-of-Wordsrepresentation, we are devoted to bridging the gap between the two tasks byintegrating techniques from image search in person re-identification. We showthat our system sets up an effective yet efficient baseline that is amenable tofurther supervised/unsupervised improvements. 2) We contribute a new highquality dataset which uses DPM detector and includes a number of distractorimages. Our dataset reaches closer to realistic settings, and new perspectivesare provided. Compared with approaches that rely on feature-feature match, our method isfaster by over two orders of magnitude. Moreover, on three datasets, we reportcompetitive results compared with the state-of-the-art methods.
arxiv-9000-110 | A Survey on Hough Transform, Theory, Techniques and Applications | http://arxiv.org/pdf/1502.02160v1.pdf | author:Allam Shehata Hassanein, Sherien Mohammad, Mohamed Sameer, Mohammad Ehab Ragab category:cs.CV published:2015-02-07 summary:For more than half a century, the Hough transform is ever-expanding for newfrontiers. Thousands of research papers and numerous applications have evolvedover the decades. Carrying out an all-inclusive survey is hardly possible andenormously space-demanding. What we care about here is emphasizing some of themost crucial milestones of the transform. We describe its variationselaborating on the basic ones such as the line and circle Hough transforms. Thehigh demand for storage and computation time is clarified with differentsolution approaches. Since most uses of the transform take place on binaryimages, we have been concerned with the work done directly on gray or colorimages. The myriad applications of the standard transform and its variationshave been classified highlighting the up-to-date and the unconventional ones.Due to its merits such as noise-immunity and expandability, the transform hasan excellent history, and a bright future as well.
arxiv-9000-111 | Learning Parametric-Output HMMs with Two Aliased States | http://arxiv.org/pdf/1502.02158v1.pdf | author:Roi Weiss, Boaz Nadler category:cs.LG published:2015-02-07 summary:In various applications involving hidden Markov models (HMMs), some of thehidden states are aliased, having identical output distributions. Theminimality, identifiability and learnability of such aliased HMMs have beenlong standing problems, with only partial solutions provided thus far. In thispaper we focus on parametric-output HMMs, whose output distributions come froma parametric family, and that have exactly two aliased states. For this class,we present a complete characterization of their minimality and identifiability.Furthermore, for a large family of parametric output distributions, we derivecomputationally efficient and statistically consistent algorithms to detect thepresence of aliasing and learn the aliased HMM transition and emissionparameters. We illustrate our theoretical analysis by several simulations.
arxiv-9000-112 | Distributed Robust Learning | http://arxiv.org/pdf/1409.5937v2.pdf | author:Jiashi Feng, Huan Xu, Shie Mannor category:stat.ML cs.LG published:2014-09-21 summary:We propose a framework for distributed robust statistical learning on {\embig contaminated data}. The Distributed Robust Learning (DRL) framework canreduce the computational time of traditional robust learning methods by severalorders of magnitude. We analyze the robustness property of DRL, showing thatDRL not only preserves the robustness of the base robust learning method, butalso tolerates contaminations on a constant fraction of results from computingnodes (node failures). More precisely, even in presence of the most adversarialoutlier distribution over computing nodes, DRL still achieves a breakdown pointof at least $ \lambda^*/2 $, where $ \lambda^* $ is the break down point ofcorresponding centralized algorithm. This is in stark contrast with naivedivision-and-averaging implementation, which may reduce the breakdown point bya factor of $ k $ when $ k $ computing nodes are used. We then specialize theDRL framework for two concrete cases: distributed robust principal componentanalysis and distributed robust regression. We demonstrate the efficiency andthe robustness advantages of DRL through comprehensive simulations andpredicting image tags on a large-scale image set.
arxiv-9000-113 | RELEAF: An Algorithm for Learning and Exploiting Relevance | http://arxiv.org/pdf/1502.01418v2.pdf | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2015-02-05 summary:Recommender systems, medical diagnosis, network security, etc., requireon-going learning and decision-making in real time. These -- and many others --represent perfect examples of the opportunities and difficulties presented byBig Data: the available information often arrives from a variety of sources andhas diverse features so that learning from all the sources may be valuable butintegrating what is learned is subject to the curse of dimensionality. Thispaper develops and analyzes algorithms that allow efficient learning anddecision-making while avoiding the curse of dimensionality. We formalize theinformation available to the learner/decision-maker at a particular time as acontext vector which the learner should consider when taking actions. Ingeneral the context vector is very high dimensional, but in many settings, themost relevant information is embedded into only a few relevant dimensions. Ifthese relevant dimensions were known in advance, the problem would be simple --but they are not. Moreover, the relevant dimensions may be different fordifferent actions. Our algorithm learns the relevant dimensions for eachaction, and makes decisions based in what it has learned. Formally, we build onthe structure of a contextual multi-armed bandit by adding and exploiting arelevance relation. We prove a general regret bound for our algorithm whosetime order depends only on the maximum number of relevant dimensions among allthe actions, which in the special case where the relevance relation issingle-valued (a function), reduces to $\tilde{O}(T^{2(\sqrt{2}-1)})$; in theabsence of a relevance relation, the best known contextual bandit algorithmsachieve regret $\tilde{O}(T^{(D+1)/(D+2)})$, where $D$ is the full dimension ofthe context vector.
arxiv-9000-114 | Reflectance Hashing for Material Recognition | http://arxiv.org/pdf/1502.02092v1.pdf | author:Hang Zhang, Kristin Dana, Ko Nishino category:cs.CV published:2015-02-07 summary:We introduce a novel method for using reflectance to identify materials.Reflectance offers a unique signature of the material but is challenging tomeasure and use for recognizing materials due to its high-dimensionality. Inthis work, one-shot reflectance is captured using a unique optical camerameasuring {\it reflectance disks} where the pixel coordinates correspond tosurface viewing angles. The reflectance has class-specific stucture and angulargradients computed in this reflectance space reveal the material class. These reflectance disks encode discriminative information for efficient andaccurate material recognition. We introduce a framework called reflectancehashing that models the reflectance disks with dictionary learning and binaryhashing. We demonstrate the effectiveness of reflectance hashing for materialrecognition with a number of real-world materials.
arxiv-9000-115 | Discriminative training for Convolved Multiple-Output Gaussian processes | http://arxiv.org/pdf/1502.02089v1.pdf | author:Sebastián Gómez-González, Mauricio A. Álvarez, Hernán Felipe García category:stat.ML published:2015-02-07 summary:Multi-output Gaussian processes (MOGP) are probability distributions overvector-valued functions, and have been previously used for multi-outputregression and for multi-class classification. A less explored facet of themulti-output Gaussian process is that it can be used as a generative model forvector-valued random fields in the context of pattern recognition. As agenerative model, the multi-output GP is able to handle vector-valued functionswith continuous inputs, as opposed, for example, to hidden Markov models. Italso offers the ability to model multivariate random functions with highdimensional inputs. In this report, we use a discriminative training criteriaknown as Minimum Classification Error to fit the parameters of a multi-outputGaussian process. We compare the performance of generative training anddiscriminative training of MOGP in emotion recognition, activity recognition,and face recognition. We also compare the proposed methodology against hiddenMarkov models trained in a generative and in a discriminative way.
arxiv-9000-116 | Massively Multitask Networks for Drug Discovery | http://arxiv.org/pdf/1502.02072v1.pdf | author:Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, Vijay Pande category:stat.ML cs.LG cs.NE published:2015-02-06 summary:Massively multitask neural architectures provide a learning framework fordrug discovery that synthesizes information from many distinct biologicalsources. To train these architectures at scale, we gather large amounts of datafrom public sources to create a dataset of nearly 40 million measurementsacross more than 200 biological targets. We investigate several aspects of themultitask framework by performing a series of empirical studies and obtain someinteresting results: (1) massively multitask networks obtain predictiveaccuracies significantly better than single-task methods, (2) the predictivepower of multitask networks improves as additional tasks and data are added,(3) the total amount of data and the total number of tasks both contributesignificantly to multitask improvement, and (4) multitask networks affordlimited transferability to tasks not in the training set. Our resultsunderscore the need for greater data sharing and further algorithmic innovationto accelerate the drug discovery process.
arxiv-9000-117 | Learning from Multiple Sources for Video Summarisation | http://arxiv.org/pdf/1501.03069v2.pdf | author:Xiatian Zhu, Chen Change Loy, Shaogang Gong category:cs.CV published:2015-01-13 summary:Many visual surveillance tasks, e.g.video summarisation, is conventionallyaccomplished through analysing imagerybased features. Relying solely on visualcues for public surveillance video understanding is unreliable, since visualobservations obtained from public space CCTV video data are often notsufficiently trustworthy and events of interest can be subtle. On the otherhand, non-visual data sources such as weather reports and traffic sensorysignals are readily accessible but are not explored jointly to complementvisual data for video content analysis and summarisation. In this paper, wepresent a novel unsupervised framework to learn jointly from both visual andindependently-drawn non-visual data sources for discovering meaningful latentstructure of surveillance video data. In particular, we investigate ways tocope with discrepant dimension and representation whist associating theseheterogeneous data sources, and derive effective mechanism to tolerate withmissing and incomplete data from different sources. We show that the proposedmulti-source learning framework not only achieves better video contentclustering than state-of-the-art methods, but also is capable of accuratelyinferring missing non-visual semantics from previously unseen videos. Inaddition, a comprehensive user study is conducted to validate the quality ofvideo summarisation generated using the proposed multi-source model.
arxiv-9000-118 | Temporally Coherent Bayesian Models for Entity Discovery in Videos by Tracklet Clustering | http://arxiv.org/pdf/1409.6080v2.pdf | author:Adway Mitra, Soma Biswas, Chiranjib Bhattacharyya category:cs.CV published:2014-09-22 summary:A video can be represented as a sequence of tracklets, each spanning 10-20frames, and associated with one entity (eg. a person). The task of \emph{EntityDiscovery} in videos can be naturally posed as tracklet clustering. We approachthis task by leveraging \emph{Temporal Coherence}(TC): the fundamental propertyof videos that each tracklet is likely to be associated with the same entity asits temporal neighbors. Our major contributions are the first Bayesiannonparametric models for TC at tracklet-level. We extend Chinese RestaurantProcess (CRP) to propose TC-CRP, and further to Temporally Coherent ChineseRestaurant Franchise (TC-CRF) to jointly model short temporal segments. On thetask of discovering persons in TV serial videos without meta-data like scripts,these methods show considerable improvement in cluster purity and personcoverage compared to state-of-the-art approaches to tracklet clustering. Werepresent entities with mixture components, and tracklets with vectors of verygeneric features, which can work for any type of entity (not necessarilyperson). The proposed methods can perform online tracklet clustering onstreaming videos with little performance deterioration unlike existingapproaches, and can automatically reject tracklets resulting from falsedetections. Finally we discuss entity-driven video summarization- where sometemporal segments of the video are selected automatically based on thediscovered entities.
arxiv-9000-119 | Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds | http://arxiv.org/pdf/1502.01403v2.pdf | author:Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan category:cs.DS cs.CC stat.ML published:2015-02-05 summary:We study the following generalized matrix rank estimation problem: given an$n \times n$ matrix and a constant $c \geq 0$, estimate the number ofeigenvalues that are greater than $c$. In the distributed setting, the matrixof interest is the sum of $m$ matrices held by separate machines. We show thatany deterministic algorithm solving this problem must communicate $\Omega(n^2)$bits, which is order-equivalent to transmitting the whole matrix. In contrast,we propose a randomized algorithm that communicates only $\widetilde O(n)$bits. The upper bound is matched by an $\Omega(n)$ lower bound on therandomized communication complexity. We demonstrate the practical effectivenessof the proposed algorithm with some numerical experiments.
arxiv-9000-120 | Active Function Cross-Entropy Clustering | http://arxiv.org/pdf/1502.01943v1.pdf | author:P. Spurek, J. Tabor, P. Markowicz category:stat.ML published:2015-02-06 summary:Gaussian Mixture Models (GMM) have found many applications in densityestimation and data clustering. However, the model does not adapt well tocurved and strongly nonlinear data. Recently there appeared an improvementcalled AcaGMM (Active curve axis Gaussian Mixture Model), which fits Gaussiansalong curves using an EM-like (Expectation Maximization) approach. Using the ideas standing behind AcaGMM, we build an alternative activefunction model of clustering, which has some advantages over AcaGMM. Inparticular it is naturally defined in arbitrary dimensions and enables an easyadaptation to clustering of complicated datasets along the predefined family offunctions. Moreover, it does not need external methods to determine the numberof clusters as it automatically reduces the number of groups on-line.
arxiv-9000-121 | Data Fusion by Matrix Factorization | http://arxiv.org/pdf/1307.0803v2.pdf | author:Marinka Žitnik, Blaž Zupan category:cs.LG cs.AI cs.DB stat.ML published:2013-07-02 summary:For most problems in science and engineering we can obtain data sets thatdescribe the observed system from various perspectives and record the behaviorof its individual components. Heterogeneous data sets can be collectively minedby data fusion. Fusion can focus on a specific target relation and exploitdirectly associated data together with contextual data and data about system'sconstraints. In the paper we describe a data fusion approach with penalizedmatrix tri-factorization (DFMF) that simultaneously factorizes data matrices toreveal hidden associations. The approach can directly consider any data thatcan be expressed in a matrix, including those from feature-basedrepresentations, ontologies, associations and networks. We demonstrate theutility of DFMF for gene function prediction task with eleven different datasources and for prediction of pharmacologic actions by fusing six data sources.Our data fusion algorithm compares favorably to alternative data integrationapproaches and achieves higher accuracy than can be obtained from any singledata source alone.
arxiv-9000-122 | Feature Selection for Linear SVM with Provable Guarantees | http://arxiv.org/pdf/1406.0167v3.pdf | author:Saurabh Paul, Malik Magdon-Ismail, Petros Drineas category:stat.ML cs.LG published:2014-06-01 summary:We give two provably accurate feature-selection techniques for the linearSVM. The algorithms run in deterministic and randomized time respectively. Ouralgorithms can be used in an unsupervised or supervised setting. The supervisedapproach is based on sampling features from support vectors. We prove that themargin in the feature space is preserved to within $\epsilon$-relative error ofthe margin in the full feature space in the worst-case. In the unsupervisedsetting, we also provide worst-case guarantees of the radius of the minimumenclosing ball, thereby ensuring comparable generalization as in the fullfeature space and resolving an open problem posed in Dasgupta et al. We presentextensive experiments on real-world datasets to support our theory and todemonstrate that our method is competitive and often better than priorstate-of-the-art, for which there are no known provable guarantees.
arxiv-9000-123 | On the Incommensurability Phenomenon | http://arxiv.org/pdf/1301.1954v5.pdf | author:Donniell E. Fishkind, Cencheng Shen, Youngser Park, Carey E. Priebe category:stat.ML published:2013-01-09 summary:Suppose that two large, multi-dimensional data sets are each noisymeasurements of the same underlying random process, and principle componentsanalysis is performed separately on the data sets to reduce theirdimensionality. In some circumstances it may happen that the twolower-dimensional data sets have an inordinately large Procrusteanfitting-error between them. The purpose of this manuscript is to quantify this"incommensurability phenomenon." In particular, under specified conditions, thesquare Procrustean fitting-error of the two normalized lower-dimensional datasets is (asymptotically) a convex combination (via a correlation parameter) ofthe Hausdorff distance between the projection subspaces and the maximumpossible value of the square Procrustean fitting-error for normalized data. Weshow how this gives rise to the incommensurability phenomenon, and we employillustrative simulations as well as a real data experiment to explore how theincommensurability phenomenon may have an appreciable impact.
arxiv-9000-124 | A Fingerprint-based Access Control using Principal Component Analysis and Edge Detection | http://arxiv.org/pdf/1502.01880v1.pdf | author:E. F. Melo, H. M. de Oliveira category:cs.CV cs.CR stat.AP published:2015-02-06 summary:This paper presents a novel approach for deciding on the appropriateness ornot of an acquired fingerprint image into a given database. The process beginswith the assembly of a training base in an image space constructed by combiningPrincipal Component Analysis (PCA) and edge detection. Then, the parameter H, anew feature that helps in the decision making about the relevance of afingerprint image in databases, is derived from a relationship betweenEuclidean and Mahalanobian distances. This procedure ends with the lifting ofthe curve of the Receiver Operating Characteristic (ROC), where the thresholdsdefined on the parameter H are chosen according to the acceptable rates offalse positives and false negatives.
arxiv-9000-125 | Generalized Inpainting Method for Hyperspectral Image Acquisition | http://arxiv.org/pdf/1502.01853v1.pdf | author:K. Degraux, V. Cambareri, L. Jacques, B. Geelen, C. Blanch, G. Lafruit category:cs.CV published:2015-02-06 summary:A recently designed hyperspectral imaging device enables multiplexedacquisition of an entire data volume in a single snapshot thanks tomonolithically-integrated spectral filters. Such an agile imaging techniquecomes at the cost of a reduced spatial resolution and the need for ademosaicing procedure on its interleaved data. In this work, we address bothissues and propose an approach inspired by recent developments in compressedsensing and analysis sparse models. We formulate our superresolution anddemosaicing task as a 3-D generalized inpainting problem. Interestingly, thetarget spatial resolution can be adjusted for mitigating the compression levelof our sensing. The reconstruction procedure uses a fast greedy method calledPseudo-inverse IHT. We also show on simulations that a random arrangement ofthe spectral filters on the sensor is preferable to regular mosaic layout as itimproves the quality of the reconstruction. The efficiency of our technique isdemonstrated through numerical experiments on both synthetic and real data asacquired by the snapshot imager.
arxiv-9000-126 | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | http://arxiv.org/pdf/1502.01852v1.pdf | author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV cs.AI cs.LG published:2015-02-06 summary:Rectified activation units (rectifiers) are essential for state-of-the-artneural networks. In this work, we study rectifier neural networks for imageclassification from two aspects. First, we propose a Parametric RectifiedLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLUimproves model fitting with nearly zero extra computational cost and littleoverfitting risk. Second, we derive a robust initialization method thatparticularly considers the rectifier nonlinearities. This method enables us totrain extremely deep rectified models directly from scratch and to investigatedeeper or wider network architectures. Based on our PReLU networks(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012classification dataset. This is a 26% relative improvement over the ILSVRC 2014winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpasshuman-level performance (5.1%, Russakovsky et al.) on this visual recognitionchallenge.
arxiv-9000-127 | Hierarchical Maximum-Margin Clustering | http://arxiv.org/pdf/1502.01827v1.pdf | author:Guang-Tong Zhou, Sung Ju Hwang, Mark Schmidt, Leonid Sigal, Greg Mori category:cs.LG cs.CV published:2015-02-06 summary:We present a hierarchical maximum-margin clustering method for unsuperviseddata analysis. Our method extends beyond flat maximum-margin clustering, andperforms clustering recursively in a top-down manner. We propose an effectivegreedy splitting criteria for selecting which cluster to split next, and employregularizers that enforce feature sharing/competition for capturing datasemantics. Experimental results obtained on four standard datasets show thatour method outperforms flat and hierarchical clustering baselines, whileforming clean and semantically meaningful cluster hierarchies.
arxiv-9000-128 | Unsupervised Fusion Weight Learning in Multiple Classifier Systems | http://arxiv.org/pdf/1502.01823v1.pdf | author:Anurag Kumar, Bhiksha Raj category:cs.LG cs.CV published:2015-02-06 summary:In this paper we present an unsupervised method to learn the weights withwhich the scores of multiple classifiers must be combined in classifier fusionsettings. We also introduce a novel metric for ranking instances based on anindex which depends upon the rank of weighted scores of test points among theweighted scores of training points. We show that the optimized index can beused for computing measures such as average precision. Unlike most classifierfusion methods where a single weight is learned to weigh all examples ourmethod learns instance-specific weights. The problem is formulated as learningthe weight which maximizes a clarity index; subsequently the index itself andthe learned weights both are used separately to rank all the test points. Ourmethod gives an unsupervised method of optimizing performance on actual testdata, unlike the well known stacking-based methods where optimization is doneover a labeled training set. Moreover, we show that our method is tolerant tonoisy classifiers and can be used for selecting N-best classifiers.
arxiv-9000-129 | Crowded Scene Analysis: A Survey | http://arxiv.org/pdf/1502.01812v1.pdf | author:Teng Li, Huan Chang, Meng Wang, Bingbing Ni, Richang Hong, Shuicheng Yan category:cs.CV 68-02 published:2015-02-06 summary:Automated scene analysis has been a topic of great interest in computervision and cognitive science. Recently, with the growth of crowd phenomena inthe real world, crowded scene analysis has attracted much attention. However,the visual occlusions and ambiguities in crowded scenes, as well as the complexbehaviors and scene semantics, make the analysis a challenging task. In thepast few years, an increasing number of works on crowded scene analysis havebeen reported, covering different aspects including crowd motion patternlearning, crowd behavior and activity analysis, and anomaly detection incrowds. This paper surveys the state-of-the-art techniques on this topic. Wefirst provide the background knowledge and the available features related tocrowded scenes. Then, existing models, popular algorithms, evaluationprotocols, as well as system performance are provided corresponding todifferent aspects of crowded scene analysis. We also outline the availabledatasets for performance evaluation. Finally, some research problems andpromising future directions are presented with discussions.
arxiv-9000-130 | Learning Efficient Anomaly Detectors from $K$-NN Graphs | http://arxiv.org/pdf/1502.01783v1.pdf | author:Jing Qian, Jonathan Root, Venkatesh Saligrama category:cs.LG stat.ML published:2015-02-06 summary:We propose a non-parametric anomaly detection algorithm for high dimensionaldata. We score each datapoint by its average $K$-NN distance, and rank themaccordingly. We then train limited complexity models to imitate these scoresbased on the max-margin learning-to-rank framework. A test-point is declared asan anomaly at $\alpha$-false alarm level if the predicted score is in the$\alpha$-percentile. The resulting anomaly detector is shown to beasymptotically optimal in that for any false alarm rate $\alpha$, its decisionregion converges to the $\alpha$-percentile minimum volume level set of theunknown underlying density. In addition, we test both the statisticalperformance and computational efficiency of our algorithm on a number ofsynthetic and real-data experiments. Our results demonstrate the superiority ofour algorithm over existing $K$-NN based anomaly detection algorithms, withsignificant computational savings.
arxiv-9000-131 | Multi-Action Recognition via Stochastic Modelling of Optical Flow and Gradients | http://arxiv.org/pdf/1502.01782v1.pdf | author:Johanna Carvajal, Conrad Sanderson, Chris McCool, Brian C. Lovell category:cs.CV published:2015-02-06 summary:In this paper we propose a novel approach to multi-action recognition thatperforms joint segmentation and classification. This approach models eachaction using a Gaussian mixture using robust low-dimensional action features.Segmentation is achieved by performing classification on overlapping temporalwindows, which are then merged to produce the final result. This approach isconsiderably less complicated than previous methods which use dynamicprogramming or computationally expensive hidden Markov models (HMMs). Initialexperiments on a stitched version of the KTH dataset show that the proposedapproach achieves an accuracy of 78.3%, outperforming a recent HMM-basedapproach which obtained 71.2%.
arxiv-9000-132 | Multiscale Event Detection in Social Media | http://arxiv.org/pdf/1404.7048v2.pdf | author:Xiaowen Dong, Dimitrios Mavroeidis, Francesco Calabrese, Pascal Frossard category:cs.SI cs.LG physics.soc-ph stat.ML published:2014-04-25 summary:Event detection has been one of the most important research topics in socialmedia analysis. Most of the traditional approaches detect events based on fixedtemporal and spatial resolutions, while in reality events of different scalesusually occur simultaneously, namely, they span different intervals in time andspace. In this paper, we propose a novel approach towards multiscale eventdetection using social media data, which takes into account different temporaland spatial scales of events in the data. Specifically, we explore theproperties of the wavelet transform, which is a well-developed multiscaletransform in signal processing, to enable automatic handling of the interactionbetween temporal and spatial scales. We then propose a novel algorithm tocompute a data similarity graph at appropriate scales and detect events ofdifferent scales simultaneously by a single graph-based clustering process.Furthermore, we present spatiotemporal statistical analysis of the noisyinformation present in the data stream, which allows us to define a novelterm-filtering procedure for the proposed event detection algorithm and helpsus study its behavior using simulated noisy data. Experimental results on bothsynthetically generated data and real world data collected from Twitterdemonstrate the meaningfulness and effectiveness of the proposed approach. Ourframework further extends to numerous application domains that involvemultiscale and multiresolution data analysis.
arxiv-9000-133 | A Framework for Symmetric Part Detection in Cluttered Scenes | http://arxiv.org/pdf/1502.01761v1.pdf | author:Tom Lee, Sanja Fidler, Alex Levinshtein, Cristian Sminchisescu, Sven Dickinson category:cs.CV published:2015-02-05 summary:The role of symmetry in computer vision has waxed and waned in importanceduring the evolution of the field from its earliest days. At first figuringprominently in support of bottom-up indexing, it fell out of favor as shapegave way to appearance and recognition gave way to detection. With a strongprior in the form of a target object, the role of the weaker priors offered byperceptual grouping was greatly diminished. However, as the field returns tothe problem of recognition from a large database, the bottom-up recovery of theparts that make up the objects in a cluttered scene is critical for theirrecognition. The medial axis community has long exploited the ubiquitousregularity of symmetry as a basis for the decomposition of a closed contourinto medial parts. However, today's recognition systems are faced withcluttered scenes, and the assumption that a closed contour exists, i.e. thatfigure-ground segmentation has been solved, renders much of the medial axiscommunity's work inapplicable. In this article, we review a computationalframework, previously reported in Lee et al. (2013), Levinshtein et al. (2009,2013), that bridges the representation power of the medial axis and the need torecover and group an object's parts in a cluttered scene. Our framework isrooted in the idea that a maximally inscribed disc, the building block of amedial axis, can be modeled as a compact superpixel in the image. We evaluatethe method on images of cluttered scenes.
arxiv-9000-134 | Monitoring Term Drift Based on Semantic Consistency in an Evolving Vector Field | http://arxiv.org/pdf/1502.01753v1.pdf | author:Peter Wittek, Sándor Darányi, Efstratios Kontopoulos, Theodoros Moysiadis, Ioannis Kompatsiaris category:cs.CL cs.LG cs.NE stat.ML published:2015-02-05 summary:Based on the Aristotelian concept of potentiality vs. actuality allowing forthe study of energy and dynamics in language, we propose a field approach tolexical analysis. Falling back on the distributional hypothesis tostatistically model word meaning, we used evolving fields as a metaphor toexpress time-dependent changes in a vector space model by a combination ofrandom indexing and evolving self-organizing maps (ESOM). To monitor semanticdrifts within the observation period, an experiment was carried out on the termspace of a collection of 12.8 million Amazon book reviews. For evaluation, thesemantic consistency of ESOM term clusters was compared with their respectiveneighbourhoods in WordNet, and contrasted with distances among term vectors byrandom indexing. We found that at 0.05 level of significance, the terms in theclusters showed a high level of semantic consistency. Tracking the drift ofdistributional patterns in the term space across time periods, we found thatconsistency decreased, but not at a statistically significant level. Our methodis highly scalable, with interpretations in philosophy.
arxiv-9000-135 | Mesh Learning for Classifying Cognitive Processes | http://arxiv.org/pdf/1205.2382v3.pdf | author:Mete Ozay, Ilke Öztekin, Uygar Öztekin, Fatos T. Yarman Vural category:cs.NE cs.AI cs.CV stat.ML published:2012-05-10 summary:A relatively recent advance in cognitive neuroscience has been multi-voxelpattern analysis (MVPA), which enables researchers to decode brain statesand/or the type of information represented in the brain during a cognitiveoperation. MVPA methods utilize machine learning algorithms to distinguishamong types of information or cognitive states represented in the brain, basedon distributed patterns of neural activity. In the current investigation, wepropose a new approach for representation of neural data for pattern analysis,namely a Mesh Learning Model. In this approach, at each time instant, a starmesh is formed around each voxel, such that the voxel corresponding to thecenter node is surrounded by its p-nearest neighbors. The arc weights of eachmesh are estimated from the voxel intensity values by least squares method. Theestimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),are then used to train a classifier, such as Neural Networks, k-NearestNeighbor, Na\"ive Bayes and Support Vector Machines. The proposed Mesh Modelwas tested on neuroimaging data acquired via functional magnetic resonanceimaging (fMRI) during a recognition memory experiment using categorized wordlists, employing a previously established experimental paradigm (\"Oztekin &Badre, 2011). Results suggest that the proposed Mesh Learning approach canprovide an effective algorithm for pattern analysis of brain activity duringcognitive processing.
arxiv-9000-136 | Arrhythmia Detection using Mutual Information-Based Integration Method | http://arxiv.org/pdf/1502.01733v1.pdf | author:Othman Soufan, Samer Arafat category:cs.CE cs.LG published:2015-02-05 summary:The aim of this paper is to propose an application of mutualinformation-based ensemble methods to the analysis and classification of heartbeats associated with different types of Arrhythmia. Models of multilayerperceptrons, support vector machines, and radial basis function neural networkswere trained and tested using the MIT-BIH arrhythmia database. This researchbrings a focus to an ensemble method that, to our knowledge, is a novelapplication in the area of ECG Arrhythmia detection. The proposed classifierensemble method showed improved performance, relative to either majority votingclassifier integration or to individual classifier performance. The overallensemble accuracy was 98.25%.
arxiv-9000-137 | Differentially- and non-differentially-private random decision trees | http://arxiv.org/pdf/1410.6973v2.pdf | author:Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Yann LeCun category:cs.LG published:2014-10-26 summary:We consider supervised learning with random decision trees, where the treeconstruction is completely random. The method is popularly used and works wellin practice despite the simplicity of the setting, but its statisticalmechanism is not yet well-understood. In this paper we provide strongtheoretical guarantees regarding learning with random decision trees. Weanalyze and compare three different variants of the algorithm that have minimalmemory requirements: majority voting, threshold averaging and probabilisticaveraging. The random structure of the tree enables us to adapt these methodsto a differentially-private setting thus we also propose differentially-privateversions of all three schemes. We give upper-bounds on the generalization errorand mathematically explain how the accuracy depends on the number of randomdecision trees. Furthermore, we prove that only logarithmic (in the size of thedataset) number of independently selected random decision trees suffice tocorrectly classify most of the data, even when differential-privacy guaranteesmust be maintained. We empirically show that majority voting and thresholdaveraging give the best accuracy, also for conservative users requiring highprivacy guarantees. Furthermore, we demonstrate that a simple majority votingrule is an especially good candidate for the differentially-private classifiersince it is much less sensitive to the choice of forest parameters than othermethods.
arxiv-9000-138 | A Confident Information First Principle for Parametric Reduction and Model Selection of Boltzmann Machines | http://arxiv.org/pdf/1502.01705v1.pdf | author:Xiaozhao Zhao, Yuexian Hou, Dawei Song, Wenjie Li category:cs.LG stat.ML published:2015-02-05 summary:Typical dimensionality reduction (DR) methods are often data-oriented,focusing on directly reducing the number of random variables (features) whileretaining the maximal variations in the high-dimensional data. In unsupervisedsituations, one of the main limitations of these methods lies in theirdependency on the scale of data features. This paper aims to address theproblem from a new perspective and considers model-oriented dimensionalityreduction in parameter spaces of binary multivariate distributions. Specifically, we propose a general parameter reduction criterion, calledConfident-Information-First (CIF) principle, to maximally preserve confidentparameters and rule out less confident parameters. Formally, the confidence ofeach parameter can be assessed by its contribution to the expected Fisherinformation distance within the geometric manifold over the neighbourhood ofthe underlying real distribution. We then revisit Boltzmann machines (BM) from a model selection perspectiveand theoretically show that both the fully visible BM (VBM) and the BM withhidden units can be derived from the general binary multivariate distributionusing the CIF principle. This can help us uncover and formalize the essentialparts of the target density that BM aims to capture and the non-essential partsthat BM should discard. Guided by the theoretical analysis, we develop asample-specific CIF for model selection of BM that is adaptive to the observedsamples. The method is studied in a series of density estimation experimentsand has been shown effective in terms of the estimate accuracy.
arxiv-9000-139 | Graph Partitioning for Independent Sets | http://arxiv.org/pdf/1502.01687v1.pdf | author:Sebastian Lamm, Peter Sanders, Christian Schulz category:cs.DS cs.NE published:2015-02-05 summary:Computing maximum independent sets in graphs is an important problem incomputer science. In this paper, we develop an evolutionary algorithm to tacklethe problem. The core innovations of the algorithm are very natural combineoperations based on graph partitioning and local search algorithms. Moreprecisely, we employ a state-of-the-art graph partitioner to derive operationsthat enable us to quickly exchange whole blocks of given independent sets. Toenhance newly computed offsprings we combine our operators with a local searchalgorithm. Our experimental evaluation indicates that we are able to outperformstate-of-the-art algorithms on a variety of instances.
arxiv-9000-140 | On Anomaly Ranking and Excess-Mass Curves | http://arxiv.org/pdf/1502.01684v1.pdf | author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML math.PR published:2015-02-05 summary:Learning how to rank multivariate unlabeled observations depending on theirdegree of abnormality/novelty is a crucial problem in a wide range ofapplications. In practice, it generally consists in building a real valued"scoring" function on the feature space so as to quantify to which extentobservations should be considered as abnormal. In the 1-d situation,measurements are generally considered as "abnormal" when they are remote fromcentral measures such as the mean or the median. Anomaly detection then relieson tail analysis of the variable of interest. Extensions to the multivariatesetting are far from straightforward and it is precisely the main purpose ofthis paper to introduce a novel and convenient (functional) criterion formeasuring the performance of a scoring function regarding the anomaly rankingtask, referred to as the Excess-Mass curve (EM curve). In addition, an adaptivealgorithm for building a scoring function based on unlabeled data X1 , . . . ,Xn with a nearly optimal EM is proposed and is analyzed from a statisticalperspective.
arxiv-9000-141 | Use of Modality and Negation in Semantically-Informed Syntactic MT | http://arxiv.org/pdf/1502.01682v1.pdf | author:Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin, Scott Miller category:cs.CL cs.LG stat.ML published:2015-02-05 summary:This paper describes the resource- and system-building efforts of aneight-week Johns Hopkins University Human Language Technology Center ofExcellence Summer Camp for Applied Language Exploration (SCALE-2009) onSemantically-Informed Machine Translation (SIMT). We describe a newmodality/negation (MN) annotation scheme, the creation of a (publiclyavailable) MN lexicon, and two automated MN taggers that we built using theannotation scheme and lexicon. Our annotation scheme isolates three componentsof modality and negation: a trigger (a word that conveys modality or negation),a target (an action associated with modality or negation) and a holder (anexperiencer of modality). We describe how our MN lexicon was semi-automaticallyproduced and we demonstrate that a structure-based MN tagger results inprecision around 86% (depending on genre) for tagging of a standard LDC dataset. We apply our MN annotation scheme to statistical machine translation using asyntactic framework that supports the inclusion of semantic annotations.Syntactic tags enriched with semantic annotations are assigned to parse treesin the target-language training texts through a process of tree grafting. Whilethe focus of our work is modality and negation, the tree grafting procedure isgeneral and supports other types of semantic information. We exploit thiscapability by including named entities, produced by a pre-existing tagger, inaddition to the MN elements produced by the taggers described in this paper.The resulting system significantly outperformed a linguistically naive baselinemodel (Hiero), and reached the highest scores yet reported on the NIST 2009Urdu-English test set. This finding supports the hypothesis that both syntacticand semantic information can improve translation quality.
arxiv-9000-142 | Estimating Optimal Active Learning via Model Retraining Improvement | http://arxiv.org/pdf/1502.01664v1.pdf | author:Lewis P. G. Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG published:2015-02-05 summary:A central question for active learning (AL) is: "what is the optimalselection?" Defining optimality by classifier loss produces a newcharacterisation of optimal AL behaviour, by treating expected loss reductionas a statistical target for estimation. This target forms the basis of modelretraining improvement (MRI), a novel approach providing a statisticalestimation framework for AL. This framework is constructed to address thecentral question of AL optimality, and to motivate the design of estimationalgorithms. MRI allows the exploration of optimal AL behaviour, and theexamination of AL heuristics, showing precisely how they make sub-optimalselections. The abstract formulation of MRI is used to provide a new guaranteefor AL, that an unbiased MRI estimator should outperform random selection. ThisMRI framework reveals intricate estimation issues that in turn motivate theconstruction of new statistical AL algorithms. One new algorithm in particularperforms strongly in a large-scale experimental study, compared to standard ALmethods. This competitive performance suggests that practical efforts tominimise estimation bias may be important for AL applications.
arxiv-9000-143 | Learning Articulated Motions From Visual Demonstration | http://arxiv.org/pdf/1502.01659v1.pdf | author:Sudeep Pillai, Matthew R. Walter, Seth Teller category:cs.RO cs.CV published:2015-02-05 summary:Many functional elements of human homes and workplaces consist of rigidcomponents which are connected through one or more sliding or rotatinglinkages. Examples include doors and drawers of cabinets and appliances;laptops; and swivel office chairs. A robotic mobile manipulator would benefitfrom the ability to acquire kinematic models of such objects from observation.This paper describes a method by which a robot can acquire an object model bycapturing depth imagery of the object as a human moves it through its range ofmotion. We envision that in future, a machine newly introduced to anenvironment could be shown by its human user the articulated objects particularto that environment, inferring from these "visual demonstrations" enoughinformation to actuate each object independently of the user. Our method employs sparse (markerless) feature tracking, motion segmentation,component pose estimation, and articulation learning; it does not require priorobject models. Using the method, a robot can observe an object being exercised,infer a kinematic model incorporating rigid, prismatic and revolute joints,then use the model to predict the object's motion from a novel vantage point.We evaluate the method's performance, and compare it to that of a previouslypublished technique, for a variety of household objects.
arxiv-9000-144 | Performance Analysis of Cone Detection Algorithms | http://arxiv.org/pdf/1502.01643v1.pdf | author:Letizia Mariotti, Nicholas Devaney category:physics.med-ph cs.CV published:2015-02-05 summary:Many algorithms have been proposed to help clinicians evaluate cone densityand spacing, as these may be related to the onset of retinal diseases. However,there has been no rigorous comparison of the performance of these algorithms.In addition, the performance of such algorithms is typically determined bycomparison with human observers. Here we propose a technique to simulaterealistic images of the cone mosaic. We use the simulated images to test theperformance of two popular cone detection algorithms and we introduce analgorithm which is used by astronomers to detect stars in astronomical images.We use Free Response Operating Characteristic (FROC) curves to evaluate andcompare the performance of the three algorithms. This allows us to optimize theperformance of each algorithm. We observe that performance is significantlyenhanced by up-sampling the images. We investigate the effect of noise andimage quality on cone mosaic parameters estimated using the differentalgorithms, finding that the estimated regularity is the most sensitiveparameter. This paper was published in JOSA A and is made available as an electronicreprint with the permission of OSA. The paper can be found at the following URLon the OSA website: http://www.opticsinfobase.org/abstract.cfm?msid=224577.Systematic or multiple reproduction or distribution to multiple locations viaelectronic or other means is prohibited and is subject to penalties under law.
arxiv-9000-145 | A Simple Expression for Mill's Ratio of the Student's $t$-Distribution | http://arxiv.org/pdf/1502.01632v1.pdf | author:Francesco Orabona category:cs.LG math.PR published:2015-02-05 summary:I show a simple expression of the Mill's ratio of the Student'st-Distribution. I use it to prove Conjecture 1 in P. Auer, N. Cesa-Bianchi, andP. Fischer. Finite-time analysis of the multiarmed bandit problem. Mach.Learn., 47(2-3):235--256, May 2002.
arxiv-9000-146 | A PARTAN-Accelerated Frank-Wolfe Algorithm for Large-Scale SVM Classification | http://arxiv.org/pdf/1502.01563v1.pdf | author:Emanuele Frandi, Ricardo Nanculef, Johan A. K. Suykens category:stat.ML cs.LG math.OC published:2015-02-05 summary:Frank-Wolfe algorithms have recently regained the attention of the MachineLearning community. Their solid theoretical properties and sparsity guaranteesmake them a suitable choice for a wide range of problems in this field. Inaddition, several variants of the basic procedure exist that improve itstheoretical properties and practical performance. In this paper, we investigatethe application of some of these techniques to Machine Learning, focusing inparticular on a Parallel Tangent (PARTAN) variant of the FW algorithm that hasnot been previously suggested or studied for this type of problems. We provideexperiments both in a standard setting and using a stochastic speed-uptechnique, showing that the considered algorithms obtain promising results onseveral medium and large-scale benchmark datasets for SVM classification.
arxiv-9000-147 | Semantic Embedding Space for Zero-Shot Action Recognition | http://arxiv.org/pdf/1502.01540v1.pdf | author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV published:2015-02-05 summary:The number of categories for action recognition is growing rapidly. It isthus becoming increasingly hard to collect sufficient training data to learnconventional models for each category. This issue may be ameliorated by theincreasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework amapping is constructed between visual features and a human interpretablesemantic description of each category, allowing categories to be recognised inthe absence of any training data. Existing ZSL studies focus primarily on imagedata, and attribute-based semantic representations. In this paper, we addresszero-shot recognition in contemporary video action recognition tasks, usingsemantic word vector space as the common space to embed videos and categorylabels. This is more challenging because the mapping between the semantic spaceand space-time features of videos containing complex actions is more complexand harder to learn. We demonstrate that a simple self-training and dataaugmentation strategy can significantly improve the efficacy of this mapping.Experiments on human action datasets including HMDB51 and UCF101 demonstratethat our approach achieves the state-of-the-art zero-shot action recognitionperformance.
arxiv-9000-148 | Object Proposal via Partial Re-ranking | http://arxiv.org/pdf/1502.01526v1.pdf | author:Jing Wang, Jie Shen category:cs.CV published:2015-02-05 summary:Object proposals are an ensemble of bounding boxes with high potential tocontain objects. Usually, the ranking models are utilized in order to provide amanageable number of candidate boxes. To obtain the rank for each candidate,prior ranking models generally compare each pair of candidates. However, onemay be interested in only the top-$k$ candidates rather than all ones. Thus, inthis paper, we propose a new ranking model for object proposals, which aims toproduce a reliable estimation for only the top-$k$ candidates. To this end, wecompute the IoU for each candidate and split the candidates into two subsetsconsisting of the top-$k$ candidates and the others respectively. Partialranking constraints are imposed on the two subsets: any candidate from thefirst subset is better than that from the second one. In this way, theconstraints are reduced dramatically compared to the full ranking model, whichfurther facilitates an efficient learning procedure. Moreover, we show that ourpartial ranking model can be reduced into the large margin based framework.Extensive experiments demonstrate that after a re-ranking step of our model,the top-$k$ detection rate can be significantly improved.
arxiv-9000-149 | A specialized face-processing network consistent with the representational geometry of monkey face patches | http://arxiv.org/pdf/1502.01241v2.pdf | author:Amirhossein Farzmahdi, Karim Rajaei, Masoud Ghodrati, Reza Ebrahimpour, Seyed-Mahdi Khaligh-Razavi category:q-bio.NC cs.CV published:2015-02-04 summary:Ample evidence suggests that face processing in human and non-human primatesis performed differently compared with other objects. Converging reports, bothphysiologically and psychophysically, indicate that faces are processed inspecialized neural networks in the brain -i.e. face patches in monkeys and thefusiform face area (FFA) in humans. We are all expert face-processing agents,and able to identify very subtle differences within the category of faces,despite substantial visual and featural similarities. Identification isperformed rapidly and accurately after viewing a whole face, whilesignificantly drops if some of the face configurations (e.g. inversion,misalignment) are manipulated or if partial views of faces are shown due toocclusion. This refers to a hotly-debated, yet highly-supported concept, knownas holistic face processing. We built a hierarchical computational model offace-processing based on evidence from recent neuronal and behavioural studieson faces processing in primates. Representational geometries of the last threelayers of the model have characteristics similar to those observed in monkeyface patches (posterior, middle and anterior patches). Furthermore, severalface-processing-related phenomena reported in the literature automaticallyemerge as properties of this model. The representations are evolved throughseveral computational layers, using biologically plausible learning rules. Themodel satisfies face inversion effect, composite face effect, other raceeffect, view and identity selectivity, and canonical face views. To ourknowledge, no models have so far been proposed with this performance andagreement with biological data.
arxiv-9000-150 | A mixture Cox-Logistic model for feature selection from survival and classification data | http://arxiv.org/pdf/1502.01493v1.pdf | author:Samuel Branders, Roberto D'Ambrosio, Pierre Dupont category:stat.ML cs.LG stat.ME published:2015-02-05 summary:This paper presents an original approach for jointly fitting survival timesand classifying samples into subgroups. The Coxlogit model is a generalizedlinear model with a common set of selected features for both tasks. Survivaltimes and class labels are here assumed to be conditioned by a common riskscore which depends on those features. Learning is then naturally expressed asmaximizing the joint probability of subgroup labels and the ordering ofsurvival events, conditioned to a common weight vector. The model is estimatedby minimizing a regularized log-likelihood through a coordinate descentalgorithm. Validation on synthetic and breast cancer data shows that the proposedapproach outperforms a standard Cox model or logistic regression when bothpredicting the survival times and classifying new samples into subgroups. It isalso better at selecting informative features for both tasks.
arxiv-9000-151 | Ring artifacts correction in compressed sensing tomographic reconstruction | http://arxiv.org/pdf/1502.01480v1.pdf | author:Pierre Paleo, Alessandro Mirone category:cs.CV published:2015-02-05 summary:We present a novel approach to handle ring artifacts correction in compressedsensing tomographic reconstruction. The correction is part of thereconstruction process, which differs from classical sinogram pre-processingand image post-processing techniques. The principle of compressed sensingtomographic reconstruction is presented. Then, we show that the ring artifactscorrection can be integrated in the reconstruction problem formalism. Weprovide numerical results for both simulated and real data. This technique isincluded in the PyHST2 code which is used at the European Synchrotron RadiationFacility for tomographic reconstruction.
arxiv-9000-152 | Fast Constraint Propagation for Image Segmentation | http://arxiv.org/pdf/1502.01475v1.pdf | author:Peng Han category:cs.CV published:2015-02-05 summary:This paper presents a novel selective constraint propagation method forconstrained image segmentation. In the literature, many pairwise constraintpropagation methods have been developed to exploit pairwise constraints forcluster analysis. However, since most of these methods have a polynomial timecomplexity, they are not much suitable for segmentation of images even with amoderate size, which is actually equivalent to cluster analysis with a largedata size. Considering the local homogeneousness of a natural image, we chooseto perform pairwise constraint propagation only over a selected subset ofpixels, but not over the whole image. Such a selective constraint propagationproblem is then solved by an efficient graph-based learning algorithm. Tofurther speed up our selective constraint propagation, we also discard thoseless important propagated constraints during graph-based learning. Finally, theselectively propagated constraints are exploited based on $L_1$-minimizationfor normalized cuts over the whole image. The experimental results demonstratethe promising performance of the proposed method for segmentation withselectively propagated constraints.
arxiv-9000-153 | Exponentiated Subgradient Algorithm for Online Optimization under the Random Permutation Model | http://arxiv.org/pdf/1410.7171v2.pdf | author:Reza Eghbali, Jon Swenson, Maryam Fazel category:math.OC cs.DS cs.LG published:2014-10-27 summary:Online optimization problems arise in many resource allocation tasks, wherethe future demands for each resource and the associated utility functionschange over time and are not known apriori, yet resources need to be allocatedat every point in time despite the future uncertainty. In this paper, weconsider online optimization problems with general concave utilities. We modifyand extend an online optimization algorithm proposed by Devanur et al. forlinear programming to this general setting. The model we use for the arrival ofthe utilities and demands is known as the random permutation model, where afixed collection of utilities and demands are presented to the algorithm inrandom order. We prove that under this model the algorithm achieves acompetitive ratio of $1-O(\epsilon)$ under a near-optimal assumption that thebid to budget ratio is $O (\frac{\epsilon^2}{\log({m}/{\epsilon})})$, where $m$is the number of resources, while enjoying a significantly lower computationalcost than the optimal algorithm proposed by Kesselheim et al. We draw aconnection between the proposed algorithm and subgradient methods used inconvex optimization. In addition, we present numerical experiments thatdemonstrate the performance and speed of this algorithm in comparison toexisting algorithms.
arxiv-9000-154 | Beyond Word-based Language Model in Statistical Machine Translation | http://arxiv.org/pdf/1502.01446v1.pdf | author:Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, Chengqing Zong category:cs.CL published:2015-02-05 summary:Language model is one of the most important modules in statistical machinetranslation and currently the word-based language model dominants thiscommunity. However, many translation models (e.g. phrase-based models) generatethe target language sentences by rendering and compositing the phrases ratherthan the words. Thus, it is much more reasonable to model dependency betweenphrases, but few research work succeed in solving this problem. In this paper,we tackle this problem by designing a novel phrase-based language model whichattempts to solve three key sub-problems: 1, how to define a phrase in languagemodel; 2, how to determine the phrase boundary in the large-scale monolingualdata in order to enlarge the training set; 3, how to alleviate the datasparsity problem due to the huge vocabulary size of phrases. By carefullyhandling these issues, the extensive experiments on Chinese-to-Englishtranslation show that our phrase-based language model can significantly improvethe translation quality by up to +1.47 absolute BLEU score.
arxiv-9000-155 | Empirical Q-Value Iteration | http://arxiv.org/pdf/1412.0180v2.pdf | author:Dileep Kalathil, Vivek S. Borkar, Rahul Jain category:math.OC cs.LG published:2014-11-30 summary:We propose a new simple and natural algorithm for learning the optimal$Q$-value function of a discounted-cost Markov Decision Process (MDP) when thetransition kernels are unknown. Unlike the classical learning algorithms forMDPs, such as $Q$-learning and `actor-critic' algorithms, this algorithmdoesn't depend on a stochastic approximation-based method. We show that ouralgorithm, which we call the empirical $Q$-value iteration (EQVI) algorithm,converges almost surely to the optimal $Q$-value function. To the best of ourknowledge, this is the first algorithm for learning in MDPs that guarantees analmost sure convergence without using stochastic approximations. We also give arate of convergence or a non-aymptotic sample complexity bound, and also showthat an asynchronous (or online) version of the algorithm will also work.Preliminary experimental results suggest a faster rate of convergence to a ballpark estimate for our algorithm compared to stochastic approximation-basedalgorithms. In fact, the asynchronous setting EQVI vastly outperforms thepopular and widely-used Q-learning algorithm.
arxiv-9000-156 | Super-resolution MRI Using Finite Rate of Innovation Curves | http://arxiv.org/pdf/1501.01697v2.pdf | author:Greg Ongie, Mathews Jacob category:cs.CV published:2015-01-08 summary:We propose a two-stage algorithm for the super-resolution of MR images fromtheir low-frequency k-space samples. In the first stage we estimate aresolution-independent mask whose zeros represent the edges of the image. Thisbuilds off recent work extending the theory of sampling signals of finite rateof innovation (FRI) to two-dimensional curves. We enable its application to MRIby proposing extensions of the signal models allowed by FRI theory, and bydeveloping a more robust and efficient means to determine the edge mask. In thesecond stage of the scheme, we recover the super-resolved MR image using thediscretized edge mask as an image prior. We evaluate our scheme on simulatedsingle-coil MR data obtained from analytical phantoms, and compare againsttotal variation reconstructions. Our experiments show improved performance inboth noiseless and noisy settings.
arxiv-9000-157 | Open System Categorical Quantum Semantics in Natural Language Processing | http://arxiv.org/pdf/1502.00831v2.pdf | author:Robin Piedeleu, Dimitri Kartsaklis, Bob Coecke, Mehrnoosh Sadrzadeh category:cs.CL cs.LO math.CT math.QA published:2015-02-03 summary:Originally inspired by categorical quantum mechanics (Abramsky and Coecke,LiCS'04), the categorical compositional distributional model of naturallanguage meaning of Coecke, Sadrzadeh and Clark provides a conceptuallymotivated procedure to compute the meaning of a sentence, given its grammaticalstructure within a Lambek pregroup and a vectorial representation of themeaning of its parts. The predictions of this first model have outperformedthat of other models in mainstream empirical language processing tasks on largescale data. Moreover, just like CQM allows for varying the model in which weinterpret quantum axioms, one can also vary the model in which we interpretword meaning. In this paper we show that further developments in categorical quantummechanics are relevant to natural language processing too. Firstly, Selinger'sCPM-construction allows for explicitly taking into account lexical ambiguityand distinguishing between the two inherently different notions of homonymy andpolysemy. In terms of the model in which we interpret word meaning, this meansa passage from the vector space model to density matrices. Despite this changeof model, standard empirical methods for comparing meanings can be easilyadopted, which we demonstrate by a small-scale experiment on real-world data.This experiment moreover provides preliminary evidence of the validity of ourproposed new model for word meaning. Secondly, commutative classical structures as well as their non-commutativecounterparts that arise in the image of the CPM-construction allow for encodingrelative pronouns, verbs and adjectives, and finally, iteration of theCPM-construction, something that has no counterpart in the quantum realm,enables one to accommodate both entailment and ambiguity.
arxiv-9000-158 | Optimal learning with Bernstein Online Aggregation | http://arxiv.org/pdf/1404.1356v3.pdf | author:Olivier Wintenberger category:stat.ML cs.LG math.ST stat.TH published:2014-04-04 summary:We introduce a new recursive aggregation procedure called Bernstein OnlineAggregation (BOA). The exponential weights include an accuracy term and asecond order term that is a proxy of the quadratic variation as in Hazan andKale (2010). This second term stabilizes the procedure that is optimal indifferent senses. We first obtain optimal regret bounds in the deterministiccontext. Then, an adaptive version is the first exponential weights algorithmthat exhibits a second order bound with excess losses that appears first inGaillard et al. (2014). The second order bounds in the deterministic contextare extended to a general stochastic context using the cumulative predictiverisk. Such conversion provides the main result of the paper, an inequality of anovel type comparing the procedure with any deterministic aggregation procedurefor an integrated criteria. Then we obtain an observable estimate of the excessof risk of the BOA procedure. To assert the optimality, we consider finally theiid case for strongly convex and Lipschitz continuous losses and we prove thatthe optimal rate of aggregation of Tsybakov (2003) is achieved. The batchversion of the BOA procedure is then the first adaptive explicit algorithm thatsatisfies an optimal oracle inequality with high probability.
arxiv-9000-159 | Authorship recognition via fluctuation analysis of network topology and word intermittency | http://arxiv.org/pdf/1502.01245v1.pdf | author:Diego R. Amancio category:cs.CL published:2015-02-04 summary:Statistical methods have been widely employed in many practical naturallanguage processing applications. More specifically, complex networks conceptsand methods from dynamical systems theory have been successfully applied torecognize stylistic patterns in written texts. Despite the large amount ofstudies devoted to represent texts with physical models, only a few studieshave assessed the relevance of attributes derived from the analysis ofstylistic fluctuations. Because fluctuations represent a pivotal factor forcharacterizing a myriad of real systems, this study focused on the analysis ofthe properties of stylistic fluctuations in texts via topological analysis ofcomplex networks and intermittency measurements. The results showed thatdifferent authors display distinct fluctuation patterns. In particular, it wasfound that it is possible to identify the authorship of books using theintermittency of specific words. Taken together, the results described heresuggest that the patterns found in stylistic fluctuations could be used toanalyze other related complex systems. Furthermore, the discovery of novelpatterns related to textual stylistic fluctuations indicates that thesepatterns could be useful to improve the state of the art of manystylistic-based natural language processing tasks.
arxiv-9000-160 | Freehand Sketch Recognition Using Deep Features | http://arxiv.org/pdf/1502.00254v2.pdf | author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV published:2015-02-01 summary:Freehand sketches often contain sparse visual detail. In spite of thesparsity, they are easily and consistently recognized by humans acrosscultures, languages and age groups. Therefore, analyzing such sparse sketchescan aid our understanding of the neuro-cognitive processes involved in visualrepresentation and recognition. In the recent past, Convolutional NeuralNetworks (CNNs) have emerged as a powerful framework for feature representationand recognition for a variety of image domains. However, the domain of sketchimages has not been explored. This paper introduces a freehand sketchrecognition framework based on "deep" features extracted from CNNs. We use twopopular CNNs for our experiments -- Imagenet CNN and a modified version ofLeNet CNN. We evaluate our recognition framework on a publicly availablebenchmark database containing thousands of freehand sketches depicting everydayobjects. Our results are an improvement over the existing state-of-the-artaccuracies by 3% - 11%. The effectiveness and relative compactness of our deepfeatures also make them an ideal candidate for related problems such assketch-based image retrieval. In addition, we provide a preliminary glimpse ofhow such features can help identify crucial attributes (e.g. object-parts) ofthe sketched objects.
arxiv-9000-161 | Learning Local Invariant Mahalanobis Distances | http://arxiv.org/pdf/1502.01176v1.pdf | author:Ethan Fetaya, Shimon Ullman category:cs.LG stat.ML published:2015-02-04 summary:For many tasks and data types, there are natural transformations to which thedata should be invariant or insensitive. For instance, in visual recognition,natural images should be insensitive to rotation and translation. Thisrequirement and its implications have been important in many machine learningapplications, and tolerance for image transformations was primarily achieved byusing robust feature vectors. In this paper we propose a novel andcomputationally efficient way to learn a local Mahalanobis metric per datum,and show how we can learn a local invariant metric to any transformation inorder to improve performance.
arxiv-9000-162 | Constrained Extreme Learning Machines: A Study on Classification Cases | http://arxiv.org/pdf/1501.06115v2.pdf | author:Wentao Zhu, Jun Miao, Laiyun Qing category:cs.LG cs.CV cs.NE published:2015-01-25 summary:Extreme learning machine (ELM) is an extremely fast learning method and has apowerful performance for pattern recognition tasks proven by enormousresearches and engineers. However, its good generalization ability is built onlarge numbers of hidden neurons, which is not beneficial to real time responsein the test process. In this paper, we proposed new ways, named "constrainedextreme learning machines" (CELMs), to randomly select hidden neurons based onsample distribution. Compared to completely random selection of hidden nodes inELM, the CELMs randomly select hidden nodes from the constrained vector spacecontaining some basic combinations of original sample vectors. The experimentalresults show that the CELMs have better generalization ability than traditionalELM, SVM and some other related methods. Additionally, the CELMs have a similarfast learning speed as ELM.
arxiv-9000-163 | Composite convex minimization involving self-concordant-like cost functions | http://arxiv.org/pdf/1502.01068v1.pdf | author:Quoc Tran-Dinh, Yen-Huan Li, Volkan Cevher category:math.OC stat.ML published:2015-02-04 summary:The self-concordant-like property of a smooth convex function is a newanalytical structure that generalizes the self-concordant notion. While a widevariety of important applications feature the self-concordant-like property,this concept has heretofore remained unexploited in convex optimization. Tothis end, we develop a variable metric framework of minimizing the sum of a"simple" convex function and a self-concordant-like function. We introduce anew analytic step-size selection procedure and prove that the basic gradientalgorithm has improved convergence guarantees as compared to "fast" algorithmsthat rely on the Lipschitz gradient property. Our numerical tests withreal-data sets shows that the practice indeed follows the theory.
arxiv-9000-164 | The Noisy Power Method: A Meta Algorithm with Applications | http://arxiv.org/pdf/1311.2495v4.pdf | author:Moritz Hardt, Eric Price category:cs.DS cs.LG published:2013-11-11 summary:We provide a new robust convergence analysis of the well-known power methodfor computing the dominant singular vectors of a matrix that we call the noisypower method. Our result characterizes the convergence behavior of thealgorithm when a significant amount noise is introduced after eachmatrix-vector multiplication. The noisy power method can be seen as ameta-algorithm that has recently found a number of important applications in abroad range of machine learning problems including alternating minimization formatrix completion, streaming principal component analysis (PCA), andprivacy-preserving spectral analysis. Our general analysis subsumes severalexisting ad-hoc convergence bounds and resolves a number of open problems inmultiple applications including streaming PCA and privacy-preserving singularvector computation.
arxiv-9000-165 | Personalized Web Search | http://arxiv.org/pdf/1502.01057v1.pdf | author:Li Zhou category:cs.IR cs.LG published:2015-02-03 summary:Personalization is important for search engines to improve user experience.Most of the existing work do pure feature engineering and extract a lot ofsession-style features and then train a ranking model. Here we proposed a novelway to model both long term and short term user behavior using Multi-armedbandit algorithm. Our algorithm can generalize session information across userswell, and as an Explore-Exploit style algorithm, it can generalize to new urlsand new users well. Experiments show that our algorithm can improve performanceover the default ranking and outperforms several popular Multi-armed banditalgorithms.
arxiv-9000-166 | DFDL: Discriminative Feature-oriented Dictionary Learning for Histopathological Image Classification | http://arxiv.org/pdf/1502.01032v1.pdf | author:Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga, UK Arvind Rao, Ganesh Rao category:cs.CV published:2015-02-03 summary:In histopathological image analysis, feature extraction for classification isa challenging task due to the diversity of histology features suitable for eachproblem as well as presence of rich geometrical structure. In this paper, wepropose an automatic feature discovery framework for extracting discriminativeclass-specific features and present a low-complexity method for classificationand disease grading in histopathology. Essentially, our DiscriminativeFeature-oriented Dictionary Learning (DFDL) method learns class-specificfeatures which are suitable for representing samples from the same class whileare poorly capable of representing samples from other classes. Experiments onthree challenging real-world image databases: 1) histopathological images ofintraductal breast lesions, 2) mammalian lung images provided by the AnimalDiagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumorimages from The Cancer Genome Atlas (TCGA) database, show the significance ofDFDL model in a variety problems over state-of-the-art methods
arxiv-9000-167 | Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences | http://arxiv.org/pdf/1306.0514v4.pdf | author:Yann Ollivier category:cs.NE cs.LG 68T05, 68T10 published:2013-06-03 summary:Recurrent neural networks are powerful models for sequential data, able torepresent complex dependencies in the sequence that simpler models such ashidden Markov models cannot handle. Yet they are notoriously hard to train.Here we introduce a training procedure using a gradient ascent in a Riemannianmetric: this produces an algorithm independent from design choices such as theencoding of parameters and unit activities. This metric gradient ascent isdesigned to have an algorithmic cost close to backpropagation through time forsparsely connected networks. We use this procedure on gated leaky neuralnetworks (GLNNs), a variant of recurrent neural networks with an architectureinspired by finite automata and an evolution equation inspired bycontinuous-time networks. GLNNs trained with a Riemannian gradient aredemonstrated to effectively capture a variety of structures in syntheticproblems: basic block nesting as in context-free grammars (an important featureof natural languages, but difficult to learn), intersections of multipleindependent Markov-type relations, or long-distance relationships such as thedistant-XOR problem. This method does not require adjusting the networkstructure or initial parameters: the network used is a sparse random graph andthe initialization is identical for all problems considered.
arxiv-9000-168 | Riemannian metrics for neural networks I: feedforward networks | http://arxiv.org/pdf/1303.0818v5.pdf | author:Yann Ollivier category:cs.NE cs.IT cs.LG math.DG math.IT 68T05 published:2013-03-04 summary:We describe four algorithms for neural network training, each adapted todifferent scalability constraints. These algorithms are mathematicallyprincipled and invariant under a number of transformations in data and networkrepresentation, from which performance is thus independent. These algorithmsare obtained from the setting of differential geometry, and are based on eitherthe natural gradient using the Fisher information matrix, or on Hessianmethods, scaled down in a specific way to allow for scalability while keepingsome of their key mathematical properties.
arxiv-9000-169 | Classification of Hyperspectral Imagery on Embedded Grassmannians | http://arxiv.org/pdf/1502.00946v1.pdf | author:Sofya Chepushtanova, Michael Kirby category:cs.CV published:2015-02-03 summary:We propose an approach for capturing the signal variability in hyperspectralimagery using the framework of the Grassmann manifold. Labeled points from eachclass are sampled and used to form abstract points on the Grassmannian. Theresulting points on the Grassmannian have representations as orthonormalmatrices and as such do not reside in Euclidean space in the usual sense. Thereare a variety of metrics which allow us to determine a distance matrices thatcan be used to realize the Grassmannian as an embedding in Euclidean space. Weillustrate that we can achieve an approximately isometric embedding of theGrassmann manifold using the chordal metric while this is not the case withgeodesic distances. However, non-isometric embeddings generated by using apseudometric on the Grassmannian lead to the best classification results. Weobserve that as the dimension of the Grassmannian grows, the accuracy of theclassification grows to 100% on two illustrative examples. We also observe adecrease in classification rates if the dimension of the points on theGrassmannian is too large for the dimension of the Euclidean space. We usesparse support vector machines to perform additional model reduction. Theresulting classifier selects a subset of dimensions of the embedding withoutloss in classification performance.
arxiv-9000-170 | Learning Planar Ising Models | http://arxiv.org/pdf/1502.00916v1.pdf | author:Jason K. Johnson, Diane Oyen, Michael Chertkov, Praneeth Netrapalli category:stat.ML published:2015-02-03 summary:Inference and learning of graphical models are both well-studied problems instatistics and machine learning that have found many applications in scienceand engineering. However, exact inference is intractable in general graphicalmodels, which suggests the problem of seeking the best approximation to acollection of random variables within some tractable family of graphicalmodels. In this paper, we focus on the class of planar Ising models, for whichexact inference is tractable using techniques of statistical physics. Based onthese techniques and recent methods for planarity testing and planar embedding,we propose a simple greedy algorithm for learning the best planar Ising modelto approximate an arbitrary collection of binary random variables (possiblyfrom sample data). Given the set of all pairwise correlations among variables,we select a planar graph and optimal planar Ising model defined on this graphto best approximate that set of correlations. We demonstrate our method insimulations and for the application of modeling senate voting records.
arxiv-9000-171 | DeepID3: Face Recognition with Very Deep Neural Networks | http://arxiv.org/pdf/1502.00873v1.pdf | author:Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2015-02-03 summary:The state-of-the-art of face recognition has been significantly advanced bythe emergence of deep learning. Very deep neural networks recently achievedgreat success on general object recognition because of their superb learningcapacity. This motivates us to investigate their effectiveness on facerecognition. This paper proposes two very deep neural network architectures,referred to as DeepID3, for face recognition. These two architectures arerebuilt from stacked convolution and inception layers proposed in VGG net andGoogLeNet to make them suitable to face recognition. Joint faceidentification-verification supervisory signals are added to both intermediateand final feature extraction layers during training. An ensemble of theproposed two architectures achieves 99.53% LFW face verification accuracy and96.0% LFW rank-1 face identification accuracy, respectively. A furtherdiscussion of LFW face verification result is given in the end.
arxiv-9000-172 | Optimistic Risk Perception in the Temporal Difference error Explains the Relation between Risk-taking, Gambling, Sensation-seeking and Low Fear | http://arxiv.org/pdf/1404.2078v2.pdf | author:Joost Broekens, Tim Baarslag category:cs.LG q-bio.NC published:2014-04-08 summary:Understanding the affective, cognitive and behavioural processes involved inrisk taking is essential for treatment and for setting environmental conditionsto limit damage. Using Temporal Difference Reinforcement Learning (TDRL) wecomputationally investigated the effect of optimism in risk perception in avariety of goal-oriented tasks. Optimism in risk perception was studied byvarying the calculation of the Temporal Difference error, i.e., delta, in threeways: realistic (stochastically correct), optimistic (assuming action control),and overly optimistic (assuming outcome control). We show that for the gamblingtask individuals with 'healthy' perception of control, i.e., action optimism,do not develop gambling behaviour while individuals with 'unhealthy' perceptionof control, i.e., outcome optimism, do. We show that high intensity ofsensations and low levels of fear co-occur due to optimistic risk perception.We found that overly optimistic risk perception (outcome optimism) results inrisk taking and in persistent gambling behaviour in addition to high intensityof sensations. We discuss how our results replicate risk-taking relatedphenomena.
arxiv-9000-173 | Face frontalization for Alignment and Recognition | http://arxiv.org/pdf/1502.00852v1.pdf | author:Christos Sagonas, Yannis Panagakis, Stefanos Zafeiriou, Maja Pantic category:cs.CV published:2015-02-03 summary:Recently, it was shown that excellent results can be achieved in both facelandmark localization and pose-invariant face recognition. These breakthroughsare attributed to the efforts of the community to manually annotate facialimages in many different poses and to collect 3D faces data. In this paper, wepropose a novel method for joint face landmark localization and frontal facereconstruction (pose correction) using a small set of frontal images only. Byobserving that the frontal facial image is the one with the minimum rank fromall different poses we formulate an appropriate model which is able to jointlyrecover the facial landmarks as well as the frontalized version of the face. Tothis end, a suitable optimization problem, involving the minimization of thenuclear norm and the matrix $\ell_1$ norm, is solved. The proposed method isassessed in frontal face reconstruction (pose correction), face landmarklocalization, and pose-invariant face recognition and verification byconducting experiments on $6$ facial images databases. The experimental resultsdemonstrate the effectiveness of the proposed method.
arxiv-9000-174 | A multiset model of multi-species evolution to solve big deceptive problems | http://arxiv.org/pdf/1502.00839v1.pdf | author:Luis Correia, Antonio Manso category:cs.NE q-bio.PE published:2015-02-03 summary:This chapter presents SMuGA, an integration of symbiogenesis with theMultiset Genetic Algorithm (MuGA). The symbiogenetic approach used here isbased on the host-parasite model with the novelty of varying the length ofparasites along the evolutionary process. Additionally, it modelscollaborations between multiple parasites and a single host. To improveefficiency, we introduced proxy evaluation of parasites, which saves fitnessfunction calls and exponentially reduces the symbiotic collaborations produced.Another novel feature consists of breaking the evolutionary cycle into twophases: a symbiotic phase and a phase of independent evolution of both hostsand parasites. SMuGA was tested in optimization of a variety of deceptivefunctions, with results one order of magnitude better than state of the artsymbiotic algorithms. This allowed to optimize deceptive problems with largesizes, and showed a linear scaling in the number of iterations to attain theoptimum.
arxiv-9000-175 | Task-Driven Dictionary Learning for Hyperspectral Image Classification with Structured Sparsity Constraints | http://arxiv.org/pdf/1502.00836v1.pdf | author:Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV published:2015-02-03 summary:Sparse representation models a signal as a linear combination of a smallnumber of dictionary atoms. As a generative model, it requires the dictionaryto be highly redundant in order to ensure both a stable high sparsity level anda low reconstruction error for the signal. However, in practice, thisrequirement is usually impaired by the lack of labelled training samples.Fortunately, previous research has shown that the requirement for a redundantdictionary can be less rigorous if simultaneous sparse approximation isemployed, which can be carried out by enforcing various structured sparsityconstraints on the sparse codes of the neighboring pixels. In addition,numerous works have shown that applying a variety of dictionary learningmethods for the sparse representation model can also improve the classificationperformance. In this paper, we highlight the task-driven dictionary learningalgorithm, which is a general framework for the supervised dictionary learningmethod. We propose to enforce structured sparsity priors on the task-drivendictionary learning method in order to improve the performance of thehyperspectral classification. Our approach is able to benefit from both theadvantages of the simultaneous sparse representation and those of thesupervised dictionary learning. We enforce two different structured sparsitypriors, the joint and Laplacian sparsity, on the task-driven dictionarylearning method and provide the details of the corresponding optimizationalgorithms. Experiments on numerous popular hyperspectral images demonstratethat the classification performance of our approach is superior to sparserepresentation classifier with structured priors or the task-driven dictionarylearning method.
arxiv-9000-176 | Landmark-Guided Elastic Shape Analysis of Human Character Motions | http://arxiv.org/pdf/1502.07666v1.pdf | author:Martin Bauer, Markus Eslitzbichler, Markus Grasmair category:cs.CV cs.GR published:2015-02-03 summary:Motions of virtual characters in movies or video games are typicallygenerated by recording actors using motion capturing methods. Animationsgenerated this way often need postprocessing, such as improving the periodicityof cyclic animations or generating entirely new motions by interpolation ofexisting ones. Furthermore, search and classification of recorded motionsbecomes more and more important as the amount of recorded motion data grows. In this paper, we will apply methods from shape analysis to the processing ofanimations. More precisely, we will use the by now classical elastic metricmodel used in shape matching, and extend it by incorporating additional inexactfeature point information, which leads to an improved temporal alignment ofdifferent animations.
arxiv-9000-177 | An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of Sanskrit Grammar | http://arxiv.org/pdf/1410.2871v2.pdf | author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL published:2014-10-10 summary:Euphonic conjunctions (sandhis) form a very important aspect of Sanskritmorphology and phonology. The traditional and modern methods of studying abouteuphonic conjunctions in Sanskrit follow different methodologies. The formerinvolves a rigorous study of the Paninian system embodied in Panini'sAshtadhyayi, while the latter usually involves the study of a few importantsandhi rules with the use of examples. The former is not suitable forbeginners, and the latter, not sufficient to gain a comprehensive understandingof the operation of sandhi rules. This is so since there are not only numeroussandhi rules and exceptions, but also complex precedence rules involved. Theneed for a new ontology for sandhi-tutoring was hence felt. This work presentsa comprehensive ontology designed to enable a student-user to learn in stagesall about euphonic conjunctions and the relevant aphorisms of Sanskrit grammarand to test and evaluate the progress of the student-user. The ontology formsthe basis of a multimedia sandhi tutor that was given to different categoriesof users including Sanskrit scholars for extensive and rigorous testing.
arxiv-9000-178 | Parameter Selection In Particle Swarm Optimization For Transportation Network Design Problem | http://arxiv.org/pdf/1412.7185v3.pdf | author:Mehran Fasihozaman Langerudi category:math.OC cs.NE published:2014-12-22 summary:In transportation planning and development, transport network design problemseeks to optimize specific objectives (e.g. total travel time) through choosingamong a given set of projects while keeping consumption of resources (e.g.budget) within their limits. Due to the numerous cases of choosing projects,solving such a problem is very difficult and time-consuming. Based on particleswarm optimization (PSO) technique, a heuristic solution algorithm for thebi-level problem is designed. This paper evaluates the algorithm performance inthe response of changing certain basic PSO parameters.
arxiv-9000-179 | Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with Discriminatively Trained Spatio-Temporal Model | http://arxiv.org/pdf/1502.00750v1.pdf | author:Xiaodan Liang, Qingxing Cao, Rui Huang, Liang Lin category:cs.CV 68U01 published:2015-02-03 summary:The aim of this study is to provide an automatic computational framework toassist clinicians in diagnosing Focal Liver Lesions (FLLs) inContrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clipas an ensemble of Region-of-Interests (ROIs), whose locations are modeled aslatent variables in a discriminative model. Different types of FLLs arecharacterized by both spatial and temporal enhancement patterns of the ROIs.The model is learned by iteratively inferring the optimal ROI locations andoptimizing the model parameters. To efficiently search the optimal spatial andtemporal locations of the ROIs, we propose a data-driven inference algorithm bycombining effective spatial and temporal pruning. The experiments show that ourmethod achieves promising results on the largest dataset in the literature (tothe best of our knowledge), which we have made publicly available.
arxiv-9000-180 | Data-Driven Scene Understanding with Adaptively Retrieved Exemplars | http://arxiv.org/pdf/1502.00749v1.pdf | author:Xionghao Liu, Wei Yang, Liang Lin, Qing Wang, Zhaoquan Cai, Jianhuang Lai category:cs.CV 68U01 published:2015-02-03 summary:This article investigates a data-driven approach for semantically sceneunderstanding, without pixelwise annotation and classifier training. Ourframework parses a target image with two steps: (i) retrieving its exemplars(i.e. references) from an image database, where all images are unsegmented butannotated with tags; (ii) recovering its pixel labels by propagating semanticsfrom the references. We present a novel framework making the two steps mutuallyconditional and bootstrapped under the probabilistic Expectation-Maximization(EM) formulation. In the first step, the references are selected by jointlymatching their appearances with the target as well as the semantics (i.e. theassigned labels of the target and the references). We process the second stepvia a combinatorial graphical representation, in which the vertices aresuperpixels extracted from the target and its selected references. Then wederive the potentials of assigning labels to one vertex of the target, whichdepend upon the graph edges that connect the vertex to its spatial neighbors ofthe target and to its similar vertices of the references. Besides, the proposedframework can be naturally applied to perform image annotation on new testimages. In the experiments, we validate our approach on two public databases,and demonstrate superior performances over the state-of-the-art methods in bothsemantic segmentation and image annotation tasks.
arxiv-9000-181 | Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection | http://arxiv.org/pdf/1502.00744v1.pdf | author:Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan category:cs.CV 68U01 published:2015-02-03 summary:This paper proposes a reconfigurable model to recognize and detect multiclass(or multiview) objects with large variation in appearance. Compared with wellacknowledged hierarchical models, we study two advanced capabilities inhierarchy for object modeling: (i) "switch" variables(i.e. or-nodes) forspecifying alternative compositions, and (ii) making local classifiers (i.e.leaf-nodes) shared among different classes. These capabilities enable us toaccount well for structural variabilities while preserving the model compact.Our model, in the form of an And-Or Graph, comprises four layers: a batch ofleaf-nodes with collaborative edges in bottom for localizing object parts; theor-nodes over bottom to activate their children leaf-nodes; the and-nodes toclassify objects as a whole; one root-node on the top for switching multiclassclassification, which is also an or-node. For model training, we present anEM-type algorithm, namely dynamical structural optimization (DSO), toiteratively determine the structural configuration, (e.g., leaf-node generationassociated with their parent or-nodes and shared across other classes), alongwith optimizing multi-layer parameters. The proposed method is valid onchallenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achievesstate-of-the-arts performance.
arxiv-9000-182 | Deep Joint Task Learning for Generic Object Extraction | http://arxiv.org/pdf/1502.00743v1.pdf | author:Xiaolong Wang, Liliang Zhang, Liang Lin, Zhujin Liang, Wangmeng Zuo category:cs.CV 68U01 published:2015-02-03 summary:This paper investigates how to extract objects-of-interest without relying onhand-craft features and sliding windows approaches, that aims to jointly solvetwo sub-tasks: (i) rapidly localizing salient objects from images, and (ii)accurately segmenting the objects based on the localizations. We present ageneral joint task learning framework, in which each task (either objectlocalization or object segmentation) is tackled via a multi-layer convolutionalneural network, and the two networks work collaboratively to boost performance.In particular, we propose to incorporate latent variables bridging the twonetworks in a joint optimization manner. The first network directly predictsthe positions and scales of salient objects from raw images, and the latentvariables adjust the object localizations to feed the second network thatproduces pixelwise object masks. An EM-type method is presented for theoptimization, iterating with two steps: (i) by using the two networks, itestimates the latent variables by employing an MCMC-based sampling method; (ii)it optimizes the parameters of the two networks unitedly via back propagation,with the fixed latent variables. Extensive experiments suggest that ourframework significantly outperforms other state-of-the-art approaches in bothaccuracy and efficiency (e.g. 1000 times faster than competing approaches).
arxiv-9000-183 | Dynamical And-Or Graph Learning for Object Shape Modeling and Detection | http://arxiv.org/pdf/1502.00741v1.pdf | author:Xiaolong Wang, Liang Lin category:cs.CV 68U01 published:2015-02-03 summary:This paper studies a novel discriminative part-based model to represent andrecognize object shapes with an "And-Or graph". We define this model consistingof three layers: the leaf-nodes with collaborative edges for localizing localparts, the or-nodes specifying the switch of leaf-nodes, and the root-nodeencoding the global verification. A discriminative learning algorithm, extendedfrom the CCCP [23], is proposed to train the model in a dynamical manner: themodel structure (e.g., the configuration of the leaf-nodes associated with theor-nodes) is automatically determined with optimizing the multi-layerparameters during the iteration. The advantages of our method are two-fold. (i)The And-Or graph model enables us to handle well large intra-class variance andbackground clutters for object shape detection from images. (ii) The proposedlearning algorithm is able to obtain the And-Or graph representation withoutrequiring elaborate supervision and initialization. We validate the proposedmethod on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, andUIUC-People), and it outperforms the state-of-the-arts approaches.
arxiv-9000-184 | Clothing Co-Parsing by Joint Image Segmentation and Labeling | http://arxiv.org/pdf/1502.00739v1.pdf | author:Wei Yang, Ping Luo, Liang Lin category:cs.CV 68U01 published:2015-02-03 summary:This paper aims at developing an integrated system of clothing co-parsing, inorder to jointly parse a set of clothing images (unsegmented but annotated withtags) into semantic configurations. We propose a data-driven frameworkconsisting of two phases of inference. The first phase, referred as "imageco-segmentation", iterates to extract consistent regions on images and jointlyrefines the regions over all images by employing the exemplar-SVM (E-SVM)technique [23]. In the second phase (i.e. "region co-labeling"), we construct amulti-image graphical model by taking the segmented regions as vertices, andincorporate several contexts of clothing configuration (e.g., item location andmutual interactions). The joint label assignment can be solved using theefficient Graph Cuts algorithm. In addition to evaluate our framework on theFashionista dataset [30], we construct a dataset called CCP consisting of 2098high-resolution street fashion photos to demonstrate the performance of oursystem. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89%recognition rate on the Fashionista and the CCP datasets, respectively, whichare superior compared with state-of-the-art methods.
arxiv-9000-185 | Cheaper and Better: Selecting Good Workers for Crowdsourcing | http://arxiv.org/pdf/1502.00725v1.pdf | author:Hongwei Li, Qiang Liu category:stat.ML cs.AI cs.LG stat.AP published:2015-02-03 summary:Crowdsourcing provides a popular paradigm for data collection at scale. Westudy the problem of selecting subsets of workers from a given worker pool tomaximize the accuracy under a budget constraint. One natural question iswhether we should hire as many workers as the budget allows, or restrict on asmall number of top-quality workers. By theoretically analyzing the error rateof a typical setting in crowdsourcing, we frame the worker selection probleminto a combinatorial optimization problem and propose an algorithm to solve itefficiently. Empirical results on both simulated and real-world datasets showthat our algorithm is able to select a small number of high-quality workers,and performs as good as, sometimes even better than, the much larger crowds asthe budget allows.
arxiv-9000-186 | Learning Contour-Fragment-based Shape Model with And-Or Tree Representation | http://arxiv.org/pdf/1502.00723v1.pdf | author:Liang Lin, Xiaolong Wang, Wei Yang, Jianhuang Lai category:cs.CV 68U01 published:2015-02-03 summary:This paper proposes a simple yet effective method to learn the hierarchicalobject shape model consisting of local contour fragments, which represents acategory of shapes in the form of an And-Or tree. This model extends thetraditional hierarchical tree structures by introducing the "switch" variables(i.e. the or-nodes) that explicitly specify production rules to capture shapevariations. We thus define the model with three layers: the leaf-nodes fordetecting local contour fragments, the or-nodes specifying selection ofleaf-nodes, and the root-node encoding the holistic distortion. In the trainingstage, for optimization of the And-Or tree learning, we extend theconcave-convex procedure (CCCP) by embedding the structural clustering duringthe iterative learning steps. The inference of shape detection is consistentwith the model optimization, which integrates the local testings via theleaf-nodes and or-nodes with the global verification via the root-node. Theadvantages of our approach are validated on the challenging shape databases(i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed methodis able to accurately localize shape contours against unreliable edge detectionand edge tracing. (2) The And-Or tree model enables us to well capture theintraclass variance.
arxiv-9000-187 | Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image Segmentation and Cosegmentation | http://arxiv.org/pdf/1502.00717v1.pdf | author:Hongyuan Zhu, Fanman Meng, Jianfei Cai, Shijian Lu category:cs.CV published:2015-02-03 summary:Image segmentation refers to the process to divide an image intononoverlapping meaningful regions according to human perception, which hasbecome a classic topic since the early ages of computer vision. A lot ofresearch has been conducted and has resulted in many applications. However,while many segmentation algorithms exist, yet there are only a few sparse andoutdated summarizations available, an overview of the recent achievements andissues is lacking. We aim to provide a comprehensive review of the recentprogress in this field. Covering 180 publications, we give an overview of broadareas of segmentation topics including not only the classic bottom-upapproaches, but also the recent development in superpixel, interactive methods,object proposals, semantic image parsing and image cosegmentation. In addition,we also review the existing influential datasets and evaluation metrics.Finally, we suggest some design flavors and research directions for futureresearch in image segmentation.
arxiv-9000-188 | Deep Boosting: Layered Feature Mining for General Image Classification | http://arxiv.org/pdf/1502.00712v1.pdf | author:Zhanglin Peng, Liang Lin, Ruimao Zhang, Jing Xu category:cs.CV 68U01 published:2015-02-03 summary:Constructing effective representations is a critical but challenging problemin multimedia understanding. The traditional handcraft features often rely ondomain knowledge, limiting the performances of exiting methods. This paperdiscusses a novel computational architecture for general image feature mining,which assembles the primitive filters (i.e. Gabor wavelets) into compositionalfeatures in a layer-wise manner. In each layer, we produce a number of baseclassifiers (i.e. regression stumps) associated with the generated features,and discover informative compositions by using the boosting algorithm. Theoutput compositional features of each layer are treated as the base componentsto build up the next layer. Our framework is able to generate expressive imagerepresentations while inducing very discriminate functions for imageclassification. The experiments are conducted on several public datasets, andwe demonstrate superior performances over state-of-the-art approaches.
arxiv-9000-189 | Recovery of Piecewise Smooth Images from Few Fourier Samples | http://arxiv.org/pdf/1502.00705v1.pdf | author:Greg Ongie, Mathews Jacob category:cs.CV published:2015-02-03 summary:We introduce a Prony-like method to recover a continuous domain 2-D piecewisesmooth image from few of its Fourier samples. Assuming the discontinuity set ofthe image is localized to the zero level-set of a trigonometric polynomial, weshow the Fourier transform coefficients of partial derivatives of the signalsatisfy an annihilation relation. We present necessary and sufficientconditions for unique recovery of piecewise constant images using the aboveannihilation relation. We pose the recovery of the Fourier coefficients of thesignal from the measurements as a convex matrix completion algorithm, whichrelies on the lifting of the Fourier data to a structured low-rank matrix; thisapproach jointly estimates the signal and the annihilating filter. Finally, wedemonstrate our algorithm on the recovery of MRI phantoms from fewlow-resolution Fourier samples.
arxiv-9000-190 | Perturbed Message Passing for Constraint Satisfaction Problems | http://arxiv.org/pdf/1401.6686v3.pdf | author:Siamak Ravanbakhsh, Russell Greiner category:cs.AI cs.CC stat.ML published:2014-01-26 summary:We introduce an efficient message passing scheme for solving ConstraintSatisfaction Problems (CSPs), which uses stochastic perturbation of BeliefPropagation (BP) and Survey Propagation (SP) messages to bypass decimation anddirectly produce a single satisfying assignment. Our first CSP solver, calledPerturbed Blief Propagation, smoothly interpolates two well-known inferenceprocedures; it starts as BP and ends as a Gibbs sampler, which produces asingle sample from the set of solutions. Moreover we apply a similarperturbation scheme to SP to produce another CSP solver, Perturbed SurveyPropagation. Experimental results on random and real-world CSPs show thatPerturbed BP is often more successful and at the same time tens to hundreds oftimes more efficient than standard BP guided decimation. Perturbed BP alsocompares favorably with state-of-the-art SP-guided decimation, which has acomputational complexity that generally scales exponentially worse than ourmethod (wrt the cardinality of variable domains and constraints). Furthermore,our experiments with random satisfiability and coloring problems demonstratethat Perturbed SP can outperform SP-guided decimation, making it the bestincomplete random CSP-solver in difficult regimes.
arxiv-9000-191 | Learning the Matching Function | http://arxiv.org/pdf/1502.00652v1.pdf | author:Ľubor Ladický, Christian Häne, Marc Pollefeys category:cs.CV published:2015-02-02 summary:The matching function for the problem of stereo reconstruction or opticalflow has been traditionally designed as a function of the distance between thefeatures describing matched pixels. This approach works under assumption, thatthe appearance of pixels in two stereo cameras or in two consecutive videoframes does not change dramatically. However, this might not be the case, if wetry to match pixels over a large interval of time. In this paper we propose a method, which learns the matching function, thatautomatically finds the space of allowed changes in visual appearance, such asdue to the motion blur, chromatic distortions, different colour calibration orseasonal changes. Furthermore, it automatically learns the importance ofmatching scores of contextual features at different relative locations andscales. Proposed classifier gives reliable estimations of pixel disparitiesalready without any form of regularization. We evaluated our method on two standard problems - stereo matching on KITTIoutdoor dataset, optical flow on Sintel data set, and on newly introducedTimeLapse change detection dataset. Our algorithm obtained very promisingresults comparable to the state-of-the-art.
arxiv-9000-192 | Minimax rates in permutation estimation for feature matching | http://arxiv.org/pdf/1310.4661v2.pdf | author:Olivier Collier, Arnak S. Dalalyan category:math.ST cs.LG stat.TH published:2013-10-17 summary:The problem of matching two sets of features appears in various tasks ofcomputer vision and can be often formalized as a problem of permutationestimation. We address this problem from a statistical point of view andprovide a theoretical analysis of the accuracy of several natural estimators.To this end, the minimax rate of separation is investigated and its expressionis obtained as a function of the sample size, noise level and dimension. Weconsider the cases of homoscedastic and heteroscedastic noise and establish, ineach case, tight upper bounds on the separation distance of several estimators.These upper bounds are shown to be unimprovable both in the homoscedastic andheteroscedastic settings. Interestingly, these bounds demonstrate that a phasetransition occurs when the dimension $d$ of the features is of the order of thelogarithm of the number of features $n$. For $d=O(\log n)$, the rate isdimension free and equals $\sigma (\log n)^{1/2}$, where $\sigma$ is the noiselevel. In contrast, when $d$ is larger than $c\log n$ for some constant $c>0$,the minimax rate increases with $d$ and is of the order $\sigma(d\logn)^{1/4}$. We also discuss the computational aspects of the estimators andprovide empirical evidence of their consistency on synthetic data. Finally, weshow that our results extend to more general matching criteria.
arxiv-9000-193 | A Class of DCT Approximations Based on the Feig-Winograd Algorithm | http://arxiv.org/pdf/1502.00592v1.pdf | author:C. J. Tablada, F. M. Bayer, R. J. Cintra category:stat.ME cs.CV cs.MM cs.NA stat.AP published:2015-02-02 summary:A new class of matrices based on a parametrization of the Feig-Winogradfactorization of 8-point DCT is proposed. Such parametrization induces a matrixsubspace, which unifies a number of existing methods for DCT approximation. Bysolving a comprehensive multicriteria optimization problem, we identifiedseveral new DCT approximations. Obtained solutions were sought to possess thefollowing properties: (i) low multiplierless computational complexity, (ii)orthogonality or near orthogonality, (iii) low complexity invertibility, and(iv) close proximity and performance to the exact DCT. Proposed approximationswere submitted to assessment in terms of proximity to the DCT, codingperformance, and suitability for image compression. Considering Paretoefficiency, particular new proposed approximations could outperform variousexisting methods archived in literature.
arxiv-9000-194 | Generalized Dantzig Selector: Application to the k-support norm | http://arxiv.org/pdf/1406.5291v3.pdf | author:Soumyadeep Chatterjee, Sheng Chen, Arindam Banerjee category:stat.ML cs.LG published:2014-06-20 summary:We propose a Generalized Dantzig Selector (GDS) for linear models, in whichany norm encoding the parameter structure can be leveraged for estimation. Weinvestigate both computational and statistical aspects of the GDS. Based onconjugate proximal operator, a flexible inexact ADMM framework is designed forsolving GDS, and non-asymptotic high-probability bounds are established on theestimation error, which rely on Gaussian width of unit norm ball and suitableset encompassing estimation error. Further, we consider a non-trivial exampleof the GDS using $k$-support norm. We derive an efficient method to compute theproximal operator for $k$-support norm since existing methods are inapplicablein this setting. For statistical analysis, we provide upper bounds for theGaussian widths needed in the GDS analysis, yielding the first statisticalrecovery guarantee for estimation with the $k$-support norm. The experimentalresults confirm our theoretical analysis.
arxiv-9000-195 | Cascading Randomized Weighted Majority: A New Online Ensemble Learning Algorithm | http://arxiv.org/pdf/1403.0388v4.pdf | author:Mohammadzaman Zamani, Hamid Beigy, Amirreza Shaban category:stat.ML cs.LG published:2014-03-03 summary:With the increasing volume of data in the world, the best approach forlearning from this data is to exploit an online learning algorithm. Onlineensemble methods are online algorithms which take advantage of an ensemble ofclassifiers to predict labels of data. Prediction with expert advice is awell-studied problem in the online ensemble learning literature. The WeightedMajority algorithm and the randomized weighted majority (RWM) are the mostwell-known solutions to this problem, aiming to converge to the best expert.Since among some expert, the best one does not necessarily have the minimumerror in all regions of data space, defining specific regions and converging tothe best expert in each of these regions will lead to a better result. In thispaper, we aim to resolve this defect of RWM algorithms by proposing a novelonline ensemble algorithm to the problem of prediction with expert advice. Wepropose a cascading version of RWM to achieve not only better experimentalresults but also a better error bound for sufficiently large datasets.
arxiv-9000-196 | Compressed Support Vector Machines | http://arxiv.org/pdf/1501.06478v2.pdf | author:Zhixiang Xu, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger category:cs.LG published:2015-01-26 summary:Support vector machines (SVM) can classify data sets along highly non-lineardecision boundaries because of the kernel-trick. This expressiveness comes at aprice: During test-time, the SVM classifier needs to compute the kernelinner-product between a test sample and all support vectors. With largetraining data sets, the time required for this computation can be substantial.In this paper, we introduce a post-processing algorithm, which compresses thelearned SVM model by reducing and optimizing support vectors. We evaluate ouralgorithm on several medium-scaled real-world data sets, demonstrating that itmaintains high test accuracy while reducing the test-time evaluation cost byseveral orders of magnitude---in some cases from hours to seconds. It is fairto say that most of the work in this paper was previously been invented byBurges and Sch\"olkopf almost 20 years ago. For most of the time during whichwe conducted this research, we were unaware of this prior work. However, in thepast two decades, computing power has increased drastically, and we cantherefore provide empirical insights that were not possible in their originalpaper.
arxiv-9000-197 | Scaling Recurrent Neural Network Language Models | http://arxiv.org/pdf/1502.00512v1.pdf | author:Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson category:cs.CL cs.LG published:2015-02-02 summary:This paper investigates the scaling properties of Recurrent Neural NetworkLanguage Models (RNNLMs). We discuss how to train very large RNNs on GPUs andaddress the questions of how RNNLMs scale with respect to model size,training-set size, computational costs and memory. Our analysis shows thatdespite being more costly to train, RNNLMs obtain much lower perplexities onstandard benchmarks than n-gram models. We train the largest known RNNs andpresent relative word error rates gains of 18% on an ASR task. We also presentthe new lowest perplexities on the recently released billion word languagemodelling benchmark, 1 BLEU point gain on machine translation and a 17%relative hit rate gain in word prediction.
arxiv-9000-198 | An Expressive Deep Model for Human Action Parsing from A Single Image | http://arxiv.org/pdf/1502.00501v1.pdf | author:Zhujin Liang, Xiaolong Wang, Rui Huang, Liang Lin category:cs.CV 68U01 published:2015-02-02 summary:This paper aims at one newly raising task in vision and multimedia research:recognizing human actions from still images. Its main challenges lie in thelarge variations in human poses and appearances, as well as the lack oftemporal motion information. Addressing these problems, we propose to developan expressive deep model to naturally integrate human layout and surroundingcontexts for higher level action understanding from still images. Inparticular, a Deep Belief Net is trained to fuse information from differentnoisy sources such as body part detection and object detection. To bridge thesemantic gap, we used manually labeled data to greatly improve theeffectiveness and efficiency of the pre-training and fine-tuning stages of theDBN training. The resulting framework is shown to be robust to sometimesunreliable inputs (e.g., imprecise detections of human parts and objects), andoutperforms the state-of-the-art approaches.
arxiv-9000-199 | Fast and Robust Feature Matching for RGB-D Based Localization | http://arxiv.org/pdf/1502.00500v1.pdf | author:Miguel Heredia, Felix Endres, Wolfram Burgard, Rafael Sanz category:cs.CV cs.RO published:2015-02-02 summary:In this paper we present a novel approach to global localization using anRGB-D camera in maps of visual features. For large maps, the performance ofpure image matching techniques decays in terms of robustness and computationalcost. Particularly, repeated occurrences of similar features due to repeatingstructure in the world (e.g., doorways, chairs, etc.) or missing associationsbetween observations pose critical challenges to visual localization. Weaddress these challenges using a two-step approach. We first estimate acandidate pose using few correspondences between features of the current cameraframe and the feature map. The initial set of correspondences is established byproximity in feature space. The initial pose estimate is used in the secondstep to guide spatial matching of features in 3D, i.e., searching forassociations where the image features are expected to be found in the map. ARANSAC algorithm is used to compute a fine estimation of the pose from thecorrespondences. Our approach clearly outperforms localization based on featurematching exclusively in feature space, both in terms of estimation accuracy androbustness to failure and allows for global localization in real time (30Hz).
arxiv-9000-200 | From neural PCA to deep unsupervised learning | http://arxiv.org/pdf/1411.7783v2.pdf | author:Harri Valpola category:stat.ML cs.LG cs.NE published:2014-11-28 summary:A network supporting deep unsupervised learning is presented. The network isan autoencoder with lateral shortcut connections from the encoder to decoder ateach level of the hierarchy. The lateral shortcut connections allow the higherlevels of the hierarchy to focus on abstract invariant features. While standardautoencoders are analogous to latent variable models with a single layer ofstochastic variables, the proposed network is analogous to hierarchical latentvariables models. Learning combines denoising autoencoder and denoising sourcesseparation frameworks. Each layer of the network contributes to the costfunction a term which measures the distance of the representations produced bythe encoder and the decoder. Since training signals originate from all levelsof the network, all layers can learn efficiently even in deep networks. Thespeedup offered by cost terms from higher levels of the hierarchy and theability to learn invariant features are demonstrated in experiments.
arxiv-9000-201 | A scaled gradient projection method for Bayesian learning in dynamical systems | http://arxiv.org/pdf/1406.6603v3.pdf | author:Silvia Bonettini, Alessandro Chiuso, Marco Prato category:math.NA cs.LG stat.ML published:2014-06-25 summary:A crucial task in system identification problems is the selection of the mostappropriate model class, and is classically addressed resorting tocross-validation or using asymptotic arguments. As recently suggested in theliterature, this can be addressed in a Bayesian framework, where modelcomplexity is regulated by few hyperparameters, which can be estimated viamarginal likelihood maximization. It is thus of primary importance to designeffective optimization methods to solve the corresponding optimization problem.If the unknown impulse response is modeled as a Gaussian process with asuitable kernel, the maximization of the marginal likelihood leads to achallenging nonconvex optimization problem, which requires a stable andeffective solution strategy. In this paper we address this problem by means ofa scaled gradient projection algorithm, in which the scaling matrix and thesteplength parameter play a crucial role to provide a meaning solution in acomputational time comparable with second order methods. In particular, wepropose both a generalization of the split gradient approach to design thescaling matrix in the presence of box constraints, and an effectiveimplementation of the gradient and objective function. The extensive numericalexperiments carried out on several test problems show that our method is veryeffective in providing in few tenths of a second solutions of the problems withaccuracy comparable with state-of-the-art approaches. Moreover, the flexibilityof the proposed strategy makes it easily adaptable to a wider range of problemsarising in different areas of machine learning, signal processing and systemidentification.
arxiv-9000-202 | Towards a solid solution of real-time fire and flame detection | http://arxiv.org/pdf/1502.00416v1.pdf | author:Bo Jiang, Yongyi Lu, Xiying Li, Liang Lin category:cs.CV 68U01 published:2015-02-02 summary:Although the object detection and recognition has received growing attentionfor decades, a robust fire and flame detection method is rarely explored. Thispaper presents an empirical study, towards a general and solid approach to fastdetect fire and flame in videos, with the applications in video surveillanceand event retrieval. Our system consists of three cascaded steps: (1) candidateregions proposing by a background model, (2) fire region classifying withcolor-texture features and a dictionary of visual words, and (3) temporalverifying. The experimental evaluation and analysis are done for each step. Webelieve that it is a useful service to both academic research and real-worldapplication. In addition, we release the software of the proposed system withthe source code, as well as a public benchmark and data set, including 64 videoclips covered both indoor and outdoor scenes under different conditions. Weachieve an 82% Recall with 93% Precision on the data set, and greatly improvethe performance by state-of-the-arts methods.
arxiv-9000-203 | Info-Greedy sequential adaptive compressed sensing | http://arxiv.org/pdf/1407.0731v4.pdf | author:Gabor Braun, Sebastian Pokutta, Yao Xie category:cs.IT math.IT math.ST stat.ML stat.TH published:2014-07-02 summary:We present an information-theoretic framework for sequential adaptivecompressed sensing, Info-Greedy Sensing, where measurements are chosen tomaximize the extracted information conditioned on the previous measurements. Weshow that the widely used bisection approach is Info-Greedy for a family of$k$-sparse signals by connecting compressed sensing and blackbox complexity ofsequential query algorithms, and present Info-Greedy algorithms for Gaussianand Gaussian Mixture Model (GMM) signals, as well as ways to design sparseInfo-Greedy measurements. Numerical examples demonstrate the good performanceof the proposed algorithms using simulated and real data: Info-Greedy Sensingshows significant improvement over random projection for signals with sparseand low-rank covariance matrices, and adaptivity brings robustness when thereis a mismatch between the assumed and the true distributions.
arxiv-9000-204 | Integrating Graph Partitioning and Matching for Trajectory Analysis in Video Surveillance | http://arxiv.org/pdf/1502.00377v1.pdf | author:Liang Lin, Yongyi Lu, Yan Pan, Xiaowu Chen category:cs.CV 68U01 published:2015-02-02 summary:In order to track the moving objects in long range against occlusion,interruption, and background clutter, this paper proposes a unified approachfor global trajectory analysis. Instead of the traditional frame-by-frametracking, our method recovers target trajectories based on a short sequence ofvideo frames, e.g. $15$ frames. We initially calculate a foreground map at eachframe, as obtained from a state-of-the-art background model. An attribute graphis then extracted from the foreground map, where the graph vertices are imageprimitives represented by the composite features. With this graphrepresentation, we pose trajectory analysis as a joint task of spatial graphpartitioning and temporal graph matching. The task can be formulated bymaximizing a posteriori under the Bayesian framework, in which we integrate thespatio-temporal contexts and the appearance models. The probabilistic inferenceis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given aperoid of observed frames, the algorithm simulates a ergodic and aperiodicMarkov Chain, and it visits a sequence of solution states in the joint space ofspatial graph partitioning and temporal graph matching. In the experiments, ourmethod is tested on several challenging videos from the public datasets ofvisual surveillance, and it outperforms the state-of-the-art methods.
arxiv-9000-205 | Adaptive Scene Category Discovery with Generative Learning and Compositional Sampling | http://arxiv.org/pdf/1502.00374v1.pdf | author:Liang Lin, Ruimao Zhang, Xiaohua Duan category:cs.CV 68U01 published:2015-02-02 summary:This paper investigates a general framework to discover categories ofunlabeled scene images according to their appearances (i.e., textures andstructures). We jointly solve the two coupled tasks in an unsupervised manner:(i) classifying images without pre-determining the number of categories, and(ii) pursuing generative model for each category. In our method, each image isrepresented by two types of image descriptors that are effective to captureimage appearances from different aspects. By treating each image as a graphvertex, we build up an graph, and pose the image categorization as a graphpartition process. Specifically, a partitioned sub-graph can be regarded as acategory of scenes, and we define the probabilistic model of graph partition byaccumulating the generative models of all separated categories. For efficientinference with the graph, we employ a stochastic cluster sampling algorithm,which is designed based on the Metropolis-Hasting mechanism. During theiterations of inference, the model of each category is analytically updated bya generative learning algorithm. In the experiments, our approach is validatedon several challenging databases, and it outperforms other popularstate-of-the-art methods. The implementation details and empirical analysis arepresented as well.
arxiv-9000-206 | Iterated Support Vector Machines for Distance Metric Learning | http://arxiv.org/pdf/1502.00363v1.pdf | author:Wangmeng Zuo, Faqiang Wang, David Zhang, Liang Lin, Yuchi Huang, Deyu Meng, Lei Zhang category:cs.LG cs.CV published:2015-02-02 summary:Distance metric learning aims to learn from the given training data a validdistance metric, with which the similarity between data samples can be moreeffectively evaluated for classification. Metric learning is often formulatedas a convex or nonconvex optimization problem, while many existing metriclearning algorithms become inefficient for large scale problems. In this paper,we formulate metric learning as a kernel classification problem, and solve itby iterated training of support vector machines (SVM). The new formulation iseasy to implement, efficient in training, and tractable for large-scaleproblems. Two novel metric learning models, namely Positive-semidefiniteConstrained Metric Learning (PCML) and Nonnegative-coefficient ConstrainedMetric Learning (NCML), are developed. Both PCML and NCML can guarantee theglobal optimality of their solutions. Experimental results on UCI datasetclassification, handwritten digit recognition, face verification and personre-identification demonstrate that the proposed metric learning methods achievehigher classification accuracy than state-of-the-art methods and they aresignificantly more efficient in training.
arxiv-9000-207 | A Web-based Interactive Visual Graph Analytics Platform | http://arxiv.org/pdf/1502.00354v1.pdf | author:Nesreen K. Ahmed, Ryan A. Rossi category:cs.SI cs.HC stat.ML published:2015-02-02 summary:This paper proposes a web-based visual graph analytics platform forinteractive graph mining, visualization, and real-time exploration of networks.GraphVis is fast, intuitive, and flexible, combining interactive visualizationswith analytic techniques to reveal important patterns and insights for sensemaking, reasoning, and decision making. Networks can be visualized and exploredwithin seconds by simply drag-and-dropping a graph file into the web browser.The structure, properties, and patterns of the network are computedautomatically and can be instantly explored in real-time. At the heart ofGraphVis lies a multi-level interactive network visualization and analyticsengine that allows for real-time graph mining and exploration across multiplelevels of granularity simultaneously. Both the graph analytic and visualizationtechniques (at each level of granularity) are dynamic and interactive, withimmediate and continuous visual feedback upon every user interaction (e.g.,change of a slider for filtering). Furthermore, nodes, edges, and subgraphs areeasily inserted, deleted or exported via a number of novel techniques and toolsthat make it extremely easy and flexible for exploring, testing hypothesis, andunderstanding networks in real-time over the web. A number of interactivevisual graph analytic techniques are also proposed including interactive rolediscovery methods, community detection, as well as a number of novel blockmodels for generating graphs with community structure. Finally, we alsohighlight other key aspects including filtering, querying, ranking,manipulating, exporting, partitioning, as well as tools for dynamic networkanalysis and visualization, interactive graph generators, and a variety ofmulti-level network analysis, summarization, and statistical techniques.
arxiv-9000-208 | Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal Models | http://arxiv.org/pdf/1502.00344v1.pdf | author:Liang Lin, Yuanlu Xu, Xiaodan Liang, Jianhuang Lai category:cs.CV 68U01 published:2015-02-02 summary:Although it has been widely discussed in video surveillance, backgroundsubtraction is still an open problem in the context of complex scenarios, e.g.,dynamic backgrounds, illumination variations, and indistinct foregroundobjects. To address these challenges, we propose an effective backgroundsubtraction method by learning and maintaining an array of dynamic texturemodels within the spatio-temporal representations. At any location of thescene, we extract a sequence of regular video bricks, i.e. video volumesspanning over both spatial and temporal domain. The background modeling is thusposed as pursuing subspaces within the video bricks while adapting the scenevariations. For each sequence of video bricks, we pursue the subspace byemploying the ARMA (Auto Regressive Moving Average) Model that jointlycharacterizes the appearance consistency and temporal coherence of theobservations. During online processing, we incrementally update the subspacesto cope with disturbances from foreground objects and scene changes. In theexperiments, we validate the proposed method in several complex scenarios, andshow superior performances over other state-of-the-art approaches of backgroundsubtraction. The empirical studies of parameter setting and component analysisare presented as well.
arxiv-9000-209 | Discriminatively Trained And-Or Graph Models for Object Shape Detection | http://arxiv.org/pdf/1502.00341v1.pdf | author:Liang Lin, Xiaolong Wang, Wei Yang, Jian-Huang Lai category:cs.CV 68U01 published:2015-02-02 summary:In this paper, we investigate a novel reconfigurable part-based model, namelyAnd-Or graph model, to recognize object shapes in images. Our proposed modelconsists of four layers: leaf-nodes at the bottom are local classifiers fordetecting contour fragments; or-nodes above the leaf-nodes function as theswitches to activate their child leaf-nodes, making the model reconfigurableduring inference; and-nodes in a higher layer capture holistic shapedeformations; one root-node on the top, which is also an or-node, activates oneof its child and-nodes to deal with large global variations (e.g. differentposes and views). We propose a novel structural optimization algorithm todiscriminatively train the And-Or model from weakly annotated data. Thisalgorithm iteratively determines the model structures (e.g. the nodes and theirlayouts) along with the parameter learning. On several challenging datasets,our model demonstrates the effectiveness to perform robust shape-based objectdetection against background clutter and outperforms the other state-of-the-artapproaches. We also release a new shape database with annotations, whichincludes more than 1500 challenging shape instances, for recognition anddetection.
arxiv-9000-210 | Modified Fast Fractal Image Compression Algorithm in spatial domain | http://arxiv.org/pdf/1502.00324v1.pdf | author:M. Salarian, H. Miar Naimi category:cs.CV published:2015-02-01 summary:In this paper a new fractal image compression algorithm is proposed in whichthe time of encoding process is considerably reduced. The algorithm exploits adomain pool reduction approach, along with using innovative predefined valuesfor contrast scaling factor, S, instead of searching it across [0,1]. Only thedomain blocks with entropy greater than a threshold are considered as domainpool. As a novel point, it is assumed that in each step of the encodingprocess, the domain block with small enough distance shall be found only forthe range blocks with low activity (equivalently low entropy). This novel pointis used to find reasonable estimations of S, and use them in the encodingprocess as predefined values, mentioned above, the remaining range blocks aresplit into four new smaller range blocks and the algorithm must be iterated forthem, considered as the other step of encoding process. The algorithm has beenexamined for some of the well-known images and the results have been comparedwith the state-of-the-art algorithms. The experiments show that our proposedalgorithm has considerably lower encoding time than the other where the encodedimages are approximately the same in quality.
arxiv-9000-211 | Dynamic texture and scene classification by transferring deep image features | http://arxiv.org/pdf/1502.00303v1.pdf | author:Xianbiao Qi, Chun-Guang Li, Guoying Zhao, Xiaopeng Hong, Matti Pietikäinen category:cs.CV published:2015-02-01 summary:Dynamic texture and scene classification are two fundamental problems inunderstanding natural video content. Extracting robust and effective featuresis a crucial step towards solving these problems. However the existingapproaches suffer from the sensitivity to either varying illumination, orviewpoint changing, or even camera motion, and/or the lack of spatialinformation. Inspired by the success of deep structures in imageclassification, we attempt to leverage a deep structure to extract feature fordynamic texture and scene classification. To tackle with the challenges intraining a deep structure, we propose to transfer some prior knowledge fromimage domain to video domain. To be specific, we propose to apply awell-trained Convolutional Neural Network (ConvNet) as a mid-level featureextractor to extract features from each frame, and then form a representationof a video by concatenating the first and the second order statistics over themid-level features. We term this two-level feature extraction scheme as aTransferred ConvNet Feature (TCoF). Moreover we explore two differentimplementations of the TCoF scheme, i.e., the \textit{spatial} TCoF and the\textit{temporal} TCoF, in which the mean-removed frames and the differencebetween two adjacent frames are used as the inputs of the ConvNet,respectively. We evaluate systematically the proposed spatial TCoF and thetemporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN,and Maryland, and demonstrate that the proposed approach yields superiorperformance.
arxiv-9000-212 | 3D Human Activity Recognition with Reconfigurable Convolutional Neural Networks | http://arxiv.org/pdf/1501.06262v3.pdf | author:Keze Wang, Xiaolong Wang, Liang Lin, Meng Wang, Wangmeng Zuo category:cs.CV 68U01 I.4 published:2015-01-26 summary:Human activity understanding with 3D/depth sensors has received increasingattention in multimedia processing and interactions. This work targets ondeveloping a novel deep model for automatic activity recognition from RGB-Dvideos. We represent each human activity as an ensemble of cubic-like videosegments, and learn to discover the temporal structures for a category ofactivities, i.e. how the activities to be decomposed in terms ofclassification. Our model can be regarded as a structured deep architecture, asit extends the convolutional neural networks (CNNs) by incorporating structurealternatives. Specifically, we build the network consisting of 3D convolutionsand max-pooling operators over the video segments, and introduce the latentvariables in each convolutional layer manipulating the activation of neurons.Our model thus advances existing approaches in two aspects: (i) it actsdirectly on the raw inputs (grayscale-depth data) to conduct recognitioninstead of relying on hand-crafted features, and (ii) the model structure canbe dynamically adjusted accounting for the temporal variations of humanactivities, i.e. the network configuration is allowed to be partially activatedduring inference. For model training, we propose an EM-type optimization methodthat iteratively (i) discovers the latent structure by determining thedecomposed actions for each training example, and (ii) learns the networkparameters by using the back-propagation algorithm. Our approach is validatedin challenging scenarios, and outperforms state-of-the-art methods. A largehuman activity database of RGB-D videos is presented in addition.
arxiv-9000-213 | Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition | http://arxiv.org/pdf/1502.00258v1.pdf | author:Xiaodan Liang, Liang Lin, Liangliang Cao category:cs.CV 68U01 I.5; I.4 published:2015-02-01 summary:Action recognition is an important problem in multimedia understanding. Thispaper addresses this problem by building an expressive compositional actionmodel. We model one action instance in the video with an ensemble ofspatio-temporal compositions: a number of discrete temporal anchor frames, eachof which is further decomposed to a layout of deformable parts. In this way,our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent thelatent structure of actions e.g. triple jumping, swinging and high jumping. TheSTAOG model comprises four layers: (i) a batch of leaf-nodes in bottom fordetecting various action parts within video patches; (ii) the or-nodes overbottom, i.e. switch variables to activate their children leaf-nodes forstructural variability; (iii) the and-nodes within an anchor frame forverifying spatial composition; and (iv) the root-node at top for aggregatingscores over temporal anchor frames. Moreover, the contextual interactions aredefined between leaf-nodes in both spatial and temporal domains. For modeltraining, we develop a novel weakly supervised learning algorithm whichiteratively determines the structural configuration (e.g. the production ofleaf-nodes associated with the or-nodes) along with the optimization ofmulti-layer parameters. By fully exploiting spatio-temporal compositions andinteractions, our approach handles well large intra-class action variance (e.g.different views, individual appearances, spatio-temporal structures). Theexperimental results on the challenging databases demonstrate superiorperformance of our approach over other competing methods.
arxiv-9000-214 | Human Re-identification by Matching Compositional Template with Cluster Sampling | http://arxiv.org/pdf/1502.00256v1.pdf | author:Yuanlu Xu, Liang Lin, Wei-Shi Zheng, Xiaobai Liu category:cs.CV 68U01 published:2015-02-01 summary:This paper aims at a newly raising task in visual surveillance:re-identifying people at a distance by matching body information, given severalreference examples. Most of existing works solve this task by matching areference template with the target individual, but often suffer from largehuman appearance variability (e.g. different poses/views, illumination) andhigh false positives in matching caused by conjunctions, occlusions orsurrounding clutters. Addressing these problems, we construct a simple yetexpressive template from a few reference images of a certain individual, whichrepresents the body as an articulated assembly of compositional and alternativeparts, and propose an effective matching algorithm with cluster sampling. Thisalgorithm is designed within a candidacy graph whose vertices are matchingcandidates (i.e. a pair of source and target body parts), and iterates in twosteps for convergence. (i) It generates possible partial matches based oncompatible and competitive relations among body parts. (ii) It confirms thepartial matches to generate a new matching solution, which is accepted by theMarkov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstratethe superior performance of our approach on three public databases compared toexisting methods.
arxiv-9000-215 | Driver distraction detection and recognition using RGB-D sensor | http://arxiv.org/pdf/1502.00250v1.pdf | author:Céline Craye, Fakhri Karray category:cs.CV published:2015-02-01 summary:Driver inattention assessment has become a very active field in intelligenttransportation systems. Based on active sensor Kinect and computer visiontools, we have built an efficient module for detecting driver distraction andrecognizing the type of distraction. Based on color and depth map data from theKinect, our system is composed of four sub-modules. We call them eye behavior(detecting gaze and blinking), arm position (is the right arm up, down, rightof forward), head orientation, and facial expressions. Each module producesrelevant information for assessing driver inattention. They are merged togetherlater on using two different classification strategies: AdaBoost classifier andHidden Markov Model. Evaluation is done using a driving simulator and 8 driversof different gender, age and nationality for a total of more than 8 hours ofrecording. Qualitative and quantitative results show strong and accuratedetection and recognition capacity (85% accuracy for the type of distractionand 90% for distraction detection). Moreover, each module is obtainedindependently and could be used for other types of inference, such as fatiguedetection, and could be implemented for real cars systems.
arxiv-9000-216 | Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil | http://arxiv.org/pdf/1502.00245v1.pdf | author:Christian S. Perone category:cs.LG cs.AI published:2015-02-01 summary:This study describes the experimental application of Machine Learningtechniques to build prediction models that can assess the injury riskassociated with traffic accidents. This work uses an freely available data setof traffic accident records that took place in the city of Porto Alegre/RS(Brazil) during the year of 2013. This study also provides an analysis of themost important attributes of a traffic accident that could produce an outcomeof injury to the people involved in the accident.
arxiv-9000-217 | Feature selection for classification with class-separability strategy and data envelopment analysis | http://arxiv.org/pdf/1405.1119v2.pdf | author:Yishi Zhang, Chao Yang, Anrong Yang, Chan Xiong, Xingchi Zhou, Zigang Zhang category:cs.LG cs.IT math.IT stat.ML published:2014-05-06 summary:In this paper, a novel feature selection method is presented, which is basedon Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). Tobetter capture the relationship between features and the class, class labelsare separated into individual variables and relevance and redundancy areexplicitly handled on each class label. Super-efficiency DEA is employed toevaluate and rank features via their conditional dependence scores on all classlabels, and the feature with maximum super-efficiency score is then added inthe conditioning set for conditional dependence estimation in the nextiteration, in such a way as to iteratively select features and get the finalselected features. Eventually, experiments are conducted to evaluate theeffectiveness of proposed method comparing with four state-of-the-art methodsfrom the viewpoint of classification accuracy. Empirical results verify thefeasibility and the superiority of proposed feature selection method.
arxiv-9000-218 | Feature Selection with Redundancy-complementariness Dispersion | http://arxiv.org/pdf/1502.00231v1.pdf | author:Zhijun Chen, Chaozhong Wu, Yishi Zhang, Zhen Huang, Bin Ran, Ming Zhong, Nengchao Lyu category:cs.LG stat.ML I.5.2; H.1.1 published:2015-02-01 summary:Feature selection has attracted significant attention in data mining andmachine learning in the past decades. Many existing feature selection methodseliminate redundancy by measuring pairwise inter-correlation of features,whereas the complementariness of features and higher inter-correlation amongmore than two features are ignored. In this study, a modification itemconcerning the complementariness of features is introduced in the evaluationcriterion of features. Additionally, in order to identify the interferenceeffect of already-selected False Positives (FPs), theredundancy-complementariness dispersion is also taken into account to adjustthe measurement of pairwise inter-correlation of features. To illustrate theeffectiveness of proposed method, classification experiments are applied withfour frequently used classifiers on ten datasets. Classification results verifythe superiority of proposed method compared with five representative featureselection methods.
arxiv-9000-219 | Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning | http://arxiv.org/pdf/1402.4419v3.pdf | author:Julien Mairal category:math.OC cs.LG stat.ML published:2014-02-18 summary:Majorization-minimization algorithms consist of successively minimizing asequence of upper bounds of the objective function. These upper bounds aretight at the current estimate, and each iteration monotonically drives theobjective function downhill. Such a simple principle is widely applicable andhas been very popular in various scientific fields, especially in signalprocessing and statistics. In this paper, we propose an incrementalmajorization-minimization scheme for minimizing a large sum of continuousfunctions, a problem of utmost importance in machine learning. We presentconvergence guarantees for non-convex and convex optimization when the upperbounds approximate the objective up to a smooth error; we call such upperbounds "first-order surrogate functions". More precisely, we study asymptoticstationary point guarantees for non-convex problems, and for convex ones, weprovide convergence rates for the expected objective function value. We applyour scheme to composite optimization and obtain a new incremental proximalgradient algorithm with linear convergence rate for strongly convex functions.In our experiments, we show that our method is competitive with the state ofthe art for solving machine learning problems such as logistic regression whenthe number of training samples is large enough, and we demonstrate itsusefulness for sparse estimation with non-convex penalties.
arxiv-9000-220 | Falling Rule Lists | http://arxiv.org/pdf/1411.5899v3.pdf | author:Fulton Wang, Cynthia Rudin category:cs.AI cs.LG published:2014-11-21 summary:Falling rule lists are classification models consisting of an ordered list ofif-then rules, where (i) the order of rules determines which example should beclassified by each rule, and (ii) the estimated probability of successdecreases monotonically down the list. These kinds of rule lists are inspiredby healthcare applications where patients would be stratified into risk setsand the highest at-risk patients should be considered first. We provide aBayesian framework for learning falling rule lists that does not rely ontraditional greedy decision tree learning methods.
arxiv-9000-221 | Chemical Reaction Optimization for the Set Covering Problem | http://arxiv.org/pdf/1502.00199v1.pdf | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-02-01 summary:The set covering problem (SCP) is one of the representative combinatorialoptimization problems, having many practical applications. This paperinvestigates the development of an algorithm to solve SCP by employing chemicalreaction optimization (CRO), a general-purpose metaheuristic. It is tested on awide range of benchmark instances of SCP. The simulation results indicate thatthis algorithm gives outstanding performance compared with other heuristics andmetaheuristics in solving SCP.
arxiv-9000-222 | An Inter-molecular Adaptive Collision Scheme for Chemical Reaction Optimization | http://arxiv.org/pdf/1502.00197v1.pdf | author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE published:2015-02-01 summary:Optimization techniques are frequently applied in science and engineeringresearch and development. Evolutionary algorithms, as a kind of general-purposemetaheuristic, have been shown to be very effective in solving a wide range ofoptimization problems. A recently proposed chemical-reaction-inspiredmetaheuristic, Chemical Reaction Optimization (CRO), has been applied to solvemany global optimization problems. However, the functionality of theinter-molecular ineffective collision operator in the canonical CRO designoverlaps that of the on-wall ineffective collision operator, which canpotential impair the overall performance. In this paper we propose a newinter-molecular ineffective collision operator for CRO for global optimization.To fully utilize our newly proposed operator, we also design a scheme to adaptthe algorithm to optimization problems with different search spacecharacteristics. We analyze the performance of our proposed algorithm with anumber of widely used benchmark functions. The simulation results indicate thatthe new algorithm has superior performance over the canonical CRO.
arxiv-9000-223 | Optimal V2G Scheduling of Electric Vehicles and Unit Commitment using Chemical Reaction Optimization | http://arxiv.org/pdf/1502.00196v1.pdf | author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE published:2015-02-01 summary:An electric vehicle (EV) may be used as energy storage which allows thebi-directional electricity flow between the vehicle's battery and the electricpower grid. In order to flatten the load profile of the electricity system, EVscheduling has become a hot research topic in recent years. In this paper, wepropose a new formulation of the joint scheduling of EV and Unit Commitment(UC), called EVUC. Our formulation considers the characteristics of EVs whileoptimizing the system total running cost. We employ Chemical ReactionOptimization (CRO), a general-purpose optimization algorithm to solve thisproblem and the simulation results on a widely used set of instances indicatethat CRO can effectively optimize this problem.
arxiv-9000-224 | Sensor Deployment for Air Pollution Monitoring Using Public Transportation System | http://arxiv.org/pdf/1502.00195v1.pdf | author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE published:2015-02-01 summary:Air pollution monitoring is a very popular research topic and many monitoringsystems have been developed. In this paper, we formulate the Bus SensorDeployment Problem (BSDP) to select the bus routes on which sensors aredeployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO isa recently proposed metaheuristic designed to solve a wide range ofoptimization problems. Using the real world data, namely Hong Kong Island busroute data, we perform a series of simulations and the results show that CRO iscapable of solving this optimization problem efficiently.
arxiv-9000-225 | Real-Coded Chemical Reaction Optimization with Different Perturbation Functions | http://arxiv.org/pdf/1502.00194v1.pdf | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-02-01 summary:Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimicsthe interactions of molecules in chemical reactions to search for the globaloptimum. The perturbation function greatly influences the performance of CRO onsolving different continuous problems. In this paper, we study four differentprobability distributions, namely, the Gaussian distribution, the Cauchydistribution, the exponential distribution, and a modified Rayleighdistribution, for the perturbation function of CRO. Different distributionshave different impacts on the solutions. The distributions are tested by a setof well-known benchmark functions and simulation results show that problemswith different characteristics have different preference on the distributionfunction. Our study gives guidelines to design CRO for different types ofoptimization problems.
arxiv-9000-226 | Evolutionary Artificial Neural Network Based on Chemical Reaction Optimization | http://arxiv.org/pdf/1502.00193v1.pdf | author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE published:2015-02-01 summary:Evolutionary algorithms (EAs) are very popular tools to design and evolveartificial neural networks (ANNs), especially to train them. These methods haveadvantages over the conventional backpropagation (BP) method because of theirlow computational requirement when searching in a large solution space. In thispaper, we employ Chemical Reaction Optimization (CRO), a newly developed globaloptimization method, to replace BP in training neural networks. CRO is apopulation-based metaheuristics mimicking the transition of molecules and theirinteractions in a chemical reaction. Simulation results show that CROoutperforms many EA strategies commonly used to train neural networks.
arxiv-9000-227 | Pose and Shape Estimation with Discriminatively Learned Parts | http://arxiv.org/pdf/1502.00192v1.pdf | author:Menglong Zhu, Xiaowei Zhou, Kostas Daniilidis category:cs.CV published:2015-02-01 summary:We introduce a new approach for estimating the 3D pose and the 3D shape of anobject from a single image. Given a training set of view exemplars, we learnand select appearance-based discriminative parts which are mapped onto the 3Dmodel from the training set through a facil- ity location optimization. Thetraining set of 3D models is summarized into a sparse set of shapes from whichwe can generalize by linear combination. Given a test picture, we detecthypotheses for each part. The main challenge is to select from these hypothesesand compute the 3D pose and shape coefficients at the same time. To achievethis, we optimize a function that minimizes simultaneously the geometricreprojection error as well as the appearance matching of the parts. We applythe alternating direction method of multipliers (ADMM) to minimize theresulting convex function. We evaluate our approach on the Fine Grained 3D Cardataset with superior performance in shape and pose errors. Our main and novelcontribution is the simultaneous solution for part localization, 3D pose andshape by maximizing both geometric and appearance compatibility.
arxiv-9000-228 | Tracing the boundaries of materials in transparent vessels using computer vision | http://arxiv.org/pdf/1501.04691v3.pdf | author:Sagi Eppel category:cs.CV published:2015-01-20 summary:Visual recognition of material boundaries in transparent vessels is valuablefor numerous applications. Such recognition is essential for estimation offill-level, volume and phase-boundaries as well as for tracking of suchchemical processes as precipitation, crystallization, condensation, evaporationand phase-separation. The problem of material boundary recognition in images isparticularly complex for materials with non-flat surfaces, i.e., solids,powders and viscous fluids, in which the material interfaces have unpredictableshapes. This work demonstrates a general method for finding the boundaries ofmaterials inside transparent containers in images. The method uses an image ofthe transparent vessel containing the material and the boundary of the vesselin this image. The recognition is based on the assumption that the materialboundary appears in the image in the form of a curve (with various constraints)whose endpoints are both positioned on the vessel contour. The probability thata curve matches the material boundary in the image is evaluated using a costfunction based on some image properties along this curve. Several imageproperties were examined as indicators for the material boundary. The optimalboundary curve was found using Dijkstra's algorithm. The method wassuccessfully examined for recognition of various types of phase-boundaries,including liquid-air, solid-air and solid-liquid interfaces, as well as forvarious types of glassware containers from everyday life and the chemistrylaboratory (i.e., bottles, beakers, flasks, jars, columns, vials andseparation-funnels). In addition, the method can be easily extended tomaterials carried on top of carrier vessels (i.e., plates, spoons, spatulas).
arxiv-9000-229 | An evaluation framework for event detection using a morphological model of acoustic scenes | http://arxiv.org/pdf/1502.00141v1.pdf | author:Mathieu Lagrange, Grégoire Lafay, Mathias Rossignol, Emmanouil Benetos, Axel Roebel category:stat.ML cs.SD published:2015-01-31 summary:This paper introduces a model of environmental acoustic scenes which adopts amorphological approach by ab-stracting temporal structures of acoustic scenes.To demonstrate its potential, this model is employed to evaluate theperformance of a large set of acoustic events detection systems. This modelallows us to explicitly control key morphological aspects of the acoustic sceneand isolate their impact on the performance of the system under evaluation.Thus, more information can be gained on the behavior of evaluated systems,providing guidance for further improvements. The proposed model is validatedusing submitted systems from the IEEE DCASE Challenge; results indicate thatthe proposed scheme is able to successfully build datasets useful forevaluating some aspects the performance of event detection systems, moreparticularly their robustness to new listening conditions and the increasinglevel of background sounds.
arxiv-9000-230 | Sparse Dueling Bandits | http://arxiv.org/pdf/1502.00133v1.pdf | author:Kevin Jamieson, Sumeet Katariya, Atul Deshpande, Robert Nowak category:stat.ML cs.LG published:2015-01-31 summary:The dueling bandit problem is a variation of the classical multi-armed banditin which the allowable actions are noisy comparisons between pairs of arms.This paper focuses on a new approach for finding the "best" arm according tothe Borda criterion using noisy comparisons. We prove that in the absence ofstructural assumptions, the sample complexity of this problem is proportionalto the sum of the inverse squared gaps between the Borda scores of eachsuboptimal arm and the best arm. We explore this dependence further andconsider structural constraints on the pairwise comparison matrix (a particularform of sparsity natural to this problem) that can significantly reduce thesample complexity. This motivates a new algorithm called Successive Eliminationwith Comparison Sparsity (SECS) that exploits sparsity to find the Borda winnerusing fewer samples than standard algorithms. We also evaluate the newalgorithm experimentally with synthetic and real data. The results show thatthe sparsity model and the new algorithm can provide significant improvementsover standard approaches.
arxiv-9000-231 | The Search for Computational Intelligence | http://arxiv.org/pdf/1502.00130v1.pdf | author:Joseph Corneli, Ewen Maclean category:cs.NE cs.AI published:2015-01-31 summary:We define and explore in simulation several rules for the local evolution ofgenerative rules for 1D and 2D cellular automata. Our implementation usesstrategies from conceptual blending. We discuss potential applications tomodelling social dynamics.
arxiv-9000-232 | Optimized Projection for Sparse Representation Based Classification | http://arxiv.org/pdf/1502.00115v1.pdf | author:Can-Yi Lu, De-Shuang Huang category:cs.CV published:2015-01-31 summary:Dimensionality reduction (DR) methods have been commonly used as a principledway to understand the high-dimensional data such as facial images. In thispaper, we propose a new supervised DR method called Optimized Projection forSparse Representation based Classification (OP-SRC), which is based on therecent face recognition method, Sparse Representation based Classification(SRC). SRC seeks a sparse linear combination on all the training data for agiven query image, and make the decision by the minimal reconstructionresidual. OP-SRC is designed on the decision rule of SRC, it aims to reduce thewithin-class reconstruction residual and simultaneously increase thebetween-class reconstruction residual on the training data. The projections areoptimized and match well with the mechanism of SRC. Therefore, SRC performswell in the OP-SRC transformed space. The feasibility and effectiveness of theproposed method is verified on the Yale, ORL and UMIST databases with promisingresults.
arxiv-9000-233 | Twitter Hash Tag Recommendation | http://arxiv.org/pdf/1502.00094v1.pdf | author:Roman Dovgopol, Matt Nohelty category:cs.IR cs.LG published:2015-01-31 summary:The rise in popularity of microblogging services like Twitter has led toincreased use of content annotation strategies like the hashtag. Hashtagsprovide users with a tagging mechanism to help organize, group, and createvisibility for their posts. This is a simple idea but can be challenging forthe user in practice which leads to infrequent usage. In this paper, we willinvestigate various methods of recommending hashtags as new posts are createdto encourage more widespread adoption and usage. Hashtag recommendation comeswith numerous challenges including processing huge volumes of streaming dataand content which is small and noisy. We will investigate preprocessing methodsto reduce noise in the data and determine an effective method of hashtagrecommendation based on the popular classification algorithms.
arxiv-9000-234 | Deep learning of fMRI big data: a novel approach to subject-transfer decoding | http://arxiv.org/pdf/1502.00093v1.pdf | author:Sotetsu Koyamada, Yumi Shikauchi, Ken Nakae, Masanori Koyama, Shin Ishii category:stat.ML cs.LG q-bio.NC published:2015-01-31 summary:As a technology to read brain states from measurable brain activities, braindecoding are widely applied in industries and medical sciences. In spite ofhigh demands in these applications for a universal decoder that can be appliedto all individuals simultaneously, large variation in brain activities acrossindividuals has limited the scope of many studies to the development ofindividual-specific decoders. In this study, we used deep neural network (DNN),a nonlinear hierarchical model, to construct a subject-transfer decoder. Ourdecoder is the first successful DNN-based subject-transfer decoder. Whenapplied to a large-scale functional magnetic resonance imaging (fMRI) database,our DNN-based decoder achieved higher decoding accuracy than other baselinemethods, including support vector machine (SVM). In order to analyze theknowledge acquired by this decoder, we applied principal sensitivity analysis(PSA) to the decoder and visualized the discriminative features that are commonto all subjects in the dataset. Our PSA successfully visualized thesubject-independent features contributing to the subject-transferability of thetrained decoder.
arxiv-9000-235 | Category-Epitomes : Discriminatively Minimalist Representations for Object Categories | http://arxiv.org/pdf/1502.00082v1.pdf | author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV published:2015-01-31 summary:Freehand line sketches are an interesting and unique form of visualrepresentation. Typically, such sketches are studied and utilized as an endproduct of the sketching process. However, we have found it instructive tostudy the sketches as sequentially accumulated composition of drawing strokesadded over time. Studying sketches in this manner has enabled us to createnovel sparse yet discriminative sketch-based representations for objectcategories which we term category-epitomes. Our procedure for obtaining theseepitomes concurrently provides a natural measure for quantifying the sparsenessunderlying the original sketch, which we term epitome-score. We construct andanalyze category-epitomes and epitome-scores for freehand sketches belonging tovarious object categories. Our analysis provides a novel viewpoint for studyingthe semantic nature of object categories.
arxiv-9000-236 | A Batchwise Monotone Algorithm for Dictionary Learning | http://arxiv.org/pdf/1502.00064v1.pdf | author:Huan Wang, John Wright, Daniel Spielman category:cs.LG published:2015-01-31 summary:We propose a batchwise monotone algorithm for dictionary learning. Unlike thestate-of-the-art dictionary learning algorithms which impose sparsityconstraints on a sample-by-sample basis, we instead treat the samples as abatch, and impose the sparsity constraint on the whole. The benefit ofbatchwise optimization is that the non-zeros can be better allocated across thesamples, leading to a better approximation of the whole. To accomplish this, wepropose procedures to switch non-zeros in both rows and columns in the supportof the coefficient matrix to reduce the reconstruction error. We prove in theproposed support switching procedure the objective of the algorithm, i.e., thereconstruction error, decreases monotonically and converges. Furthermore, weintroduce a block orthogonal matching pursuit algorithm that also operates onsample batches to provide a warm start. Experiments on both natural imagepatches and UCI data sets show that the proposed algorithm produces a betterapproximation with the same sparsity levels compared to the state-of-the-artalgorithms.
arxiv-9000-237 | Novel Approaches for Predicting Risk Factors of Atherosclerosis | http://arxiv.org/pdf/1501.07093v2.pdf | author:V. Sree Hari Rao, M. Naresh Kumar category:cs.LG published:2015-01-28 summary:Coronary heart disease (CHD) caused by hardening of artery walls due tocholesterol known as atherosclerosis is responsible for large number of deathsworld-wide. The disease progression is slow, asymptomatic and may lead tosudden cardiac arrest, stroke or myocardial infraction. Presently, imagingtechniques are being employed to understand the molecular and metabolicactivity of atherosclerotic plaques to estimate the risk. Though imagingmethods are able to provide some information on plaque metabolism they lack therequired resolution and sensitivity for detection. In this paper we considerthe clinical observations and habits of individuals for predicting the riskfactors of CHD. The identification of risk factors helps in stratifyingpatients for further intensive tests such as nuclear imaging or coronaryangiography. We present a novel approach for predicting the risk factors ofatherosclerosis with an in-built imputation algorithm and particle swarmoptimization (PSO). We compare the performance of our methodology with othermachine learning techniques on STULONG dataset which is based on longitudinalstudy of middle aged individuals lasting for twenty years. Our methodologypowered by PSO search has identified physical inactivity as one of the riskfactor for the onset of atherosclerosis in addition to other already knownfactors. The decision rules extracted by our methodology are able to predictthe risk factors with an accuracy of $99.73%$ which is higher than theaccuracies obtained by application of the state-of-the-art machine learningtechniques presently being employed in the identification of atherosclerosisrisk studies.
arxiv-9000-238 | A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue Fever | http://arxiv.org/pdf/1502.00062v1.pdf | author:Vadrevu Sree Hari Rao, Mallenahalli Naresh Kumar category:stat.ML cs.AI cs.LG published:2015-01-31 summary:Identification of the influential clinical symptoms and laboratory featuresthat help in the diagnosis of dengue fever in early phase of the illness wouldaid in designing effective public health management and virologicalsurveillance strategies. Keeping this as our main objective we develop in thispaper, a new computational intelligence based methodology that predicts thediagnosis in real time, minimizing the number of false positives and falsenegatives. Our methodology consists of three major components (i) a novelmissing value imputation procedure that can be applied on any data setconsisting of categorical (nominal) and/or numeric (real or integer) (ii) awrapper based features selection method with genetic search for extracting asubset of most influential symptoms that can diagnose the illness and (iii) analternating decision tree method that employs boosting for generating highlyaccurate decision rules. The predictive models developed using our methodologyare found to be more accurate than the state-of-the-art methodologies used inthe diagnosis of the dengue fever.
arxiv-9000-239 | Maximally Informative Hierarchical Representations of High-Dimensional Data | http://arxiv.org/pdf/1410.7404v2.pdf | author:Greg Ver Steeg, Aram Galstyan category:stat.ML cs.LG published:2014-10-27 summary:We consider a set of probabilistic functions of some input variables as arepresentation of the inputs. We present bounds on how informative arepresentation is about input data. We extend these bounds to hierarchicalrepresentations so that we can quantify the contribution of each layer towardscapturing the information in the original data. The special form of thesebounds leads to a simple, bottom-up optimization procedure to constructhierarchical representations that are also maximally informative about thedata. This optimization has linear computational complexity and constant samplecomplexity in the number of variables. These results establish a new approachto unsupervised learning of deep representations that is both principled andpractical. We demonstrate the usefulness of the approach on both synthetic andreal-world data.
arxiv-9000-240 | Max-Margin Object Detection | http://arxiv.org/pdf/1502.00046v1.pdf | author:Davis E. King category:cs.CV published:2015-01-31 summary:Most object detection methods operate by applying a binary classifier tosub-windows of an image, followed by a non-maximum suppression step wheredetections on overlapping sub-windows are removed. Since the number of possiblesub-windows in even moderately sized image datasets is extremely large, theclassifier is typically learned from only a subset of the windows. This avoidsthe computational difficulty of dealing with the entire set of sub-windows,however, as we will show in this paper, it leads to sub-optimal detectorperformance. In particular, the main contribution of this paper is the introduction of anew method, Max-Margin Object Detection (MMOD), for learning to detect objectsin images. This method does not perform any sub-sampling, but instead optimizesover all sub-windows. MMOD can be used to improve any object detection methodwhich is linear in the learned parameters, such as HOG or bag-of-visual-wordmodels. Using this approach we show substantial performance gains on threepublicly available datasets. Strikingly, we show that a single rigid HOG filtercan outperform a state-of-the-art deformable part model on the Face DetectionData Set and Benchmark when the HOG filter is learned via MMOD.
arxiv-9000-241 | Text mixing shapes the anatomy of rank-frequency distributions: A modern Zipfian mechanics for natural language | http://arxiv.org/pdf/1409.3870v3.pdf | author:Jake Ryland Williams, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL physics.soc-ph published:2014-09-12 summary:Natural languages are full of rules and exceptions. One of the most famousquantitative rules is Zipf's law which states that the frequency of occurrenceof a word is approximately inversely proportional to its rank. Though this`law' of ranks has been found to hold across disparate texts and forms of data,analyses of increasingly large corpora over the last 15 years have revealed theexistence of two scaling regimes. These regimes have thus far been explained bya hypothesis suggesting a separability of languages into core and non-corelexica. Here, we present and defend an alternative hypothesis, that the twoscaling regimes result from the act of aggregating texts. We observe that textmixing leads to an effective decay of word introduction, which we show providesaccurate predictions of the location and severity of breaks in scaling. Uponexamining large corpora from 10 languages in the Project Gutenberg eBookscollection (eBooks), we find emphatic empirical support for the universality ofour claim.
arxiv-9000-242 | maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs | http://arxiv.org/pdf/1501.06633v3.pdf | author:Andrew Lavin category:cs.NE cs.DC cs.LG published:2015-01-27 summary:This paper describes maxDNN, a computationally efficient convolution kernelfor deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%computational efficiency on typical deep learning network architectures. Thedesign combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. Weonly address forward propagation (FPROP) operation of the network, but webelieve that the same techniques used here will be effective for backwardpropagation (BPROP) as well.
arxiv-9000-243 | SHOE: Supervised Hashing with Output Embeddings | http://arxiv.org/pdf/1502.00030v1.pdf | author:Sravanthi Bondugula, Varun Manjunatha, Larry S. Davis, David Doermann category:cs.CV published:2015-01-30 summary:We present a supervised binary encoding scheme for image retrieval thatlearns projections by taking into account similarity between classes obtainedfrom output embeddings. Our motivation is that binary hash codes learned inthis way improve both the visual quality of retrieval results and existingsupervised hashing schemes. We employ a sequential greedy optimization thatlearns relationship aware projections by minimizing the difference betweeninner products of binary codes and output embedding vectors. We develop a jointoptimization framework to learn projections which improve the accuracy ofsupervised hashing over the current state of the art with respect to standardand sibling evaluation metrics. We further boost performance by applying thesupervised dimensionality reduction technique on kernelized input CNN features.Experiments are performed on three datasets: CUB-2011, SUN-Attribute andImageNet ILSVRC 2010. As a by-product of our method, we show that using asimple k-nn pooling classifier with our discriminative codes improves over thecomplex classification models on fine grained datasets like CUB and offer animpressive compression ratio of 1024 on CNN features.
arxiv-9000-244 | A Light Transport Model for Mitigating Multipath Interference in TOF Sensors | http://arxiv.org/pdf/1501.04878v2.pdf | author:Nikhil Naik, Achuta Kadambi, Christoph Rhemann, Shahram Izadi, Ramesh Raskar, Sing Bing Kang category:cs.CV published:2015-01-20 summary:Continuous-wave Time-of-flight (TOF) range imaging has become a commerciallyviable technology with many applications in computer vision and graphics.However, the depth images obtained from TOF cameras contain scene dependenterrors due to multipath interference (MPI). Specifically, MPI occurs whenmultiple optical reflections return to a single spatial location on the imagingsensor. Many prior approaches to rectifying MPI rely on sparsity in opticalreflections, which is an extreme simplification. In this paper, we correct MPIby combining the standard measurements from a TOF camera with information fromdirect and global light transport. We report results on both simulatedexperiments and physical experiments (using the Kinect sensor). Our results,evaluated against ground truth, demonstrate a quantitative improvement in depthaccuracy.
arxiv-9000-245 | Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues | http://arxiv.org/pdf/1501.05703v2.pdf | author:Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, Lubomir Bourdev category:cs.CV published:2015-01-23 summary:We explore the task of recognizing peoples' identities in photo albums in anunconstrained setting. To facilitate this, we introduce the new People In PhotoAlbums (PIPA) dataset, consisting of over 60000 instances of 2000 individualscollected from public Flickr photo albums. With only about half of the personimages containing a frontal face, the recognition task is very challenging dueto the large variations in pose, clothing, camera viewpoint, image resolutionand illumination. We propose the Pose Invariant PErson Recognition (PIPER)method, which accumulates the cues of poselet-level person recognizers trainedby deep convolutional networks to discount for the pose variations, combinedwith a face recognizer and a global recognizer. Experiments on three differentsettings confirm that in our unconstrained setup PIPER significantly improveson the performance of DeepFace, which is one of the best face recognizers asmeasured on the LFW dataset.
arxiv-9000-246 | Multi-task Image Classification via Collaborative, Hierarchical Spike-and-Slab Priors | http://arxiv.org/pdf/1501.07867v1.pdf | author:Hojjat Seyed Mousavi, Umamahesh Srinivas, Vishal Monga, Yuanming Suo, Minh Dao, Trac. D. Tran category:cs.CV published:2015-01-30 summary:Promising results have been achieved in image classification problems byexploiting the discriminative power of sparse representations forclassification (SRC). Recently, it has been shown that the use of\emph{class-specific} spike-and-slab priors in conjunction with theclass-specific dictionaries from SRC is particularly effective in low trainingscenarios. As a logical extension, we build on this framework for multitaskscenarios, wherein multiple representations of the same physical phenomena areavailable. We experimentally demonstrate the benefits of mining jointinformation from different camera views for multi-view face recognition.
arxiv-9000-247 | An Analytical Study of different Document Image Binarization Methods | http://arxiv.org/pdf/1501.07862v1.pdf | author:Mahua Nandy, Satadal Saha category:cs.CV published:2015-01-30 summary:Document image has been the area of research for a couple of decades becauseof its potential application in the area of text recognition, line recognitionor any other shape recognition from the image. For most of these purposesbinarization of image becomes mandatory as far as recognition is concerned.Throughout couple decades standard algorithms have already been developed forthis purpose. Some of these algorithms are applicable to degraded image also.Our objective behind this work is to study the existing techniques, comparethem in view of advantages and disadvantages and modify some of thesealgorithms to optimize time or performance.
arxiv-9000-248 | Orthogonal Matrix Retrieval in Cryo-Electron Microscopy | http://arxiv.org/pdf/1412.0494v2.pdf | author:Tejal Bhamre, Teng Zhang, Amit Singer category:cs.CV published:2014-12-01 summary:In single particle reconstruction (SPR) from cryo-electron microscopy(cryo-EM), the 3D structure of a molecule needs to be determined from its 2Dprojection images taken at unknown viewing directions. Zvi Kam showed alreadyin 1980 that the autocorrelation function of the 3D molecule over the rotationgroup SO(3) can be estimated from 2D projection images whose viewing directionsare uniformly distributed over the sphere. The autocorrelation functiondetermines the expansion coefficients of the 3D molecule in spherical harmonicsup to an orthogonal matrix of size $(2l+1)\times (2l+1)$ for each$l=0,1,2,...$. In this paper we show how techniques for solving the phaseretrieval problem in X-ray crystallography can be modified for the cryo-EMsetup for retrieving the missing orthogonal matrices. Specifically, we presenttwo new approaches that we term Orthogonal Extension and OrthogonalReplacement, in which the main algorithmic components are the singular valuedecomposition and semidefinite programming. We demonstrate the utility of theseapproaches through numerical experiments on simulated data.
arxiv-9000-249 | A Proximal Bregman Projection Approach to Continuous Max-Flow Problems Using Entropic Distances | http://arxiv.org/pdf/1501.07844v1.pdf | author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV published:2015-01-30 summary:One issue limiting the adaption of large-scale multi-region segmentation isthe sometimes prohibitive memory requirements. This is especially troublingconsidering advances in massively parallel computing and commercial graphicsprocessing units because of their already limited memory compared to thecurrent random access memory used in more traditional computation. To addressthis issue in the field of continuous max-flow segmentation, we have developeda \textit{pseudo-flow} framework using the theory of Bregman proximalprojections and entropic distances which implicitly represents flow variablesbetween labels and designated source and sink nodes. This reduces the memoryrequirements for max-flow segmentation by approximately 20\% for Potts modelsand approximately 30\% for hierarchical max-flow (HMF) and directed acyclicgraph max-flow (DAGMF) models. This represents a great improvement in thestate-of-the-art in max-flow segmentation, allowing for much larger problems tobe addressed and accelerated using commercially available graphics processinghardware.
arxiv-9000-250 | Significant Subgraph Mining with Multiple Testing Correction | http://arxiv.org/pdf/1407.0316v3.pdf | author:Mahito Sugiyama, Felipe Llinares López, Niklas Kasenburg, Karsten M. Borgwardt category:stat.ME cs.LG stat.ML published:2014-07-01 summary:The problem of finding itemsets that are statistically significantly enrichedin a class of transactions is complicated by the need to correct for multiplehypothesis testing. Pruning untestable hypotheses was recently proposed as astrategy for this task of significant itemset mining. It was shown to lead togreater statistical power, the discovery of more truly significant itemsets,than the standard Bonferroni correction on real-world datasets. An openquestion, however, is whether this strategy of excluding untestable hypothesesalso leads to greater statistical power in subgraph mining, in which the numberof hypotheses is much larger than in itemset mining. Here we answer thisquestion by an empirical investigation on eight popular graph benchmarkdatasets. We propose a new efficient search strategy, which always returns thesame solution as the state-of-the-art approach and is approximately two ordersof magnitude faster. Moreover, we exploit the dependence between subgraphs byconsidering the effective number of tests and thereby further increase thestatistical power.
arxiv-9000-251 | RANSAC based three points algorithm for ellipse fitting of spherical object's projection | http://arxiv.org/pdf/1503.07460v1.pdf | author:Shenghui Xu category:cs.CV published:2015-01-30 summary:As the spherical object can be seen everywhere, we should extract the ellipseimage accurately and fit it by implicit algebraic curve in order to finish the3D reconstruction. In this paper, we propose a new ellipse fitting algorithmwhich only needs three points to fit the projection of spherical object and isdifferent from the traditional algorithms that need at least five point. Thefitting procedure is just similar as the estimation of Fundamental Matrixestimation by seven points, and the RANSAC algorithm has also been used toexclude the interference of noise and scattered points.
arxiv-9000-252 | Confidence intervals for AB-test | http://arxiv.org/pdf/1501.07768v1.pdf | author:Cyrille Dubarry category:stat.ML published:2015-01-30 summary:AB-testing is a very popular technique in web companies since it makes itpossible to accurately predict the impact of a modification with the simplicityof a random split across users. One of the critical aspects of an AB-test isits duration and it is important to reliably compute confidence intervalsassociated with the metric of interest to know when to stop the test. In thispaper, we define a clean mathematical framework to model the AB-test process.We then propose three algorithms based on bootstrapping and on the centrallimit theorem to compute reliable confidence intervals which extend to othermetrics than the common probabilities of success. They apply to both absoluteand relative increments of the most used comparison metrics, including thenumber of occurrences of a particular event and a click-through rate implying aratio.
arxiv-9000-253 | Gibbs-Ringing Artifact Removal Based on Local Subvoxel-shifts | http://arxiv.org/pdf/1501.07758v1.pdf | author:Elias Kellner, Bibek Dhital, Marco Reisert category:physics.med-ph cs.CV published:2015-01-30 summary:Gibbs-ringing is a well known artifact which manifests itself as spuriousoscillations in the vicinity of sharp image transients, e.g. at tissueboundaries. The origin can be seen in the truncation of k-space during MRIdata-acquisition. Consequently, correction techniques like Gegenbauerreconstruction or extrapolation methods aim at recovering these missing data.Here, we present a simple and robust method which exploits a different view onthe Gibbs-phenomena. The truncation in k-space can be interpreted as aconvolution with a sinc-function in image space. Hence, the severity of theartifacts depends on how the sinc-function is sampled. We propose tore-interpolate the image based on local, subvoxel shifts to sample the ringingpattern at the zero-crossings of the oscillating sinc-function. With this, theartifact can effectively and robustly be removed with a minimal amount ofsmoothing.
arxiv-9000-254 | Co-Regularized Deep Representations for Video Summarization | http://arxiv.org/pdf/1501.07738v1.pdf | author:Olivier Morère, Hanlin Goh, Antoine Veillard, Vijay Chandrasekhar, Jie Lin category:cs.CV published:2015-01-30 summary:Compact keyframe-based video summaries are a popular way of generatingviewership on video sharing platforms. Yet, creating relevant and compellingsummaries for arbitrarily long videos with a small number of keyframes is achallenging task. We propose a comprehensive keyframe-based summarizationframework combining deep convolutional neural networks and restricted Boltzmannmachines. An original co-regularization scheme is used to discover meaningfulsubject-scene associations. The resulting multimodal representations are thenused to select highly-relevant keyframes. A comprehensive user study isconducted comparing our proposed method to a variety of schemes, including thesummarization currently in use by one of the most popular video sharingwebsites. The results show that our method consistently outperforms thebaseline schemes for any given amount of keyframes both in terms ofattractiveness and informativeness. The lead is even more significant forsmaller summaries.
arxiv-9000-255 | Blob indentation identification via curvature measurement | http://arxiv.org/pdf/1501.07692v1.pdf | author:Matthew Sottile category:cs.CV published:2015-01-30 summary:This paper presents a novel method for identifying indentations on theboundary of solid 2D shape. It uses the signed curvature at a set of pointsalong the boundary to identify indentations and provides one parameter fortuning the selection mechanism for discriminating indentations from otherboundary irregularities. An efficient implementation is described based on theFourier transform for calculating curvature from a sequence of points obtainedfrom the boundary of a binary blob.
arxiv-9000-256 | Structure Regularization for Structured Prediction: Theories and Experiments | http://arxiv.org/pdf/1411.6243v2.pdf | author:Xu Sun category:cs.LG published:2014-11-23 summary:While there are many studies on weight regularization, the study on structureregularization is rare. Many existing systems on structured prediction focus onincreasing the level of structural dependencies within the model. However, thistrend could have been misdirected, because our study suggests that complexstructures are actually harmful to generalization ability in structuredprediction. To control structure-based overfitting, we propose a structureregularization framework via \emph{structure decomposition}, which decomposestraining samples into mini-samples with simpler structures, deriving a modelwith better generalization power. We show both theoretically and empiricallythat structure regularization can effectively control overfitting risk and leadto better accuracy. As a by-product, the proposed method can also substantiallyaccelerate the training speed. The method and the theoretical results can applyto general graphical models with arbitrary structures. Experiments onwell-known tasks demonstrate that our method can easily beat the benchmarksystems on those highly-competitive tasks, achieving state-of-the-artaccuracies yet with substantially faster training speed.
arxiv-9000-257 | Downscaling Microwave Brightness Temperatures Using Self Regularized Regressive Models | http://arxiv.org/pdf/1501.07683v1.pdf | author:Subit Chakrabarti, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV 68 published:2015-01-30 summary:A novel algorithm is proposed to downscale microwave brightness temperatures($\mathrm{T_B}$), at scales of 10-40 km such as those from the Soil MoistureActive Passive mission to a resolution meaningful for hydrological andagricultural applications. This algorithm, called Self-Regularized RegressiveModels (SRRM), uses auxiliary variables correlated to $\mathrm{T_B}$ along-witha limited set of \textit{in-situ} SM observations, which are converted to highresolution $\mathrm{T_B}$ observations using biophysical models. It includes aninformation-theoretic clustering step based on all auxiliary variables toidentify areas of similarity, followed by a kernel regression step thatproduces downscaled $\mathrm{T_B}$. This was implemented on a multi-scalesynthetic data-set over NC-Florida for one year. An RMSE of 5.76~K withstandard deviation of 2.8~k was achieved during the vegetated season and anRMSE of 1.2~K with a standard deviation of 0.9~K during periods of novegetation.
arxiv-9000-258 | Vector Quantization by Minimizing Kullback-Leibler Divergence | http://arxiv.org/pdf/1501.07681v1.pdf | author:Lan Yang, Jingbin Wang, Yujin Tu, Prarthana Mahapatra, Nelson Cardoso category:cs.CV published:2015-01-30 summary:This paper proposes a new method for vector quantization by minimizing theKullback-Leibler Divergence between the class label distributions over thequantization inputs, which are original vectors, and the output, which is thequantization subsets of the vector set. In this way, the vector quantizationoutput can keep as much information of the class label as possible. Anobjective function is constructed and we also developed an iterative algorithmto minimize it. The new method is evaluated on bag-of-features based imageclassification problem.
arxiv-9000-259 | Towards Resolving Software Quality-in-Use Measurement Challenges | http://arxiv.org/pdf/1501.07676v1.pdf | author:Issa Atoum, Chih How Bong, Narayanan Kulathuramaiyer category:cs.SE cs.CL published:2015-01-30 summary:Software quality-in-use comprehends the quality from user's perspectives. Ithas gained its importance in e-learning applications, mobile service basedapplications and project management tools. User's decisions on softwareacquisitions are often ad hoc or based on preference due to difficulty inquantitatively measure software quality-in-use. However, why quality-in-usemeasurement is difficult? Although there are many software quality models toour knowledge, no works surveys the challenges related to softwarequality-in-use measurement. This paper has two main contributions; 1) presentsmajor issues and challenges in measuring software quality-in-use in the contextof the ISO SQuaRE series and related software quality models, 2) Presents anovel framework that can be used to predict software quality-in-use, and 3)presents preliminary results of quality-in-use topic prediction. Concisely, theissues are related to the complexity of the current standard models and thelimitations and incompleteness of the customized software quality models. Theproposed framework employs sentiment analysis techniques to predict softwarequality-in-use.
arxiv-9000-260 | ImageNet Large Scale Visual Recognition Challenge | http://arxiv.org/pdf/1409.0575v3.pdf | author:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei category:cs.CV I.4.8; I.5.2 published:2014-09-01 summary:The ImageNet Large Scale Visual Recognition Challenge is a benchmark inobject category classification and detection on hundreds of object categoriesand millions of images. The challenge has been run annually from 2010 topresent, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advancesin object recognition that have been possible as a result. We discuss thechallenges of collecting large-scale ground truth annotation, highlight keybreakthroughs in categorical object recognition, provide a detailed analysis ofthe current state of the field of large-scale image classification and objectdetection, and compare the state-of-the-art computer vision accuracy with humanaccuracy. We conclude with lessons learned in the five years of the challenge,and propose future directions and improvements.
arxiv-9000-261 | Representing Objects, Relations, and Sequences | http://arxiv.org/pdf/1501.07627v1.pdf | author:Stephen I. Gallant, T. Wendy Okaywe category:cs.LG published:2015-01-29 summary:Vector Symbolic Architectures (VSAs) are high-dimensional vectorrepresentations of objects (eg., words, image parts), relations (eg., sentencestructures), and sequences for use with machine learning algorithms. Theyconsist of a vector addition operator for representing a collection ofunordered objects, a Binding operator for associating groups of objects, and amethodology for encoding complex structures. We first develop Constraints that machine learning imposes upon VSAs: forexample, similar structures must be represented by similar vectors. Theconstraints suggest that current VSAs should represent phrases ("The smartBrazilian girl") by binding sums of terms, in addition to simply binding theterms directly. We show that matrix multiplication can be used as the binding operator for aVSA, and that matrix elements can be chosen at random. A consequence for livingsystems is that binding is mathematically possible without the need to specify,in advance, precise neuron-to-neuron connection properties for large numbers ofsynapses. A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms),is described that satisfies all Constraints. With respect to machine learning, for some types of problems appropriate VSArepresentations permit us to prove learnability, rather than relying onsimulations. We also propose dividing machine (and neural) learning andrepresentation into three Stages, with differing roles for learning in eachstage. For neural modeling, we give "representational reasons" for nervous systemsto have many recurrent connections, as well as for the importance of phrases inlanguage processing. Sizing simulations and analyses suggest that VSAs in general, and MBAT inparticular, are ready for real-world applications.
arxiv-9000-262 | Deep Representations for Iris, Face, and Fingerprint Spoofing Detection | http://arxiv.org/pdf/1410.1980v3.pdf | author:David Menotti, Giovani Chiachia, Allan Pinto, William Robson Schwartz, Helio Pedrini, Alexandre Xavier Falcao, Anderson Rocha category:cs.CV published:2014-10-08 summary:Biometrics systems have significantly improved person identification andauthentication, playing an important role in personal, national, and globalsecurity. However, these systems might be deceived (or "spoofed") and, despitethe recent advances in spoofing detection, current solutions often rely ondomain knowledge, specific biometric reading systems, and attack types. Weassume a very limited knowledge about biometric spoofing at the sensor toderive outstanding spoofing detection systems for iris, face, and fingerprintmodalities based on two deep learning approaches. The first approach consistsof learning suitable convolutional network architectures for each domain, whilethe second approach focuses on learning the weights of the network viaback-propagation. We consider nine biometric spoofing benchmarks --- each onecontaining real and fake samples of a given biometric modality and attack type--- and learn deep representations for each benchmark by combining andcontrasting the two learning approaches. This strategy not only provides bettercomprehension of how these approaches interplay, but also creates systems thatexceed the best known results in eight out of the nine benchmarks. The resultsstrongly indicate that spoofing detection systems based on convolutionalnetworks can be robust to attacks already known and possibly adapted, withlittle effort, to image-based attacks that are yet to come.
arxiv-9000-263 | Efficient Divide-And-Conquer Classification Based on Feature-Space Decomposition | http://arxiv.org/pdf/1501.07584v1.pdf | author:Qi Guo, Bo-Wei Chen, Feng Jiang, Xiangyang Ji, Sun-Yuan Kung category:cs.LG published:2015-01-29 summary:This study presents a divide-and-conquer (DC) approach based on feature spacedecomposition for classification. When large-scale datasets are present,typical approaches usually employed truncated kernel methods on the featurespace or DC approaches on the sample space. However, this did not guaranteeseparability between classes, owing to overfitting. To overcome such problems,this work proposes a novel DC approach on feature spaces consisting of threesteps. Firstly, we divide the feature space into several subspaces using thedecomposition method proposed in this paper. Subsequently, these featuresubspaces are sent into individual local classifiers for training. Finally, theoutcomes of local classifiers are fused together to generate the finalclassification results. Experiments on large-scale datasets are carried out forperformance evaluation. The results show that the error rates of the proposedDC method decreased comparing with the state-of-the-art fast SVM solvers, e.g.,reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasetsrespectively.
arxiv-9000-264 | High-Dimensional Longitudinal Classification with the Multinomial Fused Lasso | http://arxiv.org/pdf/1501.07518v1.pdf | author:Samrachana Adhikari, Fabrizio Lecci, James T. Becker, Brian W. Junker, Lewis H. Kuller, Oscar L. Lopez, Ryan J. Tibshirani category:stat.AP stat.ML published:2015-01-29 summary:We study regularized estimation in high-dimensional longitudinalclassification problems, using the lasso and fused lasso regularizers. Theconstructed coefficient estimates are piecewise constant across the timedimension in the longitudinal problem, with adaptively selected change points(break points). We present an efficient algorithm for computing such estimates,based on proximal gradient descent. We apply our proposed technique to alongitudinal data set on Alzheimer's disease from the Cardiovascular HealthStudy Cognition Study, and use this data set to motivate and demonstrateseveral practical considerations such as the selection of tuning parameters,and the assessment of model stability.
arxiv-9000-265 | Implementation of an Automatic Syllabic Division Algorithm from Speech Files in Portuguese Language | http://arxiv.org/pdf/1501.07496v1.pdf | author:E. L. F. Da Silva, H. M. de Oliveira category:cs.SD cs.CL cs.DS published:2015-01-29 summary:A new algorithm for voice automatic syllabic splitting in the Portugueselanguage is proposed, which is based on the envelope of the speech signal ofthe input audio file. A computational implementation in MatlabTM is presentedand made available at the URLhttp://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to itsstraightforwardness, the proposed method is very attractive for embeddedsystems (e.g. i-phones). It can also be used as a screen to assist moresophisticated methods. Voice excerpts containing more than one syllable andidentified by the same envelope are named as super-syllables and they aresubsequently separated. The results indicate which samples corresponds to thebeginning and end of each detected syllable. Preliminary tests were performedto fifty words at an identification rate circa 70% (further improvements may beincorporated to treat particular phonemes). This algorithm is also useful invoice command systems, as a tool in the teaching of Portuguese language or evenfor patients with speech pathology.
arxiv-9000-266 | Regression and Learning to Rank Aggregation for User Engagement Evaluation | http://arxiv.org/pdf/1501.07467v1.pdf | author:Hamed Zamani, Azadeh Shakery, Pooya Moradi category:cs.IR cs.LG H.2.8; J.4 published:2015-01-29 summary:User engagement refers to the amount of interaction an instance (e.g., tweet,news, and forum post) achieves. Ranking the items in social media websitesbased on the amount of user participation in them, can be used in differentapplications, such as recommender systems. In this paper, we consider a tweetcontaining a rating for a movie as an instance and focus on ranking theinstances of each user based on their engagement, i.e., the total number ofretweets and favorites it will gain. For this task, we define several features which can be extracted from themeta-data of each tweet. The features are partitioned into three categories:user-based, movie-based, and tweet-based. We show that in order to obtain goodresults, features from all categories should be considered. We exploitregression and learning to rank methods to rank the tweets and propose toaggregate the results of regression and learning to rank methods to achievebetter performance. We have run our experiments on an extended version ofMovieTweeting dataset provided by ACM RecSys Challenge 2014. The results showthat learning to rank approach outperforms most of the regression models andthe combination can improve the performance significantly.
arxiv-9000-267 | Pairwise Rotation Hashing for High-dimensional Features | http://arxiv.org/pdf/1501.07422v1.pdf | author:Kohta Ishikawa, Ikuro Sato, Mitsuru Ambai category:cs.CV stat.ML published:2015-01-29 summary:Binary Hashing is widely used for effective approximate nearest neighborssearch. Even though various binary hashing methods have been proposed, very fewmethods are feasible for extremely high-dimensional features often used invisual tasks today. We propose a novel highly sparse linear hashing methodbased on pairwise rotations. The encoding cost of the proposed algorithm is$\mathrm{O}(n \log n)$ for n-dimensional features, whereas that of the existingstate-of-the-art method is typically $\mathrm{O}(n^2)$. The proposed method isalso remarkably faster in the learning phase. Along with the efficiency, theretrieval accuracy is comparable to or slightly outperforming thestate-of-the-art. Pairwise rotations used in our method are formulated from ananalytical study of the trade-off relationship between quantization error andentropy of binary codes. Although these hashing criteria are widely used inprevious researches, its analytical behavior is rarely studied. All buildingblocks of our algorithm are based on the analytical solution, and it thusprovides a fairly simple and efficient procedure.
arxiv-9000-268 | Particle swarm optimization for time series motif discovery | http://arxiv.org/pdf/1501.07399v1.pdf | author:Joan Serrà, Josep Lluis Arcos category:cs.LG cs.NE published:2015-01-29 summary:Efficiently finding similar segments or motifs in time series data is afundamental task that, due to the ubiquity of these data, is present in a widerange of domains and situations. Because of this, countless solutions have beendevised but, to date, none of them seems to be fully satisfactory and flexible.In this article, we propose an innovative standpoint and present a solutioncoming from it: an anytime multimodal optimization algorithm for time seriesmotif discovery based on particle swarms. By considering data from a variety ofdomains, we show that this solution is extremely competitive when compared tothe state-of-the-art, obtaining comparable motifs in considerably less timeusing minimal memory. In addition, we show that it is robust to differentimplementation choices and see that it offers an unprecedented degree offlexibility with regard to the task. All these qualities make the presentedsolution stand out as one of the most prominent candidates for motif discoveryin long time series streams. Besides, we believe the proposed standpoint can beexploited in further time series analysis and mining tasks, widening the scopeof research and potentially yielding novel effective solutions.
arxiv-9000-269 | Optimal Adaptive Learning in Uncontrolled Restless Bandit Problems | http://arxiv.org/pdf/1107.4042v3.pdf | author:Cem Tekin, Mingyan Liu category:math.OC cs.LG published:2011-07-20 summary:In this paper we consider the problem of learning the optimal policy foruncontrolled restless bandit problems. In an uncontrolled restless banditproblem, there is a finite set of arms, each of which when pulled yields apositive reward. There is a player who sequentially selects one of the arms ateach time step. The goal of the player is to maximize its undiscounted rewardover a time horizon T. The reward process of each arm is a finite state Markovchain, whose transition probabilities are unknown by the player. Statetransitions of each arm is independent of the selection of the player. Wepropose a learning algorithm with logarithmic regret uniformly over time withrespect to the optimal finite horizon policy. Our results extend the optimaladaptive learning of MDPs to POMDPs.
arxiv-9000-270 | Online discrete optimization in social networks in the presence of Knightian uncertainty | http://arxiv.org/pdf/1307.0473v2.pdf | author:Maxim Raginsky, Angelia Nedić category:math.OC cs.DC cs.LG published:2013-07-01 summary:We study a model of collective real-time decision-making (or learning) in asocial network operating in an uncertain environment, for which no a prioriprobabilistic model is available. Instead, the environment's impact on theagents in the network is seen through a sequence of cost functions, revealed tothe agents in a causal manner only after all the relevant actions are taken.There are two kinds of costs: individual costs incurred by each agent andlocal-interaction costs incurred by each agent and its neighbors in the socialnetwork. Moreover, agents have inertia: each agent has a default mixed strategythat stays fixed regardless of the state of the environment, and must expendeffort to deviate from this strategy in order to respond to cost signals comingfrom the environment. We construct a decentralized strategy, wherein each agentselects its action based only on the costs directly affecting it and on thedecisions made by its neighbors in the network. In this setting, we quantifysocial learning in terms of regret, which is given by the difference betweenthe realized network performance over a given time horizon and the bestperformance that could have been achieved in hindsight by a fictitiouscentralized entity with full knowledge of the environment's evolution. We showthat our strategy achieves the regret that scales polylogarithmically with thetime horizon and polynomially with the number of agents and the maximum numberof neighbors of any agent in the social network.
arxiv-9000-271 | Sequential Probability Assignment with Binary Alphabets and Large Classes of Experts | http://arxiv.org/pdf/1501.07340v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:cs.IT cs.LG math.IT stat.ML published:2015-01-29 summary:We analyze the problem of sequential probability assignment for binaryoutcomes with side information and logarithmic loss, where regret---or,redundancy---is measured with respect to a (possibly infinite) class ofexperts. We provide upper and lower bounds for minimax regret in terms ofsequential complexities of the class. These complexities were recently shown togive matching (up to logarithmic factors) upper and lower bounds for sequentialprediction with general convex Lipschitz loss functions (Rakhlin and Sridharan,2015). To deal with unbounded gradients of the logarithmic loss, we present anew analysis that employs a sequential chaining technique with a Bernstein-typebound. The introduced complexities are intrinsic to the problem of sequentialprobability assignment, as illustrated by our lower bound. We also consider an example of a large class of experts parametrized byvectors in a high-dimensional Euclidean ball (or a Hilbert ball). The typicaldiscretization approach fails, while our techniques give a non-trivial bound.For this problem we also present an algorithm based on regularization with aself-concordant barrier. This algorithm is of an independent interest, as itrequires a bound on the function values rather than gradients.
arxiv-9000-272 | Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and Shadow-free Image | http://arxiv.org/pdf/1407.0010v2.pdf | author:Liangqiong Qu, Jiandong Tian, Zhi Han, Yandong Tang category:cs.CV published:2014-06-30 summary:In this paper, we propose a novel, effective and fast method to obtain acolor illumination invariant and shadow-free image from a single outdoor image.Different from state-of-the-art methods for shadow-free image that either needshadow detection or statistical learning, we set up a linear equation set foreach pixel value vector based on physically-based shadow invariants, deduce apixel-wise orthogonal decomposition for its solutions, and then get anillumination invariant vector for each pixel value vector on an image. Theillumination invariant vector is the unique particular solution of the linearequation set, which is orthogonal to its free solutions. With this illuminationinvariant vector and Lab color space, we propose an algorithm to generate ashadow-free image which well preserves the texture and color information of theoriginal image. A series of experiments on a diverse set of outdoor images andthe comparisons with the state-of-the-art methods validate our method.
arxiv-9000-273 | On Vectorization of Deep Convolutional Neural Networks for Vision Tasks | http://arxiv.org/pdf/1501.07338v1.pdf | author:Jimmy SJ. Ren, Li Xu category:cs.CV published:2015-01-29 summary:We recently have witnessed many ground-breaking results in machine learningand computer vision, generated by using deep convolutional neural networks(CNN). While the success mainly stems from the large volume of training dataand the deep network architectures, the vector processing hardware (e.g. GPU)undisputedly plays a vital role in modern CNN implementations to supportmassive computation. Though much attention was paid in the extent literature tounderstand the algorithmic side of deep CNN, little research was dedicated tothe vectorization for scaling up CNNs. In this paper, we studied thevectorization process of key building blocks in deep CNNs, in order to betterunderstand and facilitate parallel implementation. Key steps in training andtesting deep CNNs are abstracted as matrix and vector operators, upon whichparallelism can be easily achieved. We developed and compared siximplementations with various degrees of vectorization with which we illustratedthe impact of vectorization on the speed of model training and testing.Besides, a unified CNN framework for both high-level and low-level vision tasksis provided, along with a vectorized Matlab implementation withstate-of-the-art speed performance.
arxiv-9000-274 | Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation | http://arxiv.org/pdf/1501.07315v1.pdf | author:Konstantinos Slavakis, Georgios B. Giannakis category:cs.LG published:2015-01-29 summary:Applications involving dictionary learning, non-negative matrixfactorization, subspace clustering, and parallel factor tensor decompositiontasks motivate well algorithms for per-block-convex and non-smooth optimizationproblems. By leveraging the stochastic approximation paradigm and first-orderacceleration schemes, this paper develops an online and modular learningalgorithm for a large class of non-convex data models, where convexity ismanifested only per-block of variables whenever the rest of them are heldfixed. The advocated algorithm incurs computational complexity that scaleslinearly with the number of unknowns. Under minimal assumptions on the costfunctions of the composite optimization task, without bounding constraints onthe optimization variables, or any explicit information on bounds of Lipschitzcoefficients, the expected cost evaluated online at the resultant iterates isprovably convergent with quadratic rate to an accumulation point of the(per-block) minima, while subgradients of the expected cost asymptoticallyvanish in the mean-squared sense. The merits of the general approach aredemonstrated in two online learning setups: (i) Robust linear regression usinga sparsity-cognizant total least-squares criterion; and (ii) semi-superviseddictionary learning for network-wide link load tracking and imputation withmissing entries. Numerical tests on synthetic and real data highlight thepotential of the proposed framework for streaming data analytics bydemonstrating superior performance over block coordinate descent, and reducedcomplexity relative to the popular alternating-direction method of multipliers.
arxiv-9000-275 | Stochastic Block Transition Models for Dynamic Networks | http://arxiv.org/pdf/1411.5404v2.pdf | author:Kevin S. Xu category:cs.SI cs.LG physics.soc-ph stat.ME published:2014-11-19 summary:There has been great interest in recent years on statistical models fordynamic networks. In this paper, I propose a stochastic block transition model(SBTM) for dynamic networks that is inspired by the well-known stochastic blockmodel (SBM) for static networks and previous dynamic extensions of the SBM.Unlike most existing dynamic network models, it does not make a hidden Markovassumption on the edge-level dynamics, allowing the presence or absence ofedges to directly influence future edge probabilities while retaining theinterpretability of the SBM. I derive an approximate inference procedure forthe SBTM and demonstrate that it is significantly better at reproducingdurations of edges in real social network data.
arxiv-9000-276 | The Beauty of Capturing Faces: Rating the Quality of Digital Portraits | http://arxiv.org/pdf/1501.07304v1.pdf | author:Miriam Redi, Nikhil Rasiwasia, Gaurav Aggarwal, Alejandro Jaimes category:cs.CV cs.CY cs.MM published:2015-01-28 summary:Digital portrait photographs are everywhere, and while the number of facepictures keeps growing, not much work has been done to on automatic portraitbeauty assessment. In this paper, we design a specific framework toautomatically evaluate the beauty of digital portraits. To this end, we procurea large dataset of face images annotated not only with aesthetic scores butalso with information about the traits of the subject portrayed. We design aset of visual features based on portrait photography literature, andextensively analyze their relation with portrait beauty, exposing interestingfindings about what makes a portrait beautiful. We find that the beauty of aportrait is linked to its artistic value, and independent from age, race andgender of the subject. We also show that a classifier trained with our featuresto separate beautiful portraits from non-beautiful portraits outperformsgeneric aesthetic classifiers.
arxiv-9000-277 | Google distance between words | http://arxiv.org/pdf/0901.4180v2.pdf | author:Bjørn Kjos-Hanssen, Alberto J. Evangelista category:cs.CL I.2.7 published:2009-01-27 summary:Cilibrasi and Vitanyi have demonstrated that it is possible to extract themeaning of words from the world-wide web. To achieve this, they rely on thenumber of webpages that are found through a Google search containing a givenword and they associate the page count to the probability that the word appearson a webpage. Thus, conditional probabilities allow them to correlate one wordwith another word's meaning. Furthermore, they have developed a similaritydistance function that gauges how closely related a pair of words is. Wepresent a specific counterexample to the triangle inequality for thissimilarity distance function.
arxiv-9000-278 | Deep Distributed Random Samplings for Supervised Learning: An Alternative to Random Forests? | http://arxiv.org/pdf/1412.1271v2.pdf | author:Xiao-Lei Zhang category:cs.LG stat.ML published:2014-12-03 summary:In (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machinelearning as a coding and dimensionality reduction problem, and further proposeda simple unsupervised dimensionality reduction method, entitled deepdistributed random samplings (DDRS). In this paper, we further extend it tosupervised learning incrementally. The key idea here is to incorporate labelinformation into the coding process by reformulating that each center in DDRShas multiple output units indicating which class the center belongs to. Thesupervised learning method seems somewhat similar with random forests(\cite{breiman2001random}), here we emphasize their differences as follows. (i)Each layer of our method considers the relationship between part of the datapoints in training data with all training data points, while random forestsfocus on building each decision tree on only part of training data pointsindependently. (ii) Our method builds gradually-narrowed network by samplingless and less data points, while random forests builds gradually-narrowednetwork by merging subclasses. (iii) Our method is trained more straightforwardfrom bottom layer to top layer, while random forests build each tree from toplayer to bottom layer by splitting. (iv) Our method encodes output targetsimplicitly in sparse codes, while random forests encode output targets byremembering the class attributes of the activated nodes. Therefore, our methodis a simpler, more straightforward, and maybe a better alternative choice,though both methods use two very basic elements---randomization and nearestneighbor optimization---as the core. This preprint is used to protect theincremental idea from (\cite{zhang2014nonlinear,zhang2014nonlinear2}). Fullempirical evaluation will be announced carefully later.
arxiv-9000-279 | A Discrete Tchebichef Transform Approximation for Image and Video Coding | http://arxiv.org/pdf/1502.00555v1.pdf | author:P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Madanayake category:stat.ME cs.CV cs.MM cs.NA stat.CO published:2015-01-28 summary:In this paper, we introduce a low-complexity approximation for the discreteTchebichef transform (DTT). The proposed forward and inverse transforms aremultiplication-free and require a reduced number of additions and bit-shiftingoperations. Numerical compression simulations demonstrate the efficiency of theproposed transform for image and video coding. Furthermore, Xilinx Virtex-6FPGA based hardware realization shows 44.9% reduction in dynamic powerconsumption and 64.7% lower area when compared to the literature.
arxiv-9000-280 | Understanding Kernel Ridge Regression: Common behaviors from simple functions to density functionals | http://arxiv.org/pdf/1501.03854v2.pdf | author:Kevin Vu, John Snyder, Li Li, Matthias Rupp, Brandon F. Chen, Tarek Khelif, Klaus-Robert Müller, Kieron Burke category:cs.LG stat.ML published:2015-01-16 summary:Accurate approximations to density functionals have recently been obtainedvia machine learning (ML). By applying ML to a simple function of one variablewithout any random sampling, we extract the qualitative dependence of errors onhyperparameters. We find universal features of the behavior in extreme limits,including both very small and very large length scales, and the noise-freelimit. We show how such features arise in ML models of density functionals.
arxiv-9000-281 | Enhanced Mixtures of Part Model for Human Pose Estimation | http://arxiv.org/pdf/1501.05382v2.pdf | author:Wenjuan Gong, Yongzhen Huang, Jordi Gonzalez, and Liang Wang category:cs.CV published:2015-01-22 summary:Mixture of parts model has been successfully applied to 2D human poseestimation problem either as explicitly trained body part model or as latentvariables for the whole human body model. Mixture of parts model usuallyutilize tree structure for representing relations between body parts. Treestructures facilitate training and referencing of the model but could not dealwith double counting problems, which hinder its applications in 3D poseestimation. While most of work targeted to solve these problems tend to modifythe tree models or the optimization target. We incorporate other cues frominput features. For example, in surveillance environments, human silhouettescan be extracted relative easily although not flawlessly. In this condition, wecan combine extracted human blobs with histogram of gradient feature, which iscommonly used in mixture of parts model for training body part templates. Themethod can be easily extend to other candidate features under our generalizedframework. We show 2D body part detection results on a public availabledataset: HumanEva dataset. Furthermore, a 2D to 3D pose estimator is trainedwith Gaussian process regression model and 2D body part detections from theproposed method is fed to the estimator, thus 3D poses are predictable givennew 2D body part detections. We also show results of 3D pose estimation onHumanEva dataset.
arxiv-9000-282 | Survey:Natural Language Parsing For Indian Languages | http://arxiv.org/pdf/1501.07005v1.pdf | author:Monika T. Makwana, Deepak C. Vegda category:cs.CL published:2015-01-28 summary:Syntactic parsing is a necessary task which is required for NLP applicationsincluding machine translation. It is a challenging task to develop aqualitative parser for morphological rich and agglutinative languages.Syntactic analysis is used to understand the grammatical structure of a naturallanguage sentence. It outputs all the grammatical information of each word andits constituent. Also issues related to it help us to understand the languagein a more detailed way. This literature survey is groundwork to understand thedifferent parser development for Indian languages and various approaches thatare used to develop such tools and techniques. This paper provides a survey ofresearch papers from well known journals and conferences.
arxiv-9000-283 | Feature Sampling Strategies for Action Recognition | http://arxiv.org/pdf/1501.06993v1.pdf | author:Youjie Zhou, Hongkai Yu, Song Wang category:cs.CV published:2015-01-28 summary:Although dense local spatial-temporal features with bag-of-featuresrepresentation achieve state-of-the-art performance for action recognition, thehuge feature number and feature size prevent current methods from scaling up toreal size problems. In this work, we investigate different types of featuresampling strategies for action recognition, namely dense sampling, uniformlyrandom sampling and selective sampling. We propose two effective selectivesampling methods using object proposal techniques. Experiments conducted on alarge video dataset show that we are able to achieve better average recognitionaccuracy using 25% less features, through one of proposed selective samplingmethods, and even remain comparable accuracy while discarding 70% features.
arxiv-9000-284 | A Probabilistic Least-Mean-Squares Filter | http://arxiv.org/pdf/1501.06929v1.pdf | author:Jesus Fernandez-Bes, Víctor Elvira, Steven Van Vaerenbergh category:stat.ML cs.SY stat.AP published:2015-01-27 summary:We introduce a probabilistic approach to the LMS filter. By means of anefficient approximation, this approach provides an adaptable step-size LMSalgorithm together with a measure of uncertainty about the estimation. Inaddition, the proposed approximation preserves the linear complexity of thestandard LMS. Numerical results show the improved performance of the algorithmwith respect to standard LMS and state-of-the-art algorithms with similarcomplexity. The goal of this work, therefore, is to open the door to bring somemore Bayesian machine learning techniques to adaptive filtering.
arxiv-9000-285 | Microscopic Advances with Large-Scale Learning: Stochastic Optimization for Cryo-EM | http://arxiv.org/pdf/1501.04656v2.pdf | author:Ali Punjani, Marcus A. Brubaker category:stat.ML cs.CV cs.LG q-bio.QM published:2015-01-19 summary:Determining the 3D structures of biological molecules is a key problem forboth biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promisingtechnique for structure estimation which relies heavily on computationalmethods to reconstruct 3D structures from 2D images. This paper introduces thechallenging Cryo-EM density estimation problem as a novel application forstochastic optimization techniques. Structure discovery is formulated as MAPestimation in a probabilistic latent-variable model, resulting in anoptimization problem to which an array of seven stochastic optimization methodsare applied. The methods are tested on both real and synthetic data, with somemethods recovering reasonable structures in less than one epoch from a randominitialization. Complex quasi-Newton methods are found to converge more slowlythan simple gradient-based methods, but all stochastic methods are found toconverge to similar optima. This method represents a major improvement overexisting methods as it is significantly faster and is able to converge from arandom initialization.
arxiv-9000-286 | The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation | http://arxiv.org/pdf/1411.2674v3.pdf | author:Fangjian Guo, Charles Blundell, Hanna Wallach, Katherine Heller category:stat.ML cs.CL cs.LG cs.SI published:2014-11-11 summary:We present the Bayesian Echo Chamber, a new Bayesian generative model forsocial interaction data. By modeling the evolution of people's language usageover time, this model discovers latent influence relationships between them.Unlike previous work on inferring influence, which has primarily focused onsimple temporal dynamics evidenced via turn-taking behavior, our model capturesmore nuanced influence relationships, evidenced via linguistic accommodationpatterns in interaction content. The model, which is based on a discrete analogof the multivariate Hawkes process, permits a fully Bayesian inferencealgorithm. We validate our model's ability to discover latent influencepatterns using transcripts of arguments heard by the US Supreme Court and themovie "12 Angry Men." We showcase our model's capabilities by using it to inferlatent influence patterns from Federal Open Market Committee meetingtranscripts, demonstrating state-of-the-art performance at uncovering socialdynamics in group discussions.
arxiv-9000-287 | Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations | http://arxiv.org/pdf/1501.06794v1.pdf | author:Bernhard Schölkopf, Krikamol Muandet, Kenji Fukumizu, Jonas Peters category:stat.ML cs.DS cs.LG published:2015-01-27 summary:We describe a method to perform functional operations on probabilitydistributions of random variables. The method uses reproducing kernel Hilbertspace representations of probability distributions, and it is applicable to alloperations which can be applied to points drawn from the respectivedistributions. We refer to our approach as {\em kernel probabilisticprogramming}. We illustrate it on synthetic data, and show how it can be usedfor nonparametric structural equation models, with an application to causalinference.
arxiv-9000-288 | A Cheap System for Vehicle Speed Detection | http://arxiv.org/pdf/1501.06751v1.pdf | author:Chaim Ginzburg, Amit Raphael, Daphna Weinshall category:cs.CV published:2015-01-27 summary:The reliable detection of speed of moving vehicles is considered key totraffic law enforcement in most countries, and is seen by many as an importanttool to reduce the number of traffic accidents and fatalities. Many automaticsystems and different methods are employed in different countries, but as arule they tend to be expensive and/or labor intensive, often employing outdatedtechnology due to the long development time. Here we describe a speed detectionsystem that relies on simple everyday equipment - a laptop and a consumer webcamera. Our method is based on tracking the license plates of cars, which givesthe relative movement of the cars in the image. This image displacement istranslated to actual motion by using the method of projection to a referenceplane, where the reference plane is the road itself. However, since licenseplates do not touch the road, we must compensate for the entailed distortion inspeed measurement. We show how to compute the compensation factor usingknowledge of the license plate standard dimensions. Consequently our systemcomputes the true speed of moving vehicles fast and accurately. We showpromising results on videos obtained in a number of scenes and with differentcar models.
arxiv-9000-289 | Teaching Deep Convolutional Neural Networks to Play Go | http://arxiv.org/pdf/1412.3409v2.pdf | author:Christopher Clark, Amos Storkey category:cs.AI cs.LG cs.NE published:2014-12-10 summary:Mastering the game of Go has remained a long standing challenge to the fieldof AI. Modern computer Go systems rely on processing millions of possiblefuture positions to play well, but intuitively a stronger and more 'humanlike'way to play the game would be to rely on pattern recognition abilities ratherthen brute force computation. Following this sentiment, we train deepconvolutional neural networks to play Go by training them to predict the movesmade by expert Go players. To solve this problem we introduce a number of noveltechniques, including a method of tying weights in the network to 'hard code'symmetries that are expect to exist in the target function, and demonstrate inan ablation study they considerably improve performance. Our final networks areable to achieve move prediction accuracies of 41.1% and 44.4% on two differentGo datasets, surpassing previous state of the art on this task by significantmargins. Additionally, while previous move prediction programs have not yieldedstrong Go playing programs, we show that the networks trained in this workacquired high levels of skill. Our convolutional neural networks canconsistently defeat the well known Go program GNU Go, indicating it is state ofthe art among programs that do not use Monte Carlo Tree Search. It is also ableto win some games against state of the art Go playing program Fuego while usinga fraction of the play time. This success at playing Go indicates high levelprinciples of the game were learned.
arxiv-9000-290 | Parametric Image Segmentation of Humans with Structural Shape Priors | http://arxiv.org/pdf/1501.06722v1.pdf | author:Alin-Ionut Popa, Cristian Sminchisescu category:cs.CV published:2015-01-27 summary:The figure-ground segmentation of humans in images captured in naturalenvironments is an outstanding open problem due to the presence of complexbackgrounds, articulation, varying body proportions, partial views andviewpoint changes. In this work we propose class-specific segmentation modelsthat leverage parametric max-flow image segmentation and a large dataset ofhuman shapes. Our contributions are as follows: (1) formulation of asub-modular energy model that combines class-specific structural constraintsand data-driven shape priors, within a parametric max-flow optimizationmethodology that systematically computes all breakpoints of the model inpolynomial time; (2) design of a data-driven class-specific fusion methodology,based on matching against a large training set of exemplar human shapes(100,000 in our experiments), that allows the shape prior to be constructedon-the-fly, for arbitrary viewpoints and partial views. (3) demonstration ofstate of the art results, in two challenging datasets, H3D and MPII (wherefigure-ground segmentation annotations have been added by us), where wesubstantially improve on the first ranked hypothesis estimates of mid-levelsegmentation methods, by 20%, with hypothesis set sizes that are up to oneorder of magnitude smaller.
arxiv-9000-291 | A General Preprocessing Method for Improved Performance of Epipolar Geometry Estimation Algorithms | http://arxiv.org/pdf/1501.06716v1.pdf | author:Maria Kushnir, Ilan Shimshoni category:cs.CV published:2015-01-27 summary:In this paper a deterministic preprocessing algorithm is presented, whoseoutput can be given as input to most state-of-the-art epipolar geometryestimation algorithms, improving their results considerably. They are now ableto succeed on hard cases for which they failed before. The algorithm consistsof three steps, whose scope changes from local to global. In the local step itextracts from a pair of images local features (e.g. SIFT). Similar featuresfrom each image are clustered and the clusters are matched yielding a largenumber of putative matches. In the second step pairs of spatially closefeatures (called 2keypoints) are matched and ranked by a classifier. The2keypoint matches with the highest ranks are selected. In the global step, fromeach two 2keypoint matches a fundamental matrix is computed. As quite a few ofthe matrices are generated from correct matches they are used to rank theputative matches found in the first step. For each match the number offundamental matrices, for which it approximately satisfies the epipolarconstraint, is calculated. This set of matches is combined with the putativematches generated by standard methods and their probabilities to be correct areestimated by a classifier. These are then given as input to state-of-the-artepipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yieldingmuch better results than the original algorithms. This was shown in extensivetesting performed on almost 900 image pairs from six publicly availabledata-sets.
arxiv-9000-292 | Risk Estimation Without Using Stein's Lemma -- Application to Image Denoising | http://arxiv.org/pdf/1412.2210v2.pdf | author:Sagar Venkatesh Gubbi, Chandra Sekhar Seelamantula category:cs.CV published:2014-12-06 summary:We address the problem of image denoising in additive white noise withoutplacing restrictive assumptions on its statistical distribution. In the recentliterature, specific noise distributions have been considered andcorrespondingly, optimal denoising techniques have been developed. One of thesuccessful approaches for denoising relies on the notion of unbiased riskestimation, which enables one to obtain a useful substitute for the mean-squareerror. For the case of additive white Gaussian noise contamination, the riskestimation procedure relies on Stein's lemma. Sophisticated wavelet-baseddenoising techniques, which are essentially nonlinear, have been developed withthe help of the lemma. We show that, for linear, shift-invariant denoisers, itis possible to obtain unbiased risk estimates of the mean-square error withoutusing Stein's lemma. An interesting consequence of this development is that theunbiased risk estimator becomes agnostic to the statistical distribution of thenoise. As a proof of principle, we show how the new methodology can be used tooptimize the parameters of a simple Gaussian smoother. By locally adapting theparameters of the Gaussian smoother, we obtain a shift-variant smoother, whichhas a denoising performance (quantified by the improvement in peaksignal-to-noise ratio (PSNR)) that is competitive to far more sophisticatedmethods reported in the literature. The proposed solution exhibits considerableparallelism, which we exploit in a Graphics Processing Unit (GPU)implementation.
arxiv-9000-293 | Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits | http://arxiv.org/pdf/1410.0949v3.pdf | author:Branislav Kveton, Zheng Wen, Azin Ashkan, Csaba Szepesvari category:cs.LG cs.AI stat.ML published:2014-10-03 summary:A stochastic combinatorial semi-bandit is an online learning problem where ateach step a learning agent chooses a subset of ground items subject toconstraints, and then observes stochastic weights of these items and receivestheir sum as a payoff. In this paper, we close the problem of computationallyand sample efficient learning in stochastic combinatorial semi-bandits. Inparticular, we analyze a UCB-like algorithm for solving the problem, which isknown to be computationally efficient; and prove $O(K L (1 / \Delta) \log n)$and $O(\sqrt{K L n \log n})$ upper bounds on its $n$-step regret, where $L$ isthe number of ground items, $K$ is the maximum number of chosen items, and$\Delta$ is the gap between the expected returns of the optimal and bestsuboptimal solutions. The gap-dependent bound is tight up to a constant factorand the gap-free bound is tight up to a polylogarithmic factor.
arxiv-9000-294 | High Dimensional Expectation-Maximization Algorithm: Statistical Optimization and Asymptotic Normality | http://arxiv.org/pdf/1412.8729v2.pdf | author:Zhaoran Wang, Quanquan Gu, Yang Ning, Han Liu category:stat.ML published:2014-12-30 summary:We provide a general theory of the expectation-maximization (EM) algorithmfor inferring high dimensional latent variable models. In particular, we maketwo contributions: (i) For parameter estimation, we propose a novel highdimensional EM algorithm which naturally incorporates sparsity structure intoparameter estimation. With an appropriate initialization, this algorithmconverges at a geometric rate and attains an estimator with the (near-)optimalstatistical rate of convergence. (ii) Based on the obtained estimator, wepropose new inferential procedures for testing hypotheses and constructingconfidence intervals for low dimensional components of high dimensionalparameters. For a broad family of statistical models, our framework establishesthe first computationally feasible approach for optimal estimation andasymptotic inference in high dimensions. Our theory is supported by thoroughnumerical results.
arxiv-9000-295 | Optimal computational and statistical rates of convergence for sparse nonconvex learning problems | http://arxiv.org/pdf/1306.4960v5.pdf | author:Zhaoran Wang, Han Liu, Tong Zhang category:stat.ML published:2013-06-20 summary:We provide theoretical analysis of the statistical and computationalproperties of penalized $M$-estimators that can be formulated as the solutionto a possibly nonconvex optimization problem. Many important estimators fall inthis category, including least squares regression with nonconvexregularization, generalized linear models with nonconvex regularization andsparse elliptical random design regression. For these problems, it isintractable to calculate the global solution due to the nonconvex formulation.In this paper, we propose an approximate regularization path-following methodfor solving a variety of learning problems with nonconvex objective functions.Under a unified analytic framework, we simultaneously provide explicitstatistical and computational rates of convergence for any local solutionattained by the algorithm. Computationally, our algorithm attains a globalgeometric rate of convergence for calculating the full regularization path,which is optimal among all first-order algorithms. Unlike most existing methodsthat only attain geometric rates of convergence for one single regularizationparameter, our algorithm calculates the full regularization path with the sameiteration complexity. In particular, we provide a refined iteration complexitybound to sharply characterize the performance of each stage along theregularization path. Statistically, we provide sharp sample complexity analysisfor all the approximate local solutions along the regularization path. Inparticular, our analysis improves upon existing results by providing a morerefined sample complexity bound as well as an exact support recovery result forthe final estimator. These results show that the final estimator attains anoracle statistical property due to the usage of nonconvex penalty.
arxiv-9000-296 | Two-stage Sampled Learning Theory on Distributions | http://arxiv.org/pdf/1402.1754v6.pdf | author:Zoltan Szabo, Arthur Gretton, Barnabas Poczos, Bharath Sriperumbudur category:math.ST cs.LG math.FA stat.ML stat.TH G.3; I.2.6 published:2014-02-07 summary:We focus on the distribution regression problem: regressing to a real-valuedresponse from a probability distribution. Although there exist a large numberof similarity measures between distributions, very little is known about theirgeneralization performance in specific learning tasks. Learning problemsformulated on distributions have an inherent two-stage sampled difficulty: inpractice only samples from sampled distributions are observable, and one has tobuild an estimate on similarities computed between sets of points. To the bestof our knowledge, the only existing method with consistency guarantees fordistribution regression requires kernel density estimation as an intermediatestep (which suffers from slow convergence issues in high dimensions), and thedomain of the distributions to be compact Euclidean. In this paper, we providetheoretical guarantees for a remarkably simple algorithmic alternative to solvethe distribution regression problem: embed the distributions to a reproducingkernel Hilbert space, and learn a ridge regressor from the embeddings to theoutputs. Our main contribution is to prove the consistency of this technique inthe two-stage sampled setting under mild conditions (on separable, topologicaldomains endowed with kernels). For a given total number of observations, wederive convergence rates as an explicit function of the problem difficulty. Asa special case, we answer a 15-year-old open question: we establish theconsistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002]in regression, and cover more recent kernels on distributions, including thosedue to [Christmann and Steinwart, 2010].
arxiv-9000-297 | Online Nonparametric Regression with General Loss Functions | http://arxiv.org/pdf/1501.06598v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.IT cs.LG math.IT published:2015-01-26 summary:This paper establishes minimax rates for online regression with arbitraryclasses of functions and general losses. We show that below a certain thresholdfor the complexity of the function class, the minimax rates depend on both thecurvature of the loss function and the sequential complexities of the class.Above this threshold, the curvature of the loss does not affect the rates.Furthermore, for the case of square loss, our results point to the interestingphenomenon: whenever sequential and i.i.d. empirical entropies match, the ratesfor statistical and online learning are the same. In addition to the study of minimax regret, we derive a generic forecasterthat enjoys the established optimal rates. We also provide a recipe fordesigning online prediction algorithms that can be computationally efficientfor certain problems. We illustrate the techniques by deriving existing and newforecasters for the case of finite experts and for online linear regression.
arxiv-9000-298 | Consensus Message Passing for Layered Graphical Models | http://arxiv.org/pdf/1410.7452v2.pdf | author:Varun Jampani, S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli, John Winn category:cs.CV cs.AI cs.LG published:2014-10-27 summary:Generative models provide a powerful framework for probabilistic reasoning.However, in many domains their use has been hampered by the practicaldifficulties of inference. This is particularly the case in computer vision,where models of the imaging process tend to be large, loopy and layered. Forthis reason bottom-up conditional models have traditionally dominated in suchdomains. We find that widely-used, general-purpose message passing inferencealgorithms such as Expectation Propagation (EP) and Variational Message Passing(VMP) fail on the simplest of vision models. With these models in mind, weintroduce a modification to message passing that learns to exploit theirlayered structure by passing 'consensus' messages that guide inference towardsgood solutions. Experiments on a variety of problems show that the proposedtechnique leads to significantly more accurate inference results, not only whencompared to standard EP and VMP, but also when compared to competitivebottom-up conditional models.
arxiv-9000-299 | Measuring academic influence: Not all citations are equal | http://arxiv.org/pdf/1501.06587v1.pdf | author:Xiaodan Zhu, Peter Turney, Daniel Lemire, André Vellino category:cs.DL cs.CL cs.LG published:2015-01-26 summary:The importance of a research article is routinely measured by counting howmany times it has been cited. However, treating all citations with equal weightignores the wide variety of functions that citations perform. We want toautomatically identify the subset of references in a bibliography that have acentral academic influence on the citing paper. For this purpose, we examinethe effectiveness of a variety of features for determining the academicinfluence of a citation. By asking authors to identify the key references intheir own work, we created a data set in which citations were labeled accordingto their academic influence. Using automatic feature selection with supervisedmachine learning, we found a model for predicting academic influence thatachieves good performance on this data set using only four features. The bestfeatures, among those we evaluated, were those based on the number of times areference is mentioned in the body of a citing paper. The performance of thesefeatures inspired us to design an influence-primed h-index (the hip-index).Unlike the conventional h-index, it weights citations by how many times areference is mentioned. According to our experiments, the hip-index is a betterindicator of researcher performance than the conventional h-index.
arxiv-9000-300 | Multiscale statistical testing for connectome-wide association studies in fMRI | http://arxiv.org/pdf/1409.2080v2.pdf | author:P. Bellec, Y. Benhajali, F. Carbonell, C. Dansereau, G. Albouy, M. Pelland, C. Craddock, O. Collignon, J. Doyon, E. Stip, P. Orban category:q-bio.QM cs.CV stat.AP published:2014-09-07 summary:Alterations in brain connectivity have been associated with a variety ofclinical disorders using functional magnetic resonance imaging (fMRI). Weinvestigated empirically how the number of brain parcels (or scale) impactedthe results of a mass univariate general linear model (GLM) on connectomes. Thebrain parcels used as nodes in the connectome analysis were functionnallydefined by a group cluster analysis. We first validated that a classicBenjamini-Hochberg procedure with parametric GLM tests did controlappropriately the false-discovery rate (FDR) at a given scale. We then observedon realistic simulations that there was no substantial inflation of the FDRacross scales, as long as the FDR was controlled independently within eachscale, and the presence of true associations could be established using anomnibus permutation test combining all scales. Second, we observed both onsimulations and on three real resting-state fMRI datasets (schizophrenia,congenital blindness, motor practice) that the rate of discovery variedmarkedly as a function of scales, and was relatively higher for low scales,below 25. Despite the differences in discovery rate, the statistical mapsderived at different scales were generally very consistent in the three realdatasets. Some seeds still showed effects better observed around 50,illustrating the potential benefits of multiscale analysis. On real data, thestatistical maps agreed well with the existing literature. Overall, our resultssupport that the multiscale GLM connectome analysis with FDR is statisticallyvalid and can capture biologically meaningful effects in a variety ofexperimental conditions.
