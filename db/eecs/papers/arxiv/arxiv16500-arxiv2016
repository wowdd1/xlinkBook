arxiv-16500-1 | Instance-sensitive Fully Convolutional Networks | http://arxiv.org/pdf/1603.08678v1.pdf | author:Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun category:cs.CV published:2016-03-29 summary:Fully convolutional networks (FCNs) have been proven very successful forsemantic segmentation, but the FCN outputs are unaware of object instances. Inthis paper, we develop FCNs that are capable of proposing instance-levelsegment candidates. In contrast to the previous FCN that generates one scoremap, our FCN is designed to compute a small set of instance-sensitive scoremaps, each of which is the outcome of a pixel-wise classifier of a relativeposition to instances. On top of these instance-sensitive score maps, a simpleassembling module is able to output instance candidate at each position. Incontrast to the recent DeepMask method for segmenting instances, our methoddoes not have any high-dimensional layer related to the mask resolution, butinstead exploits image local coherence for estimating instances. We presentcompetitive results of instance segment proposal on both PASCAL VOC and MSCOCO.
arxiv-16500-2 | Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements | http://arxiv.org/pdf/1501.04308v2.pdf | author:Enea Bongiorno, Aldo Goia category:math.PR math.ST stat.AP stat.ME stat.ML stat.TH 62G99 published:2015-01-18 summary:Asymptotic factorizations for the small-ball probability (SmBP) of a Hilbertvalued random element $X$ are rigorously established and discussed. Inparticular, given the first $d$ principal components (PCs) and as the radius$\varepsilon$ of the ball tends to zero, the SmBP is asymptoticallyproportional to (a) the joint density of the first $d$ PCs, (b) the volume ofthe $d$-dimensional ball with radius $\varepsilon$, and (c) a correction factorweighting the use of a truncated version of the process expansion. Moreover,under suitable assumptions on the spectrum of the covariance operator of $X$and as $d$ diverges to infinity when $\varepsilon$ vanishes, somesimplifications occur. In particular, the SmBP factorizes asymptotically as theproduct of the joint density of the first $d$ PCs and a pure volume parameter.All the provided factorizations allow to define a surrogate intensity of theSmBP that, in some cases, leads to a genuine intensity. To operationalize thestated results, a non-parametric estimator for the surrogate intensity isintroduced and it is proved that the use of estimated PCs, instead of the trueones, does not affect the rate of convergence. Finally, as an illustration,simulations in controlled frameworks are provided.
arxiv-16500-3 | Flexible Multi-layer Sparse Approximations of Matrices and Applications | http://arxiv.org/pdf/1506.07300v2.pdf | author:Luc Le Magoarou, Rémi Gribonval category:cs.LG published:2015-06-24 summary:The computational cost of many signal processing and machine learningtechniques is often dominated by the cost of applying certain linear operatorsto high-dimensional vectors. This paper introduces an algorithm aimed atreducing the complexity of applying linear operators in high dimension byapproximately factorizing the corresponding matrix into few sparse factors. Theapproach relies on recent advances in non-convex optimization. It is firstexplained and analyzed in details and then demonstrated experimentally onvarious problems including dictionary learning for image denoising, and theapproximation of large matrices arising in inverse problems.
arxiv-16500-4 | Cross-Language Domain Adaptation for Classifying Crisis-Related Short Messages | http://arxiv.org/pdf/1602.05388v2.pdf | author:Muhammad Imran, Prasenjit Mitra, Jaideep Srivastava category:cs.CL published:2016-02-17 summary:Rapid crisis response requires real-time analysis of messages. After adisaster happens, volunteers attempt to classify tweets to determine needs,e.g., supplies, infrastructure damage, etc. Given labeled data, supervisedmachine learning can help classify these messages. Scarcity of labeled datacauses poor performance in machine training. Can we reuse old tweets to trainclassifiers? How can we choose labeled tweets for training? Specifically, westudy the usefulness of labeled data of past events. Do labeled tweets indifferent language help? We observe the performance of our classifiers trainedusing different combinations of training sets obtained from past disasters. Weperform extensive experimentation on real crisis datasets and show that thepast labels are useful when both source and target events are of the same type(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),cross-language domain adaptation was useful, however, when for differentlanguages (e.g., Italian and English), the performance decreased.
arxiv-16500-5 | On the Influence of Momentum Acceleration on Online Learning | http://arxiv.org/pdf/1603.04136v2.pdf | author:Kun Yuan, Bicheng Ying, Ali H. Sayed category:math.OC cs.LG stat.ML published:2016-03-14 summary:The article examines in some detail the convergence rate andmean-square-error performance of momentum stochastic gradient methods in theconstant step-size case and slow adaptation regime. The results establish thatmomentum methods are equivalent to the standard stochastic gradient method witha re-scaled (larger) step-size value. The size of the re-scaling is determinedby the value of the momentum parameter. The equivalence result is establishedfor all time instants and not only in steady-state. The analysis is carried outfor general risk functions, and is not limited to quadratic risks. One notableconclusion is that the well-known benefits of momentum constructions fordeterministic optimization problems do not necessarily carry over to thestochastic (online) setting when adaptation becomes necessary and when the truegradient vectors are not known beforehand. The analysis also suggests a methodto retain some of the advantages of the momentum construction by employing adecaying momentum parameter, as opposed to a decaying step-size. In this way,the enhanced convergence rate during the initial stages of adaptation ispreserved without the often-observed degradation in MSD performance.
arxiv-16500-6 | Named Entity Recognition with Bidirectional LSTM-CNNs | http://arxiv.org/pdf/1511.08308v3.pdf | author:Jason P. C. Chiu, Eric Nichols category:cs.CL cs.LG cs.NE 68T50 I.2.7 published:2015-11-26 summary:Named entity recognition is a challenging task that has traditionallyrequired large amounts of knowledge in the form of feature engineering andlexicons to achieve high performance. In this paper, we present a novel neuralnetwork architecture that automatically detects word- and character-levelfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminatingthe need for most feature engineering. We also propose a novel method ofencoding partial lexicon matches in neural networks and compare it to existingapproaches. Extensive evaluation shows that, given only tokenized text andpublicly available word embeddings, our system is competitive on the CoNLL-2003dataset and surpasses the previously reported state of the art on the OntoNotes5.0 dataset by 2.13 F1 points. By using two lexicons from public sources, weestablish new states of the art with an F1 score of 91.62 on CoNLL-2003 and86.28 on OntoNotes, surpassing systems that employ heavy feature engineering,proprietary lexicons, and rich entity linking information.
arxiv-16500-7 | Regularization and Kernelization of the Maximin Correlation Approach | http://arxiv.org/pdf/1502.06105v2.pdf | author:Taehoon Lee, Taesup Moon, Seung Jean Kim, Sungroh Yoon category:cs.CV cs.LG published:2015-02-21 summary:Robust classification becomes challenging when each class consists ofmultiple subclasses. Examples include multi-font optical character recognitionand automated protein function prediction. In correlation-basednearest-neighbor classification, the maximin correlation approach (MCA)provides the worst-case optimal solution by minimizing the maximummisclassification risk through an iterative procedure. Despite the optimality,the original MCA has drawbacks that have limited its wide applicability inpractice. That is, the MCA tends to be sensitive to outliers, cannoteffectively handle nonlinearities in datasets, and suffers from having highcomputational complexity. To address these limitations, we propose an improvedsolution, named regularized maximin correlation approach (R-MCA). We firstreformulate MCA as a quadratically constrained linear programming (QCLP)problem, incorporate regularization by introducing slack variables in theprimal problem of the QCLP, and derive the corresponding Lagrangian dual. Thedual formulation enables us to apply the kernel trick to R-MCA so that it canbetter handle nonlinearities. Our experimental results demonstrate that theregularization and kernelization make the proposed R-MCA more robust andaccurate for various classification tasks than the original MCA. Furthermore,when the data size or dimensionality grows, R-MCA runs substantially faster bysolving either the primal or dual (whichever has a smaller variable dimension)of the QCLP.
arxiv-16500-8 | Learning a Predictable and Generative Vector Representation for Objects | http://arxiv.org/pdf/1603.08637v1.pdf | author:Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, Abhinav Gupta category:cs.CV published:2016-03-29 summary:What is a good vector representation of an object? We believe that it shouldbe generative in 3D, in the sense that it can produce new 3D objects; as wellas be predictable from 2D, in the sense that it can be perceived from 2Dimages. We propose a novel architecture, called the TL-embedding network, tolearn an embedding space with these properties. The network consists of twocomponents: (a) an autoencoder that ensures the representation is generative;and (b) a convolutional network that ensures the representation is predictable.This enables tackling a number of tasks including voxel prediction from 2Dimages and 3D model retrieval. Extensive experimental analysis demonstrates theusefulness and versatility of this embedding.
arxiv-16500-9 | Learning Image Matching by Simply Watching Video | http://arxiv.org/pdf/1603.06041v2.pdf | author:Gucan Long, Laurent Kneip, Jose M. Alvarez, Hongdong Li category:cs.CV published:2016-03-19 summary:This work presents an unsupervised learning based approach to the ubiquitouscomputer vision problem of image matching. We start from the insight that theproblem of frame-interpolation implicitly solves for inter-framecorrespondences. This permits the application of analysis-by-synthesis: wefirstly train and apply a Convolutional Neural Network for frame-interpolation,then obtain correspondences by inverting the learned CNN. The key benefitbehind this strategy is that the CNN for frame-interpolation can be trained inan unsupervised manner by exploiting the temporal coherency that is naturallycontained in real-world video sequences. The present model therefore learnsimage matching by simply watching videos. Besides a promise to be moregenerally applicable, the presented approach achieves surprising performancecomparable to traditional empirically designed methods.
arxiv-16500-10 | Towards an Automated Requirements-driven Development of Smart Cyber-Physical Systems | http://arxiv.org/pdf/1603.08636v1.pdf | author:Jiri Vinarek, Petr Hnetynka category:cs.SE cs.CL published:2016-03-29 summary:The Invariant Refinement Method for Self Adaptation (IRM-SA) is a designmethod targeting development of smart Cyber-Physical Systems (sCPS). It allowsfor a systematic translation of the system requirements into the systemarchitecture expressed as an ensemble-based component system (EBCS). However,since the requirements are captured using natural language, there exists thedanger of their misinterpretation due to natural language requirements'ambiguity, which could eventually lead to design errors. Thus, automation andvalidation of the design process is desirable. In this paper, we (i) analyzethe translation process of natural language requirements into the IRM-SA model,(ii) identify individual steps that can be automated and/or validated usingnatural language processing techniques, and (iii) propose suitable methods.
arxiv-16500-11 | Classification of Alzheimer's Disease using fMRI Data and Deep Learning Convolutional Neural Networks | http://arxiv.org/pdf/1603.08631v1.pdf | author:Saman Sarraf, Ghassem Tofighi category:cs.CV published:2016-03-29 summary:Over the past decade, machine learning techniques especially predictivemodeling and pattern recognition in biomedical sciences from drug deliverysystem to medical imaging has become one of the important methods which areassisting researchers to have deeper understanding of entire issue and to solvecomplex medical problems. Deep learning is power learning machine learningalgorithm in classification while extracting high-level features. In thispaper, we used convolutional neural network to classify Alzheimer's brain fromnormal healthy brain. The importance of classifying this kind of medical datais to potentially develop a predict model or system in order to recognize thetype disease from normal subjects or to estimate the stage of the disease.Classification of clinical data such as Alzheimer's disease has been alwayschallenging and most problematic part has been always selecting the mostdiscriminative features. Using Convolutional Neural Network (CNN) and thefamous architecture LeNet-5, we successfully classified functional MRI data ofAlzheimer's subjects from normal controls where the accuracy of test data ontrained data reached 96.85%. This experiment suggests us the shift and scaleinvariant features extracted by CNN followed by deep learning classification ismost powerful method to distinguish clinical data from healthy data in fMRI.This approach also enables us to expand our methodology to predict morecomplicated systems.
arxiv-16500-12 | Exact Subsampling MCMC | http://arxiv.org/pdf/1603.08232v2.pdf | author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.CO stat.ME stat.ML published:2016-03-27 summary:Speeding up Markov Chain Monte Carlo (MCMC) for datasets with manyobservations by data subsampling has recently received considerable attentionin the literature. Most of the proposed methods are approximate, and the onlyexact solution has been documented to be highly inefficient. We propose asimulation consistent subsampling method for estimating expectations of anyfunction of the parameters using a combination of MCMC subsampling and theimportance sampling correction for occasionally negative likelihood estimatesin Lyne et al. (2015). Our algorithm is based on first obtaining an unbiasedbut not necessarily positive estimate of the likelihood. The estimator uses asoft lower bound such that the likelihood estimate is positive with a highprobability, and computationally cheap control variables to lower variability.Second, we carry out a correlated pseudo marginal MCMC on the absolute value ofthe likelihood estimate. Third, the sign of the likelihood is corrected usingan importance sampling step that has low variance by construction. Weillustrate the usefulness of the method with two examples.
arxiv-16500-13 | Submodular Variational Inference for Network Reconstruction | http://arxiv.org/pdf/1603.08616v1.pdf | author:Lin Chen, Amin Karbasi, Forrest W Crawford category:cs.LG cs.DS cs.SI stat.ML published:2016-03-29 summary:In real-world and online social networks, individuals receive and transmitinformation in real time. Cascading information transmissions --- phone calls,text messages, social media posts --- may be understood as a realization of adiffusion process operating on the network, and its branching path can berepresented by a directed tree. One important feature of dynamic real-worlddiffusion processes is that the process may not traverse every edge in thenetwork on which it operates. When the network itself is unknown, the path ofthe diffusion process may reveal some, but not all, of the edges connectingnodes that have received the diffusing information. The networkreconstruction/inference problem is to estimate connections that are notrevealed by the diffusion processes. This problem naturally arises in a manydisciplines. Most of existing works on network reconstruction study thisproblem by deriving a likelihood function for the realized diffusion processgiven full knowledge of the network on which it operates, and attempting tofind the network topology that maximizes this likelihood. The major challengein this work is the intractability of the optimization problem. In this paper,we focus on the network reconstruction problem for a broad class of real-worlddiffusion processes, exemplified by a network diffusion scheme calledrespondent-driven sampling (RDS) that is widely used in epidemiology. We provethat under a reasonable model of network diffusion, the likelihood of anobserved RDS realization is a Bayesian log-submodular model. We propose anovel, accurate, and computationally efficient variational inference algorithmfor the network reconstruction problem under this model. In this algorithm, weallow for more flexibility for the possible deviation of the subjects' reportedtotal degrees in the underlying graphical structure from the true ones.
arxiv-16500-14 | Classiffication-based Financial Markets Prediction using Deep Neural Networks | http://arxiv.org/pdf/1603.08604v1.pdf | author:Matthew Dixon, Diego Klabjan, Jin Hoon Bang category:cs.LG cs.CE published:2016-03-29 summary:Deep neural networks (DNNs) are powerful types of artificial neural networks(ANNs) that use several hidden layers. They have recently gained considerableattention in the speech transcription and image recognition community(Krizhevsky et al., 2012) for their superior predictive properties includingrobustness to overfitting. However their application to algorithmic trading hasnot been previously researched, partly because of their computationalcomplexity. This paper describes the application of DNNs to predictingfinancial market movement directions. In particular we describe theconfiguration and training approach and then demonstrate their application tobacktesting a simple trading strategy over 43 different Commodity and FX futuremid-prices at 5-minute intervals. All results in this paper are generated usinga C++ implementation on the Intel Xeon Phi co-processor which is 11.4x fasterthan the serial version and a Python strategy backtesting environment both ofwhich are available as open source code written by the authors.
arxiv-16500-15 | Weakly Supervised Localization using Deep Feature Maps | http://arxiv.org/pdf/1603.00489v2.pdf | author:Archith J. Bency, Heesung Kwon, Hyungtae Lee, S. Karthikeyan, B. S. Manjunath category:cs.CV published:2016-03-01 summary:Object localization is an important computer vision problem with a variety ofapplications. The lack of large scale object-level annotations and the relativeabundance of image-level labels makes a compelling case for weak supervision inthe object localization task. Deep Convolutional Neural Networks are a class ofstate-of-the-art methods for the related problem of object recognition. In thispaper, we describe a novel object localization algorithm which usesclassification networks trained on only image labels. This weakly supervisedmethod leverages local spatial and semantic patterns captured in theconvolutional layers of classification networks. We propose an efficient beamsearch based approach to detect and localize multiple objects in images. Theproposed method significantly outperforms the state-of-the-art in standardobject localization data-sets with a 8 point increase in mAP scores.
arxiv-16500-16 | Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms | http://arxiv.org/pdf/1407.8339v6.pdf | author:Wei Chen, Yajun Wang, Yang Yuan, Qinshi Wang category:cs.LG published:2014-07-31 summary:We define a general framework for a large class of combinatorial multi-armedbandit (CMAB) problems, where subsets of base arms with unknown distributionsform super arms. In each round, a super arm is played and the base armscontained in the super arm are played and their outcomes are observed. Wefurther consider the extension in which more based arms could beprobabilistically triggered based on the outcomes of already triggered arms.The reward of the super arm depends on the outcomes of all played arms, and itonly needs to satisfy two mild assumptions, which allow a large class ofnonlinear reward instances. We assume the availability of an offline(\alpha,\beta)-approximation oracle that takes the means of the outcomedistributions of arms and outputs a super arm that with probability {\beta}generates an {\alpha} fraction of the optimal expected reward. The objective ofan online learning algorithm for CMAB is to minimize(\alpha,\beta)-approximation regret, which is the difference between the\alpha{\beta} fraction of the expected reward when always playing the optimalsuper arm, and the expected reward of playing super arms according to thealgorithm. We provide CUCB algorithm that achieves O(log n)distribution-dependent regret, where n is the number of rounds played, and wefurther provide distribution-independent bounds for a large class of rewardfunctions. Our regret analysis is tight in that it matches the bound of UCB1algorithm (up to a constant factor) for the classical MAB problem, and itsignificantly improves the regret bound in a earlier paper on combinatorialbandits with linear rewards. We apply our CMAB framework to two newapplications, probabilistic maximum coverage and social influence maximization,both having nonlinear reward structures. In particular, application to socialinfluence maximization requires our extension on probabilistically triggeredarms.
arxiv-16500-17 | The Conditional Lucas & Kanade Algorithm | http://arxiv.org/pdf/1603.08597v1.pdf | author:Chen-Hsuan Lin, Rui Zhu, Simon Lucey category:cs.CV published:2016-03-29 summary:The Lucas & Kanade (LK) algorithm is the method of choice for efficient denseimage and object alignment. The approach is efficient as it attempts to modelthe connection between appearance and geometric displacement through a linearrelationship that assumes independence across pixel coordinates. A drawback ofthe approach, however, is its generative nature. Specifically, its performanceis tightly coupled with how well the linear model can synthesize appearancefrom geometric displacement, even though the alignment task itself isassociated with the inverse problem. In this paper, we present a new approach,referred to as the Conditional LK algorithm, which: (i) directly learns linearmodels that predict geometric displacement as a function of appearance, and(ii) employs a novel strategy for ensuring that the generative pixelindependence assumption can still be taken advantage of. We demonstrate thatour approach exhibits superior performance to classical generative forms of theLK algorithm. Furthermore, we demonstrate its comparable performance tostate-of-the-art methods such as the Supervised Descent Method withsubstantially less training examples, as well as the unique ability to "swap"geometric warp functions without having to retrain from scratch. Finally, froma theoretical perspective, our approach hints at possible redundancies thatexist in current state-of-the-art methods for alignment that could be leveragedin vision systems of the future.
arxiv-16500-18 | Long-Range Motion Trajectories Extraction of Articulated Human Using Mesh Evolution | http://arxiv.org/pdf/1506.09075v3.pdf | author:Yuanyuan Wu, Xiaohai He, Byeongkeun Kang, Haiying Song, Truong Q. Nguyen category:cs.CV published:2015-06-30 summary:This letter presents a novel approach to extract reliable dense andlong-range motion trajectories of articulated human in a video sequence.Compared with existing approaches that emphasize temporal consistency of eachtracked point, we also consider the spatial structure of tracked points on thearticulated human. We treat points as a set of vertices, and build a trianglemesh to join them in image space. The problem of extracting long-range motiontrajectories is changed to the issue of consistency of mesh evolution overtime. First, self-occlusion is detected by a novel mesh-based method and anadaptive motion estimation method is proposed to initialize mesh betweensuccessive frames. Furthermore, we propose an iterative algorithm toefficiently adjust vertices of mesh for a physically plausible deformation,which can meet the local rigidity of mesh and silhouette constraints. Finally,we compare the proposed method with the state-of-the-art methods on a set ofchallenging sequences. Evaluations demonstrate that our method achievesfavorable performance in terms of both accuracy and integrity of extractedtrajectories.
arxiv-16500-19 | Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments | http://arxiv.org/pdf/1603.08594v1.pdf | author:Geetanjali Rakshit, Sagar Sontakke, Pushpak Bhattacharyya, Gholamreza Haffari category:cs.CL published:2016-03-29 summary:In this paper, we attempt to solve the problem of Prepositional Phrase (PP)attachments in English. The motivation for the work comes from NLP applicationslike Machine Translation, for which, getting the correct attachment ofprepositions is very crucial. The idea is to correct the PP-attachments for asentence with the help of alignments from parallel data in another language.The novelty of our work lies in the formulation of the problem into a dualdecomposition based algorithm that enforces agreement between the parse treesfrom two languages as a constraint. Experiments were performed on theEnglish-Hindi language pair and the performance improved by 10% over thebaseline, where the baseline is the attachment predicted by the MSTParser modeltrained for English.
arxiv-16500-20 | Exploring Local Context for Multi-target Tracking in Wide Area Aerial Surveillance | http://arxiv.org/pdf/1603.08592v1.pdf | author:Bor-Jeng Chen, Gerard Medioni category:cs.CV published:2016-03-28 summary:Tracking many vehicles in wide coverage aerial imagery is crucial forunderstanding events in a large field of view. Most approaches aim to associatedetections from frame differencing into tracks. However, slow or stoppedvehicles result in long-term missing detections and further cause trackingdiscontinuities. Relying merely on appearance clue to recover missingdetections is difficult as targets are extremely small and in grayscale. Inthis paper, we address the limitations of detection association methods bycoupling it with a local context tracker (LCT), which does not rely on motiondetections. On one hand, our LCT learns neighboring spatial relation and trackseach target in consecutive frames using graph optimization. It takes theadvantage of context constraints to avoid drifting to nearby targets. Wegenerate hypotheses from sparse and dense flow efficiently to keep solutionstractable. On the other hand, we use detection association strategy to extractshort tracks in batch processing. We explicitly handle merged detections bygenerating additional hypotheses from them. Our evaluation on wide area aerialimagery sequences shows significant improvement over state-of-the-art methods.
arxiv-16500-21 | Generalized Exponential Concentration Inequality for Rényi Divergence Estimation | http://arxiv.org/pdf/1603.08589v1.pdf | author:Shashank Singh, Barnabás Póczos category:cs.IT math.IT math.ST stat.ML stat.TH published:2016-03-28 summary:Estimating divergences in a consistent way is of great importance in manymachine learning tasks. Although this is a fundamental problem in nonparametricstatistics, to the best of our knowledge there has been no finite sampleexponential inequality convergence bound derived for any divergence estimators.The main contribution of our work is to provide such a bound for an estimatorof R\'enyi-$\alpha$ divergence for a smooth H\"older class of densities on the$d$-dimensional unit cube $[0, 1]^d$. We also illustrate our theoreticalresults with a numerical experiment.
arxiv-16500-22 | Exponential Concentration of a Density Functional Estimator | http://arxiv.org/pdf/1603.08584v1.pdf | author:Shashank Singh, Barnabás P óczos category:math.ST cs.IT math.IT stat.ML stat.TH published:2016-03-28 summary:We analyze a plug-in estimator for a large class of integral functionals ofone or more continuous probability densities. This class includes importantfamilies of entropy, divergence, mutual information, and their conditionalversions. For densities on the $d$-dimensional unit cube $[0,1]^d$ that lie ina $\beta$-H\"older smoothness class, we prove our estimator converges at therate $O \left( n^{-\frac{\beta}{\beta + d}} \right)$. Furthermore, we prove theestimator is exponentially concentrated about its mean, whereas most previousrelated results have proven only expected error bounds on estimators.
arxiv-16500-23 | Analysis of classifiers' robustness to adversarial perturbations | http://arxiv.org/pdf/1502.02590v4.pdf | author:Alhussein Fawzi, Omar Fawzi, Pascal Frossard category:cs.LG cs.CV stat.ML published:2015-02-09 summary:The goal of this paper is to analyze an intriguing phenomenon recentlydiscovered in deep networks, namely their instability to adversarialperturbations (Szegedy et. al., 2014). We provide a theoretical framework foranalyzing the robustness of classifiers to adversarial perturbations, and showfundamental upper bounds on the robustness of classifiers. Specifically, weestablish a general upper bound on the robustness of classifiers to adversarialperturbations, and then illustrate the obtained upper bound on the families oflinear and quadratic classifiers. In both cases, our upper bound depends on adistinguishability measure that captures the notion of difficulty of theclassification task. Our results for both classes imply that in tasks involvingsmall distinguishability, no classifier in the considered set will be robust toadversarial perturbations, even if a good accuracy is achieved. Our theoreticalframework moreover suggests that the phenomenon of adversarial instability isdue to the low flexibility of classifiers, compared to the difficulty of theclassification task (captured by the distinguishability). Moreover, we show theexistence of a clear distinction between the robustness of a classifier torandom noise and its robustness to adversarial perturbations. Specifically, theformer is shown to be larger than the latter by a factor that is proportionalto \sqrt{d} (with d being the signal dimension) for linear classifiers. Thisresult gives a theoretical explanation for the discrepancy between the tworobustness properties in high dimensional problems, which was empiricallyobserved in the context of neural networks. To the best of our knowledge, ourresults provide the first theoretical work that addresses the phenomenon ofadversarial instability recently observed for deep networks. Our analysis iscomplemented by experimental results on controlled and real-world data.
arxiv-16500-24 | Analysis of k-Nearest Neighbor Distances with Application to Entropy Estimation | http://arxiv.org/pdf/1603.08578v1.pdf | author:Shashank Singh, Barnabás Póczos category:math.ST cs.IT math.IT stat.ML stat.TH published:2016-03-28 summary:Estimating entropy and mutual information consistently is important for manymachine learning applications. The Kozachenko-Leonenko (KL) estimator(Kozachenko & Leonenko, 1987) is a widely used nonparametric estimator for theentropy of multivariate continuous random variables, as well as the basis ofthe mutual information estimator of Kraskov et al. (2004), perhaps the mostwidely used estimator of mutual information in this setting. Despite thepractical importance of these estimators, major theoretical questions regardingtheir finite-sample behavior remain open. This paper proves finite-samplebounds on the bias and variance of the KL estimator, showing that it achievesthe minimax convergence rate for certain classes of smooth functions. Inproving these bounds, we analyze finite-sample behavior of k-nearest neighbors(k-NN) distance statistics (on which the KL estimator is based). We deriveconcentration inequalities for k-NN distances and a general expectation boundfor statistics of k-NN distances, which may be useful for other analyses ofk-NN methods.
arxiv-16500-25 | Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net | http://arxiv.org/pdf/1511.06881v5.pdf | author:Fangting Xia, Peng Wang, Liang-Chieh Chen, Alan L. Yuille category:cs.CV cs.LG published:2015-11-21 summary:Parsing articulated objects, e.g. humans and animals, into semantic parts(e.g. body, head and arms, etc.) from natural images is a challenging andfundamental problem for computer vision. A big difficulty is the largevariability of scale and location for objects and their corresponding parts.Even limited mistakes in estimating scale and location will degrade the parsingoutput and cause errors in boundary details. To tackle these difficulties, wepropose a "Hierarchical Auto-Zoom Net" (HAZN) for object part parsing whichadapts to the local scales of objects and parts. HAZN is a sequence of two"Auto-Zoom Net" (AZNs), each employing fully convolutional networks thatperform two tasks: (1) predict the locations and scales of object instances(the first AZN) or their parts (the second AZN); (2) estimate the part scoresfor predicted object instance or part regions. Our model can adaptively "zoom"(resize) predicted image regions into their proper scales to refine theparsing. We conduct extensive experiments over the PASCAL part datasets on humans,horses, and cows. For humans, our approach significantly outperforms thestate-of-the-arts by 5% mIOU and is especially better at segmenting smallinstances and small parts. We obtain similar improvements for parsing cows andhorses over alternative methods. In summary, our strategy of first zooming intoobjects and then zooming into parts is very effective. It also enables us toprocess different regions of the image at different scales adaptively so that,for example, we do not need to waste computational resources scaling the entireimage.
arxiv-16500-26 | One-to-many face recognition with bilinear CNNs | http://arxiv.org/pdf/1506.01342v5.pdf | author:Aruni RoyChowdhury, Tsung-Yu Lin, Subhransu Maji, Erik Learned-Miller category:cs.CV published:2015-06-03 summary:The recent explosive growth in convolutional neural network (CNN) researchhas produced a variety of new architectures for deep learning. One intriguingnew architecture is the bilinear CNN (B-CNN), which has shown dramaticperformance gains on certain fine-grained recognition problems [15]. We applythis new CNN to the challenging new face recognition benchmark, the IARPA JanusBenchmark A (IJB-A) [12]. It features faces from a large number of identitiesin challenging real-world conditions. Because the face images were notidentified automatically using a computerized face detection system, it doesnot have the bias inherent in such a database. We demonstrate the performanceof the B-CNN model beginning from an AlexNet-style network pre-trained onImageNet. We then show results for fine-tuning using a moderate-sized andpublic external database, FaceScrub [17]. We also present results withadditional fine-tuning on the limited training data provided by the protocol.In each case, the fine-tuned bilinear model shows substantial improvements overthe standard CNN. Finally, we demonstrate how a standard CNN pre-trained on alarge face database, the recently released VGG-Face model [20], can beconverted into a B-CNN without any additional feature training. This B-CNNimproves upon the CNN performance on the IJB-A benchmark, achieving 89.5%rank-1 recall.
arxiv-16500-27 | Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation | http://arxiv.org/pdf/1603.08564v1.pdf | author:Satrajit Mukherjee, Bodhisattwa Prasad Majumder, Aritran Piplai, Swagatam Das category:cs.CV stat.ML published:2016-03-28 summary:The paper proposes a novel Kernelized image segmentation scheme for noisyimages that utilizes the concept of Smallest Univalue Segment AssimilatingNucleus (SUSAN) and incorporates spatial constraints by computing circularcolour map induced weights. Fuzzy damping coefficients are obtained for eachnucleus or center pixel on the basis of the corresponding weighted SUSAN areavalues, the weights being equal to the inverse of the number of horizontal andvertical moves required to reach a neighborhood pixel from the center pixel.These weights are used to vary the contributions of the different nuclei in theKernel based framework. The paper also presents an edge quality metric obtainedby fuzzy decision based edge candidate selection and final computation of theblurriness of the edges after their selection. The inability of existingalgorithms to preserve edge information and structural details in theirsegmented maps necessitates the computation of the edge quality factor (EQF)for all the competing algorithms. Qualitative and quantitative analysis havebeen rendered with respect to state-of-the-art algorithms and for images riddenwith varying types of noises. Speckle noise ridden SAR images and Rician noiseridden Magnetic Resonance Images have also been considered for evaluating theeffectiveness of the proposed algorithm in extracting important segmentationinformation.
arxiv-16500-28 | Unsupervised Learning using Sequential Verification for Action Recognition | http://arxiv.org/pdf/1603.08561v1.pdf | author:Ishan Misra, C. Lawrence Zitnick, Martial Hebert category:cs.CV cs.AI cs.LG published:2016-03-28 summary:In this paper, we consider the problem of learning a visual representationfrom the raw spatiotemporal signals in videos for use in action recognition.Our representation is learned without supervision from semantic labels. Weformulate it as an unsupervised sequential verification task, i.e., wedetermine whether a sequence of frames from a video is in the correct temporalorder. With this simple task and no semantic labels, we learn a powerfulunsupervised representation using a Convolutional Neural Network (CNN). Therepresentation contains complementary information to that learned fromsupervised image datasets like ImageNet. Qualitative results show that ourmethod captures information that is temporally varying, such as human pose.When used as pre-training for action recognition, our method gives significantgains over learning without external data on benchmark datasets like UCF101 andHMDB51. Our method can also be combined with supervised representations toprovide an additional boost in accuracy for action recognition. Finally, toquantify its sensitivity to human pose, we show results for human poseestimation on the FLIC dataset that are competitive with approaches usingsignificantly more supervised training data.
arxiv-16500-29 | Genetic cellular neural networks for generating three-dimensional geometry | http://arxiv.org/pdf/1603.08551v1.pdf | author:Hugo Martay category:cs.NE cs.GR 92B20 published:2016-03-28 summary:There are a number of ways to procedurally generate interestingthree-dimensional shapes, and a method where a cellular neural network iscombined with a mesh growth algorithm is presented here. The aim is to create ashape from a genetic code in such a way that a crude search can findinteresting shapes. Identical neural networks are placed at each vertex of amesh which can communicate with neural networks on neighboring vertices. Theoutput of the neural networks determine how the mesh grows, allowinginteresting shapes to be produced emergently, mimicking some of the complexityof biological organism development. Since the neural networks' parameters canbe freely mutated, the approach is amenable for use in a genetic algorithm.
arxiv-16500-30 | Colorful Image Colorization | http://arxiv.org/pdf/1603.08511v1.pdf | author:Richard Zhang, Phillip Isola, Alexei A. Efros category:cs.CV published:2016-03-28 summary:Given a grayscale photograph as input, this paper attacks the problem ofhallucinating a {\em plausible} color version of the photograph. This problemis clearly underconstrained, so previous approaches have either relied onsignificant user interaction or resulted in desaturated colorizations. Wepropose a fully automatic approach that produces vibrant and realisticcolorizations. We embrace the underlying uncertainty of the problem by posingit as a classification task and explore using class-rebalancing at trainingtime to increase the diversity of colors in the result. The system isimplemented as a feed-forward operation in a CNN at test time and is trained onover a million color images. We evaluate our algorithm using a "colorizationTuring test", asking human subjects to choose between a generated and groundtruth color image. Our method successfully fools humans 20\% of the time,significantly higher than previous methods.
arxiv-16500-31 | Generating Visual Explanations | http://arxiv.org/pdf/1603.08507v1.pdf | author:Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell category:cs.CV cs.AI cs.CL published:2016-03-28 summary:Clearly explaining a rationale for a classification decision to an end-usercan be as important as the decision itself. Existing approaches for deep visualrecognition are generally opaque and do not output any justification text;contemporary vision-language models can describe image content but fail to takeinto account class-discriminative image aspects which justify visualpredictions. We propose a new model that focuses on the discriminatingproperties of the visible object, jointly predicts a class label, and explainswhy the predicted label is appropriate for the image. We propose a novel lossfunction based on sampling and reinforcement learning that learns to generatesentences that realize a global sentence property, such as class specificity.Our results on a fine-grained bird species classification dataset show that ourmodel is able to generate explanations which are not only consistent with animage but also more discriminative than descriptions produced by existingcaptioning methods.
arxiv-16500-32 | Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation | http://arxiv.org/pdf/1603.08486v1.pdf | author:Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, Ronald M Summers category:cs.CV published:2016-03-28 summary:Despite the recent advances in automatically describing image contents, theirapplications have been mostly limited to image caption datasets containingnatural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deeplearning model to efficiently detect a disease from an image and annotate itscontexts (e.g., location, severity and the affected organs). We employ apublicly available radiology dataset of chest x-rays and their reports, and useits image annotations to mine disease names to train convolutional neuralnetworks (CNNs). In doing so, we adopt various regularization techniques tocircumvent the large normal-vs-diseased cases bias. Recurrent neural networks(RNNs) are then trained to describe the contexts of a detected disease, basedon the deep CNN features. Moreover, we introduce a novel approach to use theweights of the already trained pair of CNN/RNN on the domain-specificimage/text dataset, to infer the joint image/text contexts for composite imagelabeling. Significantly improved image annotation results are demonstratedusing the recurrent neural cascade model by taking the joint image/textcontexts into account.
arxiv-16500-33 | Estimating Mixture Models via Mixtures of Polynomials | http://arxiv.org/pdf/1603.08482v1.pdf | author:Sida I. Wang, Arun Tejasvi Chaganty, Percy Liang category:stat.ML cs.LG published:2016-03-28 summary:Mixture modeling is a general technique for making any simple model moreexpressive through weighted combination. This generality and simplicity in partexplains the success of the Expectation Maximization (EM) algorithm, in whichupdates are easy to derive for a wide class of mixture models. However, thelikelihood of a mixture model is non-convex, so EM has no known globalconvergence guarantees. Recently, method of moments approaches offer globalguarantees for some mixture models, but they do not extend easily to the rangeof mixture models that exist. In this work, we present Polymom, an unifyingframework based on method of moments in which estimation procedures are easilyderivable, just as in EM. Polymom is applicable when the moments of a singlemixture component are polynomials of the parameters. Our key observation isthat the moments of the mixture model are a mixture of these polynomials, whichallows us to cast estimation as a Generalized Moment Problem. We solve itsrelaxations using semidefinite optimization, and then extract parameters usingideas from computer algebra. This framework allows us to draw insights andapply tools from convex optimization, computer algebra and the theory ofmoments to study problems in statistical estimation.
arxiv-16500-34 | Deep Embedding for Spatial Role Labeling | http://arxiv.org/pdf/1603.08474v1.pdf | author:Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens category:cs.CL cs.CV cs.LG cs.NE published:2016-03-28 summary:This paper introduces the visually informed embedding of word (VIEW), acontinuous vector representation for a word extracted from a deep neural modeltrained using the Microsoft COCO data set to forecast the spatial arrangementsbetween visual objects, given a textual description. The model is composed of adeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory(LSTM) network, the latter being preceded by an embedding layer. The VIEW isapplied to transferring multimodal background knowledge to Spatial RoleLabeling (SpRL) algorithms, which recognize spatial relations between objectsmentioned in the text. This work also contributes with a new method to selectcomplementary features and a fine-tuning method for MLP that improves the $F1$measure in classifying the words into spatial roles. The VIEW is evaluated withthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.
arxiv-16500-35 | Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm | http://arxiv.org/pdf/1602.06929v2.pdf | author:Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford category:cs.LG cs.DS cs.NE stat.ML published:2016-02-22 summary:This work provides improved guarantees for streaming principle componentanalysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampledindependently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for$\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-timesingle-pass streaming algorithm for estimating the top eigenvector of $\Sigma$.The algorithm nearly matches (and in certain cases improves upon) the accuracyobtained by the standard batch method that computes top eigenvector of theempirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by thematrix Bernstein inequality. Moreover, to achieve constant accuracy, ouralgorithm improves upon the best previous known sample complexities ofstreaming algorithms by either a multiplicative factor of $O(d)$ or$1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the toptwo eigenvalues of $\Sigma$. These results are achieved through a novel analysis of the classic Oja'salgorithm, one of the oldest and most popular algorithms for streaming PCA. Inparticular, this work shows that simply picking a random initial point $w_0$and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices toaccurately estimate the top eigenvector, with a suitable choice of $\eta_i$. Webelieve our result sheds light on how to efficiently perform streaming PCA bothin theory and in practice and we hope that our analysis may serve as the basisfor analyzing many variants and extensions of streaming PCA.
arxiv-16500-36 | Learning-based Compressive Subsampling | http://arxiv.org/pdf/1510.06188v3.pdf | author:Luca Baldassarre, Yen-Huan Li, Jonathan Scarlett, Baran Gözcü, Ilija Bogunovic, Volkan Cevher category:cs.IT cs.LG math.IT stat.ML published:2015-10-21 summary:The problem of recovering a structured signal $\mathbf{x} \in \mathbb{C}^p$from a set of dimensionality-reduced linear measurements $\mathbf{b} = \mathbf{A}\mathbf {x}$ arises in a variety of applications, such as medical imaging,spectroscopy, Fourier optics, and computerized tomography. Due to computationaland storage complexity or physical constraints imposed by the problem, themeasurement matrix $\mathbf{A} \in \mathbb{C}^{n \times p}$ is often of theform $\mathbf{A} = \mathbf{P}_{\Omega}\boldsymbol{\Psi}$ for some orthonormalbasis matrix $\boldsymbol{\Psi}\in \mathbb{C}^{p \times p}$ and subsamplingoperator $\mathbf{P}_{\Omega}: \mathbb{C}^{p} \rightarrow \mathbb{C}^{n}$ thatselects the rows indexed by $\Omega$. This raises the fundamental question ofhow best to choose the index set $\Omega$ in order to optimize the recoveryperformance. Previous approaches to addressing this question rely onnon-uniform \emph{random} subsampling using application-specific knowledge ofthe structure of $\mathbf{x}$. In this paper, we instead take a principledlearning-based approach in which a \emph{fixed} index set is chosen based on aset of training signals $\mathbf{x}_1,\dotsc,\mathbf{x}_m$. We formulatecombinatorial optimization problems seeking to maximize the energy captured inthese signals in an average-case or worst-case sense, and we show that thesecan be efficiently solved either exactly or approximately via theidentification of modularity and submodularity structures. We provide bothdeterministic and statistical theoretical guarantees showing how the resultingmeasurement matrices perform on signals differing from the training signals,and we provide numerical examples showing our approach to be effective on avariety of data sets.
arxiv-16500-37 | Provable approximation properties for deep neural networks | http://arxiv.org/pdf/1509.07385v3.pdf | author:Uri Shaham, Alexander Cloninger, Ronald R. Coifman category:stat.ML cs.LG cs.NE published:2015-09-24 summary:We discuss approximation of functions using deep neural nets. Given afunction $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, weconstruct a sparsely-connected depth-4 neural network and bound its error inapproximating $f$. The size of the network depends on dimension and curvatureof the manifold $\Gamma$, the complexity of $f$, in terms of its waveletdescription, and only weakly on the ambient dimension $m$. Essentially, ournetwork computes wavelet functions, which are computed from Rectified LinearUnits (ReLU)
arxiv-16500-38 | Sparse Activity and Sparse Connectivity in Supervised Learning | http://arxiv.org/pdf/1603.08367v1.pdf | author:Markus Thom, Günther Palm category:cs.LG cs.CG cs.CV cs.NE published:2016-03-28 summary:Sparseness is a useful regularizer for learning in a wide range ofapplications, in particular in neural networks. This paper proposes a modeltargeted at classification tasks, where sparse activity and sparse connectivityare used to enhance classification capabilities. The tool for achieving this isa sparseness-enforcing projection operator which finds the closest vector witha pre-defined sparseness for any given vector. In the theoretical part of thispaper, a comprehensive theory for such a projection is developed. Inconclusion, it is shown that the projection is differentiable almost everywhereand can thus be implemented as a smooth neuronal transfer function. The entiremodel can hence be tuned end-to-end using gradient-based methods. Experimentson the MNIST database of handwritten digits show that classificationperformance can be boosted by sparse activity or sparse connectivity. With acombination of both, performance can be significantly better compared toclassical non-sparse approaches.
arxiv-16500-39 | Temporally coherent 4D reconstruction of complex dynamic scenes | http://arxiv.org/pdf/1603.03381v2.pdf | author:Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton category:cs.CV published:2016-03-10 summary:This paper presents an approach for reconstruction of 4D temporally coherentmodels of complex dynamic scenes. No prior knowledge is required of scenestructure or camera calibration allowing reconstruction from multiple movingcameras. Sparse-to-dense temporal correspondence is integrated with jointmulti-view segmentation and reconstruction to obtain a complete 4Drepresentation of static and dynamic objects. Temporal coherence is exploitedto overcome visual ambiguities resulting in improved reconstruction of complexscenes. Robust joint segmentation and reconstruction of dynamic objects isachieved by introducing a geodesic star convexity constraint. Comparativeevaluation is performed on a variety of unstructured indoor and outdoor dynamicscenes with hand-held cameras and multiple people. This demonstratesreconstruction of complete temporally coherent 4D scene models with improvednonrigid object segmentation and shape reconstruction.
arxiv-16500-40 | GPU-Based Fuzzy C-Means Clustering Algorithm for Image Segmentation | http://arxiv.org/pdf/1601.00072v3.pdf | author:Mishal Almazrooie, Mogana Vadiveloo, Rosni Abdullah category:cs.DC cs.CV published:2016-01-01 summary:In this paper, a fast and practical GPU-based implementation of FuzzyC-Means(FCM) clustering algorithm for image segmentation is proposed. First, anextensive analysis is conducted to study the dependency among the image pixelsin the algorithm for parallelization. The proposed GPU-based FCM has beentested on digital brain simulated dataset to segment white matter(WM), graymatter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The executiontime of the sequential FCM is 519 seconds for an image dataset with the size of1MB. While the proposed GPU-based FCM requires only 2.33 seconds for thesimilar size of image dataset. An estimated 245-fold speedup is measured forthe data size of 40 KB on a CUDA device that has 448 processors.
arxiv-16500-41 | Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes | http://arxiv.org/pdf/1603.08342v1.pdf | author:Łukasz P. Olech, Mariusz Paradowski category:cs.LG cs.CV published:2016-03-28 summary:A hierarchical clustering algorithm based on Gaussian mixture model ispresented. The key difference to regular hierarchical mixture models is theability to store objects in both terminal and nonterminal nodes. Upper levelsof the hierarchy contain sparsely distributed objects, while lower levelscontain densely represented ones. As it was shown by experiments, this abilityhelps in noise detection (modelling). Furthermore, compared to regularhierarchical mixture model, the presented method generates more compactdendrograms with higher quality measured by adopted F-measure.
arxiv-16500-42 | Continuous Stereo Matching using Local Expansion Moves | http://arxiv.org/pdf/1603.08328v1.pdf | author:Tatsunori Taniai, Yasuyuki Matsushita, Yoichi Sato, Takeshi Naemura category:cs.CV published:2016-03-28 summary:We present an accurate and efficient stereo matching method using localexpansion moves, a new move making scheme using graph cuts. The local expansionmoves are presented as many alpha-expansions defined for small grid regions.The local expansion moves extend the traditional expansion moves by two ways:localization and spatial propagation. By localization, we use differentcandidate alpha-labels according to the locations of local alpha-expansions. Byspatial propagation, we design our local alpha-expansions to propagatecurrently assigned labels for nearby regions. With this localization andspatial propagation, our method can efficiently infer Markov random fieldmodels with a huge or continuous label space using a randomized search scheme.Our local expansion move method has several advantages over previous approachesthat are based on fusion moves or belief propagation; it produces submodularmoves deriving a subproblem optimality; it helps find good, smooth, piecewiselinear disparity maps; it is suitable for parallelization; it can usecost-volume filtering techniques for accelerating the matching costcomputations. Our method is evaluated using the Middlebury stereo benchmark andshown to have the best performance in sub-pixel accuracy.
arxiv-16500-43 | Hierarchy of Groups Evaluation Using Different F-score Variants | http://arxiv.org/pdf/1603.08323v1.pdf | author:Michał Spytkowski, Łukasz P. Olech, Halina Kwaśnicka category:cs.CV published:2016-03-28 summary:The paper presents a cursory examination of clustering, focusing on a rarelyexplored field of hierarchy of clusters. Based on this, a short discussion ofclustering quality measures is presented and the F-score measure is examinedmore deeply. As there are no attempts to assess the quality for hierarchies ofclusters, three variants of the F-Score based index are presented: classic,hierarchical and partial order. The partial order index is the authors'approach to the subject. Conducted experiments show the properties of theconsidered measures. In conclusions, the strong and weak sides of each variantare presented.
arxiv-16500-44 | Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention | http://arxiv.org/pdf/1603.08321v1.pdf | author:Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li, Zhengqi Wen category:cs.CV cs.CL cs.LG published:2016-03-28 summary:This paper focuses on two key problems for audio-visual emotion recognitionin the video. One is the audio and visual streams temporal alignment forfeature level fusion. The other one is locating and re-weighting the perceptionattentions in the whole audio-visual stream for better recognition. The LongShort Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the mainclassification architecture. Firstly, soft attention mechanism aligns the audioand visual streams. Secondly, seven emotion embedding vectors, which arecorresponding to each classification emotion type, are added to locate theperception attentions. The locating and re-weighting process is also based onthe soft attention mechanism. The experiment results on EmotiW2015 dataset andthe qualitative analysis show the efficiency of the proposed two techniques.
arxiv-16500-45 | LSTM-based Deep Learning Models for Non-factoid Answer Selection | http://arxiv.org/pdf/1511.04108v4.pdf | author:Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou category:cs.CL cs.LG published:2015-11-12 summary:In this paper, we apply a general deep learning (DL) framework for the answerselection task, which does not depend on manually defined features orlinguistic tools. The basic framework is to build the embeddings of questionsand answers based on bidirectional long short-term memory (biLSTM) models, andmeasure their closeness by cosine similarity. We further extend this basicmodel in two directions. One direction is to define a more compositerepresentation for questions and answers by combining convolutional neuralnetwork with the basic framework. The other direction is to utilize a simplebut efficient attention mechanism in order to generate the answerrepresentation according to the question context. Several variations of modelsare provided. The models are examined by two datasets, including TREC-QA andInsuranceQA. Experimental results demonstrate that the proposed modelssubstantially outperform several strong baselines.
arxiv-16500-46 | Non-Greedy L21-Norm Maximization for Principal Component Analysis | http://arxiv.org/pdf/1603.08293v1.pdf | author:Feiping Nie, Heng Huang category:cs.LG published:2016-03-28 summary:Principal Component Analysis (PCA) is one of the most important unsupervisedmethods to handle high-dimensional data. However, due to the high computationalcomplexity of its eigen decomposition solution, it hard to apply PCA to thelarge-scale data with high dimensionality. Meanwhile, the squared L2-norm basedobjective makes it sensitive to data outliers. In recent research, the L1-normmaximization based PCA method was proposed for efficient computation and beingrobust to outliers. However, this work used a greedy strategy to solve theeigen vectors. Moreover, the L1-norm maximization based objective may not bethe correct robust PCA formulation, because it loses the theoretical connectionto the minimization of data reconstruction error, which is one of the mostimportant intuitions and goals of PCA. In this paper, we propose to maximizethe L21-norm based robust PCA objective, which is theoretically connected tothe minimization of reconstruction error. More importantly, we propose theefficient non-greedy optimization algorithms to solve our objective and themore general L21-norm maximization problem with theoretically guaranteedconvergence. Experimental results on real world data sets show theeffectiveness of the proposed method for principal component analysis.
arxiv-16500-47 | Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems | http://arxiv.org/pdf/1512.08756v4.pdf | author:Colin Raffel, Daniel P. W. Ellis category:cs.LG cs.NE published:2015-12-29 summary:We propose a simplified model of attention which is applicable tofeed-forward neural networks and demonstrate that the resulting model can solvethe synthetic "addition" and "multiplication" long-term memory problems forsequence lengths which are both longer and more widely varying than the bestpublished results for these tasks.
arxiv-16500-48 | Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing | http://arxiv.org/pdf/1603.08270v1.pdf | author:Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, Dharmendra S. Modha category:cs.NE published:2016-03-28 summary:Deep networks are now able to achieve human-level performance on a broadspectrum of recognition tasks. Independently, neuromorphic computing has nowdemonstrated unprecedented energy-efficiency through a new chip architecturebased on spiking neurons, low precision synapses, and a scalable communicationnetwork. Here, we demonstrate that neuromorphic computing, despite its novelarchitectural primitives, can implement deep convolution networks that i)approach state-of-the-art classification accuracy across 8 standard datasets,encompassing vision and speech, ii) perform inference while preserving thehardware's underlying energy-efficiency and high throughput, running on theaforementioned datasets at between 1100 and 2300 frames per second and usingbetween 25 and 325 mW (effectively > 5000 frames / sec / W) and iii) can bespecified and trained using backpropagation with the same ease-of-use ascontemporary deep learning. For the first time, the algorithmic power of deeplearning can be merged with the efficiency of neuromorphic processors, bringingthe promise of embedded, intelligent, brain-inspired computing one step closer.
arxiv-16500-49 | Towards Machine Intelligence | http://arxiv.org/pdf/1603.08262v1.pdf | author:Kamil Rocki category:cs.AI cs.LG cs.NE published:2016-03-27 summary:There exists a theory of a single general-purpose learning algorithm whichcould explain the principles of its operation. This theory assumes that thebrain has some initial rough architecture, a small library of simple innatecircuits which are prewired at birth and proposes that all significant mentalalgorithms can be learned. Given current understanding and observations, thispaper reviews and lists the ingredients of such an algorithm from botharchitectural and functional perspectives.
arxiv-16500-50 | Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyperspectral Images | http://arxiv.org/pdf/1407.4420v2.pdf | author:Fei Zhu, Paul Honeine, Maya Kallas category:cs.CV cs.IT cs.LG cs.NE math.IT stat.ML published:2014-07-16 summary:The nonnegative matrix factorization (NMF) is widely used in signal and imageprocessing, including bio-informatics, blind source separation andhyperspectral image analysis in remote sensing. A great challenge arises whendealing with a nonlinear formulation of the NMF. Within the framework of kernelmachines, the models suggested in the literature do not allow therepresentation of the factorization matrices, which is a fallout of the curseof the pre-image. In this paper, we propose a novel kernel-based model for theNMF that does not suffer from the pre-image problem, by investigating theestimation of the factorization matrices directly in the input space. Fordifferent kernel functions, we describe two schemes for iterative algorithms:an additive update rule based on a gradient descent scheme and a multiplicativeupdate rule in the same spirit as in the Lee and Seung algorithm. Within theproposed framework, we develop several extensions to incorporate constraints,including sparseness, smoothness, and spatial regularization with atotal-variation-like penalty. The effectiveness of the proposed method isdemonstrated with the problem of unmixing hyperspectral images, usingwell-known real images and results with state-of-the-art techniques.
arxiv-16500-51 | DeLight-Net: Decomposing Reflectance Maps into Specular Materials and Natural Illumination | http://arxiv.org/pdf/1603.08240v1.pdf | author:Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Luc Van Gool, Tinne Tuytelaars category:cs.CV published:2016-03-27 summary:In this paper we are extracting surface reflectance and natural environmentalillumination from a reflectance map, i.e. from a single 2D image of a sphere ofone material under one illumination. This is a notoriously difficult problem,yet key to various re-rendering applications. With the recent advances inestimating reflectance maps from 2D images their further decomposition hasbecome increasingly relevant. To this end, we propose a Convolutional Neural Network (CNN) architecture toreconstruct both material parameters (i.e. Phong) as well as illumination (i.e.high-resolution spherical illumination maps), that is solely trained onsynthetic data. We demonstrate that decomposition of synthetic as well as realphotographs of reflectance maps, both in High Dynamic Range (HDR), and, for thefirst time, on Low Dynamic Range (LDR) as well. Results are compared toprevious approaches quantitatively as well as qualitatively in terms ofre-renderings where illumination, material, view or shape are changed.
arxiv-16500-52 | U-CATCH: Using Color ATtribute of image patCHes in binary descriptors | http://arxiv.org/pdf/1603.04408v2.pdf | author:Ozgur Yilmaz, Alisher Abdulkhaev category:cs.CV published:2016-03-14 summary:In this study, we propose a simple yet very effective method for extractingcolor information through binary feature description framework. Our methodexpands the dimension of binary comparisons into RGB and YCbCr spaces, showingmore than 100% matching improve ment compared to non-color binary descriptorsfor a wide range of hard-to-match cases. The proposed method is general and canbe applied to any binary descriptor to make it color sensitive. It is fasterthan classical binary descriptors for RGB sampling due to the abandonment ofgrayscale conversion and has almost identical complexity (insignificantcompared to smoothing operation) for YCbCr sampling.
arxiv-16500-53 | Evolution of active categorical image classification via saccadic eye movement | http://arxiv.org/pdf/1603.08233v1.pdf | author:Randal S. Olson, Jason H. Moore, Christoph Adami category:cs.CV cs.LG cs.NE published:2016-03-27 summary:Pattern recognition and classification is a central concern for moderninformation processing systems. In particular, one key challenge to image andvideo classification has been that the computational cost of image processingscales linearly with the number of pixels in the image or video. Here wepresent an intelligent machine (the "active categorical classifier," or ACC)that is inspired by the saccadic movements of the eye, and is capable ofclassifying images by selectively scanning only a portion of the image. Weharness evolutionary computation to optimize the ACC on the MNIST hand-writtendigit classification task, and provide a proof-of-concept that the ACC works onnoisy multi-class data. We further analyze the ACC and demonstrate its abilityto classify images after viewing only a fraction of the pixels, and provideinsight on future research paths to further improve upon the ACC presentedhere.
arxiv-16500-54 | Human Pose Estimation using Deep Consensus Voting | http://arxiv.org/pdf/1603.08212v1.pdf | author:Ita Lifshitz, Ethan Fetaya, Shimon Ullman category:cs.CV cs.LG published:2016-03-27 summary:In this paper we consider the problem of human pose estimation from a singlestill image. We propose a novel approach where each location in the image votesfor the position of each keypoint using a convolutional neural net. The votingscheme allows us to utilize information from the whole image, rather than relyon a sparse set of keypoint locations. Using dense, multi-target votes, notonly produces good keypoint predictions, but also enables us to computeimage-dependent joint keypoint probabilities by looking at consensus voting.This differs from most previous methods where joint probabilities are learnedfrom relative keypoint locations and are independent of the image. We finallycombine the keypoints votes and joint probabilities in order to identify theoptimal pose configuration. We show our competitive performance on the MPIIHuman Pose and Leeds Sports Pose datasets.
arxiv-16500-55 | A Dual-Source Approach for 3D Pose Estimation from a Single Image | http://arxiv.org/pdf/1509.06720v2.pdf | author:Hashim Yasin, Umar Iqbal, Björn Krüger, Andreas Weber, Juergen Gall category:cs.CV published:2015-09-22 summary:One major challenge for 3D pose estimation from a single RGB image is theacquisition of sufficient training data. In particular, collecting largeamounts of training data that contain unconstrained images and are annotatedwith accurate 3D poses is infeasible. We therefore propose to use twoindependent training sources. The first source consists of images withannotated 2D poses and the second source consists of accurate 3D motion capturedata. To integrate both sources, we propose a dual-source approach thatcombines 2D pose estimation with efficient and robust 3D pose retrieval. In ourexperiments, we show that our approach achieves state-of-the-art results and iseven competitive when the skeleton structure of the two sources differsubstantially.
arxiv-16500-56 | 3DMatch: Learning the Matching of Local 3D Geometry in Range Scans | http://arxiv.org/pdf/1603.08182v1.pdf | author:Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao category:cs.CV published:2016-03-27 summary:Establishing correspondences between 3D geometries is essential to a largevariety of graphics and vision applications, including 3D reconstruction,localization, and shape matching. Despite significant progress, geometricmatching on real-world 3D data is still a challenging task due to the noisy,low-resolution, and incomplete nature of scanning data. These difficultieslimit the performance of current state-of-art methods which are typically basedon histograms over geometric properties. In this paper, we introduce 3DMatch, adata-driven local feature learner that jointly learns a geometric featurerepresentation and an associated metric function from a large collection ofreal-world scanning data. We represent 3D geometry using accumulated distancefields around key-point locations. This representation is suited to handlenoisy and partial scanning data, and concurrently supports deep learning withconvolutional neural networks directly in 3D. To train the networks, we proposea way to automatically generate correspondence labels for deep learning byleveraging existing RGB-D reconstruction algorithms. In our results, wedemonstrate that we are able to outperform state-of-the-art approaches by asignificant margin. In addition, we show the robustness of our descriptor in apurely geometric sparse bundle adjustment pipeline for 3D reconstruction.
arxiv-16500-57 | MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification | http://arxiv.org/pdf/1603.00968v2.pdf | author:Ye Zhang, Stephen Roller, Byron Wallace category:cs.CL published:2016-03-03 summary:We introduce a novel, simple convolution neural network (CNN) architecture -multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets ofword embeddings for sentence classification. MGNC-CNN extracts features frominput embedding sets independently and then joins these at the penultimatelayer in the network to form a final feature vector. We then adopt a groupregularization strategy that differentially penalizes weights associated withthe subcomponents generated from the respective embedding sets. This model ismuch simpler than comparable alternative architectures and requiressubstantially less training time. Furthermore, it is flexible in that it doesnot require input word embeddings to be of the same dimensionality. We showthat MGNC-CNN consistently outperforms baseline models.
arxiv-16500-58 | Regularization Parameter Selection for a Bayesian Multi-Level Group Lasso Regression Model with Application to Imaging Genomics | http://arxiv.org/pdf/1603.08163v1.pdf | author:Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance category:stat.ML stat.AP stat.CO published:2016-03-27 summary:We investigate the choice of tuning parameters for a Bayesian multi-levelgroup lasso model developed for the joint analysis of neuroimaging and geneticdata. The regression model we consider relates multivariate phenotypesconsisting of brain summary measures (volumetric and cortical thickness values)to single nucleotide polymorphism (SNPs) data and imposes penalization at twonested levels, the first corresponding to genes and the second corresponding toSNPs. Associated with each level in the penalty is a tuning parameter whichcorresponds to a hyperparameter in the hierarchical Bayesian formulation.Following previous work on Bayesian lassos we consider the estimation of tuningparameters through either hierarchical Bayes based on hyperpriors and Gibbssampling or through empirical Bayes based on maximizing the marginal likelihoodusing a Monte Carlo EM algorithm. For the specific model under consideration wefind that these approaches can lead to severe overshrinkage of the regressionparameter estimates in the high-dimensional setting or when the genetic effectsare weak. We demonstrate these problems through simulation examples and studyan approximation to the marginal likelihood which sheds light on the cause ofthis problem. We then suggest an alternative approach based on the widelyapplicable information criterion (WAIC), an asymptotic approximation toleave-one-out cross-validation that can be computed conveniently within an MCMCframework.
arxiv-16500-59 | VolumeDeform: Real-time Volumetric Non-rigid Reconstruction | http://arxiv.org/pdf/1603.08161v1.pdf | author:Matthias Innmann, Michael Zollhöfer, Matthias Nießner, Christian Theobalt, Marc Stamminger category:cs.CV published:2016-03-27 summary:We present a novel approach for the reconstruction of dynamic geometricshapes using a single hand-held consumer-grade RGB-D sensor at real-time rates.Our method does not require a pre-defined shape template to start with andbuilds up the scene model from scratch during the scanning process. Geometryand motion are parameterized in a unified manner by a volumetric representationthat encodes a distance field of the surface geometry as well as the non-rigidspace deformation. Motion tracking is based on a set of extracted sparse colorfeatures in combination with a dense depth-based constraint formulation. Thisenables accurate tracking and drastically reduces drift inherent to standardmodel-to-depth alignment. We cast finding the optimal deformation of space as anon-linear regularized variational optimization problem by enforcing localsmoothness and proximity to the input constraints. The problem is tackled inreal-time at the camera's capture rate using a data-parallel flip-flopoptimization strategy. Our results demonstrate robust tracking even for fastmotion and scenes that lack geometric features.
arxiv-16500-60 | Perceptual Losses for Real-Time Style Transfer and Super-Resolution | http://arxiv.org/pdf/1603.08155v1.pdf | author:Justin Johnson, Alexandre Alahi, Li Fei-Fei category:cs.CV cs.LG published:2016-03-27 summary:We consider image transformation problems, where an input image istransformed into an output image. Recent methods for such problems typicallytrain feed-forward convolutional neural networks using a \emph{per-pixel} lossbetween the output and ground-truth images. Parallel work has shown thathigh-quality images can be generated by defining and optimizing\emph{perceptual} loss functions based on high-level features extracted frompretrained networks. We combine the benefits of both approaches, and proposethe use of perceptual loss functions for training feed-forward networks forimage transformation tasks. We show results on image style transfer, where afeed-forward network is trained to solve the optimization problem proposed byGatys et al in real-time. Compared to the optimization-based method, ournetwork gives similar qualitative results but is three orders of magnitudefaster. We also experiment with single-image super-resolution, where replacinga per-pixel loss with a perceptual loss gives visually pleasing results.
arxiv-16500-61 | Perceptron like Algorithms for Online Learning to Rank | http://arxiv.org/pdf/1508.00842v3.pdf | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG stat.ML published:2015-08-04 summary:Perceptron is a classic online algorithm for learning a classificationfunction. In this paper, we provide a novel extension of the perceptronalgorithm to the learning to rank problem in information retrieval. We considerpopular listwise performance measures such as Normalized Discounted CumulativeGain (NDCG) and Average Precision (AP). A modern perspective on perceptron forclassification is that it is simply an instance of online gradient descent(OGD), during mistake rounds, using the hinge loss function. Motivated by thisinterpretation, we propose a novel family of listwise, large margin rankingsurrogates. Members of this family can be thought of as analogs of the hingeloss. Exploiting a certain self-bounding property of the proposed family, weprovide a guarantee on the cumulative NDCG (or AP) induced loss incurred by ourperceptron-like algorithm. We show that, if there exists a perfect oracleranker which can correctly rank each instance in an online sequence of rankingdata, with some margin, the cumulative loss of perceptron algorithm on thatsequence is bounded by a constant, irrespective of the length of the sequence.This result is reminiscent of Novikoff's convergence theorem for theclassification perceptron. Moreover, we prove a lower bound on the cumulativeloss achievable by any deterministic algorithm, under the assumption ofexistence of perfect oracle ranker. The lower bound shows that our perceptronbound is not tight, and we propose another, \emph{purely online}, algorithmwhich achieves the lower bound. We provide empirical results on simulated andlarge commercial datasets to corroborate our theoretical results.
arxiv-16500-62 | How useful is photo-realistic rendering for visual learning? | http://arxiv.org/pdf/1603.08152v1.pdf | author:Yair Movshovitz-Attias, Takeo Kanade, Yaser Sheikh category:cs.CV published:2016-03-26 summary:Data seems cheap to get, and in many ways it is, but the process of creatinga high quality labeled dataset from a mass of data is time-consuming andexpensive. With the advent of rich 3D repositories, photo-realistic rendering systemsoffer the opportunity to provide nearly limitless data. Yet, their primaryvalue for visual learning may be the quality of the data they can providerather than the quantity. Rendering engines offer the promise of perfect labelsin addition to the data: what the precise camera pose is; what the preciselighting location, temperature, and distribution is; what the geometry of theobject is. In this work we focus on semi-automating dataset creation through use ofsynthetic data and apply this method to an important task -- object viewpointestimation. Using state-of-the-art rendering software we generate a largelabeled dataset of cars rendered densely in viewpoint space. We investigate theeffect of rendering parameters on estimation performance and show realism isimportant. We show that generalizing from synthetic data is not harder than thedomain adaptation required between two real-image datasets and that combiningsynthetic images with a small amount of real data improves estimation accuracy.
arxiv-16500-63 | Graph Cuts with Interacting Edge Costs - Examples, Approximations, and Algorithms | http://arxiv.org/pdf/1402.0240v4.pdf | author:Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.CV cs.DM math.OC published:2014-02-02 summary:We study an extension of the classical graph cut problem, wherein we replacethe modular (sum of edge weights) cost function by a submodular set functiondefined over graph edges. Special cases of this problem have appeared indifferent applications in signal processing, machine learning, and computervision. In this paper, we connect these applications via the genericformulation of "cooperative graph cuts", for which we study complexity,algorithms, and connections to polymatroidal network flows. Finally, we comparethe proposed algorithms empirically.
arxiv-16500-64 | Data-Driven Dynamic Decision Models | http://arxiv.org/pdf/1603.08150v1.pdf | author:John J. Nay, Jonathan M. Gilligan category:stat.ML cs.GT cs.MA cs.NE published:2016-03-26 summary:This article outlines a method for automatically generating models of dynamicdecision-making that both have strong predictive power and are interpretable inhuman terms. This is useful for designing empirically grounded agent-basedsimulations and for gaining direct insight into observed dynamic processes. Weuse an efficient model representation and a genetic algorithm-based estimationprocess to generate simple approximations that explain most of the structure ofcomplex stochastic processes. This method, implemented in C++ and R, scaleswell to large data sets. We apply our methods to empirical data from humansubjects game experiments and international relations. We also demonstrate themethod's ability to recover known data-generating processes by simulating datawith agent-based models and correctly deriving the underlying decision modelsfor multiple agent models and degrees of stochasticity.
arxiv-16500-65 | A Draft Memory Model on Spiking Neural Assemblies | http://arxiv.org/pdf/1603.08146v1.pdf | author:João Ranhel, João H. Albuquerque, Bruno P. M. Azevedo, Nathalia M. Cunha, Pedro J. Ishimaru category:cs.NE published:2016-03-26 summary:A draft memory model (DM) for neural networks with spike propagation delay(SNNwD) is described. Novelty in this approach are that the DM learnsimmediately, with stimuli presented once, without synaptic weight changes, andwithout external learning algorithm. Basal on this model is to trap spikeswithin neural loops. In order to construct the DM we developed two functionalblocks, also described herein. The decoder block receives input from a singlespikes source and connect it to one among many outputs. The selector blockoperates in the opposite direction, receiving many spikes sources andconnecting one of them to a single output. We realized conceptual proofs bytesting the DM in the prime numbers classifying task. This activation-basedmemory can be used as immediate and short-term memory.
arxiv-16500-66 | A Randomized Rounding Algorithm for Sparse PCA | http://arxiv.org/pdf/1508.03337v4.pdf | author:Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou, Petros Drineas category:cs.DS cs.LG stat.ML published:2015-08-13 summary:We present and analyze a simple, two-step algorithm to approximate theoptimal solution of the sparse PCA problem. Our approach first solves a L1penalized version of the NP-hard sparse PCA optimization problem and then usesa randomized rounding strategy to sparsify the resulting dense solution. Ourmain theoretical result guarantees an additive error approximation and providesa tradeoff between sparsity and accuracy. Our experimental evaluation indicatesthat our approach is competitive in practice, even compared to state-of-the-arttoolboxes such as Spasm.
arxiv-16500-67 | Video Interpolation using Optical Flow and Laplacian Smoothness | http://arxiv.org/pdf/1603.08124v1.pdf | author:Wenbin Li, Darren Cosker category:cs.CV published:2016-03-26 summary:Non-rigid video interpolation is a common computer vision task. In this paperwe present an optical flow approach which adopts a Laplacian Cotangent Meshconstraint to enhance the local smoothness. Similar to Li et al., our approachadopts a mesh to the image with a resolution up to one vertex per pixel anduses angle constraints to ensure sensible local deformations between imagepairs. The Laplacian Mesh constraints are expressed wholly inside the opticalflow optimization, and can be applied in a straightforward manner to a widerange of image tracking and registration problems. We evaluate our approach bytesting on several benchmark datasets, including the Middlebury and Garg et al.datasets. In addition, we show application of our method for constructing 3DMorphable Facial Models from dynamic 3D data.
arxiv-16500-68 | Dense Nonrigid Ground Truth for Optical Flow in Real-World Scenes | http://arxiv.org/pdf/1603.08120v1.pdf | author:Wenbin Li, Darren Cosker, Zhihan Lv, Matthew Brown category:cs.CV published:2016-03-26 summary:In this paper we present the first ground truth dataset of nonrigidlydeforming real-world scenes (both long and short video sequences) in order toquantitatively evaluate RGB based tracking and registration methods. Toconstruct ground truth for the RGB sequences, we simultaneously captureNear-Infrared (NIR) image sequences where dense markers - visible only in NIR -represent ground truth positions. This allows for comparison with automaticallytracked RGB positions and the formation of error metrics. Most previousdatasets containing nonrigidly deforming sequences are based on synthetic data.Our capture protocol enables us to acquire real-world deforming objects withrealistic photometric effects - such as blur and illumination change - as wellas occlusion and complex deformations. A public evaluation website isconstructed to allow for ranking of RGB image based optical flow and otherdense tracking algorithms, with various statistical measures. Furthermore, wepresent the first RGB-NIR multispectral optical flow model allowing for energyoptimization by combining information from both the RGB and the complementaryNIR channels. In our experiments we evaluate eight existing RGB based opticalflow methods on our new dataset. We also evaluate our multispectral opticalflow algorithm in real-world scenes by varying the input channels across RGB,NIR and RGB-NIR.
arxiv-16500-69 | Reconstructing undirected graphs from eigenspaces | http://arxiv.org/pdf/1603.08113v1.pdf | author:Yohann De Castro, Thibault Espinasse, Paul Rochet category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2016-03-26 summary:In this paper, we aim at recovering an undirected weighted graph of $N$vertices from the knowledge of a perturbed version of the eigenspaces of itsadjacency matrix $W$. Our approach is based on minimizing a cost function givenby the Frobenius norm of the commutator $\mathsf{A} \mathsf{B}-\mathsf{B}\mathsf{A}$ between symmetric matrices $\mathsf{A}$ and $\mathsf{B}$. In the Erd\H{o}s-R\'enyi model with no self-loops, we show thatidentifiability (i.e. the ability to reconstruct $W$ from the knowledge of itseigenspaces) follows a sharp phase transition on the expected number of edgeswith threshold function $N\log N/2$. Given an estimation of the eigenspaces based on a $n$-sample, we providebackward-type support selection procedures from theoretical and practical pointof views. In particular, deleting an edge from the active support, our studyunveils that the empirical contrast is of the order of $\mathcal O(1/n)$ whenwe overestimate the true support and lower bounded by a positive constant whenthe estimated support is smaller than the true support. This feature leads to apowerful practical support estimation procedure when properly thresholding theempirical contrast. Simulated and real life numerical experiments assert ournew methodology.
arxiv-16500-70 | Fast and Provably Accurate Bilateral Filtering | http://arxiv.org/pdf/1603.08109v1.pdf | author:Kunal N. Chaudhury, Swapnil D. Dabhade category:cs.CV published:2016-03-26 summary:The bilateral filter is a non-linear filter that uses a range filter alongwith a spatial filter to perform edge-preserving smoothing of images. A directcomputation of the bilateral filter requires $O(S)$ operations per pixel, where$S$ is the size of the support of the spatial filter. In this paper, we presenta fast and provably accurate algorithm for approximating the bilateral filterwhen the range kernel is Gaussian. In particular, for box and Gaussian spatialfilters, the proposed algorithm can cut down the complexity to $O(1)$ per pixelfor any arbitrary $S$. The algorithm has a simple implementation involving$N+1$ spatial filterings, where $N$ is the approximation order. We give adetailed analysis of the filtering accuracy that can be achieved by theproposed approximation in relation to the target bilateral filter. This allowsus to to estimate the order $N$ required to obtain a given accuracy. We alsopresent comprehensive numerical results to demonstrate that the proposedalgorithm is competitive with state-of-the-art methods in terms of speed andaccuracy.
arxiv-16500-71 | Support Driven Wavelet Frame-based Image Deblurring | http://arxiv.org/pdf/1603.08108v1.pdf | author:Liangtian He, Yilun Wang, Zhaoyin Xiang category:cs.CV 90C26 I.4.3 published:2016-03-26 summary:The wavelet frame systems have been playing an active role in imagerestoration and many other image processing fields over the past decades, owingto the good capability of sparsely approximating piece-wise smooth functionssuch as images. In this paper, we propose a novel wavelet frame based sparserecovery model called \textit{Support Driven Sparse Regularization} (SDSR) forimage deblurring, where the partial support information of frame coefficientsis attained via a self-learning strategy and exploited via the proposedtruncated $\ell_0$ regularization. Moreover, the state-of-the-art imagerestoration methods can be naturally incorporated into our proposed waveletframe based sparse recovery framework. In particular, in order to achievereliable support estimation of the frame coefficients, we make use of thestate-of-the-art image restoration result such as that from the IDD-BM3D methodas the initial reference image for support estimation. Our extensiveexperimental results have shown convincing improvements over existingstate-of-the-art deblurring methods.
arxiv-16500-72 | Unsupervised Domain Adaptation in the Wild: Dealing with Asymmetric Label Sets | http://arxiv.org/pdf/1603.08105v1.pdf | author:Ayush Mittal, Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV published:2016-03-26 summary:The goal of domain adaptation is to adapt models learned on a source domainto a particular target domain. Most methods for unsupervised domain adaptationproposed in the literature to date, assume that the set of classes present inthe target domain is identical to the set of classes present in the sourcedomain. This is a restrictive assumption that limits the practicalapplicability of unsupervised domain adaptation techniques in real worldsettings ("in the wild"). Therefore, we relax this constraint and propose atechnique that allows the set of target classes to be a subset of the sourceclasses. This way, large publicly available annotated datasets with a widevariety of classes can be used as source, even if the actual set of classes intarget can be more limited and, maybe most importantly, unknown beforehand. To this end, we propose an algorithm that orders a set of source subspacesthat are relevant to the target classification problem. Our method then choosesa restricted set from this ordered set of source subspaces. As an extension,even starting from multiple source datasets with varied sets of categories,this method automatically selects an appropriate subset of source categoriesrelevant to a target dataset. Empirical analysis on a number of source andtarget domain datasets shows that restricting the source subspace to only asubset of categories does indeed substantially improve the eventual targetclassification accuracy over the baseline that considers all source classes.
arxiv-16500-73 | Exploring Context with Deep Structured models for Semantic Segmentation | http://arxiv.org/pdf/1603.03183v2.pdf | author:Guosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid category:cs.CV published:2016-03-10 summary:State-of-the-art semantic image segmentation methods are mostly based ontraining deep convolutional neural networks (CNNs). In this work, we proffer toimprove semantic segmentation with the use of contextual information. Inparticular, we explore `patch-patch' context and `patch-background' context indeep CNNs. We formulate deep structured models by combining CNNs andConditional Random Fields (CRFs) for learning the patch-patch context betweenimage regions. Specifically, we formulate CNN-based pairwise potentialfunctions to capture semantic correlations between neighboring patches.Efficient piecewise training of the proposed deep structured model is thenapplied in order to avoid repeated expensive CRF inference during the course ofback propagation. For capturing the patch-background context, we show that anetwork design with traditional multi-scale image inputs and sliding pyramidpooling is very effective for improving performance. We perform comprehensiveevaluation of the proposed method. We achieve new state-of-the-art performanceon a number of challenging semantic segmentation datasets including $NYUDv2$,$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report anintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.
arxiv-16500-74 | Blind signal separation and identification of mixtures of images | http://arxiv.org/pdf/1603.08095v1.pdf | author:Felipe P. do Carmo, Joaquim T. de Assis, Vania V. Estrela, Alessandra M. Coelho category:cs.CV published:2016-03-26 summary:In this paper, a fresh procedure to handle image mixtures by means of blindsignal separation relying on a combination of second order and higher orderstatistics techniques are introduced. The problem of blind signal separation isreassigned to the wavelet domain. The key idea behind this method is that theimage mixture can be decomposed into the sum of uncorrelated and/or independentsub-bands using wavelet transform. Initially, the observed image ispre-whitened in the space domain. Afterwards, an initial separation matrix isestimated from the second order statistics de-correlation model in the waveletdomain. Later, this matrix will be used as an initial separation matrix for thehigher order statistics stage in order to find the best separation matrix. Thesuggested algorithm was tested using natural images.Experiments have confirmedthat the use of the proposed process provides promising outcomes in identifyingan image from noisy mixtures of images.
arxiv-16500-75 | Learning Hough Regression Models via Bridge Partial Least Squares for Object Detection | http://arxiv.org/pdf/1603.08092v1.pdf | author:Jianyu Tang, Hanzi Wang, Yan Yan category:cs.CV published:2016-03-26 summary:Popular Hough Transform-based object detection approaches usually constructan appearance codebook by clustering local image features. However, how tochoose appropriate values for the parameters used in the clustering stepremains an open problem. Moreover, some popular histogram features extractedfrom overlapping image blocks may cause a high degree of redundancy andmulticollinearity. In this paper, we propose a novel Hough Transform-basedobject detection approach. First, to address the above issues, we exploit aBridge Partial Least Squares (BPLS) technique to establish context-encodedHough Regression Models (HRMs), which are linear regression models that castprobabilistic Hough votes to predict object locations. BPLS is an efficientvariant of Partial Least Squares (PLS). PLS-based regression techniques(including BPLS) can reduce the redundancy and eliminate the multicollinearityof a feature set. And the appropriate value of the only parameter used in PLS(i.e., the number of latent components) can be determined by using across-validation procedure. Second, to efficiently handle object scale changes,we propose a novel multi-scale voting scheme. In this scheme, multiple Houghimages corresponding to multiple object scales can be obtained simultaneously.Third, an object in a test image may correspond to multiple true and falsepositive hypotheses at different scales. Based on the proposed multi-scalevoting scheme, a principled strategy is proposed to fuse hypotheses to reducefalse positives by evaluating normalized pointwise mutual information betweenhypotheses. In the experiments, we also compare the proposed HRM approach withits several variants to evaluate the influences of its components on itsperformance. Experimental results show that the proposed HRM approach hasachieved desirable performances on popular benchmark datasets.
arxiv-16500-76 | Measuring Book Impact Based on the Multi-granularity Online Review Mining | http://arxiv.org/pdf/1603.08091v1.pdf | author:Qingqing Zhou, Chengzhi Zhang, Star X. Zhao, Bikun Chen category:cs.DL cs.CL published:2016-03-26 summary:As with articles and journals, the customary methods for measuring books'academic impact mainly involve citations, which is easy but limited tointerrogating traditional citation databases and scholarly book reviews,Researchers have attempted to use other metrics, such as Google Books,libcitation, and publisher prestige. However, these approaches lackcontent-level information and cannot determine the citation intentions ofusers. Meanwhile, the abundant online review resources concerning academicbooks can be used to mine deeper information and content utilizing altmetricperspectives. In this study, we measure the impacts of academic books bymulti-granularity mining online reviews, and we identify factors that affect abook's impact. First, online reviews of a sample of academic books on Amazon.cnare crawled and processed. Then, multi-granularity review mining is conductedto identify review sentiment polarities and aspects' sentiment values. Lastly,the numbers of positive reviews and negative reviews, aspect sentiment values,star values, and information regarding helpfulness are integrated via theentropy method, and lead to the calculation of the final book impact scores.The results of a correlation analysis of book impact scores obtained via ourmethod versus traditional book citations show that, although there aresubstantial differences between subject areas, online book reviews tend toreflect the academic impact. Thus, we infer that online reviews represent apromising source for mining book impact within the altmetric perspective and atthe multi-granularity content level. Moreover, our proposed method might alsobe a means by which to measure other books besides academic publications.
arxiv-16500-77 | Online shopping behavior study based on multi-granularity opinion mining: China vs. America | http://arxiv.org/pdf/1603.08089v1.pdf | author:Qingqing Zhou, Rui Xia, Chengzhi Zhang category:cs.CY cs.CL cs.HC published:2016-03-26 summary:With the development of e-commerce, many products are now being soldworldwide, and manufacturers are eager to obtain a better understanding ofcustomer behavior in various regions. To achieve this goal, most previousefforts have focused mainly on questionnaires, which are time-consuming andcostly. The tremendous volume of product reviews on e-commerce websites hasseen a new trend emerge, whereby manufacturers attempt to understand userpreferences by analyzing online reviews. Following this trend, this paperaddresses the problem of studying customer behavior by exploiting recentlydeveloped opinion mining techniques. This work is novel for three reasons.First, questionnaire-based investigation is automatically enabled by employingalgorithms for template-based question generation and opinion mining-basedanswer extraction. Using this system, manufacturers are able to obtain reportsof customer behavior featuring a much larger sample size, more directinformation, a higher degree of automation, and a lower cost. Second,international customer behavior study is made easier by integrating tools formultilingual opinion mining. Third, this is the ?rst time an automaticquestionnaire investigation has been conducted to compare customer behavior inChina and America, where product reviews are written and read in Chinese andEnglish, respectively. Our study on digital cameras, smartphones, and tabletcomputers yields three ?ndings. First, Chinese customers follow the Doctrine ofthe Mean, and often use euphemistic expressions, while American customersexpress their opinions more directly. Second, Chinese customers care more aboutgeneral feelings, while American customers pay more attention to productdetails. Third, Chinese customers focus on external features, while Americancustomers care more about the internal features of products.
arxiv-16500-78 | On Fast Bilateral Filtering using Fourier Kernels | http://arxiv.org/pdf/1603.08081v1.pdf | author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV published:2016-03-26 summary:It was demonstrated in earlier work that, by approximating its range kernelusing shiftable functions, the non-linear bilateral filter can be computedusing a series of fast convolutions. Previous approaches based on shiftableapproximation have, however, been restricted to Gaussian range kernels. In thiswork, we propose a novel approximation that can be applied to any range kernel,provided it has a pointwise-convergent Fourier series. More specifically, wepropose to approximate the Gaussian range kernel of the bilateral filter usinga Fourier basis, where the coefficients of the basis are obtained by solving aseries of least-squares problems. The coefficients can be efficiently computedusing a recursive form of the QR decomposition. By controlling the cardinalityof the Fourier basis, we can obtain a good tradeoff between the run-time andthe filtering accuracy. In particular, we are able to guarantee sub-pixelaccuracy for the overall filtering, which is not provided by most existingmethods for fast bilateral filtering. We present simulation results todemonstrate the speed and accuracy of the proposed algorithm.
arxiv-16500-79 | Do You See What I Mean? Visual Resolution of Linguistic Ambiguities | http://arxiv.org/pdf/1603.08079v1.pdf | author:Yevgeni Berzak, Andrei Barbu, Daniel Harari, Boris Katz, Shimon Ullman category:cs.CV cs.AI cs.CL published:2016-03-26 summary:Understanding language goes hand in hand with the ability to integratecomplex contextual information obtained via perception. In this work, wepresent a novel task for grounded language understanding: disambiguating asentence given a visual scene which depicts one of the possible interpretationsof that sentence. To this end, we introduce a new multimodal corpus containingambiguous sentences, representing a wide range of syntactic, semantic anddiscourse ambiguities, coupled with videos that visualize the differentinterpretations for each sentence. We address this task by extending a visionmodel which determines if a sentence is depicted by a video. We demonstrate howsuch a model can be adjusted to recognize different interpretations of the sameunderlying sentence, allowing to disambiguate sentences in a unified fashionacross the different ambiguity types.
arxiv-16500-80 | Classification of Large-Scale Fundus Image Data Sets: A Cloud-Computing Framework | http://arxiv.org/pdf/1603.08071v1.pdf | author:Sohini Roychowdhury category:cs.CV published:2016-03-26 summary:Large medical image data sets with high dimensionality require substantialamount of computation time for data creation and data processing. This paperpresents a novel generalized method that finds optimal image-based feature setsthat reduce computational time complexity while maximizing overallclassification accuracy for detection of diabetic retinopathy (DR). First,region-based and pixel-based features are extracted from fundus images forclassification of DR lesions and vessel-like structures. Next, feature rankingstrategies are used to distinguish the optimal classification feature sets. DRlesion and vessel classification accuracies are computed using the boosteddecision tree and decision forest classifiers in the Microsoft Azure MachineLearning Studio platform, respectively. For images from the DIARETDB1 data set,40 of its highest-ranked features are used to classify four DR lesion typeswith an average classification accuracy of 90.1% in 792 seconds. Also, forclassification of red lesion regions and hemorrhages from microaneurysms,accuracies of 85% and 72% are observed, respectively. For images from STAREdata set, 40 high-ranked features can classify minor blood vessels with anaccuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysissystems can significantly enhance the borderline classification performances inautomated screening systems.
arxiv-16500-81 | A generalized flow for multi-class and binary classification tasks: An Azure ML approach | http://arxiv.org/pdf/1603.08070v1.pdf | author:Matthew Bihis, Sohini Roychowdhury category:cs.CV published:2016-03-26 summary:The constant growth in the present day real-world databases posecomputational challenges for a single computer. Cloud-based platforms, on theother hand, are capable of handling large volumes of information manipulationtasks, thereby necessitating their use for large real-world data setcomputations. This work focuses on creating a novel Generalized Flow within thecloud-based computing platform: Microsoft Azure Machine Learning Studio (MAMLS)that accepts multi-class and binary classification data sets alike andprocesses them to maximize the overall classification accuracy. First, eachdata set is split into training and testing data sets, respectively. Then,linear and nonlinear classification model parameters are estimated using thetraining data set. Data dimensionality reduction is then performed to maximizeclassification accuracy. For multi-class data sets, data centric information isused to further improve overall classification accuracy by reducing themulti-class classification to a series of hierarchical binary classificationtasks. Finally, the performance of optimized classification model thus achievedis evaluated and scored on the testing data set. The classificationcharacteristics of the proposed flow are comparatively evaluated on 3 publicdata sets and a local data set with respect to existing state-of-the-artmethods. On the 3 public data sets, the proposed flow achieves 78-97.5%classification accuracy. Also, the local data set, created using theinformation regarding presence of Diabetic Retinopathy lesions in fundusimages, results in 85.3-95.7% average classification accuracy, which is higherthan the existing methods. Thus, the proposed generalized flow can be usefulfor a wide range of application-oriented "big data sets".
arxiv-16500-82 | Recognizing Car Fluents from Video | http://arxiv.org/pdf/1603.08067v1.pdf | author:Bo Li, Tianfu Wu, Caiming Xiong, Song-Chun Zhu category:cs.CV published:2016-03-26 summary:Physical fluents, a term originally used by Newton [40], refers totime-varying object states in dynamic scenes. In this paper, we are interestedin inferring the fluents of vehicles from video. For example, a door (hood,trunk) is open or closed through various actions, light is blinking to turn.Recognizing these fluents has broad applications, yet have received scantattention in the computer vision literature. Car fluent recognition entails aunified framework for car detection, car part localization and part statusrecognition, which is made difficult by large structural and appearancevariations, low resolutions and occlusions. This paper learns aspatial-temporal And-Or hierarchical model to represent car fluents. Thelearning of this model is formulated under the latent structural SVM framework.Since there are no publicly related dataset, we collect and annotate a carfluent dataset consisting of car videos with diverse fluents. In experiments,the proposed method outperforms several highly related baseline methods interms of car fluent recognition and car part localization.
arxiv-16500-83 | "Did I Say Something Wrong?" A Word-Level Analysis of Wikipedia Articles for Deletion Discussions | http://arxiv.org/pdf/1603.08048v1.pdf | author:Michael Ruster category:cs.CL cs.SI stat.ML published:2016-03-25 summary:This thesis focuses on gaining linguistic insights into textual discussionson a word level. It was of special interest to distinguish messages thatconstructively contribute to a discussion from those that are detrimental tothem. Thereby, we wanted to determine whether "I"- and "You"-messages areindicators for either of the two discussion styles. These messages are nowadaysoften used in guidelines for successful communication. Although their effectshave been successfully evaluated multiple times, a large-scale analysis hasnever been conducted. Thus, we used Wikipedia Articles for Deletion (short: AfD) discussionstogether with the records of blocked users and developed a fully automatedcreation of an annotated data set. In this data set, messages were labelledeither constructive or disruptive. We applied binary classifiers to the data todetermine characteristic words for both discussion styles. Thereby, we alsoinvestigated whether function words like pronouns and conjunctions play animportant role in distinguishing the two. We found that "You"-messages were a strong indicator for disruptive messageswhich matches their attributed effects on communication. However, we found"I"-messages to be indicative for disruptive messages as well which is contraryto their attributed effects. The importance of function words could neither beconfirmed nor refuted. Other characteristic words for either communicationstyle were not found. Yet, the results suggest that a different model mightrepresent disruptive and constructive messages in textual discussions better.
arxiv-16500-84 | Hamiltonian Monte Carlo Without Detailed Balance | http://arxiv.org/pdf/1409.5191v5.pdf | author:Jascha Sohl-Dickstein, Mayur Mudigonda, Michael R. DeWeese category:stat.CO stat.ML published:2014-09-18 summary:We present a method for performing Hamiltonian Monte Carlo that largelyeliminates sample rejection for typical hyperparameters. In situations thatwould normally lead to rejection, instead a longer trajectory is computed untila new state is reached that can be accepted. This is achieved using Markovchain transitions that satisfy the fixed point equation, but do not satisfydetailed balance. The resulting algorithm significantly suppresses the randomwalk behavior and wasted function evaluations that are typically theconsequence of update rejection. We demonstrate a greater than factor of twoimprovement in mixing time on three test problems. We release the source codeas Python and MATLAB packages.
arxiv-16500-85 | An Empirical Study of Dimensional Reduction Techniques for Facial Action Units Detection | http://arxiv.org/pdf/1603.08039v1.pdf | author:Zhuo Hui, Wen-Sheng Chu category:cs.CV published:2016-03-25 summary:Biologically inspired features, such as Gabor filters, result in very highdimensional measurement. Does reducing the dimensionality of the feature spaceafford advantages beyond computational efficiency? Do some approaches todimensionality reduction (DR) yield improved action unit detection? To answerthese questions, we compared DR approaches in two relatively large databases ofspontaneous facial behavior (45 participants in total with over 2 minutes ofFACS-coded video per participant). Facial features were tracked and alignedusing active appearance models (AAM). SIFT and Gabor features were extractedfrom local facial regions. We compared linear (PCA and KPCA), manifold (LPP andLLE), supervised (LDA and KDA) and hybrid approaches (LSDA) to DR with respectto AU detection. For further comparison, a no-DR control condition was includedas well. Linear support vector machine classifiers with independent train andtest sets were used for AU detection. AU detection was quantified using areaunder the ROC curve and F1. Baseline results for PCA with Gabor features werecomparable with previous research. With some notable exceptions, DR improved AUdetection relative to no-DR. Locality embedding approaches proved vulnerable to\emph{out-of-sample} problems. Gradient-based SIFT lead to better AU detectionthan the filter-based Gabor features. For area under the curve, few differenceswere found between linear and other DR approaches. For F1, results were mixed.For both metrics, the pattern of results varied among action units. Thesefindings suggest that action unit detection may be optimized by using specificDR for specific action units. PCA and LDA were the most efficient approaches;KDA was the least efficient.
arxiv-16500-86 | On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem | http://arxiv.org/pdf/1603.08037v1.pdf | author:Kevin Jamieson, Daniel Haas, Ben Recht category:cs.LG published:2016-03-25 summary:This paper studies the trade-off between two different kinds of pureexploration: breadth versus depth. The most biased coin problem asks how manytotal coin flips are required to identify a "heavy" coin from an infinite bagcontaining both "heavy" coins with mean $\theta_1 \in (0,1)$, and "light" coinswith mean $\theta_0 \in (0,\theta_1)$, where heavy coins are drawn from the bagwith probability $\alpha \in (0,1/2)$. The key difficulty of this problem liesin distinguishing whether the two kinds of coins have very similar means, orwhether heavy coins are just extremely rare. This problem has applications incrowdsourcing, anomaly detection, and radio spectrum search. Chandrasekaran et.al. (2014) recently introduced a solution to this problem but it requiredperfect knowledge of $\theta_0,\theta_1,\alpha$. In contrast, we derivealgorithms that are adaptive to partial or absent knowledge of the problemparameters. Moreover, our techniques generalize beyond coins to more generalinstances of infinitely many armed bandit problems. We also prove lower boundsthat show our algorithm's upper bounds are tight up to $\log$ factors, and onthe way characterize the sample complexity of differentiating between a singleparametric distribution and a mixture of two such distributions. As a result,these bounds have surprising implications both for solutions to the most biasedcoin problem and for anomaly detection when only partial information about theparameters is known.
arxiv-16500-87 | Universality of Mallows' and degeneracy of Kendall's kernels for rankings | http://arxiv.org/pdf/1603.08035v1.pdf | author:Horia Mania, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan, Benjamin Recht category:stat.ML cs.DM cs.LG published:2016-03-25 summary:Kernel methods provide an attractive framework for aggregating and learningfrom ranking data, and so understanding the fundamental properties of kernelsover permutations is a question of broad interest. We provide a detailedanalysis of the Fourier spectra of the standard Kendall and Mallows kernels,and a new class of polynomial-type kernels. We prove that the Kendall kernelhas exactly two irreducible representations at which the Fourier transform isnon-zero, and moreover, the associated matrices are rank one. This implies thatthe Kendall kernel is nearly degenerate, with limited expressive anddiscriminative power. In sharp contrast, we prove that the Fourier transform ofthe Mallows kernel is a strictly positive definite matrix at all irreduciblerepresentations. This property guarantees that the Mallows kernel is bothcharacteristic and universal. We introduce a family of normalized polynomialkernels of degree p that interpolates between the Kendall (degree one) andMallows (infinite degree) kernels, and show that for d-dimensionalpermutations, the p-th degree kernel is characteristic when p is greater orequal than d - 1, unlike the Euclidean case in which no finite-degreepolynomial kernel is characteristic.
arxiv-16500-88 | Resnet in Resnet: Generalizing Residual Architectures | http://arxiv.org/pdf/1603.08029v1.pdf | author:Sasha Targ, Diogo Almeida, Kevin Lyman category:cs.LG cs.CV cs.NE stat.ML published:2016-03-25 summary:Residual networks (ResNets) have recently achieved state-of-the-art onchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deepdual-stream architecture that generalizes ResNets and standard CNNs and iseasily implemented with no computational overhead. RiR consistently improvesperformance over ResNets, outperforms architectures with similar amounts ofaugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.
arxiv-16500-89 | On the Simultaneous Preservation of Privacy and Community Structure in Anonymized Networks | http://arxiv.org/pdf/1603.08028v1.pdf | author:Daniel Cullina, Kushagra Singhal, Negar Kiyavash, Prateek Mittal category:cs.LG cs.CR cs.SI published:2016-03-25 summary:We consider the problem of performing community detection on a network, whilemaintaining privacy, assuming that the adversary has access to an auxiliarycorrelated network. We ask the question "Does there exist a regime where thenetwork cannot be deanonymized perfectly, yet the community structure could belearned?." To answer this question, we derive information theoretic conversesfor the perfect deanonymization problem using the Stochastic Block Model andedge sub-sampling. We also provide an almost tight achievability result forperfect deanonymization. We also evaluate the performance of percolation based deanonymizationalgorithm on Stochastic Block Model data-sets that satisfy the conditions ofour converse. Although our converse applies to exact deanonymization, thealgorithm fails drastically when the conditions of the converse are met.Additionally, we study the effect of edge sub-sampling on the communitystructure of a real world dataset. Results show that the dataset falls underthe purview of the idea of this paper. There results suggest that it may bepossible to prove stronger partial deanonymizability converses, which wouldenable better privacy guarantees.
arxiv-16500-90 | How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation | http://arxiv.org/pdf/1603.08023v1.pdf | author:Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE published:2016-03-25 summary:We investigate evaluation metrics for end-to-end dialogue systems wheresupervised labels, such as task completion, are not available. Recent works inend-to-end dialogue systems have adopted metrics from machine translation andtext summarization to compare a model's generated response to a single targetresponse. We show that these metrics correlate very weakly or not at all withhuman judgements of the response quality in both technical and non-technicaldomains. We provide quantitative and qualitative results highlighting specificweaknesses in existing metrics, and provide recommendations for futuredevelopment of better automatic evaluation metrics for dialogue systems.
arxiv-16500-91 | Perturbed Iterate Analysis for Asynchronous Stochastic Optimization | http://arxiv.org/pdf/1507.06970v2.pdf | author:Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, Michael I. Jordan category:stat.ML cs.DC cs.DS cs.LG math.OC published:2015-07-24 summary:We introduce and analyze stochastic optimization methods where the input toeach gradient update is perturbed by bounded noise. We show that this frameworkforms the basis of a unified approach to analyze asynchronous implementationsof stochastic optimization algorithms.In this framework, asynchronousstochastic optimization algorithms can be thought of as serial methodsoperating on noisy inputs. Using our perturbed iterate framework, we providenew analyses of the Hogwild! algorithm and asynchronous stochastic coordinatedescent, that are simpler than earlier analyses, remove many assumptions ofprevious models, and in some cases yield improved upper bounds on theconvergence rates. We proceed to apply our framework to develop and analyzeKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that thesparse and parallel version of SVRG is in some cases more than four orders ofmagnitude faster than the standard SVRG algorithm.
arxiv-16500-92 | Friction from Reflectance: Deep Reflectance Codes for Predicting Physical Surface Properties from One-Shot In-Field Reflectance | http://arxiv.org/pdf/1603.07998v1.pdf | author:Hang Zhang, Kristin Dana, Ko Nishino category:cs.CV published:2016-03-25 summary:Images are the standard input for vision algorithms, but one-shot infieldreflectance measurements are creating new opportunities for recognition andscene understanding. In this work, we address the question of what reflectancecan reveal about materials in an efficient manner. We go beyond the question ofrecognition and labeling and ask the question: What intrinsic physicalproperties of the surface can be estimated using reflectance? We introduce aframework that enables prediction of actual friction values for surfaces usingone-shot reflectance measurements. This work is a first of its kindvision-based friction estimation. We develop a novel representation forreflectance disks that capture partial BRDF measurements instantaneously. Ourmethod of deep reflectance codes combines CNN features and fisher vectorpooling with optimal binary embedding to create codes that have sufficientdiscriminatory power and have important properties of illumination and spatialinvariance. The experimental results demonstrate that reflectance can play anew role in deciphering the underlying physical properties of real-worldscenes.
arxiv-16500-93 | Modular Decomposition and Analysis of Registration based Trackers | http://arxiv.org/pdf/1603.01292v2.pdf | author:Abhineet Singh, Ankush Roy, Xi Zhang, Martin Jagersand category:cs.CV published:2016-03-03 summary:This paper presents a new way to study registration based trackers bydecomposing them into three constituent sub modules: appearance model, statespace model and search method. It is often the case that when a new tracker isintroduced in literature, it only contributes to one or two of these submodules while using existing methods for the rest. Since these are oftenselected arbitrarily by the authors, they may not be optimal for the newmethod. In such cases, our breakdown can help to experimentally find the bestcombination of methods for these sub modules while also providing a frameworkwithin which the contributions of the new tracker can be clearly demarcated andthus studied better. We show how existing trackers can be broken down using thesuggested methodology and compare the performance of the default configurationchosen by the authors against other possible combinations to demonstrate thenew insights that can be gained by such an approach. We also present an opensource system that provides a convenient interface to plug in a new method forany sub module and test it against all possible combinations of methods for theother two sub modules while also serving as a fast and efficient solution forpractical tracking requirements.
arxiv-16500-94 | Developing Quantum Annealer Driven Data Discovery | http://arxiv.org/pdf/1603.07980v1.pdf | author:Joseph Dulny III, Michael Kim category:quant-ph cs.LG published:2016-03-25 summary:Machine learning applications are limited by computational power. In thispaper, we gain novel insights into the application of quantum annealing (QA) tomachine learning (ML) through experiments in natural language processing (NLP),seizure prediction, and linear separability testing. These experiments areperformed on QA simulators and early-stage commercial QA hardware and comparedto an unprecedented number of traditional ML techniques. We extend QBoost, anearly implementation of a binary classifier that utilizes a quantum annealer,via resampling and ensembling of predicted probabilities to produce a morerobust class estimator. To determine the strengths and weaknesses of thisapproach, resampled QBoost (RQBoost) is tested across several datasets andcompared to QBoost and traditional ML. We show and explain how QBoost incombination with a commercial QA device are unable to perfectly separate binaryclass data which is linearly separable via logistic regression with shrinkage.We further explore the performance of RQBoost in the space of NLP and seizureprediction and find QA-enabled ML using QBoost and RQBoost is outperformed bytraditional techniques. Additionally, we provide a detailed discussion ofalgorithmic constraints and trade-offs imposed by the use of this QA hardware.Through these experiments, we provide unique insights into the state of quantumML via boosting and the use of quantum annealing hardware that are valuable toinstitutions interested in applying QA to problems in ML and beyond.
arxiv-16500-95 | Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach | http://arxiv.org/pdf/1601.05285v2.pdf | author:Tianwei Yu category:stat.ML published:2016-01-20 summary:We present a method of variable selection for the situation where somepredictors are nonlinearly associated with a continuous outcome variable. Themethod doesn't assume any specific functional form, and can select from a largenumber of candidates. It takes the form of incremental forward stagewiseregression, in which very small steps are taken to select the variables. Givenno functional form is assumed, we devised an approach termed roughening toadjust the residuals in the iterations. In simulations, we show the new methodis competitive against popular machine learning approaches. We also demonstrateits performance using some real datasets.
arxiv-16500-96 | Modular Tracking Framework: A Unified Approach to Registration based Tracking | http://arxiv.org/pdf/1602.09130v2.pdf | author:Abhineet Singh, Martin Jagersand category:cs.CV cs.RO published:2016-02-29 summary:This paper presents a modular, extensible and highly efficient open sourceframework for registration based tracking. It is implemented entirely in C++and is designed from the ground up to easily integrate with systems thatsupport any of several major vision and robotics libraries including OpenCV,ROS and Eigen. To establish the theoretical basis for the design of thissystem, we introduce a new way to study registration based trackers bydecomposing them into three constituent sub modules while also extending theunifying formulation described in \cite{Baker04lucasKanade_paper} to accountfor several important advances in the field since its publication. In addition to being a practical solution for fast and high precisiontracking, this system can also serve as a useful research tool by allowingexisting and new methods for any of the aforementioned sub modules to bestudied better. When a new method for one of these sub modules is introduced inliterature, this breakdown can help to experimentally find the combination ofmethods for the other sub modules that is optimum for it while also allowingmore comprehensive comparisons with existing methods to understand itscontributions better. By extensive use of generic programming, the system makes it easy to plug ina new method for any of the sub modules so that it can not only be tested withexisting methods for other sub modules but also become immediately availablefor deployment in any system that uses the framework.
arxiv-16500-97 | Unsupervised Category Discovery via Looped Deep Pseudo-Task Optimization Using a Large Scale Radiology Image Database | http://arxiv.org/pdf/1603.07965v1.pdf | author:Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Isabella Nogues, Jianhua Yao, Ronald Summers category:cs.CV published:2016-03-25 summary:Obtaining semantic labels on a large scale radiology image database (215,786key images from 61,845 unique patients) is a prerequisite yet bottleneck totrain highly effective deep convolutional neural network (CNN) models for imagerecognition. Nevertheless, conventional methods for collecting image labels(e.g., Google search followed by crowd-sourcing) are not applicable due to theformidable difficulties of medical annotation tasks for those who are notclinically trained. This type of image labeling task remains non-trivial evenfor radiologists due to uncertainty and possible drastic inter-observervariation or inconsistency. In this paper, we present a looped deep pseudo-task optimization procedurefor automatic category discovery of visually coherent and clinically semantic(concept) clusters. Our system can be initialized by domain-specific (CNNtrained on radiology images and text report derived labels) or generic(ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks areexploited by the looped deep image feature clustering (to refine image labels)and deep CNN training/classification using new labels (to obtain more taskrepresentative deep features). Our method is conceptually simple and based onthe hypothesized "convergence" of better labels leading to better trained CNNmodels which in turn feed more effective deep image features to facilitate moremeaningful clustering/labels. We have empirically validated the convergence anddemonstrated promising quantitative and qualitative results. Category labels ofsignificantly higher quality than those in previous work are discovered. Thisallows for further investigation of the hierarchical semantic nature of thegiven large-scale radiology image database.
arxiv-16500-98 | Unsupervised Transductive Domain Adaptation | http://arxiv.org/pdf/1602.03534v3.pdf | author:Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese category:stat.ML cs.LG published:2016-02-10 summary:Supervised learning with large scale labeled datasets and deep layered modelshas made a paradigm shift in diverse areas in learning and recognition.However, this approach still suffers generalization issues under the presenceof a domain shift between the training and the test data distribution. In thisregard, unsupervised domain adaptation algorithms have been proposed todirectly address the domain shift problem. In this paper, we approach theproblem from a transductive perspective. We incorporate the domain shift andthe transductive target inference into our framework by jointly solving for anasymmetric similarity metric and the optimal transductive target labelassignment. We also show that our model can easily be extended for deep featurelearning in order to learn features which are discriminative in the targetdomain. Our experiments show that the proposed method significantly outperformsstate-of-the-art algorithms in both object recognition and digit classificationexperiments by a large margin.
arxiv-16500-99 | Object Recognition Based on Amounts of Unlabeled Data | http://arxiv.org/pdf/1603.07957v1.pdf | author:Fuqiang Liu, Fukun Bi, Liang Chen category:cs.CV published:2016-03-25 summary:This paper proposes a novel semi-supervised method on object recognition.First, based on Boost Picking, a universal algorithm, Boost Picking Teaching(BPT), is proposed to train an effective binary-classifier just using a fewlabeled data and amounts of unlabeled data. Then, an ensemble strategy isdetailed to synthesize multiple BPT-trained binary-classifiers to be ahigh-performance multi-classifier. The rationality of the strategy is alsoanalyzed in theory. Finally, the proposed method is tested on two databases,CIFAR-10 and CIFAR-100. Using 2% labeled data and 98% unlabeled data, theaccuracies of the proposed method on the two data sets are 78.39% and 50.77%respectively.
arxiv-16500-100 | Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning | http://arxiv.org/pdf/1603.07954v1.pdf | author:Karthik Narasimhan, Adam Yala, Regina Barzilay category:cs.CL published:2016-03-25 summary:In traditional formulations, information extraction systems operate on afixed collection of documents. In this work, we explore the task of acquiringand incorporating external evidence to improve extraction accuracy. Thisprocess entails query reformulation for search, extraction from new sources andreconciliation of extracted values, which are repeated until sufficientevidence is collected. We approach the problem using a reinforcement learningframework where our model learns to select optimal actions based on contextualinformation. We employ a deep Q-network, trained to optimize a reward functionthat reflects extraction accuracy while penalizing extra effort. Ourexperiments on a publicly available database of shooting incidents demonstratethat our system outperforms traditional extractors by 7.2% on average.
arxiv-16500-101 | Generalized system identification with stable spline kernels | http://arxiv.org/pdf/1309.7857v3.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC 62F35, 65K10 published:2013-09-30 summary:Regularized least-squares approaches have been successfully applied to linearsystem identification. Recent approaches use quadratic penalty terms on theunknown impulse response defined by stable spline kernels, which control modelspace complexity by leveraging regularity and bounded-input bounded-outputstability. This paper extends linear system identification to a wide class ofnonsmooth stable spline estimators, where regularization functionals and datamisfits can be selected from a rich set of piecewise linear quadraticpenalties. This class encompasses the 1-norm, huber, and vapnik, in addition tothe least-squares penalty, and the approach allows linear inequalityconstraints on the unknown impulse response. We develop a customized interior point solver for the entire class ofproposed formulations. By representing penalties through their conjugates, weallow a simple interface that enables the user to specify any piecewise linearquadratic penalty for misfit and regularizer, together with inequalityconstraints on the response. The solver is locally quadratically convergent,with O(n2(m+n)) arithmetic operations per iteration, for n impulse responsecoefficients and m output measurements. In the system identification context,where n << m, IPsolve is competitive with available alternatives, illustratedby a comparison with TFOCS and libSVM. The modeling framework is illustrated with a range of numerical experiments,featuring robust formulations for contaminated data, relaxation systems, andnonnegativity and unimodality constraints on the impulse response.Incorporating constraints yields significant improvements in systemidentification. The solver used to obtain the results is distributed via anopen source code repository.
arxiv-16500-102 | Transfer Learning Based on AdaBoost for Feature Selection from Multiple ConvNet Layer Features | http://arxiv.org/pdf/1602.00417v2.pdf | author:Jumabek Alikhanov, Myeong Hyeon Ga, Seunghyun Ko, Geun-Sik Jo category:cs.CV published:2016-02-01 summary:Convolutional Networks (ConvNets) are powerful models that learn hierarchiesof visual features, which could also be used to obtain image representationsfor transfer learning. The basic pipeline for transfer learning is to firsttrain a ConvNet on a large dataset (source task) and then use feed-forwardunits activation of the trained ConvNet as image representation for smallerdatasets (target task). Our key contribution is to demonstrate superiorperformance of multiple ConvNet layer features over single ConvNet layerfeatures. Combining multiple ConvNet layer features will result in more complexfeature space with some features being repetitive. This requires some form offeature selection. We use AdaBoost with single stumps to implicitly select onlydistinct features that are useful towards classification from concatenatedConvNet features. Experimental results show that using multiple ConvNet layeractivation features instead of single ConvNet layer features consistently willproduce superior performance. Improvements becomes significant as we increasethe distance between source task and the target task.
arxiv-16500-103 | A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity | http://arxiv.org/pdf/1603.07886v1.pdf | author:Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo Zhang category:cs.CV cs.AI cs.LG published:2016-03-25 summary:Integration between biology and information science benefits both fields.Many related models have been proposed, such as computational visual cognitionmodels, computational motor control models, integrations of both and so on. Ingeneral, the robustness and precision of recognition is one of the key problemsfor object recognition models. In this paper, inspired by features of human recognition process and theirbiological mechanisms, a new integrated and dynamic framework is proposed tomimic the semantic extraction, concept formation and feature re-selection inhuman visual processing. The main contributions of the proposed model are asfollows: (1) Semantic feature extraction: Local semantic features are learnt fromepisodic features that are extracted from raw images through a deep neuralnetwork; (2) Integrated concept formation: Concepts are formed with local semanticinformation and structural information learnt through network. (3) Feature re-selection: When ambiguity is detected during recognitionprocess, distinctive features according to the difference between ambiguouscandidates are re-selected for recognition. Experimental results on hand-written digits and facial shape dataset showthat, compared with other methods, the new proposed model exhibits higherrobustness and precision for visual recognition, especially in the conditionwhen input samples are smantic ambiguous. Meanwhile, the introduced biologicalmechanisms further strengthen the interaction between neuroscience andinformation science.
arxiv-16500-104 | Hybridization of Expectation-Maximization and K-Means Algorithms for Better Clustering Performance | http://arxiv.org/pdf/1603.07879v1.pdf | author:D. Raja Kishor, N. B. Venkateswarlu category:cs.LG stat.ML published:2016-03-25 summary:The present work proposes hybridization of Expectation-Maximization (EM) andK-Means techniques as an attempt to speed-up the clustering process. Thoughboth K-Means and EM techniques look into different areas, K-means can be viewedas an approximate way to obtain maximum likelihood estimates for the means.Along with the proposed algorithm for hybridization, the present work alsoexperiments with the Standard EM algorithm. Six different datasets are used forthe experiments of which three are synthetic datasets. Clustering fitness andSum of Squared Errors (SSE) are computed for measuring the clusteringperformance. In all the experiments it is observed that the proposed algorithmfor hybridization of EM and K-Means techniques is consistently taking lessexecution time with acceptable Clustering Fitness value and less SSE than thestandard EM algorithm. It is also observed that the proposed algorithm isproducing better clustering results than the Cluster package of PurdueUniversity.
arxiv-16500-105 | The Benefit of Multitask Representation Learning | http://arxiv.org/pdf/1505.06279v2.pdf | author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:stat.ML cs.LG published:2015-05-23 summary:We discuss a general method to learn data representations from multipletasks. We provide a justification for this method in both settings of multitasklearning and learning-to-learn. The method is illustrated in detail in thespecial case of linear feature learning. Conditions on the theoreticaladvantage offered by multitask representation learning over independent tasklearning are established. In particular, focusing on the important example ofhalf-space learning, we derive the regime in which multitask representationlearning is beneficial over independent task learning, as a function of thesample size, the number of tasks and the intrinsic data dimensionality. Otherpotential applications of our results include multitask feature learning inreproducing kernel Hilbert spaces and multilayer, deep networks.
arxiv-16500-106 | Exact Bayesian inference for off-line change-point detection in tree-structured graphical models | http://arxiv.org/pdf/1603.07871v1.pdf | author:Loïc Schwaller, Stéphane Robin category:stat.ML published:2016-03-25 summary:We consider the problem of change-point detection in multivariatetime-series. The multivariate distribution of the observations is supposed tofollow a graphical model, whose graph and parameters are affected by abruptchanges throughout time. We demonstrate that it is possible to perform exactBayesian inference whenever one considers a simple class of undirected graphscalled spanning trees as possible structures. We are then able to integrate onthe graph and segmentation spaces at the same time by combining classicaldynamic programming with algebraic results pertaining to spanning trees. Inparticular, we show that quantities such as posterior distributions forchange-points or posterior edge probabilities over time can efficiently beobtained. We illustrate our results on both synthetic and experimental dataarising from biology and neuroscience.
arxiv-16500-107 | The Asymptotic Performance of Linear Echo State Neural Networks | http://arxiv.org/pdf/1603.07866v1.pdf | author:Romain Couillet, Gilles Wainrib, Harry Sevi, Hafiz Tiomoko Ali category:cs.LG cs.NE math.PR published:2016-03-25 summary:In this article, a study of the mean-square error (MSE) performance of linearecho-state neural networks is performed, both for training and testing tasks.Considering the realistic setting of noise present at the network nodes, wederive deterministic equivalents for the aforementioned MSE in the limit wherethe number of input data $T$ and network size $n$ both grow large. Specializingthen the network connectivity matrix to specific random settings, we furtherobtain simple formulas that provide new insights on the performance of suchnetworks.
arxiv-16500-108 | Markov substitute processes : a new model for linguistics and beyond | http://arxiv.org/pdf/1603.07850v1.pdf | author:Olivier Catoni, Thomas Mainguy category:stat.ML math.ST stat.TH published:2016-03-25 summary:We introduce Markov substitute processes, a new model at the crossroad ofstatistics and formal grammars, and prove its main property : Markov substituteprocesses with a given support form an exponential family.
arxiv-16500-109 | A movie genre prediction based on Multivariate Bernoulli model and genre correlations | http://arxiv.org/pdf/1604.08608v1.pdf | author:Eric Makita, Artem Lenskiy category:cs.IR cs.LG published:2016-03-25 summary:Movie ratings play an important role both in determining the likelihood of apotential viewer to watch the movie and in reflecting the current viewersatisfaction with the movie. They are available in several sources like thetelevision guide, best-selling reference books, newspaper columns, andtelevision programs. Furthermore, movie ratings are crucial for recommendationengines that track the behavior of all users and utilize the information tosuggest items they might like. Movie ratings in most cases, thus, provideinformation that might be more important than movie feature-based data. It isintuitively appealing that information about the viewing preferences in moviegenres is sufficient for predicting a genre of an unlabeled movie. In order topredict movie genres, we treat ratings as a feature vector, apply the Bernoullievent model to estimate the likelihood of a movies given genre, and evaluatethe posterior probability of the genre of a given movie using the Bayes rule.The goal of the proposed technique is to efficiently use the movie ratings forthe task of predicting movie genres. In our approach we attempted to answer thequestion: "Given the set of users who watched a movie, is it possible topredict the genre of a movie based on its ratings?" Our simulation results withMovieLens 100k data demonstrated the efficiency and accuracy of our proposedtechnique, achieving 59% prediction rate for exact prediction and 69% whenincluding correlated genres.
arxiv-16500-110 | A multinomial probabilistic model for movie genre predictions | http://arxiv.org/pdf/1603.07849v1.pdf | author:Eric Makita, Artem Lenskiy category:cs.IR cs.LG published:2016-03-25 summary:This paper proposes a movie genre-prediction based on multinomial probabilitymodel. To the best of our knowledge, this problem has not been addressed yet inthe field of recommender system. The prediction of a movie genre has manypractical applications including complementing the items categories given byexperts and providing a surprise effect in the recommendations given to a user.We employ mulitnomial event model to estimate a likelihood of a movie givengenre and the Bayes rule to evaluate the posterior probability of a genre givena movie. Experiments with the MovieLens dataset validate our approach. Weachieved 70% prediction rate using only 15% of the whole set for training.
arxiv-16500-111 | Deep Learning At Scale and At Ease | http://arxiv.org/pdf/1603.07846v1.pdf | author:Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, Sheng Wang category:cs.LG cs.DC published:2016-03-25 summary:Recently, deep learning techniques have enjoyed success in various multimediaapplications, such as image classification and multi-modal data analysis. Largedeep learning models are developed for learning rich representations of complexdata. There are two challenges to overcome before deep learning can be widelyadopted in multimedia and other applications. One is usability, namely theimplementation of different models and training algorithms must be done bynon-experts without much effort especially when the model is large and complex.The other is scalability, that is the deep learning system must be able toprovision for a huge demand of computing resources for training large modelswith massive datasets. To address these two challenges, in this paper, wedesign a distributed deep learning platform called SINGA which has an intuitiveprogramming model based on the common layer abstraction of deep learningmodels. Good scalability is achieved through flexible distributed trainingarchitecture and specific optimization techniques. SINGA runs on GPUs as wellas on CPUs, and we show that it outperforms many other state-of-the-art deeplearning systems. Our experience with developing and training deep learningmodels for real-life multimedia applications in SINGA shows that the platformis both usable and scalable.
arxiv-16500-112 | Orthogonal Echo State Networks and stochastic evaluations of likelihoods | http://arxiv.org/pdf/1601.05911v2.pdf | author:Norbert Michael Mayer, Ying-Hao Yu category:cs.NE published:2016-01-22 summary:In this paper we report the likelihood estimates that are performed on timeseries using a echo state network with orthogonal recurrent connectivity. Theresults indicate that the optimal performance depends on the way of balancingthe input strength with the recurrent activity, which also has an influence onthe network with regard to the quality of the short term prediction versusprediction that accounts for influences that date back a long time in the inputhistory. Finally, sensitivity of such networks against noise/finite accuracy ofnetwork states in the recurrent layer is investigated. In addition, a measurethat bases on mutual information is introduced in order to best quantify theperformance of the network with the time series.
arxiv-16500-113 | Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video | http://arxiv.org/pdf/1603.07839v1.pdf | author:Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar category:cs.CV cs.LG cs.NE published:2016-03-25 summary:This paper proposes an end-to-end convolutional selective autoencoderapproach for early detection of combustion instabilities using rapidly arrivingflame image frames. The instabilities arising in combustion processes causesignificant deterioration and safety issues in various human-engineered systemssuch as land and air based gas turbine engines. These properties are describedas self-sustaining, large amplitude pressure oscillations and show varyingspatial scales periodic coherent vortex structure shedding. However, suchinstability is extremely difficult to detect before a combustion processbecomes completely unstable due to its sudden (bifurcation-type) nature. Inthis context, an autoencoder is trained to selectively mask stable flame andallow unstable flame image frames. In that process, the model learns toidentify and extract rich descriptive and explanatory flame shape features.With such a training scheme, the selective autoencoder is shown to be able todetect subtle instability features as a combustion process makes transitionfrom stable to unstable region. As a consequence, the deep learning tool-chaincan perform as an early detection framework for combustion instabilities thatwill have a transformative impact on the safety and performance of modernengines.
arxiv-16500-114 | An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection | http://arxiv.org/pdf/1603.07834v1.pdf | author:Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam, Soumik Sarkar category:cs.CV cs.LG stat.ML published:2016-03-25 summary:This paper proposes a novel selective autoencoder approach within theframework of deep convolutional networks. The crux of the idea is to train adeep convolutional autoencoder to suppress undesired parts of an image framewhile allowing the desired parts resulting in efficient object detection. Theefficacy of the framework is demonstrated on a critical plant science problem.In the United States, approximately $1 billion is lost per annum due to anematode infection on soybean plants. Currently, plant-pathologists rely onlabor-intensive and time-consuming identification of Soybean Cyst Nematode(SCN) eggs in soil samples via manual microscopy. The proposed frameworkattempts to significantly expedite the process by using a series of manuallylabeled microscopic images for training followed by automated high-throughputegg detection. The problem is particularly difficult due to the presence of alarge population of non-egg particles (disturbances) in the image frames thatare very similar to SCN eggs in shape, pose and illumination. Therefore, theselective autoencoder is trained to learn unique features related to theinvariant shapes and sizes of the SCN eggs without handcrafting. After that, acomposite non-maximum suppression and differencing is applied at thepost-processing stage.
arxiv-16500-115 | Privacy-Preserved Big Data Analysis Based on Asymmetric Imputation Kernels | http://arxiv.org/pdf/1603.07828v1.pdf | author:Bo-Wei Chen category:cs.LG cs.CR published:2016-03-25 summary:This study presents an efficient approach for incomplete data classification,where the entries of samples are missing or masked due to privacy preservation.To deal with these incomplete data, a new kernel function with asymmetricintrinsic mappings is proposed in this study. Such a new kernel uses three-sidesimilarities for kernel matrix formation. The similarity between a testinstance and a training sample relies on not only their distance but also therelation between this test sample and the centroid of the class, where thetraining sample belongs. This reduces biased estimation compared with typicalmethods when only one training sample is used for kernel matrix formation.Furthermore, the proposed kernel is capable of performing data imputation byusing class-dependent averages. This enhances Fisher Discriminant Ratios anddata discriminability. Experiments on two databases were carried out forevaluating the proposed method. The result indicated that the accuracy of theproposed method was higher than that of the baseline. These findings therebydemonstrate the effectiveness of the proposed idea.
arxiv-16500-116 | Training-Free Synthesized Face Sketch Recognition Using Image Quality Assessment Metrics | http://arxiv.org/pdf/1603.07823v1.pdf | author:Nannan Wang, Jie Li, Leiyu Sun, Bin Song, Xinbo Gao category:cs.CV published:2016-03-25 summary:Face sketch synthesis has wide applications ranging from digitalentertainments to law enforcements. Objective image quality assessment scoresand face recognition accuracy are two mainly used tools to evaluate thesynthesis performance. In this paper, we proposed a synthesized face sketchrecognition framework based on full-reference image quality assessment metrics.Synthesized sketches generated from four state-of-the-art methods are utilizedto test the performance of the proposed recognition framework. For the imagequality assessment metrics, we employed the classical structured similarityindex metric and other three prevalent metrics: visual information fidelity,feature similarity index metric and gradient magnitude similarity deviation.Extensive experiments compared with baseline methods illustrate theeffectiveness of the proposed synthesized face sketch recognition framework.Data and implementation code in this paper are available online atwww.ihitworld.com/WNN/IQA_Sketch.zip.
arxiv-16500-117 | Equitability of Dependence Measure | http://arxiv.org/pdf/1501.02102v2.pdf | author:Hangjin Jiang, Kan Liu, Yiming Ding category:stat.ML published:2015-01-09 summary:A measure of dependence is said to be equitable if it gives similar scores toequally noisy relationship of different types. In practice, we do not know whatkind of functional relationship is underlying two given observations, Hence theequitability of dependence measure is critical in analysis and by scoringrelationships according to an equitable measure one hopes to find importantpatterns of any type of further examination. In this paper, we introduce ourdefinition of equitability of a dependence measure, which is naturally fromthis initial description, and Further more power-equitable(weak-equitable) isintroduced which is of the most practical meaning in evaluating the equitablityof a dependence measure.
arxiv-16500-118 | Disentangling Nonlinear Perceptual Embeddings With Multi-Query Triplet Networks | http://arxiv.org/pdf/1603.07810v1.pdf | author:Andreas Veit, Serge Belongie, Theofanis Karaletsos category:cs.CV cs.AI cs.LG published:2016-03-25 summary:In typical perceptual tasks, higher-order concepts are inferred from visualfeatures to assist with perceptual decision making. However, there is amultitude of visual concepts which can be inferred from a single stimulus. Whenlearning nonlinear embeddings with siamese or triplet networks fromsimilarities, we typically assume they are sourced from a single visualconcept. In this paper, we are concerned with the hypothesis that it can bepotentially harmful to ignore the heterogeneity of concepts affiliated withobserved similarities when learning these embedding networks. We demonstrateempirically that this hypothesis holds and suggest an approach that deals withthese shortcomings, by combining multiple notions of similarities in onecompact system. We propose Multi-Query Networks (MQNs) that leverage recentadvances in representation learning on factorized triplet embeddings incombination with Convolutional Networks in order to learn embeddingsdifferentiated into semantically distinct subspaces, which are learned with alatent space attention mechanism. We show that the resulting model learnsvisually relevant semantic subspaces with features that do not only outperformsingle triplet networks, but even sets of concept specific networks.
arxiv-16500-119 | Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting | http://arxiv.org/pdf/1603.07807v1.pdf | author:Hanzi Wang, Guobao Xiao, Yan Yan, David Suter category:cs.CV published:2016-03-25 summary:In this paper, we propose a novel geometric model fitting method, calledMode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in thepresence of severe outliers. The proposed method formulates geometric modelfitting as a mode seeking problem on a hypergraph in which vertices representmodel hypotheses and hyperedges denote data points. MSH intuitively detectsmodel instances by a simple and effective mode seeking algorithm. In additionto the mode seeking algorithm, MSH includes a similarity measure betweenvertices on the hypergraph and a weight-aware sampling technique. The proposedmethod not only alleviates sensitivity to the data distribution, but also isscalable to large scale problems. Experimental results further demonstrate thatthe proposed method has significant superiority over the state-of-the-artfitting methods on both synthetic data and real images.
arxiv-16500-120 | Investigation of event-based memory surfaces for high-speed tracking, unsupervised feature extraction and object recognition | http://arxiv.org/pdf/1603.04223v2.pdf | author:Saeed Afshar, Gregory Cohen, Chetan Singh Thakur, Jonathan Tapson, Tara Julia Hamilton, Andre van Schaik category:cs.NE cs.CV published:2016-03-14 summary:In this paper an event-based tracking, feature extraction, and classificationsystem is presented for performing object recognition using an event-basedcamera. The high-speed recognition task involves detecting and classifyingmodel airplanes that are dropped free-hand close to the camera lens so as togenerate a challenging highly varied dataset of spatio-temporal event patterns.We investigate the use of time decaying memory surfaces to capture the temporalaspect of the event-based data. These surfaces are then used to performunsupervised feature extraction, tracking and recognition. Both linear andexponentially decaying surfaces were found to result in equally highrecognition accuracy. Using only twenty five event-based feature extractingneurons in series with a linear classifier, the system achieves 98.61%recognition accuracy within 156 milliseconds of the airplane entering the fieldof view. By comparing the linear classifier results to a high-capacity ELMclassifier, we find that a small number of event-based feature extractors caneffectively project the complex spatio-temporal event patterns of the data-setto a linearly separable representation in the feature space.
arxiv-16500-121 | An Effective Unconstrained Correlation Filter and Its Kernelization for Face Recognition | http://arxiv.org/pdf/1603.07800v1.pdf | author:Yan Yan, Hanzi Wang, Cuihua Li, Chenhui Yang, Bineng Zhong category:cs.CV published:2016-03-25 summary:In this paper, an effective unconstrained correlation filter called Uncon-strained Optimal Origin Tradeoff Filter (UOOTF) is presented and applied torobust face recognition. Compared with the conventional correlation filters inClass-dependence Feature Analysis (CFA), UOOTF improves the overall performancefor unseen patterns by removing the hard constraints on the origin correlationoutputs during the filter design. To handle non-linearly separabledistributions between different classes, we further develop a non- linearextension of UOOTF based on the kernel technique. The kernel ex- tension ofUOOTF allows for higher flexibility of the decision boundary due to a widerrange of non-linearity properties. Experimental results demon- strate theeffectiveness of the proposed unconstrained correlation filter and itskernelization in the task of face recognition.
arxiv-16500-122 | Quadratic Projection Based Feature Extraction with Its Application to Biometric Recognition | http://arxiv.org/pdf/1603.07797v1.pdf | author:Yan Yan, Hanzi Wang, Si Chen, Xiaochun Cao, David Zhang category:cs.CV published:2016-03-25 summary:This paper presents a novel quadratic projection based feature extractionframework, where a set of quadratic matrices is learned to distinguish eachclass from all other classes. We formulate quadratic matrix learning (QML) as astandard semidefinite programming (SDP) problem. However, the con- ventionalinterior-point SDP solvers do not scale well to the problem of QML forhigh-dimensional data. To solve the scalability of QML, we develop an efficientalgorithm, termed DualQML, based on the Lagrange duality theory, to extractnonlinear features. To evaluate the feasibility and effectiveness of theproposed framework, we conduct extensive experiments on biometric recognition.Experimental results on three representative biometric recogni- tion tasks,including face, palmprint, and ear recognition, demonstrate the superiority ofthe DualQML-based feature extraction algorithm compared to the currentstate-of-the-art algorithms
arxiv-16500-123 | Contextual Action Recognition with R*CNN | http://arxiv.org/pdf/1505.01197v3.pdf | author:Georgia Gkioxari, Ross Girshick, Jitendra Malik category:cs.CV published:2015-05-05 summary:There are multiple cues in an image which reveal what action a person isperforming. For example, a jogger has a pose that is characteristic forjogging, but the scene (e.g. road, trail) and the presence of other joggers canbe an additional source of information. In this work, we exploit the simpleobservation that actions are accompanied by contextual cues to build a strongaction recognition system. We adapt RCNN to use more than one region forclassification while still maintaining the ability to localize the action. Wecall our system R*CNN. The action-specific models and the feature maps aretrained jointly, allowing for action specific representations to emerge. R*CNNachieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all otherapproaches in the field by a significant margin. Last, we show that R*CNN isnot limited to action recognition. In particular, R*CNN can also be used totackle fine-grained tasks such as attribute classification. We validate thisclaim by reporting state-of-the-art performance on the Berkeley Attributes ofPeople dataset.
arxiv-16500-124 | Text-Attentional Convolutional Neural Networks for Scene Text Detection | http://arxiv.org/pdf/1510.03283v2.pdf | author:Tong He, Weilin Huang, Yu Qiao, Jian Yao category:cs.CV published:2015-10-12 summary:Recent deep learning models have demonstrated strong capabilities forclassifying text and non-text components in natural images. They extract ahigh-level feature computed globally from a whole image component (patch),where the cluttered background information may dominate true text features inthe deep representation. This leads to less discriminative power and poorerrobustness. In this work, we present a new system for scene text detection byproposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) thatparticularly focuses on extracting text-related regions and features from theimage components. We develop a new learning mechanism to train the Text-CNNwith multi-level and rich supervised information, including text region mask,character label, and binary text/nontext information. The rich supervisioninformation enables the Text-CNN with a strong capability for discriminatingambiguous texts, and also increases its robustness against complicatedbackground components. The training process is formulated as a multi-tasklearning problem, where low-level supervised information greatly facilitatesmain task of text/non-text classification. In addition, a powerful low-leveldetector called Contrast- Enhancement Maximally Stable Extremal Regions(CE-MSERs) is developed, which extends the widely-used MSERs by enhancingintensity contrast between text patterns and background. This allows it todetect highly challenging text patterns, resulting in a higher recall. Ourapproach achieved promising results on the ICDAR 2013 dataset, with a F-measureof 0.82, improving the state-of-the-art results substantially.
arxiv-16500-125 | Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks | http://arxiv.org/pdf/1603.07772v1.pdf | author:Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, Xiaohui Xie category:cs.CV cs.LG published:2016-03-24 summary:Skeleton based action recognition distinguishes human actions using thetrajectories of skeleton joints, which provide a very good representation fordescribing actions. Considering that recurrent neural networks (RNNs) with LongShort-Term Memory (LSTM) can learn feature representations and model long-termtemporal dependencies automatically, we propose an end-to-end fully connecteddeep LSTM network for skeleton based action recognition. Inspired by theobservation that the co-occurrences of the joints intrinsically characterizehuman actions, we take the skeleton as the input at each time slot andintroduce a novel regularization scheme to learn the co-occurrence features ofskeleton joints. To train the deep LSTM network effectively, we propose a newdropout algorithm which simultaneously operates on the gates, cells, and outputresponses of the LSTM neurons. Experimental results on three human actionrecognition datasets consistently demonstrate the effectiveness of the proposedmodel.
arxiv-16500-126 | Generating Text from Structured Data with Application to the Biography Domain | http://arxiv.org/pdf/1603.07771v1.pdf | author:Remi Lebret, David Grangier, Michael Auli category:cs.CL published:2016-03-24 summary:This paper introduces a neural model for concept-to-text generation thatscales to large, rich domains. We experiment with a new dataset of biographiesfrom Wikipedia that is an order of magni- tude larger than existing resourceswith over 700k samples. The dataset is also vastly more diverse with a 400kvocab- ulary, compared to a few hundred words for Weathergov or Robocup. Ourmodel builds upon recent work on conditional neural language model for textgenera- tion. To deal with the large vocabulary, we extend these models to mixa fixed vocabulary with copy actions that trans- fer sample-specific words fromthe in- put database to the generated output sen- tence. Our neural modelsignificantly out- performs a classical Kneser-Ney language model adapted tothis task by nearly 15 BLEU.
arxiv-16500-127 | Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video | http://arxiv.org/pdf/1603.07763v1.pdf | author:Hao Jiang, Kristen Grauman category:cs.CV published:2016-03-24 summary:Understanding the camera wearer's activity is central to egocentric vision,yet one key facet of that activity is inherently invisible to the camera--thewearer's body pose. Prior work focuses on estimating the pose of hands and armswhen they come into view, but this 1) gives an incomplete view of the full bodyposture, and 2) prevents any pose estimate at all in many frames, since thehands are only visible in a fraction of daily life activities. We propose toinfer the "invisible pose" of a person behind the egocentric camera. Given asingle video, our efficient learning-based approach returns the full body 3Djoint positions for each frame. Our method exploits cues from the dynamicmotion signatures of the surrounding scene--which changes predictably as afunction of body pose--as well as static scene structures that reveal theviewpoint (e.g., sitting vs. standing). We further introduce a novel energyminimization scheme to infer the pose sequence. It uses soft predictions of theposes per time instant together with a non-parametric model of human posedynamics over longer windows. Our method outperforms an array of possiblealternatives, including deep learning approaches for direct pose regressionfrom images.
arxiv-16500-128 | Information-based inference in sloppy and singular models | http://arxiv.org/pdf/1506.05855v3.pdf | author:Colin H. LaMont, Paul A. Wiggins category:stat.ML cs.LG published:2015-06-19 summary:A central problem in statistics is model selection: the choice betweencompeting models of a stochastic process whose observables are corrupted bynoise. In information-based inference, model selection is performed bymaximizing the estimated predictive performance. We propose a frequen- tistinformation criterion (FIC) which extends the applicability ofinformation-based inference to the analysis of singular and sloppy models. Inthese scenarios, the Akaike information criterion (AIC) can result insignificant under or over-estimates of the predictive complexity. Two importantmechanisms for this failure are examined: an implicit multiple testing problemand the presence of unidentifiable parameters. FIC rectifies this failure byapplying a frequentist approximation to compute the com- plexity. For regularmodels in the large-sample-size limit, AIC and FIC are equal, but in generalthe complexity exhibits a sample-size dependent scaling. In the context ofsingular models, FIC can ex- hibit Bayesian information criterion-like orHannan-Quinn-like scalings with sample size. FIC does not depend on ad hocprior distributions or exogenous regularization and can be applied when struc-tured data complicates the use of cross-validatation.
arxiv-16500-129 | A universal tradeoff between power, precision and speed in physical communication | http://arxiv.org/pdf/1603.07758v1.pdf | author:Subhaneil Lahiri, Jascha Sohl-Dickstein, Surya Ganguli category:cs.IT math.IT physics.bio-ph q-bio.NC stat.ML published:2016-03-24 summary:Maximizing the speed and precision of communication while minimizing powerdissipation is a fundamental engineering design goal. Also, biological systemsachieve remarkable speed, precision and power efficiency using poorlyunderstood physical design principles. Powerful theories like informationtheory and thermodynamics do not provide general limits on power, precision andspeed. Here we go beyond these classical theories to prove that the product ofprecision and speed is universally bounded by power dissipation in any physicalcommunication channel whose dynamics is faster than that of the signal.Moreover, our derivation involves a novel connection between friction andinformation geometry. These results may yield insight into both the engineeringdesign of communication devices and the structure and function of biologicalsignaling systems.
arxiv-16500-130 | New metrics for learning and inference on sets, ontologies, and functions | http://arxiv.org/pdf/1603.06846v2.pdf | author:Ruiyu Yang, Yuxiang Jiang, Matthew W. Hahn, Elizabeth A. Housworth, Predrag Radivojac category:stat.ML published:2016-03-22 summary:We propose new metrics on sets, ontologies, and functions that can be used invarious stages of probabilistic modeling, including exploratory data analysis,learning, inference, and result interpretation. These new functions unify andgeneralize some of the popular metrics on sets and functions, such as theJaccard and bag distances on sets and Marczewski-Steinhaus distance onfunctions. We then introduce information-theoretic metrics on directed acyclicgraphs drawn independently according to a fixed probability distribution andshow how they can be used to calculate similarity between class labels for theobjects with hierarchical output spaces (e.g., protein function). Finally, weprovide evidence that the proposed metrics are useful by clustering speciesbased solely on functional annotations available for subsets of their genes.The functional trees resemble evolutionary trees obtained by the phylogeneticanalysis of their genomes.
arxiv-16500-131 | Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High Dimensional Mediators | http://arxiv.org/pdf/1603.07749v1.pdf | author:Yi Zhao, Xi Luo category:stat.ML stat.AP stat.ME published:2016-03-24 summary:In many scientific studies, it becomes increasingly important to delineatethe causal pathways through a large number of mediators, such as genetic andbrain mediators. Structural equation modeling (SEM) is a popular technique toestimate the pathway effects, commonly expressed as products of coefficients.However, it becomes unstable to fit such models with high dimensionalmediators, especially for a general setting where all the mediators arecausally dependent but the exact causal relationships between them are unknown.This paper proposes a sparse mediation model using a regularized SEM approach,where sparsity here means that a small number of mediators have nonzeromediation effects between a treatment and an outcome. To address the modelselection challenge, we innovate by introducing a new penalty called PathwayLasso. This penalty function is a convex relaxation of the non-convex productfunction, and it enables a computationally tractable optimization criterion toestimate and select many pathway effects simultaneously. We develop a fastADMM-type algorithm to compute the model parameters, and we show that theiterative updates can be expressed in closed form. On both simulated data and areal fMRI dataset, the proposed approach yields higher pathway selectionaccuracy and lower estimation bias than other competing methods.
arxiv-16500-132 | Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces | http://arxiv.org/pdf/1603.07745v1.pdf | author:Ganesh Sundaramoorthi, Naeemullah Khan, Byung-Woo Hong category:cs.CV published:2016-03-24 summary:We formulate a general energy and method for segmentation that is designed tohave preference for segmenting the coarse structure over the fine structure ofthe data, without smoothing across boundaries of regions. The energy isformulated by considering data terms at a continuum of scales from the scalespace computed from the Heat Equation within regions, and integrating theseterms over all time. We show that the energy may be approximately optimizedwithout solving for the entire scale space, but rather solving time-independentlinear equations at the native scale of the image, making the methodcomputationally feasible. We provide a multi-region scheme, and apply ourmethod to motion segmentation. Experiments on a benchmark dataset shows thatour method is less sensitive to clutter or other undesirable fine-scalestructure, and leads to better performance in motion segmentation.
arxiv-16500-133 | Skill-Based Differences in Spatio-Temporal Team Behavior in Defence of The Ancients 2 | http://arxiv.org/pdf/1603.07738v1.pdf | author:Anders Drachen, Matthew Yancey, John Maguire, Derrek Chu, Iris Yuhui Wang, Tobias Mahlmann, Matthias Schubert, Diego Klabjan category:stat.ML published:2016-03-24 summary:Multiplayer Online Battle Arena (MOBA) games are among the most playeddigital games in the world. In these games, teams of players fight against eachother in arena environments, and the gameplay is focused on tactical combat.Mastering MOBAs requires extensive practice, as is exemplified in the popularMOBA Defence of the Ancients 2 (DotA 2). In this paper, we present threedata-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2)Distribution of team members and: 3) Time series clustering via a fuzzyapproach. We present a method for obtaining accurate positional data from DotA2. We investigate how behavior varies across these measures as a function ofthe skill level of teams, using four tiers from novice to professional players.Results indicate that spatio-temporal behavior of MOBA teams is related to teamskill, with professional teams having smaller within-team distances andconducting more zone changes than amateur teams. The temporal distribution ofthe within-team distances of professional and high-skilled teams also generallyfollows patterns distinct from lower skill ranks.
arxiv-16500-134 | Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes | http://arxiv.org/pdf/1602.05242v3.pdf | author:Nima Anari, Shayan Oveis Gharan, Alireza Rezaei category:cs.LG cs.DS math.PR published:2016-02-16 summary:Strongly Rayleigh distributions are natural generalizations of product anddeterminantal probability distributions and satisfy strongest form of negativedependence properties. We show that the "natural" Monte Carlo Markov Chain(MCMC) is rapidly mixing in the support of a {\em homogeneous} stronglyRayleigh distribution. As a byproduct, our proof implies Markov chains can beused to efficiently generate approximate samples of a $k$-determinantal pointprocess. This answers an open question raised by Deshpande and Rademacher.
arxiv-16500-135 | Probabilistic Reasoning via Deep Learning: Neural Association Models | http://arxiv.org/pdf/1603.07704v1.pdf | author:Quan Liu, Hui Jiang, Zhen-Hua Ling, Si Wei, Yu Hu category:cs.AI cs.LG cs.NE published:2016-03-24 summary:In this paper, we propose a new deep learning approach, called neuralassociation model (NAM), for probabilistic reasoning in artificialintelligence. We propose to use neural networks to model association betweenany two events in a domain. Neural networks take one event as input and computea conditional probability of the other event to model how likely these twoevents are associated. The actual meaning of the conditional probabilitiesvaries between applications and depends on how the models are trained. In thiswork, as two case studies, we have investigated two NAM structures, namely deepneural networks (DNNs) and relation modulated neural nets (RMNNs), on severalprobabilistic reasoning tasks in AI, including recognizing textual entailment,triple classification in multirelational knowledge bases and common-sensereasoning. Experimental results on several popular data sets derived fromWordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNsperform equally well and they can significantly outperform the conventionalmethods available for these reasoning tasks. Moreover, comparing with DNNs,RMNNs are superior in knowledge transfer, where a pre-trained model can bequickly extended to an unseen relation after observing only a few trainingsamples.
arxiv-16500-136 | Joint Projection and Dictionary Learning using Low-rank Regularization and Graph Constraints | http://arxiv.org/pdf/1603.07697v1.pdf | author:Homa Foroughi, Nilanjan Ray, Hong Zhang category:cs.CV published:2016-03-24 summary:In this paper, we aim at learning simultaneously a discriminative dictionaryand a robust projection matrix from noisy data. The joint learning, makes thelearned projection and dictionary a better fit for each other, so a moreaccurate classification can be obtained. However, current prevailing jointdimensionality reduction and dictionary learning methods, would fail when thetraining samples are noisy or heavily corrupted. To address this issue, wepropose a joint projection and dictionary learning using low-rankregularization and graph constraints (JPDL-LR). Specifically, thediscrimination of the dictionary is achieved by imposing Fisher criterion onthe coding coefficients. In addition, our method explicitly encodes the localstructure of data by incorporating a graph regularization term, that furtherimproves the discriminative ability of the projection matrix. Inspired byrecent advances of low-rank representation for removing outliers and noise, weenforce a low-rank constraint on sub-dictionaries of all classes to make themmore compact and robust to noise. Experimental results on several benchmarkdatasets verify the effectiveness and robustness of our method for bothdimensionality reduction and image classification, especially when the datacontains considerable noise or variations.
arxiv-16500-137 | Part-of-Speech Relevance Weights for Learning Word Embeddings | http://arxiv.org/pdf/1603.07695v1.pdf | author:Quan Liu, Zhen-Hua Ling, Hui Jiang, Yu Hu category:cs.CL published:2016-03-24 summary:This paper proposes a model to learn word embeddings with weighted contextsbased on part-of-speech (POS) relevance weights. POS is a fundamental elementin natural language. However, state-of-the-art word embedding models fail toconsider it. This paper proposes to use position-dependent POS relevanceweighting matrices to model the inherent syntactic relationship among wordswithin a context window. We utilize the POS relevance weights to model eachword-context pairs during the word embedding training process. The modelproposed in this paper paper jointly optimizes word vectors and the POSrelevance matrices. Experiments conducted on popular word analogy and wordsimilarity tasks all demonstrated the effectiveness of the proposed method.
arxiv-16500-138 | Predictive Analytics Using Smartphone Sensors for Depressive Episodes | http://arxiv.org/pdf/1603.07692v1.pdf | author:Taeheon Jeong, Diego Klabjan, Justin Starren category:cs.CY cs.HC stat.ML published:2016-03-24 summary:The behaviors of patients with depression are usually difficult to predictbecause the patients demonstrate the symptoms of a depressive episode without awarning at unexpected times. The goal of this research is to build algorithmsthat detect signals of such unusual moments so that doctors can be proactive inapproaching already diagnosed patients before they fall in depression. Eachpatient is equipped with a smartphone with the capability to track its sensors.We first find the home location of a patient, which is then augmented withother sensor data to identify sleep patterns and select communication patterns.The algorithms require two to three weeks of training data to build standardpatterns, which are considered normal behaviors; and then, the methods identifyany anomalies in day-to-day data readings of sensors. Four smartphone sensors,including the accelerometer, the gyroscope, the location probe and thecommunication log probe are used for anomaly detection in sleeping andcommunication patterns.
arxiv-16500-139 | What is the right way to represent document images? | http://arxiv.org/pdf/1603.01076v2.pdf | author:Gabriela Csurka, Diane Larlus, Albert Gordo, Jon Almazan category:cs.CV published:2016-03-03 summary:In this article we study the problem of document image representation basedon visual features. We propose a comprehensive experimental study that comparesthree types of visual document image representations: (1) traditional so-calledshallow features, such as the RunLength and the Fisher-Vector descriptors, (2)deep features based on Convolutional Neural Networks, and (3) featuresextracted from hybrid architectures that take inspiration from the two previousones. We evaluate these features in several tasks (i.e. classification, clustering,and retrieval) and in different setups (e.g. domain transfer) using severalpublic and in-house datasets. Our results show that deep features generallyoutperform other types of features when there is no domain shift and the newtask is closely related to the one used to train the model. However, when alarge domain or task shift is present, the Fisher-Vector shallow featuresgeneralize better and often obtain the best results.
arxiv-16500-140 | Recursive Neural Language Architecture for Tag Prediction | http://arxiv.org/pdf/1603.07646v1.pdf | author:Saurabh Kataria category:cs.IR cs.CL cs.LG cs.NE published:2016-03-24 summary:We consider the problem of learning distributed representations for tags fromtheir associated content for the task of tag recommendation. Consideringtagging information is usually very sparse, effective learning from content andtag association is very crucial and challenging task. Recently, various neuralrepresentation learning models such as WSABIE and its variants show promisingperformance, mainly due to compact feature representations learned in asemantic space. However, their capacity is limited by a linear compositionalapproach for representing tags as sum of equal parts and hurt theirperformance. In this work, we propose a neural feedback relevance model forlearning tag representations with weighted feature representations. Ourexperiments on two widely used datasets show significant improvement forquality of recommendations over various baselines.
arxiv-16500-141 | A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements | http://arxiv.org/pdf/1506.06081v3.pdf | author:Qinqing Zheng, John Lafferty category:stat.ML cs.LG published:2015-06-19 summary:We propose a simple, scalable, and fast gradient descent algorithm tooptimize a nonconvex objective for the rank minimization problem and a closelyrelated family of semidefinite programs. With $O(r^3 \kappa^2 n \log n)$ randommeasurements of a positive semidefinite $n \times n$ matrix of rank $r$ andcondition number $\kappa$, our method is guaranteed to converge linearly to theglobal optimum.
arxiv-16500-142 | Position and Vector Detection of Blind Spot motion with the Horn-Schunck Optical Flow | http://arxiv.org/pdf/1603.07625v1.pdf | author:Stephen Yu, Mike Wu category:cs.CV published:2016-03-24 summary:The proposed method uses live image footage which, based on calculations ofpixel motion, decides whether or not an object is in the blind-spot. If found,the driver is notified by a sensory light or noise built into the vehicle'sCPU. The new technology incorporates optical vectors and flow fields ratherthan expensive radar-waves, creating cheaper detection systems that retain theneeded accuracy while adapting to the current processor speeds.
arxiv-16500-143 | Semantic Properties of Customer Sentiment in Tweets | http://arxiv.org/pdf/1603.07624v1.pdf | author:Eun Hee Ko, Diego Klabjan category:cs.CL cs.IR cs.SI stat.ML published:2016-03-24 summary:An increasing number of people are using online social networking services(SNSs), and a significant amount of information related to experiences inconsumption is shared in this new media form. Text mining is an emergingtechnique for mining useful information from the web. We aim at discovering inparticular tweets semantic patterns in consumers' discussions on social media.Specifically, the purposes of this study are twofold: 1) finding similarity anddissimilarity between two sets of textual documents that include consumers'sentiment polarities, two forms of positive vs. negative opinions and 2)driving actual content from the textual data that has a semantic trend. Theconsidered tweets include consumers opinions on US retail companies (e.g.,Amazon, Walmart). Cosine similarity and K-means clustering methods are used toachieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topicmodeling algorithm, is used for the latter purpose. This is the first studywhich discover semantic properties of textual data in consumption contextbeyond sentiment analysis. In addition to major findings, we apply LDA (LatentDirichlet Allocations) to the same data and drew latent topics that representconsumers' positive opinions and negative opinions on social media.
arxiv-16500-144 | Going Out of Business: Auction House Behavior in the Massively Multi-Player Online Game | http://arxiv.org/pdf/1603.07610v1.pdf | author:Anders Drachen, Joseph Riley, Shawna Baskin, Diego Klabjan category:cs.CY cs.HC stat.ML published:2016-03-24 summary:The in-game economies of massively multi-player online games (MMOGs) arecomplex systems that have to be carefully designed and managed. This paperpresents the results of an analysis of auction house data from the MMOG Glitch,across a 14 month time period, the entire lifetime of the game. The datacomprise almost 3 million data points, over 20,000 unique players and more than650 products. Furthermore, an interactive visualization, based on Sankey flowdiagrams, is presented which shows the proportion of the different clustersacross each time bin, as well as the flow of players between clusters. Thediagram allows evaluation of migration of players between clusters as afunction of time, as well as churn analysis. The presented work provides atemplate analysis and visualization model for progression-based ortemporal-based analysis of player behavior broadly applicable to games.
arxiv-16500-145 | Contrastive Analysis with Predictive Power: Typology Driven Estimation of Grammatical Error Distributions in ESL | http://arxiv.org/pdf/1603.07609v1.pdf | author:Yevgeni Berzak, Roi Reichart, Boris Katz category:cs.CL published:2016-03-24 summary:This work examines the impact of cross-linguistic transfer on grammaticalerrors in English as Second Language (ESL) texts. Using a computationalframework that formalizes the theory of Contrastive Analysis (CA), wedemonstrate that language specific error distributions in ESL writing can bepredicted from the typological properties of the native language and theirrelation to the typology of English. Our typology driven model enables toobtain accurate estimates of such distributions without access to any ESL datafor the target languages. Furthermore, we present a strategy for adjusting ourmethod to low-resource languages that lack typological documentation using abootstrapping approach which approximates native language typology from ESLtexts. Finally, we show that our framework is instrumental for linguisticinquiry seeking to identify first language factors that contribute to a widerange of difficulties in second language acquisition.
arxiv-16500-146 | Multi-Subregion Based Correlation Filter Bank for Robust Face Recognition | http://arxiv.org/pdf/1603.07604v1.pdf | author:Yan Yan, Hanzi Wang, David Suter category:cs.CV published:2016-03-24 summary:In this paper, we propose an effective feature extraction algorithm, calledMulti-Subregion based Correlation Filter Bank (MS-CFB), for robust facerecognition. MS-CFB combines the benefits of global-based and local-basedfeature extraction algorithms, where multiple correlation filters correspond-ing to different face subregions are jointly designed to optimize the overallcorrelation outputs. Furthermore, we reduce the computational complexi- ty ofMS-CFB by designing the correlation filter bank in the spatial domain andimprove its generalization capability by capitalizing on the unconstrained formduring the filter bank design process. MS-CFB not only takes the d- ifferencesamong face subregions into account, but also effectively exploits thediscriminative information in face subregions. Experimental results on variouspublic face databases demonstrate that the proposed algorithm pro- vides abetter feature representation for classification and achieves higherrecognition rates compared with several state-of-the-art algorithms.
arxiv-16500-147 | Semantic Regularities in Document Representations | http://arxiv.org/pdf/1603.07603v1.pdf | author:Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, Xueqi Cheng category:cs.CL published:2016-03-24 summary:Recent work exhibited that distributed word representations are good atcapturing linguistic regularities in language. This allows vector-orientedreasoning based on simple linear algebra between words. Since many differentmethods have been proposed for learning document representations, it is naturalto ask whether there is also linear structure in these learned representationsto allow similar reasoning at document level. To answer this question, wedesign a new document analogy task for testing the semantic regularities indocument representations, and conduct empirical evaluations over severalstate-of-the-art document representation models. The results reveal that neuralembedding based document representations work better on this analogy task thanconventional methods, and we provide some preliminary explanations over theseobservations.
arxiv-16500-148 | Clustering Time-Series Energy Data from Smart Meters | http://arxiv.org/pdf/1603.07602v1.pdf | author:Alexander Lavin, Diego Klabjan category:stat.ML published:2016-03-24 summary:Investigations have been performed into using clustering methods in datamining time-series data from smart meters. The problem is to identify patternsand trends in energy usage profiles of commercial and industrial customers over24-hour periods, and group similar profiles. We tested our method on energyusage data provided by several U.S. power utilities. The results show accurategrouping of accounts similar in their energy usage patterns, and potential forthe method to be utilized in energy efficiency programs.
arxiv-16500-149 | Borrowing strengh in hierarchical Bayes: Posterior concentration of the Dirichlet base measure | http://arxiv.org/pdf/1301.0802v4.pdf | author:XuanLong Nguyen category:math.ST cs.LG math.PR stat.TH published:2013-01-04 summary:This paper studies posterior concentration behavior of the base probabilitymeasure of a Dirichlet measure, given observations associated with the sampledDirichlet processes, as the number of observations tends to infinity. The basemeasure itself is endowed with another Dirichlet prior, a construction known asthe hierarchical Dirichlet processes (Teh et al. [J. Amer. Statist. Assoc. 101(2006) 1566-1581]). Convergence rates are established in transportationdistances (i.e., Wasserstein metrics) under various conditions on the geometryof the support of the true base measure. As a consequence of the theory, wedemonstrate the benefit of "borrowing strength" in the inference of multiplegroups of data - a powerful insight often invoked to motivate hierarchicalmodeling. In certain settings, the gain in efficiency due to the latenthierarchy can be dramatic, improving from a standard nonparametric rate to aparametric rate of convergence. Tools developed include transportationdistances for nonparametric Bayesian hierarchies of random measures, theexistence of tests for Dirichlet measures, and geometric properties of thesupport of Dirichlet measures.
arxiv-16500-150 | Source Localization on Graphs via l1 Recovery and Spectral Graph Theory | http://arxiv.org/pdf/1603.07584v1.pdf | author:Rodrigo Pena, Xavier Bresson, Pierre Vandergheynst category:cs.LG published:2016-03-24 summary:We cast the problem of source localization on graphs as the simultaneousproblem of sparse recovery and diffusion ker- nel learning. An l1regularization term enforces the sparsity constraint while we recover thesources of diffusion from a single snapshot of the diffusion process. Thediffusion ker- nel is estimated by assuming the process to be as generic as thestandard heat diffusion. We show with synthetic data that we can concomitantlylearn the diffusion kernel and the sources, given an estimated initialization.We validate our model with cholera mortality and atmospheric tracer diffusiondata, showing also that the accuracy of the solution depends on theconstruction of the graph from the data points.
arxiv-16500-151 | Mapping Out Narrative Structures and Dynamics Using Networks and Textual Information | http://arxiv.org/pdf/1604.03029v1.pdf | author:Semi Min, Juyong Park category:cs.CL cs.SI physics.soc-ph published:2016-03-24 summary:Human communication is often executed in the form of a narrative, an accountof connected events composed of characters, actions, and settings. A coherentnarrative structure is therefore a requisite for a well-formulated narrative --be it fictional or nonfictional -- for informative and effective communication,opening up the possibility of a deeper understanding of a narrative by studyingits structural properties. In this paper we present a network-based frameworkfor modeling and analyzing the structure of a narrative, which is furtherexpanded by incorporating methods from computational linguistics to utilize thenarrative text. Modeling a narrative as a dynamically unfolding system, wecharacterize its progression via the growth patterns of the character network,and use sentiment analysis and topic modeling to represent the actual contentof the narrative in the form of interaction maps between characters withassociated sentiment values and keywords. This is a network framework advancedbeyond the simple occurrence-based one most often used until now, allowing oneto utilize the unique characteristics of a given narrative to a high degree.Given the ubiquity and importance of narratives, such advanced network-basedrepresentation and analysis framework may lead to a more systematic modelingand understanding of narratives for social interactions, expression of humansentiments, and communication.
arxiv-16500-152 | A Kernel Test of Goodness of Fit | http://arxiv.org/pdf/1602.02964v3.pdf | author:Kacper Chwialkowski, Heiko Strathmann, Arthur Gretton category:stat.ML published:2016-02-09 summary:We propose a nonparametric statistical test for goodness-of-fit: given a setof samples, the test determines how likely it is that these were generated froma target density function. The measure of goodness-of-fit is a divergenceconstructed via Stein's method using functions from a Reproducing KernelHilbert Space. Our test statistic is based on an empirical estimate of thisdivergence, taking the form of a V-statistic in terms of the log gradients ofthe target density and the kernel. We derive a statistical test, both fori.i.d. and non-i.i.d. samples, where we estimate the null distributionquantiles using a wild bootstrap procedure. We apply our test to quantifyingconvergence of approximate Markov Chain Monte Carlo methods, statistical modelcriticism, and evaluating quality of fit vs model complexity in nonparametricdensity estimation.
arxiv-16500-153 | Weakly Supervised Semantic Labelling and Instance Segmentation | http://arxiv.org/pdf/1603.07485v1.pdf | author:Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele category:cs.CV published:2016-03-24 summary:Semantic labelling and instance segmentation are two tasks that requireparticularly costly annotations. Starting from weak supervision in the form ofbounding box detection annotations, we propose to recursively train a convnetsuch that outputs are improved after each iteration. We explore which aspectsaffect the recursive training, and which is the most suitable box-guidedsegmentation to use as initialisation. Our results improve significantly overpreviously reported ones, even when using rectangles as rough initialisation.Overall, our weak supervision approach reaches ~95% of the quality of the fullysupervised model, both for semantic labelling and instance segmentation.
arxiv-16500-154 | Fine-scale Surface Normal Estimation using a Single NIR Image | http://arxiv.org/pdf/1603.07475v1.pdf | author:Youngjin Yoon, Gyeongmin Choe, Namil Kim, Joon-Young Lee, In So Kweon category:cs.CV published:2016-03-24 summary:We present surface normal estimation using a single near infrared (NIR)image. We are focusing on fine-scale surface geometry captured with anuncalibrated light source. To tackle this ill-posed problem, we adopt agenerative adversarial network which is effective in recovering a sharp output,which is also essential for fine-scale surface normal estimation. Weincorporate angular error and integrability constraint into the objectivefunction of the network to make estimated normals physically meaningful. Wetrain and validate our network on a recent NIR dataset, and also evaluate thegenerality of our trained model by using new external datasets which arecaptured with a different camera under different environment.
arxiv-16500-155 | Data fluidity in DARIAH -- pushing the agenda forward | http://arxiv.org/pdf/1603.03170v2.pdf | author:Laurent Romary, Mike Mertens, Anne Baillot category:cs.CY cs.CL cs.DL published:2016-03-10 summary:This paper provides both an update concerning the setting up of the EuropeanDARIAH infrastructure and a series of strong action lines related to thedevelopment of a data centred strategy for the humanities in the coming years.In particular we tackle various aspect of data management: data hosting, thesetting up of a DARIAH seal of approval, the establishment of a charter betweencultural heritage institutions and scholars and finally a specific view oncertification mechanisms for data.
arxiv-16500-156 | Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics | http://arxiv.org/pdf/1603.07454v1.pdf | author:Chao Ma, Tianchenghou, Bin Lan, Jinhui Xu, Zhenhua Zhang category:cs.LG cs.NE published:2016-03-24 summary:In this paper, we present Deep Extreme Feature Extraction (DEFE), a newensemble MVA method for searching $\tau^{+}\tau^{-}$ channel of Higgs bosons inhigh energy physics. DEFE can be viewed as a deep ensemble learning scheme thattrains a strongly diverse set of neural feature learners without explicitlyencouraging diversity and penalizing correlations. This is achieved by adoptingan implicit neural controller (not involved in feedforward compuation) thatdirectly controls and distributes gradient flows from higher level deepprediction network. Such model-independent controller results in that everysingle local feature learned are used in the feature-to-output mapping stage,avoiding the blind averaging of features. DEFE makes the ensembles 'deep' inthe sense that it allows deep post-process of these features that tries tolearn to select and abstract the ensemble of neural feature learners. With theapplication of this model, a selection regions full of signal process can beobtained through the training of a miniature collision events set. Incomparison of the Classic Deep Neural Network, DEFE shows a state-of-the-artperformance: the error rate has decreased by about 37\%, the accuracy hasbroken through 90\% for the first time, along with the discovery significancehas reached a standard deviation of 6.0 $\sigma$. Experimental data shows that,DEFE is able to train an ensemble of discriminative feature learners thatboosts the overperformance of final prediction.
arxiv-16500-157 | Pixel-Level Domain Transfer | http://arxiv.org/pdf/1603.07442v1.pdf | author:Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S. Paek, In So Kweon category:cs.CV cs.AI published:2016-03-24 summary:We present an image-conditional image generation model. The model transfersan input domain to a target domain in semantic level, and generates the targetimage in pixel level. To generate realistic target images, we employ thereal/fake-discriminator in Generative Adversarial Nets, but also introduce anovel domain-discriminator to make the generated image relevant to the inputimage. We verify our model through a challenging task of generating a piece ofclothing from an input image of a dressed person. We present a high qualityclothing dataset containing the two domains, and succeed in demonstratingdecent results.
arxiv-16500-158 | Revealing the Hidden Patterns of News Photos: Analysis of Millions of News Photos Using GDELT and Deep Learning-based Vision APIs | http://arxiv.org/pdf/1603.04531v2.pdf | author:Haewoon Kwak, Jisun An category:cs.CY cs.CV cs.IR published:2016-03-15 summary:In this work, we analyze more than two million news photos published inJanuary 2016. We demonstrate i) which objects appear the most in news photos;ii) what the sentiments of news photos are; iii) whether the sentiment of newsphotos is aligned with the tone of the text; iv) how gender is treated; and v)how differently political candidates are portrayed. To our best knowledge, thisis the first large-scale study of news photo contents using deep learning-basedvision APIs.
arxiv-16500-159 | Learning Real and Boolean Functions: When Is Deep Better Than Shallow | http://arxiv.org/pdf/1603.00988v3.pdf | author:Hrushikesh Mhaskar, Qianli Liao, Tomaso Poggio category:cs.LG published:2016-03-03 summary:We describe computational tasks - especially in vision - that correspond tocompositional/hierarchical functions. While the universal approximationproperty holds both for hierarchical and shallow networks, we prove that deep(hierarchical) networks can approximate the class of compositional functionswith the same accuracy as shallow networks but with exponentially lowerVC-dimension as well as the number of training parameters. This leads to thequestion of approximation by sparse polynomials (in the number of independentparameters) and, as a consequence, by deep networks. We also discussconnections between our results and learnability of sparse Boolean functions,settling an old conjecture by Bengio.
arxiv-16500-160 | On the Powerball Method | http://arxiv.org/pdf/1603.07421v1.pdf | author:Ye Yuan, Mu Li, Claire J. Tomlin category:cs.SY cs.LG math.OC published:2016-03-24 summary:We propose a new method to accelerate the convergence of optimizationalgorithms. This method adds a power coefficient $\gamma\in(0,1)$ to thegradient during optimization. We call this the Powerball method after thewell-known Heavy-ball method \cite{heavyball}. We prove that the Powerballmethod can achieve $\epsilon$ accuracy for strongly convex functions by using$O\left((1-\gamma)^{-1}\epsilon^{\gamma-1}\right)$ iterations. We alsodemonstrate that the Powerball method provides a $10$-fold speed up of theconvergence of both gradient descent and L-BFGS on multiple real datasets.
arxiv-16500-161 | Attentive Contexts for Object Detection | http://arxiv.org/pdf/1603.07415v1.pdf | author:Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi Feng, Shuicheng Yan category:cs.CV published:2016-03-24 summary:Modern deep neural network based object detection methods typically classifycandidate proposals using their interior features. However, global and localsurrounding contexts that are believed to be valuable for object detection arenot fully exploited by existing methods yet. In this work, we take a steptowards understanding what is a robust practice to extract and utilizecontextual information to facilitate object detection in practice.Specifically, we consider the following two questions: "how to identify usefulglobal contextual information for detecting a certain object?" and "how toexploit local context surrounding a proposal for better inferring itscontents?". We provide preliminary answers to these questions throughdeveloping a novel Attention to Context Convolution Neural Network (AC-CNN)based object detection model. AC-CNN effectively incorporates global and localcontextual information into the region-based CNN (e.g. Fast RCNN) detectionmodel and provides better object detection performance. It consists of oneattention-based global contextualized (AGC) sub-network and one multi-scalelocal contextualized (MLC) sub-network. To capture global context, the AGCsub-network recurrently generates an attention map for an input image tohighlight useful global contextual locations, through multiple stacked LongShort-Term Memory (LSTM) layers. For capturing surrounding local context, theMLC sub-network exploits both the inside and outside contextual information ofeach specific proposal at multiple scales. The global and local context arethen fused together for making the final decision for detection. Extensiveexperiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority ofthe proposed AC-CNN over well-established baselines. In particular, AC-CNNoutperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 interms of mAP, respectively.
arxiv-16500-162 | A Reconfigurable Low Power High Throughput Streaming Architecture for Big Data Processing | http://arxiv.org/pdf/1603.07400v1.pdf | author:Raqibul Hasan, Tarek Taha, Zahangir Alom category:cs.LG cs.AR cs.DC published:2016-03-24 summary:General purpose computing systems are used for a large variety ofapplications. Extensive supports for flexibility in these systems limit theirenergy efficiencies. Given that big data applications are among the mainemerging workloads for computing systems, specialized architectures for bigdata processing are needed to enable low power and high throughput execution.Several big data applications are particularly focused on classification andclustering tasks. In this paper we propose a multicore heterogeneousarchitecture for big data processing. This system has the capability to processkey machine learning algorithms such as deep neural network, autoencoder, andk-means clustering. Memristor crossbars are utilized to provide low power highthroughput execution of neural networks. The system has both training andrecognition (evaluation of new input) capabilities. The proposed system couldbe used for classification, unsupervised clustering, dimensionality reduction,feature extraction, and anomaly detection applications. The system level areaand power benefits of the specialized architecture is compared with the NVIDIATelsa K20 GPGPU. Our experimental evaluations show that the proposedarchitecture can provide four to six orders of magnitude more energy efficiencyover GPGPUs for big data processing.
arxiv-16500-163 | A Diagram Is Worth A Dozen Images | http://arxiv.org/pdf/1603.07396v1.pdf | author:Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi category:cs.CV cs.AI published:2016-03-24 summary:Diagrams are common tools for representing complex concepts, relationshipsand events, often when it would be difficult to portray the same informationwith natural images. Understanding natural images has been extensively studiedin computer vision, while diagram understanding has received little attention.In this paper, we study the problem of diagram interpretation and reasoning,the challenging task of identifying the structure of a diagram and thesemantics of its constituents and their relationships. We introduce DiagramParse Graphs (DPG) as our representation to model the structure of diagrams. Wedefine syntactic parsing of diagrams as learning to infer DPGs for diagrams andstudy semantic interpretation and reasoning of diagrams in the context ofdiagram question answering. We devise an LSTM-based method for syntacticparsing of diagrams and introduce a DPG-based attention model for diagramquestion answering. We compile a new dataset of diagrams with exhaustiveannotations of constituents and relationships for over 5,000 diagrams and15,000 questions and answers. Our results show the significance of our modelsfor syntactic parsing and question answering in diagrams using DPGs.
arxiv-16500-164 | Predicting litigation likelihood and time to litigation for patents | http://arxiv.org/pdf/1603.07394v1.pdf | author:Papis Wongchaisuwat, Diego Klabjan, John O. McGinnis category:stat.ML published:2016-03-23 summary:Patent lawsuits are costly and time-consuming. An ability to forecast apatent litigation and time to litigation allows companies to better allocatebudget and time in managing their patent portfolios. We develop predictivemodels for estimating the likelihood of litigation for patents and the expectedtime to litigation based on both textual and non-textual features. Our workfocuses on improving the state-of-the-art by relying on a different set offeatures and employing more sophisticated algorithms with more realistic data.The rate of patent litigations is very low, which consequently makes theproblem difficult. The initial model for predicting the likelihood is furthermodified to capture a time-to-litigation perspective.
arxiv-16500-165 | Face Recognition Using Deep Multi-Pose Representations | http://arxiv.org/pdf/1603.07388v1.pdf | author:Wael AbdAlmageed, Yue Wua, Stephen Rawlsa, Shai Harel, Tal Hassner, Iacopo Masi, Jongmoo Choi, Jatuporn Toy Leksut, Jungyeon Kim, Prem Natarajan, Ram Nevatia, Gerard Medioni category:cs.CV published:2016-03-23 summary:We introduce our method and system for face recognition using multiplepose-aware deep learning models. In our representation, a face image isprocessed by several pose-specific deep convolutional neural network (CNN)models to generate multiple pose-specific features. 3D rendering is used togenerate multiple face poses from the input image. Sensitivity of therecognition system to pose variations is reduced since we use an ensemble ofpose-specific CNN features. The paper presents extensive experimental resultson the effect of landmark detection, CNN layer selection and pose modelselection on the performance of the recognition pipeline. Our novelrepresentation achieves better results than the state-of-the-art on IARPA's CS2and NIST's IJB-A in both verification and identification (i.e. search) tasks.
arxiv-16500-166 | A Character-level Decoder without Explicit Segmentation for Neural Machine Translation | http://arxiv.org/pdf/1603.06147v2.pdf | author:Junyoung Chung, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG published:2016-03-19 summary:The existing machine translation systems, whether phrase-based or neural,have relied almost exclusively on word-level modelling with explicitsegmentation. In this paper, we ask a fundamental question: can neural machinetranslation generate a character sequence without any explicit segmentation? Toanswer this question, we evaluate an attention-based encoder-decoder with asubword-level encoder and a character-level decoder on four languagepairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.Our experiments show that the models with a character-level decoder outperformthe ones with a subword-level decoder on all of the four language pairs.Furthermore, the ensembles of neural models with a character-level decoderoutperform the state-of-the-art non-neural machine translation systems onEn-Cs, En-De and En-Fi and perform comparably on En-Ru.
arxiv-16500-167 | Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices | http://arxiv.org/pdf/1603.07341v1.pdf | author:Tayfun Gokmen, Yurii Vlasov category:cs.LG cs.NE stat.ML published:2016-03-23 summary:In recent years, deep neural networks (DNN) have demonstrated significantbusiness impact in large scale analysis and classification tasks such as speechrecognition, visual object detection, pattern extraction, etc. Training oflarge DNNs, however, is universally considered as time consuming andcomputationally intensive task that demands datacenter-scale computationalresources recruited for many days. Here we propose a concept of resistiveprocessing unit (RPU) devices that can potentially accelerate DNN training byorders of magnitude while using much less power. The proposed RPU device canstore and update the weight values locally thus minimizing data movement duringtraining and allowing to fully exploit the locality and the parallelism of thetraining algorithm. We identify the RPU device and system specifications forimplementation of an accelerator chip for DNN training in a realisticCMOS-compatible technology. For large DNNs with about 1 billion weights thismassively parallel RPU architecture can achieve acceleration factors of 30,000Xcompared to state-of-the-art microprocessors while providing power efficiencyof 84,000 GigaOps/s/W. Problems that currently require days of training on adatacenter-size cluster with thousands of machines can be addressed withinhours on a single RPU accelerator. A system consisted of a cluster of RPUaccelerators will be able to tackle Big Data problems with trillions ofparameters that is impossible to address today like, for example, naturalspeech recognition and translation between all world languages, real-timeanalytics on large streams of business and scientific data, integration andanalysis of multimodal sensory data flows from massive number of IoT (Internetof Things) sensors.
arxiv-16500-168 | Learning Mixtures of Plackett-Luce models | http://arxiv.org/pdf/1603.07323v1.pdf | author:Zhibing Zhao, Peter Piech, Lirong Xia category:cs.LG published:2016-03-23 summary:In this paper we address the identifiability and efficient learning problemsof finite mixtures of Plackett-Luce models for rank data. We prove that for any$k\geq 2$, the mixture of $k$ Plackett-Luce models for no more than $2k-1$alternatives is non-identifiable and this bound is tight for $k=2$. For genericidentifiability, we prove that the mixture of $k$ Plackett-Luce models over $m$alternatives is generically identifiable if $k\leq\lfloor\frac {m-2}2\rfloor!$. We also propose an efficient generalized method of moments (GMM) algorithm tolearn the mixture of two Plackett-Luce models and show that the algorithm isconsistent. Our experiments show that our GMM algorithm is significantly fasterthan the EMM algorithm by Gormley and Murphy (2008), while achievingcompetitive statistical efficiency.
arxiv-16500-169 | CONDITOR1: Topic Maps and DITA labelling tool for textual documents with historical information | http://arxiv.org/pdf/1603.07313v1.pdf | author:Piedad Garrido, Jesus Tramullas, Manuel Coll category:cs.DL cs.CL cs.IR published:2016-03-23 summary:Conditor is a software tool which works with textual documents containinghistorical information. The purpose of this work two-fold: firstly to show thevalidity of the developed engine to correctly identify and label the entitiesof the universe of discourse with a labelled-combined XTM-DITA model. Secondlyto explain the improvements achieved in the information retrieval processthanks to the use of a object-oriented database (JPOX) as well as itsintegration into the Lucene-type database search process to not only accomplishmore accurate searches, but to also help the future development of arecommender system. We finish with a brief demo in a 3D-graph of the results ofthe aforementioned search.
arxiv-16500-170 | Rapid Exact Signal Scanning with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1508.06904v2.pdf | author:Markus Thom, Franz Gritschneder category:cs.LG cs.CV cs.NE published:2015-08-27 summary:We introduce and analyze a rigorous formulation of the dynamics of a signalprocessing scheme aimed at exact dense signal scanning. Related methodsproposed in the recent past lack a satisfactory analysis whether they actuallyfulfill any exactness constraints. We improve on this through an exactcharacterization of the requirements for a sound sliding window approach. Thetools developed in this paper are especially beneficial if Convolutional NeuralNetworks are employed, but can also be used as a more general framework tovalidate related approaches to signal scanning. The contributed theory helps toeliminate redundant computations and renders special case treatmentunnecessary, resulting in a dramatic boost in efficiency particularly onmassively parallel processors. This is demonstrated both theoretically in acomputational complexity analysis and empirically on modern parallelprocessors.
arxiv-16500-171 | On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis | http://arxiv.org/pdf/1603.07294v1.pdf | author:James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri category:cs.LG cs.AI cs.CR stat.ML published:2016-03-23 summary:Bayesian inference has great promise for the privacy-preserving analysis ofsensitive data, as posterior sampling automatically preserves differentialprivacy, an algorithmic notion of data privacy, under certain conditions (Wanget al., 2015). While Wang et al. (2015)'s one posterior sample (OPS) approachelegantly provides privacy "for free," it is data inefficient in the sense ofasymptotic relative efficiency (ARE). We show that a simple alternative basedon the Laplace mechanism, the workhorse technique of differential privacy, isas asymptotically efficient as non-private posterior inference, under generalassumptions. The Laplace mechanism has additional practical advantagesincluding efficient use of the privacy budget for MCMC. We demonstrate thepracticality of our approach on a time-series analysis of sensitive militaryrecords from the Afghanistan and Iraq wars disclosed by the Wikileaksorganization.
arxiv-16500-172 | Debugging Machine Learning Tasks | http://arxiv.org/pdf/1603.07292v1.pdf | author:Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, Deepak Vijaykeerthy category:cs.LG cs.AI cs.PL stat.ML D.2.5; I.2.3 published:2016-03-23 summary:Unlike traditional programs (such as operating systems or word processors)which have large amounts of code, machine learning tasks use programs withrelatively small amounts of code (written in machine learning libraries), butvoluminous amounts of data. Just like developers of traditional programs debugerrors in their code, developers of machine learning tasks debug and fix errorsin their data. However, algorithms and tools for debugging and fixing errors indata are less common, when compared to their counterparts for detecting andfixing errors in code. In this paper, we consider classification tasks whereerrors in training data lead to misclassifications in test points, and proposean automated method to find the root causes of such misclassifications. Ourroot cause analysis is based on Pearl's theory of causation, and uses Pearl'sPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,encodes the computation of PS as a probabilistic program, and uses recent workon probabilistic programs and transformations on probabilistic programs (alongwith gray-box models of machine learning algorithms) to efficiently compute PS.Psi is able to identify root causes of data errors in interesting data sets.
arxiv-16500-173 | Fast moment estimation for generalized latent Dirichlet models | http://arxiv.org/pdf/1603.05324v2.pdf | author:Shiwen Zhao, Barbara E. Engelhardt, Sayan Mukherjee, David B. Dunson category:math.ST cs.LG stat.AP stat.ME stat.TH published:2016-03-17 summary:We develop a generalized method of moments (GMM) approach for fast parameterestimation in a new class of Dirichlet latent variable models with mixed datatypes. Parameter estimation via GMM has been demonstrated to have computationaland statistical advantages over alternative methods, such as expectationmaximization, variational inference, and Markov chain Monte Carlo. The keycomputational advan- tage of our method (MELD) is that parameter estimationdoes not require instantiation of the latent variables. Moreover, arepresentational advantage of the GMM approach is that the behavior of themodel is agnostic to distributional assumptions of the observations. We derivepopulation moment conditions after marginalizing out the sample-specificDirichlet latent variables. The moment conditions only depend on component meanparameters. We illustrate the utility of our approach on simulated data,comparing results from MELD to alternative methods, and we show the promise ofour approach through the application of MELD to several data sets.
arxiv-16500-174 | A guide to convolution arithmetic for deep learning | http://arxiv.org/pdf/1603.07285v1.pdf | author:Vincent Dumoulin, Francesco Visin category:stat.ML cs.LG cs.NE published:2016-03-23 summary:We introduce a guide to help deep learning practitioners understand andmanipulate convolutional neural network architectures. The guide clarifies therelationship between various properties (input shape, kernel shape, zeropadding, strides and output shape) of convolutional, pooling and transposedconvolutional layers, as well as the relationship between convolutional andtransposed convolutional layers. Relationships are derived for various cases,and are illustrated in order to make them intuitive.
arxiv-16500-175 | Nuclear norm penalization and optimal rates for noisy low rank matrix completion | http://arxiv.org/pdf/1011.6256v4.pdf | author:Vladimir Koltchinskii, Alexandre B. Tsybakov, Karim Lounici category:math.ST stat.ML stat.TH published:2010-11-29 summary:This paper deals with the trace regression model where $n$ entries or linearcombinations of entries of an unknown $m_1\times m_2$ matrix $A_0$ corrupted bynoise are observed. We propose a new nuclear norm penalized estimator of $A_0$and establish a general sharp oracle inequality for this estimator forarbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation.Then this method is applied to the matrix completion problem. In this case, theestimator admits a simple explicit form and we prove that it satisfies oracleinequalities with faster rates of convergence than in the previous works. Theyare valid, in particular, in the high-dimensional setting $m_1m_2\gg n$. Weshow that the obtained rates are optimal up to logarithmic factors in a minimaxsense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound onthe rate of convergence of our estimator, which coincides with the upper boundup to a constant factor. Finally, we show that our procedure provides an exactrecovery of the rank of $A_0$ with probability close to 1. We also discuss thestatistical learning setting where there is no underlying model determined by$A_0$ and the aim is to find the best trace regression model approximating thedata.
arxiv-16500-176 | Gaussian Process Morphable Models | http://arxiv.org/pdf/1603.07254v1.pdf | author:Marcel Lüthi, Christoph Jud, Thomas Gerig, Thomas Vetter category:cs.CV published:2016-03-23 summary:Statistical shape models (SSMs) represent a class of shapes as a normaldistribution of point variations, whose parameters are estimated from exampleshapes. Principal component analysis (PCA) is applied to obtain alow-dimensional representation of the shape variation in terms of the leadingprincipal components. In this paper, we propose a generalization of SSMs,called Gaussian Process Morphable Models (GPMMs). We model the shape variationswith a Gaussian process, which we represent using the leading components of itsKarhunen-Loeve expansion. To compute the expansion, we make use of anapproximation scheme based on the Nystrom method. The resulting model can beseen as a continuous analogon of an SSM. However, while for SSMs the shapevariation is restricted to the span of the example data, with GPMMs we candefine the shape variation using any Gaussian process. For example, we canbuild shape models that correspond to classical spline models, and thus do notrequire any example data. Furthermore, Gaussian processes make it possible tocombine different models. For example, an SSM can be extended with a splinemodel, to obtain a model that incorporates learned shape characteristics, butis flexible enough to explain shapes that cannot be represented by the SSM. Weintroduce a simple algorithm for fitting a GPMM to a surface or image. Thisresults in a non-rigid registration approach, whose regularization propertiesare defined by a GPMM. We show how we can obtain different registrationschemes,including methods for multi-scale, spatially-varying or hybridregistration, by constructing an appropriate GPMM. As our approach strictlyseparates modelling from the fitting process, this is all achieved withoutchanges to the fitting algorithm. We show the applicability and versatility ofGPMMs on a clinical use case, where the goal is the model-based segmentation of3D forearm images.
arxiv-16500-177 | Evaluating semantic models with word-sentence relatedness | http://arxiv.org/pdf/1603.07253v1.pdf | author:Kimberly Glasgow, Matthew Roos, Amy Haufler, Mark Chevillet, Michael Wolmetz category:cs.CL published:2016-03-23 summary:Semantic textual similarity (STS) systems are designed to encode and evaluatethe semantic similarity between words, phrases, sentences, and documents. Onemethod for assessing the quality or authenticity of semantic informationencoded in these systems is by comparison with human judgments. A data set forevaluating semantic models was developed consisting of 775 Englishword-sentence pairs, each annotated for semantic relatedness by human ratersengaged in a Maximum Difference Scaling (MDS) task, as well as a fasteralternative task. As a sample application of this relatedness data,behavior-based relatedness was compared to the relatedness computed via fouroff-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, andUMBC Ebiquity. Some STS models captured much of the variance in the humanjudgments collected, but they were not sensitive to the implicatures andentailments that were processed and considered by the participants. All textstimuli and judgment data have been made freely available.
arxiv-16500-178 | Neural Summarization by Extracting Sentences and Words | http://arxiv.org/pdf/1603.07252v1.pdf | author:Jianpeng Cheng, Mirella Lapata category:cs.CL published:2016-03-23 summary:Traditional approaches to extractive summarization rely heavily onhuman-engineered features. In this work we propose a data-driven approach basedon neural networks and continuous sentence features. We develop a generalframework for single-document summarization composed of a hierarchical documentencoder and an attention-based extractor. This architecture allows us todevelop different classes of summarization models which can extract sentencesor words. We train our models on large scale corpora containing hundreds ofthousands of document-summary pairs. Experimental results on two summarizationdatasets demonstrate that our models obtain results comparable to the state ofthe art without any access to linguistic annotation.
arxiv-16500-179 | A Tutorial on Deep Neural Networks for Intelligent Systems | http://arxiv.org/pdf/1603.07249v1.pdf | author:Juan C. Cuevas-Tello, Manuel Valenzuela-Rendon, Juan A. Nolazco-Flores category:cs.NE cs.LG J.4.6 published:2016-03-23 summary:Developing Intelligent Systems involves artificial intelligence approachesincluding artificial neural networks. Here, we present a tutorial of DeepNeural Networks (DNNs), and some insights about the origin of the term "deep";references to deep learning are also given. Restricted Boltzmann Machines,which are the core of DNNs, are discussed in detail. An example of a simpletwo-layer network, performing unsupervised learning for unlabeled data, isshown. Deep Belief Networks (DBNs), which are used to build networks with morethan two layers, are also described. Moreover, examples for supervised learningwith DNNs performing simple prediction and classification tasks, are presentedand explained. This tutorial includes two intelligent pattern recognitionapplications: hand- written digits (benchmark known as MNIST) and speechrecognition.
arxiv-16500-180 | Lightweight Unsupervised Domain Adaptation by Convolutional Filter Reconstruction | http://arxiv.org/pdf/1603.07234v1.pdf | author:Rahaf Aljundi, Tinne Tuytelaars category:cs.CV published:2016-03-23 summary:End-to-end learning methods have achieved impressive results in many areas ofcomputer vision. At the same time, these methods still suffer from adegradation in performance when testing on new datasets that stem from adifferent distribution. This is known as the domain shift effect. Recentlyproposed adaptation methods focus on retraining the network parameters.However, this requires access to all (labeled) source data, a large amount of(unlabeled) target data, and plenty of computational resources. In this work,we propose a lightweight alternative, that allows adapting to the target domainbased on a limited number of target samples in a matter of minutes rather thanhours, days or even weeks. To this end, we first analyze the output of eachconvolutional layer from a domain adaptation perspective. Surprisingly, we findthat already at the very first layer, domain shift effects pop up. We thenpropose a new domain adaptation method, where first layer convolutional filtersthat are badly affected by the domain shift are reconstructed based on lessaffected ones. This improves the performance of the deep network on variousbenchmark datasets.
arxiv-16500-181 | Scalable Metric Learning via Weighted Approximate Rank Component Analysis | http://arxiv.org/pdf/1603.00370v2.pdf | author:Cijo Jose, Francois Fleuret category:cs.CV published:2016-03-01 summary:We are interested in the large-scale learning of Mahalanobis distances, witha particular focus on person re-identification. We propose a metric learning formulation called Weighted Approximate RankComponent Analysis (WARCA). WARCA optimizes the precision at top ranks bycombining the WARP loss with a regularizer that favors orthonormal linearmappings, and avoids rank-deficient embeddings. Using this new regularizerallows us to adapt the large-scale WSABIE procedure and to leverage the Adamstochastic optimization algorithm, which results in an algorithm that scalesgracefully to very large data-sets. Also, we derive a kernelized version whichallows to take advantage of state-of-the-art features for re-identificationwhen data-set size permits kernel computation. Benchmarks on recent and standard re-identification data-sets show that ourmethod beats existing state-of-the-art techniques both in term of accuracy andspeed. We also provide experimental analysis to shade lights on the propertiesof the regularizer we use, and how it improves performance.
arxiv-16500-182 | A Decentralized Quasi-Newton Method for Dual Formulations of Consensus Optimization | http://arxiv.org/pdf/1603.07195v1.pdf | author:Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.DC cs.LG published:2016-03-23 summary:This paper considers consensus optimization problems where each node of anetwork has access to a different summand of an aggregate cost function. Nodestry to minimize the aggregate cost function, while they exchange informationonly with their neighbors. We modify the dual decomposition method toincorporate a curvature correction inspired by theBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dualD-BFGS method is a fully decentralized algorithm in which nodes approximatecurvature information of themselves and their neighbors through thesatisfaction of a secant condition. Dual D-BFGS is of interest in consensusoptimization problems that are not well conditioned, making first orderdecentralized methods ineffective, and in which second order information is notreadily available, making decentralized second order methods infeasible.Asynchronous implementation is discussed and convergence of D-BFGS isestablished formally for both synchronous and asynchronous implementations.Performance advantages relative to alternative decentralized algorithms areshown numerically.
arxiv-16500-183 | Weakly-Supervised Semantic Segmentation using Motion Cues | http://arxiv.org/pdf/1603.07188v1.pdf | author:Pavel Tokmakov, Karteek Alahari, Cordelia Schmid category:cs.CV published:2016-03-23 summary:Fully convolutional neural networks (FCNNs) trained on a large number ofimages with strong pixel-level annotations have become the new state of the artfor the semantic segmentation task. While there have been recent attempts tolearn FCNNs from image-level weak annotations, they need additionalconstraints, such as the size of an object, to obtain reasonable performance.To address this issue, we present motion-CNN (M-CNN), a novel FCNN frameworkwhich incorporates motion cues and is learned from video-level weakannotations. Our learning scheme to train the network uses motion segments assoft constraints, thereby handling noisy motion information. When trained onweakly-annotated videos, our method outperforms the state-of-the-art approachof Papandreou et al. by a factor of 2 on the PASCAL VOC 2012 image segmentationbenchmark. We also demonstrate that the performance of M-CNN learned with 150weak video annotations is on par with state-of-the-art weakly-supervisedmethods trained with thousands of images. Finally, M-CNN substantiallyoutperforms recent approaches in a related task of video co-localization on theYouTube-Objects dataset.
arxiv-16500-184 | Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings | http://arxiv.org/pdf/1603.07185v1.pdf | author:Rajesh Bordawekar, Oded Shmueli category:cs.CL cs.DB published:2016-03-23 summary:We apply distributed language embedding methods from Natural LanguageProcessing to assign a vector to each database entity associated token (forexample, a token may be a word occurring in a table row, or the name of acolumn). These vectors, of typical dimension 200, capture the meaning of tokensbased on the contexts in which the tokens appear together. To form vectors, weapply a learning method to a token sequence derived from the database. Wedescribe various techniques for extracting token sequences from a database. Thetechniques differ in complexity, in the token sequences they output and in thedatabase information used (e.g., foreign keys). The vectors can be used toalgebraically quantify semantic relationships between the tokens such assimilarities and analogies. Vectors enable a dual view of the data: relationaland (meaningful rather than purely syntactical) text. We introduce and explorea new class of queries called cognitive intelligence (CI) queries that extractinformation from the database based, in part, on the relationships encoded byvectors. We have implemented a prototype system on top of Spark to exhibit thepower of CI queries. Here, CI queries are realized via SQL UDFs. This powergoes far beyond text extensions to relational systems due to the informationencoded in vectors. We also consider various extensions to the basic scheme,including using a collection of views derived from the database to focus on adomain of interest, utilizing vectors and/or text from external sources,maintaining vectors as the database evolves and exploring a database withoututilizing its schema. For the latter, we consider minimal extensions to SQL tovastly improve query expressiveness.
arxiv-16500-185 | The Anatomy of a Search and Mining System for Digital Archives | http://arxiv.org/pdf/1603.07150v1.pdf | author:Martyn Harris, Mark Levene, Dell Zhang, Dan Levene category:cs.DL cs.CL cs.IR published:2016-03-23 summary:Samtla (Search And Mining Tools with Linguistic Analysis) is a digitalhumanities system designed in collaboration with historians and linguists toassist them with their research work in quantifying the content of any textualcorpora through approximate phrase search and document comparison. Theretrieval engine uses a character-based n-gram language model rather than theconventional word-based one so as to achieve great flexibility in languageagnostic query processing. The index is implemented as a space-optimised character-based suffix treewith an accompanying database of document content and metadata. A number oftext mining tools are integrated into the system to allow researchers todiscover textual patterns, perform comparative analysis, and find out what iscurrently popular in the research community. Herein we describe the system architecture, user interface, models andalgorithms, and data storage of the Samtla system. We also present several casestudies of its usage in practice together with an evaluation of the systems'ranking performance through crowdsourcing.
arxiv-16500-186 | BreakingNews: Article Annotation by Image and Text Processing | http://arxiv.org/pdf/1603.07141v1.pdf | author:Arnau Ramisa, Fei Yan, Francesc Moreno-Noguer, Krystian Mikolajczyk category:cs.CV published:2016-03-23 summary:Building upon recent Deep Neural Network architectures, current approacheslying in the intersection of computer vision and natural language processinghave achieved unprecedented breakthroughs in tasks like automatic captioning orimage retrieval. Most of these learning methods, though, rely on large trainingsets of images associated with human annotations that specifically describe thevisual content. In this paper we propose to go a step further and explore themore complex cases where textual descriptions are loosely related to theimages. We focus on the particular domain of News articles in which the textualcontent often expresses connotative and ambiguous relations that are onlysuggested but not directly inferred from images. We introduce new deep learningmethods that address source detection, popularity prediction, articleillustration and geolocation of articles. An adaptive CNN architecture isproposed, that shares most of the structure for all the tasks, and is suitablefor multitask and transfer learning. Deep Canonical Correlation Analysis isdeployed for article illustration, and a new loss function based on GreatCircle Distance is proposed for geolocation. Furthermore, we presentBreakingNews, a novel dataset with approximately 100K news articles includingimages, text and captions, and enriched with heterogeneous meta-data (such asGPS coordinates and popularity metrics). We show this dataset to be appropriateto explore all aforementioned problems, for which we provide a baselineperformance using various Deep Learning architectures, and differentrepresentations of the textual and visual features. We report very promisingresults and bring to light several limitations of current state-of-the-art inthis kind of domain, which we hope will help spur progress in the field.
arxiv-16500-187 | Comparison of Bayesian predictive methods for model selection | http://arxiv.org/pdf/1503.08650v4.pdf | author:Juho Piironen, Aki Vehtari category:stat.ME cs.LG published:2015-03-30 summary:The goal of this paper is to compare several widely used Bayesian modelselection methods in practical model selection problems, highlight theirdifferences and give recommendations about the preferred approaches. We focuson the variable subset selection for regression and classification and performseveral numerical experiments using both simulated and real world data. Theresults show that the optimization of a utility estimate such as thecross-validation (CV) score is liable to finding overfitted models due torelatively high variance in the utility estimates when the data is scarce. Thiscan also lead to substantial selection induced bias and optimism in theperformance evaluation for the selected model. From a predictive viewpoint,best results are obtained by accounting for model uncertainty by forming thefull encompassing model, such as the Bayesian model averaging solution over thecandidate models. If the encompassing model is too complex, it can be robustlysimplified by the projection method, in which the information of the full modelis projected onto the submodels. This approach is substantially less prone tooverfitting than selection based on CV-score. Overall, the projection methodappears to outperform also the maximum a posteriori model and the selection ofthe most probable variables. The study also demonstrates that the modelselection can greatly benefit from using cross-validation outside the searchingprocess both for guiding the model size selection and assessing the predictiveperformance of the finally selected model.
arxiv-16500-188 | Robust cDNA microarray image segmentation and analysis technique based on Hough circle transform | http://arxiv.org/pdf/1603.07123v1.pdf | author:R. M. Farouk, M. A. SayedElahl category:cs.CV published:2016-03-23 summary:One of the most challenging tasks in microarray image analysis is spotsegmentation. A solution to this problem is to provide an algorithm than can beused to find any spot within the microarray image. Circular HoughTransformation (CHT) is a powerful feature extraction technique used in imageanalysis, computer vision, and digital image processing. CHT algorithm isapplied on the cDNA microarray images to develop the accuracy and theefficiency of the spots localization, addressing and segmentation process. Thepurpose of the applied technique is to find imperfect instances of spots withina certain class of circles by applying a voting procedure on the cDNAmicroarray images for spots localization, addressing and characterizing thepixels of each spot into foreground pixels and background simultaneously.Intensive experiments on the University of North Carolina (UNC) microarraydatabase indicate that the proposed method is superior to the K-means methodand the Support vector machine (SVM). Keywords: Hough circle transformation,cDNA microarray image analysis, cDNA microarray image segmentation, spotslocalization and addressing, spots segmentation
arxiv-16500-189 | Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos | http://arxiv.org/pdf/1603.07120v1.pdf | author:Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang category:cs.CV published:2016-03-23 summary:Single modality action recognition on RGB or depth sequences has beenextensively explored recently. It is generally accepted that each of these twomodalities has different strengths and limitations for the task of actionrecognition. Therefore, analysis of the RGB+D videos can help us to betterstudy the complementary properties of these two types of modalities and achievehigher levels of performance. In this paper, we propose a new deep autoencoderbased shared-specific feature factorization network to separate inputmultimodal signals into a hierarchy of components. Further, based on thestructure of the features, a structured sparsity learning machine is proposedwhich utilizes mixed norms to apply regularization within components and groupselection between them for better classification performance. Our experimentalresults show the effectiveness of our cross-modality feature analysis frameworkby achieving state-of-the-art accuracy for action classification on fourchallenging benchmark datasets, for which we reduce the error rate by more than40% in three datasets and saturating the benchmark with perfect accuracy forthe other one.
arxiv-16500-190 | Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating Clustering-based Predictors | http://arxiv.org/pdf/1603.07094v1.pdf | author:Motohide Higaki, Kai Morino, Hiroshi Murata, Ryo Asaoka, Kenji Yamanishi category:stat.ML cs.LG published:2016-03-23 summary:This study addresses the issue of predicting the glaucomatous visual fieldloss from patient disease datasets. Our goal is to accurately predict theprogress of the disease in individual patients. As very few measurements areavailable for each patient, it is difficult to produce good predictors forindividuals. A recently proposed clustering-based method enhances the power ofprediction using patient data with similar spatiotemporal patterns. Eachpatient is categorized into a cluster of patients, and a predictive model isconstructed using all of the data in the class. Predictions are highlydependent on the quality of clustering, but it is difficult to identify thebest clustering method. Thus, we propose a method for aggregating cluster-basedpredictors to obtain better prediction accuracy than from a singlecluster-based prediction. Further, the method shows very high performances byhierarchically aggregating experts generated from several cluster-basedmethods. We use real datasets to demonstrate that our method performssignificantly better than conventional clustering-based and patient-wiseregression methods, because the hierarchical aggregating strategy has amechanism whereby good predictors in a small community can thrive.
arxiv-16500-191 | Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement and Retrieval | http://arxiv.org/pdf/1503.08248v3.pdf | author:Xirong Li, Tiberio Uricchio, Lamberto Ballan, Marco Bertini, Cees G. M. Snoek, Alberto Del Bimbo category:cs.IR cs.CV cs.MM cs.SI H.3.1; H.3.3 published:2015-03-28 summary:Where previous reviews on content-based image retrieval emphasize on what canbe seen in an image to bridge the semantic gap, this survey considers whatpeople tag about an image. A comprehensive treatise of three closely linkedproblems, i.e., image tag assignment, refinement, and tag-based image retrievalis presented. While existing works vary in terms of their targeted tasks andmethodology, they rely on the key functionality of tag relevance, i.e.estimating the relevance of a specific tag with respect to the visual contentof a given image and its social context. By analyzing what information aspecific method exploits to construct its tag relevance function and how suchinformation is exploited, this paper introduces a taxonomy to structure thegrowing literature, understand the ingredients of the main works, clarify theirconnections and difference, and recognize their merits and limitations. For ahead-to-head comparison between the state-of-the-art, a new experimentalprotocol is presented, with training sets containing 10k, 100k and 1m imagesand an evaluation on three test sets, contributed by various research groups.Eleven representative works are implemented and evaluated. Putting all thistogether, the survey aims to provide an overview of the past and fosterprogress for the near future.
arxiv-16500-192 | Deep Ranking for Person Re-identification via Joint Representation Learning | http://arxiv.org/pdf/1505.06821v2.pdf | author:Shi-Zhe Chen, Chun-Chao Guo, Jian-Huang Lai category:cs.CV published:2015-05-26 summary:This paper proposes a novel approach to person re-identification, afundamental task in distributed multi-camera surveillance systems. Although avariety of powerful algorithms have been presented in the past few years, mostof them usually focus on designing hand-crafted features and learning metricseither individually or sequentially. Different from previous works, weformulate a unified deep ranking framework that jointly tackles both of thesekey components to maximize their strengths. We start from the principle thatthe correct match of the probe image should be positioned in the top rankwithin the whole gallery set. An effective learning-to-rank algorithm isproposed to minimize the cost corresponding to the ranking disorders of thegallery. The ranking model is solved with a deep convolutional neural network(CNN) that builds the relation between input image pairs and their similarityscores through joint representation learning directly from raw image pixels.The proposed framework allows us to get rid of feature engineering and does notrely on any assumption. An extensive comparative evaluation is given,demonstrating that our approach significantly outperforms all state-of-the-artapproaches, including both traditional and CNN-based methods on the challengingVIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has betterability to generalize across datasets without fine-tuning.
arxiv-16500-193 | Semantic Object Parsing with Graph LSTM | http://arxiv.org/pdf/1603.07063v1.pdf | author:Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV published:2016-03-23 summary:By taking the semantic object parsing task as an exemplar applicationscenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,which is the generalization of LSTM from sequential data or multi-dimensionaldata to general graph-structured data. Particularly, instead of evenly andfixedly dividing an image to pixels or patches in existing multi-dimensionalLSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take eacharbitrary-shaped superpixel as a semantically consistent node, and adaptivelyconstruct an undirected graph for each image, where the spatial relations ofthe superpixels are naturally used as edges. Constructed on such an adaptivegraph topology, the Graph LSTM is more naturally aligned with the visualpatterns in the image (e.g., object boundaries or appearance similarities) andprovides a more economical information propagation route. Furthermore, for eachoptimization step over Graph LSTM, we propose to use a confidence-driven schemeto update the hidden and memory states of nodes progressively till all nodesare updated. In addition, for each node, the forgets gates are adaptivelylearned to capture different degrees of semantic correlation with neighboringnodes. Comprehensive evaluations on four diverse semantic object parsingdatasets well demonstrate the significant superiority of our Graph LSTM overother state-of-the-art solutions.
arxiv-16500-194 | A Survey on Object Detection in Optical Remote Sensing Images | http://arxiv.org/pdf/1603.06201v2.pdf | author:Gong Cheng, Junwei Han category:cs.CV published:2016-03-20 summary:Object detection in optical remote sensing images, being a fundamental butchallenging problem in the field of aerial and satellite image analysis, playsan important role for a wide range of applications and is receiving significantattention in recent years. While enormous methods exist, a deep review of theliterature concerning generic object detection is still lacking. This paperaims to provide a review of the recent progress in this field. Different fromseveral previously published surveys that focus on a specific object class suchas building and road, we concentrate on more generic object categoriesincluding, but are not limited to, road, building, tree, vehicle, ship,airport, urban-area. Covering about 270 publications we survey 1) templatematching-based object detection methods, 2) knowledge-based object detectionmethods, 3) object-based image analysis (OBIA)-based object detection methods,4) machine learning-based object detection methods, and 5) five publiclyavailable datasets and three standard evaluation metrics. We also discuss thechallenges of current studies and propose two promising research directions,namely deep learning-based feature representation and weakly supervisedlearning-based geospatial object detection. It is our hope that this surveywill be beneficial for the researchers to have better understanding of thisresearch field.
arxiv-16500-195 | Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief Problem | http://arxiv.org/pdf/1603.07051v1.pdf | author:Mohamed El Yafrani, Belaïd Ahiod category:cs.AI cs.DS cs.NE published:2016-03-23 summary:Real-world problems are very difficult to optimize. However, many researchershave been solving benchmark problems that have been extensively investigatedfor the last decades even if they have very few direct applications. TheTraveling Thief Problem (TTP) is a NP-hard optimization problem that aims toprovide a more realistic model. TTP targets particularly routing problem underpacking/loading constraints which can be found in supply chain management andtransportation. In this paper, TTP is presented and formulated mathematically.A combined local search algorithm is proposed and compared with Random LocalSearch (RLS) and Evolutionary Algorithm (EA). The obtained results are quitepromising since new better solutions were found.
arxiv-16500-196 | Recurrent Neural Network Encoder with Attention for Community Question Answering | http://arxiv.org/pdf/1603.07044v1.pdf | author:Wei-Ning Hsu, Yu Zhang, James Glass category:cs.CL cs.LG cs.NE published:2016-03-23 summary:We apply a general recurrent neural network (RNN) encoder framework tocommunity question answering (cQA) tasks. Our approach does not rely on anylinguistic processing, and can be applied to different languages or domains.Further improvements are observed when we extend the RNN encoders with a neuralattention mechanism that encourages reasoning over entire sequences. To dealwith practical issues such as data sparsity and imbalanced labels, we applyvarious techniques such as transfer learning and multitask learning. Ourexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP scorecompared to an information retrieval-based approach, and achieve comparableperformance to a strong handcrafted feature-based method.
arxiv-16500-197 | MOON: A Mixed Objective Optimization Network for the Recognition of Facial Attributes | http://arxiv.org/pdf/1603.07027v1.pdf | author:Ethan Rudd, Manuel Günther, Terrance Boult category:cs.CV published:2016-03-22 summary:Multi-task vision problems can often be decomposed into separate tasks andstages, e.g., separating feature extraction and model building, or trainingindependent models for each task. Joint optimization has been shown to improveperformance, but can be difficult to apply to deep convolution neural networks(DCNN), especially with unbalanced data. This paper introduces a novel mixedobjective optimization network (MOON), with a loss function which mixes errorsfrom multiple tasks and supports domain adaptation when label frequenciesdiffer between training and operational testing. Experiments demonstrate thatnot only does MOON advance the state of the art in facial attributerecognition, but it also outperforms independently trained DCNNs using the samedata.
arxiv-16500-198 | Active Detection and Localization of Textureless Objects in Cluttered Environments | http://arxiv.org/pdf/1603.07022v1.pdf | author:Marco Imperoli, Alberto Pretto category:cs.CV cs.RO published:2016-03-22 summary:This paper introduces an active object detection and localization frameworkthat combines a robust untextured object detection and 3D pose estimationalgorithm with a novel next-best-view selection strategy. We address thedetection and localization problems by proposing an edge-based registrationalgorithm that refines the object position by minimizing a cost directlyextracted from a 3D image tensor that encodes the minimum distance to an edgepoint in a joint direction/location space. We face the next-best-view problemby exploiting a sequential decision process that, for each step, selects thenext camera position which maximizes the mutual information between the stateand the next observations. We solve the intrinsic intractability of thissolution by generating observations that represent scene realizations, i.e.combination samples of object hypothesis provided by the object detector, whilemodeling the state by means of a set of constantly resampled particles.Experiments performed on different real world, challenging datasets confirm theeffectiveness of the proposed methods.
arxiv-16500-199 | BLC: Private Matrix Factorization Recommenders via Automatic Group Learning | http://arxiv.org/pdf/1509.05789v2.pdf | author:Alessandro Checco, Giuseppe Bianchi, Doug Leith category:cs.LG stat.ML published:2015-09-18 summary:We propose a privacy-enhanced matrix factorization recommender that exploitsthe fact that users can often be grouped together by interest. This allows aform of "hiding in the crowd" privacy. We introduce a novel matrixfactorization approach suited to making recommendations in a shared group (ornym) setting and the BLC algorithm for carrying out this matrix factorizationin a privacy-enhanced manner. We demonstrate that the increased privacy doesnot come at the cost of reduced recommendation accuracy.
arxiv-16500-200 | Word Sense Disambiguation with Neural Language Models | http://arxiv.org/pdf/1603.07012v1.pdf | author:Dayu Yuan, Ryan Doherty, Julian Richardson, Colin Evans, Eric Altendorf category:cs.CL published:2016-03-22 summary:Determining the intended sense of words in text -- word sense disambiguation(WSD) -- is a long-standing problem in natural language processing. In thispaper, we present WSD algorithms which use neural network language models toachieve state-of-the-art precision. Each of these methods learns todisambiguate word senses using only a set of word senses, a few examplesentences for each sense taken from a licensed lexicon, and a large unlabeledtext corpus. We classify based on cosine similarity of vectors derived from thecontexts in unlabeled query and labeled example sentences. We demonstratestate-of-the-art results when using the WordNet sense inventory, andsignificantly better than baseline performance using the New Oxford AmericanDictionary inventory. The best performance was achieved by combining an LSTMlanguage model with graph label propagation.
arxiv-16500-201 | Knowledge Transfer for Scene-specific Motion Prediction | http://arxiv.org/pdf/1603.06987v1.pdf | author:Lamberto Ballan, Francesco Castaldo, Alexandre Alahi, Francesco Palmieri, Silvio Savarese category:cs.CV published:2016-03-22 summary:When given a single frame of the video, humans can not only interpret thecontent of the scene, but also they are able to forecast the near future. Thisability is mostly driven by their rich prior knowledge about the visual world,both in terms of (\emph{i}) the dynamics of moving agents, as well as(\emph{ii}) the semantic of the scene. In this work we exploit the interplaybetween these two key elements to predict scene-specific motion patterns.First, we extract patch descriptors encoding the probability of moving to theadjacent patches, and the probability of being in that particular patch orchanging behavior. Then, we introduce a Dynamic Bayesian Network which exploitsthis scene specific knowledge for trajectory prediction. Experimental resultsdemonstrate that our method is able to accurately predict trajectories andtransfer predictions to a novel scene characterized by similar elements.
arxiv-16500-202 | Mutual Information and Diverse Decoding Improve Neural Machine Translation | http://arxiv.org/pdf/1601.00372v2.pdf | author:Jiwei Li, Dan Jurafsky category:cs.CL cs.AI published:2016-01-04 summary:Sequence-to-sequence neural translation models learn semantic and syntacticrelations between sentence pairs by optimizing the likelihood of the targetgiven the source, i.e., $p(yx)$, an objective that ignores other potentiallyuseful sources of information. We introduce an alternative objective functionfor neural MT that maximizes the mutual information between the source andtarget sentences, modeling the bi-directional dependency of sources andtargets. We implement the model with a simple re-ranking method, and alsointroduce a decoding algorithm that increases diversity in the N-best listproduced by the first pass. Applied to the WMT German/English andFrench/English tasks, the proposed models offers a consistent performance booston both standard LSTM and attention-based neural MT architectures.
arxiv-16500-203 | Segmental Spatio-Temporal CNNs for Fine-grained Action Segmentation and Classification | http://arxiv.org/pdf/1602.02995v2.pdf | author:Colin Lea, Austin Reiter, Rene Vidal, Gregory D. Hager category:cs.CV cs.RO published:2016-02-09 summary:Joint segmentation and classification of fine-grained actions is importantfor applications in human-robot interaction, video surveillance, and humanskill evaluation. However, despite substantial recent progress in large scaleaction classification, the performance of state-of-the-art fine-grained actionrecognition approaches remains low. In this paper, we propose a newspatio-temporal CNN model for fine-grained action classification andsegmentation, which combines (1) a spatial CNN to represent objects in thescene and their spatial relationships; (2) a temporal CNN that captures howobject relationships within an action change over time; and (3) a semi-Markovmodel that captures transitions from one action to another. In addition, weintroduce an efficient segmental inference algorithm for joint segmentation andclassification of actions that is orders of magnitude faster thanstate-of-the-art approaches. We highlight the effectiveness of our approach oncooking and surgical action datasets for which we observe substantiallyimproved performance relative to recent baseline methods.
arxiv-16500-204 | Stacked Hourglass Networks for Human Pose Estimation | http://arxiv.org/pdf/1603.06937v1.pdf | author:Alejandro Newell, Kaiyu Yang, Jia Deng category:cs.CV published:2016-03-22 summary:This work introduces a novel Convolutional Network architecture for the taskof human pose estimation. Features are processed across all scales andconsolidated to best capture the various spatial relationships associated withthe body. We show how repeated bottom-up, top-down processing used inconjunction with intermediate supervision is critical to improving theperformance of the network. We refer to the architecture as a 'stackedhourglass' network based on the successive steps of pooling and upsampling thatare done to produce a final set of estimates. State-of-the-art results areachieved on the FLIC and MPII benchmarks outcompeting all recent methods.
arxiv-16500-205 | Inference via Message Passing on Partially Labeled Stochastic Block Models | http://arxiv.org/pdf/1603.06923v1.pdf | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH published:2016-03-22 summary:We study the community detection and recovery problem in partially-labeledstochastic block models (SBM). We develop a fast linearized message-passingalgorithm to reconstruct labels for SBM (with $n$ nodes, $k$ blocks, $p,q$intra and inter block connectivity) when $\delta$ proportion of node labels arerevealed. The signal-to-noise ratio ${\sf SNR}(n,k,p,q,\delta)$ is shown tocharacterize the fundamental limitations of inference via local algorithms. Onthe one hand, when ${\sf SNR}>1$, the linearized message-passing algorithmprovides the statistical inference guarantee with mis-classification rate atmost $\exp(-({\sf SNR}-1)/2)$, thus interpolating smoothly between strong andweak consistency. This exponential dependence improves upon the known errorrate $({\sf SNR}-1)^{-1}$ in the literature on weak recovery. On the otherhand, when ${\sf SNR}<1$ (for $k=2$) and ${\sf SNR}<1/4$ (for general growing$k$), we prove that local algorithms suffer an error rate at least $\frac{1}{2}- \sqrt{\delta \cdot {\sf SNR}}$, which is only slightly better than randomguess for small $\delta$.
arxiv-16500-206 | Completely random measures for modeling power laws in sparse graphs | http://arxiv.org/pdf/1603.06915v1.pdf | author:Diana Cai, Tamara Broderick category:stat.ML math.ST stat.ME stat.TH published:2016-03-22 summary:Network data appear in a number of applications, such as online socialnetworks and biological networks, and there is growing interest in bothdeveloping models for networks as well as studying the properties of such data.Since individual network datasets continue to grow in size, it is necessary todevelop models that accurately represent the real-life scaling properties ofnetworks. One behavior of interest is having a power law in the degreedistribution. However, other types of power laws that have been observedempirically and considered for applications such as clustering and featureallocation models have not been studied as frequently in models for graph data.In this paper, we enumerate desirable asymptotic behavior that may be ofinterest for modeling graph data, including sparsity and several types of powerlaws. We outline a general framework for graph generative models usingcompletely random measures; by contrast to the pioneering work of Caron and Fox(2015), we consider instantiating more of the existing atoms of the randommeasure as the dataset size increases rather than adding new atoms to themeasure. We see that these two models can be complementary; they respectivelyyield interpretations as (1) time passing among existing members of a networkand (2) new individuals joining a network. We detail a particular instance ofthis framework and show simulated results that suggest this model exhibits somedesirable asymptotic power-law behavior.
arxiv-16500-207 | Edge-exchangeable graphs and sparsity | http://arxiv.org/pdf/1603.06898v1.pdf | author:Tamara Broderick, Diana Cai category:math.ST stat.ME stat.ML stat.TH published:2016-03-22 summary:A known failing of many popular random graph models is that the Aldous-HooverTheorem guarantees these graphs are dense with probability one; that is, thenumber of edges grows quadratically with the number of nodes. This behavior isconsidered unrealistic in observed graphs. We define a notion of edgeexchangeability for random graphs in contrast to the established notion ofinfinite exchangeability for random graphs --- which has traditionally reliedon exchangeability of nodes (rather than edges) in a graph. We show that,unlike node exchangeability, edge exchangeability encompasses models that areknown to provide a projective sequence of random graphs that circumvent theAldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of thenumber of edges with the number of nodes. We show how edge-exchangeability ofgraphs relates naturally to existing notions of exchangeability from clustering(a.k.a. partitions) and other familiar combinatorial structures.
arxiv-16500-208 | Homology Computation of Large Point Clouds using Quantum Annealing | http://arxiv.org/pdf/1512.09328v2.pdf | author:Raouf Dridi, Hedayat Alghassi category:quant-ph cs.LG published:2015-12-23 summary:Homology is a tool in topological data analysis which measures the shape ofthe data. In many cases, these measurements translate into new insights whichare not readily available by other means. To compute homology, we rely onmathematical constructions which scale exponentially with the size of the data.Therefore, for large point clouds, the computation is infeasible usingclassical computers. In this paper, we present a quantum annealing pipeline forcomputation of homology of large point clouds. It is designed to workconcurrently with resizable cloud computing platforms. The pipeline takes asinput a witness graph approximating the given point cloud. It uses quantumannealing to compute a clique covering of the graph and then uses this cover toconstruct a Mayer-Vietoris complex. The pipeline terminates by performing asimplified homology computation of the Mayer-Vietoris complex in parallel. Wehave designed three different clique coverings and their quantum annealingformulation with which our algorithm exhibits an exponential speed-up overclassical implementations. In fact, not only the computation is simplified butalso the simplicial complex construction itself is greatly simplified. We havealso included tests using D-Wave 2X quantum processor.
arxiv-16500-209 | Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons | http://arxiv.org/pdf/1603.06881v1.pdf | author:Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML published:2016-03-22 summary:We study methods for aggregating pairwise comparison data in order toestimate outcome probabilities for future comparisons among a collection of nitems. Working within a flexible framework that imposes only a form of strongstochastic transitivity (SST), we introduce an adaptivity index defined by theindifference sets of the pairwise comparison probabilities. In addition tomeasuring the usual worst-case risk of an estimator, this adaptivity index alsocaptures the extent to which the estimator adapts to instance-specificdifficulty relative to an oracle estimator. We prove three main results thatinvolve this adaptivity index and different algorithms. First, we propose athree-step estimator termed Count-Randomize-Least squares (CRL), and show thatit has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors.We then show that that conditional on the hardness of planted clique, nocomputationally efficient estimator can achieve an adaptivity index smallerthan $\sqrt{n}$. Second, we show that a regularized least squares estimator canachieve a poly-logarithmic adaptivity index, thereby demonstrating a$\sqrt{n}$-gap between optimal and computationally achievable adaptivity.Finally, we prove that the standard least squares estimator, which is known tobe optimally adaptive in several closely related problems, fails to adapt inthe context of estimating pairwise probabilities.
arxiv-16500-210 | Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems | http://arxiv.org/pdf/1505.05114v2.pdf | author:Yuxin Chen, Emmanuel J. Candes category:cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH published:2015-05-19 summary:We consider the fundamental problem of solving quadratic systems of equationsin $n$ variables, where $y_i = \langle \boldsymbol{a}_i, \boldsymbol{x}\rangle^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ isunknown. We propose a novel method, which starting with an initial guesscomputed by means of a spectral method, proceeds by minimizing a nonconvexfunctional as in the Wirtinger flow approach. There are several keydistinguishing features, most notably, a distinct objective functional andnovel update rules, which operate in an adaptive fashion and drop terms bearingtoo much influence on the search direction. These careful selection rulesprovide a tighter initial guess, better descent directions, and thus enhancedpractical performance. On the theoretical side, we prove that for certainunstructured models of quadratic systems, our algorithms return the correctsolution in linear time, i.e. in time proportional to reading the data$\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between thenumber of equations and unknowns exceeds a fixed numerical constant. We extendthe theory to deal with noisy systems in which we only have $y_i \approx\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle^2$ and prove that ouralgorithms achieve a statistical accuracy, which is nearly un-improvable. Wecomplement our theoretical study with numerical examples showing that solvingrandom quadratic systems is both computationally and statistically not muchharder than solving linear systems of the same size---hence the title of thispaper. For instance, we demonstrate empirically that the computational cost ofour algorithm is about four times that of solving a least-squares problem ofthe same size.
arxiv-16500-211 | Trading-off variance and complexity in stochastic gradient descent | http://arxiv.org/pdf/1603.06861v1.pdf | author:Vatsal Shah, Megasthenis Asteris, Anastasios Kyrillidis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT math.OC published:2016-03-22 summary:Stochastic gradient descent is the method of choice for large-scale machinelearning problems, by virtue of its light complexity per iteration. However, itlags behind its non-stochastic counterparts with respect to the convergencerate, due to high variance introduced by the stochastic updates. The popularStochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,introducing a new update rule which requires infrequent passes over the entireinput dataset to compute the full-gradient. In this work, we propose CheapSVRG, a stochastic variance-reductionoptimization scheme. Our algorithm is similar to SVRG but instead of the fullgradient, it uses a surrogate which can be efficiently computed on a smallsubset of the input data. It achieves a linear convergence rate ---up to someerror level, depending on the nature of the optimization problem---and featuresa trade-off between the computational complexity and the convergence rate.Empirical evaluation shows that CheapSVRG performs at least competitivelycompared to the state of the art.
arxiv-16500-212 | Enhanced perceptrons using contrastive biclusters | http://arxiv.org/pdf/1603.06859v1.pdf | author:André L. V. Coelho, Fabrício O. de França category:cs.NE cs.LG stat.ML published:2016-03-22 summary:Perceptrons are neuronal devices capable of fully discriminating linearlyseparable classes. Although straightforward to implement and train, theirapplicability is usually hindered by non-trivial requirements imposed byreal-world classification problems. Therefore, several approaches, such askernel perceptrons, have been conceived to counteract such difficulties. Inthis paper, we investigate an enhanced perceptron model based on the notion ofcontrastive biclusters. From this perspective, a good discriminative biclustercomprises a subset of data instances belonging to one class that show highcoherence across a subset of features and high differentiation from nearestinstances of the other class under the same features (referred to as itscontrastive bicluster). Upon each local subspace associated with a pair ofcontrastive biclusters a perceptron is trained and the model with highest areaunder the receiver operating characteristic curve (AUC) value is selected asthe final classifier. Experiments conducted on a range of data sets, includingthose related to a difficult biosignal classification problem, show that theproposed variant can be indeed very useful, prevailing in most of the casesupon standard and kernel perceptrons in terms of accuracy and AUC measures.
arxiv-16500-213 | Learning to Understand Phrases by Embedding the Dictionary | http://arxiv.org/pdf/1504.00548v4.pdf | author:Felix Hill, Kyunghyun Cho, Anna Korhonen, Yoshua Bengio category:cs.CL published:2015-04-02 summary:Distributional models that learn rich semantic word representations are asuccess story of recent NLP research. However, developing models that learnuseful representations of phrases and sentences has proved far harder. Wepropose using the definitions found in everyday dictionaries as a means ofbridging this gap between lexical and phrasal semantics. Neural languageembedding models can be effectively trained to map dictionary definitions(phrases) to (lexical) representations of the words defined by thosedefinitions. We present two applications of these architectures: "reversedictionaries" that return the name of a concept given a definition ordescription and general-knowledge crossword question answerers. On both tasks,neural language embedding models trained on definitions from a handful offreely-available lexical resources perform as well or better than existingcommercial systems that rely on significant task-specific engineering. Theresults highlight the effectiveness of both neural embedding architectures anddefinition-based training for developing models that understand phrases andsentences.
arxiv-16500-214 | Multi-velocity neural networks for gesture recognition in videos | http://arxiv.org/pdf/1603.06829v1.pdf | author:Otkrist Gupta, Dan Raviv, Ramesh Raskar category:cs.CV cs.LG published:2016-03-22 summary:We present a new action recognition deep neural network which adaptivelylearns the best action velocities in addition to the classification. While deepneural networks have reached maturity for image understanding tasks, we arestill exploring network topologies and features to handle the richerenvironment of video clips. Here, we tackle the problem of multiple velocitiesin action recognition, and provide state-of-the-art results for gesturerecognition, on known and new collected datasets. We further provide thetraining steps for our semi-supervised network, suited to learn from hugeunlabeled datasets with only a fraction of labeled examples.
arxiv-16500-215 | Patterns of Scalable Bayesian Inference | http://arxiv.org/pdf/1602.05221v2.pdf | author:Elaine Angelino, Matthew James Johnson, Ryan P. Adams category:stat.ML published:2016-02-16 summary:Datasets are growing not just in size but in complexity, creating a demandfor rich models and quantification of uncertainty. Bayesian methods are anexcellent fit for this demand, but scaling Bayesian inference is a challenge.In response to this challenge, there has been considerable recent work based onvarying assumptions about model structure, underlying computational resources,and the importance of asymptotic correctness. As a result, there is a zoo ofideas with few clear overarching principles. In this paper, we seek to identify unifying principles, patterns, andintuitions for scaling Bayesian inference. We review existing work on utilizingmodern computing resources with both MCMC and variational approximationtechniques. From this taxonomy of ideas, we characterize the general principlesthat have proven successful for designing scalable inference procedures andcomment on the path forward.
arxiv-16500-216 | Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus | http://arxiv.org/pdf/1603.06807v1.pdf | author:Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE published:2016-03-22 summary:Over the past decade, large-scale supervised learning corpora have enabledmachine learning researchers to make substantial advances. However, to thisdate, there are no large-scale question-answer corpora available. In this paperwe present the 30M Factoid Question-Answer Corpus, an enormous question-answerpair corpus produced by applying a novel neural network architecture on theknowledge base Freebase to transduce facts into natural language questions. Theproduced question-answer pairs are evaluated both by human evaluators and usingautomatic evaluation metrics, including well-established machine translationand sentence similarity metrics. Across all evaluation criteria thequestion-generation model outperforms the competing template-based baseline.Furthermore, when presented to human evaluators, the generated questions appearto be indistinguishable from real human-generated questions.
arxiv-16500-217 | Using real-time cluster configurations of streaming asynchronous features as online state descriptors in financial markets | http://arxiv.org/pdf/1603.06805v1.pdf | author:Dieter Hendricks category:q-fin.TR cs.LG q-fin.CP published:2016-03-22 summary:We present a scheme for online, unsupervised state discovery and detectionfrom streaming, multi-featured, asynchronous data in high-frequency financialmarkets. Online feature correlations are computed using an unbiased, losslessFourier estimator. A high-speed maximum likelihood clustering algorithm is thenused to find the feature cluster configuration which best explains thestructure in the correlation matrix. We conjecture that this featureconfiguration is a candidate descriptor for the temporal state of the system.Using a simple cluster configuration similarity metric, we are able toenumerate the state space based on prevailing feature configurations. Theproposed state representation removes the need for human-driven datapre-processing for state attribute specification, allowing a learning agent tofind structure in streaming data, discern changes in the system, enumerate itsperceived state space and learn suitable action-selection policies.
arxiv-16500-218 | Adaptive Parameter Selection in Evolutionary Algorithms by Reinforcement Learning with Dynamic Discretization of Parameter Range | http://arxiv.org/pdf/1603.06788v1.pdf | author:Arkady Rost, Irina Petrova, Arina Buzdalova category:cs.NE 68T05 G.1.6; I.2.6 published:2016-03-22 summary:Online parameter controllers for evolutionary algorithms adjust values ofparameters during the run of an evolutionary algorithm. Recently a newefficient parameter controller based on reinforcement learning was proposed byKarafotias et al. In this method ranges of parameters are discretized intoseveral intervals before the run. However, performing adaptive discretizationduring the run may increase efficiency of an evolutionary algorithm. Aleti etal. proposed another efficient controller with adaptive discretization. In the present paper we propose a parameter controller based on reinforcementlearning with adaptive discretization. The proposed controller is compared withthe existing parameter adjusting methods on several test problems usingdifferent configurations of an evolutionary algorithm. For the test problems,we consider four continuous functions, namely the sphere function, theRosenbrock function, the Levi function and the Rastrigin function. Results showthat the new controller outperforms the other controllers on most of theconsidered test problems.
arxiv-16500-219 | Multi-domain machine translation enhancements by parallel data extraction from comparable corpora | http://arxiv.org/pdf/1603.06785v1.pdf | author:Krzysztof Wołk, Emilia Rejmund, Krzysztof Marasek category:cs.CL stat.ML published:2016-03-22 summary:Parallel texts are a relatively rare language resource, however, theyconstitute a very useful research material with a wide range of applications.This study presents and analyses new methodologies we developed for obtainingsuch data from previously built comparable corpora. The methodologies areautomatic and unsupervised which makes them good for large scale research. Thetask is highly practical as non-parallel multilingual data occur much morefrequently than parallel corpora and accessing them is easy, although parallelsentences are a considerably more useful resource. In this study, we propose amethod of automatic web crawling in order to build topic-aligned comparablecorpora, e.g. based on the Wikipedia or Euronews.com. We also developed newmethods of obtaining parallel sentences from comparable data and proposedmethods of filtration of corpora capable of selecting inconsistent or onlypartially equivalent translations. Our methods are easily scalable to otherlanguages. Evaluation of the quality of the created corpora was performed byanalysing the impact of their use on statistical machine translation systems.Experiments were presented on the basis of the Polish-English language pair fortexts from different domains, i.e. lectures, phrasebooks, film dialogues,European Parliament proceedings and texts contained medicines leaflets. We alsotested a second method of creating parallel corpora based on data fromcomparable corpora which allows for automatically expanding the existing corpusof sentences about a given domain on the basis of analogies found between them.It does not require, therefore, having past parallel resources in order totrain a classifier.
arxiv-16500-220 | Doubly Random Parallel Stochastic Methods for Large Scale Learning | http://arxiv.org/pdf/1603.06782v1.pdf | author:Aryan Mokhtari, Alec Koppel, Alejandro Ribeiro category:cs.LG math.OC published:2016-03-22 summary:We consider learning problems over training sets in which both, the number oftraining examples and the dimension of the feature vectors, are large. To solvethese problems we propose the random parallel stochastic algorithm (RAPSA). Wecall the algorithm random parallel because it utilizes multiple processors tooperate in a randomly chosen subset of blocks of the feature vector. We callthe algorithm parallel stochastic because processors choose elements of thetraining set randomly and independently. Algorithms that are parallel in eitherof these dimensions exist, but RAPSA is the first attempt at a methodology thatis parallel in both, the selection of blocks and the selection of elements ofthe training set. In RAPSA, processors utilize the randomly chosen functions tocompute the stochastic gradient component associated with a randomly chosenblock. The technical contribution of this paper is to show that this minimallycoordinated algorithm converges to the optimal classifier when the trainingobjective is convex. In particular, we show that: (i) When using decreasingstepsizes, RAPSA converges almost surely over the random choice of blocks andfunctions. (ii) When using constant stepsizes, convergence is to a neighborhoodof optimality with a rate that is linear in expectation. RAPSA is numericallyevaluated on the MNIST digit recognition problem.
arxiv-16500-221 | Energy-Efficient ConvNets Through Approximate Computing | http://arxiv.org/pdf/1603.06777v1.pdf | author:Bert Moons, Bert De Brabandere, Luc Van Gool, Marian Verhelst category:cs.CV published:2016-03-22 summary:Recently ConvNets or convolutional neural networks (CNN) have come up asstate-of-the-art classification and detection algorithms, achieving near-humanperformance in visual detection. However, ConvNet algorithms are typically verycomputation and memory intensive. In order to be able to embed ConvNet-basedclassification into wearable platforms and embedded systems such as smartphonesor ubiquitous electronics for the internet-of-things, their energy consumptionshould be reduced drastically. This paper proposes methods based on approximatecomputing to reduce energy consumption in state-of-the-art ConvNetaccelerators. By combining techniques both at the system- and circuit level, wecan gain energy in the systems arithmetic: up to 30x without losingclassification accuracy and more than 100x at 99% classification accuracy,compared to the commonly used 16-bit fixed point number format.
arxiv-16500-222 | Fully Convolutional Attention Localization Networks: Efficient Attention Localization for Fine-Grained Recognition | http://arxiv.org/pdf/1603.06765v1.pdf | author:Xiao Liu, Tian Xia, Jiang Wang, Yuanqing Lin category:cs.CV published:2016-03-22 summary:Fine-grained recognition is challenging mainly because the inter-classdifferences between fine-grained classes are usually local and subtle whileintra-class differences could be large due to pose variations. In order todistinguish them from intra-class variations, it is essential to zoom in onhighly discriminative local regions. In this work, we introduce a reinforcementlearning-based fully convolutional attention localization network to adaptivelyselect multiple task-driven visual attention regions. We show that zooming inon the selected attention regions significantly improves the performance offine-grained recognition. Compared to previous reinforcement learning-basedmodels, the proposed approach is noticeably more computationally efficientduring both training and testing because of its fully-convolutionalarchitecture, and it is capable of simultaneous focusing its glimpse onmultiple visual attention regions. The experiments demonstrate that theproposed method achieves notably higher classification accuracy on threebenchmark fine-grained recognition datasets: Stanford Dogs, Stanford Cars, andCUB-200-2011.
arxiv-16500-223 | Convolution in Convolution for Network in Network | http://arxiv.org/pdf/1603.06759v1.pdf | author:Yanwei Pang, Manli Sun, Xiaoheng Jiang, Xuelong Li category:cs.CV published:2016-03-22 summary:Network in Netwrok (NiN) is an effective instance and an important extensionof Convolutional Neural Network (CNN) consisting of alternating convolutionallayers and pooling layers. Instead of using a linear filter for convolution,NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, toreplace the linear filter. Because of the powerfulness of MLP and $ 1\times 1 $convolutions in spatial domain, NiN has stronger ability of featurerepresentation and hence results in better recognition rate. However, MLPitself consists of fully connected layers which give rise to a large number ofparameters. In this paper, we propose to replace dense shallow MLP with sparseshallow MLP. One or more layers of the sparse shallow MLP are sparely connectedin the channel dimension or channel-spatial domain. The proposed method isimplemented by applying unshared convolution across the channel dimension andapplying shared convolution across the spatial dimension in some computationallayers. The proposed method is called CiC. Experimental results on the CIFAR10dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate theeffectiveness of the proposed CiC method.
arxiv-16500-224 | Design of Kernels in Convolutional Neural Networks for Image Classification | http://arxiv.org/pdf/1511.09231v2.pdf | author:Zhun Sun, Mete Ozay, Takayuki Okatani category:cs.CV published:2015-11-30 summary:Despite the effectiveness of Convolutional Neural Networks (CNNs) for imageclassification, our understanding of the relationship between shape ofconvolution kernels and learned representations is limited. In this work, weexplore and employ the relationship between shape of kernels which defineReceptive Fields (RFs) in CNNs for learning of feature representations andimage classification. For this purpose, we first propose a featurevisualization method for visualization of pixel-wise classification score mapsof learned features. Motivated by our experimental results, and observationsreported in the literature for modeling of visual systems, we propose a noveldesign of shape of kernels for learning of representations in CNNs. In theexperimental results, we achieved a state-of-the-art classification performancecompared to a base CNN model [28] by reducing the number of parameters andcomputational time of the model using the ILSVRC-2012 dataset [24]. Theproposed models also outperform the state-of-the-art models employed on theCIFAR-10/100 datasets [12] for image classification. Additionally, we analyzedthe robustness of the proposed method to occlusion for classification ofpartially occluded images compared with the state-of-the-art methods. Ourresults indicate the effectiveness of the proposed approach.
arxiv-16500-225 | Latent Predictor Networks for Code Generation | http://arxiv.org/pdf/1603.06744v1.pdf | author:Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin Wang, Phil Blunsom category:cs.CL cs.NE published:2016-03-22 summary:Many language generation tasks require the production of text conditioned onboth structured and unstructured inputs. We present a novel neural networkarchitecture which generates an output sequence conditioned on an arbitrarynumber of input functions. Crucially, our approach allows both the choice ofconditioning context and the granularity of generation, for example charactersor tokens, to be marginalised, thus permitting scalable and effective training.Using this framework, we address the problem of generating programming codefrom a mixed natural language and structured specification. We create two newdata sets for this paradigm derived from the collectible trading card gamesMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,we demonstrate that marginalising multiple predictors allows our model tooutperform strong benchmarks.
arxiv-16500-226 | Tree-to-Sequence Attentional Neural Machine Translation | http://arxiv.org/pdf/1603.06075v2.pdf | author:Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL published:2016-03-19 summary:Most of the existing neural machine translation (NMT) models focus on theconversion of sequential data and do not directly take syntax intoconsideration. We propose a novel end-to-end syntactic NMT model, extending asequence-to-sequence model with the source-side phrase structure. Our model hasan attention mechanism that enables the decoder to generate a translated wordwhile softly aligning it with phrases as well as words of the source sentence.Experimental results on the WAT'15 English-to-Japanese dataset demonstrate thatour proposed model outperforms sequence-to-sequence attentional NMT models andcompares favorably with the state-of-the-art tree-to-string SMT system.
arxiv-16500-227 | Recognition from Hand Cameras | http://arxiv.org/pdf/1512.01881v3.pdf | author:Cheng-Sheng Chan, Shou-Zhong Chen, Pei-Xuan Xie, Chiung-Chih Chang, Min Sun category:cs.CV published:2015-12-07 summary:We revisit the study of a wrist-mounted camera system (referred to asHandCam) for recognizing activities of hands. HandCam has two unique propertiesas compared to egocentric systems (referred to as HeadCam): (1) it avoids theneed to detect hands; (2) it more consistently observes the activities ofhands. By taking advantage of these properties, we propose adeep-learning-based method to recognize hand states (free v.s. active hands,hand gestures, object categories), and discover object categories. Moreover, wepropose a novel two-streams deep network to further take advantage of bothHandCam and HeadCam. We have collected a new synchronized HandCam and HeadCamdataset with 20 videos captured in three scenes for hand states recognition.Experiments show that our HandCam system consistently outperforms adeep-learning-based HeadCam method (with estimated manipulation regions) and adense-trajectory-based HeadCam method in all tasks. We also show that HandCamvideos captured by different users can be easily aligned to improve free v.s.active recognition accuracy (3.3% improvement) in across-scenes use case.Moreover, we observe that finetuning Convolutional Neural Network consistentlyimproves accuracy. Finally, our novel two-streams deep network combiningHandCam and HeadCam features achieves the best performance in four out of fivetasks. With more data, we believe a joint HandCam and HeadCam system canrobustly log hand states in daily life.
arxiv-16500-228 | Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings | http://arxiv.org/pdf/1603.06067v2.pdf | author:Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL published:2016-03-19 summary:We present a novel method for jointly learning compositional andnon-compositional phrase embeddings by adaptively weighting both types ofembeddings using a compositionality scoring function. The scoring function isused to quantify the level of compositionality of each phrase, and theparameters of the function are jointly optimized with the objective forlearning phrase embeddings. In experiments, we apply the adaptive jointlearning method to the task of learning embeddings of transitive verb phrases,and show that the compositionality scores have strong correlation with humanratings for verb-object compositionality, substantially outperforming theprevious state of the art. Moreover, our embeddings improve upon the previousbest model on a transitive verb disambiguation task. We also show that a simpleensemble technique further improves the results for both tasks.
arxiv-16500-229 | Generating Natural Questions About an Image | http://arxiv.org/pdf/1603.06059v2.pdf | author:Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Larry Zitnick, Margaret Mitchell, Xiaodong He, Lucy Vanderwende category:cs.CL cs.AI cs.CV published:2016-03-19 summary:There has been an explosion of work in the vision & language community duringthe past few years from image captioning to video transcription, and answeringquestions about images. These tasks focus on literal descriptions of the image.To move beyond the literal, we choose to explore how questions about an imageoften address abstract events that the objects evoke. In this paper, weintroduce the novel task of 'Visual Question Generation (VQG)', where thesystem is tasked with asking a natural and engaging question when shown animage. We provide three datasets which cover a variety of images fromobject-centric to event-centric, providing different and more abstract trainingdata than the state-of-the-art captioning systems have used thus far. We trainand test several generative and retrieval models to tackle the task of VQG.Evaluation results show that while such models ask reasonable questions givenvarious images, there is still a wide gap with human performance. Our proposedtask offers a new challenge to the community which we hope can spur furtherinterest in exploring deeper connections between vision & language.
arxiv-16500-230 | Image Super-Resolution Based on Sparsity Prior via Smoothed $l_0$ Norm | http://arxiv.org/pdf/1603.06680v1.pdf | author:Mohammad Rostami, Zhou Wang category:cs.CV published:2016-03-22 summary:In this paper we aim to tackle the problem of reconstructing ahigh-resolution image from a single low-resolution input image, known as singleimage super-resolution. In the literature, sparse representation has been usedto address this problem, where it is assumed that both low-resolution andhigh-resolution images share the same sparse representation over a pair ofcoupled jointly trained dictionaries. This assumption enables us to use thecompressed sensing theory to find the jointly sparse representation via thelow-resolution image and then use it to recover the high-resolution image.However, sparse representation of a signal over a known dictionary is anill-posed, combinatorial optimization problem. Here we propose an algorithmthat adopts the smoothed $l_0$-norm (SL0) approach to find the jointly sparserepresentation. Improved quality of the reconstructed image is obtained formost images in terms of both peak signal-to-noise-ratio (PSNR) and structuralsimilarity (SSIM) measures.
arxiv-16500-231 | Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis | http://arxiv.org/pdf/1603.06679v1.pdf | author:Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, Xiaokui Xiao category:cs.CL cs.IR cs.LG published:2016-03-22 summary:Aspect-based sentiment analysis has obtained substantial popularity due toits ability to extract useful information from customer reviews. In most cases,aspect terms in a review sentence have strong relations with opinion termsbecause an aspect is the target where an opinion is expressed. With thisconnection, some of the existing work focused on designing syntactic rules todouble propagate information between aspect and opinion terms. However, thesemethods require large amount of efforts and domain knowledge to design precisesyntactic rules and fail to handle uncertainty. In this paper, we propose anovel joint model that integrates recursive neural networks and conditionalrandom fields into a unified framework for aspect-based sentiment analysis. Ourtask is to extract aspect and opinion terms/phrases for each review. Theproposed model is able to learn high-level discriminative features and doublepropagate information between aspect and opinion terms simultaneously.Furthermore, it is flexible to incorporate linguistic or lexicon features intothe proposed model to further boost its performance in terms of informationextraction. Experimental results on the SemEval Challenge 2014 dataset show thesuperiority of our proposed model over several baseline methods as well as thewinning systems.
arxiv-16500-232 | Stitching Stabilizer: Two-frame-stitching Video Stabilization for Embedded Systems | http://arxiv.org/pdf/1603.06678v1.pdf | author:Masaki Satoh category:cs.CV published:2016-03-22 summary:In conventional electronic video stabilization, the stabilized frame isobtained by cropping the input frame to cancel camera shake. While a smallcropping size results in strong stabilization, it does not provide ussatisfactory results from the viewpoint of image quality, because it narrowsthe angle of view. By fusing several frames, we can effectively expand the areaof input frames, and achieve strong stabilization even with a large croppingsize. Several methods for doing so have been studied. However, theircomputational costs are too high for embedded systems such as smartphones. We propose a simple, yet surprisingly effective algorithm, called thestitching stabilizer. It stitches only two frames together with a minimalcomputational cost. It can achieve real-time processes in embedded systems, forFull HD and 30 FPS videos. To clearly show the effect, we apply it tohyperlapse. Using several clips, we show it produces more strongly stabilizedand natural results than the existing solutions from Microsoft and Instagram.
arxiv-16500-233 | Learning Executable Semantic Parsers for Natural Language Understanding | http://arxiv.org/pdf/1603.06677v1.pdf | author:Percy Liang category:cs.CL cs.AI published:2016-03-22 summary:For building question answering systems and natural language interfaces,semantic parsing has emerged as an important and powerful paradigm. Semanticparsers map natural language into logical forms, the classic representation formany important linguistic phenomena. The modern twist is that we are interestedin learning semantic parsers from data, which introduces a new layer ofstatistical and computational issues. This article lays out the components of astatistical semantic parser, highlighting the key challenges. We will see thatsemantic parsing is a rich fusion of the logical and the statistical world, andthat this fusion will play an integral role in the future of natural languageunderstanding systems.
arxiv-16500-234 | Implementation of a FPGA-Based Feature Detection and Networking System for Real-time Traffic Monitoring | http://arxiv.org/pdf/1603.06669v1.pdf | author:Jieshi Chen, Benjamin Carrion Schafer, Ivan Wang-Hei Ho category:cs.CV published:2016-03-22 summary:With the growing demand of real-time traffic monitoring nowadays,software-based image processing can hardly meet the real-time data processingrequirement due to the serial data processing nature. In this paper, theimplementation of a hardware-based feature detection and networking systemprototype for real-time traffic monitoring as well as data transmission ispresented. The hardware architecture of the proposed system is mainly composedof three parts: data collection, feature detection, and data transmission.Overall, the presented prototype can tolerate a high data rate of about 60frames per second. By integrating the feature detection and data transmissionfunctions, the presented system can be further developed for various VANETapplication scenarios to improve road safety and traffic efficiency. Forexample, detection of vehicles that violate traffic rules, parking enforcement,etc.
arxiv-16500-235 | Learning Representations for Automatic Colorization | http://arxiv.org/pdf/1603.06668v1.pdf | author:Gustav Larsson, Michael Maire, Gregory Shakhnarovich category:cs.CV published:2016-03-22 summary:We develop a fully automatic image colorization system. Our approachleverages recent advances in deep networks, exploiting both low-level andsemantic representations during colorization. As many scene elements naturallyappear according to multimodal color distributions, we train our model topredict per-pixel color histograms. This intermediate output can be used toautomatically generate a color image, or further manipulated prior to imageformation; our experiments consider both scenarios. On both fully and partiallyautomatic colorization tasks, our system significantly outperforms all existingmethods.
arxiv-16500-236 | Incorporating Copying Mechanism in Sequence-to-Sequence Learning | http://arxiv.org/pdf/1603.06393v2.pdf | author:Jiatao Gu, Zhengdong Lu, Hang Li, Victor O. K. Li category:cs.CL cs.AI cs.LG cs.NE published:2016-03-21 summary:We address an important problem in sequence-to-sequence (Seq2Seq) learningreferred to as copying, in which certain segments in the input sequence areselectively replicated in the output sequence. A similar phenomenon isobservable in human language communication. For example, humans tend to repeatentity names or even long phrases in conversation. The challenge with regard tocopying in Seq2Seq is that new machinery is needed to decide when to performthe operation. In this paper, we incorporate copying into neural network-basedSeq2Seq learning and propose a new model called CopyNet with encoder-decoderstructure. CopyNet can nicely integrate the regular way of word generation inthe decoder with the new copying mechanism which can choose sub-sequences inthe input sequence and put them at proper places in the output sequence. Ourempirical study on both synthetic data sets and real world data setsdemonstrates the efficacy of CopyNet. For example, CopyNet can outperformregular RNN-based model with remarkable margins on text summarization tasks.
arxiv-16500-237 | Information Processing by Nonlinear Phase Dynamics in Locally Connected Arrays | http://arxiv.org/pdf/1603.06665v1.pdf | author:Richard A. Kiehl category:cs.NE cs.ET published:2016-03-22 summary:Research toward powerful information processing systems that circumvent theinterconnect bottleneck by exploiting the nonlinear evolution of multiple phasedynamics in locally connected arrays is discussed. We focus on a scheme inwhich logic states are defined by the electrical phase of a dynamic process andinformation processing is realized through interactions between the elements inthe array. Simulation results are given for networks comprised of neuron-likeintegrate-and-fire elements, which could potentially be implemented byultra-small tunnel junctions, molecules and other types of nanoscale elements.This approach could lead to powerful information processing systems due tomassive parallelism in simple, highly scalable nano-architectures. The rationalfor this approach, its advantages, simulation results, critical issues, andfuture research directions are discussed.
arxiv-16500-238 | Input Aggregated Network for Face Video Representation | http://arxiv.org/pdf/1603.06655v1.pdf | author:Zhen Dong, Su Jia, Chi Zhang, Mingtao Pei category:cs.CV published:2016-03-22 summary:Recently, deep neural network has shown promising performance in face imagerecognition. The inputs of most networks are face images, and there is hardlyany work reported in literature on network with face videos as input. Tosufficiently discover the useful information contained in face videos, wepresent a novel network architecture called input aggregated network which isable to learn fixed-length representations for variable-length face videos. Toaccomplish this goal, an aggregation unit is designed to model a face videowith various frames as a point on a Riemannian manifold, and the mapping unitaims at mapping the point into high-dimensional space where face videosbelonging to the same subject are close-by and others are distant. These twounits together with the frame representation unit build an end-to-end learningsystem which can learn representations of face videos for the specific tasks.Experiments on two public face video datasets demonstrate the effectiveness ofthe proposed network.
arxiv-16500-239 | Information Theoretic-Learning Auto-Encoder | http://arxiv.org/pdf/1603.06653v1.pdf | author:Eder Santana, Matthew Emigh, Jose C Principe category:cs.LG published:2016-03-22 summary:We propose Information Theoretic-Learning (ITL) divergence measures forvariational regularization of neural networks. We also explore ITL-regularizedautoencoders as an alternative to variational autoencoding bayes, adversarialautoencoders and generative adversarial networks for randomly generating sampledata without explicitly defining a partition function. This paper alsoformalizes, generative moment matching networks under the ITL framework.
arxiv-16500-240 | Morphological Inflection Generation Using Character Sequence to Sequence Learning | http://arxiv.org/pdf/1512.06110v3.pdf | author:Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, Chris Dyer category:cs.CL published:2015-12-18 summary:Morphological inflection generation is the task of generating the inflectedform of a given lemma corresponding to a particular linguistic transformation.We model the problem of inflection generation as a character sequence tosequence learning problem and present a variant of the neural encoder-decodermodel for solving it. Our model is language independent and can be trained inboth supervised and semi-supervised settings. We evaluate our system on sevendatasets of morphologically rich languages and achieve either better orcomparable results to existing state-of-the-art models of inflectiongeneration.
arxiv-16500-241 | Towards Automatic Wild Animal Monitoring: Identification of Animal Species in Camera-trap Images using Very Deep Convolutional Neural Networks | http://arxiv.org/pdf/1603.06169v2.pdf | author:Alexander Gomez, Augusto Salazar, Francisco Vargas category:cs.CV published:2016-03-20 summary:Non intrusive monitoring of animals in the wild is possible using cameratrapping framework, which uses cameras triggered by sensors to take a burst ofimages of animals in their habitat. However camera trapping framework producesa high volume of data (in the order on thousands or millions of images), whichmust be analyzed by a human expert. In this work, a method for animal speciesidentification in the wild using very deep convolutional neural networks ispresented. Multiple versions of the Snapshot Serengeti dataset were used inorder to probe the ability of the method to cope with different challenges thatcamera-trap images demand. The method reached 88.9% of accuracy in Top-1 and98.1% in Top-5 in the evaluation set using a residual network topology. Also,the results show that the proposed method outperforms previous approximationsand proves that recognition in camera-trap images can be automated.
arxiv-16500-242 | Modelling Temporal Information Using Discrete Fourier Transform for Video Classification | http://arxiv.org/pdf/1603.06182v2.pdf | author:Haimin Zhang, Min Xu, Changsheng Xu, Ramesh Jain category:cs.CV published:2016-03-20 summary:Recently, video classification attracts intensive research efforts. However,most existing works are based on framelevel visual features, which might failto model the temporal information, e.g. characteristics accumulated along time.In order to capture video temporal information, we propose to analyse featuresin frequency domain transformed by discrete Fourier transform (DFT features).Frame-level features are firstly extract by a pre-trained deep convolutionalneural network (CNN). Then, time domain features are transformed andinterpolated into DFT features. CNN and DFT features are further encoded byusing different pooling methods and fused for video classification. In thisway, static image features extracted from a pre-trained deep CNN and temporalinformation represented by DFT features are jointly considered for videoclassification. We test our method for video emotion classification and actionrecognition. Experimental results demonstrate that combining DFT features caneffectively capture temporal information and therefore improve the performanceof both video emotion classification and action recognition. Our approach hasachieved a state-of-the-art performance on the largest video emotion dataset(VideoEmotion-8 dataset) and competitive results on UCF-101.
arxiv-16500-243 | Variational Autoencoders for Feature Detection of Magnetic Resonance Imaging Data | http://arxiv.org/pdf/1603.06624v1.pdf | author:R. Devon Hjelm, Sergey M. Plis, Vince C. Calhoun category:cs.LG cs.NE stat.ML published:2016-03-21 summary:Independent component analysis (ICA), as an approach to the blindsource-separation (BSS) problem, has become the de-facto standard in manymedical imaging settings. Despite successes and a large ongoing researcheffort, the limitation of ICA to square linear transformations have not beenovercome, so that general INFOMAX is still far from being realized. As analternative, we present feature analysis in medical imaging as a problem solvedby Helmholtz machines, which include dimensionality reduction andreconstruction of the raw data under the same objective, and which recentlyhave overcome major difficulties in inference and learning with deep andnonlinear configurations. We demonstrate one approach to training Helmholtzmachines, variational auto-encoders (VAE), as a viable approach toward featureextraction with magnetic resonance imaging (MRI) data.
arxiv-16500-244 | How Robust are Reconstruction Thresholds for Community Detection? | http://arxiv.org/pdf/1511.01473v2.pdf | author:Ankur Moitra, William Perry, Alexander S. Wein category:cs.DS cs.IT cs.LG math.IT math.PR stat.ML published:2015-11-04 summary:The stochastic block model is one of the oldest and most ubiquitous modelsfor studying clustering and community detection. In an exciting sequence ofdevelopments, motivated by deep but non-rigorous ideas from statisticalphysics, Decelle et al. conjectured a sharp threshold for when communitydetection is possible in the sparse regime. Mossel, Neeman and Sly andMassoulie proved the conjecture and gave matching algorithms and lower bounds. Here we revisit the stochastic block model from the perspective of semirandommodels where we allow an adversary to make `helpful' changes that strengthenties within each community and break ties between them. We show a surprisingresult that these `helpful' changes can shift the information-theoreticthreshold, making the community detection problem strictly harder. Wecomplement this by showing that an algorithm based on semidefinite programming(which was known to get close to the threshold) continues to work in thesemirandom model (even for partial recovery). This suggests that algorithmsbased on semidefinite programming are robust in ways that any algorithm meetingthe information-theoretic threshold cannot be. These results point to an interesting new direction: Can we find robust,semirandom analogues to some of the classical, average-case thresholds instatistics? We also explore this question in the broadcast tree model, and weshow that the viewpoint of semirandom models can help explain why somealgorithms are preferred to others in practice, in spite of the gaps in theirstatistical performance on random models.
arxiv-16500-245 | Stack-propagation: Improved Representation Learning for Syntax | http://arxiv.org/pdf/1603.06598v1.pdf | author:Yuan Zhang, David Weiss category:cs.CL published:2016-03-21 summary:Traditional syntax models typically leverage part-of-speech (POS) informationby constructing features from hand-tuned templates. We demonstrate that abetter approach is to utilize POS tags as a regularizer of learnedrepresentations. We propose a simple method for learning a stacked pipeline ofmodels which we call "stack-propagation". We apply this to dependency parsingand tagging, where we use the hidden layer of the tagger network as arepresentation of the input tokens for the parser. At test time, our parserdoes not require predicted POS tags. On 19 languages from the UniversalDependencies, our method is 1.3% (absolute) more accurate than astate-of-the-art graph-based approach and 2.7% more accurate than the mostcomparable greedy model.
arxiv-16500-246 | The SVM Classifier Based on the Modified Particle Swarm Optimization | http://arxiv.org/pdf/1603.08296v1.pdf | author:L. Demidova, E. Nikulchev, Yu. Sokolova category:cs.LG cs.NE published:2016-03-21 summary:The problem of development of the SVM classifier based on the modifiedparticle swarm optimization has been considered. This algorithm carries out thesimultaneous search of the kernel function type, values of the kernel functionparameters and value of the regularization parameter for the SVM classifier.Such SVM classifier provides the high quality of data classification. The ideaof particles' {\guillemotleft}regeneration{\guillemotright} is put on the basisof the modified particle swarm optimization algorithm. At the realization ofthis idea, some particles change their kernel function type to the one whichcorresponds to the particle with the best value of the classification accuracy.The offered particle swarm optimization algorithm allows reducing the timeexpenditures for development of the SVM classifier. The results of experimentalstudies confirm the efficiency of this algorithm.
arxiv-16500-247 | Facial Landmark Detection with Tweaked Convolutional Neural Networks | http://arxiv.org/pdf/1511.04031v2.pdf | author:Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan category:cs.CV published:2015-11-12 summary:We present a novel convolutional neural network (CNN) design for faciallandmark coordinate regression. We examine the intermediate features of astandard CNN trained for landmark detection and show that features extractedfrom later, more specialized layers capture rough landmark locations. Thisprovides a natural means of applying differential treatment midway through thenetwork, tweaking processing based on facial alignment. The resulting TweakedCNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in anappearance-sensitive manner without training multi-part or multi-scale models.Our results on standard face landmark detection and face verificationbenchmarks show TCNN to surpasses previously published performances by widemargins.
arxiv-16500-248 | Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits | http://arxiv.org/pdf/1603.06560v1.pdf | author:Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar category:cs.LG stat.ML published:2016-03-21 summary:Performance of machine learning algorithms depends critically on identifyinga good set of hyperparameters. While current methods offer efficiencies byadaptively choosing new configurations to train, an alternative strategy is toadaptively allocate resources across the selected configurations. We formulatehyperparameter optimization as a pure-exploration non-stochastic infinitelymany armed bandit problem where allocation of additional resources to an armcorresponds to training a configuration on larger subsets of the data. Weintroduce Hyperband for this framework and analyze its theoretical properties,providing several desirable guarantees. We compare Hyperband withstate-of-the-art Bayesian optimization methods and a random search baseline ona comprehensive benchmark including 117 datasets. Our results on this benchmarkdemonstrate that while Bayesian optimization methods do not outperform randomsearch trained for twice as long, Hyperband in favorable settings offersvaluable speedups.
arxiv-16500-249 | Action-Affect Classification and Morphing using Multi-Task Representation Learning | http://arxiv.org/pdf/1603.06554v1.pdf | author:Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar category:cs.CV cs.AI cs.HC cs.LG published:2016-03-21 summary:Most recent work focused on affect from facial expressions, and not as muchon body. This work focuses on body affect analysis. Affect does not occur inisolation. Humans usually couple affect with an action in natural interactions;for example, a person could be talking and smiling. Recognizing body affect insequences requires efficient algorithms to capture both the micro movementsthat differentiate between happy and sad and the macro variations betweendifferent actions. We depart from traditional approaches for time-series dataanalytics by proposing a multi-task learning model that learns a sharedrepresentation that is well-suited for action-affect classification as well asgeneration. For this paper we choose Conditional Restricted Boltzmann Machinesto be our building block. We propose a new model that enhances the CRBM modelwith a factored multi-task component to become Multi-Task ConditionalRestricted Boltzmann Machines (MTCRBMs). We evaluate our approach on twopublicly available datasets, the Body Affect dataset and the Tower Gamedataset, and show superior classification performance improvement over thestate-of-the-art, as well as the generative abilities of our model.
arxiv-16500-250 | Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning | http://arxiv.org/pdf/1503.09105v11.pdf | author:Prasenjit Karmakar, Shalabh Bhatnagar category:math.DS cs.AI stat.ML published:2015-03-31 summary:We present for the first time an asymptotic convergence analysis of twotime-scale stochastic approximation driven by `controlled' Markov noise. Inparticular, both the faster and slower recursions have non-additive controlledMarkov noise components in addition to martingale difference noise. We analyzethe asymptotic behavior of our framework by relating it to limitingdifferential inclusions in both time-scales that are defined in terms of theergodic occupation measures associated with the controlled Markov processes.Finally, we present a solution to the off-policy convergence problem fortemporal difference learning with linear function approximation, using ourresults.
arxiv-16500-251 | A Comparison Study of Nonlinear Kernels | http://arxiv.org/pdf/1603.06541v1.pdf | author:Ping Li category:stat.ML cs.LG published:2016-03-21 summary:In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF(folded RBF), acos, and acos-$\chi^2$, on a wide range of publicly availabledatasets. The proposed fRBF kernel performs very similarly to the RBF kernel.Both RBF and fRBF kernels require an important tuning parameter ($\gamma$).Interestingly, for a significant portion of the datasets, the min-max kerneloutperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\chi^2$kernel also perform well in general and in some datasets achieve the bestaccuracies. One crucial issue with the use of nonlinear kernels is the excessivecomputational and memory cost. These days, one increasingly popular strategy isto linearize the kernels through various randomization algorithms. In ourstudy, the randomization method for the min-max kernel demonstrates excellentperformance compared to the randomization methods for other types of nonlinearkernels, measured in terms of the number of nonzero terms in the transformeddataset. Our study provides evidence for supporting the use of the min-max kernel andthe corresponding randomized linearization method (i.e., the so-called "0-bitCWS"). Furthermore, the results motivate at least two directions for futureresearch: (i) To develop new (and linearizable) nonlinear kernels for betteraccuracies; and (ii) To develop better linearization algorithms for improvingthe current linearization methods for the RBF kernel, the acos kernel, and theacos-$\chi^2$ kernel. One attempt is to combine the min-max kernel with theacos kernel or the acos-$\chi^2$ kernel. The advantages of these two new andtuning-free nonlinear kernels are demonstrated vias our extensive experiments.
arxiv-16500-252 | Deep video gesture recognition using illumination invariants | http://arxiv.org/pdf/1603.06531v1.pdf | author:Otkrist Gupta, Dan Raviv, Ramesh Raskar category:cs.CV cs.LG published:2016-03-21 summary:In this paper we present architectures based on deep neural nets for gesturerecognition in videos, which are invariant to local scaling. We amalgamateautoencoder and predictor architectures using an adaptive weighting schemecoping with a reduced size labeled dataset, while enriching our models fromenormous unlabeled sets. We further improve robustness to lighting conditionsby introducing a new adaptive filer based on temporal local scalenormalization. We provide superior results over known methods, including recentreported approaches based on neural nets.
arxiv-16500-253 | Phase transitions and sample complexity in Bayes-optimal matrix factorization | http://arxiv.org/pdf/1402.1298v3.pdf | author:Yoshiyuki Kabashima, Florent Krzakala, Marc Mézard, Ayaka Sakata, Lenka Zdeborová category:cs.NA cs.IT cs.LG math.IT stat.ML published:2014-02-06 summary:We analyse the matrix factorization problem. Given a noisy measurement of aproduct of two matrices, the problem is to estimate back the original matrices.It arises in many applications such as dictionary learning, blind matrixcalibration, sparse principal component analysis, blind source separation, lowrank matrix completion, robust principal component analysis or factor analysis.It is also important in machine learning: unsupervised representation learningcan often be studied through matrix factorization. We use the tools ofstatistical mechanics - the cavity and replica methods - to analyze theachievability and computational tractability of the inference problems in thesetting of Bayes-optimal inference, which amounts to assuming that the twomatrices have random independent elements generated from some knowndistribution, and this information is available to the inference algorithm. Inthis setting, we compute the minimal mean-squared-error achievable in principlein any computational time, and the error that can be achieved by an efficientapproximate message passing algorithm. The computation is based on theasymptotic state-evolution analysis of the algorithm. The performance that ouranalysis predicts, both in terms of the achieved mean-squared-error, and interms of sample complexity, is extremely promising and motivating for a furtherdevelopment of the algorithm.
arxiv-16500-254 | Static and Dynamic Feature Selection in Morphosyntactic Analyzers | http://arxiv.org/pdf/1603.06503v1.pdf | author:Bernd Bohnet, Miguel Ballesteros, Ryan McDonald, Joakim Nivre category:cs.CL published:2016-03-21 summary:We study the use of greedy feature selection methods for morphosyntactictagging under a number of different conditions. We compare a static ordering offeatures to a dynamic ordering based on mutual information statistics, and weapply the techniques to standalone taggers as well as joint systems for taggingand parsing. Experiments on five languages show that feature selection canresult in more compact models as well as higher accuracy under all conditions,but also that a dynamic ordering works better than a static ordering and thatjoint systems benefit more than standalone taggers. We also show that the sametechniques can be used to select which morphosyntactic categories to predict inorder to maximize syntactic accuracy in a joint system. Our final resultsrepresent a substantial improvement of the state of the art for severallanguages, while at the same time reducing both the number of features and therunning time by up to 80% in some cases.
arxiv-16500-255 | Instance Influence Estimation for Hyperspectral Target Signature Characterization using Extended Functions of Multiple Instances | http://arxiv.org/pdf/1603.06496v1.pdf | author:Sheng Zou, Alina Zare category:cs.CV published:2016-03-21 summary:The Extended Functions of Multiple Instances (eFUMI) algorithm is ageneralization of Multiple Instance Learning (MIL). In eFUMI, only bag level(i.e. set level) labels are needed to estimate target signatures from mixeddata. The training bags in eFUMI are labeled positive if any data point in abag contains or represents any proportion of the target signature and arelabeled as a negative bag if all data points in the bag do not represent anytarget. From these imprecise labels, eFUMI has been shown to be effective atestimating target signatures in hyperspectral subpixel target detectionproblems. One motivating scenario for the use of eFUMI is where an analystcircles objects/regions of interest in a hyperspectral scene such that thetarget signatures of these objects can be estimated and be used to determinewhether other instances of the object appear elsewhere in the image collection.The regions highlighted by the analyst serve as the imprecise labels for eFUMI.Often, an analyst may want to iteratively refine their imprecise labels. Inthis paper, we present an approach for estimating the influence on theestimated target signature if the label for a particular input data point ismodified. This "instance influence estimation" guides an analyst to focus on(re-)labeling the data points that provide the largest change in the resultingestimated target signature and, thus, reduce the amount of time an analystneeds to spend refining the labels for a hyperspectral scene. Results are shownon real hyperspectral sub-pixel target detection data sets.
arxiv-16500-256 | A System for Probabilistic Linking of Thesauri and Classification Systems | http://arxiv.org/pdf/1603.06485v1.pdf | author:Lisa Posch, Philipp Schaer, Arnim Bleier, Markus Strohmaier category:cs.AI cs.CL cs.DL published:2016-03-21 summary:This paper presents a system which creates and visualizes probabilisticsemantic links between concepts in a thesaurus and classes in a classificationsystem. For creating the links, we build on the Polylingual Labeled Topic Model(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class inthe classification system by using information from the natural language textof documents, their assigned thesaurus descriptors and their designatedclasses. The links are then presented to users of the system in an interactivevisualization, providing them with an automatically generated overview of therelations between the thesaurus and the classification system.
arxiv-16500-257 | Bayesian Neural Word Embedding | http://arxiv.org/pdf/1603.06571v1.pdf | author:Oren Barkan category:cs.CL cs.LG published:2016-03-21 summary:Recently, several works in the domain of natural language processingpresented successful methods for word embedding. Among them, the Skip-gram (SG)with negative sampling, known also as Word2Vec, advanced the state-of-the-artof various linguistics tasks. In this paper, we propose a scalable Bayesianneural word embedding algorithm that can be beneficial to general itemsimilarity tasks as well. The algorithm relies on a Variational Bayes solutionfor the SG objective and a detailed step by step description of the algorithmis provided. We present experimental results that demonstrate the performanceof the proposed algorithm and show it is competitive with the original SGmethod.
arxiv-16500-258 | Hard-Clustering with Gaussian Mixture Models | http://arxiv.org/pdf/1603.06478v1.pdf | author:Johannes Blömer, Sascha Brauer, Kathrin Bujna category:cs.LG cs.DS published:2016-03-21 summary:Training the parameters of statistical models to describe a given data set isa central task in the field of data mining and machine learning. A very popularand powerful way of parameter estimation is the method of maximum likelihoodestimation (MLE). Among the most widely used families of statistical models aremixture models, especially, mixtures of Gaussian distributions. A popularhard-clustering variant of the MLE problem is the so-called complete-datamaximum likelihood estimation (CMLE) method. The standard approach to solve theCMLE problem is the Classification-Expectation-Maximization (CEM) algorithm.Unfortunately, it is only guaranteed that the algorithm converges to some(possibly arbitrarily poor) stationary point of the objective function. In this paper, we present two algorithms for a restricted version of the CMLEproblem. That is, our algorithms approximate reasonable solutions to the CMLEproblem which satisfy certain natural properties. Moreover, they computesolutions whose cost (i.e. complete-data log-likelihood values) are at most afactor $(1+\epsilon)$ worse than the cost of the solutions that we search for.Note the CMLE problem in its most general, i.e. unrestricted, form is not welldefined and allows for trivial optimal solutions that can be thought of asdegenerated solutions.
arxiv-16500-259 | Frankenstein: Learning Deep Face Representations using Small Data | http://arxiv.org/pdf/1603.06470v1.pdf | author:Guosheng Hu, Xiaojiang Peng, Yongxin Yang, Timothy Hospedales, Jakob Verbeek category:cs.CV published:2016-03-21 summary:Deep convolutional neural networks have recently proven extremely effectivefor difficult face recognition problems in uncontrolled settings. To train suchnetworks very large training sets are needed with millions of labeled images.For some applications, such as near-infrared (NIR) face recognition, suchlarger training datasets are, however, not publicly available and verydifficult to collect. We propose a method to generate very large trainingdatasets of synthetic images by compositing real face images in a givendataset. We show that this method enables to learn models from as few as 10,000training images, which perform on par with models trained from 500,000 images.Using our approach we also improve the state-of-the-art results on the CASIANIR-VIS heterogeneous face recognition dataset.
arxiv-16500-260 | Illumination-invariant image mosaic calculation based on logarithmic search | http://arxiv.org/pdf/1603.06433v1.pdf | author:Wolfgang Konen category:cs.CV published:2016-03-21 summary:This technical report describes an improved image mosaicking algorithm. It isbased on Jain's logarithmic search algorithm [Jain 1981] which is coupled tothe method of Kourogi (1999} for matching images in a video sequence.Logarithmic search has a better invariance against illumination changes thanthe original optical-flow-based method of Kourogi.
arxiv-16500-261 | Beyond Sharing Weights for Deep Domain Adaptation | http://arxiv.org/pdf/1603.06432v1.pdf | author:Artem Rozantsev, Mathieu Salzmann, Pascal Fua category:cs.CV published:2016-03-21 summary:Deep Neural Networks have demonstrated outstanding performance in manyComputer Vision tasks but typically require large amounts of labeled trainingdata to achieve it. This is a serious limitation when such data is difficult toobtain. In traditional Machine Learning, Domain Adaptation is an approach toovercoming this problem by leveraging annotated data from a source domain, inwhich it is abundant, to train a classifier to operate in a target domain, inwhich labeled data is either sparse or even lacking altogether. In the DeepLearning case, most existing methods use the same architecture with the sameweights for both source and target data, which essentially amounts to learningdomain invariant features. Here, we show that it is more effective toexplicitly model the shift from one domain to the other. To this end, weintroduce a two-stream architecture, one of which operates in the source domainand the other in the target domain. In contrast to other approaches, theweights in corresponding layers are related but not shared to account fordifferences between the two domains. We demonstrate that this both yieldshigher accuracy than state-of-the-art methods on several object recognition anddetection tasks and consistently outperforms networks with shared weights inboth supervised and unsupervised settings.
arxiv-16500-262 | Multimodal Pivots for Image Caption Translation | http://arxiv.org/pdf/1601.03916v2.pdf | author:Julian Hitschler, Shigehiko Schamoni, Stefan Riezler category:cs.CL published:2016-01-15 summary:We present an approach to improve statistical machine translation of imagedescriptions by multimodal pivots defined in visual space. The key idea is todisambiguate and ground the translation of an image desription by involving theimage as a pivot into the translation process. We compute image similarity by aconvolutional neural network, and use descriptions of most similar pivot imagesfor crosslingual reranking of translation outputs. Our approach does not dependon the availability of large amounts of in-domain parallel data and achievesimprovements of 1 BLEU point over strong baselines.
arxiv-16500-263 | Descriptor transition tables for object retrieval using unconstrained cluttered video acquired using a consumer level handheld mobile device | http://arxiv.org/pdf/1603.05073v2.pdf | author:Warren Rieutort-Louis, Ognjen Arandjelovic category:cs.CV published:2016-03-16 summary:Visual recognition and vision based retrieval of objects from large databasesare tasks with a wide spectrum of potential applications. In this paper wepropose a novel recognition method from video sequences suitable for retrievalfrom databases acquired in highly unconstrained conditions e.g. using a mobileconsumer-level device such as a phone. On the lowest level, we represent eachsequence as a 3D mesh of densely packed local appearance descriptors. Whileimage plane geometry is captured implicitly by a large overlap of neighbouringregions from which the descriptors are extracted, 3D information is extractedby means of a descriptor transition table, learnt from a single sequence foreach known gallery object. These allow us to connect local descriptors alongthe 3rd dimension (which corresponds to viewpoint changes), thus resulting in aset of variable length Markov chains for each video. The matching of two setsof such chains is formulated as a statistical hypothesis test, whereby a subsetof each is chosen to maximize the likelihood that the corresponding videosequences show the same object. The effectiveness of the proposed algorithm isempirically evaluated on the Amsterdam Library of Object Images and a newhighly challenging video data set acquired using a mobile phone. On both datasets our method is shown to be successful in recognition in the presence ofbackground clutter and large viewpoint changes.
arxiv-16500-264 | Appearance Harmonization for Single Image Shadow Removal | http://arxiv.org/pdf/1603.06398v1.pdf | author:Liqian Ma, Jue Wang, Eli Shechtman, Kalyan Sunkavalli, Shimin Hu category:cs.CV published:2016-03-21 summary:Shadows often create unwanted artifacts in photographs, and removing them canbe very challenging. Previous shadow removal methods often produce de-shadowedregions that are visually inconsistent with the rest of the image. In this workwe propose a fully automatic shadow region harmonization approach that improvesthe appearance compatibility of the de-shadowed region as typically produced byprevious methods. It is based on a shadow-guided patch-based image synthesisapproach that reconstructs the shadow region using patches sampled fromnon-shadowed regions. The result is then refined based on the reconstructionconfidence to handle unique image patterns. Many shadow removal results andcomparisons are show the effectiveness of our improvement. Quantitativeevaluation on a benchmark dataset suggests that our automatic shadowharmonization approach effectively improves upon the state-of-the-art.
arxiv-16500-265 | Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse Random Graphs | http://arxiv.org/pdf/1503.00164v2.pdf | author:Braxton Osting, Jiechao Xiong, Qianqian Xu, Yuan Yao category:stat.ML cs.LG published:2015-02-28 summary:Crowdsourcing platforms are now extensively used for conducting subjectivepairwise comparison studies. In this setting, a pairwise comparison dataset istypically gathered via random sampling, either \emph{with} or \emph{without}replacement. In this paper, we use tools from random graph theory to analyzethese two random sampling methods for the HodgeRank estimator. Using theFiedler value of the graph as a measurement for estimator stability(informativeness), we provide a new estimate of the Fiedler value for these tworandom graph models. In the asymptotic limit as the number of vertices tends toinfinity, we prove the validity of the estimate. Based on our findings, for asmall number of items to be compared, we recommend a two-stage samplingstrategy where a greedy sampling method is used initially and random sampling\emph{without} replacement is used in the second stage. When a large number ofitems is to be compared, we recommend random sampling with replacement as thisis computationally inexpensive and trivially parallelizable. Experiments onsynthetic and real-world datasets support our analysis.
arxiv-16500-266 | Predictive Interval Models for Non-parametric Regression | http://arxiv.org/pdf/1402.5874v2.pdf | author:Mohammad Ghasemi Hamed, Mathieu Serrurier, Nicolas Durand category:cs.LG stat.ML published:2014-02-24 summary:Having a regression model, we are interested in finding two-sided intervalsthat are guaranteed to contain at least a desired proportion of the conditionaldistribution of the response variable given a specific combination ofpredictors. We name such intervals predictive intervals. This work presents anew method to find two-sided predictive intervals for non-parametric leastsquares regression without the homoscedasticity assumption. Our predictiveintervals are built by using tolerance intervals on prediction errors in thequery point's neighborhood. We proposed a predictive interval model test and wealso used it as a constraint in our hyper-parameter tuning algorithm. Thisgives an algorithm that finds the smallest reliable predictive intervals for agiven dataset. We also introduce a measure for comparing different intervalprediction methods yielding intervals having different size and coverage. Theseexperiments show that our methods are more reliable, effective and precise thanother interval prediction methods.
arxiv-16500-267 | STDP as presynaptic activity times rate of change of postsynaptic activity | http://arxiv.org/pdf/1509.05936v2.pdf | author:Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, Yuhuai Wu category:cs.NE cs.LG q-bio.NC published:2015-09-19 summary:We introduce a weight update formula that is expressed only in terms offiring rates and their derivatives and that results in changes consistent withthose associated with spike-timing dependent plasticity (STDP) rules andbiological observations, even though the explicit timing of spikes is notneeded. The new rule changes a synaptic weight in proportion to the product ofthe presynaptic firing rate and the temporal rate of change of activity on thepostsynaptic side. These quantities are interesting for studying theoreticalexplanation for synaptic changes from a machine learning perspective. Inparticular, if neural dynamics moved neural activity towards reducing someobjective function, then this STDP rule would correspond to stochastic gradientdescent on that objective function.
arxiv-16500-268 | Analyzing coevolutionary games with dynamic fitness landscapes | http://arxiv.org/pdf/1603.06374v1.pdf | author:Hendrik Richter category:q-bio.PE cs.GT cs.NE published:2016-03-21 summary:Coevolutionary games cast players that may change their strategies as well astheir networks of interaction. In this paper a framework is introduced fordescribing coevolutionary game dynamics by landscape models. It is shown thatcoevolutionary games invoke dynamic landscapes. Numerical experiments are shownfor a prisoner's dilemma (PD) and a snow drift (SD) game that both use eitherbirth-death (BD) or death-birth (DB) strategy updating. The resultinglandscapes are analyzed with respect to modality and ruggedness
arxiv-16500-269 | Unified Depth Prediction and Intrinsic Image Decomposition from a Single Image via Joint Convolutional Neural Fields | http://arxiv.org/pdf/1603.06359v1.pdf | author:Seungryong Kim, Kihong Park, Kwanghoon Sohn, Stephen Lin category:cs.CV published:2016-03-21 summary:We present a method for jointly predicting a depth map and intrinsic imagesfrom single-image input. The two tasks are formulated in a synergistic mannerthrough a joint conditional random field (CRF) that is solved using a novelconvolutional neural network (CNN) architecture, called the joint convolutionalneural field (JCNF) model. Tailored to our joint estimation problem, JCNFdiffers from previous CNNs in its sharing of convolutional activations andlayers between networks for each task, its inference in the gradient domainwhere there exists greater correlation between depth and intrinsic images, andthe incorporation of a gradient scale network that learns the confidence ofestimated gradients in order to effectively balance them in the solution. Thisapproach is shown to surpass state-of-the-art methods both on single-imagedepth estimation and on intrinsic image decomposition.
arxiv-16500-270 | A Discontinuous Neural Network for Non-Negative Sparse Approximation | http://arxiv.org/pdf/1603.06353v1.pdf | author:Martijn Arts, Marius Cordts, Monika Gorin, Marc Spehr, Rudolf Mathar category:cs.NE math.OC q-bio.NC published:2016-03-21 summary:This paper investigates a discontinuous neural network which is used as amodel of the mammalian olfactory system and can more generally be applied tosolve non-negative sparse approximation problems. By inherently limiting thesystems integrators to having non-negative outputs, the system function becomesdiscontinuous since the integrators switch between being inactive and beingactive. It is shown that the presented network converges to equilibrium pointswhich are solutions to general non-negative least squares optimizationproblems. We specify a Caratheodory solution and prove that the network isstable, provided that the system matrix has full column-rank. Under a mildcondition on the equilibrium point, we show that the network converges to itsequilibrium within a finite number of switches. Two applications of the neuralnetwork are shown. Firstly, we apply the network as a model of the olfactorysystem and show that in principle it may be capable of performing complexsparse signal recovery tasks. Secondly, we generalize the application toinclude non-negative sparse approximation problems and compare the recoveryperformance to a classical non-negative basis pursuit denoising algorithm. Weconclude that the recovery performance differs only marginally from theclassical algorithm, while the neural network has the advantage that noperformance critical regularization parameter has to be chosen prior torecovery.
arxiv-16500-271 | Online Learning with Low Rank Experts | http://arxiv.org/pdf/1603.06352v1.pdf | author:Elad Hazan, Tomer Koren, Roi Livni, Yishay Mansour category:cs.LG published:2016-03-21 summary:We consider the problem of prediction with expert advice when the losses ofthe experts have low-dimensional structure: they are restricted to an unknown$d$-dimensional subspace. We devise algorithms with regret bounds that areindependent of the number of experts and depends only on the rank $d$. For thestochastic model we show a tight bound of $\Theta(\sqrt{dT})$, and extend it toa setting of an approximate $d$ subspace. For the adversarial model we show anupper bound of $O(d\sqrt{T})$ and a lower bound of $\Omega(\sqrt{dT})$.
arxiv-16500-272 | Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration | http://arxiv.org/pdf/1603.06348v1.pdf | author:Abhishek Gupta, Clemens Eppner, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO published:2016-03-21 summary:Dexterous multi-fingered hands can accomplish fine manipulation behaviorsthat are infeasible with simple robotic grippers. However, sophisticatedmulti-fingered hands are often expensive and fragile. Low-cost soft hands offeran appealing alternative to more conventional devices, but present considerablechallenges in sensing and actuation, making them difficult to apply to morecomplex manipulation tasks. In this paper, we describe an approach to learningfrom demonstration that can be used to train soft robotic hands to performdexterous manipulation tasks. Our method uses object-centric demonstrations,where a human demonstrates the desired motion of manipulated objects with theirown hands, and the robot autonomously learns to imitate these demonstrationsusing reinforcement learning. We propose a novel algorithm that allows us toblend and select a subset of the most feasible demonstrations to learn toimitate on the hardware, which we use with an extension of the guided policysearch framework to use multiple demonstrations to learn generalizable neuralnetwork policies. We demonstrate our approach on the RBO Hand 2, with learnedmotor skills for turning a valve, manipulating an abacus, and grasping.
arxiv-16500-273 | Data Augmentation via Levy Processes | http://arxiv.org/pdf/1603.06340v1.pdf | author:Stefan Wager, William Fithian, Percy Liang category:stat.ML published:2016-03-21 summary:If a document is about travel, we may expect that short snippets of thedocument should also be about travel. We introduce a general framework forincorporating these types of invariances into a discriminative classifier. Theframework imagines data as being drawn from a slice of a Levy process. If weslice the Levy process at an earlier point in time, we obtain additionalpseudo-examples, which can be used to train the classifier. We show that thisscheme has two desirable properties: it preserves the Bayes decision boundary,and it is equivalent to fitting a generative model in the limit where we rewindtime back to 0. Our construction captures popular schemes such as Gaussianfeature noising and dropout training, as well as admitting new generalizations.
arxiv-16500-274 | DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding | http://arxiv.org/pdf/1603.04922v2.pdf | author:Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, Jianxiong Xiao category:cs.CV published:2016-03-16 summary:While deep neural networks have led to human-level performance on computervision tasks, they have yet to demonstrate similar gains for holistic sceneunderstanding. In particular, 3D context has been shown to be an extremelyimportant cue for scene understanding - yet very little research has been doneon integrating context information with deep models. This paper presents anapproach to embed 3D context into the topology of a neural network trained toperform holistic scene understanding. Given a depth image depicting a 3D scene,our network aligns the observed scene with a predefined 3D scene template, andthen reasons about the existence and location of each object within the scenetemplate. In doing so, our model recognizes multiple objects in a singleforward pass of a 3D convolutional neural network, capturing both global sceneand local object information simultaneously. To create training data for this3D network, we generate partly hallucinated depth images which are rendered byreplacing real objects with a repository of CAD models of the same objectcategory. Extensive experiments demonstrate the effectiveness of our algorithmcompared to the state-of-the-arts. Source code and data will be available.
arxiv-16500-275 | Deep Self-Convolutional Activations Descriptor for Dense Cross-Modal Correspondence | http://arxiv.org/pdf/1603.06327v1.pdf | author:Seungryong Kim, Dongbo Min, Stephen Lin, Kwanghoon Sohn category:cs.CV published:2016-03-21 summary:We present a novel descriptor, called deep self-convolutional activations(DeSCA), designed for establishing dense correspondences between images takenunder different imaging modalities, such as different spectral ranges orlighting conditions. Motivated by descriptors based on local self-similarity(LSS), we formulate a novel descriptor by leveraging LSS in a deeparchitecture, leading to better discriminative power and greater robustness tonon-rigid image deformations than state-of-the-art cross-modality descriptors.The DeSCA first computes self-convolutions over a local support window forrandomly sampled patches, and then builds self-convolution activations byperforming an average pooling through a hierarchical formulation within a deepconvolutional architecture. Finally, the feature responses on theself-convolution activations are encoded through a spatial pyramid pooling in acircular configuration. In contrast to existing convolutional neural networks(CNNs) based descriptors, the DeSCA is training-free (i.e., randomly sampledpatches are utilized as the convolution kernels), is robust to cross-modalimaging, and can be densely computed in an efficient manner that significantlyreduces computational redundancy. The state-of-the-art performance of DeSCA onchallenging cases of cross-modal image pairs is demonstrated through extensiveexperiments.
arxiv-16500-276 | Modeling Coverage for Neural Machine Translation | http://arxiv.org/pdf/1601.04811v3.pdf | author:Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li category:cs.CL published:2016-01-19 summary:Attention mechanism advanced state-of-the-art neural machine translation(NMT) by jointly learning to align and translate. However, attention-based NMTignores past alignment information, which often leads to over-translation andunder-translation. In response to this problem, we maintain a coverage vectorto keep track of the attention history. The coverage vector is fed to theattention model to help adjust future attention, which guides NMT to considermore about the untranslated source words. Experiments show that the proposedapproach significantly improves both translation quality and alignment qualityover traditional attention-based NMT.
arxiv-16500-277 | Resolving References to Objects in Photographs using the Words-As-Classifiers Model | http://arxiv.org/pdf/1510.02125v2.pdf | author:David Schlangen, Sina Zarriess, Casey Kennington category:cs.CL published:2015-10-07 summary:A common use of language is to refer to visually present objects. Modellingit in computers requires modelling the link between language and perception.The "words as classifiers" model of grounded semantics views words asclassifiers of perceptual contexts, and composes the meaning of a phrasethrough composition of the denotations of its component words. It was recentlyshown to perform well in a game-playing scenario with a small number of objecttypes. We apply it to two large sets of real-world photographs that contain amuch larger variety of types and for which referring expressions are available.Using a pre-trained convolutional neural network to extract image features, andaugmenting these with in-picture positional information, we show that the modelachieves performance competitive with the state of the art in a referenceresolution task (given expression, find bounding box of its referent), while,as we argue, being conceptually simpler and more flexible.
arxiv-16500-278 | Additive Approximations in High Dimensional Nonparametric Regression via the SALSA | http://arxiv.org/pdf/1602.00287v2.pdf | author:Kirthevasan Kandasamy, Yaoliang Yu category:stat.ML cs.LG published:2016-01-31 summary:High dimensional nonparametric regression is an inherently difficult problemwith known lower bounds depending exponentially in dimension. A popularstrategy to alleviate this curse of dimensionality has been to use additivemodels of \emph{first order}, which model the regression function as a sum ofindependent functions on each dimension. Though useful in controlling thevariance of the estimate, such models are often too restrictive in practicalsettings. Between non-additive models which often have large variance and firstorder additive models which have large bias, there has been little work toexploit the trade-off in the middle via additive models of intermediate order.In this work, we propose SALSA, which bridges this gap by allowing interactionsbetween variables, but controls model capacity by limiting the order ofinteractions. SALSA minimises the residual sum of squares with squared RKHSnorm penalties. Algorithmically, it can be viewed as Kernel Ridge Regressionwith an additive kernel. When the regression function is additive, the excessrisk is only polynomial in dimension. Using the Girard-Newton formulae, weefficiently sum over a combinatorial number of terms in the additive expansion.Via a comparison on $16$ real datasets, we show that our method is competitiveagainst $21$ other alternatives.
arxiv-16500-279 | Multi-fidelity Gaussian Process Bandit Optimisation | http://arxiv.org/pdf/1603.06288v1.pdf | author:Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff Schneider, Barnabas Poczos category:stat.ML cs.AI cs.LG published:2016-03-20 summary:In many scientific and engineering applications, we are tasked with theoptimisation of an expensive to evaluate black box function $f$. Traditionalmethods for this problem assume just the availability of this single function.However, in many cases, cheap approximations to $f$ may be obtainable. Forexample, the expensive real world behaviour of a robot can be approximated by acheap computer simulation. We can use these approximations to eliminate lowfunction value regions and use the expensive evaluations to $f$ in a smallpromising region and speedily identify the optimum. We formalise this task as a\emph{multi-fidelity} bandit problem where the target function and itsapproximations are sampled from a Gaussian process. We develop a method basedon upper confidence bound techniques and prove that it exhibits precisely theabove behaviour, hence achieving better regret than strategies which ignoremulti-fidelity information. Our method outperforms such naive strategies onseveral synthetic and real experiments.
arxiv-16500-280 | Structured VAEs: Composing Probabilistic Graphical Models and Variational Autoencoders | http://arxiv.org/pdf/1603.06277v1.pdf | author:Matthew J. Johnson, David Duvenaud, Alexander B. Wiltschko, Sandeep R. Datta, Ryan P. Adams category:stat.ML published:2016-03-20 summary:We develop a new framework for unsupervised learning that composesprobabilistic graphical models with deep learning methods and combines theirrespective strengths. Our method uses graphical models to express structuredprobability distributions and recent advances from deep learning to learnflexible feature models and bottom-up recognition networks. All components ofthese models are learned simultaneously using a single objective, and wedevelop scalable fitting algorithms that can leverage natural gradientstochastic variational inference, graphical model message passing, andbackpropagation with the reparameterization trick. We illustrate this frameworkwith a new structured time series model and an application to mouse behavioralphenotyping.
arxiv-16500-281 | Multi-Task Cross-Lingual Sequence Tagging from Scratch | http://arxiv.org/pdf/1603.06270v1.pdf | author:Zhilin Yang, Ruslan Salakhutdinov, William Cohen category:cs.CL cs.LG published:2016-03-20 summary:We present a deep hierarchical recurrent neural network for sequence tagging.Given a sequence of words, our model employs deep gated recurrent units on bothcharacter and word levels to encode morphology and context information, andapplies a conditional random field layer to predict the tags. Our model is taskindependent, language independent, and feature engineering free. We furtherextend our model to multi-task and cross-lingual joint training by sharing thearchitecture and parameters. Our model achieves state-of-the-art results inmultiple languages on several benchmark tasks including POS tagging, chunking,and NER. We also demonstrate that multi-task and cross-lingual joint trainingcan improve the performance in various cases.
arxiv-16500-282 | Bayesian multi-tensor factorization | http://arxiv.org/pdf/1412.4679v3.pdf | author:Suleiman A. Khan, Eemeli Leppäaho, Samuel Kaski category:stat.ML published:2014-12-15 summary:We introduce Bayesian multi-tensor factorization, a model that is the firstBayesian formulation for joint factorization of multiple matrices and tensors.The research problem generalizes the joint matrix-tensor factorization problemto arbitrary sets of tensors of any depth, including matrices, can beinterpreted as unsupervised multi-view learning from multiple data tensors, andcan be generalized to relax the usual trilinear tensor factorizationassumptions. The result is a factorization of the set of tensors into factorsshared by any subsets of the tensors, and factors private to individualtensors. We demonstrate the performance against existing baselines in multipletensor factorization tasks in structural toxicogenomics and functionalneuroimaging.
arxiv-16500-283 | Flow of Information in Feed-Forward Deep Neural Networks | http://arxiv.org/pdf/1603.06220v1.pdf | author:Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan category:cs.IT cs.LG math.IT published:2016-03-20 summary:Feed-forward deep neural networks have been used extensively in variousmachine learning applications. Developing a precise understanding of theunderling behavior of neural networks is crucial for their efficientdeployment. In this paper, we use an information theoretic approach to studythe flow of information in a neural network and to determine how entropy ofinformation changes between consecutive layers. Moreover, using the InformationBottleneck principle, we develop a constrained optimization problem that can beused in the training process of a deep neural network. Furthermore, wedetermine a lower bound for the level of data representation that can beachieved in a deep neural network with an acceptable level of distortion.
arxiv-16500-284 | Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science | http://arxiv.org/pdf/1603.06212v1.pdf | author:Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore category:cs.NE cs.AI cs.LG published:2016-03-20 summary:As the field of data science continues to grow, there will be anever-increasing demand for tools that make machine learning accessible tonon-experts. In this paper, we introduce the concept of tree-based pipelineoptimization for automating one of the most tedious parts of machinelearning---pipeline design. We implement an open source Tree-based PipelineOptimization Tool (TPOT) in Python and demonstrate its effectiveness on aseries of simulated and real-world benchmark data sets. In particular, we showthat TPOT can design machine learning pipelines that provide a significantimprovement over a basic machine learning analysis while requiring little to noinput nor prior knowledge from the user. We also address the tendency for TPOTto design overly complex pipelines by integrating Pareto optimization, whichproduces compact pipelines without sacrificing classification accuracy. Assuch, this work represents an important step toward fully automating machinelearning pipeline design.
arxiv-16500-285 | Statistical Foundation of Spectral Graph Theory | http://arxiv.org/pdf/1602.03861v3.pdf | author:Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH published:2016-02-11 summary:Spectral graph theory is undoubtedly the most favored graph data analysistechnique, both in theory and practice. It has emerged as a versatile tool fora wide variety of applications including data mining, web search, quantumcomputing, computer vision, image segmentation, and among others. However, theway in which spectral graph theory is currently taught and practiced is rathermechanical, consisting of a series of matrix calculations that at first glanceseem to have very little to do with statistics, thus posing a seriouslimitation to our understanding of graph problems from a statisticalperspective. Our work is motivated by the following question: How can wedevelop a general statistical foundation of "spectral heuristics" that avoidsthe cookbook mechanical approach? A unified method is proposed that permitsfrequency analysis of graphs from a nonparametric perspective by viewing it asfunction estimation problem. We show that the proposed formalism incorporatesseemingly unrelated spectral modeling tools (e.g., Laplacian, modularity,regularized Laplacian, etc.) under a single general method, thus providingbetter fundamental understanding. It is the purpose of this paper to bridge thegap between two spectral graph modeling cultures: Statistical theory (based onnonparametric function approximation and smoothing methods) and Algorithmiccomputing (based on matrix theory and numerical linear algebra basedtechniques) to provide transparent and complementary insight into graphproblems.
arxiv-16500-286 | RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation | http://arxiv.org/pdf/1603.06208v1.pdf | author:Asako Kanezaki category:cs.CV published:2016-03-20 summary:The recent popularization of depth sensors and the availability oflarge-scale 3D model databases such as ShapeNet have drawn increased attentionto 3D object recognition. Despite the convenience of using 3D models capturedoffline, we are allowed to observe only a single view of an object at once,with the exception of the use of special environments such as multi-camerastudios. This impedes the recognition of diverse objects in a real environment.If a mechanical system (or a robot) has access to multi-view models of objectsand is able to estimate the viewpoint of a currently observed object, it canrotate the object to a better view for classification. In this paper, wepropose a novel method to learn a deep convolutional neural network that bothclassifies an object and estimates the rotation path to its best view under thepredicted object category. We conduct experiments on a 3D model database aswell as a real image dataset to demonstrate that our system can achieve aneffective strategy of object rotation for category classification.
arxiv-16500-287 | Extracting Predictive Information from Heterogeneous Data Streams using Gaussian Processes | http://arxiv.org/pdf/1603.06202v1.pdf | author:Sid Ghoshal, Stephen Roberts category:q-fin.ST stat.ML published:2016-03-20 summary:Financial markets are notoriously complex environments, presenting vastamounts of noisy, yet potentially informative data. We consider the problem offorecasting financial time series from a wide range of information sourcesusing online Gaussian Processes with Automatic Relevance Determination (ARD)kernels. We measure the performance gain, quantified in terms of NormalisedRoot Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearsoncorrelation, from fusing each of four separate data domains: time seriestechnicals, sentiment analysis, options market data and broker recommendations.We show evidence that ARD kernels produce meaningful feature rankings that helpretain salient inputs and reduce input dimensionality, providing a frameworkfor sifting through financial complexity. We measure the performance gain fromfusing each domain's heterogeneous data streams into a single probabilisticmodel. In particular our findings highlight the critical value of options datain mapping out the curvature of price space and inspire an intuitive, noveldirection for research in financial prediction.
arxiv-16500-288 | Stochastic model for phonemes uncovers an author-dependency of their usage | http://arxiv.org/pdf/1510.01315v2.pdf | author:Weibing Deng, Armen E. Allahverdyan category:cs.CL nlin.AO published:2015-10-05 summary:We study rank-frequency relations for phonemes, the minimal units that stillrelate to linguistic meaning. We show that these relations can be described bythe Dirichlet distribution, a direct analogue of the ideal-gas model instatistical mechanics. This description allows us to demonstrate that therank-frequency relations for phonemes of a text do depend on its author. Theauthor-dependency effect is not caused by the author's vocabulary (common wordsused in different texts), and is confirmed by several alternative means. Thissuggests that it can be directly related to phonemes. These features contrastto rank-frequency relations for words, which are both author and textindependent and are governed by the Zipf's law.
arxiv-16500-289 | The Multiscale Laplacian Graph Kernel | http://arxiv.org/pdf/1603.06186v1.pdf | author:Risi Kondor, Horace Pan category:stat.ML published:2016-03-20 summary:Many real world graphs, such as the graphs of molecules, exhibit structure atmultiple different scales, but most existing kernels between graphs are eitherpurely local or purely global in character. In contrast, by building ahierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLGkernels) that we define in this paper can account for structure at a range ofdifferent scales. At the heart of the MLG construction is another new graphkernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which hasthe property that it can lift a base kernel defined on the vertices of twographs to a kernel between the graphs. The MLG kernel applies such FLG kernelsto subgraphs recursively. To make the MLG kernel computationally feasible, wealso introduce a randomized projection procedure, similar to the Nystr\"ommethod, but for RKHS operators.
arxiv-16500-290 | Modelling Temporal Information Using Discrete Fourier Transform for Recognizing Emotions in User-generated Videos | http://arxiv.org/pdf/1603.06568v1.pdf | author:Haimin Zhang, Min Xu category:cs.CV published:2016-03-20 summary:With the widespread of user-generated Internet videos, emotion recognition inthose videos attracts increasing research efforts. However, most existing worksare based on framelevel visual features and/or audio features, which might failto model the temporal information, e.g. characteristics accumulated along time.In order to capture video temporal information, in this paper, we propose toanalyse features in frequency domain transformed by discrete Fourier transform(DFT features). Frame-level features are firstly extract by a pre-trained deepconvolutional neural network (CNN). Then, time domain features are transferredand interpolated into DFT features. CNN and DFT features are further encodedand fused for emotion classification. By this way, static image featuresextracted from a pre-trained deep CNN and temporal information represented byDFT features are jointly considered for video emotion recognition. Experimentalresults demonstrate that combining DFT features can effectively capturetemporal information and therefore improve emotion recognition performance. Ourapproach has achieved a state-of-the-art performance on the largest videoemotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to62.6%.
arxiv-16500-291 | Segmentation from Natural Language Expressions | http://arxiv.org/pdf/1603.06180v1.pdf | author:Ronghang Hu, Marcus Rohrbach, Trevor Darrell category:cs.CV published:2016-03-20 summary:In this paper we approach the novel problem of segmenting an image based on anatural language expression. This is different from traditional semanticsegmentation over a predefined set of semantic classes, as e.g., the phrase"two men sitting on the right bench" requires segmenting only the two people onthe right bench and no one standing or sitting on another bench. Previousapproaches suitable for this task were limited to a fixed set of categoriesand/or rectangular regions. To produce pixelwise segmentation for the languageexpression, we propose an end-to-end trainable recurrent and convolutionalnetwork model that jointly learns to process visual and linguistic information.In our model, a recurrent LSTM network is used to encode the referentialexpression into a vector representation, and a fully convolutional network isused to a extract a spatial feature map from the image and output a spatialresponse map for the target object. We demonstrate on a benchmark dataset thatour model can produce quality segmentation output from the natural languageexpression, and outperforms baseline methods by a large margin.
arxiv-16500-292 | Joint Stochastic Approximation learning of Helmholtz Machines | http://arxiv.org/pdf/1603.06170v1.pdf | author:Haotian Xu, Zhijian Ou category:cs.LG stat.ML published:2016-03-20 summary:Though with progress, model learning and performing posterior inference stillremains a common challenge for using deep generative models, especially forhandling discrete hidden variables. This paper is mainly concerned withalgorithms for learning Helmholz machines, which is characterized by pairingthe generative model with an auxiliary inference model. A common drawback ofprevious learning algorithms is that they indirectly optimize some bounds ofthe targeted marginal log-likelihood. In contrast, we successfully develop anew class of algorithms, based on stochastic approximation (SA) theory of theRobbins-Monro type, to directly optimize the marginal log-likelihood andsimultaneously minimize the inclusive KL-divergence. The resulting learningalgorithm is thus called joint SA (JSA). Moreover, we construct an effectiveMCMC operator for JSA. Our results on the MNIST datasets demonstrate that theJSA's performance is consistently superior to that of competing algorithms likeRWS, for learning a range of difficult models.
arxiv-16500-293 | ACDC: A Structured Efficient Linear Layer | http://arxiv.org/pdf/1511.05946v5.pdf | author:Marcin Moczulski, Misha Denil, Jeremy Appleyard, Nando de Freitas category:cs.LG cs.NE published:2015-11-18 summary:The linear layer is one of the most pervasive modules in deep learningrepresentations. However, it requires $O(N^2)$ parameters and $O(N^2)$operations. These costs can be prohibitive in mobile applications or preventscaling in many domains. Here, we introduce a deep, differentiable,fully-connected neural network module composed of diagonal matrices ofparameters, $\mathbf{A}$ and $\mathbf{D}$, and the discrete cosine transform$\mathbf{C}$. The core module, structured as $\mathbf{ACDC^{-1}}$, has $O(N)$parameters and incurs $O(N log N )$ operations. We present theoretical resultsshowing how deep cascades of ACDC layers approximate linear layers. ACDC is,however, a stand-alone module and can be used in combination with any othertypes of module. In our experiments, we show that it can indeed be successfullyinterleaved with ReLU modules in convolutional neural networks for imagerecognition. Our experiments also study critical factors in the training ofthese structured modules, including initialization and depth. Finally, thispaper also provides a connection between structured linear transforms used indeep learning and the field of Fourier optics, illustrating how ACDC could inprinciple be implemented with lenses and diffractive elements.
arxiv-16500-294 | Fast Incremental Method for Nonconvex Optimization | http://arxiv.org/pdf/1603.06159v1.pdf | author:Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG stat.ML published:2016-03-19 summary:We analyze a fast incremental aggregated gradient method for optimizingnonconvex problems of the form $\min_x \sum_i f_i(x)$. Specifically, we analyzethe SAGA algorithm within an Incremental First-order Oracle framework, and showthat it converges to a stationary point provably faster than both gradientdescent and stochastic gradient descent. We also discuss a Polyak's specialclass of nonconvex problems for which SAGA converges at a linear rate to theglobal optimum. Finally, we analyze the practically valuable regularized andminibatch variants of SAGA. To our knowledge, this paper presents the firstanalysis of fast convergence for an incremental aggregated gradient method fornonconvex problems.
arxiv-16500-295 | A Persona-Based Neural Conversation Model | http://arxiv.org/pdf/1603.06155v1.pdf | author:Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan category:cs.CL published:2016-03-19 summary:We present persona-based models for handling the issue of speaker consistencyin neural response generation. A speaker model encodes personas in distributedembeddings that capture individual characteristics such as backgroundinformation and speaking style. A dyadic speaker-addressee model capturesproperties of interactions between two interlocutors. Our models yieldqualitative performance improvements in both perplexity and BLEU scores overbaseline sequence-to-sequence models, with similar gain in speaker consistencyas measured by human judges.
arxiv-16500-296 | Evolving Shepherding Behavior with Genetic Programming Algorithms | http://arxiv.org/pdf/1603.06141v1.pdf | author:Joshua Brulé, Kevin Engel, Nick Fung, Isaac Julien category:cs.AI cs.NE published:2016-03-19 summary:We apply genetic programming techniques to the `shepherding' problem, inwhich a group of one type of animal (sheep dogs) attempts to control themovements of a second group of animals (sheep) obeying flocking behavior. Ourgenetic programming algorithm evolves an expression tree that governs themovements of each dog. The operands of the tree are hand-selected features ofthe simulation environment that may allow the dogs to herd the sheepeffectively. The algorithm uses tournament-style selection, crossoverreproduction, and a point mutation. We find that the evolved solutionsgeneralize well and outperform a (naive) human-designed algorithm.
arxiv-16500-297 | Discriminative Embeddings of Latent Variable Models for Structured Data | http://arxiv.org/pdf/1603.05629v2.pdf | author:Hanjun Dai, Bo Dai, Le Song category:cs.LG published:2016-03-17 summary:Kernel classifiers and regressors designed for structured data, such assequences, trees and graphs, have significantly advanced in a number ofinterdisciplinary areas such as computational biology and drug design.Typically, kernel functions are designed beforehand for a data type whicheither exploit statistics of the structures or make use of probabilisticgenerative models, and then a discriminative classifier is learned based on thekernels via convex optimization. However, such an elegant two-stage approachalso limited kernel methods from scaling up to millions of data points, andexploiting discriminative information to learn feature representations. We propose an effective and scalable approach for structured datarepresentation which is based on the idea of embedding latent variable modelsinto feature spaces, and learning such feature spaces using discriminativeinformation. Furthermore, our feature learning algorithm runs a sequence offunction mappings in a way similar to graphical model inference procedures,such as mean field and belief propagation. In real world applications involvingsequences and graphs, we showed that the proposed approach is much morescalable than alternatives while at the same time produce comparable results tothe state-of-the-art in terms of classification and regression.
arxiv-16500-298 | Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks | http://arxiv.org/pdf/1603.06129v1.pdf | author:Sahil Bhatia, Rishabh Singh category:cs.PL cs.AI cs.LG cs.SE published:2016-03-19 summary:We present a method for automatically generating repair feedback for syntaxerrors for introductory programming problems. Syntax errors constitute one ofthe largest classes of errors (34%) in our dataset of student submissionsobtained from a MOOC course on edX. The previous techniques for generatingautomated feed- back on programming assignments have focused on functionalcorrectness and style considerations of student programs. These techniquesanalyze the program AST of the program and then perform some dynamic andsymbolic analyses to compute repair feedback. Unfortunately, it is not possibleto generate ASTs for student pro- grams with syntax errors and therefore theprevious feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that usesRecurrent neural networks (RNNs) to model syntactically valid token sequences.Our approach is inspired from the recent work on learning language models fromBig Code (large code corpus). For a given programming assignment, we firstlearn an RNN to model all valid token sequences using the set of syntacticallycorrect student submissions. Then, for a student submission with syntax errors,we query the learnt RNN model with the prefix to- ken sequence to predict tokensequences that can fix the error by either replacing or inserting the predictedtoken sequence at the error location. We evaluate our technique on over 14, 000student submissions with syntax errors. Our technique can completely re- pair31.69% (4501/14203) of submissions with syntax errors and in addition partiallycorrect 6.39% (908/14203) of the submissions.
arxiv-16500-299 | The Computational Power of Dynamic Bayesian Networks | http://arxiv.org/pdf/1603.06125v1.pdf | author:Joshua Brulé category:cs.AI stat.ML published:2016-03-19 summary:This paper considers the computational power of constant size, dynamicBayesian networks. Although discrete dynamic Bayesian networks are no morepowerful than hidden Markov models, dynamic Bayesian networks with continuousrandom variables and discrete children of continuous parents are capable ofperforming Turing-complete computation. With modified versions of existingalgorithms for belief propagation, such a simulation can be carried out in realtime. This result suggests that dynamic Bayesian networks may be more powerfulthan previously considered. Relationships to causal models and recurrent neuralnetworks are also discussed.
arxiv-16500-300 | Buried object detection using handheld WEMI with task-driven extended functions of multiple instances | http://arxiv.org/pdf/1603.06121v1.pdf | author:Matthew Cook, Alina Zare, Dominic Ho category:cs.CV published:2016-03-19 summary:Many effective supervised discriminative dictionary learning methods havebeen developed in the literature. However, when training these algorithms,precise ground-truth of the training data is required to provide very accuratepoint-wise labels. Yet, in many applications, accurate labels are not alwaysfeasible. This is especially true in the case of buried object detection inwhich the size of the objects are not consistent. In this paper, a new multipleinstance dictionary learning algorithm for detecting buried objects using ahandheld WEMI sensor is detailed. The new algorithm, Task Driven ExtendedFunctions of Multiple Instances, can overcome data that does not have veryprecise point-wise labels and still learn a highly discriminative dictionary.Results are presented and discussed on measured WEMI data.
