arxiv-1511-06811 | Learning visual groups from co-occurrences in space and time | http://arxiv.org/abs/1511.06811 | id:1511.06811 author:Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson category:cs.LG cs.CV  published:2015-11-21 summary:We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories. version:1
arxiv-1511-06807 | Adding Gradient Noise Improves Learning for Very Deep Networks | http://arxiv.org/abs/1511.06807 | id:1511.06807 author:Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, James Martens category:stat.ML cs.LG  published:2015-11-21 summary:Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures. version:1
arxiv-1510-04342 | Estimation and Inference of Heterogeneous Treatment Effects using Random Forests | http://arxiv.org/abs/1510.04342 | id:1510.04342 author:Stefan Wager, Susan Athey category:stat.ME math.ST stat.ML stat.TH  published:2015-10-14 summary:Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. Given a potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms, to our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially as the number of covariates increases. version:2
arxiv-1511-06798 | Conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability | http://arxiv.org/abs/1511.06798 | id:1511.06798 author:Luke Miratrix, Robin Ackerman category:cs.CL cs.IR stat.AP  published:2015-11-20 summary:We propose a general framework for topic-specific summarization of large text corpora, and illustrate how it can be used for analysis in two quite different contexts: an OSHA database of fatality and catastrophe reports (to facilitate surveillance for patterns in circumstances leading to injury or death) and legal decisions on workers' compensation claims (to explore relevant case law). Our summarization framework, built on sparse classification methods, is a compromise between simple word frequency based methods currently in wide use, and more heavyweight, model-intensive methods such as Latent Dirichlet Allocation (LDA). For a particular topic of interest (e.g., mental health disability, or chemical reactions), we regress a labeling of documents onto the high-dimensional counts of all the other words and phrases in the documents. The resulting small set of phrases found as predictive are then harvested as the summary. Using a branch-and-bound approach, this method can be extended to allow for phrases of arbitrary length, which allows for potentially rich summarization. We discuss how focus on the purpose of the summaries can inform choices of regularization parameters and model constraints. We evaluate this tool by comparing computational time and summary statistics of the resulting word lists to three other methods in the literature. We also present a new R package, textreg. Overall, we argue that sparse methods have much to offer text analysis, and is a branch of research that should be considered further in this context. version:1
arxiv-1511-06789 | The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition | http://arxiv.org/abs/1511.06789 | id:1511.06789 author:Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom Duerig, James Philbin, Li Fei-Fei category:cs.CV  published:2015-11-20 summary:While models of fine-grained recognition have made great progress in recent years, little work has focused on a key ingredient of making recognition work: data. We use publicly available, noisy data sources to train generic models which vastly improve upon state-of-the-art on fine-grained benchmarks. First, we present an active learning system using non-expert human raters, and improve upon state-of-the-art performance without any text or other metadata associated with the images. Second, we show that training on publicly-available noisy web image search results achieves even higher accuracies, without using any expert-annotated training data, while scaling to over ten thousand fine-grained categories. We analyze the behavior of our models and data and make a strong case for the importance of data over special-purpose modeling: using only an off-the-shelf CNN, we obtain top-1 accuracies of 92.8\% on CUB-200-2011 Birds, 85.4\% on Birdsnap, 95.9\% on FGVC-Aircraft, and 82.6\% on Stanford Dogs. version:1
arxiv-1507-07680 | Training recurrent networks online without backtracking | http://arxiv.org/abs/1507.07680 | id:1507.07680 author:Yann Ollivier, Corentin Tallec, Guillaume Charpiat category:cs.NE cs.LG stat.ML  published:2015-07-28 summary:We introduce the "NoBackTrack" algorithm to train the parameters of dynamical systems such as recurrent neural networks. This algorithm works in an online, memoryless setting, thus requiring no backpropagation through time, and is scalable, avoiding the large computational and memory cost of maintaining the full gradient of the current state with respect to the parameters. The algorithm essentially maintains, at each time, a single search direction in parameter space. The evolution of this search direction is partly stochastic and is constructed in such a way to provide, at every time, an unbiased random estimate of the gradient of the loss function with respect to the parameters. Because the gradient estimate is unbiased, on average over time the parameter is updated as it should. The resulting gradient estimate can then be fed to a lightweight Kalman-like filter to yield an improved algorithm. For recurrent neural networks, the resulting algorithms scale linearly with the number of parameters. Small-scale experiments confirm the suitability of the approach, showing that the stochastic approximation of the gradient introduced in the algorithm is not detrimental to learning. In particular, the Kalman-like version of NoBackTrack is superior to backpropagation through time (BPTT) when the time span of dependencies in the data is longer than the truncation span for BPTT. version:2
arxiv-1511-07422 | Variational Bayes Factor Analysis for i-Vector Extraction | http://arxiv.org/abs/1511.07422 | id:1511.07422 author:Jesús Villalba category:stat.ML  published:2015-11-20 summary:In this document we are going to derive the equations needed to implement a Variational Bayes i-vector extractor. This can be used to extract longer i-vectors reducing the risk of overfittig or to adapt an i-vector extractor from a database to another with scarce development data. This work is based on Patrick Kenny's joint factor analysis and Christopher Bishop's variational principal components. version:1
arxiv-1511-07421 | Unsupervised Adaptation of SPLDA | http://arxiv.org/abs/1511.07421 | id:1511.07421 author:Jesús Villalba category:stat.ML  published:2015-11-20 summary:State-of-the-art speaker recognition relays on models that need a large amount of training data. This models are successful in tasks like NIST SRE because there is sufficient data available. However, in real applications, we usually do not have so much data and, in many cases, the speaker labels are unknown. We present a method to adapt a PLDA model from a domain with a large amount of labeled data to another with unlabeled data. We describe a generative model that produces both sets of data where the unknown labels are modeled like latent variables. We used variational Bayes to estimate the hidden variables. Here, we derive the equations for this model. This model has been used in the papers: "UNSUPERVISED ADAPTATION OF PLDA BY USING VARIATIONAL BAYES METHODS" publised at ICASSP 2014, "Unsupervised Training of PLDA with Variational Bayes" published at Iberspeech 2014, and "VARIATIONAL BAYESIAN PLDA FOR SPEAKER DIARIZATION IN THE MGB CHALLENGE" published at ASRU 2015. version:1
arxiv-1511-06772 | PLDA with Two Sources of Inter-session Variability | http://arxiv.org/abs/1511.06772 | id:1511.06772 author:Jesús Villalba category:stat.ML  published:2015-11-20 summary:In some speaker recognition scenarios we find conversations recorded simultaneously over multiple channels. That is the case of the interviews in the NIST SRE dataset. To take advantage of that, we propose a modification of the PLDA model that considers two different inter-session variability terms. The first term is tied between all the recordings belonging to the same conversation whereas the second is not. Thus, the former mainly intends to capture the variability due to the phonetic content of the conversation while the latter tries to capture the channel variability. In this document, we derive the equations for this model. This model was applied in the paper "Handling Recordings Acquired Simultaneously over Multiple Channels with PLDA" published at Interspeech 2013. version:1
arxiv-1511-07318 | Bayesian SPLDA | http://arxiv.org/abs/1511.07318 | id:1511.07318 author:Jesús Villalba category:stat.ML  published:2015-11-20 summary:In this document we are going to derive the equations needed to implement a Variational Bayes estimation of the parameters of the simplified probabilistic linear discriminant analysis (SPLDA) model. This can be used to adapt SPLDA from one database to another with few development data or to implement the fully Bayesian recipe. Our approach is similar to Bishop's VB PPCA. version:1
arxiv-1511-06746 | Images Don't Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank | http://arxiv.org/abs/1511.06746 | id:1511.06746 author:Corey Lynch, Kamelia Aryafar, Josh Attenberg category:cs.CV cs.LG  published:2015-11-20 summary:Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multibillion dollar machine learning problem. Traditional models optimize over a few hand-constructed features based on the item's text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy, we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can capture fine-grained style information not available in a text-only representation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model. version:1
arxiv-1511-06251 | Dynamics of Stochastic Gradient Algorithms | http://arxiv.org/abs/1511.06251 | id:1511.06251 author:Qianxiao Li, Cheng Tai, Weinan E category:cs.LG stat.ML 68W20  published:2015-11-19 summary:Stochastic gradient algorithms (SGA) are increasingly popular in machine learning applications and have become "the algorithm" for extremely large scale problems. Although there are some convergence results, little is known about their dynamics. In this paper, We propose the method of stochastic modified equations (SME) to analyze the dynamics of the SGA. Using this technique, we can give precise characterizations for both the initial convergence speed and the eventual oscillations, at least in some special cases. Furthermore, the SME formalism allows us to characterize various speed-up techniques, such as introducing momentum, adjusting the learning rate and the mini-batch sizes. Previously, these techniques relied mostly on heuristics. Besides introducing simple examples to illustrate the SME formalism, we also apply the framework to improve the relaxed randomized Kaczmarz method for solving linear equations. The SME framework is a precise and unifying approach to understanding and improving the SGA, and has the potential to be applied to many more stochastic algorithms. version:2
arxiv-1506-02535 | Efficient Learning of Ensembles with QuadBoost | http://arxiv.org/abs/1506.02535 | id:1506.02535 author:Louis Fortier-Dubois, François Laviolette, Mario Marchand, Louis-Emile Robitaille, Jean-Francis Roy category:cs.LG  published:2015-06-08 summary:We first present a general risk bound for ensembles that depends on the Lp norm of the weighted combination of voters which can be selected from a continuous set. We then propose a boosting method, called QuadBoost, which is strongly supported by the general risk bound and has very simple rules for assigning the voters' weights. Moreover, QuadBoost exhibits a rate of decrease of its empirical error which is slightly faster than the one achieved by AdaBoost. The experimental results confirm the expectation of the theory that QuadBoost is a very efficient method for learning ensembles. version:5
arxiv-1407-4118 | Machine Learning Classification of SDSS Transient Survey Images | http://arxiv.org/abs/1407.4118 | id:1407.4118 author:L. du Buisson, N. Sivanandam, B. A. Bassett, M. Smith category:astro-ph.IM astro-ph.CO cs.CV  published:2014-07-15 summary:We show that multiple machine learning algorithms can match human performance in classifying transient imaging data from the Sloan Digital Sky Survey (SDSS) supernova survey into real objects and artefacts. This is a first step in any transient science pipeline and is currently still done by humans, but future surveys such as the Large Synoptic Survey Telescope (LSST) will necessitate fully machine-enabled solutions. Using features trained from eigenimage analysis (principal component analysis, PCA) of single-epoch g, r and i-difference images, we can reach a completeness (recall) of 96 per cent, while only incorrectly classifying at most 18 per cent of artefacts as real objects, corresponding to a precision (purity) of 84 per cent. In general, random forests performed best, followed by the k-nearest neighbour and the SkyNet artificial neural net algorithms, compared to other methods such as na\"ive Bayes and kernel support vector machine. Our results show that PCA-based machine learning can match human success levels and can naturally be extended by including multiple epochs of data, transient colours and host galaxy information which should allow for significant further improvements, especially at low signal-to-noise. version:3
arxiv-1511-06718 | Top-N recommendations from expressive recommender systems | http://arxiv.org/abs/1511.06718 | id:1511.06718 author:Cyril Stark category:cs.LG stat.ML  published:2015-11-20 summary:Normalized nonnegative models assign probability distributions to users and random variables to items; see [Stark, 2015]. Rating an item is regarded as sampling the random variable assigned to the item with respect to the distribution assigned to the user who rates the item. Models of that kind are highly expressive. For instance, using normalized nonnegative models we can understand users' preferences as mixtures of interpretable user stereotypes, and we can arrange properties of users and items in a hierarchical manner. These features would not be useful if the predictive power of normalized nonnegative models was poor. Thus, we analyze here the performance of normalized nonnegative models for top-N recommendation and observe that their performance matches the performance of methods like PureSVD which was introduced in [Cremonesi et al., 2010]. We conclude that normalized nonnegative models not only provide accurate recommendations but they also deliver (for free) representations that are interpretable. We deepen the discussion of normalized nonnegative models by providing further theoretical insights. In particular, we introduce total variational distance as an operational similarity measure, we discover scenarios where normalized nonnegative models yield unique representations of users and items, we prove that the inference of optimal normalized nonnegative models is NP-hard and finally, we discuss the relationship between normalized nonnegative models and nonnegative matrix factorization. version:1
arxiv-1511-06704 | Semantic Diversity versus Visual Diversity in Visual Dictionaries | http://arxiv.org/abs/1511.06704 | id:1511.06704 author:Otávio A. B. Penatti, Sandra Avila, Eduardo Valle, Ricardo da S. Torres category:cs.CV  published:2015-11-20 summary:Visual dictionaries are a critical component for image classification/retrieval systems based on the bag-of-visual-words (BoVW) model. Dictionaries are usually learned without supervision from a training set of images sampled from the collection of interest. However, for large, general-purpose, dynamic image collections (e.g., the Web), obtaining a representative sample in terms of semantic concepts is not straightforward. In this paper, we evaluate the impact of semantics in the dictionary quality, aiming at verifying the importance of semantic diversity in relation visual diversity for visual dictionaries. In the experiments, we vary the amount of classes used for creating the dictionary and then compute different BoVW descriptors, using multiple codebook sizes and different coding and pooling methods (standard BoVW and Fisher Vectors). Results for image classification show that as visual dictionaries are based on low-level visual appearances, visual diversity is more important than semantic diversity. Our conclusions open the opportunity to alleviate the burden in generating visual dictionaries as we need only a visually diverse set of images instead of the whole collection to create a good dictionary. version:1
arxiv-1511-06702 | Single-view to Multi-view: Reconstructing Unseen Views with a Convolutional Network | http://arxiv.org/abs/1511.06702 | id:1511.06702 author:Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox category:cs.CV  published:2015-11-20 summary:We present a convolutional network capable of generating images of a previously unseen object from arbitrary viewpoints given a single image of this object. The input to the network is a single image and the desired new viewpoint; the output is a view of the object from this desired viewpoint. The network is trained on renderings of synthetic 3D models. It learns an implicit 3D representation of the object class, which allows it to transfer shape knowledge from training instances to a new object instance. Beside the color image, the network can also generate the depth map of an object from arbitrary viewpoints. This allows us to predict 3D point clouds from a single image, which can be fused into a surface mesh. We experimented with cars and chairs. Even though the network is trained on artificial data, it generalizes well to objects in natural images without any modifications. version:1
arxiv-1511-06683 | Top-k Multiclass SVM | http://arxiv.org/abs/1511.06683 | id:1511.06683 author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.CV cs.LG  published:2015-11-20 summary:Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines. version:1
arxiv-1511-06681 | Deep End2End Voxel2Voxel Prediction | http://arxiv.org/abs/1511.06681 | id:1511.06681 author:Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri category:cs.CV  published:2015-11-20 summary:Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive pre-processing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains. version:1
arxiv-1511-06674 | Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation | http://arxiv.org/abs/1511.06674 | id:1511.06674 author:Anirudh Goyal, Marius Leordeanu category:cs.CV cs.CL  published:2015-11-20 summary:Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge. version:1
arxiv-1511-06663 | L1 logistic regression as a feature selection step for training stable classification trees for the prediction of severity criteria in imported malaria | http://arxiv.org/abs/1511.06663 | id:1511.06663 author:Luca Talenti, Margaux Luck, Anastasia Yartseva, Nicolas Argy, Sandrine Houzé, Cecilia Damon category:cs.LG q-bio.QM stat.AP  published:2015-11-20 summary:Multivariate classification methods using explanatory and predictive models are necessary for characterizing subgroups of patients according to their risk profiles. Popular methods include logistic regression and classification trees with performances that vary according to the nature and the characteristics of the dataset. In the context of imported malaria, we aimed at classifying severity criteria based on a heterogeneous patient population. We investigated these approaches by implementing two different strategies: L1 logistic regression (L1LR) that models a single global solution and classification trees that model multiple local solutions corresponding to discriminant subregions of the feature space. For each strategy, we built a standard model, and a sparser version of it. As an alternative to pruning, we explore a promising approach that first constrains the tree model with an L1LR-based feature selection, an approach we called L1LR-Tree. The objective is to decrease its vulnerability to small data variations by removing variables corresponding to unstable local phenomena. Our study is twofold: i) from a methodological perspective comparing the performances and the stability of the three previous methods, i.e L1LR, classification trees and L1LR-Tree, for the classification of severe forms of imported malaria, and ii) from an applied perspective improving the actual classification of severe forms of imported malaria by identifying more personalized profiles predictive of several clinical criteria based on variables dismissed for the clinical definition of the disease. The main methodological results show that the combined method L1LR-Tree builds sparse and stable models that significantly predicts the different severity criteria and outperforms all the other methods in terms of accuracy. version:1
arxiv-1510-07526 | Empirical Study on Deep Learning Models for Question Answering | http://arxiv.org/abs/1510.07526 | id:1510.07526 author:Yang Yu, Wei Zhang, Chung-Wei Hang, Bing Xiang, Bowen Zhou category:cs.CL cs.AI cs.LG  published:2015-10-26 summary:In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem. version:3
arxiv-1511-06631 | Multi-Contrast MRI Reconstruction with Structure-Guided Total Variation | http://arxiv.org/abs/1511.06631 | id:1511.06631 author:Matthias J. Ehrhardt, Marta M. Betcke category:math.NA cs.CV math.OC  published:2015-11-20 summary:Magnetic resonance imaging (MRI) is a versatile imaging technique that allows different contrasts depending on the acquisition parameters. Many clinical imaging studies acquire MRI data for more than one of these contrasts---such as for instance T1 and T2 weighted images---which makes the overall scanning procedure very time consuming. As all of these images show the same underlying anatomy one can try to omit unnecessary measurements by taking the similarity into account during reconstruction. We will discuss two modifications of total variation---based on i) location and ii) direction---that take structural a priori knowledge into account and reduce to total variation in the degenerate case when no structural knowledge is available. We solve the resulting convex minimization problem with the alternating direction method of multipliers that separates the forward operator from the prior. For both priors the corresponding proximal operator can be implemented as an extension of the fast gradient projection method on the dual problem for total variation. We tested the priors on six data sets that are based on phantoms and real MRI images. In all test cases exploiting the structural information from the other contrast yields better results than separate reconstruction with total variation in terms of standard metrics like peak signal-to-noise ratio and structural similarity index. Furthermore, we found that exploiting the two dimensional directional information results in images with well defined edges, superior to those reconstructed solely using a priori information about the edge location. version:1
arxiv-1511-06627 | Towards Arbitrary-View Face Alignment by Recommendation Trees | http://arxiv.org/abs/1511.06627 | id:1511.06627 author:Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2015-11-20 summary:Learning to simultaneously handle face alignment of arbitrary views, e.g. frontal and profile views, appears to be more challenging than we thought. The difficulties lay in i) accommodating the complex appearance-shape relations exhibited in different views, and ii) encompassing the varying landmark point sets due to self-occlusion and different landmark protocols. Most existing studies approach this problem via training multiple viewpoint-specific models, and conduct head pose estimation for model selection. This solution is intuitive but the performance is highly susceptible to inaccurate head pose estimation. In this study, we address this shortcoming through learning an Ensemble of Model Recommendation Trees (EMRT), which is capable of selecting optimal model configuration without prior head pose estimation. The unified framework seamlessly handles different viewpoints and landmark protocols, and it is trained by optimising directly on landmark locations, thus yielding superior results on arbitrary-view face alignment. This is the first study that performs face alignment on the full AFLWdataset with faces of different views including profile view. State-of-the-art performances are also reported on MultiPIE and AFW datasets containing both frontaland profile-view faces. version:1
arxiv-1506-07365 | Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images | http://arxiv.org/abs/1506.07365 | id:1506.07365 author:Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller category:cs.LG cs.CV stat.ML  published:2015-06-24 summary:We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems. version:3
arxiv-1511-05607 | Identifying the Absorption Bump with Deep Learning | http://arxiv.org/abs/1511.05607 | id:1511.05607 author:Min Li, Sudeep Gaddam, Xiaolin Li, Yinan Zhao, Jingzhe Ma, Jian Ge category:cs.CV cs.LG cs.NE  published:2015-11-17 summary:The pervasive interstellar dust grains provide significant insights to understand the formation and evolution of the stars, planetary systems, and the galaxies, and may harbor the building blocks of life. One of the most effective way to analyze the dust is via their interaction with the light from background sources. The observed extinction curves and spectral features carry the size and composition information of dust. The broad absorption bump at 2175 Angstrom is the most prominent feature in the extinction curves. Traditionally, statistical methods are applied to detect the existence of the absorption bump. These methods require heavy preprocessing and the co-existence of other reference features to alleviate the influence from the noises. In this paper, we apply Deep Learning techniques to detect the broad absorption bump. We demonstrate the key steps for training the selected models and their results. The success of Deep Learning based method inspires us to generalize a common methodology for broader science discovery problems. We present our on-going work to build the DeepDis system for such kind of applications. version:2
arxiv-1511-06603 | Exponential Natural Particle Filter | http://arxiv.org/abs/1511.06603 | id:1511.06603 author:Ghazal Zand, Mojtaba Taherkhani, Reza Safabakhsh category:cs.LG cs.NE cs.RO  published:2015-11-20 summary:Particle Filter algorithm (PF) suffers from some problems such as the loss of particle diversity, the need for large number of particles, and the costly selection of the importance density functions. In this paper, a novel Exponential Natural Particle Filter (xNPF) is introduced to solve the above problems. In this approach, a state transitional probability with the use of natural gradient learning is proposed which balances exploration and exploitation more robustly. The results show that xNPF converges much closer to the true target states than the other state of the art particle filter. version:1
arxiv-1511-06591 | Polysemy in Controlled Natural Language Texts | http://arxiv.org/abs/1511.06591 | id:1511.06591 author:Normunds Gruzitis, Guntis Barzdins category:cs.CL  published:2015-11-20 summary:Computational semantics and logic-based controlled natural languages (CNL) do not address systematically the word sense disambiguation problem of content words, i.e., they tend to interpret only some functional words that are crucial for construction of discourse representation structures. We show that micro-ontologies and multi-word units allow integration of the rich and polysemous multi-domain background knowledge into CNL thus providing interpretation for the content words. The proposed approach is demonstrated by extending the Attempto Controlled English (ACE) with polysemous and procedural constructs resulting in a more natural CNL named PAO covering narrative multi-domain texts. version:1
arxiv-1511-06586 | Crowd Behavior Analysis: A Review where Physics meets Biology | http://arxiv.org/abs/1511.06586 | id:1511.06586 author:Ven Jyn Kok, Mei Kuan Lim, Chee Seng Chan category:cs.CV cs.AI cs.NE  published:2015-11-20 summary:Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision stud- ies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision. version:1
arxiv-1511-00438 | Semantic Summarization of Egocentric Photo Stream Events | http://arxiv.org/abs/1511.00438 | id:1511.00438 author:Aniol Lidon, Marc Bolaños, Mariella Dimiccoli, Petia Radeva, Maite Garolera, Xavier Giró-i-Nieto category:cs.CV  published:2015-11-02 summary:With the rapid increase of users of wearable cameras in recent years and of the amount of data they produce, there is a strong need for automatic retrieval and summarization techniques. This work addresses the problem of automatically summarizing egocentric photo streams captured through a wearable camera by taking an image retrieval perspective. After removing non-informative images by a new CNN-based filter, images are ranked by relevance to ensure semantic diversity and finally re-ranked by a novelty criterion to reduce redundancy. To assess the results, a new evaluation metric is proposed which takes into account the non-uniqueness of the solution. Experimental results applied on a database of 7,110 images from 6 different subjects and evaluated by experts gave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of 5.0. version:2
arxiv-1511-00043 | Learning Adversary Behavior in Security Games: A PAC Model Perspective | http://arxiv.org/abs/1511.00043 | id:1511.00043 author:Arunesh Sinha, Debarun Kar, Milind Tambe category:cs.AI cs.GT cs.LG  published:2015-10-30 summary:Recent applications of Stackelberg Security Games (SSG), from wildlife crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interactions. Given these recent developments, this paper commits to an approach of directly learning the response function of the adversary. Using the PAC model, this paper lays a firm theoretical foundation for learning in SSGs (e.g., theoretically answer questions about the numbers of samples required to learn adversary behavior) and provides utility guarantees when the learned adversary model is used to plan the defender's strategy. The paper also aims to answer practical questions such as how much more data is needed to improve an adversary model's accuracy. Additionally, we explain a recently observed phenomenon that prediction accuracy of learned adversary behavior is not enough to discover the utility maximizing defender strategy. We provide four main contributions: (1) a PAC model of learning adversary response functions in SSGs; (2) PAC-model analysis of the learning of key, existing bounded rationality models in SSGs; (3) an entirely new approach to adversary modeling based on a non-parametric class of response functions with PAC-model analysis and (4) identification of conditions under which computing the best defender strategy against the learned adversary behavior is indeed the optimal strategy. Finally, we conduct experiments with real-world data from a national park in Uganda, showing the benefit of our new adversary modeling approach and verification of our PAC model predictions. version:3
arxiv-1511-06523 | WIDER FACE: A Face Detection Benchmark | http://arxiv.org/abs/1511.06523 | id:1511.06523 author:Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2015-11-20 summary:Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated. Dataset can be downloaded at: mmlab.ie.cuhk.edu.hk/projects/WIDERFace version:1
arxiv-1511-05688 | A Distribution Adaptive Framework for Prediction Interval Estimation Using Nominal Variables | http://arxiv.org/abs/1511.05688 | id:1511.05688 author:Ameen Eetemadi, Ilias Tagkopoulos category:cs.LG  published:2015-11-18 summary:Proposed methods for prediction interval estimation so far focus on cases where input variables are numerical. In datasets with solely nominal input variables, we observe records with the exact same input $x^u$, but different real valued outputs due to the inherent noise in the system. Existing prediction interval estimation methods do not use representations that can accurately model such inherent noise in the case of nominal inputs. We propose a new prediction interval estimation method tailored for this type of data, which is prevalent in biology and medicine. We call this method Distribution Adaptive Prediction Interval Estimation given Nominal inputs (DAPIEN) and has four main phases. First, we select a distribution function that can best represent the inherent noise of the system for all unique inputs. Then we infer the parameters $\theta_i$ (e.g. $\theta_i=[mean_i, variance_i]$) of the selected distribution function for all unique input vectors $x^u_i$ and generate a new corresponding training set using pairs of $x^u_i, \theta_i$. III). Then, we train a model to predict $\theta$ given a new $x_u$. Finally, we calculate the prediction interval for a new sample using the inverse of the cumulative distribution function once the parameters $\theta$ is predicted by the trained model. We compared DAPIEN to the commonly used Bootstrap method on three synthetic datasets. Our results show that DAPIEN provides tighter prediction intervals while preserving the requested coverage when compared to Bootstrap. This work can facilitate broader usage of regression methods in medicine and biology where it is necessary to provide tight prediction intervals while preserving coverage when input variables are nominal. version:2
arxiv-1412-5474 | Flattened Convolutional Neural Networks for Feedforward Acceleration | http://arxiv.org/abs/1412.5474 | id:1412.5474 author:Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello category:cs.NE cs.LG  published:2014-12-17 summary:We present flattened convolutional neural networks that are designed for fast feedforward execution. The redundancy of the parameters, especially weights of the convolutional filters in convolutional neural networks has been extensively studied and different heuristics have been proposed to construct a low rank basis of the filters after training. In this work, we train flattened networks that consist of consecutive sequence of one-dimensional filters across all directions in 3D space to obtain comparable performance as conventional convolutional networks. We tested flattened model on different datasets and found that the flattened layer can effectively substitute for the 3D filters without loss of accuracy. The flattened convolution pipelines provide around two times speed-up during feedforward pass compared to the baseline model due to the significant reduction of learning parameters. Furthermore, the proposed method does not require efforts in manual tuning or post processing once the model is trained. version:4
arxiv-1511-06494 | Bidirectional Warping of Active Appearance Model | http://arxiv.org/abs/1511.06494 | id:1511.06494 author:Ali Mollahosseini, Mohammad H. Mahoor category:cs.CV  published:2015-11-20 summary:Active Appearance Model (AAM) is a commonly used method for facial image analysis with applications in face identification and facial expression recognition. This paper proposes a new approach based on image alignment for AAM fitting called bidirectional warping. Previous approaches warp either the input image or the appearance template. We propose to warp both the input image, using incremental update by an affine transformation, and the appearance template, using an inverse compositional approach. Our experimental results on Multi-PIE face database show that the bidirectional approach outperforms state-of-the-art inverse compositional fitting approaches in extracting landmark points of faces with shape and pose variations. version:1
arxiv-1511-06489 | A Simple Hierarchical Pooling Data Structure for Loop Closure | http://arxiv.org/abs/1511.06489 | id:1511.06489 author:Xiaohan Fei, Konstantine Tsotsos, Stefano Soatto category:cs.CV cs.RO  published:2015-11-20 summary:We propose a data structure obtained by hierarchically averaging bag-of-word descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 4 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance. version:1
arxiv-1511-05392 | Infinite Dimensional Word Embeddings | http://arxiv.org/abs/1511.05392 | id:1511.05392 author:Eric Nalisnick, Sachin Ravi category:stat.ML cs.CL cs.LG  published:2015-11-17 summary:We describe a method for learning word embeddings with stochastic dimensionality. Our Infinite Skip-Gram (iSG) model specifies an energy-based joint distribution over a word vector, a context vector, and their dimensionality. By employing the same techniques used to make the Infinite Restricted Boltzmann Machine (Cote & Larochelle, 2015) tractable, we define vector dimensionality over a countably infinite domain, allowing vectors to grow as needed during training. After training, we find that the distribution over embedding dimensionality for a given word is highly interpretable and leads to an elegant probabilistic mechanism for word sense induction. We show qualitatively and quantitatively that the iSG produces parameter-efficient representations that are robust to language's inherent ambiguity. version:2
arxiv-1503-02945 | Fast Multi-class Dictionaries Learning with Geometrical Directions in MRI Reconstruction | http://arxiv.org/abs/1503.02945 | id:1503.02945 author:Zhifang Zhan, Jian-Feng Cai, Di Guo, Yunsong Liu, Zhong Chen, Xiaobo Qu category:cs.CV math.OC physics.med-ph  published:2015-03-10 summary:Objective: Improve the reconstructed image with fast and multi-class dictionaries learning when magnetic resonance imaging is accelerated by undersampling the k-space data. Methods: A fast orthogonal dictionary learning method is introduced into magnetic resonance image reconstruction to providing adaptive sparse representation of images. To enhance the sparsity, image is divided into classified patches according to the same geometrical direction and dictionary is trained within each class. A new sparse reconstruction model with the multi-class dictionaries is proposed and solved using a fast alternating direction method of multipliers. Results: Experiments on phantom and brain imaging data with acceleration factor up to 10 and various undersampling patterns are conducted. The proposed method is compared with state-of-the-art magnetic resonance image reconstruction methods. Conclusion: Artifacts are better suppressed and image edges are better preserved than the compared methods. Besides, the computation of the proposed approach is much faster than the typical K-SVD dictionary learning method in magnetic resonance image reconstruction. Significance: The proposed method can be exploited in undersapmled magnetic resonance imaging to reduce data acquisition time and reconstruct images with better image quality. version:2
arxiv-1510-02934 | Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A framework for single-subject analysis in diffusion tensor imaging | http://arxiv.org/abs/1510.02934 | id:1510.02934 author:Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan İrfanoğlu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard Riedy category:physics.med-ph cs.CV stat.AP stat.CO stat.ME  published:2015-10-10 summary:The purpose of this work is to develop a framework for single-subject analysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI) is capable of testing whether an individual tract as represented by the major eigenvector of the diffusion tensor and its corresponding angular dispersion are significantly different from a group of tracts on a voxel-by-voxel basis. This work develops two complementary statistical tests based on the elliptical cone of uncertainty (COU), which is a model of uncertainty or dispersion of the major eigenvector of the diffusion tensor. The orientation deviation test examines whether the major eigenvector from a single subject is within the average elliptical COU formed by a collection of elliptical COUs. The shape deviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test between the normalized shape measures (area and circumference) of the elliptical cones of uncertainty of the single subject against a group of controls. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR) were incorporated in the orientation deviation test. The shape deviation test uses FDR only. TOADDI was found to be numerically accurate and statistically effective. Clinical data from two Traumatic Brain Injury (TBI) patients and one non-TBI subject were tested against the data obtained from a group of 45 non-TBI controls to illustrate the application of the proposed framework in single-subject analysis. The frontal portion of the superior longitudinal fasciculus seemed to be implicated in both tests as significantly different from that of the control group. The TBI patients and the single non-TBI subject were well separated under the shape deviation test at the chosen FDR level of 0.0005. TOADDI is a simple but novel geometrically based statistical framework for analyzing DTI data. version:2
arxiv-1511-06462 | Joint Inverse Covariances Estimation with Mutual Linear Structure | http://arxiv.org/abs/1511.06462 | id:1511.06462 author:Ilya Soloveychik, Ami Wiesel category:stat.ML stat.AP  published:2015-11-20 summary:We consider the problem of joint estimation of structured inverse covariance matrices. We perform the estimation using groups of measurements with different covariances of the same unknown structure. Assuming the inverse covariances to span a low dimensional linear subspace in the space of symmetric matrices, our aim is to determine this structure. It is then utilized to improve the estimation of the inverse covariances. We propose a novel optimization algorithm discovering and exploiting the underlying structure and provide its efficient implementation. Numerical simulations are presented to illustrate the performance benefits of the proposed algorithm. version:1
arxiv-1511-05067 | Efficient Likelihood Learning of a Generic CNN-CRF Model for Semantic Segmentation | http://arxiv.org/abs/1511.05067 | id:1511.05067 author:Alexander Kirillov, Dmitrij Schlesinger, Walter Forkel, Anatoly Zelenin, Shuai Zheng, Philip Torr, Carsten Rother category:cs.CV  published:2015-11-16 summary:Deep Models, such as Convolutional Neural Networks (CNNs), are omnipresent in computer vision, as well as, structured models, such as Conditional Random Fields (CRFs). Combining them brings many advantages, foremost the ability to in-cooperate prior knowledge into CNNs, e.g. by explicitly modelling the dependencies between output variables. In this work we present a CRF model were unary factors are dependent on a CNN. Our main contribution is an efficient and scalable, maximum likelihood-based, learning procedure to infer all model parameters jointly. Previous work either concentrated on piecewise-training, or maximum likelihood learning of restricted model families, such as Gaussian CRFs or CRFs with a few variables only. In contrast, we are the first to perform maximum likelihood learning for large-sized factor graphs with non-parametric potentials. We have applied our model to the task of semantic labeling of body parts in depth images. We show that it is superior to selected competing models and learning strategies. Furthermore, we empirically observe that our model can capture shape and context information of relating body parts. version:2
arxiv-1511-05653 | Why are deep nets reversible: A simple theory, with implications for training | http://arxiv.org/abs/1511.05653 | id:1511.05653 author:Sanjeev Arora, Yingyu Liang, Tengyu Ma category:cs.LG  published:2015-11-18 summary:Generative models for deep learning are promising both to improve understanding of the model, and yield training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above. However, there is no accompanying "proof of correctness" for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated. The current paper takes a more theoretical tack. It presents a very simple generative model for RELU deep nets, with the following characteristics: (i) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old weight tying idea for denoising autoencoders.) (ii) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. The generative model suggests a simple modification for training: use the generative model to produce synthetic data with labels and include it in the training set. Experiments are shown to support this theory of random-like deep nets; and that it helps the training. version:2
arxiv-1507-00418 | No-Regret Learning in Bayesian Games | http://arxiv.org/abs/1507.00418 | id:1507.00418 author:Jason Hartline, Vasilis Syrgkanis, Eva Tardos category:cs.GT cs.LG  published:2015-07-02 summary:Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information. version:2
arxiv-1511-06452 | Deep Metric Learning via Lifted Structured Feature Embedding | http://arxiv.org/abs/1511.06452 | id:1511.06452 author:Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese category:cs.CV cs.LG  published:2015-11-19 summary:Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network. version:1
arxiv-1511-04623 | Learning to Represent Words in Context with Multilingual Supervision | http://arxiv.org/abs/1511.04623 | id:1511.04623 author:Kazuya Kawakami, Chris Dyer category:cs.CL  published:2015-11-14 summary:We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these. version:2
arxiv-1511-06438 | Joint Word Representation Learning using a Corpus and a Semantic Lexicon | http://arxiv.org/abs/1511.06438 | id:1511.06438 author:Danushka Bollegala, Alsuhaibani Mohammed, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL cs.AI  published:2015-11-19 summary:Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection. Despite their success, these data-driven word representation learning methods do not consider the rich semantic relational structure between words in a co-occurring context. On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language. We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?. For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon. We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus. Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy. version:1
arxiv-1511-05265 | AUC-maximized Deep Convolutional Neural Fields for Sequence Labeling | http://arxiv.org/abs/1511.05265 | id:1511.05265 author:Sheng Wang, Siqi Sun, Jinbo Xu category:stat.ML cs.LG  published:2015-11-17 summary:Deep Convolutional Neural Networks (DCNN) has shown excellent performance in a variety of machine learning tasks. This manuscript presents Deep Convolutional Neural Fields (DeepCNF), a combination of DCNN with Conditional Random Field (CRF), for sequence labeling with highly imbalanced label distribution. The widely-used training methods, such as maximum-likelihood and maximum labelwise accuracy, do not work well on highly imbalanced data. To handle this, we present a new training algorithm called maximum-AUC for DeepCNF. That is, we train DeepCNF by directly maximizing the empirical Area Under the ROC Curve (AUC), which is an unbiased measurement for imbalanced data. To fulfill this, we formulate AUC in a pairwise ranking framework, approximate it by a polynomial function and then apply a gradient-based procedure to optimize it. We then test our AUC-maximized DeepCNF on three very different protein sequence labeling tasks: solvent accessibility prediction, 8-state secondary structure prediction, and disorder prediction. Our experimental results confirm that maximum-AUC greatly outperforms the other two training methods on 8-state secondary structure prediction and disorder prediction since their label distributions are highly imbalanced and also have similar performance as the other two training methods on the solvent accessibility prediction problem which has three equally-distributed labels. Furthermore, our experimental results also show that our AUC-trained DeepCNF models greatly outperform existing popular predictors of these three tasks. version:2
arxiv-1507-00814 | Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models | http://arxiv.org/abs/1507.00814 | id:1507.00814 author:Bradly C. Stadie, Sergey Levine, Pieter Abbeel category:cs.AI cs.LG stat.ML  published:2015-07-03 summary:Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark. version:3
arxiv-1506-04579 | ParseNet: Looking Wider to See Better | http://arxiv.org/abs/1506.04579 | id:1506.04579 author:Wei Liu, Andrew Rabinovich, Alexander C. Berg category:cs.CV  published:2015-06-15 summary:We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn . version:2
arxiv-1511-06419 | Canonical Autocorrelation Analysis | http://arxiv.org/abs/1511.06419 | id:1511.06419 author:Maria De-Arteaga, Artur Dubrawski, Peter Huggins category:stat.ML cs.LG  published:2015-11-19 summary:We present an extension of sparse Canonical Correlation Analysis (CCA) designed for finding multiple-to-multiple linear correlations within a single set of variables. Unlike CCA, which finds correlations between two sets of data where the rows are matched exactly but the columns represent separate sets of variables, the method proposed here, Canonical Autocorrelation Analysis (CAA), finds multivariate correlations within just one set of variables. This can be useful when we look for hidden parsimonious structures in data, each involving only a small subset of all features. In addition, the discovered correlations are highly interpretable as they are formed by pairs of sparse linear combinations of the original features. We show how CAA can be of use as a tool for anomaly detection when the expected structure of correlations is not followed by anomalous data. We illustrate the utility of CAA in two application domains where single-class and unsupervised learning of correlation structures are particularly relevant: breast cancer diagnosis and radiation threat detection. When applied to the Wisconsin Breast Cancer data, single-class CAA is competitive with supervised methods used in literature. On the radiation threat detection task, unsupervised CAA performs significantly better than an unsupervised alternative prevalent in the domain, while providing valuable additional insights for threat analysis. version:1
arxiv-1511-06416 | Fast Parallel SAME Gibbs Sampling on General Discrete Bayesian Networks | http://arxiv.org/abs/1511.06416 | id:1511.06416 author:Daniel Seita, Haoyu Chen, John Canny category:cs.LG stat.ML  published:2015-11-19 summary:A fundamental task in machine learning and related fields is to perform inference on Bayesian networks. Since exact inference takes exponential time in general, a variety of approximate methods are used. Gibbs sampling is one of the most accurate approaches and provides unbiased samples from the posterior but it has historically been too expensive for large models. In this paper, we present an optimized, parallel Gibbs sampler augmented with state replication (SAME or State Augmented Marginal Estimation) to decrease convergence time. We find that SAME can improve the quality of parameter estimates while accelerating convergence. Experiments on both synthetic and real data show that our Gibbs sampler is substantially faster than the state of the art sampler, JAGS, without sacrificing accuracy. Our ultimate objective is to introduce the Gibbs sampler to researchers in many fields to expand their range of feasible inference problems. version:1
arxiv-1511-06388 | sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings | http://arxiv.org/abs/1511.06388 | id:1511.06388 author:Andrew Trask, Phil Michalak, John Liu category:cs.CL cs.LG  published:2015-11-19 summary:Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or "senses". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages. version:1
arxiv-1511-06385 | A Unified Gradient Regularization Family for Adversarial Examples | http://arxiv.org/abs/1511.06385 | id:1511.06385 author:Chunchuan Lyu, Kaizhu Huang, Hai-Ning Liang category:cs.LG stat.ML  published:2015-11-19 summary:Adversarial examples are augmented data points generated by imperceptible perturbation of input samples. They have recently drawn much attention with the machine learning and data mining community. Being difficult to distinguish from real examples, such adversarial examples could change the prediction of many of the best learning models including the state-of-the-art deep learning models. Recent attempts have been made to build robust models that take into account adversarial examples. However, these methods can either lead to performance drops or lack mathematical motivations. In this paper, we propose a unified framework to build robust machine learning models against adversarial examples. More specifically, using the unified framework, we develop a family of gradient regularization methods that effectively penalize the gradient of loss function w.r.t. inputs. Our proposed framework is appealing in that it offers a unified view to deal with adversarial examples. It incorporates another recently-proposed perturbation based approach as a special case. In addition, we present some visual effects that reveals semantic meaning in those perturbations, and thus support our regularization method and provide another explanation for generalizability of adversarial examples. By applying this technique to Maxout networks, we conduct a series of experiments and achieve encouraging results on two benchmark datasets. In particular,we attain the best accuracy on MNIST data (without data augmentation) and competitive performance on CIFAR-10 data. version:1
arxiv-1511-06379 | Dynamic Adaptive Network Intelligence | http://arxiv.org/abs/1511.06379 | id:1511.06379 author:Richard Searle, Megan Bingham-Walker category:cs.CL cs.LG  published:2015-11-19 summary:Accurate representational learning of both the explicit and implicit relationships within data is critical to the ability of machines to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015). version:1
arxiv-1511-06351 | Learning Representations Using Complex-Valued Nets | http://arxiv.org/abs/1511.06351 | id:1511.06351 author:Andy M. Sarroff, Victor Shepardson, Michael A. Casey category:cs.LG cs.NE  published:2015-11-19 summary:Complex-valued neural networks (CVNNs) are an emerging field of research in neural networks due to their potential representational properties for audio, image, and physiological signals. It is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions, such as the Fourier transform. We show how CVNNs can be used to learn complex representations of real valued time-series data. We present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the Wirtinger derivative. We test our methods on a representation learning task for real-valued signals, recurrent complex-valued networks and their real-valued counterparts. Our results show that recurrent complex-valued networks can perform as well as their real-valued counterparts while learning filters that are representative of the domain of the data. version:1
arxiv-1511-05045 | Handcrafted Local Features are Convolutional Neural Networks | http://arxiv.org/abs/1511.05045 | id:1511.05045 author:Zhenzhong Lan, Shoou-I Yu, Ming Lin, Bhiksha Raj, Alexander G. Hauptmann category:cs.CV  published:2015-11-16 summary:Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition. We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks (CNNs) share the same convolution-pooling network structure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm. Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources. Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm. version:2
arxiv-1511-06340 | Robust Classification by Pre-conditioned LASSO and Transductive Diffusion Component Analysis | http://arxiv.org/abs/1511.06340 | id:1511.06340 author:Yanwei Fu, De-An Huang, Leonid Sigal category:cs.LG cs.CV math.ST stat.ML stat.TH  published:2015-11-19 summary:Modern machine learning-based recognition approaches require large-scale datasets with large number of labelled training images. However, such datasets are inherently difficult and costly to collect and annotate. Hence there is a great and growing interest in automatic dataset collection methods that can leverage the web. % which are collected % in a cheap, efficient and yet unreliable way. Collecting datasets in this way, however, requires robust and efficient ways for detecting and excluding outliers that are common and prevalent. % Outliers are thus a % prominent treat of using these dataset. So far, there have been a limited effort in machine learning community to directly detect outliers for robust classification. Inspired by the recent work on Pre-conditioned LASSO, this paper formulates the outlier detection task using Pre-conditioned LASSO and employs \red{unsupervised} transductive diffusion component analysis to both integrate the topological structure of the data manifold, from labeled and unlabeled instances, and reduce the feature dimensionality. Synthetic experiments as well as results on two real-world classification tasks show that our framework can robustly detect the outliers and improve classification. version:1
arxiv-1509-03044 | Recurrent Reinforcement Learning: A Hybrid Approach | http://arxiv.org/abs/1509.03044 | id:1509.03044 author:Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, Ji He category:cs.LG cs.AI cs.SY  published:2015-09-10 summary:Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent's entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we propose a new family of hybrid models that combines the strength of both supervised learning (SL) and reinforcement learning (RL), trained in a joint fashion: The SL component can be a recurrent neural networks (RNN) or its long short-term memory (LSTM) version, which is equipped with the desired property of being able to capture long-term dependency on history, thus providing an effective way of learning the representation of hidden states. The RL component is a deep Q-network (DQN) that learns to optimize the control for maximizing long-term rewards. Extensive experiments in a direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best among a set of previous state-of-the-art methods. version:2
arxiv-1511-06316 | face anti-spoofing based on color texture analysis | http://arxiv.org/abs/1511.06316 | id:1511.06316 author:Zinelabidine Boulkenafet, Jukka Komulainen, Abdenour Hadid category:cs.CV  published:2015-11-19 summary:Research on face spoofing detection has mainly been focused on analyzing the luminance of the face images, hence discarding the chrominance information which can be useful for discriminating fake faces from genuine ones. In this work, we propose a new face anti-spoofing method based on color texture analysis. We analyze the joint color-texture information from the luminance and the chrominance channels using a color local binary pattern descriptor. More specifically, the feature histograms are extracted from each image band separately. Extensive experiments on two benchmark datasets, namely CASIA face anti-spoofing and Replay-Attack databases, showed excellent results compared to the state-of-the-art. Most importantly, our inter-database evaluation depicts that the proposed approach showed very promising generalization capabilities. version:1
arxiv-1511-04868 | An Online Sequence-to-Sequence Model Using Partial Conditioning | http://arxiv.org/abs/1511.04868 | id:1511.04868 author:Navdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, Samy Bengio category:cs.LG cs.CL cs.NE  published:2015-11-16 summary:Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online - at the end of each block, the best candidates from the previous block are extended through the local sequence-to-sequence model. On TIMIT, our online method achieves 19.8% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7% PER compared to 17.6% from the best reported sequence-to-sequence model. Importantly, unlike sequence-to-sequence our model is minimally impacted by the length of the input. On artificially created longer utterances, it achieves 20.9% with a unidirectional model, compared to 20% from the best bidirectional sequence-to-sequence models. version:3
arxiv-1511-06314 | Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks | http://arxiv.org/abs/1511.06314 | id:1511.06314 author:Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, Dhruv Batra category:cs.CV cs.LG cs.NE  published:2015-11-19 summary:Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher "oracle" accuracies than classical ensembles. version:1
arxiv-1511-06312 | Good, Better, Best: Choosing Word Embedding Context | http://arxiv.org/abs/1511.06312 | id:1511.06312 author:James Cross, Bing Xiang, Bowen Zhou category:cs.CL  published:2015-11-19 summary:We propose two methods of learning vector representations of words and phrases that each combine sentence context with structural features extracted from dependency trees. Using several variations of neural network classifier, we show that these combined methods lead to improved performance when used as input features for supervised term-matching. version:1
arxiv-1511-06276 | Faster method for Deep Belief Network based Object classification using DWT | http://arxiv.org/abs/1511.06276 | id:1511.06276 author:Saurabh Sihag, Pranab Kumar Dutta category:cs.CV cs.LG  published:2015-11-19 summary:A Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs. version:1
arxiv-1511-06248 | Critical Parameters in Particle Swarm Optimisation | http://arxiv.org/abs/1511.06248 | id:1511.06248 author:J. Michael Herrmann, Adam Erskine, Thomas Joyce category:cs.NE  published:2015-11-19 summary:Particle swarm optimisation is a metaheuristic algorithm which finds reasonable solutions in a wide range of applied problems if suitable parameters are used. We study the properties of the algorithm in the framework of random dynamical systems which, due to the quasi-linear swarm dynamics, yields analytical results for the stability properties of the particles. Such considerations predict a relationship between the parameters of the algorithm that marks the edge between convergent and divergent behaviours. Comparison with simulations indicates that the algorithm performs best near this margin of instability. version:1
arxiv-1511-06246 | Gaussian Mixture Embeddings for Multiple Word Prototypes | http://arxiv.org/abs/1511.06246 | id:1511.06246 author:Xinchi Chen, Xipeng Qiu, Jingxiang Jiang, Xuanjing Huang category:cs.CL  published:2015-11-19 summary:Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model. version:1
arxiv-1511-06233 | Towards Open Set Deep Networks | http://arxiv.org/abs/1511.06233 | id:1511.06233 author:Abhijit Bendale, Terrance Boult category:cs.CV cs.LG  published:2015-11-19 summary:Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of "fooling" and unrelated open set images presented to the system; OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities. version:1
arxiv-1511-06214 | Automatically selecting inference algorithms for discrete energy minimisation | http://arxiv.org/abs/1511.06214 | id:1511.06214 author:Paul Henderson, Vittorio Ferrari category:cs.CV  published:2015-11-19 summary:Minimisation of discrete energies defined over factors is an important problem in computer vision, and a vast number of MAP inference algorithms have been proposed. Different inference algorithms perform better on factor graph models (GMs) from different underlying problem classes, and in general it is difficult to know which algorithm will yield the lowest energy for a given GM. To mitigate this difficulty, survey papers advise the practitioner on what algorithms perform well on what classes of models. We take the next step forward, and present a technique to automatically select the best inference algorithm for an input GM. We validate our method experimentally on an extended version of the OpenGM2 benchmark, containing a diverse set of vision problems. On average, our method selects an inference algorithm yielding labellings with 96% of variables the same as the best available algorithm. version:1
arxiv-1506-03340 | Teaching Machines to Read and Comprehend | http://arxiv.org/abs/1506.03340 | id:1506.03340 author:Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom category:cs.CL cs.AI cs.NE  published:2015-06-10 summary:Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure. version:3
arxiv-1511-06208 | Diffusion Representations | http://arxiv.org/abs/1511.06208 | id:1511.06208 author:Moshe Salhov, Amit Bermanis, Guy Wolf, Amir Averbuch category:stat.ML cs.LG math.SP  published:2015-11-19 summary:Diffusion Maps framework is a kernel based method for manifold learning and data analysis that defines diffusion similarities by imposing a Markovian process on the given dataset. Analysis by this process uncovers the intrinsic geometric structures in the data. Recently, it was suggested to replace the standard kernel by a measure-based kernel that incorporates information about the density of the data. Thus, the manifold assumption is replaced by a more general measure-based assumption. The measure-based diffusion kernel incorporates two separate independent representations. The first determines a measure that correlates with a density that represents normal behaviors and patterns in the data. The second consists of the analyzed multidimensional data points. In this paper, we present a representation framework for data analysis of datasets that is based on a closed-form decomposition of the measure-based kernel. The proposed representation preserves pairwise diffusion distances that does not depend on the data size while being invariant to scale. For a stationary data, no out-of-sample extension is needed for embedding newly arrived data points in the representation space. Several aspects of the presented methodology are demonstrated on analytically generated data. version:1
arxiv-1511-06201 | Adjustable Bounded Rectifiers: Towards Deep Binary Representations | http://arxiv.org/abs/1511.06201 | id:1511.06201 author:Zhirong Wu, Dahua Lin, Xiaoou Tang category:cs.LG stat.ML  published:2015-11-19 summary:Binary representation is desirable for its memory efficiency, computation speed and robustness. In this paper, we propose adjustable bounded rectifiers to learn binary representations for deep neural networks. While hard constraining representations across layers to be binary makes training unreasonably difficult, we softly encourage activations to diverge from real values to binary by approximating step functions. Our final representation is completely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012 dataset, and systematically study the training dynamics of the binarization process. Our approach can binarize the last layer representation without loss of performance and binarize all the layers with reasonably small degradations. The memory space that it saves may allow more sophisticated models to be deployed, thus compensating the loss. To the best of our knowledge, this is the first work to report results on current deep network architectures using complete binary middle representations. Given the learned representations, we find that the firing or inhibition of a binary neuron is usually associated with a meaningful interpretation across different classes. This suggests that the semantic structure of a neural network may be manifested through a guided binarization process. version:1
arxiv-1511-06198 | Spherical Cap Packing Asymptotics and Rank-Extreme Detection | http://arxiv.org/abs/1511.06198 | id:1511.06198 author:Kai Zhang category:math.ST cs.IT math.IT physics.data-an stat.ME stat.ML stat.TH  published:2015-11-19 summary:We study the spherical cap packing problem with a probabilistic approach. Such probabilistic considerations result in an asymptotic sharp universal uniform bound on the maximal inner product between any set of unit vectors and a stochastically independent uniformly distributed unit vector. When the set of unit vectors are themselves independently uniformly distributed, we further develop the extreme value distribution limit of the maximal inner product, which characterizes its uncertainty around the bound. As applications of the above asymptotic results, we derive (1) an asymptotic sharp universal uniform bound on the maximal spurious correlation, as well as its uniform convergence in distribution when the explanatory variables are independently Gaussian distributed; and (2) an asymptotic sharp universal bound on the maximum norm of a low-rank elliptically distributed vector, as well as related limiting distributions. With these results, we develop a fast detection method for a low-rank structure in high-dimensional Gaussian data without using the spectrum information. version:1
arxiv-1312-1277 | Bandits and Experts in Metric Spaces | http://arxiv.org/abs/1312.1277 | id:1312.1277 author:Robert Kleinberg, Aleksandrs Slivkins, Eli Upfal category:cs.DS cs.LG  published:2013-12-04 summary:In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the "Lipschitz MAB problem". We present a solution for the multi-armed bandit problem in this setting. That is, for every metric space we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for this metric space, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions. We also address the full-feedback ("best expert") version of the problem, where after every round the payoffs from all arms are revealed. version:2
arxiv-1409-7794 | Large-scale Online Feature Selection for Ultra-high Dimensional Sparse Data | http://arxiv.org/abs/1409.7794 | id:1409.7794 author:Yue Wu, Steven C. H. Hoi, Tao Mei, Nenghai Yu category:cs.LG cs.CV  published:2014-09-27 summary:Feature selection with large-scale high-dimensional data is important yet very challenging in machine learning and data mining. Online feature selection is a promising new paradigm that is more efficient and scalable than batch feature section methods, but the existing online approaches usually fall short in their inferior efficacy as compared with batch approaches. In this paper, we present a novel second-order online feature selection scheme that is simple yet effective, very fast and extremely scalable to deal with large-scale ultra-high dimensional sparse data streams. The basic idea is to improve the existing first-order online feature selection methods by exploiting second-order information for choosing the subset of important features with high confidence weights. However, unlike many second-order learning methods that often suffer from extra high computational cost, we devise a novel smart algorithm for second-order online feature selection using a MaxHeap-based approach, which is not only more effective than the existing first-order approaches, but also significantly more efficient and scalable for large-scale feature selection with ultra-high dimensional sparse data, as validated from our extensive experiments. Impressively, on a billion-scale synthetic dataset (1-billion dimensions, 1-billion nonzero features, and 1-million samples), our new algorithm took only 8 minutes on a single PC, which is orders of magnitudes faster than traditional batch approaches. \url{http://arxiv.org/abs/1409.7794} version:3
arxiv-1511-05788 | From Pose to Activity: Surveying Datasets and Introducing CONVERSE | http://arxiv.org/abs/1511.05788 | id:1511.05788 author:Michael Edwards, Jingjing Deng, Xianghua Xie category:cs.CV  published:2015-11-18 summary:We present a review on the current state of publicly available datasets within the human action recognition community; highlighting the revival of pose based methods and recent progress of understanding person-person interaction modeling. We categorize datasets regarding several key properties for usage as a benchmark dataset; including the number of class labels, ground truths provided, and application domain they occupy. We also consider the level of abstraction of each dataset; grouping those that present actions, interactions and higher level semantic activities. The survey identifies key appearance and pose based datasets, noting a tendency for simplistic, emphasized, or scripted action classes that are often readily definable by a stable collection of sub-action gestures. There is a clear lack of datasets that provide closely related actions, those that are not implicitly identified via a series of poses and gestures, but rather a dynamic set of interactions. We therefore propose a novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors. The intention is to provide events that are constructed from numerous primitive actions, interactions and motions, over a period of time; providing a set of subtle action classes that are more representative of the real world, and a challenge to currently developed recognition methodologies. We believe this is among one of the first datasets devoted to conversational interaction classification using 3D pose features and the attributed papers show this task is indeed possible. The full dataset is made publicly available to the research community at www.csvision.swansea.ac.uk/converse. version:2
arxiv-1511-06147 | Coreset-Based Adaptive Tracking | http://arxiv.org/abs/1511.06147 | id:1511.06147 author:Abhimanyu Dubey, Nikhil Naik, Dan Raviv, Rahul Sukthankar, Ramesh Raskar category:cs.CV cs.LG  published:2015-11-19 summary:We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a 'coreset' representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data. version:1
arxiv-1501-06297 | Geodesic convolutional neural networks on Riemannian manifolds | http://arxiv.org/abs/1501.06297 | id:1501.06297 author:Jonathan Masci, Davide Boscaini, Michael M. Bronstein, Pierre Vandergheynst category:cs.CV  published:2015-01-26 summary:Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract "patches", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence. version:2
arxiv-1511-06120 | The Kernel Two-Sample Test for Brain Networks | http://arxiv.org/abs/1511.06120 | id:1511.06120 author:Emanuele Olivetti, Sandro Vega-Pons, Paolo Avesani category:stat.ML  published:2015-11-19 summary:In clinical and neuroscientific studies, systematic differences between two populations of brain networks are investigated in order to characterize mental diseases or processes. Those networks are usually represented as graphs built from neuroimaging data and studied by means of graph analysis methods. The typical machine learning approach to study these brain graphs creates a classifier and tests its ability to discriminate the two populations. In contrast to this approach, in this work we propose to directly test whether two populations of graphs are different or not, by using the kernel two-sample test (KTST), without creating the intermediate classifier. We claim that, in general, the two approaches provides similar results and that the KTST requires much less computation. Additionally, in the regime of low sample size, we claim that the KTST has lower frequency of Type II error than the classification approach. Besides providing algorithmic considerations to support these claims, we show strong evidence through experiments and one simulation. version:1
arxiv-1511-06072 | Mediated Experts for Deep Convolutional Networks | http://arxiv.org/abs/1511.06072 | id:1511.06072 author:Sebastian Agethen, Winston H. Hsu category:cs.LG cs.NE  published:2015-11-19 summary:We present a new supervised architecture termed Mediated Mixture-of-Experts (MMoE) that allows us to improve classification accuracy of Deep Convolutional Networks (DCN). Our architecture achieves this with the help of expert networks: A network is trained on a disjoint subset of a given dataset and then run in parallel to other experts during deployment. A mediator is employed if experts contradict each other. This allows our framework to naturally support incremental learning, as adding new classes requires (re-)training of the new expert only. We also propose two measures to control computational complexity: An early-stopping mechanism halts experts that have low confidence in their prediction. The system allows to trade-off accuracy and complexity without further retraining. We also suggest to share low-level convolutional layers between experts in an effort to avoid computation of a near-duplicate feature set. We evaluate our system on a popular dataset and report improved accuracy compared to a single model of same configuration. version:1
arxiv-1511-06070 | Structured Depth Prediction in Challenging Monocular Video Sequences | http://arxiv.org/abs/1511.06070 | id:1511.06070 author:Miaomiao Liu, Mathieu Salzmann, Xuming He category:cs.CV  published:2015-11-19 summary:In this paper, we tackle the problem of estimating the depth of a scene from a monocular video sequence. In particular, we handle challenging scenarios, such as non-translational camera motion and dynamic scenes, where traditional structure from motion and motion stereo methods do not apply. To this end, we first study the problem of depth estimation from a single image. In this context, we exploit the availability of a pool of images for which the depth is known, and formulate monocular depth estimation as a discrete-continuous optimization problem, where the continuous variables encode the depth of the superpixels in the input image, and the discrete ones represent relationships between neighboring superpixels. The solution to this discrete-continuous optimization problem is obtained by performing inference in a graphical model using particle belief propagation. To handle video sequences, we then extend our single image model to a two-frame one that naturally encodes short-range temporal consistency and inherently handles dynamic objects. Based on the prediction of this model, we then introduce a fully-connected pairwise CRF that accounts for longer range spatio-temporal interactions throughout a video. We demonstrate the effectiveness of our model in both the indoor and outdoor scenarios. version:1
arxiv-1511-05680 | Wishart Mechanism for Differentially Private Principal Components Analysis | http://arxiv.org/abs/1511.05680 | id:1511.05680 author:Wuxuan Jiang, Cong Xie, Zhihua Zhang category:cs.CR cs.DS stat.ML  published:2015-11-18 summary:We propose a new input perturbation mechanism for publishing a covariance matrix to achieve $(\epsilon,0)$-differential privacy. Our mechanism uses a Wishart distribution to generate matrix noise. In particular, We apply this mechanism to principal component analysis. Our mechanism is able to keep the positive semi-definiteness of the published covariance matrix. Thus, our approach gives rise to a general publishing framework for input perturbation of a symmetric positive semidefinite matrix. Moreover, compared with the classic Laplace mechanism, our method has better utility guarantee. To the best of our knowledge, Wishart mechanism is the best input perturbation approach for $(\epsilon,0)$-differentially private PCA. We also compare our work with previous exponential mechanism algorithms in the literature and provide near optimal bound while having more flexibility and less computational intractability. version:2
arxiv-1511-06066 | Transfer Learning for Speech and Language Processing | http://arxiv.org/abs/1511.06066 | id:1511.06066 author:Dong Wang, Thomas Fang Zheng category:cs.CL cs.LG  published:2015-11-19 summary:Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field. version:1
arxiv-1402-1526 | Dual Query: Practical Private Query Release for High Dimensional Data | http://arxiv.org/abs/1402.1526 | id:1402.1526 author:Marco Gaboardi, Emilio Jesús Gallego Arias, Justin Hsu, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.CR cs.DB cs.LG  published:2014-02-06 summary:We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude. version:2
arxiv-1511-06049 | What Objective Does Self-paced Learning Indeed Optimize? | http://arxiv.org/abs/1511.06049 | id:1511.06049 author:Deyu Meng, Qian Zhao category:cs.LG cs.CV  published:2015-11-19 summary:Self-paced learning (SPL) has been attracting increasing attention in machine learning and computer vision. Albeit empirically substantiated to be effective, the investigation on its theoretical insight is still a blank. It is even unknown that what objective a general SPL regime converges to. To this issue, this study attempts to initially provide some new insights under this "heuristic" learning scheme. Specifically, we prove that the solving strategy on SPL exactly accords with a majorization minimization algorithm, a well known technique in optimization and machine learning, implemented on a latent objective. A more interesting finding is that, the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty, an attractive topic in statistics and machine learning. In particular, we show that the previous hard and linear self-paced regularizers are equivalent to the capped norm and minimax concave plus penalties, respectively, both being widely investigated in statistics. Such connections between SPL and previous known researches enhance new insightful comprehension on SPL, like convergence and parameter setting rationality. The correctness of the proposed theory is substantiated by experimental results on synthetic and UCI data sets. version:1
arxiv-1511-06036 | Stochastic gradient method with accelerated stochastic dynamics | http://arxiv.org/abs/1511.06036 | id:1511.06036 author:Masayuki Ohzeki category:stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.CV  published:2015-11-19 summary:In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance. version:1
arxiv-1507-02620 | Deep filter banks for texture recognition, description, and segmentation | http://arxiv.org/abs/1507.02620 | id:1507.02620 author:Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Andrea Vedaldi category:cs.CV  published:2015-07-09 summary:Visual textures have played a key role in image understanding because they convey important semantics of images, and because texture representations that pool local image descriptors in an orderless manner have had a tremendous impact in diverse applications. In this paper we make several contributions to texture understanding. First, instead of focusing on texture instance and material category recognition, we propose a human-interpretable vocabulary of texture attributes to describe common texture patterns, complemented by a new describable texture dataset for benchmarking. Second, we look at the problem of recognizing materials and texture attributes in realistic imaging conditions, including when textures appear in clutter, developing corresponding benchmarks on top of the recently proposed OpenSurfaces dataset. Third, we revisit classic texture representations, including bag-of-visual-words and the Fisher vectors, in the context of deep learning and show that these have excellent efficiency and generalization properties if the convolutional layers of a deep model are used as filter banks. We obtain in this manner state-of-the-art performance in numerous datasets well beyond textures, an efficient method to apply deep features to image regions, as well as benefit in transferring features from one domain to another. version:2
arxiv-1511-06015 | Active Object Localization with Deep Reinforcement Learning | http://arxiv.org/abs/1511.06015 | id:1511.06015 author:Juan C. Caicedo, Svetlana Lazebnik category:cs.CV  published:2015-11-18 summary:We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization. version:1
arxiv-1511-06004 | Studying the control of non invasive prosthetic hands over large time spans | http://arxiv.org/abs/1511.06004 | id:1511.06004 author:Mara Graziani category:cs.LG cs.HC  published:2015-11-18 summary:The electromyography (EMG) signal is the electrical manifestation of a neuromuscular activation that provides access to physiological processes which cause the muscle to generate force and produce movement. Non invasive prostheses use such signals detected by the electrodes placed on the user's stump, as input to generate hand posture movements according to the intentions of the prosthesis wearer. The aim of this pilot study is to explore the repeatability issue, i.e. the ability to classify 17 different hand postures, represented by EMG signal, across a time span of days by a control algorithm. Data collection experiments lasted four days and signals were collected from the forearm of a single subject. We find that Support Vector Machine (SVM) classification results are high enough to guarantee a correct classification of more than 10 postures in each moment of the considered time span. version:1
arxiv-1503-03585 | Deep Unsupervised Learning using Nonequilibrium Thermodynamics | http://arxiv.org/abs/1503.03585 | id:1503.03585 author:Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli category:cs.LG cond-mat.dis-nn q-bio.NC stat.ML  published:2015-03-12 summary:A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm. version:8
arxiv-1511-05943 | Unitary-Group Invariant Kernels and Features from Transformed Unlabeled Data | http://arxiv.org/abs/1511.05943 | id:1511.05943 author:Dipan K. Pal, Marios Savvides category:cs.LG  published:2015-11-18 summary:The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as (1) unavailability of transformed versions of labelled data and (2) not observing all transformations. We present a theoretically motivated alternate approach to the invariant kernel SVM. Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned. We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets to illustrate and validate our methods. version:1
arxiv-1511-05932 | On the Global Linear Convergence of Frank-Wolfe Optimization Variants | http://arxiv.org/abs/1511.05932 | id:1511.05932 author:Simon Lacoste-Julien, Martin Jaggi category:math.OC cs.LG stat.ML 90C52  90C90  68T05 G.1.6; I.2.6  published:2015-11-18 summary:The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization. version:1
arxiv-1511-05926 | Combining Neural Networks and Log-linear Models to Improve Relation Extraction | http://arxiv.org/abs/1511.05926 | id:1511.05926 author:Thien Huu Nguyen, Ralph Grishman category:cs.CL cs.LG  published:2015-11-18 summary:The last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive k-grams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset. version:1
arxiv-1511-05914 | Collecting and Annotating the Large Continuous Action Dataset | http://arxiv.org/abs/1511.05914 | id:1511.05914 author:Daniel Paul Barrett, Ran Xu, Haonan Yu, Jeffrey Mark Siskind category:cs.CV  published:2015-11-18 summary:We make available to the community a new dataset to support action-recognition research. This dataset is different from prior datasets in several key ways. It is significantly larger. It contains streaming video with long segments containing multiple action occurrences that often overlap in space and/or time. All actions were filmed in the same collection of backgrounds so that background gives little clue as to action class. We had five humans replicate the annotation of temporal extent of action occurrences labeled with their class and measured a surprisingly low level of intercoder agreement. A baseline experiment shows that recent state-of-the-art methods perform poorly on this dataset. This suggests that this will be a challenging dataset to foster advances in action-recognition research. This manuscript serves to describe the novel content and characteristics of the LCA dataset, present the design decisions made when filming the dataset, and document the novel methods employed to annotate the dataset. version:1
arxiv-1511-05904 | Dense Human Body Correspondences Using Convolutional Networks | http://arxiv.org/abs/1511.05904 | id:1511.05904 author:Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, Hao Li category:cs.CV cs.GR  published:2015-11-18 summary:We propose a deep learning approach for finding dense correspondences between 3D scans of people. Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time. We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries. This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans. We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate than those found using methods that require full watertight 3D geometry. version:1
arxiv-1510-04709 | Multilingual Image Description with Neural Sequence Models | http://arxiv.org/abs/1510.04709 | id:1510.04709 author:Desmond Elliott, Stella Frank, Eva Hasler category:cs.CL cs.CV cs.LG cs.NE  published:2015-10-15 summary:In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline. version:2
arxiv-1505-02921 | How Far Can You Get By Combining Change Detection Algorithms? | http://arxiv.org/abs/1505.02921 | id:1505.02921 author:Simone Bianco, Gianluigi Ciocca, Raimondo Schettini category:cs.CV I.4.8; G.1.6  published:2015-05-12 summary:In this paper we investigate how state-of-the-art change detection algorithms can be combined and used to create a more robust change algorithm leveraging their individual peculiarities. We exploited Genetic Programming (GP) to automatically select the best algorithms, combine them in different ways, and perform the most suitable post-processing operations on the outputs of the algorithms. In particular, algorithms' combination and post-processing operations are achieved with unary, binary and $n$-ary functions embedded into the GP framework. Using different experimental settings for combining existing algorithms we obtained different GP solutions that we termed IUTIS (In Unity There Is Strength). These solutions are then compared against state-of-the-art change detection algorithms on the video sequences and ground truth annotations of the ChandeDetection.net (CDNET 2014) challenge. Results demonstrate that using GP, our solutions are able to outperform all the considered single state-of-the-art change detection algorithms, as well as other combination strategies. version:2
arxiv-1511-05846 | Eigenspectra optoacoustic tomography achieves quantitative blood oxygenation imaging deep in tissues | http://arxiv.org/abs/1511.05846 | id:1511.05846 author:Stratis Tzoumas, Antonio Nunes, Ivan Olefir, Stefan Stangl, Panagiotis Symvoulidis, Sarah Glasl, Christine Bayer, Gabriele Multhoff, Vasilis Ntziachristos category:physics.med-ph cs.CV physics.optics q-bio.QM  published:2015-11-18 summary:Light propagating in tissue attains a spectrum that varies with location due to wavelength-dependent fluence attenuation by tissue optical properties, an effect that causes spectral corruption. Predictions of the spectral variations of light fluence in tissue are challenging since the spatial distribution of optical properties in tissue cannot be resolved in high resolution or with high accuracy by current methods. Spectral corruption has fundamentally limited the quantification accuracy of optical and optoacoustic methods and impeded the long sought-after goal of imaging blood oxygen saturation (sO2) deep in tissues; a critical but still unattainable target for the assessment of oxygenation in physiological processes and disease. We discover a new principle underlying light fluence in tissues, which describes the wavelength dependence of light fluence as an affine function of a few reference base spectra, independently of the specific distribution of tissue optical properties. This finding enables the introduction of a previously undocumented concept termed eigenspectra Multispectral Optoacoustic Tomography (eMSOT) that can effectively account for wavelength dependent light attenuation without explicit knowledge of the tissue optical properties. We validate eMSOT in more than 2000 simulations and with phantom and animal measurements. We find that eMSOT can quantitatively image tissue sO2 reaching in many occasions a better than 10-fold improved accuracy over conventional spectral optoacoustic methods. Then, we show that eMSOT can spatially resolve sO2 in muscle and tumor; revealing so far unattainable tissue physiology patterns. Last, we related eMSOT readings to cancer hypoxia and found congruence between eMSOT tumor sO2 images and tissue perfusion and hypoxia maps obtained by correlative histological analysis. version:1
arxiv-1511-05837 | Using Machine Learning to Predict the Outcome of English County twenty over Cricket Matches | http://arxiv.org/abs/1511.05837 | id:1511.05837 author:Stylianos Kampakis, William Thomas category:stat.ML stat.AP  published:2015-11-18 summary:Cricket betting is a multi-billion dollar market. Therefore, there is a strong incentive for models that can predict the outcomes of games and beat the odds provided by bookers. The aim of this study was to investigate to what degree it is possible to predict the outcome of cricket matches. The target competition was the English twenty over county cricket cup. The original features alongside engineered features gave rise to more than 500 team and player statistics. The models were optimized firstly with team features only and then both team and player features. The performance of the models was tested over individual seasons from 2009 to 2014 having been trained over previous season data in each case. The optimal model was a simple prediction method combined with complex hierarchical features and was shown to significantly outperform a gambling industry benchmark. version:1
arxiv-1511-09392 | Enhancements in statistical spoken language translation by de-normalization of ASR results | http://arxiv.org/abs/1511.09392 | id:1511.09392 author:Agnieszka Wołk, Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML  published:2015-11-18 summary:Spoken language translation (SLT) has become very important in an increasingly globalized world. Machine translation (MT) for automatic speech recognition (ASR) systems is a major challenge of great interest. This research investigates that automatic sentence segmentation of speech that is important for enriching speech recognition output and for aiding downstream language processing. This article focuses on the automatic sentence segmentation of speech and improving MT results. We explore the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems in the Polish language. We also experiment with reverse normalization of the recognized speech samples. version:1
arxiv-1511-06285 | Harvesting comparable corpora and mining them for equivalent bilingual sentences using statistical classification and analogy- based heuristics | http://arxiv.org/abs/1511.06285 | id:1511.06285 author:Krzysztof Wołk, Emilia Rejmund, Krzysztof Marasek category:cs.CL stat.ML  published:2015-11-18 summary:Parallel sentences are a relatively scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation. This research explores our new methodologies for mining such data from previously obtained comparable corpora. The task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora, but parallel sentences are a much more useful resource. Here we propose a web crawling method for building subject-aligned comparable corpora from e.g. Wikipedia dumps and Euronews web page. The improvements in machine translation are shown on Polish-English language pair for various text domains. We also tested another method of building parallel corpora based on comparable corpora data. It lets automatically broad existing corpus of sentences from subject of corpora based on analogies between them. version:1
arxiv-1501-06727 | Factorization, Inference and Parameter Learning in Discrete AMP Chain Graphs | http://arxiv.org/abs/1501.06727 | id:1501.06727 author:Jose M. Peña category:stat.ML cs.AI  published:2015-01-27 summary:We address some computational issues that may hinder the use of AMP chain graphs in practice. Specifically, we show how a discrete probability distribution that satisfies all the independencies represented by an AMP chain graph factorizes according to it. We show how this factorization makes it possible to perform inference and parameter learning efficiently, by adapting existing algorithms for Markov and Bayesian networks. Finally, we turn our attention to another issue that may hinder the use of AMP CGs, namely the lack of an intuitive interpretation of their edges. We provide one such interpretation. version:2
arxiv-1511-05768 | Labeled pupils in the wild: A dataset for studying pupil detection in unconstrained environments | http://arxiv.org/abs/1511.05768 | id:1511.05768 author:Marc Tonsen, Xucong Zhang, Yusuke Sugano, Andreas Bulling category:cs.CV  published:2015-11-18 summary:We present labelled pupils in the wild (LPW), a novel dataset of 66 high-quality, high-speed eye region videos for the development and evaluation of pupil detection algorithms. The videos in our dataset were recorded from 22 participants in everyday locations at about 95 FPS using a state-of-the-art dark-pupil head-mounted eye tracker. They cover people with different ethnicities, a diverse set of everyday indoor and outdoor illumination environments, as well as natural gaze direction distributions. The dataset also includes participants wearing glasses, contact lenses, as well as make-up. We benchmark five state-of-the-art pupil detection algorithms on our dataset with respect to robustness and accuracy. We further study the influence of image resolution, vision aids, as well as recording location (indoor, outdoor) on pupil detection performance. Our evaluations provide valuable insights into the general pupil detection problem and allow us to identify key challenges for robust pupil detection on head-mounted eye trackers. version:1
arxiv-1511-05756 | Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction | http://arxiv.org/abs/1511.05756 | id:1511.05756 author:Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han category:cs.CV cs.CL cs.LG  published:2015-11-18 summary:We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks. version:1
arxiv-1506-02494 | backShift: Learning causal cyclic graphs from unknown shift interventions | http://arxiv.org/abs/1506.02494 | id:1506.02494 author:Dominik Rothenhäusler, Christina Heinze, Jonas Peters, Nicolai Meinshausen category:stat.ME stat.ML  published:2015-06-08 summary:We propose a simple method to learn linear causal cyclic models in the presence of latent variables. The method relies on equilibrium data of the model recorded under a specific kind of interventions ("shift interventions"). The location and strength of these interventions do not have to be known and can be estimated from the data. Our method, called backShift, only uses second moments of the data and performs simple joint matrix diagonalization, applied to differences between covariance matrices. We give a sufficient and necessary condition for identifiability of the system, which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings, one of which can be pure observational data. We demonstrate the performance on some simulated data and applications in flow cytometry and financial time series. The code is made available as R-package backShift. version:3
arxiv-1511-05743 | Sparse learning of maximum likelihood model for optimization of complex loss function | http://arxiv.org/abs/1511.05743 | id:1511.05743 author:Ning Zhang, Prathamesh Chandrasekar category:cs.LG  published:2015-11-18 summary:Traditional machine learning methods usually minimize a simple loss function to learn a predictive model, and then use a complex performance measure to measure the prediction performance. However, minimizing a simple loss function cannot guarantee that an optimal performance. In this paper, we study the problem of optimizing the complex performance measure directly to obtain a predictive model. We proposed to construct a maximum likelihood model for this problem, and to learn the model parameter, we minimize a com- plex loss function corresponding to the desired complex performance measure. To optimize the loss function, we approximate the upper bound of the complex loss. We also propose impose the sparsity to the model parameter to obtain a sparse model. An objective is constructed by combining the upper bound of the loss function and the sparsity of the model parameter, and we develop an iterative algorithm to minimize it by using the fast iterative shrinkage- thresholding algorithm framework. The experiments on optimization on three different complex performance measures, including F-score, receiver operating characteristic curve, and recall precision curve break even point, over three real-world applications, aircraft event recognition of civil aviation safety, in- trusion detection in wireless mesh networks, and image classification, show the advantages of the proposed method over state-of-the-art methods. version:1
arxiv-1511-05741 | A Random Forest Guided Tour | http://arxiv.org/abs/1511.05741 | id:1511.05741 author:Gérard Biau, Erwan Scornet category:math.ST stat.ML stat.TH  published:2015-11-18 summary:The random forest algorithm, proposed by L. Breiman in 2001, has been extremely successful as a general-purpose classification and regression method. The approach, which combines several randomized decision trees and aggregates their predictions by averaging, has shown excellent performance in settings where the number of variables is much larger than the number of observations. Moreover, it is versatile enough to be applied to large-scale problems, is easily adapted to various ad-hoc learning tasks, and returns measures of variable importance. The present article reviews the most recent theoretical and methodological developments for random forests. Emphasis is placed on the mathematical forces driving the algorithm, with special attention given to the selection of parameters, the resampling mechanism, and variable importance measures. This review is intended to provide non-experts easy access to the main ideas. version:1
arxiv-1506-04132 | Stochastic Expectation Propagation | http://arxiv.org/abs/1506.04132 | id:1506.04132 author:Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner category:stat.ML cs.LG  published:2015-06-12 summary:Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of $N$. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting. version:2
arxiv-1511-05720 | Online learning in repeated auctions | http://arxiv.org/abs/1511.05720 | id:1511.05720 author:Jonathan Weed, Vianney Perchet, Philippe Rigollet category:cs.GT cs.LG stat.ML  published:2015-11-18 summary:Motivated by online advertising auctions, we consider repeated Vickrey auctions where goods of unknown value are sold sequentially and bidders only learn (potentially noisy) information about a good's value once it is purchased. We adopt an online learning approach with bandit feedback to model this problem and derive bidding strategies for two models: stochastic and adversarial. In the stochastic model, the observed values of the goods are random variables centered around the true value of the good. In this case, logarithmic regret is achievable when competing against well behaved adversaries. In the adversarial model, the goods need not be identical and we simply compare our performance against that of the best fixed bid in hindsight. We show that sublinear regret is also achievable in this case and prove matching minimax lower bounds. To our knowledge, this is the first complete set of strategies for bidders participating in auctions of this type. version:1
arxiv-1506-03736 | GAP Safe screening rules for sparse multi-task and multi-class models | http://arxiv.org/abs/1506.03736 | id:1506.03736 author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO  published:2015-06-11 summary:High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be \emph{safe}. In this paper we derive new safe rules for generalized linear models regularized with $\ell_1$ and $\ell_1/\ell_2$ norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules. version:2
arxiv-1511-05710 | Complex-Valued Gaussian Processes for Regression: A Widely Non-Linear Approach | http://arxiv.org/abs/1511.05710 | id:1511.05710 author:Rafael Boloix-Tortosa, Eva Arias-de-Reyna, F. Javier Payan-Somet, Juan J. Murillo-Fuentes category:cs.LG  published:2015-11-18 summary:In this paper we propose a novel Bayesian kernel based solution for regression in complex fields. We develop the formulation of the Gaussian process for regression (GPR) to deal with complex-valued outputs. Previous solutions for kernels methods usually assume a complexification approach, where the real-valued kernel is replaced by a complex-valued one. However, based on the results in complex-valued linear theory, we prove that both a kernel and a pseudo-kernel are to be included in the solution. This is the starting point to develop the new formulation for the complex-valued GPR. The obtained formulation resembles the one of the widely linear minimum mean-squared (WLMMSE) approach. Just in the particular case where the outputs are proper, the pseudo-kernel cancels and the solution simplifies to a real-valued GPR structure, as the WLMMSE does into a strictly linear solution. We include some numerical experiments to show that the novel solution, denoted as widely non-linear complex GPR (WCGPR), outperforms a strictly complex GPR where a pseudo-kernel is not included. version:1
arxiv-1511-05706 | Efficient Output Kernel Learning for Multiple Tasks | http://arxiv.org/abs/1511.05706 | id:1511.05706 author:Pratik Jawanpuria, Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.LG  published:2015-11-18 summary:The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step. \mbox{Using} the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance. version:1
arxiv-1511-01245 | Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset | http://arxiv.org/abs/1511.01245 | id:1511.01245 author:Thierry Bouwmans, Andrews Sobral, Sajid Javed, Soon Ki Jung, El-Hadi Zahzah category:cs.CV  published:2015-11-04 summary:Recent research on problem formulations based on decomposition into low-rank plus sparse matrices shows a suitable framework to separate moving objects from the background. The most representative problem formulation is the Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit (PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix. However, similar robust implicit or explicit decompositions can be made in the following problem formulations: Robust Non-negative Matrix Factorization (RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), Robust Subspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goal of these similar problem formulations is to obtain explicitly or implicitly a decomposition into low-rank matrix plus additive matrices. In this context, this work aims to initiate a rigorous and comprehensive review of the similar problem formulations in robust subspace learning and tracking based on decomposition into low-rank plus additive matrices for testing and ranking existing algorithms for background/foreground separation. For this, we first provide a preliminary review of the recent developments in the different problem formulations which allows us to define a unified view that we called Decomposition into Low-rank plus Additive Matrices (DLAM). Then, we examine carefully each method in each robust subspace learning/tracking frameworks with their decomposition, their loss functions, their optimization problem and their solvers. Furthermore, we investigate if incremental algorithms and real-time implementations can be achieved for background/foreground separation. Finally, experimental results on a large-scale dataset called Background Models Challenge (BMC 2012) show the comparative performance of 32 different robust subspace learning/tracking methods. version:2
arxiv-1511-04813 | Budget Online Multiple Kernel Learning | http://arxiv.org/abs/1511.04813 | id:1511.04813 author:Jing Lu, Steven C. H. Hoi, Doyen Sahoo, Peilin Zhao category:cs.LG  published:2015-11-16 summary:Online learning with multiple kernels has gained increasing interests in recent years and found many applications. For classification tasks, Online Multiple Kernel Classification (OMKC), which learns a kernel based classifier by seeking the optimal linear combination of a pool of single kernel classifiers in an online fashion, achieves superior accuracy and enjoys great flexibility compared with traditional single-kernel classifiers. Despite being studied extensively, existing OMKC algorithms suffer from high computational cost due to their unbounded numbers of support vectors. To overcome this drawback, we present a novel framework of Budget Online Multiple Kernel Learning (BOMKL) and propose a new Sparse Passive Aggressive learning to perform effective budget online learning. Specifically, we adopt a simple yet effective Bernoulli sampling to decide if an incoming instance should be added to the current set of support vectors. By limiting the number of support vectors, our method can significantly accelerate OMKC while maintaining satisfactory accuracy that is comparable to that of the existing OMKC algorithms. We theoretically prove that our new method achieves an optimal regret bound in expectation, and empirically found that the proposed algorithm outperforms various OMKC algorithms and can easily scale up to large-scale datasets. version:2
arxiv-1511-05676 | Compositional Memory for Visual Question Answering | http://arxiv.org/abs/1511.05676 | id:1511.05676 author:Aiwen Jiang, Fang Wang, Fatih Porikli, Yi Li category:cs.CV  published:2015-11-18 summary:Visual Question Answering (VQA) emerges as one of the most fascinating topics in computer vision recently. Many state of the art methods naively use holistic visual features with language features into a Long Short-Term Memory (LSTM) module, neglecting the sophisticated interaction between them. This coarse modeling also blocks the possibilities of exploring finer-grained local features that contribute to the question answering dynamically over time. This paper addresses this fundamental problem by directly modeling the temporal dynamics between language and all possible local image patches. When traversing the question words sequentially, our end-to-end approach explicitly fuses the features associated to the words and the ones available at multiple local patches in an attention mechanism, and further combines the fused information to generate dynamic messages, which we call episode. We then feed the episodes to a standard question answering module together with the contextual visual information and linguistic information. Motivated by recent practices in deep learning, we use auxiliary loss functions during training to improve the performance. Our experiments on two latest public datasets suggest that our method has a superior performance. Notably, on the DARQUAR dataset we advanced the state of the art by 6$\%$, and we also evaluated our approach on the most recent MSCOCO-VQA dataset. version:1
arxiv-1510-08520 | Learning with $\ell^{0}$-Graph: $\ell^{0}$-Induced Sparse Subspace Clustering | http://arxiv.org/abs/1510.08520 | id:1510.08520 author:Yingzhen Yang, Jiashi Feng, Jianchao Yang, Thomas S. Huang category:cs.LG cs.CV  published:2015-10-28 summary:Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC) \cite{ElhamifarV13} and $\ell^{1}$-graph \cite{YanW09,ChengYYFH10}, are effective in partitioning the data that lie in a union of subspaces. Most of those methods use $\ell^{1}$-norm or $\ell^{2}$-norm with thresholding to impose the sparsity of the constructed sparse similarity graph, and certain assumptions, e.g. independence or disjointness, on the subspaces are required to obtain the subspace-sparse representation, which is the key to their success. Such assumptions are not guaranteed to hold in practice and they limit the application of sparse subspace clustering on subspaces with general location. In this paper, we propose a new sparse subspace clustering method named $\ell^{0}$-graph. In contrast to the required assumptions on subspaces for most existing sparse subspace clustering methods, it is proved that subspace-sparse representation can be obtained by $\ell^{0}$-graph for arbitrary distinct underlying subspaces almost surely under the mild i.i.d. assumption on the data generation. We develop a proximal method to obtain the sub-optimal solution to the optimization problem of $\ell^{0}$-graph with proved guarantee of convergence. Moreover, we propose a regularized $\ell^{0}$-graph that encourages nearby data to have similar neighbors so that the similarity graph is more aligned within each cluster and the graph connectivity issue is alleviated. Extensive experimental results on various data sets demonstrate the superiority of $\ell^{0}$-graph compared to other competing clustering methods, as well as the effectiveness of regularized $\ell^{0}$-graph. version:2
arxiv-1511-04517 | Reversible Recursive Instance-level Object Segmentation | http://arxiv.org/abs/1511.04517 | id:1511.04517 author:Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Zequn Jie, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV  published:2015-11-14 summary:In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two sub-networks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the $\text{AP}^r$ over $20$ classes at $0.5$ IoU achieves $66.7\%$, which significantly outperforms the results of $58.7\%$ by PFN~\cite{PFN} and $46.3\%$ by~\cite{liu2015multi}. version:2
arxiv-1505-06798 | Accelerating Very Deep Convolutional Networks for Classification and Detection | http://arxiv.org/abs/1505.06798 | id:1505.06798 author:Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun category:cs.CV cs.LG cs.NE  published:2015-05-26 summary:This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., >=10) layers are approximated. For the widely used very deep VGG-16 model, our method achieves a whole-model speedup of 4x with merely a 0.3% increase of top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector. version:2
arxiv-1511-05660 | Bayesian hypothesis testing for one bit compressed sensing with sensing matrix perturbation | http://arxiv.org/abs/1511.05660 | id:1511.05660 author:H. Zayyani, M. Korki, F. Marvasti category:stat.ML cs.IT math.IT  published:2015-11-18 summary:This letter proposes a low-computational Bayesian algorithm for noisy sparse recovery in the context of one bit compressed sensing with sensing matrix perturbation. The proposed algorithm which is called BHT-MLE comprises a sparse support detector and an amplitude estimator. The support detector utilizes Bayesian hypothesis test, while the amplitude estimator uses an ML estimator which is obtained by solving a convex optimization problem. Simulation results show that BHT-MLE algorithm offers more reconstruction accuracy than that of an ML estimator (MLE) at a low computational cost. version:1
arxiv-1504-01033 | Watch and Learn: Optimizing from Revealed Preferences Feedback | http://arxiv.org/abs/1504.01033 | id:1504.01033 author:Aaron Roth, Jonathan Ullman, Zhiwei Steven Wu category:cs.DS cs.GT cs.LG  published:2015-04-04 summary:A Stackelberg game is played between a leader and a follower. The leader first chooses an action, then the follower plays his best response. The goal of the leader is to pick the action that will maximize his payoff given the follower's best response. In this paper we present an approach to solving for the leader's optimal strategy in certain Stackelberg games where the follower's utility function (and thus the subsequent best response of the follower) is unknown. Stackelberg games capture, for example, the following interaction between a producer and a consumer. The producer chooses the prices of the goods he produces, and then a consumer chooses to buy a utility maximizing bundle of goods. The goal of the seller here is to set prices to maximize his profit---his revenue, minus the production cost of the purchased bundle. It is quite natural that the seller in this example should not know the buyer's utility function. However, he does have access to revealed preference feedback---he can set prices, and then observe the purchased bundle and his own profit. We give algorithms for efficiently solving, in terms of both computational and query complexity, a broad class of Stackelberg games in which the follower's utility function is unknown, using only "revealed preference" access to it. This class includes in particular the profit maximization problem, as well as the optimal tolling problem in nonatomic congestion games, when the latency functions are unknown. Surprisingly, we are able to solve these problems even though the optimization problems are non-convex in the leader's actions. version:2
arxiv-1502-07432 | Coercive Region-level Registration for Multi-modal Images | http://arxiv.org/abs/1502.07432 | id:1502.07432 author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Jeffrey Simmons, Alfred Hero category:cs.CV physics.data-an  published:2015-02-26 summary:We propose a coercive approach to simultaneously register and segment multi-modal images which share similar spatial structure. Registration is done at the region level to facilitate data fusion while avoiding the need for interpolation. The algorithm performs alternating minimization of an objective function informed by statistical models for pixel values in different modalities. Hypothesis tests are developed to determine whether to refine segmentations by splitting regions. We demonstrate that our approach has significantly better performance than the state-of-the-art registration and segmentation methods on microscopy images. version:3
arxiv-1511-05650 | Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models | http://arxiv.org/abs/1511.05650 | id:1511.05650 author:Juho Lee, Seungjin Choi category:stat.ML cs.LG  published:2015-11-18 summary:Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real-world datasets demonstrate the benefit of our method. version:1
arxiv-1511-05643 | A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation | http://arxiv.org/abs/1511.05643 | id:1511.05643 author:Md Kamrul Hasan, Christopher J. Pal category:cs.CV cs.AI cs.IR cs.LG  published:2015-11-18 summary:We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance. version:1
arxiv-1511-05635 | Competitive Multi-scale Convolution | http://arxiv.org/abs/1511.05635 | id:1511.05635 author:Zhibin Liao, Gustavo Carneiro category:cs.CV cs.LG cs.NE  published:2015-11-18 summary:In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN. version:1
arxiv-1511-05625 | MOEA/D-GM: Using probabilistic graphical models in MOEA/D for solving combinatorial optimization problems | http://arxiv.org/abs/1511.05625 | id:1511.05625 author:Murilo Zangari de Souza, Roberto Santana, Aurora Trinidad Ramirez Pozo, Alexander Mendiburu category:cs.NE  published:2015-11-18 summary:Evolutionary algorithms based on modeling the statistical dependencies (interactions) between the variables have been proposed to solve a wide range of complex problems. These algorithms learn and sample probabilistic graphical models able to encode and exploit the regularities of the problem. This paper investigates the effect of using probabilistic modeling techniques as a way to enhance the behavior of MOEA/D framework. MOEA/D is a decomposition based evolutionary algorithm that decomposes a multi-objective optimization problem (MOP) in a number of scalar single-objective subproblems and optimizes them in a collaborative manner. MOEA/D framework has been widely used to solve several MOPs. The proposed algorithm, MOEA/D using probabilistic Graphical Models (MOEA/D-GM) is able to instantiate both univariate and multi-variate probabilistic models for each subproblem. To validate the introduced framework algorithm, an experimental study is conducted on a multi-objective version of the deceptive function Trap5. The results show that the variant of the framework (MOEA/D-Tree), where tree models are learned from the matrices of the mutual information between the variables, is able to capture the structure of the problem. MOEA/D-Tree is able to achieve significantly better results than both MOEA/D using genetic operators and MOEA/D using univariate probability models, in terms of the approximation to the true Pareto front. version:1
arxiv-1511-05612 | A Block Regression Model for Short-Term Mobile Traffic Forecasting | http://arxiv.org/abs/1511.05612 | id:1511.05612 author:Huimin Pan, Jingchu Liu, Sheng Zhou, Zhisheng Niu category:cs.NI cs.LG  published:2015-11-17 summary:Accurate mobile traffic forecast is important for efficient network planning and operations. However, existing traffic forecasting models have high complexity, making the forecasting process slow and costly. In this paper, we analyze some characteristics of mobile traffic such as periodicity, spatial similarity and short term relativity. Based on these characteristics, we propose a \emph{Block Regression} ({BR}) model for mobile traffic forecasting. This model employs seasonal differentiation so as to take into account of the temporally repetitive nature of mobile traffic. One of the key features of our {BR} model lies in its low complexity since it constructs a single model for all base stations. We evaluate the accuracy of {BR} model based on real traffic data and compare it with the existing models. Results show that our {BR} model offers equal accuracy to the existing models but has much less complexity. version:1
arxiv-1504-06785 | Complete Dictionary Recovery over the Sphere | http://arxiv.org/abs/1504.06785 | id:1504.06785 author:Ju Sun, Qing Qu, John Wright category:cs.IT cs.CV cs.LG math.IT math.OC stat.ML  published:2015-04-26 summary:We consider the problem of recovering a complete (i.e., square and invertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb R^{n \times p}$ with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ is sufficiently sparse. This recovery problem is central to the theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals, and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\mathbf X_0$. In contrast, prior results based on efficient algorithms provide recovery guarantees when $\mathbf X_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta \in (0, 1)$. Our algorithmic pipeline centers around solving a certain nonconvex optimization problem with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. To show this apparently hard problem is tractable, we first provide a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no "spurious" local minima. This particular geometric structure allows us to design a Riemannian trust region algorithm over the sphere that provably converges to one local minimizer with an arbitrary initialization, despite the presence of saddle points. The geometric approach we develop here may also shed light on other problems arising from nonconvex recovery of structured signals. version:3
arxiv-1507-04208 | Combinatorial Cascading Bandits | http://arxiv.org/abs/1507.04208 | id:1507.04208 author:Branislav Kveton, Zheng Wen, Azin Ashkan, Csaba Szepesvari category:cs.LG stat.ML  published:2015-07-15 summary:We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its $n$-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm. version:3
arxiv-1511-05526 | Articulated Motion Learning via Visual and Lingual Signals | http://arxiv.org/abs/1511.05526 | id:1511.05526 author:Zhengyang Wu, Mohit Bansal, Matthew R. Walter category:cs.RO cs.CL cs.CV  published:2015-11-17 summary:In order for robots to operate effectively in homes and workplaces, they must be able to manipulate the articulated objects common to environments built for and by humans. Previous work learns kinematic models that prescribe this manipulation from visual demonstrations. Lingual signals, such as natural language descriptions and instructions, offer a complementary means of conveying knowledge of such manipulation models and are suitable to a wide range of interactions (e.g., remote manipulation). In this paper, we present a multimodal learning framework that incorporates both visual and lingual information to estimate the structure and parameters that define kinematic models of articulated objects. The visual signal takes the form of an RGB-D image stream that opportunistically captures object motion in an unprepared scene. Accompanying natural language descriptions of the motion constitute the lingual signal. We present a probabilistic language model that uses word embeddings to associate lingual verbs with their corresponding kinematic structures. By exploiting the complementary nature of the visual and lingual input, our method infers correct kinematic structures for various multiple-part objects on which the previous state-of-the-art, visual-only system fails. We evaluate our multimodal learning framework on a dataset comprised of a variety of household objects, and demonstrate a 36% improvement in model accuracy over the vision-only baseline. version:1
arxiv-1511-05520 | Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks | http://arxiv.org/abs/1511.05520 | id:1511.05520 author:Peter Li, Jiyuan Qian, Tian Wang category:cs.SD cs.IR cs.LG cs.NE  published:2015-11-17 summary:Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these "shallow" architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features. version:1
arxiv-1511-05512 | Moral Lineage Tracing | http://arxiv.org/abs/1511.05512 | id:1511.05512 author:Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, Bjoern Andres category:cs.CV cs.DM  published:2015-11-17 summary:Lineage tracing, the tracking of living cells as they move and divide, is a central problem in biological image analysis. Solutions, called lineage forests, are key to understanding how the structure of multicellular organisms emerges. We propose an integer linear program (ILP) whose feasible solutions define a decomposition of each image in a sequence into cells (segmentation), and a lineage forest of cells across images (tracing). Unlike previous formulations, we do not constrain the set of decompositions, except by contracting pixels to superpixels. The main challenge, as we show, is to enforce the morality of lineages, i.e., the constraint that cells do not merge. To enforce morality, we introduce path-cut inequalities. To find feasible solutions of the NP-hard ILP, with certified bounds to the global optimum, we define efficient separation procedures and apply these as part of a branch-and-cut algorithm. We show the effectiveness of this approach by analyzing feasible solutions for real microscopy data in terms of bounds and run-time, and by their weighted edit distance to ground truth lineage forests traced by humans. version:1
arxiv-1511-05497 | Learning the Architecture of Deep Neural Networks | http://arxiv.org/abs/1511.05497 | id:1511.05497 author:Suraj Srinivas, R. Venkatesh Babu category:cs.LG cs.CV cs.NE  published:2015-11-17 summary:Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy. version:1
arxiv-1511-05483 | Accelerating pseudo-marginal Metropolis-Hastings by correlating auxiliary variables | http://arxiv.org/abs/1511.05483 | id:1511.05483 author:Johan Dahlin, Fredrik Lindsten, Joel Kronander, Thomas B. Schön category:stat.CO stat.ML  published:2015-11-17 summary:Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesian inference in models where the posterior distribution is analytical intractable or computationally costly to evaluate directly. It operates by introducing additional auxiliary variables into the model and form an extended target distribution, which then can be evaluated point-wise. In many cases, the standard Metropolis-Hastings is then applied to sample from the extended target and the sought posterior can be obtained by marginalisation. However, in some implementations this approach suffers from poor mixing as the auxiliary variables are sampled from an independent proposal. We propose a modification to the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead. This results in that we introduce a positive correlation in the auxiliary variables. We investigate how to tune the CN proposal and its impact on the mixing of the resulting pmMH sampler. The conclusion is that the proposed modification can have a beneficial effect on both the mixing of the Markov chain and the computational cost for each iteration of the pmMH algorithm. version:1
arxiv-1510-03336 | Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly Benchmark | http://arxiv.org/abs/1510.03336 | id:1510.03336 author:Alexander Lavin, Subutai Ahmad category:cs.AI cs.LG  published:2015-10-12 summary:Much of the world's data is streaming, time-series data, where anomalies give significant information in critical situations; examples abound in domains such as finance, IT, security, medical, and energy. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, not batches, and learn while simultaneously making predictions. There are no benchmarks to adequately test and score the efficacy of real-time anomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which attempts to provide a controlled and repeatable environment of open-source tools to test and measure anomaly detection algorithms on streaming data. The perfect detector would detect all anomalies as soon as possible, trigger no false alarms, work with real-world time-series data across a variety of domains, and automatically adapt to changing statistics. Rewarding these characteristics is formalized in NAB, using a scoring algorithm designed for streaming data. NAB evaluates detectors on a benchmark dataset with labeled, real-world time-series data. We present these components, and give results and analyses for several open source, commercially-used algorithms. The goal for NAB is to provide a standard, open source framework with which the research community can compare and evaluate different algorithms for detecting anomalies in streaming data. version:4
arxiv-1411-2021 | Partitioning Well-Clustered Graphs: Spectral Clustering Works! | http://arxiv.org/abs/1411.2021 | id:1411.2021 author:Richard Peng, He Sun, Luca Zanetti category:cs.DS cs.LG  published:2014-11-07 summary:In this paper we study variants of the widely used spectral clustering that partitions a graph into k clusters by (1) embedding the vertices of a graph into a low-dimensional space using the bottom eigenvectors of the Laplacian matrix, and (2) grouping the embedded points into k clusters via k-means algorithms. We show that, for a wide class of graphs, spectral clustering gives a good approximation of the optimal clustering. While this approach was proposed in the early 1990s and has comprehensive applications, prior to our work similar results were known only for graphs generated from stochastic models. We also give a nearly-linear time algorithm for partitioning well-clustered graphs based on heat kernel embeddings and approximate nearest neighbor data structures. version:2
arxiv-1511-05464 | Extending Gossip Algorithms to Distributed Estimation of U-Statistics | http://arxiv.org/abs/1511.05464 | id:1511.05464 author:Igor Colin, Aurélien Bellet, Joseph Salmon, Stéphan Clémençon category:stat.ML cs.DC cs.LG cs.SY  published:2015-11-17 summary:Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of $U$-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the $U$-statistic of interest. We establish convergence rate bounds of $O(1/t)$ and $O(\log t / t)$ for the synchronous and asynchronous cases respectively, where $t$ is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach. version:1
arxiv-1511-05424 | Simple, Fast and Accurate Photometric Estimation of Specific Star Formation Rate | http://arxiv.org/abs/1511.05424 | id:1511.05424 author:Kristoffer Stensbo-Smidt, Fabian Gieseke, Christian Igel, Andrew Zirm, Kim Steenstrup Pedersen category:astro-ph.IM stat.ML  published:2015-11-17 summary:Large-scale surveys make huge amounts of photometric data available. Because of the sheer amount of objects, spectral data cannot be obtained for all of them. Therefore it is important to devise techniques for reliably estimating physical properties of objects from photometric information alone. These estimates are needed to automatically identify interesting objects worth a follow-up investigation as well as to produce the required data for a statistical analysis of the space covered by a survey. We argue that machine learning techniques are suitable to compute these estimates accurately and efficiently. This study considers the task of estimating the specific star formation rate (sSFR) of galaxies. It is shown that a nearest neighbours algorithm can produce better sSFR estimates than traditional SED fitting. We show that we can obtain accurate estimates of the sSFR even at high redshifts using only broad-band photometry based on the u, g, r, i and z filters from Sloan Digital Sky Survey (SDSS). We addtionally demonstrate that combining magnitudes estimated with different methods from the same photometry can lead to a further improvement in accuracy. The study highlights the general importance of performing proper model selection to improve the results of machine learning systems and how feature selection can provide insights into the predictive relevance of particular input features. Furthermore, the use of massively parallel computation on graphics processing units (GPUs) for handling large amounts of astronomical data is advocated. version:1
arxiv-1511-05385 | Bayesian Optimization with Dimension Scheduling: Application to Biological Systems | http://arxiv.org/abs/1511.05385 | id:1511.05385 author:Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter Deisenroth, Ruth Misener category:stat.ML cs.LG math.OC  published:2015-11-17 summary:Bayesian Optimization (BO) is a data-efficient method for global black-box optimization of an expensive-to-evaluate fitness function. BO typically assumes that computation cost of BO is cheap, but experiments are time consuming or costly. In practice, this allows us to optimize ten or fewer critical parameters in up to 1,000 experiments. But experiments may be less expensive than BO methods assume: In some simulation models, we may be able to conduct multiple thousands of experiments in a few hours, and the computational burden of BO is no longer negligible compared to experimentation time. To address this challenge we introduce a new Dimension Scheduling Algorithm (DSA), which reduces the computational burden of BO for many experiments. The key idea is that DSA optimizes the fitness function only along a small set of dimensions at each iteration. This DSA strategy (1) reduces the necessary computation time, (2) finds good solutions faster than the traditional BO method, and (3) can be parallelized straightforwardly. We evaluate the DSA in the context of optimizing parameters of dynamic models of microalgae metabolism and show faster convergence than traditional BO. version:1
arxiv-1511-05371 | Constant Time EXPected Similarity Estimation using Stochastic Optimization | http://arxiv.org/abs/1511.05371 | id:1511.05371 author:Markus Schneider, Wolfgang Ertel, Günther Palm category:cs.LG  published:2015-11-17 summary:A new algorithm named EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of $n$ samples, EXPoSE needs only $\mathcal{O}(n)$ (linear time) to build a model and $\mathcal{O}(1)$ (constant time) to make a prediction. In this work we improve the linear computational complexity and show that an $\epsilon$-accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we cast the original EXPoSE formulation into a stochastic optimization problem. It is crucial that this approach allows us to determine the number of iteration based on a desired accuracy $\epsilon$, independent of the dataset size $n$. We will show that the proposed stochastic gradient descent algorithm works in general (possible infinite-dimensional) Hilbert spaces, is easy to implement and requires no additional step-size parameters. version:1
arxiv-1312-2638 | Vertex nomination schemes for membership prediction | http://arxiv.org/abs/1312.2638 | id:1312.2638 author:D. E. Fishkind, V. Lyzinski, H. Pao, L. Chen, C. E. Priebe category:stat.ML math.OC stat.AP  published:2013-12-10 summary:Suppose that a graph is realized from a stochastic block model where one of the blocks is of interest, but many or all of the vertices' block labels are unobserved. The task is to order the vertices with unobserved block labels into a ``nomination list'' such that, with high probability, vertices from the interesting block are concentrated near the list's beginning. We propose several vertex nomination schemes. Our basic - but principled - setting and development yields a best nomination scheme (which is a Bayes-Optimal analogue), and also a likelihood maximization nomination scheme that is practical to implement when there are a thousand vertices, and which is empirically near-optimal when the number of vertices is small enough to allow comparison to the best nomination scheme. We then illustrate the robustness of the likelihood maximization nomination scheme to the modeling challenges inherent in real data, using examples which include a social network involving human trafficking, the Enron Graph, a worm brain connectome and a political blog network. version:5
arxiv-1511-03853 | When Naïve Bayes Nearest Neighbours Meet Convolutional Neural Networks | http://arxiv.org/abs/1511.03853 | id:1511.03853 author:Ilja Kuzborskij, Fabio Maria Carlucci, Barbara Caputo category:cs.CV  published:2015-11-12 summary:Since Convolutional Neural Networks (CNNs) have become the leading learning paradigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-based classifiers have lost momentum in the community. This is because (1) such algorithms cannot use CNN activations as input features; (2) they cannot be used as final layer of CNN architectures for end-to-end training , and (3) they are generally not scalable and hence cannot handle big data. This paper proposes a framework that addresses all these issues, thus bringing back NBNNs on the map. We solve the first by extracting CNN activations from local patches at multiple scale levels, similarly to [1]. We address simultaneously the second and third by proposing a scalable version of Naive Bayes Non-linear Learning (NBNL, [2]). Results obtained using pre-trained CNNs on standard scene and domain adaptation databases show the strength of our approach, opening a new season for NBNNs. version:2
arxiv-1511-05084 | Understanding learned CNN features through Filter Decoding with Substitution | http://arxiv.org/abs/1511.05084 | id:1511.05084 author:Ivet Rafegas, Maria Vanrell category:cs.CV  published:2015-11-16 summary:In parallel with the success of CNNs to solve vision problems, there is a growing interest in developing methodologies to understand and visualize the internal representations of these networks. How the responses of a trained CNN encode the visual information is a fundamental question both for computer and human vision research. Image representations provided by the first convolutional layer as well as the resolution change provided by the max-polling operation are easy to understand, however, as soon as a second and further convolutional layers are added in the representation, any intuition is lost. A usual way to deal with this problem has been to define deconvolutional networks that somehow allow to explore the internal representations of the most important activations towards the image space, where deconvolution is assumed as a convolution with the transposed filter. However, this assumption is not the best approximation of an inverse convolution. In this paper we propose a new assumption based on filter substitution to reverse the encoding of a convolutional layer. This provides us with a new tool to directly visualize any CNN single neuron as a filter in the first layer, this is in terms of the image space. version:2
arxiv-1404-6769 | Aggregation of predictors for nonstationary sub-linear processes and online adaptive forecasting of time varying autoregressive processes | http://arxiv.org/abs/1404.6769 | id:1404.6769 author:Christophe Giraud, François Roueff, Andres Sanchez-Perez category:math.ST stat.ML stat.TH  published:2014-04-27 summary:In this work, we study the problem of aggregating a finite number of predictors for nonstationary sub-linear processes. We provide oracle inequalities relying essentially on three ingredients: (1) a uniform bound of the $\ell^1$ norm of the time varying sub-linear coefficients, (2) a Lipschitz assumption on the predictors and (3) moment conditions on the noise appearing in the linear representation. Two kinds of aggregations are considered giving rise to different moment conditions on the noise and more or less sharp oracle inequalities. We apply this approach for deriving an adaptive predictor for locally stationary time varying autoregressive (TVAR) processes. It is obtained by aggregating a finite number of well chosen predictors, each of them enjoying an optimal minimax convergence rate under specific smoothness conditions on the TVAR coefficients. We show that the obtained aggregated predictor achieves a minimax rate while adapting to the unknown smoothness. To prove this result, a lower bound is established for the minimax rate of the prediction risk for the TVAR process. Numerical experiments complete this study. An important feature of this approach is that the aggregated predictor can be computed recursively and is thus applicable in an online prediction context. version:5
arxiv-1511-05309 | Optimized Linear Imputation | http://arxiv.org/abs/1511.05309 | id:1511.05309 author:Yehezkel S. Resheff, Daphna Weinshall category:stat.ML stat.AP stat.CO stat.ME  published:2015-11-17 summary:Often in real-world datasets, especially in high dimensional data, some feature values are missing. Since most data analysis and statistical methods do not handle gracefully missing values, the ?rst step in the analysis requires the imputation of missing values. Indeed, there has been a long standing interest in methods for the imputation of missing values as a pre-processing step. One recent and e?ective approach, the IRMI stepwise regression imputation method, uses a linear regression model for each real-valued feature on the basis of all other features in the dataset. However, the proposed iterative formulation lacks convergence guarantee. Here we propose a closely related method, stated as a single optimization problem and a block coordinate-descent solution which is guaranteed to converge to a local minimum. Experiments show results on both synthetic and benchmark datasets, which are comparable to the results of the IRMI method whenever it converges. However, while in the set of experiments described here IRMI often does not converge, the performance of our methods is shown to be markedly superior in comparison with other methods. version:1
arxiv-1511-05286 | Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning | http://arxiv.org/abs/1511.05286 | id:1511.05286 author:Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey category:cs.CV q-bio.SC stat.ML  published:2015-11-17 summary:Convolutional neural networks (CNN) have achieved state of the art performance on both classification and segmentation tasks. Applying CNNs to microscopy images is challenging due to the lack of datasets labeled at the single cell level. We extend the application of CNNs to microscopy image classification and segmentation using multiple instance learning (MIL). We present the adaptive Noisy-AND MIL pooling function, a new MIL operator that is robust to outliers. Combining CNNs with MIL enables training CNNs using full resolution microscopy images with global labels. We base our approach on the similarity between the aggregation function used in MIL and pooling layers used in CNNs. We show that training MIL CNNs end-to-end outperforms several previous methods on both mammalian and yeast microscopy images without requiring any segmentation steps. version:1
arxiv-1506-03412 | Convergence rates for pretraining and dropout: Guiding learning parameters using network structure | http://arxiv.org/abs/1506.03412 | id:1506.03412 author:Vamsi K. Ithapu, Sathya Ravi, Vikas Singh category:cs.LG cs.CV cs.NE math.OC stat.ML  published:2015-06-10 summary:Unsupervised pretraining and dropout have been well studied, especially with respect to regularization and output consistency. However, our understanding about the explicit convergence rates of the parameter estimates, and their dependence on the learning (like denoising and dropout rate) and structural (like depth and layer lengths) aspects of the network is less mature. An interesting question in this context is to ask if the network structure could "guide" the choices of such learning parameters. In this work, we explore these gaps between network structure, the learning mechanisms and their interaction with parameter convergence rates. We present a way to address these issues based on the backpropagation convergence rates for general nonconvex objectives using first-order information. We then incorporate two learning mechanisms into this general framework -- denoising autoencoder and dropout, and subsequently derive the convergence rates of deep networks. Building upon these bounds, we provide insights into the choices of learning parameters and network sizes that achieve certain levels of convergence accuracy. The results derived here support existing empirical observations, and we also conduct a set of experiments to evaluate them. version:2
arxiv-1511-05266 | Semi-supervised Collaborative Ranking with Push at Top | http://arxiv.org/abs/1511.05266 | id:1511.05266 author:Iman Barjasteh, Rana Forsati, Abdol-Hossein Esfahanian, Hayder Radha category:cs.LG cs.IR  published:2015-11-17 summary:Existing collaborative ranking based recommender systems tend to perform best when there is enough observed ratings for each user and the observation is made completely at random. Under this setting recommender systems can properly suggest a list of recommendations according to the user interests. However, when the observed ratings are extremely sparse (e.g. in the case of cold-start users where no rating data is available), and are not sampled uniformly at random, existing ranking methods fail to effectively leverage side information to transduct the knowledge from existing ratings to unobserved ones. We propose a semi-supervised collaborative ranking model, dubbed \texttt{S$^2$COR}, to improve the quality of cold-start recommendation. \texttt{S$^2$COR} mitigates the sparsity issue by leveraging side information about both observed and missing ratings by collaboratively learning the ranking model. This enables it to deal with the case of missing data not at random, but to also effectively incorporate the available side information in transduction. We experimentally evaluated our proposed algorithm on a number of challenging real-world datasets and compared against state-of-the-art models for cold-start recommendation. We report significantly higher quality recommendations with our algorithm compared to the state-of-the-art. version:1
arxiv-1511-05261 | Robust PCA via Nonconvex Rank Approximation | http://arxiv.org/abs/1511.05261 | id:1511.05261 author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.LG cs.NA stat.ML  published:2015-11-17 summary:Numerous applications in data mining and machine learning require recovering a matrix of minimal rank. Robust principal component analysis (RPCA) is a general framework for handling this kind of problems. Nuclear norm based convex surrogate of the rank function in RPCA is widely investigated. Under certain assumptions, it can recover the underlying true low rank matrix with high probability. However, those assumptions may not hold in real-world applications. Since the nuclear norm approximates the rank by adding all singular values together, which is essentially a $\ell_1$-norm of the singular values, the resulting approximation error is not trivial and thus the resulting matrix estimator can be significantly biased. To seek a closer approximation and to alleviate the above-mentioned limitations of the nuclear norm, we propose a nonconvex rank approximation. This approximation to the matrix rank is tighter than the nuclear norm. To solve the associated nonconvex minimization problem, we develop an efficient augmented Lagrange multiplier based optimization algorithm. Experimental results demonstrate that our method outperforms current state-of-the-art algorithms in both accuracy and efficiency. version:1
arxiv-1506-02078 | Visualizing and Understanding Recurrent Networks | http://arxiv.org/abs/1506.02078 | id:1506.02078 author:Andrej Karpathy, Justin Johnson, Li Fei-Fei category:cs.LG cs.CL cs.NE  published:2015-06-05 summary:Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study. version:2
arxiv-1511-05240 | An extension of McDiarmid's inequality | http://arxiv.org/abs/1511.05240 | id:1511.05240 author:Richard Combes category:cs.LG math.PR math.ST stat.TH  published:2015-11-17 summary:We derive an extension of McDiarmid's inequality for functions $f$ with bounded differences on a high probability set ${\cal Y}$ (instead of almost surely). The behavior of $f$ outside ${\cal Y}$ may be arbitrary. The proof is short and elementary, and relies on an extension argument similar to Kirszbraun's theorem. version:1
arxiv-1511-05219 | Controlling Bias in Adaptive Data Analysis Using Information Theory | http://arxiv.org/abs/1511.05219 | id:1511.05219 author:Daniel Russo, James Zou category:stat.ML cs.LG  published:2015-11-16 summary:Modern data is messy and high-dimensional, and it is often not clear a priori what are the right questions to ask. Instead, the analyst typically needs to use the data to search for interesting analyses to perform and hypotheses to test. This is an adaptive process, where the choice of analysis to be performed next depends on the results of the previous analyses on the same data. It's widely recognized that this process, even if well-intentioned, can lead to biases and false discoveries, contributing to the crisis of reproducibility in science. But while adaptivity renders standard statistical theory invalid, folklore and experience suggest that not all types of adaptive analysis are equally at risk for false discoveries. In this paper, we propose a general information-theoretic framework to quantify and provably bound the bias and other statistics of an arbitrary adaptive analysis process. We prove that our mutual information based bound is tight in natural models, and then use it to give rigorous insights into when commonly used procedures do or do not lead to substantially biased estimation. We first consider several popular feature selection protocols, like rank selection or variance-based selection. We then consider the practice of adding random noise to the observations or to the reported statistics, which is advocated by related ideas from differential privacy and blinded data analysis. We discuss the connections between these techniques and our framework, and supplement our results with illustrative simulations. version:1
arxiv-1511-05204 | Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition | http://arxiv.org/abs/1511.05204 | id:1511.05204 author:Mengyi Liu, Shiguang Shan, Ruiping Wang, Xilin Chen category:cs.CV  published:2015-11-16 summary:Facial expression is temporally dynamic event which can be decomposed into a set of muscle motions occurring in different facial regions over various time intervals. For dynamic expression recognition, two key issues, temporal alignment and semantics-aware dynamic representation, must be taken into account. In this paper, we attempt to solve both problems via manifold modeling of videos based on a novel mid-level representation, i.e. \textbf{expressionlet}. Specifically, our method contains three key stages: 1) each expression video clip is characterized as a spatial-temporal manifold (STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM) is learned over all low-level features and represented as a set of local modes to statistically unify all the STMs. 3) the local modes on each STM can be instantiated by fitting to UMM, and the corresponding expressionlet is constructed by modeling the variations in each local mode. With above strategy, expression videos are naturally aligned both spatially and temporally. To enhance the discriminative power, the expressionlet-based STM representation is further processed with discriminant embedding. Our method is evaluated on four public expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, our method outperforms the known state-of-the-art by a large margin. version:1
arxiv-1511-05194 | Sparse-promoting Full Waveform Inversion based on Online Orthonormal Dictionary Learning | http://arxiv.org/abs/1511.05194 | id:1511.05194 author:Lingchen Zhu, Entao Liu, James H. McClellan category:physics.geo-ph cs.LG cs.NA math.NA  published:2015-11-16 summary:Full waveform inversion (FWI) delivers high-resolution images of a subsurface medium model by minimizing iteratively the least-squares misfit between the observed and simulated seismic data. Due to the limited accuracy of the starting model and the inconsistency of the seismic waveform data, the FWI problem is inherently ill-posed, so that regularization techniques are typically applied to obtain better models. FWI is also a computationally expensive problem because modern seismic surveys cover very large areas of interest and collect massive volumes of data. The dimensionality of the problem and the heterogeneity of the medium both stress the need for faster algorithms and sparse regularization techniques to accelerate and improve imaging results. This paper reaches these goals by developing a compressive sensing approach for the FWI problem, where the sparsity of model perturbations is exploited within learned dictionaries. Based on stochastic approximations, the dictionaries are updated iteratively to adapt to dynamic model perturbations. Meanwhile, the dictionaries are kept orthonormal in order to maintain the corresponding transform in a fast and compact manner without introducing extra computational overhead to FWI. Such a sparsity regularization on model perturbations enables us to take randomly subsampled data for computation and thus significantly reduce the cost. Compared with other approaches that employ sparsity constraints in the fixed curvelet transform domain, our approach can achieve more robust inversion results with better model fit and visual quality. version:1
arxiv-1511-05191 | Binary Classifier Calibration using an Ensemble of Near Isotonic Regression Models | http://arxiv.org/abs/1511.05191 | id:1511.05191 author:Mahdi Pakdaman Naeini, Gregory F. Cooper category:cs.LG stat.ML  published:2015-11-16 summary:Learning accurate probabilistic models from data is crucial in many practical tasks in data mining. In this paper we present a new non-parametric calibration method called \textit{ensemble of near isotonic regression} (ENIR). The method can be considered as an extension of BBQ, a recently proposed calibration method, as well as the commonly used calibration method based on isotonic regression. ENIR is designed to address the key limitation of isotonic regression which is the monotonicity assumption of the predictions. Similar to BBQ, the method post-processes the output of a binary classifier to obtain calibrated probabilities. Thus it can be combined with many existing classification models. We demonstrate the performance of ENIR on synthetic and real datasets for the commonly used binary classification models. Experimental results show that the method outperforms several common binary classifier calibration methods. In particular on the real data, ENIR commonly performs statistically significantly better than the other methods, and never worse. It is able to improve the calibration power of classifiers, while retaining their discrimination power. The method is also computationally tractable for large scale datasets, as it is $O(N \log N)$ time, where $N$ is the number of samples. version:1
arxiv-1506-03338 | Neural Adaptive Sequential Monte Carlo | http://arxiv.org/abs/1506.03338 | id:1506.03338 author:Shixiang Gu, Zoubin Ghahramani, Richard E. Turner category:cs.LG stat.ML  published:2015-06-10 summary:Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference. version:3
arxiv-1511-05174 | Cross-scale predictive dictionaries | http://arxiv.org/abs/1511.05174 | id:1511.05174 author:Vishwanath Saragadam, Aswin Sankaranarayanan, Xin Li category:cs.CV stat.ML  published:2015-11-16 summary:We propose a novel signal model, based on sparse representations, that captures cross-scale features for visual signals. We show that cross-scale predictive model enables faster solutions to sparse approximation problems. This is achieved by first solving the sparse approximation problem for the downsampled signal and using the support of the solution to constrain the support at the original resolution. The speedups obtained are especially compelling for high-dimensional signals that require large dictionaries to provide precise sparse approximations. We demonstrate speedups in the order of 10-100x for denoising and up to 15x speedups for compressive sensing of images, videos, hyperspectral images and light-field images. version:1
arxiv-1511-05169 | Nonlinear Local Metric Learning for Person Re-identification | http://arxiv.org/abs/1511.05169 | id:1511.05169 author:Siyuan Huang, Jiwen Lu, Jie Zhou, Anil K. Jain category:cs.CV  published:2015-11-16 summary:Person re-identification aims at matching pedestrians observed from non-overlapping camera views. Feature descriptor and metric learning are two significant problems in person re-identification. A discriminative metric learning method should be capable of exploiting complex nonlinear transformations due to the large variations in feature space. In this paper, we propose a nonlinear local metric learning (NLML) method to improve the state-of-the-art performance of person re-identification on public datasets. Motivated by the fact that local metric learning has been introduced to handle the data which varies locally and deep neural network has presented outstanding capability in exploiting the nonlinearity of samples, we utilize the merits of both local metric learning and deep neural network to learn multiple sets of nonlinear transformations. By enforcing a margin between the distances of positive pedestrian image pairs and distances of negative pairs in the transformed feature subspace, discriminative information can be effectively exploited in the developed neural networks. Our experiments show that the proposed NLML method achieves the state-of-the-art results on the widely used VIPeR, GRID, and CUHK 01 datasets. version:1
arxiv-1511-05102 | Resolving the Geometric Locus Dilemma for Support Vector Learning Machines | http://arxiv.org/abs/1511.05102 | id:1511.05102 author:Denise M. Reeves category:cs.LG stat.ML  published:2015-11-16 summary:Capacity control, the bias/variance dilemma, and learning unknown functions from data, are all concerned with identifying effective and consistent fits of unknown geometric loci to random data points. A geometric locus is a curve or surface formed by points, all of which possess some uniform property. A geometric locus of an algebraic equation is the set of points whose coordinates are solutions of the equation. Any given curve or surface must pass through each point on a specified locus. This paper argues that it is impossible to fit random data points to algebraic equations of partially configured geometric loci that reference arbitrary Cartesian coordinate systems. It also argues that the fundamental curve of a linear decision boundary is actually a principal eigenaxis. It is shown that learning principal eigenaxes of linear decision boundaries involves finding a point of statistical equilibrium for which eigenenergies of principal eigenaxis components are symmetrically balanced with each other. It is demonstrated that learning linear decision boundaries involves strong duality relationships between a statistical eigenlocus of principal eigenaxis components and its algebraic forms, in primal and dual, correlated Hilbert spaces. Locus equations are introduced and developed that describe principal eigen-coordinate systems for lines, planes, and hyperplanes. These equations are used to introduce and develop primal and dual statistical eigenlocus equations of principal eigenaxes of linear decision boundaries. Important generalizations for linear decision boundaries are shown to be encoded within a dual statistical eigenlocus of principal eigenaxis components. Principal eigenaxes of linear decision boundaries are shown to encode Bayes' likelihood ratio for common covariance data and a robust likelihood ratio for all other data. version:1
arxiv-1511-05101 | How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? | http://arxiv.org/abs/1511.05101 | id:1511.05101 author:Ferenc Huszár category:stat.ML cs.AI cs.IT cs.LG math.IT  published:2015-11-16 summary:Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions. Firstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical performance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm. Secondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal training objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality. version:1
arxiv-1505-00864 | Accurate estimation of influenza epidemics using Google search data via ARGO | http://arxiv.org/abs/1505.00864 | id:1505.00864 author:Shihao Yang, Mauricio Santillana, S. C. Kou category:stat.AP cs.SI stat.ML  published:2015-05-05 summary:Accurate real-time tracking of influenza outbreaks helps public health officials make timely and meaningful decisions that could save lives. We propose an influenza tracking model, ARGO (AutoRegression with GOogle search data), that uses publicly available online search data. In addition to having a rigorous statistical foundation, ARGO outperforms all previously available Google-search-based tracking models, including the latest version of Google Flu Trends, even though it uses only low-quality search data as input from publicly available Google Trends and Google Correlate websites. ARGO not only incorporates the seasonality in influenza epidemics but also captures changes in people's online search behavior over time. ARGO is also flexible, self-correcting, robust, and scalable, making it a potentially powerful tool that can be used for real-time tracking of other social events at multiple temporal and spatial resolutions. version:2
arxiv-1405-4980 | Convex Optimization: Algorithms and Complexity | http://arxiv.org/abs/1405.4980 | id:1405.4980 author:Sébastien Bubeck category:math.OC cs.CC cs.LG cs.NA stat.ML  published:2014-05-20 summary:This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods. version:2
arxiv-1511-05082 | Topic Modeling of Behavioral Modes Using Sensor Data | http://arxiv.org/abs/1511.05082 | id:1511.05082 author:Yehezkel S. Resheff, Shay Rotics, Ran Nathan, Daphna Weinshall category:cs.LG  published:2015-11-16 summary:The field of Movement Ecology, like so many other fields, is experiencing a period of rapid growth in availability of data. As the volume rises, traditional methods are giving way to machine learning and data science, which are playing an increasingly large part it turning this data into science-driving insights. One rich and interesting source is the bio-logger. These small electronic wearable devices are attached to animals free to roam in their natural habitats, and report back readings from multiple sensors, including GPS and accelerometer bursts. A common use of accelerometer data is for supervised learning of behavioral modes. However, we need unsupervised analysis tools as well, in order to overcome the inherent difficulties of obtaining a labeled dataset, which in some cases is either infeasible or does not successfully encompass the full repertoire of behavioral modes of interest. Here we present a matrix factorization based topic-model method for accelerometer bursts, derived using a linear mixture property of patch features. Our method is validated via comparison to a labeled dataset, and is further compared to standard clustering algorithms. version:1
arxiv-1511-05076 | Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation | http://arxiv.org/abs/1511.05076 | id:1511.05076 author:Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain category:cs.CL  published:2015-11-16 summary:This paper presents a new method for the discovery of latent domains in diverse speech data, for the use of adaptation of Deep Neural Networks (DNNs) for Automatic Speech Recognition. Our work focuses on transcription of multi-genre broadcast media, which is often only categorised broadly in terms of high level genres such as sports, news, documentary, etc. However, in terms of acoustic modelling these categories are coarse. Instead, it is expected that a mixture of latent domains can better represent the complex and diverse behaviours within a TV show, and therefore lead to better and more robust performance. We propose a new method, whereby these latent domains are discovered with Latent Dirichlet Allocation, in an unsupervised manner. These are used to adapt DNNs using the Unique Binary Code (UBIC) representation for the LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more than 2,000 shows for training and 47 shows for testing, show that the use of LDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline hybrid DNN models. version:1
arxiv-1511-05049 | An Empirical Study of Recent Face Alignment Methods | http://arxiv.org/abs/1511.05049 | id:1511.05049 author:Heng Yang, Xuhui Jia, Chen Change Loy, Peter Robinson category:cs.CV  published:2015-11-16 summary:The problem of face alignment has been intensively studied in the past years. A large number of novel methods have been proposed and reported very good performance on benchmark dataset such as 300W. However, the differences in the experimental setting and evaluation metric, missing details in the description of the methods make it hard to reproduce the results reported and evaluate the relative merits. For instance, most recent face alignment methods are built on top of face detection but from different face detectors. In this paper, we carry out a rigorous evaluation of these methods by making the following contributions: 1) we proposes a new evaluation metric for face alignment on a set of images, i.e., area under error distribution curve within a threshold, AUC$_\alpha$, given the fact that the traditional evaluation measure (mean error) is very sensitive to big alignment error. 2) we extend the 300W database with more practical face detections to make fair comparison possible. 3) we carry out face alignment sensitivity analysis w.r.t. face detection, on both synthetic and real data, using both off-the-shelf and re-retrained models. 4) we study factors that are particularly important to achieve good performance and provide suggestions for practical applications. Most of the conclusions drawn from our comparative analysis cannot be inferred from the original publications. version:1
arxiv-1507-08818 | A Visual Embedding for the Unsupervised Extraction of Abstract Semantics | http://arxiv.org/abs/1507.08818 | id:1507.08818 author:D. Garcia-Gasulla, J. Béjar, U. Cortés, E. Ayguadé, J. Labarta category:cs.CV cs.LG cs.NE  published:2015-07-31 summary:Vector-space word representations obtained from neural network models have been shown to enable semantic operations based on vector arithmetic. In this paper, we explore the existence of similar information on vector representations of images. For that purpose we define a methodology to obtain large, sparse vector representations of image classes, and generate vectors through the state-of-the-art deep learning architecture GoogLeNet for 20K images obtained from ImageNet. We first evaluate the resultant vector-space semantics through its correlation with WordNet distances, and find vector distances to be strongly correlated with linguistic semantics. We then explore the location of images within the vector space, finding elements close in WordNet to be clustered together, regardless of significant visual variances (e.g., 118 dog types). More surprisingly, we find that the space unsupervisedly separates complex classes without prior knowledge (e.g., living things). Finally, we consider vector arithmetics, and find them to be related with image concatenation (e.g., "Horse cart - Horse = Rickshaw"), image overlap ("Panda - Brown bear = Skunk") and regularities ("Panda is to Brown bear as Soccer ball is to Helmet"). These results indicate that image vector embeddings as the one proposed here contain rich visual semantics usable for learning and reasoning purposes. version:3
arxiv-1511-04970 | Learning Spanish dialects through Twitter | http://arxiv.org/abs/1511.04970 | id:1511.04970 author:Bruno Gonçalves, David Sánchez category:stat.ML cs.CL cs.CY physics.soc-ph stat.AP  published:2015-11-16 summary:We map the large-scale variation of the Spanish language by employing a corpus based on geographically tagged Twitter messages. Lexical dialects are extracted from an analysis of variants of tens of concepts. The resulting maps show linguistic variations on an unprecedented scale across the globe. We discuss the properties of the main dialects within a machine learning approach and find that varieties spoken in urban areas have an international character in contrast to country areas where dialects show a more regional uniformity. version:1
arxiv-1511-02352 | Performance Analysis of Multiclass Support Vector Machine Classification for Diagnosis of Coronary Heart Diseases | http://arxiv.org/abs/1511.02352 | id:1511.02352 author:Wiharto Wiharto, Hari Kusnanto, Herianto Herianto category:cs.LG  published:2015-11-07 summary:Automatic diagnosis of coronary heart disease helps the doctor to support in decision making a diagnosis. Coronary heart disease have some types or levels. Referring to the UCI Repository dataset, it divided into 4 types or levels that are labeled numbers 1-4 (low, medium, high and serious). The diagnosis models can be analyzed with multiclass classification approach. One of multiclass classification approach used, one of which is a support vector machine (SVM). The SVM use due to strong performance of SVM in binary classification. This research study multiclass performance classification support vector machine to diagnose the type or level of coronary heart disease. Coronary heart disease patient data taken from the UCI Repository. Stages in this study is preprocessing, which consist of, to normalizing the data, divide the data into data training and testing. The next stage of multiclass classification and performance analysis. This study uses multiclass SVM algorithm, namely: Binary Tree Support Vector Machine (BTSVM), One-Against-One (OAO), One-Against-All (OAA), Decision Direct Acyclic Graph (DDAG) and Exhaustive Output Error Correction Code (ECOC). Performance parameter used is recall, precision, F-measure and Overall accuracy. version:2
arxiv-1410-4627 | Learning visual biases from human imagination | http://arxiv.org/abs/1410.4627 | id:1410.4627 author:Carl Vondrick, Hamed Pirsiavash, Aude Oliva, Antonio Torralba category:cs.CV  published:2014-10-17 summary:Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available. version:2
arxiv-1504-04407 | Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting | http://arxiv.org/abs/1504.04407 | id:1504.04407 author:Jakub Konečný, Jie Liu, Peter Richtárik, Martin Takáč category:cs.LG stat.ML  published:2015-04-16 summary:We propose mS2GD: a method incorporating a mini-batching scheme for improving the theoretical complexity and practical performance of semi-stochastic gradient descent (S2GD). We consider the problem of minimizing a strongly convex function represented as the sum of an average of a large number of smooth convex functions, and a simple nonsmooth convex regularizer. Our method first performs a deterministic step (computation of the gradient of the objective function at the starting point), followed by a large number of stochastic steps. The process is repeated a few times with the last iterate becoming the new starting point. The novelty of our method is in introduction of mini-batching into the computation of stochastic steps. In each step, instead of choosing a single function, we sample $b$ functions, compute their gradients, and compute the direction based on this. We analyze the complexity of the method and show that it benefits from two speedup effects. First, we prove that as long as $b$ is below a certain threshold, we can reach any predefined accuracy with less overall work than without mini-batching. Second, our mini-batching scheme admits a simple parallel implementation, and hence is suitable for further acceleration by parallelization. version:2
arxiv-1511-04934 | Identification and Counting White Blood Cells and Red Blood Cells using Image Processing Case Study of Leukemia | http://arxiv.org/abs/1511.04934 | id:1511.04934 author:Esti Suryani, Wiharto Wiharto, Nizomjon Polvonov category:cs.CV  published:2015-11-16 summary:Leukemia is diagnosed with complete blood counts which is by calculating all blood cells and compare the number of white blood cells (White Blood Cells / WBC) and red blood cells (Red Blood Cells / RBC). Information obtained from a complete blood count, has become a cornerstone in the hematology laboratory for diagnostic purposes and monitoring of hematological disorders. However, the traditional procedure for counting blood cells manually requires effort and a long time, therefore this method is one of the most expensive routine tests in laboratory hematology clinic. Solution for such kind of time consuming task and necessity of data tracability can be found in image processing techniques based on blood cell morphology . This study aims to identify Acute Lymphocytic Leukemia (ALL) and Acute Myeloid Leukemia type M3 (AML M3) using Fuzzy Rule Based System based on morphology of white blood cells. Characteristic parameters witch extractedare WBC Area, Nucleus and Granule Ratio of white blood cells. Image processing algorithms such as thresholding, Canny edge detection and color identification filters are used.Then for identification of ALL, AML M3 and Healthy cells used Fuzzy Rule Based System with Sugeno method. In the testing process used 104 images out of which 29 ALL - Positive, 50 AML M3 - Positive and 25 Healthy cells. Test results showed 83.65 % accuracy . version:1
arxiv-1511-04902 | Graph-based denoising for time-varying point clouds | http://arxiv.org/abs/1511.04902 | id:1511.04902 author:Yann Schoenenberger, Johan Paratte, Pierre Vandergheynst category:cs.CV cs.GR I.5.4  published:2015-11-16 summary:Noisy 3D point clouds arise in many applications. They may be due to errors when constructing a 3D model from images or simply to imprecise depth sensors. Point clouds can be given geometrical structure using graphs created from the similarity information between points. This paper introduces a technique that uses this graph structure and convex optimization methods to denoise 3D point clouds. A short discussion presents how those methods naturally generalize to time-varying inputs such as 3D point cloud time series. version:1
arxiv-1511-04901 | Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression | http://arxiv.org/abs/1511.04901 | id:1511.04901 author:Zhiao Huang, Erjin Zhou, Zhimin Cao category:cs.CV  published:2015-11-16 summary:Facial landmark localization plays an important role in face recognition and analysis applications. In this paper, we give a brief introduction to a coarse-to-fine pipeline with neural networks and sequential regression. First, a global convolutional network is applied to the holistic facial image to give an initial landmark prediction. A pyramid of multi-scale local image patches is then cropped to feed to a new network for each landmark to refine the prediction. As the refinement network outputs a more accurate position estimation than the input, such procedure could be repeated several times until the estimation converges. We evaluate our system on the 300-W dataset [11] and it outperforms the recent state-of-the-arts. version:1
arxiv-1511-04898 | Fast clustering for scalable statistical analysis on structured images | http://arxiv.org/abs/1511.04898 | id:1511.04898 author:Bertrand Thirion, Andrés Hoyos-Idrobo, Jonas Kahn, Gael Varoquaux category:stat.ML cs.CV  published:2015-11-16 summary:The use of brain images as markers for diseases or behavioral differences is challenged by the small effects size and the ensuing lack of power, an issue that has incited researchers to rely more systematically on large cohorts. Coupled with resolution increases, this leads to very large datasets. A striking example in the case of brain imaging is that of the Human Connectome Project: 20 Terabytes of data and growing. The resulting data deluge poses severe challenges regarding the tractability of some processing steps (discriminant analysis, multivariate models) due to the memory demands posed by these data. In this work, we revisit dimension reduction approaches, such as random projections, with the aim of replacing costly function evaluations by cheaper ones while decreasing the memory requirements. Specifically, we investigate the use of alternate schemes, based on fast clustering, that are well suited for signals exhibiting a strong spatial structure, such as anatomical and functional brain images. Our contribution is twofold: i) we propose a linear-time clustering scheme that bypasses the percolation issues inherent in these algorithms and thus provides compressions nearly as good as traditional quadratic-complexity variance-minimizing clustering schemes, ii) we show that cluster-based compression can have the virtuous effect of removing high-frequency noise, actually improving subsequent estimations steps. As a consequence, the proposed approach yields very accurate models on several large-scale problems yet with impressive gains in computational efficiency, making it possible to analyze large datasets. version:1
arxiv-1511-04211 | Active Contextual Entropy Search | http://arxiv.org/abs/1511.04211 | id:1511.04211 author:Jan Hendrik Metzen category:stat.ML cs.LG  published:2015-11-13 summary:Contextual policy search allows adapting robotic movement primitives to different situations. For instance, a locomotion primitive might be adapted to different terrain inclinations or desired walking speeds. Such an adaptation is often achievable by modifying a small number of hyperparameters. However, learning, when performed on real robotic systems, is typically restricted to a small number of trials. Bayesian optimization has recently been proposed as a sample-efficient means for contextual policy search that is well suited under these conditions. In this work, we extend entropy search, a variant of Bayesian optimization, such that it can be used for active contextual policy search where the agent selects those tasks during training in which it expects to learn the most. Empirical results in simulation suggest that this allows learning successful behavior with less trials. version:2
arxiv-1305-6387 | Higher-order Segmentation via Multicuts | http://arxiv.org/abs/1305.6387 | id:1305.6387 author:Joerg Hendrik Kappes, Markus Speth, Gerhard Reinelt, Christoph Schnoerr category:cs.CV  published:2013-05-28 summary:Multicuts enable to conveniently represent discrete graphical models for unsupervised and supervised image segmentation, in the case of local energy functions that exhibit symmetries. The basic Potts model and natural extensions thereof to higher-order models provide a prominent class of such objectives, that cover a broad range of segmentation problems relevant to image analysis and computer vision. We exhibit a way to systematically take into account such higher-order terms for computational inference. Furthermore, we present results of a comprehensive and competitive numerical evaluation of a variety of dedicated cutting-plane algorithms. Our approach enables the globally optimal evaluation of a significant subset of these models, without compromising runtime. Polynomially solvable relaxations are studied as well, along with advanced rounding schemes for post-processing. version:3
arxiv-1511-03042 | Analyzing Stability of Convolutional Neural Networks in the Frequency Domain | http://arxiv.org/abs/1511.03042 | id:1511.03042 author:Elnaz J. Heravi, Hamed H. Aghdam, Domenec Puig category:cs.CV  published:2015-11-10 summary:Understanding the internal process of ConvNets is commonly done using visualization techniques. However, these techniques do not usually provide a tool for estimating the stability of a ConvNet against noise. In this paper, we show how to analyze a ConvNet in the frequency domain using a 4-dimensional visualization technique. Using the frequency domain analysis, we show the reason that a ConvNet might be sensitive to a very low magnitude additive noise. Our experiments on a few ConvNets trained on different datasets revealed that convolution kernels of a trained ConvNet usually pass most of the frequencies and they are not able to effectively eliminate the effect of high frequencies. Our next experiments shows that a convolution kernel which has a more concentrated frequency response could be more stable. Finally, we show that fine-tuning a ConvNet using a training set augmented with noisy images can produce more stable ConvNets. version:2
arxiv-1511-04855 | Deep Learning for steganalysis is better than a Rich Model with an Ensemble Classifier, and is natively robust to the cover source-mismatch | http://arxiv.org/abs/1511.04855 | id:1511.04855 author:Lionel Pibre, Pasquet Jérôme, Dino Ienco, Marc Chaumont category:cs.MM cs.CV cs.LG cs.NE  published:2015-11-16 summary:Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best " shape " of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained. version:1
arxiv-1511-04817 | Probabilistic Segmentation via Total Variation Regularization | http://arxiv.org/abs/1511.04817 | id:1511.04817 author:Matt Wytock, J. Zico Kolter category:stat.ML  published:2015-11-16 summary:We present a convex approach to probabilistic segmentation and modeling of time series data. Our approach builds upon recent advances in multivariate total variation regularization, and seeks to learn a separate set of parameters for the distribution over the observations at each time point, but with an additional penalty that encourages the parameters to remain constant over time. We propose efficient optimization methods for solving the resulting (large) optimization problems, and a two-stage procedure for estimating recurring clusters under such models, based upon kernel density estimation. Finally, we show on a number of real-world segmentation tasks, the resulting methods often perform as well or better than existing latent variable models, while being substantially easier to train. version:1
arxiv-1511-01042 | Detecting Interrogative Utterances with Recurrent Neural Networks | http://arxiv.org/abs/1511.01042 | id:1511.01042 author:Junyoung Chung, Jacob Devlin, Hany Hassan Awadalla category:cs.CL cs.LG cs.NE  published:2015-11-03 summary:In this paper, we explore different neural network architectures that can predict if a speaker of a given utterance is asking a question or making a statement. We com- pare the outcomes of regularization methods that are popularly used to train deep neural networks and study how different context functions can affect the classification performance. We also compare the efficacy of gated activation functions that are favorably used in recurrent neural networks and study how to combine multimodal inputs. We evaluate our models on two multimodal datasets: MSR-Skype and CALLHOME. version:2
arxiv-1511-04808 | Learning Mid-level Words on Riemannian Manifold for Action Recognition | http://arxiv.org/abs/1511.04808 | id:1511.04808 author:Mengyi Liu, Ruiping Wang, Shiguang Shan, Xilin Chen category:cs.CV  published:2015-11-16 summary:Human action recognition remains a challenging task due to the various sources of video data and large intra-class variations. It thus becomes one of the key issues in recent research to explore effective and robust representation to handle such challenges. In this paper, we propose a novel representation approach by constructing mid-level words in videos and encoding them on Riemannian manifold. Specifically, we first conduct a global alignment on the densely extracted low-level features to build a bank of corresponding feature groups, each of which can be statistically modeled as a mid-level word lying on some specific Riemannian manifold. Based on these mid-level words, we construct intrinsic Riemannian codebooks by employing K-Karcher-means clustering and Riemannian Gaussian Mixture Model, and consequently extend the Riemannian manifold version of three well studied encoding methods in Euclidean space, i.e. Bag of Visual Words (BoVW), Vector of Locally Aggregated Descriptors (VLAD), and Fisher Vector (FV), to obtain the final action video representations. Our method is evaluated in two tasks on four popular realistic datasets: action recognition on YouTube, UCF50, HMDB51 databases, and action similarity labeling on ASLAN database. In all cases, the reported results achieve very competitive performance with those most recent state-of-the-art works. version:1
arxiv-1511-04798 | Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization | http://arxiv.org/abs/1511.04798 | id:1511.04798 author:Baohan Xu, Yanwei Fu, Yu-Gang Jiang, Boyang Li, Leonid Sigal category:cs.CV cs.AI cs.MM  published:2015-11-16 summary:Emotional content is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames that express emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in video emotion understanding: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpus for zero-shot \pl{recognition} of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework. version:1
arxiv-1511-04780 | Causal interpretation rules for encoding and decoding models in neuroimaging | http://arxiv.org/abs/1511.04780 | id:1511.04780 author:Sebastian Weichwald, Timm Meyer, Ozan Özdenizci, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML cs.LG q-bio.NC stat.AP  published:2015-11-15 summary:Causal terminology is often introduced in the interpretation of encoding and decoding models trained on neuroimaging data. In this article, we investigate which causal statements are warranted and which ones are not supported by empirical evidence. We argue that the distinction between encoding and decoding models is not sufficient for this purpose: relevant features in encoding and decoding models carry a different meaning in stimulus- and in response-based experimental paradigms. We show that only encoding models in the stimulus-based setting support unambiguous causal interpretations. By combining encoding and decoding models trained on the same data, however, we obtain insights into causal relations beyond those that are implied by each individual model type. We illustrate the empirical relevance of our theoretical findings on EEG data recorded during a visuo-motor learning task. version:1
arxiv-1511-04775 | Expressive recommender systems through normalized nonnegative models | http://arxiv.org/abs/1511.04775 | id:1511.04775 author:Cyril Stark category:cs.LG stat.ML  published:2015-11-15 summary:We introduce normalized nonnegative models (NNM) for explorative data analysis. NNMs are partial convexifications of models from probability theory. We demonstrate their value at the example of item recommendation. We show that NNM-based recommender systems satisfy three criteria that all recommender systems should ideally satisfy: high predictive power, computational tractability, and expressive representations of users and items. Expressive user and item representations are important in practice to succinctly summarize the pool of customers and the pool of items. In NNMs, user representations are expressive because each user's preference can be regarded as normalized mixture of preferences of stereotypical users. The interpretability of item and user representations allow us to arrange properties of items (e.g., genres of movies or topics of documents) or users (e.g., personality traits) hierarchically. version:1
arxiv-1511-04695 | An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors | http://arxiv.org/abs/1511.04695 | id:1511.04695 author:Linxiao Yang, Jun Fang, Hongbin Li, Bing Zeng category:cs.NA cs.LG  published:2015-11-15 summary:We consider the problem of low-rank decomposition of incomplete multiway tensors. Since many real-world data lie on an intrinsically low dimensional subspace, tensor low-rank decomposition with missing entries has applications in many data analysis problems such as recommender systems and image inpainting. In this paper, we focus on Tucker decomposition which represents an Nth-order tensor in terms of N factor matrices and a core tensor via multilinear operations. To exploit the underlying multilinear low-rank structure in high-dimensional datasets, we propose a group-based log-sum penalty functional to place structural sparsity over the core tensor, which leads to a compact representation with smallest core tensor. The method for Tucker decomposition is developed by iteratively minimizing a surrogate function that majorizes the original objective function, which results in an iterative reweighted process. In addition, to reduce the computational complexity, an over-relaxed monotone fast iterative shrinkage-thresholding technique is adapted and embedded in the iterative reweighted process. The proposed method is able to determine the model complexity (i.e. multilinear rank) in an automatic way. Simulation results show that the proposed algorithm offers competitive performance compared with other existing algorithms. version:1
arxiv-1511-04687 | Separation Surfaces in the Spectral TV Domain for Texture Decomposition | http://arxiv.org/abs/1511.04687 | id:1511.04687 author:Dikla Horesh, Guy Gilboa category:cs.CV math.SP  published:2015-11-15 summary:In this paper we introduce a novel notion of separation surfaces for image decomposition. A surface is embedded in the spectral total-variation (TV) three dimensional domain and encodes a spatially-varying separation scale. The method allows good separation of textures with gradually varying pattern-size, pattern-contrast or illumination. The recently proposed total variation spectral framework is used to decompose the image into a continuum of textural scales. A desired texture, within a scale range, is found by fitting a surface to the local maximal responses in the spectral domain. A band above and below the surface, referred to as the \textit{Texture Stratum}, defines for each pixel the adaptive scale-range of the texture. Based on the decomposition an application is proposed which can attenuate or enhance textures in the image in a very natural and visually convincing manner. version:1
arxiv-1511-04685 | Semi-Inner-Products for Convex Functionals and Their Use in Image Decomposition | http://arxiv.org/abs/1511.04685 | id:1511.04685 author:Guy Gilboa category:math.NA cs.CV math.SP  published:2015-11-15 summary:Semi-inner-products in the sense of Lumer are extended to convex functionals. This yields a Hilbert-space like structure to convex functionals in Banach spaces. In particular, a general expression for semi-inner-products with respect to one homogeneous functionals is given. Thus one can use the new operator for the analysis of total variation and higher order functionals like total-generalized-variation (TGV). Having a semi-inner-product, an angle between functions can be defined in a straightforward manner. It is shown that in the one homogeneous case the Bregman distance can be expressed in terms of this newly defined angle. In addition, properties of the semi-inner-product of nonlinear eigenfunctions induced by the functional are derived. We use this construction to state a sufficient condition for a perfect decomposition of two signals and suggest numerical measures which indicate when those conditions are approximately met. version:1
arxiv-1502-07257 | Breaking Sticks and Ambiguities with Adaptive Skip-gram | http://arxiv.org/abs/1502.07257 | id:1502.07257 author:Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, Dmitry Vetrov category:cs.CL  published:2015-02-25 summary:Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task. version:2
arxiv-1511-04670 | Uncovering Temporal Context for Video Question and Answering | http://arxiv.org/abs/1511.04670 | id:1511.04670 author:Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G. Hauptmann category:cs.CV  published:2015-11-15 summary:In this work, we introduce Video Question Answering in temporal domain to infer the past, describe the present and predict the future. We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions. We explore approaches for finer understanding of video content using question form of "fill-in-the-blank", and managed to collect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD, MEDTest 14 datasets, while the corresponding 390,744 questions are generated from annotations. Extensive experiments demonstrate that our approach significantly outperforms the compared baselines. version:1
arxiv-1511-04664 | Deep Activity Recognition Models with Triaxial Accelerometers | http://arxiv.org/abs/1511.04664 | id:1511.04664 author:Mohammad Abu Alsheikh, Ahmed Selim, Dusit Niyato, Linda Doyle, Shaowei Lin, Hwee-Pink Tan category:cs.LG cs.HC cs.NE  published:2015-11-15 summary:Despite the widespread installation of accelerometers in almost all mobile phones and wearable devices, activity recognition using accelerometers is still immature due to the poor recognition accuracy of existing recognition methods and the scarcity of labeled training data. We consider the problem of human activity recognition using triaxial accelerometers and deep learning paradigms. This paper shows that deep activity recognition models (a) provide better recognition accuracy of human activities, (b) avoid the expensive design of handcrafted features in existing systems, and (c) utilize the massive unlabeled acceleration samples for unsupervised feature extraction. Moreover, a hybrid approach of deep learning and hidden Markov models (DL-HMM) is presented for sequential activity recognition. This hybrid approach integrates the hierarchical representations of deep activity recognition models with the stochastic modeling of temporal sequences in the hidden Markov models. We show substantial recognition improvement on real world datasets over state-of-the-art methods of human activity recognition using triaxial accelerometers. version:1
arxiv-1511-04661 | A System for Extracting Sentiment from Large-Scale Arabic Social Data | http://arxiv.org/abs/1511.04661 | id:1511.04661 author:Hao Wang, Vijay R. Bommireddipalli, Ayman Hanafy, Mohamed Bahgat, Sara Noeman, Ossama S. Emam category:cs.CL  published:2015-11-15 summary:Social media data in Arabic language is becoming more and more abundant. It is a consensus that valuable information lies in social media data. Mining this data and making the process easier are gaining momentum in the industries. This paper describes an enterprise system we developed for extracting sentiment from large volumes of social data in Arabic dialects. First, we give an overview of the Big Data system for information extraction from multilingual social data from a variety of sources. Then, we focus on the Arabic sentiment analysis capability that was built on top of the system including normalizing written Arabic dialects, building sentiment lexicons, sentiment classification, and performance evaluation. Lastly, we demonstrate the value of enriching sentiment results with user profiles in understanding sentiments of a specific user group. version:1
arxiv-1511-04659 | Implementation and comparative quantitative assessment of different multispectral image pansharpening approches | http://arxiv.org/abs/1511.04659 | id:1511.04659 author:Shailesh Panchal, Rajesh Thakker category:cs.CV  published:2015-11-15 summary:In remote sensing, images acquired by various earth observation satellites tend to have either a high spatial and low spectral resolution or vice versa. Pansharpening is a technique which aims to improve spatial resolution of multispectral image. The challenges involve in the pansharpening are not only to improve the spatial resolution but also to preserve spectral quality of the multispectral image. In this paper, various pansharpening algorithms are discussed and classified based on approaches they have adopted. Using MATLAB image processing toolbox, several state-of-art pan-sharpening algorithms are implemented. Quality of pansharpened images are assessed visually and quantitatively. Correlation coefficient (CC), Root mean square error (RMSE), Relative average spectral error (RASE) and Universal quality index (Q) indices are used to easure spectral quality while to spatial-CC (SCC) quantitative parameter is used for spatial quality measurement. Finally, the paper is concluded with useful remarks. version:1
arxiv-1511-04646 | Word Embedding based Correlation Model for Question/Answer Matching | http://arxiv.org/abs/1511.04646 | id:1511.04646 author:Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng, Jie Tang, Zhang Xiong category:cs.CL cs.AI  published:2015-11-15 summary:With the development of community based question answering (Q\&A) services, a large scale of Q\&A archives have been accumulated and are an important information and knowledge resource on the web. Question and answer matching has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, we try to improve the matching accuracy by overcoming the lexical gap between question and answer pairs. A Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding, given a random pair of words, WEC can score their co-occurrence probability in Q\&A pairs and it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method's promising potential. version:1
arxiv-1511-04587 | Accurate Image Super-Resolution Using Very Deep Convolutional Networks | http://arxiv.org/abs/1511.04587 | id:1511.04587 author:Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee category:cs.CV  published:2015-11-14 summary:We present a highly accurate single-image super-resolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification \cite{simonyan2015very}. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates ($10^4$ times higher than SRCNN \cite{dong2015image}) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable. version:1
arxiv-1511-04586 | Character-based Neural Machine Translation | http://arxiv.org/abs/1511.04586 | id:1511.04586 author:Wang Ling, Isabel Trancoso, Chris Dyer, Alan W Black category:cs.CL  published:2015-11-14 summary:We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models. version:1
arxiv-1511-05133 | Fast Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting | http://arxiv.org/abs/1511.05133 | id:1511.05133 author:Canyi Lu, Huan Li, Zhouchen Lin, Shuicheng Yan category:math.OC cs.LG cs.NA  published:2015-11-14 summary:The Augmented Lagragian Method (ALM) and Alternating Direction Method of Multiplier (ADMM) have been powerful optimization methods for general convex programming subject to linear constraint. We consider the convex problem whose objective consists of a smooth part and a nonsmooth but simple part. We propose the Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves the convergence rate $O(1/K^2)$, compared with $O(1/K)$ by the traditional PALM. In order to further reduce the per-iteration complexity and handle the multi-blocks problem, we propose the Fast Proximal ADMM with Parallel Splitting (Fast PL-ADMM-PS) method. It also partially improves the rate related to the smooth part of the objective function. Experimental results on both synthesized and real world data demonstrate that our fast methods significantly improve the previous PALM and ADMM. version:1
arxiv-1511-04534 | Learning Fine-grained Features via a CNN Tree for Large-scale Classification | http://arxiv.org/abs/1511.04534 | id:1511.04534 author:Zhenhua Wang, Xingxing Wang, Gang Wang category:cs.CV  published:2015-11-14 summary:We propose a novel approach to enhance the discriminability of Convolutional Neural Networks (CNN). The key idea is to build a tree structure that could progressively learn fine-grained features to distinguish a subset of classes, by learning features only among these classes. Such features are expected to be more discriminative, compared to features learned for all the classes. We develop a new algorithm to effectively learn the tree structure among a large number of classes. Experiments on large-scale image classification tasks demonstrate that our method could boost the performance of a given basic CNN model. Our method is quite general, hence it can potentially be used in combination with many other deep learning models. version:1
arxiv-1511-04514 | Sparse Nonlinear Regression: Parameter Estimation and Asymptotic Inference | http://arxiv.org/abs/1511.04514 | id:1511.04514 author:Zhuoran Yang, Zhaoran Wang, Han Liu, Yonina C. Eldar, Tong Zhang category:stat.ML cs.IT cs.LG math.IT math.OC  published:2015-11-14 summary:We study parameter estimation and asymptotic inference for sparse nonlinear regression. More specifically, we assume the data are given by $y = f( x^\top \beta^* ) + \epsilon$, where $f$ is nonlinear. To recover $\beta^*$, we propose an $\ell_1$-regularized least-squares estimator. Unlike classical linear regression, the corresponding optimization problem is nonconvex because of the nonlinearity of $f$. In spite of the nonconvexity, we prove that under mild conditions, every stationary point of the objective enjoys an optimal statistical rate of convergence. In addition, we provide an efficient algorithm that provably converges to a stationary point. We also access the uncertainty of the obtained estimator. Specifically, based on any stationary point of the objective, we construct valid hypothesis tests and confidence intervals for the low dimensional components of the high-dimensional parameter $\beta^*$. Detailed numerical results are provided to back up our theory. version:1
arxiv-1511-04511 | BING++: A Fast High Quality Object Proposal Generator at 100fps | http://arxiv.org/abs/1511.04511 | id:1511.04511 author:Ziming Zhang, Yun Liu, Tolga Bolukbasi, Ming-Ming Cheng, Venkatesh Saligrama category:cs.CV  published:2015-11-14 summary:We are motivated by the need for an object proposal generation algorithm that achieves a good balance between proposal localization quality, object recall and computational efficiency. We propose a novel object proposal algorithm {\em BING++} which inherits the good computational efficiency of BING \cite{BingObj2014} but significantly improves its proposal localization quality. Central to our success is based on the observation that good bounding boxes are those that tightly cover objects. Edge features, which can be computed efficiently, play a critical role in this context. We propose a new algorithm that recursively improves BING's proposals by exploiting the fact that edges in images are typically associated with object boundaries. BING++ improves proposals recursively by incorporating nearest edge points (to proposal boundary pixels) to obtain a tighter bounding box. This operation has linear computational complexity in number of pixels and can be done efficiently using distance transform. Superpixel merging techniques are then employed as post-processing to further improve the proposal quality. Empirically on the VOC2007 dataset, using $10^3$ proposals and IoU threshold 0.5, our method achieves 95.3\% object detection recall (DR), 79.2\% mean average best overlap (MABO), and 68.7\% mean average precision (mAP) on object detection over 20 object classes within an average time of {\bf 0.009} seconds per image. version:1
arxiv-1511-04510 | Semantic Object Parsing with Local-Global Long Short-Term Memory | http://arxiv.org/abs/1511.04510 | id:1511.04510 author:Xiaodan Liang, Xiaohui Shen, Donglai Xiang, Jiashi Feng, Liang Lin, Shuicheng Yan category:cs.CV  published:2015-11-14 summary:Semantic object parsing is a fundamental task for understanding objects in detail in computer vision community, where incorporating multi-level contextual information is critical for achieving such fine-grained pixel-level recognition. Prior methods often leverage the contextual information through post-processing predicted confidence maps. In this work, we propose a novel deep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly incorporate short-distance and long-distance spatial dependencies into the feature learning over all pixel positions. In each LG-LSTM layer, local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information. Individual LSTMs for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images, yielding distinct hidden and memory cells of each position for each dimension. In our parsing approach, several LG-LSTM layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features, allowing network parameters to be learned in an end-to-end way. The long chains of sequential computation by stacked LG-LSTM layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions. Comprehensive evaluations on three public datasets well demonstrate the significant superiority of our LG-LSTM over other state-of-the-art methods. version:1
arxiv-1511-04491 | Deeply-Recursive Convolutional Network for Image Super-Resolution | http://arxiv.org/abs/1511.04491 | id:1511.04491 author:Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee category:cs.CV  published:2015-11-14 summary:We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin. version:1
arxiv-1511-04484 | Unsupervised Learning in Synaptic Sampling Machines | http://arxiv.org/abs/1511.04484 | id:1511.04484 author:Emre O. Neftci, Bruno U. Pedroni, Siddharth Joshi, Maruan Al-Shedivat, Gert Cauwenberghs category:cs.NE  published:2015-11-14 summary:Recent studies have shown that synaptic unreliability is a robust and sufficient mechanism for inducing the stochasticity observed in cortex. Here, we introduce the Synaptic Sampling Machine (SSM), a stochastic neural network model that uses synaptic unreliability as a means to stochasticity for sampling. Synaptic unreliability plays the dual role of an efficient mechanism for sampling in neuromorphic hardware, and a regularizer during learning akin to DropConnect. Similar to the original formulation of Boltzmann machines, the SSM can be viewed as a stochastic counterpart of Hopfield networks, but where stochasticity is induced by a random mask over the connections. The SSM is trained to learn generative models with a synaptic plasticity rule implementing an event-driven form of contrastive divergence. We demonstrate this by learning a model of MNIST hand-written digit dataset, and by testing it in recognition and inference tasks. We find that SSMs outperform restricted Boltzmann machines (4.4% error rate vs. 5%), they are more robust to overfitting, and tend to learn sparser representations. SSMs are remarkably robust to weight pruning: removal of more than 80% of the weakest connections followed by cursory re-learning causes only a negligible performance loss on the MNIST task (4.8% error rate). These results show that SSMs offer substantial improvements in terms of performance, power and complexity over existing methods for unsupervised learning in spiking neural networks, and are thus promising models for machine learning in neuromorphic execution platforms. version:1
arxiv-1511-04472 | Solving Jigsaw Puzzles with Linear Programming | http://arxiv.org/abs/1511.04472 | id:1511.04472 author:Rui Yu, Chris Russell, Lourdes Agapito category:cs.CV  published:2015-11-13 summary:We propose a novel Linear Program (LP) based formula- tion for solving jigsaw puzzles. We formulate jigsaw solving as a set of successive global convex relaxations of the stan- dard NP-hard formulation, that can describe both jigsaws with pieces of unknown position and puzzles of unknown po- sition and orientation. The main contribution and strength of our approach comes from the LP assembly strategy. In contrast to existing greedy methods, our LP solver exploits all the pairwise matches simultaneously, and computes the position of each piece/component globally. The main ad- vantages of our LP approach include: (i) a reduced sensi- tivity to local minima compared to greedy approaches, since our successive approximations are global and convex and (ii) an increased robustness to the presence of mismatches in the pairwise matches due to the use of a weighted L1 penalty. To demonstrate the effectiveness of our approach, we test our algorithm on public jigsaw datasets and show that it outperforms state-of-the-art methods. version:1
arxiv-1511-04458 | Zero-Shot Action Recognition by Word-Vector Embedding | http://arxiv.org/abs/1511.04458 | id:1511.04458 author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV  published:2015-11-13 summary:The number of categories for action recognition is growing rapidly and it has become increasingly hard to label sufficient training data for learning conventional models for all categories. Instead of collecting ever more data and labelling them exhaustively for all categories, an attractive alternative approach is "zeroshot learning" (ZSL). To that end, in this study we construct a mapping between visual features and a semantic descriptor of each action category, allowing new categories to be recognised in the absence of any visual training data. Existing ZSL studies focus primarily on still images, and attribute-based semantic representations. In this work, we explore word-vectors as the shared semantic space to embed videos and category labels for ZSL action recognition. This is a more challenging problem than existing ZSL of still images and/or attributes, because the mapping between the semantic space and video space-time features of actions is more complex and harder to learn for the purpose of generalising over any cross-category domain shift. To solve this generalisation problem in ZSL action recognition, we investigate a series of synergistic improvements to the standard ZSL pipeline. First, we enhance significantly the semantic space mapping by proposing manifold-regularised regression and data augmentation strategies. Second, we evaluate two existing post processing strategies (transductive self-training and hubness correction), and show that they are complementary. We evaluate extensively our model on a wide range of human action datasets including HMDB51, UCF101, OlympicSports, CCV and TRECVID MED 13. The results demonstrate that our approach achieves the state-of-the-art zero-shot action recognition performance with a simple and efficient pipeline, and without supervised annotation of attributes. version:1
arxiv-1511-04412 | Dynamic Sum Product Networks for Tractable Inference on Sequence Data | http://arxiv.org/abs/1511.04412 | id:1511.04412 author:Mazen Melibari, Pascal Poupart, Prashant Doshi category:cs.LG cs.AI stat.ML  published:2015-11-13 summary:Sum-Product Networks (SPN) have recently emerged as a new class of tractable probabilistic graphical models. Unlike Bayesian networks and Markov networks where inference may be exponential in the size of the network, inference in SPNs is in time linear in the size of the network. Since SPNs represent distributions over a fixed set of variables only, we propose dynamic sum product networks (DSPNs) as a generalization of SPNs for sequence data of varying length. A DSPN consists of a template network that is repeated as many times as needed to model data sequences of any length. We present a local search technique to learn the structure of the template network. In contrast to dynamic Bayesian networks for which inference is generally exponential in the number of variables per time slice, DSPNs inherit the linear inference complexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other models on several datasets of sequence data. version:1
arxiv-1511-04408 | Scalable Gaussian Processes for Characterizing Multidimensional Change Surfaces | http://arxiv.org/abs/1511.04408 | id:1511.04408 author:William Herlands, Andrew Wilson, Hannes Nickisch, Seth Flaxman, Daniel Neill, Wilbert van Panhuis, Eric Xing category:stat.ML  published:2015-11-13 summary:We present a scalable Gaussian process model for identifying and characterizing smooth multidimensional changepoints, and automatically learning changes in expressive covariance structure. We use Random Kitchen Sink features to flexibly define a change surface in combination with expressive spectral mixture kernels to capture the complex statistical structure. Finally, through the use of novel methods for additive non-separable kernels, we can scale the model to large datasets. We demonstrate the model on numerical and real world data, including a large spatio-temporal disease dataset where we identify previously unknown heterogeneous changes in space and time. version:1
arxiv-1511-04404 | Robust Face Alignment Using a Mixture of Invariant Experts | http://arxiv.org/abs/1511.04404 | id:1511.04404 author:Oncel Tuzel, Salil Tambe, Tim K. Marks category:cs.CV  published:2015-11-13 summary:Face alignment, which is the task of finding the locations of a set of facial landmark points in an image of a face, is an important problem that is useful in widespread application areas. Face alignment is particularly challenging when there are large variations in pose (in-plane and out-of-plane rotations) and facial expression. To address this issue, we propose a cascade in which each stage consists of a mixture of regression experts. Each expert learns a customized regression model that is specialized to a different subset of the joint space of pose and expressions. The system is invariant to a predefined class of transformations (e.g., affine), because the input is transformed to match each expert's prototype shape before the regression is applied. We also present a method to include deformation constraints within the discriminative alignment framework, which makes our algorithm more robust. Results show that our algorithm significantly outperforms previous methods on publicly available face alignment datasets. version:1
arxiv-1511-04387 | Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode Resource-constrained Multi-project Scheduling Problem | http://arxiv.org/abs/1511.04387 | id:1511.04387 author:Shahriar Asta, Daniel Karapetyan, Ahmed Kheiri, Ender Özcan, Andrew J. Parkes category:cs.DS cs.AI cs.NE  published:2015-11-13 summary:Multi-mode resource and precedence-constrained project scheduling is a well-known challenging real-world optimisation problem. An important variant of the problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints. This problem has been addressed by a competition, and associated set of benchmark instances, as a part of the MISTA 2013 conference. A critical aspect of the benchmarks is that the primary objective is to minimise the sum of the project completion times, with the usual makespan minimisation as a secondary objective. We observe that this leads to an expected different overall structure of good solutions and discuss the effects this has on the algorithm design. This paper presents the resulting competition winning approach; it is a carefully designed hybrid of Monte-Carlo tree search, novel neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The implementation is also engineered to increase the speed with which iterations are performed, and to exploit the computing power of multicore machines. The resulting information-sharing multi-component algorithm significantly outperformed the other approaches during the competition, producing the best solution for 17 out of the 20 test instances and performing the best in around 90% of all the trials. version:1
arxiv-1511-04384 | Deep Reflectance Maps | http://arxiv.org/abs/1511.04384 | id:1511.04384 author:Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves, Tinne Tuytelaars category:cs.CV  published:2015-11-13 summary:Undoing the image formation process and therefore decomposing appearance into its intrinsic properties is a challenging task due to the under-constraint nature of this inverse problem. While significant progress has been made on inferring shape, materials and illumination from images only, progress in an unconstrained setting is still limited. We propose a convolutional neural architecture to estimate reflectance maps of specular materials in natural lighting conditions. We achieve this in an end-to-end learning formulation that directly predicts a reflectance map from the image itself. We show how to improve estimates by facilitating additional supervision in an indirect scheme that first predicts surface orientation and afterwards predicts the reflectance map by a learning-based sparse data interpolation. In order to analyze performance on this difficult task, we propose a new challenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg) using both synthetic and real images. Furthermore, we show the application of our method to a range of image-based editing tasks on real images. version:1
arxiv-1511-04348 | Large Scale Artificial Neural Network Training Using Multi-GPUs | http://arxiv.org/abs/1511.04348 | id:1511.04348 author:Linnan Wang, Wei Wu, Jianxiong Xiao, Yang Yi category:cs.DC cs.NE  published:2015-11-13 summary:This paper describes a method for accelerating large scale Artificial Neural Networks (ANN) training using multi-GPUs by reducing the forward and backward passes to matrix multiplication. We propose an out-of-core multi-GPU matrix multiplication and integrate the algorithm with the ANN training. The experiments demonstrate that our matrix multiplication algorithm achieves linear speedup on multiple inhomogeneous GPUs. The full paper of this project can be found at [1]. version:1
arxiv-1511-04320 | Standard methods for inexpensive pollen loads authentication by means of computer vision and machine learning | http://arxiv.org/abs/1511.04320 | id:1511.04320 author:Manuel Chica, Pascual Campoy category:cs.CV  published:2015-11-13 summary:We present a complete methodology for authenticating local bee pollen against fraudulent samples using image processing and machine learning techniques. The proposed standard methods do not need expensive equipment such as advanced microscopes and can be used for a preliminary fast rejection of unknown pollen types. The system is able to rapidly reject the non-local pollen samples with inexpensive hardware and without the need to send the product to the laboratory. Methods are based on the color properties of bee pollen loads images and the use of one-class classifiers which are appropriate to reject unknown pollen samples when there is limited data about them. The validation of the method is carried out by authenticating Spanish bee pollen types. Experimentation shows that the proposed methods can obtain an overall authentication accuracy of 94%. We finally illustrate the user interaction with the software in some practical cases by showing the developed application prototype. version:1
arxiv-1506-01339 | Exploiting an Oracle that Reports AUC Scores in Machine Learning Contests | http://arxiv.org/abs/1506.01339 | id:1506.01339 author:Jacob Whitehill category:cs.LG  published:2015-06-03 summary:In machine learning contests such as the ImageNet Large Scale Visual Recognition Challenge and the KDD Cup, contestants can submit candidate solutions and receive from an oracle (typically the organizers of the competition) the accuracy of their guesses compared to the ground-truth labels. One of the most commonly used accuracy metrics for binary classification tasks is the Area Under the Receiver Operating Characteristics Curve (AUC). In this paper we provide proofs-of-concept of how knowledge of the AUC of a set of guesses can be used, in two different kinds of attacks, to improve the accuracy of those guesses. On the other hand, we also demonstrate the intractability of one kind of AUC exploit by proving that the number of possible binary labelings of $n$ examples for which a candidate solution obtains a AUC score of $c$ grows exponentially in $n$, for every $c\in (0,1)$. version:2
arxiv-1511-02462 | LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks | http://arxiv.org/abs/1511.02462 | id:1511.02462 author:Steven C. H. Hoi, Xiongwei Wu, Hantang Liu, Yue Wu, Huiqiong Wang, Hui Xue, Qiang Wu category:cs.CV  published:2015-11-08 summary:Logo detection from images has many applications, particularly for brand recognition and intellectual property protection. Most existing studies for logo recognition and detection are based on small-scale datasets which are not comprehensive enough when exploring emerging deep learning techniques. In this paper, we introduce "LOGO-Net", a large-scale logo image database for logo detection and brand recognition from real-world product images. To facilitate research, LOGO-Net has two datasets: (i)"logos-18" consists of 18 logo classes, 10 brands, and 16,043 logo objects, and (ii) "logos-160" consists of 160 logo classes, 100 brands, and 130,608 logo objects. We describe the ideas and challenges for constructing such a large-scale database. Another key contribution of this work is to apply emerging deep learning techniques for logo detection and brand recognition tasks, and conduct extensive experiments by exploring several state-of-the-art deep region-based convolutional networks techniques for object detection tasks. The LOGO-net will be released at http://logo-net.org/ version:2
arxiv-1408-5405 | Recurrent Neural Network Based Hybrid Model of Gene Regulatory Network | http://arxiv.org/abs/1408.5405 | id:1408.5405 author:Khalid Raza, Mansaf Alam category:cs.NE cs.CE q-bio.MN  published:2014-08-22 summary:Systems biology is an emerging interdisciplinary area of research that focuses on study of complex interactions in a biological system, such as gene regulatory networks. The discovery of gene regulatory networks leads to a wide range of applications, such as pathways related to a disease that can unveil in what way the disease acts and provide novel tentative drug targets. In addition, the development of biological models from discovered networks or pathways can help to predict the responses to disease and can be much useful for the novel drug development and treatments. The inference of regulatory networks from biological data is still in its infancy stage. This paper proposes a recurrent neural network (RNN) based gene regulatory network (GRN) model hybridized with generalized extended Kalman filter for weight update in backpropagation through time training algorithm. The RNN is a complex neural network that gives a better settlement between the biological closeness and mathematical flexibility to model GRN. The RNN is able to capture complex, non-linear and dynamic relationship among variables. Gene expression data are inherently noisy and Kalman filter performs well for estimation even in noisy data. Hence, non-linear version of Kalman filter, i.e., generalized extended Kalman filter has been applied for weight update during network training. The developed model has been applied on DNA SOS repair network, IRMA network, and two synthetic networks from DREAM Challenge. We compared our results with other state-of-the-art techniques that show superiority of our model. Further, 5% Gaussian noise has been added in the dataset and result of the proposed model shows negligible effect of noise on the results. version:2
arxiv-1511-04242 | Volume-based Semantic Labeling with Signed Distance Functions | http://arxiv.org/abs/1511.04242 | id:1511.04242 author:Tommaso Cavallari, Luigi Di Stefano category:cs.CV  published:2015-11-13 summary:Research works on the two topics of Semantic Segmentation and SLAM (Simultaneous Localization and Mapping) have been following separate tracks. Here, we link them quite tightly by delineating a category label fusion technique that allows for embedding semantic information into the dense map created by a volume-based SLAM algorithm such as KinectFusion. Accordingly, our approach is the first to provide a semantically labeled dense reconstruction of the environment from a stream of RGB-D images. We validate our proposal using a publicly available semantically annotated RGB-D dataset and a) employing ground truth labels, b) corrupting such annotations with synthetic noise, c) deploying a state of the art semantic segmentation algorithm based on Convolutional Neural Networks. version:1
arxiv-1511-04240 | An Adaptive Data Representation for Robust Point-Set Registration and Merging | http://arxiv.org/abs/1511.04240 | id:1511.04240 author:Dylan Campbell, Lars Petersson category:cs.CV  published:2015-11-13 summary:This paper presents a framework for rigid point-set registration and merging using a robust continuous data representation. Our point-set representation is constructed by training a one-class support vector machine with a Gaussian radial basis function kernel and subsequently approximating the output function with a Gaussian mixture model. We leverage the representation's sparse parametrisation and robustness to noise, outliers and occlusions in an efficient registration algorithm that minimises the L2 distance between our support vector--parametrised Gaussian mixtures. In contrast, existing techniques, such as Iterative Closest Point and Gaussian mixture approaches, manifest a narrower region of convergence and are less robust to occlusions and missing data, as demonstrated in the evaluation on a range of 2D and 3D datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously and equitably merges aligned mixture models, allowing the framework to be used for reconstruction and mapping. version:1
arxiv-1506-00333 | Learning to Answer Questions From Image Using Convolutional Neural Network | http://arxiv.org/abs/1506.00333 | id:1506.00333 author:Lin Ma, Zhengdong Lu, Hang Li category:cs.CL cs.CV cs.LG cs.NE  published:2015-06-01 summary:In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art. version:2
arxiv-1511-00213 | Large-scale probabilistic predictors with and without guarantees of validity | http://arxiv.org/abs/1511.00213 | id:1511.00213 author:Vladimir Vovk, Ivan Petej, Valentina Fedorova category:cs.LG 68T05  published:2015-11-01 summary:This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies. version:2
arxiv-1511-03774 | On the Optimal Sample Complexity for Best Arm Identification | http://arxiv.org/abs/1511.03774 | id:1511.03774 author:Lijie Chen, Jian Li category:cs.LG cs.DS  published:2015-11-12 summary:We study the best arm identification (BEST-1-ARM) problem, which is defined as follows. We are given $n$ stochastic bandit arms. The $i$th arm has a reward distribution $D_i$ with an unknown mean $\mu_i$. Upon each play of the $i$th arm, we can get a reward, sampled i.i.d. from $D_i$. We would like to identify the arm with largest mean with probability at least $1-\delta$, using as few samples as possible. We also study an important special case where there are only two arms, which we call the sign problem. We achieve a very detailed understanding of the optimal sample complexity of sign, simplifying and significantly extending a classical result by Farrell in 1964, with a completely new proof. Using the new lower bound for sign, we obtain the first lower bound for BEST-1-ARM that goes beyond the classic Mannor-Tsitsiklis lower bound, by an interesting reduction from sign to BEST-1-ARM. To complement our lower bound, we also provide a nontrivial algorithm for BEST-1-ARM, which achieves a worst case optimal sample complexity, improving upon several prior upper bounds on the same problem. version:2
arxiv-1511-03791 | Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control | http://arxiv.org/abs/1511.03791 | id:1511.03791 author:Fangyi Zhang, Jürgen Leitner, Michael Milford, Ben Upcroft, Peter Corke category:cs.LG cs.CV cs.RO  published:2015-11-12 summary:This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images. version:2
arxiv-1511-04157 | $k$-means: Fighting against Degeneracy in Sequential Monte Carlo with an Application to Tracking | http://arxiv.org/abs/1511.04157 | id:1511.04157 author:Kai Fan, Katherine Heller category:stat.ML  published:2015-11-13 summary:For regular particle filter algorithm or Sequential Monte Carlo (SMC) methods, the initial weights are traditionally dependent on the proposed distribution, the posterior distribution at the current timestamp in the sampled sequence, and the target is the posterior distribution of the previous timestamp. This is technically correct, but leads to algorithms which usually have practical issues with degeneracy, where all particles eventually collapse onto a single particle. In this paper, we propose and evaluate using $k$ means clustering to attack and even take advantage of this degeneracy. Specifically, we propose a Stochastic SMC algorithm which initializes the set of $k$ means, providing the initial centers chosen from the collapsed particles. To fight against degeneracy, we adjust the regular SMC weights, mediated by cluster proportions, and then correct them to retain the same expectation as before. We experimentally demonstrate that our approach has better performance than vanilla algorithms. version:1
arxiv-1511-04153 | Adaptive Affinity Matrix for Unsupervised Metric Learning | http://arxiv.org/abs/1511.04153 | id:1511.04153 author:Yaoyi Li, Junxuan Chen, Hongtao Lu category:cs.CV cs.LG  published:2015-11-13 summary:Spectral clustering is one of the most popular clustering approaches with the capability to handle some challenging clustering problems. Most spectral clustering methods provide a nonlinear map from the data manifold to a subspace. Only a little work focuses on the explicit linear map which can be viewed as the unsupervised distance metric learning. In practice, the selection of the affinity matrix exhibits a tremendous impact on the unsupervised learning. While much success of affinity learning has been achieved in recent years, some issues such as noise reduction remain to be addressed. In this paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to learn an adaptive affinity matrix and derive a distance metric from the affinity. We assume the affinity matrix to be positive semidefinite with ability to quantify the pairwise dissimilarity. Our method is based on posing the optimization of objective function as a spectral decomposition problem. We yield the affinity from both the original data distribution and the widely-used heat kernel. The provided matrix can be regarded as the optimal representation of pairwise relationship on the manifold. Extensive experiments on a number of real-world data sets show the effectiveness and efficiency of AdaAM. version:1
arxiv-1511-04150 | Deep Mean Maps | http://arxiv.org/abs/1511.04150 | id:1511.04150 author:Junier B. Oliva, Dougal J. Sutherland, Barnabás Póczos, Jeff Schneider category:stat.ML cs.CV cs.LG  published:2015-11-13 summary:The use of distributions and high-level features from deep architecture has become commonplace in modern computer vision. Both of these methodologies have separately achieved a great deal of success in many computer vision tasks. However, there has been little work attempting to leverage the power of these to methodologies jointly. To this end, this paper presents the Deep Mean Maps (DMMs) framework, a novel family of methods to non-parametrically represent distributions of features in convolutional neural network models. DMMs are able to both classify images using the distribution of top-level features, and to tune the top-level features for performing this task. We show how to implement DMMs using a special mean map layer composed of typical CNN operations, making both forward and backward propagation simple. We illustrate the efficacy of DMMs at analyzing distributional patterns in image data in a synthetic data experiment. We also show that we extending existing deep architectures with DMMs improves the performance of existing CNNs on several challenging real-world datasets. version:1
arxiv-1511-04145 | A Continuous-time Mutually-Exciting Point Process Framework for Prioritizing Events in Social Media | http://arxiv.org/abs/1511.04145 | id:1511.04145 author:Mehrdad Farajtabar, Safoora Yousefi, Long Q. Tran, Le Song, Hongyuan Zha category:cs.SI cs.LG  published:2015-11-13 summary:The overwhelming amount and rate of information update in online social media is making it increasingly difficult for users to allocate their attention to their topics of interest, thus there is a strong need for prioritizing news feeds. The attractiveness of a post to a user depends on many complex contextual and temporal features of the post. For instance, the contents of the post, the responsiveness of a third user, and the age of the post may all have impact. So far, these static and dynamic features has not been incorporated in a unified framework to tackle the post prioritization problem. In this paper, we propose a novel approach for prioritizing posts based on a feature modulated multi-dimensional point process. Our model is able to simultaneously capture textual and sentiment features, and temporal features such as self-excitation, mutual-excitation and bursty nature of social interaction. As an evaluation, we also curated a real-world conversational benchmark dataset crawled from Facebook. In our experiments, we demonstrate that our algorithm is able to achieve the-state-of-the-art performance in terms of analyzing, predicting, and prioritizing events. In terms of interpretability of our method, we observe that features indicating individual user profile and linguistic characteristics of the events work best for prediction and prioritization of new events. version:1
arxiv-1510-09083 | Deep Cascaded Regression for Face Alignment | http://arxiv.org/abs/1510.09083 | id:1510.09083 author:Hanjiang Lai, Shengtao Xiao, Zhen Cui, Yan Pan, Chunyan Xu, Shuicheng Yan category:cs.CV  published:2015-10-30 summary:We propose a novel cascaded regression framework for face alignment based on a deep convolutional neural network (CNN). In most existing cascaded regression methods, the shape-indexed features are either obtained by hand-crafted visual descriptors or by leaning from the shallow models. This setting may be suboptimal for the face alignment task. To solve this problem, we propose an end-to-end CNN architecture to learn highly discriminative shape-indexed features. First, our deep architecture encodes the image into high-level feature maps in the same size of the image via three main operations: convolution, pooling and deconvolution. Then, we propose "Shape-Indexed Pooling" to extract the deep features from these high level descriptors. We refine the shape via sequential regressions by using the deep shape-indexed features, which demonstrates outstanding performance. We also propose to learn the probability mask for each landmark that can be used to choose the initialization from the shape space. Extensive evaluations conducted on several benchmark datasets demonstrate that the proposed deep framework shows significant improvement over the state-of-the-art methods. version:2
arxiv-1511-04110 | Going Deeper in Facial Expression Recognition using Deep Neural Networks | http://arxiv.org/abs/1511.04110 | id:1511.04110 author:Ali Mollahosseini, David Chan, Mohammad H. Mahoor category:cs.NE cs.CV  published:2015-11-12 summary:Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time. version:1
arxiv-1511-04067 | Deep Gaussian Conditional Random Field Network: A Model-based Deep Network for Discriminative Denoising | http://arxiv.org/abs/1511.04067 | id:1511.04067 author:Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu category:cs.CV  published:2015-11-12 summary:We propose a novel deep network architecture for image\\ denoising based on a Gaussian Conditional Random Field (GCRF) model. In contrast to the existing discriminative denoising methods that train a separate model for each noise level, the proposed deep network explicitly models the input noise variance and hence is capable of handling a range of noise levels. Our deep network, which we refer to as deep GCRF network, consists of two sub-networks: (i) a parameter generation network that generates the pairwise potential parameters based on the noisy input image, and (ii) an inference network whose layers perform the computations involved in an iterative GCRF inference procedure.\ We train the entire deep GCRF network (both parameter generation and inference networks) discriminatively in an end-to-end fashion by maximizing the peak signal-to-noise ratio measure. Experiments on Berkeley segmentation and PASCALVOC datasets show that the proposed deep GCRF network outperforms state-of-the-art image denoising approaches for several noise levels. version:1
arxiv-1511-04066 | Properly Learning Poisson Binomial Distributions in Almost Polynomial Time | http://arxiv.org/abs/1511.04066 | id:1511.04066 author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.LG math.ST stat.TH  published:2015-11-12 summary:We give an algorithm for properly learning Poisson binomial distributions. A Poisson binomial distribution (PBD) of order $n$ is the discrete probability distribution of the sum of $n$ mutually independent Bernoulli random variables. Given $\widetilde{O}(1/\epsilon^2)$ samples from an unknown PBD $\mathbf{p}$, our algorithm runs in time $(1/\epsilon)^{O(\log \log (1/\epsilon))}$, and outputs a hypothesis PBD that is $\epsilon$-close to $\mathbf{p}$ in total variation distance. The previously best known running time for properly learning PBDs was $(1/\epsilon)^{O(\log(1/\epsilon))}$. As one of our main contributions, we provide a novel structural characterization of PBDs. We prove that, for all $\epsilon >0,$ there exists an explicit collection $\cal{M}$ of $(1/\epsilon)^{O(\log \log (1/\epsilon))}$ vectors of multiplicities, such that for any PBD $\mathbf{p}$ there exists a PBD $\mathbf{q}$ with $O(\log(1/\epsilon))$ distinct parameters whose multiplicities are given by some element of ${\cal M}$, such that $\mathbf{q}$ is $\epsilon$-close to $\mathbf{p}$. Our proof combines tools from Fourier analysis and algebraic geometry. Our approach to the proper learning problem is as follows: Starting with an accurate non-proper hypothesis, we fit a PBD to this hypothesis. More specifically, we essentially start with the hypothesis computed by the computationally efficient non-proper learning algorithm in our recent work~\cite{DKS15}. Our aforementioned structural characterization allows us to reduce the corresponding fitting problem to a collection of $(1/\epsilon)^{O(\log \log(1/\epsilon))}$ systems of low-degree polynomial inequalities. We show that each such system can be solved in time $(1/\epsilon)^{O(\log \log(1/\epsilon))}$, which yields the overall running time of our algorithm. version:1
arxiv-1511-04056 | Efficient non-greedy optimization of decision trees | http://arxiv.org/abs/1511.04056 | id:1511.04056 author:Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli category:cs.LG cs.CV  published:2015-11-12 summary:Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines. version:1
arxiv-1511-04048 | Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images | http://arxiv.org/abs/1511.04048 | id:1511.04048 author:Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi category:cs.CV  published:2015-11-12 summary:In this paper, we study the challenging problem of predicting the dynamics of objects in static images. Given a query object in an image, our goal is to provide a physical understanding of the object in terms of the forces acting upon it and its long term motion as response to those forces. Direct and explicit estimation of the forces and the motion of objects from a single image is extremely challenging. We define intermediate physical abstractions called Newtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns to map a single image to a state in a Newtonian scenario. Our experimental evaluations show that our method can reliably predict dynamics of a query object from a single image. In addition, our approach can provide physical reasoning that supports the predicted dynamics in terms of velocity and force vectors. To spur research in this direction we compiled Visual Newtonian Dynamics (VIND) dataset that includes 6806 videos aligned with Newtonian scenarios represented using game engines, and 4516 still images with their ground truth dynamics. version:1
arxiv-1511-04003 | Human Curation and Convnets: Powering Item-to-Item Recommendations on Pinterest | http://arxiv.org/abs/1511.04003 | id:1511.04003 author:Dmitry Kislyuk, Yuchen Liu, David Liu, Eric Tzeng, Yushi Jing category:cs.CV  published:2015-11-12 summary:This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, the activity of users organizing content, are highly effective when used in conjunction with content-based ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from convnets, in improving the user engagement rate of our item-to-item recommendation system. version:1
arxiv-1511-03990 | Automatic Inference of the Quantile Parameter | http://arxiv.org/abs/1511.03990 | id:1511.03990 author:Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J. Thiagarajan category:stat.ML  published:2015-11-12 summary:Supervised learning is an active research area, with numerous applications in diverse fields such as data analytics, computer vision, speech and audio processing, and image understanding. In most cases, the loss functions used in machine learning assume symmetric noise models, and seek to estimate the unknown function parameters. However, loss functions such as quantile and quantile Huber generalize the symmetric $\ell_1$ and Huber losses to the asymmetric setting, for a fixed quantile parameter. In this paper, we propose to jointly infer the quantile parameter and the unknown function parameters, for the asymmetric quantile Huber and quantile losses. We explore various properties of the quantile Huber loss and implement a convexity certificate that can be used to check convexity in the quantile parameter. When the loss if convex with respect to the parameter of the function, we prove that it is biconvex in both the function and the quantile parameters, and propose an algorithm to jointly estimate these. Results with synthetic and real data demonstrate that the proposed approach can automatically recover the quantile parameter corresponding to the noise and also provide an improved recovery of function parameters. To illustrate the potential of the framework, we extend the gradient boosting machines with quantile losses to automatically estimate the quantile parameter at each iteration. version:1
arxiv-1209-1688 | Rank Centrality: Ranking from Pair-wise Comparisons | http://arxiv.org/abs/1209.1688 | id:1209.1688 author:Sahand Negahban, Sewoong Oh, Devavrat Shah category:cs.LG stat.ML  published:2012-09-08 summary:The question of aggregating pair-wise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR's TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining a ranking, finding `scores' for each object (e.g. player's rating) is of interest for understanding the intensity of the preferences. In this paper, we propose Rank Centrality, an iterative rank aggregation algorithm for discovering scores for objects (or items) from pair-wise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with an edge present between a pair of objects if they are compared; the score, which we call Rank Centrality, of an object turns out to be its stationary probability under this random walk. To study the efficacy of the algorithm, we consider the popular Bradley-Terry-Luce (BTL) model (equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which each object has an associated score which determines the probabilistic outcomes of pair-wise comparisons between objects. In terms of the pair-wise marginal probabilities, which is the main subject of this paper, the MNL model and the BTL model are identical. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. In particular, the number of samples required to learn the score well with high probability depends on the structure of the comparison graph. When the Laplacian of the comparison graph has a strictly positive spectral gap, e.g. each item is compared to a subset of randomly chosen items, this leads to dependence on the number of samples that is nearly order-optimal. version:4
arxiv-1511-03984 | Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester Using Artificial Neural Networks and Support Vector Machine | http://arxiv.org/abs/1511.03984 | id:1511.03984 author:Run Wang, Qiaoli Mo, Qian Zhang, Fudi Chen, Dazuo Yang category:cs.LG cs.NE  published:2015-11-12 summary:3\b{eta}-O-phthalic ester of betulinic acid is of great importance in anticancer studies. However, the optimization of its reaction conditions requires a large number of experimental works. To simplify the number of times of optimization in experimental works, here, we use artificial neural network (ANN) and support vector machine (SVM) models for the prediction of yields of 3\b{eta}-O-phthalic ester of betulinic acid synthesized by betulinic acid and phthalic anhydride using lipase as biocatalyst. General regression neural network (GRNN), multilayer feed-forward neural network (MLFN) and the SVM models were trained based on experimental data. Four indicators were set as independent variables, including time (h), temperature (C), amount of enzyme (mg) and molar ratio, while the yield of the 3\b{eta}-O-phthalic ester of betulinic acid was set as the dependent variable. Results show that the GRNN and SVM models have the best prediction results during the testing process, with comparatively low RMS errors (4.01 and 4.23respectively) and short training times (both 1s). The prediction accuracy of the GRNN and SVM are both 100% in testing process, under the tolerance of 30%. version:1
arxiv-1511-03947 | Bayesian Analysis of Dynamic Linear Topic Models | http://arxiv.org/abs/1511.03947 | id:1511.03947 author:Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard category:stat.ML cs.LG stat.ME  published:2015-11-12 summary:In dynamic topic modeling, the proportional contribution of a topic to a document depends on the temporal dynamics of that topic's overall prevalence in the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity. A Markov Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation is developed for posterior inference. Conditional independencies in the model and sampling are made explicit, and our MCMC algorithm is parallelized where possible to allow for inference in large corpora. To address computational bottlenecks associated with Polya-Gamma sampling, we appeal to the Central Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random variable. This approximation is fast and reliable for parameter values relevant in the text mining domain. Our model and inference algorithm are validated with multiple simulation examples, and we consider the application of modeling trends in PubMed abstracts. We demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions. We also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points. version:1
arxiv-1511-03924 | A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural Language | http://arxiv.org/abs/1511.03924 | id:1511.03924 author:Normunds Gruzitis, Dana Dannélls category:cs.CL  published:2015-11-12 summary:Berkeley FrameNet is a lexico-semantic resource for English based on the theory of frame semantics. It has been exploited in a range of natural language processing applications and has inspired the development of framenets for many languages. We present a methodological approach to the extraction and generation of a computational multilingual FrameNet-based grammar and lexicon. The approach leverages FrameNet-annotated corpora to automatically extract a set of cross-lingual semantico-syntactic valence patterns. Based on data from Berkeley FrameNet and Swedish FrameNet, the proposed approach has been implemented in Grammatical Framework (GF), a categorial grammar formalism specialized for multilingual grammars. The implementation of the grammar and lexicon is supported by the design of FrameNet, providing a frame semantic abstraction layer, an interlingual semantic API (application programming interface), over the interlingual syntactic API already provided by GF Resource Grammar Library. The evaluation of the acquired grammar and lexicon shows the feasibility of the approach. Additionally, we illustrate how the FrameNet-based grammar and lexicon are exploited in two distinct multilingual controlled natural language applications. The produced resources are available under an open source license. version:1
arxiv-1511-03803 | Private False Discovery Rate Control | http://arxiv.org/abs/1511.03803 | id:1511.03803 author:Cynthia Dwork, Weijie Su, Li Zhang category:math.ST cs.DS stat.ML stat.TH  published:2015-11-12 summary:We provide the first differentially private algorithms for controlling the false discovery rate (FDR) in multiple hypothesis testing, with essentially no loss in power under certain conditions. Our general approach is to adapt a well-known variant of the Benjamini-Hochberg procedure (BHq), making each step differentially private. This destroys the classical proof of FDR control. To prove FDR control of our method, (a) we develop a new proof of the original (non-private) BHq algorithm and its robust variants -- a proof requiring only the assumption that the true null test statistics are independent, allowing for arbitrary correlations between the true nulls and false nulls. This assumption is fairly weak compared to those previously shown in the vast literature on this topic, and explains in part the empirical robustness of BHq. Then (b) we relate the FDR control properties of the differentially private version to the control properties of the non-private version. \end{enumerate} We also present a low-distortion "one-shot" differentially private primitive for "top $k$" problems, e.g., "Which are the $k$ most popular hobbies?" (which we apply to: "Which hypotheses have the $k$ most significant $p$-values?"), and use it to get a faster privacy-preserving instantiation of our general approach at little cost in accuracy. The proof of privacy for the one-shot top~$k$ algorithm introduces a new technique of independent interest. version:1
arxiv-1502-07643 | Dynamic Belief Fusion for Object Detection | http://arxiv.org/abs/1502.07643 | id:1502.07643 author:Ryan Robinson category:cs.CV  published:2015-02-26 summary:A novel approach for the fusion of detection scores from disparate object detection methods is proposed. In order to effectively integrate the outputs of multiple detectors, the level of ambiguity in each individual detection score (called "uncertainty") is estimated using the precision/recall relationship of the corresponding detector. The proposed fusion method, called Dynamic Belief Fusion (DBF), dynamically assigns basic probabilities to propositions (target, non-target, uncertain) based on confidence levels in the detection results of individual approaches. A joint basic probability assignment, containing information from all detectors, is determined using Dempster's combination rule, and is easily reduced to a single fused detection score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF is considerably greater than conventional fusion approaches as well as state-of-the-art individual detectors. version:3
arxiv-1511-03766 | Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach | http://arxiv.org/abs/1511.03766 | id:1511.03766 author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG  published:2015-11-12 summary:In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem. Specifically, the proposed approach combines the strength of random projection with that of sparse learning: it utilizes random projection to reduce the dimensionality, and introduces $\ell_1$-norm regularization to alleviate the approximation error caused by random projection. Theoretical analysis shows that under favored conditions, the randomized algorithm can accurately recover the optimal solutions to the convex-concave optimization problem (i.e., recover both the primal and dual solutions). Furthermore, the solutions returned by our algorithm are guaranteed to be approximately sparse. version:1
arxiv-1511-03760 | Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints | http://arxiv.org/abs/1511.03760 | id:1511.03760 author:Mengdi Wang, Yichen Chen, Jialin Liu, Yuantao Gu category:stat.ML cs.LG math.OC  published:2015-11-12 summary:Consider convex optimization problems subject to a large number of constraints. We focus on stochastic problems in which the objective takes the form of expected values and the feasible set is the intersection of a large number of convex sets. We propose a class of algorithms that perform both stochastic gradient descent and random feasibility updates simultaneously. At every iteration, the algorithms sample a number of projection points onto a randomly selected small subsets of all constraints. Three feasibility update schemes are considered: averaging over random projected points, projecting onto the most distant sample, projecting onto a special polyhedral set constructed based on sample points. We prove the almost sure convergence of these algorithms, and analyze the iterates' feasibility error and optimality error, respectively. We provide new convergence rate benchmarks for stochastic first-order optimization with many constraints. The rate analysis and numerical experiments reveal that the algorithm using the polyhedral-set projection scheme is the most efficient one within known algorithms. version:1
arxiv-1511-03748 | Automatic Content-Aware Color and Tone Stylization | http://arxiv.org/abs/1511.03748 | id:1511.03748 author:Joon-Young Lee, Kalyan Sunkavalli, Zhe Lin, Xiaohui Shen, In So Kweon category:cs.CV  published:2015-11-12 summary:We introduce a new technique that automatically generates diverse, visually compelling stylizations for a photograph in an unsupervised manner. We achieve this by learning style ranking for a given input using a large photo collection and selecting a diverse subset of matching styles for final style transfer. We also propose a novel technique that transfers the global color and tone of the chosen exemplars to the input photograph while avoiding the common visual artifacts produced by the existing style transfer methods. Together, our style selection and transfer techniques produce compelling, artifact-free results on a wide range of input photographs, and a user study shows that our results are preferred over other techniques. version:1
arxiv-1511-02570 | Explicit Knowledge-based Reasoning for Visual Question Answering | http://arxiv.org/abs/1511.02570 | id:1511.02570 author:Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick category:cs.CV cs.CL  published:2015-11-09 summary:We describe a method for visual question answering which is capable of reasoning about contents of an image on the basis of information extracted from a large-scale knowledge base. The method not only answers natural language questions using concepts not contained in the image, but can provide an explanation of the reasoning by which it developed its answer. The method is capable of answering far more complex questions than the predominant long short-term memory-based approach, and outperforms it significantly in the testing. We also provide a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual ques- tion answering. version:2
arxiv-1510-07740 | The Wilson Machine for Image Modeling | http://arxiv.org/abs/1510.07740 | id:1510.07740 author:Saeed Saremi, Terrence J. Sejnowski category:stat.ML cond-mat.stat-mech cs.CV cs.LG  published:2015-10-27 summary:Learning the distribution of natural images is one of the hardest and most important problems in machine learning. The problem remains open, because the enormous complexity of the structures in natural images spans all length scales. We break down the complexity of the problem and show that the hierarchy of structures in natural images fuels a new class of learning algorithms based on the theory of critical phenomena and stochastic processes. We approach this problem from the perspective of the theory of critical phenomena, which was developed in condensed matter physics to address problems with infinite length-scale fluctuations, and build a framework to integrate the criticality of natural images into a learning algorithm. The problem is broken down by mapping images into a hierarchy of binary images, called bitplanes. In this representation, the top bitplane is critical, having fluctuations in structures over a vast range of scales. The bitplanes below go through a gradual stochastic heating process to disorder. We turn this representation into a directed probabilistic graphical model, transforming the learning problem into the unsupervised learning of the distribution of the critical bitplane and the supervised learning of the conditional distributions for the remaining bitplanes. We learnt the conditional distributions by logistic regression in a convolutional architecture. Conditioned on the critical binary image, this simple architecture can generate large, natural-looking images, with many shades of gray, without the use of hidden units, unprecedented in the studies of natural images. The framework presented here is a major step in bringing criticality and stochastic processes to machine learning and in studying natural image statistics. version:2
arxiv-1511-03690 | Deep Multimodal Semantic Embeddings for Speech and Images | http://arxiv.org/abs/1511.03690 | id:1511.03690 author:David Harwath, James Glass category:cs.CV cs.AI cs.CL  published:2015-11-11 summary:In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk. version:1
arxiv-1511-03688 | Online Principal Component Analysis in High Dimension: Which Algorithm to Choose? | http://arxiv.org/abs/1511.03688 | id:1511.03688 author:Hervé Cardot, David Degras category:stat.ML cs.LG stat.ME  published:2015-11-11 summary:In the current context of data explosion, online techniques that do not require storing all data in memory are indispensable to routinely perform tasks like principal component analysis (PCA). Recursive algorithms that update the PCA with each new observation have been studied in various fields of research and found wide applications in industrial monitoring, computer vision, astronomy, and latent semantic indexing, among others. This work provides guidance for selecting an online PCA algorithm in practice. We present the main approaches to online PCA, namely, perturbation techniques, incremental methods, and stochastic optimization, and compare their statistical accuracy, computation time, and memory requirements using artificial and real data. Extensions to missing data and to functional data are discussed. All studied algorithms are available in the R package onlinePCA on CRAN. version:1
arxiv-1511-03629 | A Continuous Max-Flow Approach to Cyclic Field Reconstruction | http://arxiv.org/abs/1511.03629 | id:1511.03629 author:John S. H. Baxter, Jonathan McLeod, Terry M. Peters category:cs.CV  published:2015-11-11 summary:Reconstruction of an image from noisy data using Markov Random Field theory has been explored by both the graph-cuts and continuous max-flow community in the form of the Potts and Ishikawa models. However, neither model takes into account the particular cyclic topology of specific intensity types such as the hue in natural colour images, or the phase in complex valued MRI. This paper presents \textit{cyclic continuous max-flow} image reconstruction which models the intensity being reconstructed as having a fundamentally cyclic topology. This model complements the Ishikawa model in that it is designed with image reconstruction in mind, having the topology of the intensity space inherent in the model while being readily extendable to an arbitrary intensity resolution. version:1
arxiv-1511-03592 | The Fourier Transform of Poisson Multinomial Distributions and its Algorithmic Applications | http://arxiv.org/abs/1511.03592 | id:1511.03592 author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.DS cs.GT cs.LG math.PR math.ST stat.TH  published:2015-11-11 summary:An $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable of the form $X = \sum_{i=1}^n X_i$, where the $X_i$'s are independent random vectors supported on the set of standard basis vectors in $\mathbb{R}^k.$ In this paper, we obtain a refined structural understanding of PMDs by analyzing their Fourier transform. As our core structural result, we prove that the Fourier transform of PMDs is {\em approximately sparse}, i.e., roughly speaking, its $L_1$-norm is small outside a small set. By building on this result, we obtain the following applications: {\bf Learning Theory.} We design the first computationally efficient learning algorithm for PMDs with respect to the total variation distance. Our algorithm learns an arbitrary $(n, k)$-PMD within variation distance $\epsilon$ using a near-optimal sample size of $\widetilde{O}_k(1/\epsilon^2),$ and runs in time $\widetilde{O}_k(1/\epsilon^2) \cdot \log n.$ Previously, no algorithm with a $\mathrm{poly}(1/\epsilon)$ runtime was known, even for $k=3.$ {\bf Game Theory.} We give the first efficient polynomial-time approximation scheme (EPTAS) for computing Nash equilibria in anonymous games. For normalized anonymous games with $n$ players and $k$ strategies, our algorithm computes a well-supported $\epsilon$-Nash equilibrium in time $n^{O(k^3)} \cdot (k/\epsilon)^{O(k^3\log(k/\epsilon)/\log\log(k/\epsilon))^{k-1}}.$ The best previous algorithm for this problem had running time $n^{(f(k)/\epsilon)^k},$ where $f(k) = \Omega(k^{k^2})$, for any $k>2.$ {\bf Statistics.} We prove a multivariate central limit theorem (CLT) that relates an arbitrary PMD to a discretized multivariate Gaussian with the same mean and covariance, in total variation distance. Our new CLT strengthens the CLT of Valiant and Valiant by completely removing the dependence on $n$ in the error bound. version:1
arxiv-1411-2698 | Bayesian group latent factor analysis with structured sparsity | http://arxiv.org/abs/1411.2698 | id:1411.2698 author:Shiwen Zhao, Chuan Gao, Sayan Mukherjee, Barbara E Engelhardt category:stat.ME q-bio.QM stat.ML  published:2014-11-11 summary:Latent factor models are the canonical statistical tool for exploratory analyses of low-dimensional linear structure for an observation matrix with p features across n samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. The main contribution of this work is to carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high-dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can both be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on both simulated data with substantial structure and real data, comparing against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation- and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and document data sizes. version:2
arxiv-1507-00710 | Fast, Provable Algorithms for Isotonic Regression in all $\ell_{p}$-norms | http://arxiv.org/abs/1507.00710 | id:1507.00710 author:Rasmus Kyng, Anup Rao, Sushant Sachdeva category:cs.LG cs.DS math.ST stat.TH  published:2015-07-02 summary:Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices, the Isotonic Regression of $y$ is a vector $x$ that respects the partial order described by $G,$ and minimizes $ x-y ,$ for a specified norm. This paper gives improved algorithms for computing the Isotonic Regression for all weighted $\ell_{p}$-norms with rigorous performance guarantees. Our algorithms are quite practical, and their variants can be implemented to run fast in practice. version:2
arxiv-1503-02346 | One Scan 1-Bit Compressed Sensing | http://arxiv.org/abs/1503.02346 | id:1503.02346 author:Ping Li category:stat.ME cs.IT cs.LG math.IT  published:2015-03-08 summary:Based on $\alpha$-stable random projections with small $\alpha$, we develop a simple algorithm for compressed sensing (sparse signal recovery) by utilizing only the signs (i.e., 1-bit) of the measurements. Using only 1-bit information of the measurements results in substantial cost reduction in collection, storage, communication, and decoding for compressed sensing. The proposed algorithm is efficient in that the decoding procedure requires only one scan of the coordinates. Our analysis can precisely show that, for a $K$-sparse signal of length $N$, $12.3K\log N/\delta$ measurements (where $\delta$ is the confidence) would be sufficient for recovering the support and the signs of the signal. While the method is very robust against typical measurement noises, we also provide the analysis of the scheme under random flipping of the signs of the measurements. \noindent Compared to the well-known work on 1-bit marginal regression (which can also be viewed as a one-scan method), the proposed algorithm requires orders of magnitude fewer measurements. Compared to 1-bit Iterative Hard Thresholding (IHT) (which is not a one-scan algorithm), our method is still significantly more accurate. Furthermore, the proposed method is reasonably robust against random sign flipping while IHT is known to be very sensitive to this type of noise. version:2
arxiv-1511-03576 | DataGrinder: Fast, Accurate, Fully non-Parametric Classification Approach Using 2D Convex Hulls | http://arxiv.org/abs/1511.03576 | id:1511.03576 author:Mohammad Khabbaz category:cs.DB cs.CG cs.LG  published:2015-11-11 summary:It has been a long time, since data mining technologies have made their ways to the field of data management. Classification is one of the most important data mining tasks for label prediction, categorization of objects into groups, advertisement and data management. In this paper, we focus on the standard classification problem which is predicting unknown labels in Euclidean space. Most efforts in Machine Learning communities are devoted to methods that use probabilistic algorithms which are heavy on Calculus and Linear Algebra. Most of these techniques have scalability issues for big data, and are hardly parallelizable if they are to maintain their high accuracies in their standard form. Sampling is a new direction for improving scalability, using many small parallel classifiers. In this paper, rather than conventional sampling methods, we focus on a discrete classification algorithm with O(n) expected running time. Our approach performs a similar task as sampling methods. However, we use column-wise sampling of data, rather than the row-wise sampling used in the literature. In either case, our algorithm is completely deterministic. Our algorithm, proposes a way of combining 2D convex hulls in order to achieve high classification accuracy as well as scalability in the same time. First, we thoroughly describe and prove our O(n) algorithm for finding the convex hull of a point set in 2D. Then, we show with experiments our classifier model built based on this idea is very competitive compared with existing sophisticated classification algorithms included in commercial statistical applications such as MATLAB. version:1
arxiv-1511-03575 | Federated Optimization:Distributed Optimization Beyond the Datacenter | http://arxiv.org/abs/1511.03575 | id:1511.03575 author:Jakub Konečný, Brendan McMahan, Daniel Ramage category:cs.LG math.OC  published:2015-11-11 summary:We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed (unevenly) over an extremely large number of \nodes, but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of utmost importance. A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead, the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network, each of which has only a tiny fraction of data available totally; in particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, we assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization. version:1
arxiv-1511-03476 | Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning | http://arxiv.org/abs/1511.03476 | id:1511.03476 author:Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang category:cs.CV  published:2015-11-11 summary:Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e., it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks. Notably, even using a single network with only RGB stream as input, HRNE beats all the recent systems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet. version:1
arxiv-1511-03472 | Instantaneous Modelling and Reverse Engineering of DataConsistent Prime Models in Seconds! | http://arxiv.org/abs/1511.03472 | id:1511.03472 author:Michael A. Idowu category:q-bio.QM nlin.AO stat.ML  published:2015-11-11 summary:A theoretical framework that supports automated construction of dynamic prime models purely from experimental time series data has been invented and developed, which can automatically generate (construct) data-driven models of any time series data in seconds. This has resulted in the formulation and formalisation of new reverse engineering and dynamic methods for automated systems modelling of complex systems, including complex biological, financial, control, and artificial neural network systems. The systems/model theory behind the invention has been formalised as a new, effective and robust system identification strategy complementary to process-based modelling. The proposed dynamic modelling and network inference solutions often involve tackling extremely difficult parameter estimation challenges, inferring unknown underlying network structures, and unsupervised formulation and construction of smart and intelligent ODE models of complex systems. In underdetermined conditions, i.e., cases of dealing with how best to instantaneously and rapidly construct data-consistent prime models of unknown (or well-studied) complex system from small-sized time series data, inference of unknown underlying network of interaction is more challenging. This article reports a robust step-by-step mathematical and computational analysis of the entire prime model construction process that determines a model from data in less than a minute. version:1
arxiv-1511-03464 | A Directional Diffusion Algorithm for Inpainting | http://arxiv.org/abs/1511.03464 | id:1511.03464 author:Jan Deriu, Rolf Jagerman, Kai-En Tsay category:cs.CV  published:2015-11-11 summary:The problem of inpainting involves reconstructing the missing areas of an image. Inpainting has many applications, such as reconstructing old damaged photographs or removing obfuscations from images. In this paper we present the directional diffusion algorithm for inpainting. Typical diffusion algorithms are bad at propagating edges from the image into the unknown masked regions. The directional diffusion algorithm improves on the regular diffusion algorithm by reconstructing edges more accurately. It scores better than regular diffusion when reconstructing images that are obfuscated by a text mask. version:1
arxiv-1511-03463 | Granger Causality in Multi-variate Time Series using a Time Ordered Restricted Vector Autoregressive Model | http://arxiv.org/abs/1511.03463 | id:1511.03463 author:Elsa Siggiridou, Dimitris Kugiumtzis category:stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH  published:2015-11-11 summary:Granger causality has been used for the investigation of the inter-dependence structure of the underlying systems of multi-variate time series. In particular, the direct causal effects are commonly estimated by the conditional Granger causality index (CGCI). In the presence of many observed variables and relatively short time series, CGCI may fail because it is based on vector autoregressive models (VAR) involving a large number of coefficients to be estimated. In this work, the VAR is restricted by a scheme that modifies the recently developed method of backward-in-time selection (BTS) of the lagged variables and the CGCI is combined with BTS. Further, the proposed approach is compared favorably to other restricted VAR representations, such as the top-down strategy, the bottom-up strategy, and the least absolute shrinkage and selection operator (LASSO), in terms of sensitivity and specificity of CGCI. This is shown by using simulations of linear and nonlinear, low and high-dimensional systems and different time series lengths. For nonlinear systems, CGCI from the restricted VAR representations are compared with analogous nonlinear causality indices. Further, CGCI in conjunction with BTS and other restricted VAR representations is applied to multi-channel scalp electroencephalogram (EEG) recordings of epileptic patients containing epileptiform discharges. CGCI on the restricted VAR, and BTS in particular, could track the changes in brain connectivity before, during and after epileptiform discharges, which was not possible using the full VAR representation. version:1
arxiv-1511-03405 | Training Deep Gaussian Processes using Stochastic Expectation Propagation and Probabilistic Backpropagation | http://arxiv.org/abs/1511.03405 | id:1511.03405 author:Thang D. Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Richard E. Turner category:stat.ML  published:2015-11-11 summary:Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are probabilistic and non-parametric and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. The focus of this paper is scalable approximate Bayesian learning of these networks. The paper develops a novel and efficient extension of probabilistic backpropagation, a state-of-the-art method for training Bayesian neural networks, that can be used to train DGPs. The new method leverages a recently proposed method for scaling Expectation Propagation, called stochastic Expectation Propagation. The method is able to automatically discover useful input warping, expansion or compression, and it is therefore is a flexible form of Bayesian kernel design. We demonstrate the success of the new method for supervised learning on several real-world datasets, showing that it typically outperforms GP regression and is never much worse. version:1
arxiv-1511-03398 | A GMM-Based Stair Quality Model for Human Perceived JPEG Images | http://arxiv.org/abs/1511.03398 | id:1511.03398 author:Sudeng Hu, Haiqiang Wang, C. -C. Jay Kuo category:cs.MM cs.CV  published:2015-11-11 summary:Based on the notion of just noticeable differences (JND), a stair quality function (SQF) was recently proposed to model human perception on JPEG images. Furthermore, a k-means clustering algorithm was adopted to aggregate JND data collected from multiple subjects to generate a single SQF. In this work, we propose a new method to derive the SQF using the Gaussian Mixture Model (GMM). The newly derived SQF can be interpreted as a way to characterize the mean viewer experience. Furthermore, it has a lower information criterion (BIC) value than the previous one, indicating that it offers a better model. A specific example is given to demonstrate the advantages of the new approach. version:1
arxiv-1510-06143 | High Performance Latent Variable Models | http://arxiv.org/abs/1510.06143 | id:1510.06143 author:Aaron Q. Li, Amr Ahmed, Mu Li, Vanja Josifovski category:cs.LG cs.AI  published:2015-10-21 summary:Latent variable models have accumulated a considerable amount of interest from the industry and academia for their versatility in a wide range of applications. A large amount of effort has been made to develop systems that is able to extend the systems to a large scale, in the hope to make use of them on industry scale data. In this paper, we describe a system that operates at a scale orders of magnitude higher than previous works, and an order of magnitude faster than state-of-the-art system at the same scale, at the same time showing more robustness and more accurate results. Our system uses a number of advances in distributed inference: high performance in synchronization of sufficient statistics with relaxed consistency model; fast sampling, using the Metropolis-Hastings-Walker method to overcome dense generative models; statistical modeling, moving beyond Latent Dirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and Hierarchical Dirichlet Process (HDP) models; sophisticated parameter projection schemes, to resolve the conflicts within the constraint between parameters arising from the relaxed consistency model. This work significantly extends the domain of applicability of what is commonly known as the Parameter Server. We obtain results with up to hundreds billion oftokens, thousands of topics, and a vocabulary of a few million token-types, using up to 60,000 processor cores operating on a production cluster of a large Internet company. This demonstrates the feasibility to scale to problems orders of magnitude larger than any previously published work. version:4
arxiv-1504-05321 | Instance Optimal Learning | http://arxiv.org/abs/1504.05321 | id:1504.05321 author:Gregory Valiant, Paul Valiant category:cs.LG  published:2015-04-21 summary:We consider the following basic learning task: given independent draws from an unknown distribution over a discrete support, output an approximation of the distribution that is as accurate as possible in $\ell_1$ distance (i.e. total variation or statistical distance). Perhaps surprisingly, it is often possible to "de-noise" the empirical distribution of the samples to return an approximation of the true distribution that is significantly more accurate than the empirical distribution, without relying on any prior assumptions on the distribution. We present an instance optimal learning algorithm which optimally performs this de-noising for every distribution for which such a de-noising is possible. More formally, given $n$ independent draws from a distribution $p$, our algorithm returns a labelled vector whose expected distance from $p$ is equal to the minimum possible expected error that could be obtained by any algorithm that knows the true unlabeled vector of probabilities of distribution $p$ and simply needs to assign labels, up to an additive subconstant term that is independent of $p$ and goes to zero as $n$ gets large. One conceptual implication of this result is that for large samples, Bayesian assumptions on the "shape" or bounds on the tail probabilities of a distribution over discrete support are not helpful for the task of learning the distribution. As a consequence of our techniques, we also show that given a set of $n$ samples from an arbitrary distribution, one can accurately estimate the expected number of distinct elements that will be observed in a sample of any size up to $n \log n$. This sort of extrapolation is practically relevant, particularly to domains such as genomics where it is important to understand how much more might be discovered given larger sample sizes, and we are optimistic that our approach is practically viable. version:2
arxiv-1511-03369 | Multimodal MRI Neuroimaging with Motion Compensation Based on Particle Filtering | http://arxiv.org/abs/1511.03369 | id:1511.03369 author:Yu-Hui Chen, Roni Mittelman, Boklye Kim, Charles Meyer, Alfred Hero category:cs.CV physics.data-an physics.med-ph  published:2015-11-11 summary:Head movement during scanning impedes activation detection in fMRI studies. Head motion in fMRI acquired using slice-based Echo Planar Imaging (EPI) can be estimated and compensated by aligning the images onto a reference volume through image registration. However, registering EPI images volume to volume fails to consider head motion between slices, which may lead to severely biased head motion estimates. Slice-to-volume registration can be used to estimate motion parameters for each slice by more accurately representing the image acquisition sequence. However, accurate slice to volume mapping is dependent on the information content of the slices: middle slices are information rich, while edge slides are information poor and more prone to distortion. In this work, we propose a Gaussian particle filter based head motion tracking algorithm to reduce the image misregistration errors. The algorithm uses a dynamic state space model of head motion with an observation equation that models continuous slice acquisition of the scanner. Under this model the particle filter provides more accurate motion estimates and voxel position estimates. We demonstrate significant performance improvement of the proposed approach as compared to registration-only methods of head motion estimation and brain activation detection. version:1
arxiv-1511-03363 | Facial Expression Detection using Patch-based Eigen-face Isomap Networks | http://arxiv.org/abs/1511.03363 | id:1511.03363 author:Sohini Roychowdhury category:cs.CV  published:2015-11-11 summary:Automated facial expression detection problem pose two primary challenges that include variations in expression and facial occlusions (glasses, beard, mustache or face covers). In this paper we introduce a novel automated patch creation technique that masks a particular region of interest in the face, followed by Eigen-value decomposition of the patched faces and generation of Isomaps to detect underlying clustering patterns among faces. The proposed masked Eigen-face based Isomap clustering technique achieves 75% sensitivity and 66-73% accuracy in classification of faces with occlusions and smiling faces in around 1 second per image. Also, betweenness centrality, Eigen centrality and maximum information flow can be used as network-based measures to identify the most significant training faces for expression classification tasks. The proposed method can be used in combination with feature-based expression classification methods in large data sets for improving expression classification accuracies. version:1
arxiv-1511-03361 | Discovery Radiomics via StochasticNet Sequencers for Cancer Detection | http://arxiv.org/abs/1511.03361 | id:1511.03361 author:Mohammad Javad Shafiee, Audrey G. Chung, Devinder Kumar, Farzad Khalvati, Masoom Haider, Alexander Wong category:cs.CV cs.AI  published:2015-11-11 summary:Radiomics has proven to be a powerful prognostic tool for cancer detection, and has previously been applied in lung, breast, prostate, and head-and-neck cancer studies with great success. However, these radiomics-driven methods rely on pre-defined, hand-crafted radiomic feature sets that can limit their ability to characterize unique cancer traits. In this study, we introduce a novel discovery radiomics framework where we directly discover custom radiomic features from the wealth of available medical imaging data. In particular, we leverage novel StochasticNet radiomic sequencers for extracting custom radiomic features tailored for characterizing unique cancer tissue phenotype. Using StochasticNet radiomic sequencers discovered using a wealth of lung CT data, we perform binary classification on 42,340 lung lesions obtained from the CT scans of 93 patients in the LIDC-IDRI dataset. Preliminary results show significant improvement over previous state-of-the-art methods, indicating the potential of the proposed discovery radiomics framework for improving cancer screening and diagnosis. version:1
arxiv-1511-03355 | Principal Autoparallel Analysis: Data Analysis in Weitzenböck Space | http://arxiv.org/abs/1511.03355 | id:1511.03355 author:Stephen Marsland, Carole J Twining category:stat.ME cs.CV math.DG  published:2015-11-11 summary:The statistical analysis of data lying on a differentiable, locally Euclidean, manifold introduces a variety of challenges because the analogous measures to standard Euclidean statistics are local, that is only defined within a neighbourhood of each datapoint. This is because the curvature of the space means that the connection of Riemannian geometry is path dependent. In this paper we transfer the problem to Weitzenb\"{o}ck space, which has torsion, but not curvature, meaning that parallel transport is path independent, and rather than considering geodesics, it is natural to consider autoparallels, which are `straight' in the sense that they follow the local basis vectors. We demonstrate how to generate these autoparallels in a data-driven fashion, and show that the resulting representation of the data is a useful space in which to perform further analysis. version:1
arxiv-1509-06086 | Fusing Multi-Stream Deep Networks for Video Classification | http://arxiv.org/abs/1509.06086 | id:1509.06086 author:Zuxuan Wu, Yu-Gang Jiang, Xi Wang, Hao Ye, Xiangyang Xue, Jun Wang category:cs.CV cs.MM  published:2015-09-21 summary:This paper studies deep network architectures to address the problem of video classification. A multi-stream framework is proposed to fully utilize the rich multimodal information in videos. Specifically, we first train three Convolutional Neural Networks to model spatial, short-term motion and audio clues respectively. Long Short Term Memory networks are then adopted to explore long-term temporal dynamics. With the outputs of the individual streams, we propose a simple and effective fusion method to generate the final predictions, where the optimal fusion weights are learned adaptively for each class, and the learning process is regularized by automatically estimated class relationships. Our contributions are two-fold. First, the proposed multi-stream framework is able to exploit multimodal features that are more comprehensive than those previously attempted. Second, we demonstrate that the adaptive fusion method using the class relationship as a regularizer outperforms traditional alternatives that estimate the weights in a "free" fashion. Our framework produces significantly better results than the state of the arts on two popular benchmarks, 92.2\% on UCF-101 (without using audio) and 84.9\% on Columbia Consumer Videos. version:2
arxiv-1506-05790 | Scalable Semi-Supervised Aggregation of Classifiers | http://arxiv.org/abs/1506.05790 | id:1506.05790 author:Akshay Balsubramani, Yoav Freund category:cs.LG  published:2015-06-18 summary:We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests. version:2
arxiv-1511-03299 | Anchored Discrete Factor Analysis | http://arxiv.org/abs/1511.03299 | id:1511.03299 author:Yoni Halpern, Steven Horng, David Sontag category:stat.ML cs.LG  published:2015-11-10 summary:We present a semi-supervised learning algorithm for learning discrete factor analysis models with arbitrary structure on the latent variables. Our algorithm assumes that every latent variable has an "anchor", an observed variable with only that latent variable as its parent. Given such anchors, we show that it is possible to consistently recover moments of the latent variables and use these moments to learn complete models. We also introduce a new technique for improving the robustness of method-of-moment algorithms by optimizing over the marginal polytope or its relaxations. We evaluate our algorithm using two real-world tasks, tag prediction on questions from the Stack Overflow website and medical diagnosis in an emergency department. version:1
arxiv-1511-03296 | The Fast Bilateral Solver | http://arxiv.org/abs/1511.03296 | id:1511.03296 author:Jonathan T. Barron, Ben Poole category:cs.CV  published:2015-11-10 summary:We present the bilateral solver, a novel algorithm for edge-aware smoothing that combines the flexibility and speed of simple filtering approaches with the accuracy of domain-specific optimization algorithms. Our technique is capable of matching or improving upon state-of-the-art results on several different computer vision tasks (stereo, depth superresolution, colorization, and semantic segmentation) while being 10-1000 times faster than competing approaches. The bilateral solver is fast, robust, straightforward to generalize to new domains, and simple to integrate into deep learning pipelines. version:1
arxiv-1511-03292 | From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge | http://arxiv.org/abs/1511.03292 | id:1511.03292 author:Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis Aloimonos category:cs.CV cs.AI cs.CL I.2.10  published:2015-11-10 summary:In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a "commonsense" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches. version:1
arxiv-1511-03257 | Online Supervised Hashing for Ever-Growing Datasets | http://arxiv.org/abs/1511.03257 | id:1511.03257 author:Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff category:cs.CV  published:2015-11-10 summary:Supervised hashing methods are widely-used for nearest neighbor search in computer vision applications. Most state-of-the-art supervised hashing approaches employ batch-learners. Unfortunately, batch-learning strategies can be inefficient when confronted with large training datasets. Moreover, with batch-learners, it is unclear how to adapt the hash functions as a dataset continues to grow and diversify over time. Yet, in many practical scenarios the dataset grows and diversifies; thus, both the hash functions and the indexing must swiftly accommodate these changes. To address these issues, we propose an online hashing method that is amenable to changes and expansions of the datasets. Since it is an online algorithm, our approach offers linear complexity with the dataset size. Our solution is supervised, in that we incorporate available label information to preserve the semantic neighborhood. Such an adaptive hashing method is attractive; but it requires recomputing the hash table as the hash functions are updated. If the frequency of update is high, then recomputing the hash table entries may cause inefficiencies in the system, especially for large indexes. Thus, we also propose a framework to reduce hash table updates. We compare our method to state-of-the-art solutions on two benchmarks and demonstrate significant improvements over previous work. version:1
arxiv-1508-05463 | StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity | http://arxiv.org/abs/1508.05463 | id:1508.05463 author:Mohammad Javad Shafiee, Parthipan Siva, Alexander Wong category:cs.CV cs.LG cs.NE  published:2015-08-22 summary:Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations, in a deep neural network architecture, can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet, using less than half the number of neural connections as a conventional deep neural network, achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and SVHN dataset. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of ~6% compared to ConvNet) on the STL-10 dataset than a conventional deep neural network. Finally, StochasticNets have faster operational speeds while achieving better or similar accuracy performances. version:4
arxiv-1506-02106 | What's the point: Semantic segmentation with point supervision | http://arxiv.org/abs/1506.02106 | id:1506.02106 author:Amy Bearman, Olga Russakovsky, Vittorio Ferrari, Li Fei-Fei category:cs.CV  published:2015-06-06 summary:The semantic image segmentation task presents a trade-off between test accuracy and the cost of obtaining training annotations. Detailed per-pixel annotations enable training accurate models but are very expensive to obtain; image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We demonstrate that this adds negligible additional annotation cost. We incorporate this point supervision along with a novel objectness potential in the training loss function of a state-of-the-art CNN model. The combined effect of these two extensions is a 12.9% increase in mean intersection over union on the PASCAL VOC 2012 segmentation task compared to a CNN model trained with only image-level labels. version:4
arxiv-1508-02765 | Are Slepian-Wolf Rates Necessary for Distributed Parameter Estimation? | http://arxiv.org/abs/1508.02765 | id:1508.02765 author:Mostafa El Gamal, Lifeng Lai category:cs.IT math.IT stat.ML  published:2015-08-11 summary:We consider a distributed parameter estimation problem, in which multiple terminals send messages related to their local observations using limited rates to a fusion center who will obtain an estimate of a parameter related to observations of all terminals. It is well known that if the transmission rates are in the Slepian-Wolf region, the fusion center can fully recover all observations and hence can construct an estimator having the same performance as that of the centralized case. One natural question is whether Slepian-Wolf rates are necessary to achieve the same estimation performance as that of the centralized case. In this paper, we show that the answer to this question is negative. We establish our result by explicitly constructing an asymptotically minimum variance unbiased estimator (MVUE) that has the same performance as that of the optimal estimator in the centralized case while requiring information rates less than the conditions required in the Slepian-Wolf rate region. version:2
arxiv-1509-09308 | Fast Algorithms for Convolutional Neural Networks | http://arxiv.org/abs/1509.09308 | id:1509.09308 author:Andrew Lavin, Scott Gray category:cs.NE cs.LG I.2.6; F.2.1  published:2015-09-30 summary:Deep convolutional neural networks take GPU days of compute time to train on large data sets. Pedestrian detection for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of convolutional neural networks in these situations is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but state of the art convolutional neural networks use small, 3x3 filters. We introduce a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64. version:2
arxiv-1511-01443 | A Distributed One-Step Estimator | http://arxiv.org/abs/1511.01443 | id:1511.01443 author:Cheng Huang, Xiaoming Huo category:stat.ME cs.DC stat.ML  published:2015-11-04 summary:Distributed statistical inference has recently attracted enormous attention. Many existing work focuses on the averaging estimator. We propose a one-step approach to enhance a simple-averaging based distributed estimator. We derive the corresponding asymptotic properties of the newly proposed estimator. We find that the proposed one-step estimator enjoys the same asymptotic properties as the centralized estimator. The proposed one-step approach merely requires one additional round of communication in relative to the averaging estimator; so the extra communication burden is insignificant. In finite sample cases, numerical examples show that the proposed estimator outperforms the simple averaging estimator with a large margin in terms of the mean squared errors. A potential application of the one-step approach is that one can use multiple machines to speed up large scale statistical inference with little compromise in the quality of estimators. The proposed method becomes more valuable when data can only be available at distributed machines with limited communication bandwidth. version:2
arxiv-1511-03244 | TemplateNet for Depth-Based Object Instance Recognition | http://arxiv.org/abs/1511.03244 | id:1511.03244 author:Ujwal Bonde, Vijay Badrinarayanan, Roberto Cipolla, Minh-Tri Pham category:cs.CV  published:2015-11-10 summary:We present a novel deep architecture termed templateNet for depth based object instance recognition. Using an intermediate template layer we exploit prior knowledge of an object's shape to sparsify the feature maps. This has three advantages: (i) the network is better regularised resulting in structured filters; (ii) the sparse feature maps results in intuitive features been learnt which can be visualized as the output of the template layer and (iii) the resulting network achieves state-of-the-art performance. The network benefits from this without any additional parametrization from the template layer. We derive the weight updates needed to efficiently train this network in an end-to-end manner. We benchmark the templateNet for depth based object instance recognition using two publicly available datasets. The datasets present multiple challenges of clutter, large pose variations and similar looking distractors. Through our experiments we show that with the addition of a template layer, a depth based CNN is able to outperform existing state-of-the-art methods in the field. version:1
arxiv-1511-03206 | The Radon cumulative distribution transform and its application to image classification | http://arxiv.org/abs/1511.03206 | id:1511.03206 author:Soheil Kolouri, Se Rim Park, Gustavo K. Rohde category:cs.CV  published:2015-11-10 summary:Invertible image representation methods (transforms) are routinely employed as low-level image processing operations based on which feature extraction and recognition algorithms are developed. Most transforms in current use (e.g. Fourier, Wavelet, etc.) are linear transforms, and, by themselves, are unable to substantially simplify the representation of image classes for classification. Here we describe a nonlinear, invertible, low-level image processing transform based on combining the well known Radon transform for image data, and the 1D Cumulative Distribution Transform proposed earlier. We describe a few of the properties of this new transform, and with both theoretical and experimental results show that it can often render certain problems linearly separable in transform space. version:1
arxiv-1511-03198 | Sliced Wasserstein Kernels for Probability Distributions | http://arxiv.org/abs/1511.03198 | id:1511.03198 author:Soheil Kolouri, Yang Zou, Gustavo K. Rohde category:cs.LG stat.ML  published:2015-11-10 summary:Optimal transport distances, otherwise known as Wasserstein distances, have recently drawn ample attention in computer vision and machine learning as a powerful discrepancy measure for probability distributions. The recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and has revamped its practical applications in machine learning. In this paper, we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the Sliced Wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks. Our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks. version:1
arxiv-1509-04098 | Fame for sale: efficient detection of fake Twitter followers | http://arxiv.org/abs/1509.04098 | id:1509.04098 author:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi category:cs.SI cs.CR cs.LG H.2.8  published:2015-09-14 summary:$\textit{Fake followers}$ are those Twitter accounts specifically created to inflate the number of followers of a target account. Fake followers are dangerous for the social platform and beyond, since they may alter concepts like popularity and influence in the Twittersphere - hence impacting on economy, politics, and society. In this paper, we contribute along different dimensions. First, we review some of the most relevant existing features and rules (proposed by Academia and Media) for anomalous Twitter accounts detection. Second, we create a baseline dataset of verified human and fake follower accounts. Such baseline dataset is publicly available to the scientific community. Then, we exploit the baseline dataset to train a set of machine-learning classifiers built over the reviewed rules and features. Our results show that most of the rules proposed by Media provide unsatisfactory performance in revealing fake followers, while features proposed in the past by Academia for spam detection provide good results. Building on the most promising features, we revise the classifiers both in terms of reduction of overfitting and cost for gathering the data needed to compute the features. The final result is a novel $\textit{Class A}$ classifier, general enough to thwart overfitting, lightweight thanks to the usage of the less costly features, and still able to correctly classify more than 95% of the accounts of the original training set. We ultimately perform an information fusion-based sensitivity analysis, to assess the global sensitivity of each of the features employed by the classifier. The findings reported in this paper, other than being supported by a thorough experimental methodology and interesting on their own, also pave the way for further investigation on the novel issue of fake Twitter followers. version:2
arxiv-1511-03183 | Dynamic Belief Fusion for Object Detection | http://arxiv.org/abs/1511.03183 | id:1511.03183 author:Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William d. Nothwang, Amar M. Marathe category:cs.CV  published:2015-11-10 summary:A novel approach for the fusion of heterogeneous object detection methods is proposed. In order to effectively integrate the outputs of multiple detectors, the level of ambiguity in each individual detection score is estimated using the precision/recall relationship of the corresponding detector. The main contribution of the proposed work is a novel fusion method, called Dynamic Belief Fusion (DBF), which dynamically assigns probabilities to hypotheses (target, non-target, intermediate state (target or non-target)) based on confidence levels in the detection results conditioned on the prior performance of individual detectors. In DBF, a joint basic probability assignment, optimally fusing information from all detectors, is determined by the Dempster's combination rule, and is easily reduced to a single fused detection score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF is considerably greater than conventional fusion approaches as well as individual detectors used for the fusion. version:1
arxiv-1511-04045 | Kernel Methods for Accurate UWB-Based Ranging with Reduced Complexity | http://arxiv.org/abs/1511.04045 | id:1511.04045 author:Vladimir Savic, Erik G. Larsson, Javier Ferrer-Coll, Peter Stenumgaard category:cs.LG cs.IT math.IT  published:2015-11-10 summary:Accurate and robust positioning in multipath environments can enable many applications, such as search-and-rescue and asset tracking. For this problem, ultra-wideband (UWB) technology can provide the most accurate range estimates, which are required for range-based positioning. However, UWB still faces a problem with non-line-of-sight (NLOS) measurements, in which the range estimates based on time-of-arrival (TOA) will typically be positively biased. There are many techniques that address this problem, mainly based on NLOS identification and NLOS error mitigation algorithms. However, these techniques do not exploit all available information in the UWB channel impulse response. Kernel-based machine learning methods, such as Gaussian Process Regression (GPR), are able to make use of all information, but they may be too complex in their original form. In this paper, we propose novel ranging methods based on kernel principal component analysis (kPCA), in which the selected channel parameters are projected onto a nonlinear orthogonal high-dimensional space, and a subset of these projections is then used as an input for ranging. We evaluate the proposed methods using real UWB measurements obtained in a basement tunnel, and found that one of the proposed methods is able to outperform state-of-the-art, even if little training samples are available. version:1
arxiv-1511-03144 | Asynchronous Decentralized 20 Questions for Adaptive Search | http://arxiv.org/abs/1511.03144 | id:1511.03144 author:Theodoros Tsiligkaridis category:cs.MA cs.IT cs.SY math.IT stat.ML 60Gxx  68T05  93E35  published:2015-11-10 summary:This paper considers the problem of adaptively searching for an unknown target using multiple agents connected through a time-varying network topology. Agents are equipped with sensors capable of fast information processing, and we propose an asynchronous decentralized algorithm for controlling their search given noisy observations. Specifically, we propose asynchronous decentralized extensions of the adaptive query-based search strategy that combines elements from the 20 questions approach and social learning. Under standard assumptions on the time-varying network dynamics, we prove convergence to correct consensus on the value of the parameter as the number of iterations go to infinity. This framework provides a flexible and tractable mathematical model for asynchronous decentralized parameter estimation systems based on adaptively-designed queries. Our results establish that stability and consistency can be maintained even with one-way updating and randomized pairwise averaging, thus providing a scalable low complexity alternative to the synchronous decentralized estimation algorithm studied in Tsiligkaridis et al [1]. We illustrate the effectiveness and robustness of our algorithm for random network topologies. version:1
arxiv-1511-03088 | USFD: Twitter NER with Drift Compensation and Linked Data | http://arxiv.org/abs/1511.03088 | id:1511.03088 author:Leon Derczynski, Isabelle Augenstein, Kalina Bontcheva category:cs.CL  published:2015-11-10 summary:This paper describes a pilot NER system for Twitter, comprising the USFD system entry to the W-NUT 2015 NER shared task. The goal is to correctly label entities in a tweet dataset, using an inventory of ten types. We employ structured learning, drawing on gazetteers taken from Linked Data, and on unsupervised clustering features, and attempting to compensate for stylistic and topic drift - a key challenge in social media text. Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task. version:1
arxiv-1511-03086 | The CTU Prague Relational Learning Repository | http://arxiv.org/abs/1511.03086 | id:1511.03086 author:Jan Motl, Oliver Schulte category:cs.LG cs.DB I.2.6; H.2.8  published:2015-11-10 summary:The aim of the CTU Prague Relational Learning Repository is to support machine learning research with multi-relational data. The repository currently contains 50 SQL databases hosted on a public MySQL server located at relational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., the number of tables in the database, the number of rows and columns in the tables, the number of foreign key constraints between tables). version:1
arxiv-1508-05288 | Dynamics of Human Cooperation in Economic Games | http://arxiv.org/abs/1508.05288 | id:1508.05288 author:Martin Spanknebel, Klaus Pawelzik category:physics.soc-ph cs.GT cs.LG math.DS  published:2015-08-21 summary:Human decision behaviour is quite diverse. In many games humans on average do not achieve maximal payoff and the behaviour of individual players remains inhomogeneous even after playing many rounds. For instance, in repeated prisoner dilemma games humans do not always optimize their mean reward and frequently exhibit broad distributions of cooperativity. The reasons for these failures of maximization are not known. Here we show that the dynamics resulting from the tendency to shift choice probabilities towards previously rewarding choices in closed loop interaction with the strategy of the opponent can not only explain systematic deviations from 'rationality', but also reproduce the diversity of choice behaviours. As a representative example we investigate the dynamics of choice probabilities in prisoner dilemma games with opponents using strategies with different degrees of extortion and generosity. We find that already a simple model for human learning can account for a surprisingly wide range of human decision behaviours. It reproduces suppression of cooperation against extortionists and increasing cooperation when playing with generous opponents, explains the broad distributions of individual choices in ensembles of players, and predicts the evolution of individual subjects' cooperation rates over the course of the games. We conclude that important aspects of human decision behaviours are rooted in elementary learning mechanisms realised in the brain. version:3
arxiv-1511-03055 | Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing | http://arxiv.org/abs/1511.03055 | id:1511.03055 author:Jie Lin, Olivier Morère, Julie Petta, Vijay Chandrasekhar, Antoine Veillard category:cs.IR cs.CV cs.LG 68P20 H.3.3; I.2.6  published:2015-11-10 summary:A typical image retrieval pipeline starts with the comparison of global descriptors from a large database to find a short list of candidate matches. A good image descriptor is key to the retrieval pipeline and should reconcile two contradictory requirements: providing recall rates as high as possible and being as compact as possible for fast matching. Following the recent successes of Deep Convolutional Neural Networks (DCNN) for large scale image classification, descriptors extracted from DCNNs are increasingly used in place of the traditional hand crafted descriptors such as Fisher Vectors (FV) with better retrieval performances. Nevertheless, the dimensionality of a typical DCNN descriptor --extracted either from the visual feature pyramid or the fully-connected layers-- remains quite high at several thousands of scalar values. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully unsupervised method to compute extremely compact binary hashes --in the 32-256 bits range-- from high-dimensional global descriptors. UTH consists of two successive deep learning steps. First, Stacked Restricted Boltzmann Machines (SRBM), a type of unsupervised deep neural nets, are used to learn binary embedding functions able to bring the descriptor size down to the desired bitrate. SRBMs are typically able to ensure a very high compression rate at the expense of loosing some desirable metric properties of the original DCNN descriptor space. Then, triplet networks, a rank learning scheme based on weight sharing nets is used to fine-tune the binary embedding functions to retain as much as possible of the useful metric properties of the original space. A thorough empirical evaluation conducted on multiple publicly available dataset using DCNN descriptors shows that our method is able to significantly outperform state-of-the-art unsupervised schemes in the target bit range. version:1
arxiv-1511-03053 | Investigating the stylistic relevance of adjective and verb simile markers | http://arxiv.org/abs/1511.03053 | id:1511.03053 author:Suzanne Mpouli, Jean-Gabriel Ganascia category:cs.CL  published:2015-11-10 summary:Similes play an important role in literary texts not only as rhetorical devices and as figures of speech but also because of their evocative power, their aptness for description and the relative ease with which they can be combined with other figures of speech (Israel et al. 2004). Detecting all types of simile constructions in a particular text therefore seems crucial when analysing the style of an author. Few research studies however have been dedicated to the study of less prominent simile markers in fictional prose and their relevance for stylistic studies. The present paper studies the frequency of adjective and verb simile markers in a corpus of British and French novels in order to determine which ones are really informative and worth including in a stylistic analysis. Furthermore, are those adjectives and verb simile markers used differently in both languages? version:1
arxiv-1511-03028 | Online Action Recognition based on Incremental Learning of Weighted Covariance Descriptors | http://arxiv.org/abs/1511.03028 | id:1511.03028 author:Chang Tang, Wanqing Li, Chunping Hou, Pichao Wang, Yonghong Hou, Jing Zhang, Philip O. Ogunbona category:cs.CV  published:2015-11-10 summary:Online action recognition aims to recognize actions from unsegmented streams of data in a continuous manner. One of the challenges in online recognition is the accumulation of evidence for decision making. This paper presents a fast and efficient online method to recognize actions from a stream of noisy skeleton data. The method adopts a covariance descriptor calculated from skeleton data and is based on a novel method developed for incrementally learning the covariance descriptors, referred to as weighted covariance descriptors, so that past frames have less contributions to the descriptor and current frames and informative frames such as key frames contributes more towards the descriptor. The online recognition is achieved using an efficient nearest neighbour search against a set of trained actions. Experimental results on MSRC-12 Kinect Gesture dataset and our newly collocated online action recognition dataset have demonstrated the efficacy of the proposed method. version:1
arxiv-1511-03019 | 3D Time-lapse Reconstruction from Internet Photos | http://arxiv.org/abs/1511.03019 | id:1511.03019 author:Ricardo Martin-Brualla, David Gallup, Steven M. Seitz category:cs.CV  published:2015-11-10 summary:Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax. Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photorealistic time-lapses of skylines and natural scenes over many years, with dramatic parallax effects. version:1
arxiv-1511-03015 | Deep Representation of Facial Geometric and Photometric Attributes for Automatic 3D Facial Expression Recognition | http://arxiv.org/abs/1511.03015 | id:1511.03015 author:Huibin Li, Jian Sun, Dong Wang, Zongben Xu, Liming Chen category:cs.CV  published:2015-11-10 summary:In this paper, we present a novel approach to automatic 3D Facial Expression Recognition (FER) based on deep representation of facial 3D geometric and 2D photometric attributes. A 3D face is firstly represented by its geometric and photometric attributes, including the geometry map, normal maps, normalized curvature map and texture map. These maps are then fed into a pre-trained deep convolutional neural network to generate the deep representation. Then the facial expression prediction is simplyachieved by training linear SVMs over the deep representation for different maps and fusing these SVM scores. The visualizations show that the deep representation provides a complete and highly discriminative coding scheme for 3D faces. Comprehensive experiments on the BU-3DFE database demonstrate that the proposed deep representation can outperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG, Gabor) and the state-of-art approaches under the same experimental protocols. version:1
arxiv-1511-03012 | Information retrieval in folktales using natural language processing | http://arxiv.org/abs/1511.03012 | id:1511.03012 author:Adrian Groza, Lidia Corde category:cs.CL cs.IR  published:2015-11-10 summary:Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology. version:1
arxiv-1511-02999 | Improvised Salient Object Detection and Manipulation | http://arxiv.org/abs/1511.02999 | id:1511.02999 author:Abhishek Maity category:cs.CV I.4  published:2015-11-10 summary:In case of salient subject recognition, computer algorithms have been heavily relied on scanning of images from top-left to bottom-right systematically and apply brute-force when attempting to locate objects of interest. Thus, the process turns out to be quite time consuming. Here a novel approach and a simple solution to the above problem is discussed. In this paper, we implement an approach to object manipulation and detection through segmentation map, which would help to desaturate or, in other words, wash out the background of the image. Evaluation for the performance is carried out using the Jaccard index against the well-known Ground-truth target box technique. version:1
arxiv-1511-02992 | Traffic Sign Classification Using Deep Inception Based Convolutional Networks | http://arxiv.org/abs/1511.02992 | id:1511.02992 author:Mrinal Haloi category:cs.CV 68T45  published:2015-11-10 summary:In this work, we propose a novel deep networks for traffic sign classification that achieves outstanding performance on GTSRB surpassing all previous methods. Our deep network consists of spatial transformer layers and a modified version of inception module specifically designed for capturing local and global features together. This features adoption allows our network to classify precisely intra class samples even under deformations. Use of spatial transformer layer make this network more robust to deformations such as translation, rotation, scaling of input images. Unlike existing approaches that are developed with hand crafted features, multiple deep networks with huge parameters and data augmentations, our method addresses the concern of exploding parameters and augmentations. We have achieved state-of-the-art performance of 99.81% on GTSRB dataset. version:1
arxiv-1504-05800 | On the Generalization Properties of Differential Privacy | http://arxiv.org/abs/1504.05800 | id:1504.05800 author:Kobbi Nissim, Uri Stemmer category:cs.LG cs.CR  published:2015-04-22 summary:A new line of work, started with Dwork et al., studies the task of answering statistical queries using a sample and relates the problem to the concept of differential privacy. By the Hoeffding bound, a sample of size $O(\log k/\alpha^2)$ suffices to answer $k$ non-adaptive queries within error $\alpha$, where the answers are computed by evaluating the statistical queries on the sample. This argument fails when the queries are chosen adaptively (and can hence depend on the sample). Dwork et al. showed that if the answers are computed with $(\epsilon,\delta)$-differential privacy then $O(\epsilon)$ accuracy is guaranteed with probability $1-O(\delta^\epsilon)$. Using the Private Multiplicative Weights mechanism, they concluded that the sample size can still grow polylogarithmically with the $k$. Very recently, Bassily et al. presented an improved bound and showed that (a variant of) the private multiplicative weights algorithm can answer $k$ adaptively chosen statistical queries using sample complexity that grows logarithmically in $k$. However, their results no longer hold for every differentially private algorithm, and require modifying the private multiplicative weights algorithm in order to obtain their high probability bounds. We greatly simplify the results of Dwork et al. and improve on the bound by showing that differential privacy guarantees $O(\epsilon)$ accuracy with probability $1-O(\delta\log(1/\epsilon)/\epsilon)$. It would be tempting to guess that an $(\epsilon,\delta)$-differentially private computation should guarantee $O(\epsilon)$ accuracy with probability $1-O(\delta)$. However, we show that this is not the case, and that our bound is tight (up to logarithmic factors). version:2
arxiv-1503-04843 | More General Queries and Less Generalization Error in Adaptive Data Analysis | http://arxiv.org/abs/1503.04843 | id:1503.04843 author:Raef Bassily, Adam Smith, Thomas Steinke, Jonathan Ullman category:cs.LG cs.DS  published:2015-03-16 summary:Adaptivity is an important feature of data analysis---typically the choice of questions asked about a dataset depends on previous interactions with the same dataset. However, generalization error is typically bounded in a non-adaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated the formal study of this problem, and gave the first upper and lower bounds on the achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution $\mathcal{P}$ and a set of $n$ independent samples $x$ is drawn from $\mathcal{P}$. We seek an algorithm that, given $x$ as input, "accurately" answers a sequence of adaptively chosen "queries" about the unknown distribution $\mathcal{P}$. How many samples $n$ must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions towards resolving this question: *We give upper bounds on the number of samples $n$ that are needed to answer statistical queries that improve over the bounds of Dwork et al. *We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and the important class of convex risk minimization queries. As in Dwork et al., our algorithms are based on a connection between differential privacy and generalization error, but we feel that our analysis is simpler and more modular, which may be useful for studying these questions in the future. version:2
arxiv-1511-03570 | Dimension of Marginals of Kronecker Product Models | http://arxiv.org/abs/1511.03570 | id:1511.03570 author:Guido Montufar, Jason Morton category:stat.ML cs.NE math.AG math.CO math.PR 14T05  52B05  published:2015-11-10 summary:A Kronecker product model is the set of visible marginal probability distributions of an exponential family whose sufficient statistics matrix factorizes as a Kronecker product of two matrices, one for the visible variables and one for the hidden variables. We estimate the dimension of these models by the maximum rank of the Jacobian in the limit of large parameters. The limit is described by the tropical morphism; a piecewise linear map with pieces corresponding to slicings of the visible matrix by the normal fan of the hidden matrix. We obtain combinatorial conditions under which the model has the expected dimension, equal to the minimum of the number of natural parameters and the dimension of the ambient probability simplex. Additionally, we prove that the binary restricted Boltzmann machine always has the expected dimension. version:1
arxiv-1511-02928 | Hyperspectral Image Recovery from Incomplete and Imperfect Measurements via Hybrid Regularization | http://arxiv.org/abs/1511.02928 | id:1511.02928 author:Reza Arablouei, Frank de Hoog category:cs.CV  published:2015-11-09 summary:Natural images tend to mostly consist of smooth regions with individual pixels having highly correlated spectra. This information can be exploited to recover hyperspectral images of natural scenes from their incomplete and noisy measurements. To perform the recovery while taking full advantage of the prior knowledge, we formulate a composite cost function containing a square-error data-fitting term and two distinct regularization terms pertaining to spatial and spectral domains. The regularization for the spatial domain is the sum of total-variation of the image frames corresponding to all spectral bands. The regularization for the spectral domain is the l_1-norm of the coefficient matrix obtained by applying a suitable sparsifying transform to the spectra of the pixels. We use an accelerated proximal-subgradient method to minimize the formulated cost function. We analyse the performance of the proposed algorithm and prove its convergence. Numerical simulations using real hyperspectral images exhibit that the proposed algorithm offers an excellent recovery performance with a number of measurements that is only a small fraction of the hyperspectral image data size. Simulation results also show that the proposed algorithm significantly outperforms an accelerated proximal-gradient algorithm that solves the classical basis-pursuit denoising problem to recover the hyperspectral image. version:1
arxiv-1507-05670 | Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries | http://arxiv.org/abs/1507.05670 | id:1507.05670 author:Yuke Zhu, Ce Zhang, Christopher Ré, Li Fei-Fei category:cs.CV cs.LG  published:2015-07-20 summary:The complexity of the visual world creates significant challenges for comprehensive visual understanding. In spite of recent successes in visual recognition, today's vision systems would still struggle to deal with visual queries that require a deeper reasoning. We propose a knowledge base (KB) framework to handle an assortment of visual queries, without the need to train new classifiers for new tasks. Building such a large-scale multimodal KB presents a major challenge of scalability. We cast a large-scale MRF into a KB representation, incorporating visual, textual and structured data, as well as their diverse relations. We introduce a scalable knowledge base construction system that is capable of building a KB with half billion variables and millions of parameters in a few hours. Our system achieves competitive results compared to purpose-built models on standard recognition and retrieval tasks, while exhibiting greater flexibility in answering richer visual queries. version:2
arxiv-1511-02919 | Massive Online Crowdsourced Study of Subjective and Objective Picture Quality | http://arxiv.org/abs/1511.02919 | id:1511.02919 author:Deepti Ghadiyaram, Alan C. Bovik category:cs.CV  published:2015-11-09 summary:Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Towards overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment subjective study. Our database consists of over 350000 opinion scores on 1162 images evaluated by over 7000 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective dataset. We also evaluate several top-performing blind Image Quality Assessment algorithms on it and present insights on how mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models. version:1
arxiv-1511-02916 | Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders | http://arxiv.org/abs/1511.02916 | id:1511.02916 author:Zhouhan Lin, Yushi Chen, Xing Zhao, Gang Wang category:cs.CV cs.AI cs.LG  published:2015-11-09 summary:Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM. version:1
arxiv-1511-02909 | Efficient Construction of Local Parametric Reduced Order Models Using Machine Learning Techniques | http://arxiv.org/abs/1511.02909 | id:1511.02909 author:Azam Moosavi, Razvan Stefanescu, Adrian Sandu category:cs.LG  published:2015-11-09 summary:Reduced order models are computationally inexpensive approximations that capture the important dynamical characteristics of large, high-fidelity computer models of physical systems. This paper applies machine learning techniques to improve the design of parametric reduced order models. Specifically, machine learning is used to develop feasible regions in the parameter space where the admissible target accuracy is achieved with a predefined reduced order basis, to construct parametric maps, to chose the best two already existing bases for a new parameter configuration from accuracy point of view and to pre-select the optimal dimension of the reduced basis such as to meet the desired accuracy. By combining available information using bases concatenation and interpolation as well as high-fidelity solutions interpolation we are able to build accurate reduced order models associated with new parameter settings. Promising numerical results with a viscous Burgers model illustrate the potential of machine learning approaches to help design better reduced order models. version:1
arxiv-1511-02872 | Visual Language Modeling on CNN Image Representations | http://arxiv.org/abs/1511.02872 | id:1511.02872 author:Hiroharu Kato, Tatsuya Harada category:cs.CV cs.AI cs.LG  published:2015-11-09 summary:Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets. version:1
arxiv-1506-04834 | Tree-structured composition in neural networks without tree-structured architectures | http://arxiv.org/abs/1506.04834 | id:1506.04834 author:Samuel R. Bowman, Christopher D. Manning, Christopher Potts category:cs.CL cs.LG  published:2015-06-16 summary:Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find an LSTM-based sequence model can indeed learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree models, even on large training sets, suggesting that tree-structured models are more effective at exploiting recursive structure. version:3
