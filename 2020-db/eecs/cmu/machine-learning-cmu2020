MLG10301 | Introduction to Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10301&SEMESTER=S20 | instructors:Gormley, Matthew prereq:(15122) and (21127 or 15151 or 21128) and (36218 or 36225 or 21325 or 15359 or 36217 or 36219) description:Machine Learning (ML) develops computer programs that automatically improve their performance through experience. This includes learning many types of tasks based on many types of experience, e.g. spotting high-risk medical patients, recognizing speech, classifying text documents, detecting credit card fraud, or driving autonomous vehicles. 10301 covers all or most of: concept learning, decision trees, neural networks, linear learning, active learning, estimation  the bias-variance tradeoff, hypothesis testing, Bayesian learning, the MDL principle, the Gibbs classifier, Naive Bayes, Bayes Nets  Graphical Models, the EM algorithm, Hidden Markov Models, K-Nearest-Neighbors and nonparametric learning, reinforcement learning, bagging, boosting and discriminative training. Grading will be based on weekly or biweekly assignments (written and/or programming), a midterm, a final exam. 10301 is recommended for undergraduates who are not SCS majors. (SCS majors should instead take 10315.) Prerequisites (strictly enforced): strong quantitative aptitude, college probability  statistics course, and programming proficiency. For learning to apply ML practically  effectively, without the above prerequisites, consider 11344/05834 instead. You can evaluate your ability to take the course via a self-assessment exam (http://bit.ly/2fkddDN). Also, be sure to read the ML course comparison (http://bit.ly/2eV3UaD).
MLG10315 | Introduction to Machine Learning (SCS Majors) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10315&SEMESTER=S20 | instructors:Virtue, Patrick prereq:(15122) and (15151 or 21128 or 21127) and (36225 or 36218 or 36217 or 15359 or 21325 or 36219) description:Machine learning is subfield of computer science with the goal of exploring, studying, and developing learning systems, methods, and algorithms that can improve their performance with learning from data. This course is designed to give undergraduate students a one-semester-long introduction to the main principles, algorithms, and applications of machine learning and is specifically designed for the SCS undergrad majors. The topics of this course will be in part parallel with those covered in the graduate machine learning courses (10-715, 10-701, 10-601), but with a greater emphasis on applications and case studies in machine learning.  After completing the course, students will be able to:  *select and apply an appropriate supervised learning algorithm for classification problems (e.g., naive Bayes, perceptron, support vector machine, logistic regression).   *select and apply an appropriate supervised learning algorithm for regression problems (e.g., linear regression, ridge regression).  *recognize different types of unsupervised learning problems, and select and apply appropriate algorithms (e.g., clustering, linear and nonlinear dimensionality reduction).   *work with probabilities (Bayes rule, conditioning, expectations, independence), linear algebra (vector and matrix operations, eigenvectors, SVD), and calculus (gradients, Jacobians) to derive machine learning methods such as linear regression, naive Bayes, and principal components analysis.   *understand machine learning principles such as model selection, overfitting, and underfitting, and techniques such as cross-validation and regularization.   *implement machine learning algorithms such as logistic regression via stochastic gradient descent, linear regression (using a linear algebra toolbox), perceptron, or k-means clustering.   *run appropriate supervised and unsupervised learning algorithms on real and synthetic data sets and interpret the results.
MLG10403 | Deep Reinforcement Learning &amp; Control | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10403&SEMESTER=S20 | instructors:Fragkiadaki, Katerina prereq:10301 or 10315 or 10401 or 10601 or 10701 description:TBD
MLG10405 | Machine Learning with Large Datasets (Undergraduate) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10405&SEMESTER=S20 | instructors:Smith, VirginiaMiller, Heather prereq:15211 or 15210 or 15214 or 17214 description:Large datasets are difficult to work with for several reasons. They are difficult to visualize, and it is difficult to understand what sort of errors and biases are present in them. They are computationally expensive to process, and often the cost of learning is hard to predict - for instance, and algorithm that runs quickly in a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.  This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the issues considered are: scalable learning techniques, such as streaming machine learning techniques; parallel infrastructures such as map-reduce; practical techniques for reducing the memory requirements for learning methods, such as feature hashing and Bloom filters; and techniques for analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity.  The class will include programming assignments, and a one-month short project chosen by the student. The project will be designed to compare the scalability of variant learning algorithms on datasets.  An introductory course in machine learning, like 10-301, 10-315, or 10-701, is a prerequisite or a co-requisite. If you plan to take this course and the introductory machine learning course concurrently please tell the instructor. The course will include several substantial programming assignments, so an additional prerequisite is 15-211, or 15-214, or comparable familiarity with Java and good programming skills.
MLG10417 | Intermediate Deep Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10417&SEMESTER=F19 | instructors:Salakhutdinov, Ruslan prereq:10301 or 10315 or 10715 or 10601 or 10701 description:Building intelligent machines that are capable of extracting meaningful representations from data lies at the core of solving many AI related tasks. In the past decade, researchers across many communities, from applied statistics to engineering, computer science and neuroscience, have developed deep models that are composed of several layers of nonlinear processing. An important property of these models is that they can learn useful representations by re-using and combining intermediate concepts, allowing these models to be successfully applied in a wide variety of domains, including visual object recognition, information retrieval, natural language processing, and speech perception.  The goal of this course is to introduce students to both the foundational ideas and the recent advances in deep learning. The first part of the course will focus on supervised learning, including neural networks, back-propagation algorithm, convolutional models, recurrent neural networks, and  their extensions with applications to image recognition, video analysis, and language modelling. The second part of the course will cover unsupervised learning, including variational autoencoders, sparse-coding, Boltzmann machines, and generative adversarial networks. This course will assume a reasonable degree of mathematical maturity and will require strong programming skills.
MLG10418 | Machine Learning for Structured Data | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10418&SEMESTER=F19 | instructors:Gormley, Matthew prereq:10301 or 10315 or 10401 or 10601 or 10701 or 10715 description:A key challenge in machine learning is that of structured prediction: taking unstructured data as input and producing a structured output. Structured prediction problems abound throughout application areas such as natural language processing, speech processing, computational biology, computer vision, healthcare, and many others. In this course, we will study modern approaches to structured prediction building on probabilistic graphical models, deep learning, and search. The course will focus on three key aspects: models, inference, and learning. The models we consider will focus on both generative and discriminative models such as Bayesian networks, Markov random fields (MRFs), conditional random fields (CRFs), and deep neural networks including convolutional neural networks (CNNs) and recurrent neural networks (RNNs) -- as well as hybrids of graphical models and neural networks. The course will explore approaches to exact and approximate inference: junction tree algorithm, approximate marginal inference by Markov chain Monte Carlo (MCMC) and variational methods, approximate MAP inference by integer linear programming (ILP) and search. We will explore unsupervised, semi-supervised, and supervised learning using different formulations of the learning problem: MLE, Bayesian inference, structured perceptron, M3Ns, learning to search, and autoencoders. Covered applications will include machine translation, speech recognition, DNA sequence analysis, scene understanding, medical diagnosis. This course is cross-listed as 10-418 and 10-618; students registered for 10-618 will do a course project.
MLG10500 | Senior Research Project | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10500&SEMESTER=S20 | instructors:Gormley, Matthew description:Register for this course if you are minoring in Machine Learning. This course is intended for research with a faculty member that would count towards the minor.
MLG10520 | Independent Study | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10520&SEMESTER=S20 | instructors:Gormley, Matthew description:Independent Study intended to work on research with a Machine Learning faculty member.
MLG10601 | Introduction to Machine Learning (Master's) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10601&SEMESTER=S20 | instructors:Gormley, Matthew prereq:(15122) and (21128 or 15151 or 21127) and (36217 or 21325 or 36225 or 36218 or 15359 or 36219) description:Machine Learning (ML) develops computer programs that automatically improve their performance through experience. This includes learning many types of tasks based on many types of experience, e.g. spotting high-risk medical patients, recognizing speech, classifying text documents, detecting credit card fraud, or driving autonomous vehicles. 10601 covers all or most of: concept learning, decision trees, neural networks, linear learning, active learning, estimation  the bias-variance tradeoff, hypothesis testing, Bayesian learning, the MDL principle, the Gibbs classifier, Naive Bayes, Bayes Nets  Graphical Models, the EM algorithm, Hidden Markov Models, K-Nearest-Neighbors and nonparametric learning, reinforcement learning, bagging, boosting and discriminative training. Grading will be based on weekly or biweekly assignments (written and/or programming), a midterm, a final exam. 10601 is recommended for CS Seniors  Juniors, quantitative Masters students,  non-MLD PhD students. Prerequisites (strictly enforced): strong quantitative aptitude, college probability  statistics course, and programming proficiency. For learning to apply ML practically  effectively, without the above prerequisites, consider 11344/05834 instead. You can evaluate your ability to take the course via a self-assessment exam (http://bit.ly/2fkddDN). Also, be sure to read the ML course comparison (http://bit.ly/2eV3UaD).
MLG10605 | Machine Learning with Large Datasets | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10605&SEMESTER=S20 | instructors:Smith, VirginiaMiller, Heather prereq:15214 or 15210 or 17214 description:Large datasets are difficult to work with for several reasons. They are difficult to visualize, and it is difficult to understand what sort of errors and biases are present in them. They are computationally expensive to process, and often the cost of learning is hard to predict - for instance, and algorithm that runs quickly in a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.  This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the issues considered are: scalable learning techniques, such as streaming machine learning techniques; parallel infrastructures such as map-reduce; practical techniques for reducing the memory requirements for learning methods, such as feature hashing and Bloom filters; and techniques for analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity.  The class will include programming assignments, and a one-month short project chosen by the student. The project will be designed to compare the scalability of variant learning algorithms on datasets.  An introductory course in machine learning, like 10-601 or 10-701, is a prerequisite or a co-requisite. If you plan to take this course and 10-601 concurrently please tell the instructor.  The course will include several substantial programming assignments, so an additional prerequisite is 15-211, or 15-214, or comparable familiarity with Java and good programming skills.
MLG10617 | Intermediate Deep Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10617&SEMESTER=F19 | instructors:Salakhutdinov, Ruslan prereq:10301 or 10315 or 10701 or 10601 or 10715 description:Building intelligent machines that are capable of extracting meaningful representations from data lies at the core of solving many AI related tasks. In the past decade, researchers across many communities, from applied statistics to engineering, computer science and neuroscience, have developed deep models that are composed of several layers of nonlinear processing. An important property of these models is that they can learn useful representations by re-using and combining intermediate concepts, allowing these models to be successfully applied in a wide variety of domains, including visual object recognition, information retrieval, natural language processing, and speech perception.   The goal of this course is to introduce students to both the foundational ideas and the recent advances in deep learning. The first part of the course will focus on supervised learning, including neural networks, back-propagation algorithm, convolutional models, recurrent neural networks, and  their extensions with applications to image recognition, video analysis, and language modelling. The second part of the course will cover unsupervised learning, including variational autoencoders, sparse-coding, Boltzmann machines, and generative adversarial networks. This course will assume a reasonable degree of mathematical maturity and will require strong programming skills.
MLG10618 | Machine Learning for Structured Data | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10618&SEMESTER=F19 | instructors:Gormley, Matthew prereq:10301 or 10315 or 10401 or 10601 or 10701 or 10715 description:A key challenge in machine learning is that of structured prediction: taking unstructured data as input and producing a structured output. Structured prediction problems abound throughout application areas such as natural language processing, speech processing, computational biology, computer vision, healthcare, and many others. In this course, we will study modern approaches to structured prediction building on probabilistic graphical models, deep learning, and search. The course will focus on three key aspects: models, inference, and learning. The models we consider will focus on both generative and discriminative models such as Bayesian networks, Markov random fields (MRFs), conditional random fields (CRFs), and deep neural networks including convolutional neural networks (CNNs) and recurrent neural networks (RNNs) -- as well as hybrids of graphical models and neural networks. The course will explore approaches to exact and approximate inference: junction tree algorithm, approximate marginal inference by Markov chain Monte Carlo (MCMC) and variational methods, approximate MAP inference by integer linear programming (ILP) and search. We will explore unsupervised, semi-supervised, and supervised learning using different formulations of the learning problem: MLE, Bayesian inference, structured perceptron, M3Ns, learning to search, and autoencoders. Covered applications will include machine translation, speech recognition, DNA sequence analysis, scene understanding, medical diagnosis. This course is cross-listed as 10-418 and 10-618; students registered for 10-618 will do a course project.
MLG10620 | Independent Study: Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10620&SEMESTER=S20 | instructors:Fragkiadaki, KaterinaSalakhutdinov, Ruslan description:Independent Study intended to work on research with a Machine Learning faculty member.
MLG10697 | Reading and Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10697&SEMESTER=S20 | instructors:Fragkiadaki, KaterinaSalakhutdinov, Ruslan description:Course for MS students to work with their advisor on research
MLG10701 | Introduction to Machine Learning (PhD) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10701&SEMESTER=S20 | instructors:Mitchell, TomWehbe, Leila prereq:(15122) and (21127 or 21128 or 15151) and (21325 or 36217 or 36219 or 36225 or 36218 or 15259 or 15359) description:Machine learning studies the question How can we build computer programs that automatically improve their performance through experience?   This includes learning to perform many types of tasks based on many types of experience.  For example, it includes robots learning to better navigate based on experience gained by roaming their environments, medical decision aids that learn to predict which therapies work best for which diseases based on data mining of historical health records, and speech recognition systems that learn to better understand your speech based on experience listening to you.  This course is designed to give PhD students a thorough grounding in the methods, mathematics and algorithms needed to do research and applications in machine learning. Students entering the class with a pre-existing working knowledge of probability, statistics and algorithms will be at an advantage, but the class has been designed so that anyone with a strong numerate background can catch up and fully participate. You can evaluate your ability to take the course via a self-assessment exam that will be made available to you  after you register.  If you are interested in this topic, but are not a PhD student, or are a PhD student not specializing in machine learning, you might consider the masters level course on Machine Learning, 10-601.  This class may be appropriate for MS and undergrad students who are interested in the theory and algorithms behind ML.  You can evaluate your ability to take the course via a self-assessment exam at: https://qna-app.appspot.com/view.html?aglzfnFuYS1hcHByGQsSDFF1ZXN0aW9uTGlzdBiAgICgpO-KCgw ML course comparison: https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit
MLG10703 | Deep Reinforcement Learning &amp; Control | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10703&SEMESTER=F19 | instructors:Fragkiadaki, Katerina prereq:10301 or 10315 or 10401 or 10601 or 10701 or 10715 description:This course will cover latest advances in Reinforcement Learning and Imitation learning. This is a fast developing research field and an official textbook is available only for about one forth of the course material. The rest will be taught from recent research papers. This course brings together many disciplines of Artificial Intelligence to show how to develop intelligent agent that can learn to sense the world and learn to act imitating others or maximizing sparse rewards Particular focus will be given in  incorporating visual sensory input and learning suitable visual state representations.
MLG10707 | Advanced Deep Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10707&SEMESTER=S20 | instructors:Risteski, Andrej prereq:10315 or 10401 or 10715 or 10701 or 10601 description:Building intelligent machines that are capable of extracting meaningful representations from high-dimensional data lies at the core of solving many AI related tasks. In the past few years, researchers across many different communities, from applied statistics to engineering, computer science and neuroscience, have developed deep (hierarchical) models -- models that are composed of several layers of nonlinear processing. An important property of these models is that they can learn useful representations by re-using and combining intermediate concepts, allowing these models to be successfully applied in a wide variety of domains, including visual object recognition, information retrieval, natural language processing, and speech perception.  This is an advanced graduate course, designed for Masters and Ph.D. level students, and will assume a reasonable degree of mathematical maturity. The goal of this course is to introduce students to the recent and exciting developments of various deep learning methods. Some topics to be covered include: restricted Boltzmann machines (RBMs) and their multi-layer extensions Deep Belief Networks and Deep Boltzmann machines; sparse coding, autoencoders, variational autoencoders, convolutional neural networks, recurrent neural networks, generative adversarial networks, and attention-based models with applications in vision, NLP, and multimodal learning. We will also address mathematical issues, focusing on efficient large-scale optimization methods for inference and learning, as well as training density models with intractable partition functions.  Prerequisite: ML: 10-701 or 10-715, and strong programming skills.
MLG10708 | Probabilistic Graphical Models | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10708&SEMESTER=S20 | instructors:Xing, Eric prereq:10715 or 10701 description:Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enabling efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.  The class will cover three aspects: The core representation, including Bayesian and Markov networks, and dynamic Bayesian networks; probabilistic inference algorithms, both exact and approximate; and, learning methods for both the parameters and the structure of graphical models. Students entering the class should have a pre-existing working knowledge of probability, statistics, and algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate.  It is expected that after taking this class, the students should have obtain sufficient working knowledge of multi-variate probabilistic modeling and inference for practical applications, should be able to formulate and solve a wide range of problems in their own domain using GM, and can advance into more specialized technical literature by themselves.  Students are required to have successfully completed 10701 or 10715, or an equivalent class.
MLG10715 | Advanced Introduction to Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10715&SEMESTER=F19 | instructors:Shah, Nihar prereq:(15122) and (21128 or 21127 or 15151) and (15259 or 15359 or 36225 or 36217 or 21325 or 36218) description:The rapid improvement of sensory techniques and processor speed, and the availability of inexpensive massive digital storage, have led to a growing demand for systems that can automatically comprehend and mine massive and complex data from diverse sources.  Machine Learning is becoming the primary mechanism by which information is extracted from Big Data, and a primary pillar that Artificial Intelligence is built upon.  This course is designed for Ph.D. students whose primary field of study is machine learning, and who intend to make machine learning methodological research a main focus of their thesis.  It will give students a thorough grounding in the algorithms, mathematics, theories, and insights needed to do in-depth research and applications in machine learning. The topics of this course will in part parallel those covered in the general PhD-level machine learning course (10-701), but with a greater emphasis on depth in theory and algorithms. The course will also include additional advanced topics such as fairness in machine learning. Students entering the class are expected to have a pre-existing strong working knowledge of algorithms, linear algebra, probability, and statistics.  If you are interested in this topic, but do not have the required background or are not planning to work on a PhD thesis with machine learning as the main focus, you might consider the general PhD-level Machine Learning course (10-701) or the Masters-level Machine Learning course (10-601). ML course comparison: https://goo.gl/mmR2eL
MLG10716 | Advanced Machine Learning: Theory and Methods | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10716&SEMESTER=S20 | instructors:Ravikumar, Pradeep prereq:(10701 or 10715) and (36700 or 36705) description:Advanced Machine Learning: Theory and Methods is a graduate level course introducing the theoretical foundations of modern machine learning, as well as advanced methods and frameworks used in modern machine learning. The course assumes that students have taken graduate level introductory courses in machine learning (Introduction to Machine Learning, 10-701 or 10-715), as well as Statistics (Intermediate Statistics, 36-700 or 36-705). The course treats both the art of designing good learning algorithms, as well as the science of analyzing an algorithms computational and statistical properties and performance guarantees. Theorems are presented together with practical aspects of methodology and intuition to help students develop tools for selecting appropriate methods and approaches to problems in their own research. We will cover theoretical foundation topics such as computational and statistical convergence rates, minimax estimation, and concentration of measure. We will also cover advanced machine learning methods such as nonparametric density estimation, nonparametric regression, and Bayesian estimation, as well as advanced frameworks such as privacy, causality, and stochastic learning algorithms.
MLG10718 | Data Analysis | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10718&SEMESTER=S20 | instructors:Ghani, RayidRodolfa, Kit description:This is a project-based course designed to provide students training and experience in solving real-world problems using machine learning, with a focus on problems from public policy and social good.  Through lectures, discussions, readings, and project assignments, students will learn about and experience building end-to-end machine learning systems, starting from project definition and scoping, through modeling, to field validation and turning their analysis into action. Through the course, students will develop skills in problem formulation, working with messy data, communicating about machine learning with non-technical stakeholders, model interpretability, understanding and mitigating algorithmic bias  disparities, and evaluating the impact of deployed models.  Students will be expected to know python, and have prior coursework in machine learning.
MLG10725 | Convex Optimization | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10725&SEMESTER=S20 | instructors:Li, Yuanzhi description:Nearly every problem in machine learning can be formulated as the optimization of some function, possibly under some set of constraints. This universal reduction may seem to suggest that such optimization tasks are intractable. Fortunately, many real world problems have special structure, such as convexity, smoothness, separability, etc., which allow us to formulate optimization problems that can often be solved efficiently. This course is designed to give a graduate-level student a thorough grounding in the formulation of optimization problems that exploit such structure, and in efficient solution methods for these problems. The main focus is on the formulation and solution of convex optimization problems, though we will discuss some recent advances in nonconvex optimization. These general concepts will also be illustrated through applications in machine learning and statistics. Students entering the class should have a pre-existing working knowledge of algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate. Though not required, having taken 10-701 or an equivalent machine learning or statistical modeling class is strongly encouraged, as we will use applications in machine learning and statistics to demonstrate the concepts we cover in class. Students will work on an extensive optimization-based project throughout the semester.
MLG10737 | Creative AI | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10737&SEMESTER=F19 | instructors:Kang, EunsuOh, Hyaejin description:Artificial intelligence (AI) systems now generate authentic paintings, compose music pieces, and find out-of-box solutions to real-life problems in our world. Creativity, which was considered to be a moon shot for AI, does not seem to be too far any more. Is that true? Are we close to see creative AI? The answer is yes and no. We are moving closer with meaningful developments in Machine Learning, however there are several questions to be explored further to achieve the creative AI. What kind of creativity we want to represent? How do we translate creativity into what machines can understand? How do we design ML algorithms to be more creative?   This course is where we explore these questions through seminars and projects. Our goal is to design computational models that present the very possibility of the creative AI.   The instructors who are specialized in Machine Learning Art and Robotics lead this course together. We introduce related examples and possible methods including multi-modal data-driven learning, learning from demonstration, and combined learning from data and human demonstrations. Students are welcome to bring in their expertise and passion from diverse backgrounds to explore this topic together.
MLG10745 | Scalability in Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10745&SEMESTER=F19 | instructors:Poczos, Barnabas prereq:10301 or 10315 or 10401 or 10601 or 10701 or 10715 description:The goal of this course is to provide a survey into some of the recent advances in the theory and practice of dealing with scalability issues in machine learning. We will investigate scalability issues along the following dimensions: Challenges with i) large datasets, ii) high-dimensions, and iii) complex data structure. The course is intended to prepare students to write research papers about scalability issues in machine learning. This is an advanced-level, fast-paced course that requires students to already have a solid understanding of machine learning (e.g. by taking an intro to ML class), good programming skills in Python, and being comfortable with dealing with abstract mathematical concepts and reading research papers. The course will have significant overlap with 10-405/605/805, but 10-745 will be faster-paced and go deeper into the theoretical investigations of the methods. Some of the classes will be flipped that will require students to watch a video lecture or read a research paper before the class, and the content will be discussed during the class time.  The class will include a course project, HW assignments, and two-in class exams.
MLG10920 | Graduate Reading and Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10920&SEMESTER=S20 | instructors:Mitchell, Tom description:This course is for graduate students to work on research with their advisor before they propose their thesis topic.
MLG10930 | Dissertation Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10930&SEMESTER=S20 | instructors:Mitchell, Tom description:This course is for graduate students to work on their dissertation research after they have proposed their thesis topic.
MLG10935 | Practicum | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10935&SEMESTER=S20 | instructors:Mitchell, Tom description:This course is intended for you to gain industry research experience while using the skills you have learned in the ML curriculum and will count towards the research units for your degree.
MLG10940 | Independent Study | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10940&SEMESTER=S20 | instructors:Mitchell, Tom description:Independent Study to be used to work on research with a Machine Learning faculty member.
