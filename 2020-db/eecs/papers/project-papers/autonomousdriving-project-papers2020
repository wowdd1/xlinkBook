autonomousdriving-paper-1 | Group Induction | http://cs.stanford.edu/people/teichman/papers/iros2013-group_induction.pdf | author:Alex Teichman and Sebastian Thrun. journal:Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013 description:Tracking-based semi-supervised learning, as originally presented at RSS2011, was an offline algorithm. This is fine in some contexts, but ideally a user could provide new hand-labeled training examples online, as the system runs, without retraining from scratch. Qualitatively, this would mean the ability to point out - from the back seat of your autonomous car - a few examples of, say, an elliptical bike or sk8poler, and the algorithm would start learning to recognize them on the fly without you having to do anything else. Group induction is a mathematical framework for this kind of learning.
autonomousdriving-paper-2 | Automatic Online Calibration of Cameras and Lasers | http://driving.stanford.edu/papers/RSS2013.pdf | author:Jesse Levinson and Sebastian Thrun. journal:Robotics: Science and Systems (RSS), 2013. description:Extending previous work on offline 6-DOF calibration of 3D laser sensors to 2D cameras, this paper presents two new real-time techniques that enable camera-laser calibration online, automatically, and in arbitrary environments. The first is a probabilistic monitoring algorithm that can detect a sudden mis-calibration in a fraction of a second. The second is a continuous calibration optimizer that adjusts transform offsets in real time, tracking gradual sensor drift as it occurs. Together, these techniques allow significantly greater flexibility and adaptability of robots in unknown and potentially harsh environments.
autonomousdriving-paper-3 | Precision Tracking with Sparse 3D and Dense Color 2D Data | http://driving.stanford.edu/papers/ICRA2013.pdf | author:David Held, Jesse Levinson, and Sebastian Thrun. journal:International Conference on Robotics and Automation (ICRA), 2013. description:Precision tracking is important for predicting the behavior of other cars in autonomous driving. We present a novel method to combine sparse laser data with a high-resolution camera image to achieve accurate velocity estimates of moving objects. We present a color-augmented, pre-filtered grid search algorithm to align the points from a tracked object, thereby obtaining much more precise estimates of the tracked vehicleâ€™s velocity than were possible with previous methods.
autonomousdriving-paper-4 | Online, semi-supervised learning for long-term interaction with object recognition systems | http://cs.stanford.edu/people/teichman/papers/rss2012_presentation.pdf | author:Alex Teichman and Sebastian Thrun. journal:Invited talk at RSS Workshop on Long-term Operation of Autonomous Robotic Systems in Changing Environments, 2012. description:Tracking-based semi-supervised learning, as originally presented at RSS2011, was an offline algorithm. This is fine in some contexts, but ideally a user could provide new hand-labeled training examples online, as the system runs, without retraining from scratch. Qualitatively, this would mean the ability to point out - from the back seat of your autonomous car - a few examples of, say, an elliptical bike or sk8poler , and tracking-based semi-supervised learning would start learning to recognize them on the fly without you having to do anything else. This talk discusses some preliminary work in this direction.
autonomousdriving-paper-5 | A Probabilistic Framework for Object Detection in Images using Context and Scale | http://driving.stanford.edu/papers/ICRA2012.pdf | author:David Held, Jesse Levinson, and Sebastian Thrun. journal:International Conference on Robotics and Automation (ICRA), 2012 description:Detecting cars in real-world images is an important task for autonomous driving, yet it remains unsolved. The system described in this paper takes advantage of context and scale to build a monocular single-frame image-based car detector that significantly outperforms previous state-of-the-art methods. By using a calibrated camera and localization on a road map, we are able to obtain context and scale information from a single image without the use of a 3D laser.
autonomousdriving-paper-6 | Practical object recognition in autonomous driving and beyond | http://cs.stanford.edu/people/teichman/papers/arso2011.pdf | author:Alex Teichman and Sebastian Thrun. journal:IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO), 2011 description:This paper gives an overview of the recent object recognition research in our lab and what is needed to make it a fully functional, high accuracy object recognition system that is applicable beyond perception for autonomous driving.
autonomousdriving-paper-7 | Tracking-based semi-supervised learning | http://cs.stanford.edu/people/teichman/papers/rss2011.pdf | author:Alex Teichman and Sebastian Thrun. journal:Robotics: Science and Systems (RSS), 2011 description:Building on previous work, we introduce a simple semi-supervised learning method that uses tracking information to find new, useful training examples automatically. This method achieves nearly the same accuracy as before, but with about two orders of magnitude less human labeling effort.
autonomousdriving-paper-8 | Towards 3D object recognition via classification of arbitrary object tracks | http://cs.stanford.edu/people/teichman/papers/icra2011.pdf | author:Alex Teichman, Jesse Levinson, and Sebastian Thrun. journal:International Conference on Robotics and Automation (ICRA), 2011 description:Breaking down the object recognition problem into segmentation, tracking, and track classification components, we show an accurate and real-time method of classifying tracked objects as car, pedestrian, bicyclist, or 'other'.
autonomousdriving-paper-9 | Towards fully autonomous driving: systems and algorithms | http://cs.stanford.edu/people/teichman/papers/iv2011.pdf | author:Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, David Held, Soeren Kammel, J. Zico Kolter, Dirk Langer, Oliver Pink, Vaughan Pratt, Michael Sokolsky, Ganymed Stanek, David Stavens, Alex Teichman, Moritz Werling, and Sebastian Thrun. journal:Intelligent Vehicles Symposium, 2011. description:This paper is a broad summary of recent work on Junior, Stanford's autonomous vehicle. Topics covered include object recognition, sensor calibration, planning, control, etc.
autonomousdriving-paper-10 | Traffic Light Mapping, Localization, and State Detection for Autonomous Vehicles | http://driving.stanford.edu/papers/ICRA2011.pdf | author:Jesse Levinson, Jake Askeland, Jennifer Dolson, and Sebastian Thrun. journal:International Conference on Robotics and Automation (ICRA), 2011. description:We present a passive camera-based pipeline for traffic light state detection using imperfect vehicle localization and assuming prior knowledge of traffic light location. To achieve robust real-time detections in a variety of lighting conditions, we combine several probabilistic stages that explicitly account for the corresponding sources of sensor and data uncertainty.
autonomousdriving-paper-11 | Automatic laser calibration, mapping, and localization for autonomous vehicles | https://stacks.stanford.edu/file/druid:zx701jr9713/JesseThesisFinal2-augmented.pdf | author:Jesse Levinson. journal:Thesis (Ph.D.), Stanford University, 2011. description:This dissertation presents several related algorithms that enable important capabilities for self-driving vehicles. These include offline mapping and online map-based localization techniques using GPS, IMU, and lasers, online localization without a prerecorded map as used in the DARPA Urban Challenge, intrinsic and extrinsic calibration algorithms for multi-beam lasers, and realtime detection of traffic lights.
autonomousdriving-paper-12 | Unsupervised Calibration for Multi-beam Lasers | http://driving.stanford.edu/papers/ISER2010.pdf | author:Jesse Levinson and Sebastian Thrun. journal:International Symposium on Experimental Robotics (ISER), 2010. description:This paper introduces an unsupervised solution for solving the intrinsic and extrinsic calibration properties of a multi-beam laser on a mobile robot in arbitrary, unknown environments. By defining and optimizing an objective function that rewards 3D consistency between points seem by different beams, we are able to calibrate internal angles, range offsets, and remittance response curves for each beam in addition to the 6-DOF position of the laser relative to the vehicle's inertial frame.
autonomousdriving-paper-13 | Robust Vehicle Localization in Urban Environments Using Probabilistic Maps | http://driving.stanford.edu/papers/ICRA2010.pdf | author:Jesse Levinson and Sebastian Thrun. journal:International Conference on Robotics and Automation (ICRA), 2010. description:We extend previous work on localization using GPS, IMU, and LIDAR data by modeling the environment as a probabilistic grid in which every cell is represented as its own gaussian distribution over remittance values. This approach offers higher precision, the ability to learn and improve maps over time, and increased robustness to environment changes and dynamic obstacles.
autonomousdriving-paper-14 | Exponential family sparse coding with application to self-taught learning | http://cs.stanford.edu/people/teichman/papers/ijcai2009.pdf | author:Honglak Lee, Rajat Raina, Alex Teichman, and Andrew Y. Ng. journal:International Joint Conference on Artificial Intelligence (IJCAI), 2009. description:
autonomousdriving-paper-15 | Map-Based Precision Vehicle Localization in Urban Environments | http://driving.stanford.edu/papers/RSS2007.pdf | author:Jesse Levinson and Sebastian Thrun. journal:Robotics: Science and Systems (RSS), 2007. description:GPS-based inertial guidance systems do not provide sufficient accuracy for many urban navigation applications, including autonomous navigation. We propose a technique for high-accuracy localization of moving vehicles that utilizes maps of urban environments. Our approach integrates GPS, IMU, wheel odometry, and LIDAR data to generate high-resolution environment maps. We use offline GraphSLAM techniques to align intersections and regions of self-overlap, and a particle filter to localize the vehicle relative to these maps in real time.
