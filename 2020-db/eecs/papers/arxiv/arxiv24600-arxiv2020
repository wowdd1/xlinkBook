arxiv-1701-04175 | 3D tracking of water hazards with polarized stereo cameras | http://arxiv.org/abs/1701.04175 | id:1701.04175 author:Chuong V. Nguyen, Michael Milford, Robert Mahony category:cs.RO cs.CV  published:2017-01-16 summary:Current self-driving car systems operate well in sunny weather but struggle in adverse conditions. One of the most commonly encountered adverse conditions involves water on the road caused by rain, sleet, melting snow or flooding. While some advances have been made in using conventional RGB camera and LIDAR technology for detecting water hazards, other sources of information such as polarization offer a promising and potentially superior approach to this problem in terms of performance and cost. In this paper, we present a novel stereo-polarization system for detecting and tracking water hazards based on polarization and color variation of reflected light. To evaluate this system, we present a new large water-on-road datasets spanning approximately 2 km of driving in various on road and off road conditions and demonstrate for the first time reliable water detection and tracking over a wide range of realistic car driving water conditions using polarized vision as the primary sensing modality. Our system successfully detects water hazards up to 80m and doubles the success detection range as compared to a similar polarized camera system. Finally, we discuss several interesting challenges and propose future research directions for further improving robust autonomous car perception in hazardous wet conditions using polarization sensors. version:1
arxiv-1701-04143 | Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks | http://arxiv.org/abs/1701.04143 | id:1701.04143 author:Vahid Behzadan, Arslan Munir category:cs.LG cs.AI  published:2017-01-16 summary:Deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. In this work, we establish that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and verify the transferability of adversarial examples across different DQN models. Furthermore, we present a novel class of attacks based on this vulnerability that enable policy manipulation and induction in the learning process of DQNs. We propose an attack mechanism that exploits the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its efficacy and impact through experimental study of a game-learning scenario. version:1
arxiv-1701-04113 | Near Optimal Behavior via Approximate State Abstraction | http://arxiv.org/abs/1701.04113 | id:1701.04113 author:David Abel, D. Ellis Hershkowitz, Michael L. Littman category:cs.LG cs.AI  published:2017-01-15 summary:The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments. version:1
arxiv-1701-04112 | Regularization, sparse recovery, and median-of-means tournaments | http://arxiv.org/abs/1701.04112 | id:1701.04112 author:Gábor Lugosi, Shahar Mendelson category:math.ST stat.ML stat.TH  published:2017-01-15 summary:A regularized risk minimization procedure for regression function estimation is introduced that achieves near optimal accuracy and confidence under general conditions, including heavy-tailed predictor and response variables. The procedure is based on median-of-means tournaments, introduced by the authors in [8]. It is shown that the new procedure outperforms standard regularized empirical risk minimization procedures such as lasso or slope in heavy-tailed problems. version:1
arxiv-1701-04101 | Light Source Estimation with Analytical Path-tracing | http://arxiv.org/abs/1701.04101 | id:1701.04101 author:Mike Kasper, Nima Keivan, Gabe Sibley, Christoffer Heckman category:cs.CV  published:2017-01-15 summary:We present a novel algorithm for light source estimation in scenes reconstructed with a RGB-D camera based on an analytically-derived formulation of path-tracing. Our algorithm traces the reconstructed scene with a custom path-tracer and computes the analytical derivatives of the light transport equation from principles in optics. These derivatives are then used to perform gradient descent, minimizing the photometric error between one or more captured reference images and renders of our current lighting estimation using an environment map parameterization for light sources. We show that our approach of modeling all light sources as points at infinity approximates lights located near the scene with surprising accuracy. Due to the analytical formulation of derivatives, optimization to the solution is considerably accelerated. We verify our algorithm using both real and synthetic data. version:1
arxiv-1701-04099 | Field-aware Factorization Machines in a Real-world Online Advertising System | http://arxiv.org/abs/1701.04099 | id:1701.04099 author:Yuchin Juan, Damien Lefortier, Olivier Chapelle category:cs.LG  published:2017-01-15 summary:Predicting user response is one of the core machine learning tasks in computational advertising. Field-aware Factorization Machines (FFM) have recently been established as a state-of-the-art method for that problem and in particular won two Kaggle challenges. This paper presents some results from implementing this method in a production system that predicts click-through and conversion rates for display advertising and shows that this method it is not only effective to win challenges but is also valuable in a real-world prediction system. We also discuss some specific challenges and solutions to reduce the training time, namely the use of an innovative seeding algorithm and a distributed learning mechanism. version:1
arxiv-1701-04082 | Embedding Watermarks into Deep Neural Networks | http://arxiv.org/abs/1701.04082 | id:1701.04082 author:Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, Shin'ichi Satoh category:cs.CV  published:2017-01-15 summary:Deep neural networks have recently achieved significant progress. Sharing trained models of these deep neural networks is very important in the rapid progress of researching or developing deep neural network systems. At the same time, it is necessary to protect the rights of shared trained models. To this end, we propose to use a digital watermarking technology to protect intellectual property or detect intellectual property infringement of trained models. Firstly, we formulate a new problem: embedding watermarks into deep neural networks. We also define requirements, embedding situations, and attack types for watermarking to deep neural networks. Secondly, we propose a general framework to embed a watermark into model parameters using a parameter regularizer. Our approach does not hurt the performance of networks into which a watermark is embedded. Finally, we perform comprehensive experiments to reveal the potential of watermarking to deep neural networks as a basis of this new problem. We show that our framework can embed a watermark in the situations of training a network from scratch, fine-tuning, and distilling without hurting the performance of a deep neural network. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark completely remains even after removing 65% of parameters were pruned. The implementation of this research is: https://github.com/yu4u/dnn-watermark version:1
arxiv-1701-04079 | Agent-Agnostic Human-in-the-Loop Reinforcement Learning | http://arxiv.org/abs/1701.04079 | id:1701.04079 author:David Abel, John Salvatier, Andreas Stuhlmüller, Owain Evans category:cs.LG cs.AI  published:2017-01-15 summary:Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains. version:1
arxiv-1701-04056 | Dialog Context Language Modeling with Recurrent Neural Networks | http://arxiv.org/abs/1701.04056 | id:1701.04056 author:Bing Liu, Ian Lane category:cs.CL  published:2017-01-15 summary:In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models. version:1
arxiv-1701-04043 | Iterative Block Tensor Singular Value Thresholding for Extraction of Low Rank Component of Image Data | http://arxiv.org/abs/1701.04043 | id:1701.04043 author:Longxi Chen, Yipeng Liu, Ce Zhu category:cs.CV  published:2017-01-15 summary:Tensor principal component analysis (TPCA) is a multi-linear extension of principal component analysis which converts a set of correlated measurements into several principal components. In this paper, we propose a new robust TPCA method to extract the princi- pal components of the multi-way data based on tensor singular value decomposition. The tensor is split into a number of blocks of the same size. The low rank component of each block tensor is extracted using iterative tensor singular value thresholding method. The prin- cipal components of the multi-way data are the concatenation of all the low rank components of all the block tensors. We give the block tensor incoherence conditions to guarantee the successful decom- position. This factorization has similar optimality properties to that of low rank matrix derived from singular value decomposition. Ex- perimentally, we demonstrate its effectiveness in two applications, including motion separation for surveillance videos and illumination normalization for face images. version:1
arxiv-1701-04039 | The Birth of Collective Memories: Analyzing Emerging Entities in Text Streams | http://arxiv.org/abs/1701.04039 | id:1701.04039 author:David Graus, Daan Odijk, Maarten de Rijke category:cs.IR cs.CL  published:2017-01-15 summary:We study how collective memories are formed online. We do so by tracking entities that emerge in public discourse, that is, in online text streams such as social media and news streams, before they are incorporated into Wikipedia, which, we argue, can be viewed as an online place for collective memory. By tracking how entities emerge in public discourse, i.e., the temporal patterns between their first mention in online text streams and subsequent incorporation into collective memory, we gain insights into how the collective remembrance process happens online. Specifically, we analyze nearly 80,000 entities as they emerge in online text streams before they are incorporated into Wikipedia. The online text streams we use for our analysis comprise of social media and news streams, and span over 579 million documents in a timespan of 18 months. We discover two main emergence patterns: entities that emerge in a "bursty" fashion, i.e., that appear in public discourse without a precedent, blast into activity and transition into collective memory. Other entities display a "delayed" pattern, where they appear in public discourse, experience a period of inactivity, and then resurface before transitioning into our cultural collective memory. version:1
arxiv-1701-04027 | Neural Models for Sequence Chunking | http://arxiv.org/abs/1701.04027 | id:1701.04027 author:Feifei Zhai, Saloni Potdar, Bing Xiang, Bowen Zhou category:cs.CL  published:2017-01-15 summary:Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-Outside-Beginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks. version:1
arxiv-1701-04018 | Boosting Dictionary Learning with Error Codes | http://arxiv.org/abs/1701.04018 | id:1701.04018 author:Yigit Oktar, Mehmet Turkan category:cs.CV  published:2017-01-15 summary:In conventional sparse representations based dictionary learning algorithms, initial dictionaries are generally assumed to be proper representatives of the system at hand. However, this may not be the case, especially in some systems restricted to random initializations. Therefore, a supposedly optimal state-update based on such an improper model might lead to undesired effects that will be conveyed to successive iterations. In this paper, we propose a dictionary learning method which includes a general feedback process that codes the intermediate error left over from a less intensive initial learning attempt, and then adjusts sparse codes accordingly. Experimental observations show that such an additional step vastly improves rates of convergence in high-dimensional cases, also results in better converged states in the case of random initializations. Improvements also scale up with more lenient sparsity constraints. version:1
arxiv-1701-04010 | Density-Wise Two Stage Mammogram Classification using Texture Exploiting Descriptors | http://arxiv.org/abs/1701.04010 | id:1701.04010 author:Deepti Tamrakar, Kapil Ahuja category:cs.CV  published:2017-01-15 summary:Breast cancer is becoming increasing pervasive, and its early detection is an important step in saving life of any patient. Mammography is an important tool in breast cancer diagnosis. The most important step here is classification of mammogram patch as normal-abnormal and benign-malignant. Texture of a breast in a mammogram patch plays a big role in these classifications. We propose a new feature extraction descriptor called Histogram of Oriented Texture (HOT), which is a combination of Histogram of Gradients (HOG) and Gabor filter, and exploits this fact. We also revisit Pass Band Discrete Cosine Transform (PB-DCT) descriptor that captures texture information well. All features of a mammogram patch may not be useful. Hence, we apply a feature selection technique called Discrimination Potentiality (DP). Our resulting descriptors, DP-HOT and DP-PB-DCT, are compared with standard descriptors. Density of mammogram is important for classification, and has not been studied exhaustively. IRMA database, containing MIAS and DDSM datasets, is the standard database that provides mammogram patches, and most researchers have tested their frameworks only on a subset of patches from this database. We apply our two new descriptors on it all images of IRMA database for density wise classification, and compare with standard descriptors. We achieve higher accuracy than all of the existing standard descriptors (100% for MIAS dataset, and more than 92% for DDSM dataset). version:1
arxiv-1701-03980 | DyNet: The Dynamic Neural Network Toolkit | http://arxiv.org/abs/1701.03980 | id:1701.03980 author:Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, Pengcheng Yin category:stat.ML cs.CL cs.MS  published:2017-01-15 summary:We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at http://github.com/clab/dynet. version:1
arxiv-1701-03947 | Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-impact Events | http://arxiv.org/abs/1701.03947 | id:1701.03947 author:Tuan Tran, Claudia Niederée, Nattiya Kanhabua, Ujwal Gadiraju, Avishek Anand category:cs.IR cs.CL H.3.3  published:2017-01-14 summary:Long-running, high-impact events such as the Boston Marathon bombing often develop through many stages and involve a large number of entities in their unfolding. Timeline summarization of an event by key sentences eases story digestion, but does not distinguish between what a user remembers and what she might want to re-check. In this work, we present a novel approach for timeline summarization of high-impact events, which uses entities instead of sentences for summarizing the event at each individual point in time. Such entity summaries can serve as both (1) important memory cues in a retrospective event consideration and (2) pointers for personalized event exploration. In order to automatically create such summaries, it is crucial to identify the "right" entities for inclusion. We propose to learn a ranking function for entities, with a dynamically adapted trade-off between the in-document salience of entities and the informativeness of entities across documents, i.e., the level of new information associated with an entity for a time point under consideration. Furthermore, for capturing collective attention for an entity we use an innovative soft labeling approach based on Wikipedia. Our experiments on a real large news datasets confirm the effectiveness of the proposed methods. version:1
arxiv-1701-03940 | Scalable and Incremental Learning of Gaussian Mixture Models | http://arxiv.org/abs/1701.03940 | id:1701.03940 author:Rafael Pinto, Paulo Engel category:cs.LG I.2.6  published:2017-01-14 summary:This work presents a fast and scalable algorithm for incremental learning of Gaussian mixture models. By performing rank-one updates on its precision matrices and determinants, its asymptotic time complexity is of \BigO{NKD^2} for $N$ data points, $K$ Gaussian components and $D$ dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm's applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated. version:1
arxiv-1701-03924 | QCRI Machine Translation Systems for IWSLT 16 | http://arxiv.org/abs/1701.03924 | id:1701.03924 author:Nadir Durrani, Fahim Dalvi, Hassan Sajjad, Stephan Vogel category:cs.CL  published:2017-01-14 summary:This paper describes QCRI's machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic->English and English->Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic->English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs. version:1
arxiv-1701-03918 | Marked Temporal Dynamics Modeling based on Recurrent Neural Network | http://arxiv.org/abs/1701.03918 | id:1701.03918 author:Yongqing Wang, Shenghua Liu, Huawei Shen, Xueqi Cheng category:cs.LG stat.ML  published:2017-01-14 summary:We are now witnessing the increasing availability of event stream data, i.e., a sequence of events with each event typically being denoted by the time it occurs and its mark information (e.g., event type). A fundamental problem is to model and predict such kind of marked temporal dynamics, i.e., when the next event will take place and what its mark will be. Existing methods either predict only the mark or the time of the next event, or predict both of them, yet separately. Indeed, in marked temporal dynamics, the time and the mark of the next event are highly dependent on each other, requiring a method that could simultaneously predict both of them. To tackle this problem, in this paper, we propose to model marked temporal dynamics by using a mark-specific intensity function to explicitly capture the dependency between the mark and the time of the next event. Extensive experiments on two datasets demonstrate that the proposed method outperforms state-of-the-art methods at predicting marked temporal dynamics. version:1
arxiv-1701-03916 | On Hölder projective divergences | http://arxiv.org/abs/1701.03916 | id:1701.03916 author:Frank Nielsen, Ke Sun, Stéphane Marchand-Maillet category:cs.LG cs.CV cs.IT math.IT  published:2017-01-14 summary:We describe a framework to build distances by measuring the tightness of inequalities, and introduce the notion of proper statistical divergences and improper pseudo-divergences. We then consider the H\"older ordinary and reverse inequalities, and present two novel classes of H\"older divergences and pseudo-divergences that both encapsulate the special case of the Cauchy-Schwarz divergence. We report closed-form formulas for those statistical dissimilarities when considering distributions belonging to the same exponential family provided that the natural parameter space is a cone (e.g., multivariate Gaussians), or affine (e.g., categorical distributions). Those new classes of H\"older distances are invariant to rescaling, and thus do not require distributions to be normalized. Finally, we show how to compute statistical H\"older centroids with respect to those divergences, and carry out center-based clustering toy experiments on a set of Gaussian distributions that demonstrate empirically that symmetrized H\"older divergences outperform the symmetric Cauchy-Schwarz divergence. version:1
arxiv-1701-03891 | Learning to Invert: Signal Recovery via Deep Convolutional Networks | http://arxiv.org/abs/1701.03891 | id:1701.03891 author:Ali Mousavi, Richard G. Baraniuk category:stat.ML cs.AI cs.IT cs.LG math.IT  published:2017-01-14 summary:The promise of compressive sensing (CS) has been offset by two significant challenges. First, real-world data is not exactly sparse in a fixed basis. Second, current high-performance recovery algorithms are slow to converge, which limits CS to either non-real-time applications or scenarios where massive back-end computing is available. In this paper, we attack both of these challenges head-on by developing a new signal recovery framework we call {\em DeepInverse} that learns the inverse transformation from measurement vectors to signals using a {\em deep convolutional network}. When trained on a set of representative images, the network learns both a representation for the signals (addressing challenge one) and an inverse map approximating a greedy or convex recovery algorithm (addressing challenge two). Our experiments indicate that the DeepInverse network closely approximates the solution produced by state-of-the-art CS recovery algorithms yet is hundreds of times faster in run time. The tradeoff for the ultrafast run time is a computationally intensive, off-line training procedure typical to deep networks. However, the training needs to be completed only once, which makes the approach attractive for a host of sparse recovery problems. version:1
arxiv-1701-03869 | Learning Linear Dynamical Systems with High-Order Tensor Data for Skeleton based Action Recognition | http://arxiv.org/abs/1701.03869 | id:1701.03869 author:Wenwen Ding, Kai Liu category:cs.CV  published:2017-01-14 summary:In recent years, there has been renewed interest in developing methods for skeleton-based human action recognition. A skeleton sequence can be naturally represented as a high-order tensor time series. In this paper, we model and analyze tensor time series with Linear Dynamical System (LDS) which is the most common for encoding spatio-temporal time-series data in various disciplines dut to its relative simplicity and efficiency. However, the traditional LDS treats the latent and observation state at each frame of video as a column vector. Such a vector representation fails to take into account the curse of dimensionality as well as valuable structural information with human action. Considering this fact, we propose generalized Linear Dynamical System (gLDS) for modeling tensor observation in the time series and employ Tucker decomposition to estimate the LDS parameters as action descriptors. Therefore, an action can be represented as a subspace corresponding to a point on a Grassmann manifold. Then we perform classification using dictionary learning and sparse coding over Grassmann manifold. Experiments on MSR Action3D Dataset, UCF Kinect Dataset and Northwestern-UCLA Multiview Action3D Dataset demonstrate that our proposed method achieves superior performance to the state-of-the-art algorithms. version:1
arxiv-1701-03866 | Long Timescale Credit Assignment in NeuralNetworks with External Memory | http://arxiv.org/abs/1701.03866 | id:1701.03866 author:Steven Stenberg Hansen category:cs.AI cs.LG cs.NE  published:2017-01-14 summary:Credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices. The length of this chain scales linearly with the number of time-steps as the same network is run at each time-step. This creates many problems, such as vanishing gradients, that have been well studied. In contrast, a NNEM's architecture recurrent activity doesn't involve a long chain of activity (though some architectures such as the NTM do utilize a traditional recurrent architecture as a controller). Rather, the externally stored embedding vectors are used at each time-step, but no messages are passed from previous time-steps. This means that vanishing gradients aren't a problem, as all of the necessary gradient paths are short. However, these paths are extremely numerous (one per embedding vector in memory) and reused for a very long time (until it leaves the memory). Thus, the forward-pass information of each memory must be stored for the entire duration of the memory. This is problematic as this additional storage far surpasses that of the actual memories, to the extent that large memories on infeasible to back-propagate through in high dimensional settings. One way to get around the need to hold onto forward-pass information is to recalculate the forward-pass whenever gradient information is available. However, if the observations are too large to store in the domain of interest, direct reinstatement of a forward pass cannot occur. Instead, we rely on a learned autoencoder to reinstate the observation, and then use the embedding network to recalculate the forward-pass. Since the recalculated embedding vector is unlikely to perfectly match the one stored in memory, we try out 2 approximations to utilize error gradient w.r.t. the vector in memory. version:1
arxiv-1701-03779 | Tumour Ellipsification in Ultrasound Images for Treatment Prediction in Breast Cancer | http://arxiv.org/abs/1701.03779 | id:1701.03779 author:Mehrdad J. Gangeh, Hamid R. Tizhoosh, Kan Wu, Dun Huang, Hadi Tadayyon, Gregory J. Czarnota category:cs.CV  published:2017-01-13 summary:Recent advances in using quantitative ultrasound (QUS) methods have provided a promising framework to non-invasively and inexpensively monitor or predict the effectiveness of therapeutic cancer responses. One of the earliest steps in using QUS methods is contouring a region of interest (ROI) inside the tumour in ultrasound B-mode images. While manual segmentation is a very time-consuming and tedious task for human experts, auto-contouring is also an extremely difficult task for computers due to the poor quality of ultrasound B-mode images. However, for the purpose of cancer response prediction, a rough boundary of the tumour as an ROI is only needed. In this research, a semi-automated tumour localization approach is proposed for ROI estimation in ultrasound B-mode images acquired from patients with locally advanced breast cancer (LABC). The proposed approach comprised several modules, including 1) feature extraction using keypoint descriptors, 2) augmenting the feature descriptors with the distance of the keypoints to the user-input pixel as the centre of the tumour, 3) supervised learning using a support vector machine (SVM) to classify keypoints as "tumour" or "non-tumour", and 4) computation of an ellipse as an outline of the ROI representing the tumour. Experiments with 33 B-mode images from 10 LABC patients yielded promising results with an accuracy of 76.7% based on the Dice coefficient performance measure. The results demonstrated that the proposed method can potentially be used as the first stage in a computer-assisted cancer response prediction system for semi-automated contouring of breast tumours. version:1
arxiv-1701-03757 | Deep Probabilistic Programming | http://arxiv.org/abs/1701.03757 | id:1701.03757 author:Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin Murphy, David M. Blei category:stat.ML cs.AI cs.LG cs.PL stat.CO  published:2017-01-13 summary:We propose Edward, a Turing-complete probabilistic programming language. Edward builds on two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation, to variational inference, to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, on a benchmark logistic regression task, Edward is at least 35x faster than Stan and PyMC3. version:1
arxiv-1701-03755 | What Can I Do Now? Guiding Users in a World of Automated Decisions | http://arxiv.org/abs/1701.03755 | id:1701.03755 author:Matthias Gallé category:stat.ML  published:2017-01-13 summary:More and more processes governing our lives use in some part an automatic decision step, where -- based on a feature vector derived from an applicant -- an algorithm has the decision power over the final outcome. Here we present a simple idea which gives some of the power back to the applicant by providing her with alternatives which would make the decision algorithm decide differently. It is based on a formalization reminiscent of methods used for evasion attacks, and consists in enumerating the subspaces where the classifiers decides the desired output. This has been implemented for the specific case of decision forests (ensemble methods based on decision trees), mapping the problem to an iterative version of enumerating $k$-cliques. version:1
arxiv-1701-03743 | Truncation-free Hybrid Inference for DPMM | http://arxiv.org/abs/1701.03743 | id:1701.03743 author:Arnim Bleier category:cs.LG stat.ML  published:2017-01-13 summary:Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian non-parametrics. While these models free from choosing the number of components a-priori, computationally attractive variational inference often reintroduces the need to do so, via a truncation on the variational distribution. In this paper we present a truncation-free hybrid inference for DPMM, combining the advantages of sampling-based MCMC and variational methods. The proposed hybridization enables more efficient variational updates, while increasing model complexity only if needed. We evaluate the properties of the hybrid updates and their empirical performance in single- as well as mixed-membership models. Our method is easy to implement and performs favorably compared to existing schemas. version:1
arxiv-1701-04313 | End-to-End ASR-free Keyword Search from Speech | http://arxiv.org/abs/1701.04313 | id:1701.04313 author:Kartik Audhkhasi, Andrew Rosenberg, Abhinav Sethy, Bhuvana Ramabhadran, Brian Kingsbury category:cs.CL cs.IR cs.LG cs.NE  published:2017-01-13 summary:End-to-end (E2E) systems have achieved competitive results compared to conventional hybrid hidden Markov model (HMM)-deep neural network based automatic speech recognition (ASR) systems. Such E2E systems are attractive due to the lack of dependence on alignments between input acoustic and output grapheme or HMM state sequence during training. This paper explores the design of an ASR-free end-to-end system for text query-based keyword search (KWS) from speech trained with minimal supervision. Our E2E KWS system consists of three sub-systems. The first sub-system is a recurrent neural network (RNN)-based acoustic auto-encoder trained to reconstruct the audio through a finite-dimensional representation. The second sub-system is a character-level RNN language model using embeddings learned from a convolutional neural network. Since the acoustic and text query embeddings occupy different representation spaces, they are input to a third feed-forward neural network that predicts whether the query occurs in the acoustic utterance or not. This E2E ASR-free KWS system performs respectably despite lacking a conventional ASR system and trains much faster. version:1
arxiv-1701-03682 | LIDE: Language Identification from Text Documents | http://arxiv.org/abs/1701.03682 | id:1701.03682 author:Priyank Mathur, Arkajyoti Misra, Emrah Budur category:cs.CL cs.NE  published:2017-01-13 summary:The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far. version:1
arxiv-1701-00295 | Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image | http://arxiv.org/abs/1701.00295 | id:1701.00295 author:Denis Tome, Chris Russell, Lourdes Agapito category:cs.CV  published:2017-01-01 summary:We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state- of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors. version:2
arxiv-1701-03647 | Restricted Boltzmann Machines with Gaussian Visible Units Guided by Pairwise Constraints | http://arxiv.org/abs/1701.03647 | id:1701.03647 author:Jielei Chu, Hongjun Wang, Hua Meng, Peng Jin, Tianrui Li category:cs.LG  published:2017-01-13 summary:Restricted Boltzmann machines (RBMs) and their variants are usually trained by contrastive divergence (CD) learning, but the training procedure is an unsupervised learning approach, without any guidances of the background knowledge. To enhance the expression ability of traditional RBMs, in this paper, we propose pairwise constraints restricted Boltzmann machine with Gaussian visible units (pcGRBM) model, in which the learning procedure is guided by pairwise constraints and the process of encoding is conducted under these guidances. The pairwise constraints are encoded in hidden layer features of pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In order to deal with real-valued data, the binary visible units are replaced by linear units with Gausian noise in the pcGRBM model. In the learning process of pcGRBM, the pairwise constraints are iterated transitions between visible and hidden units during CD learning procedure. Then, the proposed model is inferred by approximative gradient descent method and the corresponding learning algorithm is designed in this paper. In order to compare the availability of pcGRBM and traditional RBMs with Gaussian visible units, the features of the pcGRBM and RBMs hidden layer are used as input 'data' for K-means, spectral clustering (SP) and affinity propagation (AP) algorithms, respectively. A thorough experimental evaluation is performed with sixteen image datasets of Microsoft Research Asia Multimedia (MSRA-MM). The experimental results show that the clustering performance of K-means, SP and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering task shows better performance than some semi-supervised clustering algorithms. version:1
arxiv-1701-02470 | Review of Methods for Mapping Forest Disturbance and Degradation from Optical Earth Observation Data | http://arxiv.org/abs/1701.02470 | id:1701.02470 author:Manuela Hirschmugla, Heinz Gallaun, Matthias Dees, Pawan Datta, Janik Deutschera, Nikos Koutsias, Mathias Schardt category:cs.CV  published:2017-01-10 summary:This paper presents a review of the current state of the art in remote sensing based monitoring of forest disturbances and forest degradation from optical Earth Observation data. Part one comprises an overview and tabular description of currently available optical remote sensing sensors, which can be used for forest disturbance and degradation mapping. A section is devoted to currently existing mapping approaches, including both operational methods and recent developments. Part two reviews the two main categories of existing mapping approaches: first, classical image-to-image change detection and second, time series analysis. With the launch of the Sentinel-2a satellite and available Landsat imagery, time series analysis has become the most promising but also most demanding category of degradation mapping approaches. Hence, an emphasis is put on methods of time series analysis, among which four different classification methods are distinguished. The methods are explained and their benefits and drawbacks are discussed. A separate chapter presents a number of recent forest degradation mapping studies for two different ecosystems: The first ecosystem comprises the temperate forests with a geographical focus on Europe. The second ecosystem consists of the tropical forests with a geographical focus on Africa. Mapping examples from both ecosystems help to better illustrate the current state of the art. version:2
arxiv-1701-03641 | Symbolic Regression Algorithms with Built-in Linear Regression | http://arxiv.org/abs/1701.03641 | id:1701.03641 author:Jan Žegklitz, Petr Pošík category:cs.LG  published:2017-01-13 summary:Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms --- multiple regression, random forests and support vector regression. version:1
arxiv-1701-03633 | A dissimilarity-based approach to predictive maintenance with application to HVAC systems | http://arxiv.org/abs/1701.03633 | id:1701.03633 author:Riccardo Satta, Stefano Cavallari, Eraldo Pomponi, Daniele Grasselli, Davide Picheo, Carlo Annis category:cs.LG  published:2017-01-13 summary:The goal of predictive maintenance is to forecast the occurrence of faults of an appliance, in order to proactively take the necessary actions to ensure its availability. In many application scenarios, predictive maintenance is applied to a set of homogeneous appliances. In this paper, we firstly review taxonomies and main methodologies currently used for condition-based maintenance; secondly, we argue that the mutual dissimilarities of the behaviours of all appliances of this set (the "cohort") can be exploited to detect upcoming faults. Specifically, inspired by dissimilarity-based representations, we propose a novel machine learning approach based on the analysis of concurrent mutual differences of the measurements coming from the cohort. We evaluate our method over one year of historical data from a cohort of 17 HVAC (Heating, Ventilation and Air Conditioning) systems installed in an Italian hospital. We show that certain kinds of faults can be foreseen with an accuracy, measured in terms of area under the ROC curve, as high as 0.96. version:1
arxiv-1701-03619 | Diffusion-based nonlinear filtering for multimodal data fusion with application to sleep stage assessment | http://arxiv.org/abs/1701.03619 | id:1701.03619 author:Ori Katz, Ronen Talmon, Yu-Lun Lo, Hau-Tieng Wu category:stat.ML cs.LG physics.data-an 62-07  published:2017-01-13 summary:The problem of information fusion from multiple data-sets acquired by multimodal sensors has drawn significant research attention over the years. In this paper, we focus on a particular problem setting consisting of a physical phenomenon or a system of interest observed by multiple sensors. We assume that all sensors measure some aspects of the system of interest with additional sensor-specific and irrelevant components. Our goal is to recover the variables relevant to the observed system and to filter out the nuisance effects of the sensor-specific variables. We propose an approach based on manifold learning, which is particularly suitable for problems with multiple modalities, since it aims to capture the intrinsic structure of the data and relies on minimal prior model knowledge. Specifically, we propose a nonlinear filtering scheme, which extracts the hidden sources of variability captured by two or more sensors, that are independent of the sensor-specific components. In addition to presenting a theoretical analysis, we demonstrate our technique on real measured data for the purpose of sleep stage assessment based on multiple, multimodal sensor measurements. We show that without prior knowledge on the different modalities and on the measured system, our method gives rise to a data-driven representation that is well correlated with the underlying sleep process and is robust to noise and sensor-specific effects. version:1
arxiv-1701-03578 | Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network | http://arxiv.org/abs/1701.03578 | id:1701.03578 author:Seunghyun Yoon, Hyeongu Yun, Yuna Kim, Gyu-tae Park, Kyomin Jung category:cs.CL  published:2017-01-13 summary:In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects. version:1
arxiv-1701-03577 | Kernel Approximation Methods for Speech Recognition | http://arxiv.org/abs/1701.03577 | id:1701.03577 author:Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu, Aurélien Bellet, Linxi Fan, Michael Collins, Daniel Hsu, Brian Kingsbury, Michael Picheny, Fei Sha category:stat.ML cs.CL cs.LG  published:2017-01-13 summary:We study large-scale kernel methods for acoustic modeling in speech recognition and compare their performance to deep neural networks (DNNs). We perform experiments on four speech recognition datasets, including the TIMIT and Broadcast News benchmark tasks, and compare these two types of models on frame-level performance metrics (accuracy, cross-entropy), as well as on recognition metrics (word/character error rate). In order to scale kernel methods to these large datasets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, in order to reduce the number of random features required by kernel models, we propose a simple but effective method for feature selection. The method is able to explore a large number of non-linear features while maintaining a compact model more efficiently than existing approaches. Second, we present a number of frame-level metrics which correlate very strongly with recognition performance when computed on the heldout set; we take advantage of these correlations by monitoring these metrics during training in order to decide when to stop learning. This technique can noticeably improve the recognition performance of both DNN and kernel models, while narrowing the gap between them. Additionally, we show that the linear bottleneck method of Sainath et al. (2013) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Together, these three methods dramatically improve the performance of kernel acoustic models, making their performance comparable to DNNs on the tasks we explored. version:1
arxiv-1701-03572 | Real-Time Optical flow-based Video Stabilization for Unmanned Aerial Vehicles | http://arxiv.org/abs/1701.03572 | id:1701.03572 author:Anli Lim, Bharath Ramesh, Yue Yang, Cheng Xiang, Zhi Gao, Feng Lin category:cs.CV  published:2017-01-13 summary:This paper describes the development of a novel algorithm to tackle the problem of real-time video stabilization for unmanned aerial vehicles (UAVs). There are two main components in the algorithm: (1) By designing a suitable model for the global motion of UAV, the proposed algorithm avoids the necessity of estimating the most general motion model, projective transformation, and considers simpler motion models, such as rigid transformation and similarity transformation. (2) To achieve a high processing speed, optical-flow based tracking is employed in lieu of conventional tracking and matching methods used by state-of-the-art algorithms. These two new ideas resulted in a real-time stabilization algorithm, developed over two phases. Stage I considers processing the whole sequence of frames in the video while achieving an average processing speed of 50fps on several publicly available benchmark videos. Next, Stage II undertakes the task of real-time video stabilization using a multi-threading implementation of the algorithm designed in Stage I. version:1
arxiv-1701-03364 | A Digital Fuzzy Edge Detector for Color Images | http://arxiv.org/abs/1701.03364 | id:1701.03364 author:Yuan-Hang Zhang, Xie Li, Jing-Yun Xiao category:cs.CV  published:2017-01-12 summary:Edge detection is a classic problem in the field of image processing, which lays foundations for other tasks such as image segmentation. Conventionally, this operation is performed using gradient operators such as the Roberts or Sobel operator, which can discover local changes in intensity levels. These operators, however, perform poorly on low contrast images. In this paper, we propose an edge detector architecture for color images based on fuzzy theory and the Sobel operator. First, the R, G and B channels are extracted from an image and enhanced using fuzzy methods, in order to suppress noise and improve the contrast between the background and the objects. The Sobel operator is then applied to each of the channels, which are finally combined into an edge map of the origin image. Experimental results obtained through an FPGA-based implementation have proved the proposed method effective. version:2
arxiv-1612-09508 | Feedback Networks | http://arxiv.org/abs/1612.09508 | id:1612.09508 author:Amir R. Zamir, Te-Lin Wu, Lin Sun, William Shen, Jitendra Malik, Silvio Savarese category:cs.CV  published:2016-12-30 summary:Currently, the most successful learning models in computer vision are based on learning successive representations followed by a decision layer. This is usually actualized through feedforward multilayer neural networks, e.g. ConvNets, where each layer forms one of such successive representations. However, an alternative that can achieve the same goal is a feedback based approach in which the representation is formed in an iterative manner based on a feedback received from previous iteration's output. We establish that a feedback based approach has several fundamental advantages over feedforward: it enables making early predictions at the query time, its output naturally conforms to a hierarchical structure in the label space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning. We observe that feedback networks develop a considerably different representation compared to feedforward counterparts, in line with the aforementioned advantages. We put forth a general feedback based learning architecture with the endpoint results on par or better than existing feedforward networks with the addition of the above advantages. We also investigate several mechanisms in feedback architectures (e.g. skip connections in time) and design choices (e.g. feedback length). We hope this study offers new perspectives in quest for more natural and practical learning models. version:2
arxiv-1701-03555 | Active Self-Paced Learning for Cost-Effective and Progressive Face Identification | http://arxiv.org/abs/1701.03555 | id:1701.03555 author:Liang Lin, Keze Wang, Deyu Meng, Wangmeng Zuo, Lei Zhang category:cs.CV  published:2017-01-13 summary:This paper aims to develop a novel cost-effective framework for face identification, which progressively maintains a batch of classifiers with the increasing face images of different individuals. By naturally combining two recently rising techniques: active learning (AL) and self-paced learning (SPL), our framework is capable of automatically annotating new instances and incorporating them into training under weak expert re-certification. We first initialize the classifier using a few annotated samples for each individual, and extract image features using the convolutional neural nets. Then, a number of candidates are selected from the unannotated samples for classifier updating, in which we apply the current classifiers ranking the samples by the prediction confidence. In particular, our approach utilizes the high-confidence and low-confidence samples in the self-paced and the active user-query way, respectively. The neural nets are later fine-tuned based on the updated classifiers. Such heuristic implementation is formulated as solving a concise active SPL optimization problem, which also advances the SPL development by supplementing a rational dynamic curriculum constraint. The new model finely accords with the "instructor-student-collaborative" learning mode in human education. The advantages of this proposed framework are two-folds: i) The required number of annotated samples is significantly decreased while the comparable performance is guaranteed. A dramatic reduction of user effort is also achieved over other state-of-the-art active learning techniques. ii) The mixture of SPL and AL effectively improves not only the classifier accuracy compared to existing AL/SPL methods but also the robustness against noisy data. We evaluate our framework on two challenging datasets, and demonstrate very promising results. (http://hcp.sysu.edu.cn/projects/aspl/) version:1
arxiv-1701-03551 | Cost-Effective Active Learning for Deep Image Classification | http://arxiv.org/abs/1701.03551 | id:1701.03551 author:Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, Liang Lin category:cs.CV  published:2017-01-13 summary:Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human efforts. In this paper, we propose a novel active learning framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing active learning methods in two aspects. First, we incorporate deep convolutional neural networks into active learning. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high confidence samples from the unlabeled set for feature learning. Specifically, these high confidence samples are automatically selected and iteratively assigned pseudo-labels. We thus call our framework "Cost-Effective Active Learning" (CEAL) standing for the two advantages.Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification datasets, i.e., face recognition on CACD database [1] and object categorization on Caltech-256 [2]. version:1
arxiv-1701-03550 | Bayesian System Identification based on Hierarchical Sparse Bayesian Learning and Gibbs Sampling with Application to Structural Damage Assessment | http://arxiv.org/abs/1701.03550 | id:1701.03550 author:Yong Huang, James L. Beck, Hui Li category:stat.AP stat.ME stat.ML  published:2017-01-13 summary:The focus in this paper is Bayesian system identification based on noisy incomplete modal data where we can impose spatially-sparse stiffness changes when updating a structural model. To this end, based on a similar hierarchical sparse Bayesian learning model from our previous work, we propose two Gibbs sampling algorithms. The algorithms differ in their strategies to deal with the posterior uncertainty of the equation-error precision parameter, but both sample from the conditional posterior probability density functions (PDFs) for the structural stiffness parameters and system modal parameters. The effective dimension for the Gibbs sampling is low because iterative sampling is done from only three conditional posterior PDFs that correspond to three parameter groups, along with sampling of the equation-error precision parameter from another conditional posterior PDF in one of the algorithms where it is not integrated out as a "nuisance" parameter. A nice feature from a computational perspective is that it is not necessary to solve a nonlinear eigenvalue problem of a structural model. The effectiveness and robustness of the proposed algorithms are illustrated by applying them to the IASE-ASCE Phase II simulated and experimental benchmark studies. The goal is to use incomplete modal data identified before and after possible damage to detect and assess spatially-sparse stiffness reductions induced by any damage. Our past and current focus on meeting challenges arising from Bayesian inference of structural stiffness serve to strengthen the capability of vibration-based structural system identification but our methods also have much broader applicability for inverse problems in science and technology where system matrices are to be inferred from noisy partial information about their eigenquantities. version:1
arxiv-1701-02856 | Bayesian Non-Homogeneous Markov Models via Polya-Gamma Data Augmentation with Applications to Rainfall Modeling | http://arxiv.org/abs/1701.02856 | id:1701.02856 author:Tracy Holsclaw, Arthur M. Greene, Andrew W. Robertson, Padhraic Smyth category:stat.AP stat.ML  published:2017-01-11 summary:Discrete-time hidden Markov models are a broadly useful class of latent-variable models with applications in areas such as speech recognition, bioinformatics, and climate data analysis. It is common in practice to introduce temporal non-homogeneity into such models by making the transition probabilities dependent on time-varying exogenous input variables via a multinomial logistic parametrization. We extend such models to introduce additional non-homogeneity into the emission distribution using a generalized linear model (GLM), with data augmentation for sampling-based inference. However, the presence of the logistic function in the state transition model significantly complicates parameter inference for the overall model, particularly in a Bayesian context. To address this we extend the recently-proposed Polya-Gamma data augmentation approach to handle non-homogeneous hidden Markov models (NHMMs), allowing the development of an efficient Markov chain Monte Carlo (MCMC) sampling scheme. We apply our model and inference scheme to 30 years of daily rainfall in India, leading to a number of insights into rainfall-related phenomena in the region. Our proposed approach allows for fully Bayesian analysis of relatively complex NHMMs on a scale that was not possible with previous methods. Software implementing the methods described in the paper is available via the R package NHMM. version:2
arxiv-1701-03537 | Perishability of Data: Dynamic Pricing under Varying-Coefficient Models | http://arxiv.org/abs/1701.03537 | id:1701.03537 author:Adel Javanmard category:cs.GT cs.LG stat.ML  published:2017-01-13 summary:We consider a firm that sells a large number of products to its customers in an online fashion. Each product is described by a high dimensional feature vector, and the market value of a product is assumed to be linear in the values of its features. Parameters of the valuation model are unknown and can change over time. The firm sequentially observes a product's features and can use the historical sales data (binary sale/no sale feedbacks) to set the price of current product, with the objective of maximizing the collected revenue. We measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on projected stochastic gradient descent (PSGD) and characterize its regret in terms of time $T$, features dimension $d$, and the temporal variability in the model parameters, $\delta_t$. We consider two settings. In the first one, feature vectors are chosen antagonistically by nature and we prove that the regret of PSGD pricing policy is of order $O(\sqrt{T} + \sum_{t=1}^T \sqrt{t}\delta_t)$. In the second setting (referred to as stochastic features model), the feature vectors are drawn independently from an unknown distribution. We show that in this case, the regret of PSGD pricing policy is of order $O(d^2 \log T + \sum_{t=1}^T t\delta_t)$. version:1
arxiv-1701-03534 | An OpenCL(TM) Deep Learning Accelerator on Arria 10 | http://arxiv.org/abs/1701.03534 | id:1701.03534 author:Utku Aydonat, Shane O'Connell, Davor Capalija, Andrew C. Ling, Gordon R. Chiu category:cs.DC cs.AR cs.CV  published:2017-01-13 summary:Convolutional neural nets (CNNs) have become a practical means to perform vision tasks, particularly in the area of image classification. FPGAs are well known to be able to perform convolutions efficiently, however, most recent efforts to run CNNs on FPGAs have shown limited advantages over other devices such as GPUs. Previous approaches on FPGAs have often been memory bound due to the limited external memory bandwidth on the FPGA device. We show a novel architecture written in OpenCL(TM), which we refer to as a Deep Learning Accelerator (DLA), that maximizes data reuse and minimizes external memory bandwidth. Furthermore, we show how we can use the Winograd transform to significantly boost the performance of the FPGA. As a result, when running our DLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or 23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs and is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the state-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the best publicly known implementation of AlexNet on nVidia's TitanX GPU. version:1
arxiv-1701-03504 | Maximum Entropy Flow Networks | http://arxiv.org/abs/1701.03504 | id:1701.03504 author:Gabriel Loaiza-Ganem, Yuanjun Gao, John P. Cunningham category:stat.ME stat.ML  published:2017-01-12 summary:Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, rather than the traditional method of optimizing over the continuous density directly, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks. version:1
arxiv-1701-03492 | Scalable, Trie-based Approximate Entity Extraction for Real-Time Financial Transaction Screening | http://arxiv.org/abs/1701.03492 | id:1701.03492 author:Emrah Budur category:cs.CL cs.IR  published:2017-01-12 summary:Financial institutions have to screen their transactions to ensure that they are not affiliated with terrorism entities. Developing appropriate solutions to detect such affiliations precisely while avoiding any kind of interruption to large amount of legitimate transactions is essential. In this paper, we present building blocks of a scalable solution that may help financial institutions to build their own software to extract terrorism entities out of both structured and unstructured financial messages in real time and with approximate similarity matching approach. version:1
arxiv-1701-03452 | Simplified Minimal Gated Unit Variations for Recurrent Neural Networks | http://arxiv.org/abs/1701.03452 | id:1701.03452 author:Joel Heck, Fathi M. Salem category:cs.NE stat.ML  published:2017-01-12 summary:Recurrent neural networks with various types of hidden units have been used to solve a diverse range of problems involving sequence data. Two of the most recent proposals, gated recurrent units (GRU) and minimal gated units (MGU), have shown comparable promising results on example public datasets. In this paper, we introduce three model variants of the minimal gated unit (MGU) which further simplify that design by reducing the number of parameters in the forget-gate dynamic equation. These three model variants, referred to simply as MGU1, MGU2, and MGU3, were tested on sequences generated from the MNIST dataset and from the Reuters Newswire Topics (RNT) dataset. The new models have shown similar accuracy to the MGU model while using fewer parameters and thus lowering training expense. One model variant, namely MGU2, performed better than MGU on the datasets considered, and thus may be used as an alternate to MGU or GRU in recurrent neural networks. version:1
arxiv-1701-03449 | Manifold Alignment Determination: finding correspondences across different data views | http://arxiv.org/abs/1701.03449 | id:1701.03449 author:Andreas Damianou, Neil D. Lawrence, Carl Henrik Ek category:stat.ML cs.LG math.PR 60G15  published:2017-01-12 summary:We present Manifold Alignment Determination (MAD), an algorithm for learning alignments between data points from multiple views or modalities. The approach is capable of learning correspondences between views as well as correspondences between individual data-points. The proposed method requires only a few aligned examples from which it is capable to recover a global alignment through a probabilistic model. The strong, yet flexible regularization provided by the generative model is sufficient to align the views. We provide experiments on both synthetic and real data to highlight the benefit of the proposed approach. version:1
arxiv-1701-03441 | Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural Networks | http://arxiv.org/abs/1701.03441 | id:1701.03441 author:Yuzhen Lu, Fathi M. Salem category:cs.NE stat.ML  published:2017-01-12 summary:The standard LSTM recurrent neural networks while very powerful in long-range dependency sequence applications have highly complex structure and relatively large (adaptive) parameters. In this work, we present empirical comparison between the standard LSTM recurrent neural network architecture and three new parameter-reduced variants obtained by eliminating combinations of the input signal, bias, and hidden unit signals from individual gating signals. The experiments on two sequence datasets show that the three new variants, called simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the standard LSTM model with less (adaptive) parameters. version:1
arxiv-1701-03439 | Comprehension-guided referring expressions | http://arxiv.org/abs/1701.03439 | id:1701.03439 author:Ruotian Luo, Gregory Shakhnarovich category:cs.CV  published:2017-01-12 summary:We consider generation and comprehension of natural language referring expression for objects in an image. Unlike generic "image captioning" which lacks natural standard evaluation criteria, quality of a referring expression may be measured by the receiver's ability to correctly infer which object is being described. Following this intuition, we propose two approaches to utilize models trained for comprehension task to generate better expressions. First, we use a comprehension module trained on human-generated expressions, as a "critic" of referring expression generator. The comprehension module serves as a differentiable proxy of human evaluation, providing training signal to the generation module. Second, we use the comprehension module in a generate-and-rerank pipeline, which chooses from candidate expressions generated by a model according to their performance on the comprehension task. We show that both approaches lead to improved referring expression generation on multiple benchmark datasets. version:1
arxiv-1701-03434 | SMARTies: Sentiment Models for Arabic Target Entities | http://arxiv.org/abs/1701.03434 | id:1701.03434 author:Noura Farra, Kathleen McKeown category:cs.CL  published:2017-01-12 summary:We consider entity-level sentiment analysis in Arabic, a morphologically rich language with increasing resources. We present a system that is applied to complex posts written in response to Arabic newspaper articles. Our goal is to identify important entity "targets" within the post along with the polarity expressed about each target. We achieve significant improvements over multiple baselines, demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment, and that the use of distributional semantic clusters further boosts performances for these representations, especially when richer linguistic resources are not available. version:1
arxiv-1701-03420 | Joint Dictionary Learning for Example-based Image Super-resolution | http://arxiv.org/abs/1701.03420 | id:1701.03420 author:Mojtaba Sahraee-Ardakan, Mohsen Joneidi category:cs.CV  published:2017-01-12 summary:In this paper, we propose a new joint dictionary learning method for example-based image super-resolution (SR), using sparse representation. The low-resolution (LR) dictionary is trained from a set of LR sample image patches. Using the sparse representation coefficients of these LR patches over the LR dictionary, the high-resolution (HR) dictionary is trained by minimizing the reconstruction error of HR sample patches. The error criterion used here is the mean square error. In this way we guarantee that the HR patches have the same sparse representation over HR dictionary as the LR patches over the LR dictionary, and at the same time, these sparse representations can well reconstruct the HR patches. Simulation results show the effectiveness of our method compared to the state-of-art SR algorithms. version:1
arxiv-1701-03338 | LanideNN: Multilingual Language Identification on Character Window | http://arxiv.org/abs/1701.03338 | id:1701.03338 author:Tom Kocmi, Ondřej Bojar category:cs.CL  published:2017-01-12 summary:In language identification, a common first step in natural language processing, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the accuracy also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data. version:1
arxiv-1701-03330 | Two-view 3D Reconstruction for Food Volume Estimation | http://arxiv.org/abs/1701.03330 | id:1701.03330 author:Joachim Dehais, Marios Anthimopoulos, Sergey Shevchik, Stavroula Mougiakakou category:cs.CV  published:2017-01-12 summary:The increasing prevalence of diet-related chronic diseases coupled with the ineffectiveness of traditional diet management methods have resulted in a need for novel tools to accurately and automatically assess meals. Recently, computer vision based systems that use meal images to assess their content have been proposed. Food portion estimation is the most difficult task for individuals assessing their meals and it is also the least studied area. The present paper proposes a three-stage system to calculate portion sizes using two images of a dish acquired by mobile devices. The first stage consists in understanding the configuration of the different views, after which a dense 3D model is built from the two images; finally, this 3D model serves to extract the volume of the different items. The system was extensively tested on 77 real dishes of known volume, and achieved an average error of less than 10% in 5.5 seconds per dish. The proposed pipeline is computationally tractable and requires no user input, making it a viable option for fully automated dietary assessment. version:1
arxiv-1701-03281 | Modularized Morphing of Neural Networks | http://arxiv.org/abs/1701.03281 | id:1701.03281 author:Tao Wei, Changhu Wang, Chang Wen Chen category:cs.LG cs.NE  published:2017-01-12 summary:In this work we study the problem of network morphism, an effective learning scheme to morph a well-trained neural network to a new one with the network function completely preserved. Different from existing work where basic morphing types on the layer level were addressed, we target at the central problem of network morphism at a higher level, i.e., how a convolutional layer can be morphed into an arbitrary module of a neural network. To simplify the representation of a network, we abstract a module as a graph with blobs as vertices and convolutional layers as edges, based on which the morphing process is able to be formulated as a graph transformation problem. Two atomic morphing operations are introduced to compose the graphs, based on which modules are classified into two families, i.e., simple morphable modules and complex modules. We present practical morphing solutions for both of these two families, and prove that any reasonable module can be morphed from a single convolutional layer. Extensive experiments have been conducted based on the state-of-the-art ResNet on benchmark datasets, and the effectiveness of the proposed solution has been verified. version:1
arxiv-1701-03268 | Relaxation of the EM Algorithm via Quantum Annealing for Gaussian Mixture Models | http://arxiv.org/abs/1701.03268 | id:1701.03268 author:Hideyuki Miyahara, Koji Tsumura, Yuki Sughiyama category:stat.ML cond-mat.stat-mech quant-ph  published:2017-01-12 summary:We propose a modified expectation-maximization algorithm by introducing the concept of quantum annealing, which we call the deterministic quantum annealing expectation-maximization (DQAEM) algorithm. The expectation-maximization (EM) algorithm is an established algorithm to compute maximum likelihood estimates and applied to many practical applications. However, it is known that EM heavily depends on initial values and its estimates are sometimes trapped by local optima. To solve such a problem, quantum annealing (QA) was proposed as a novel optimization approach motivated by quantum mechanics. By employing QA, we then formulate DQAEM and present a theorem that supports its stability. Finally, we demonstrate numerical simulations to confirm its efficiency. version:1
arxiv-1701-03266 | Probabilistic Diffeomorphic Registration: Representing Uncertainty | http://arxiv.org/abs/1701.03266 | id:1701.03266 author:Demian Wassermann, Matt Toews, Marc Niethammer, William Wells Iii category:cs.CV  published:2017-01-12 summary:This paper presents a novel mathematical framework for representing uncertainty in large deformation diffeomorphic image registration. The Bayesian posterior distribution over the deformations aligning a moving and a fixed image is approximated via a variational formulation. A stochastic differential equation (SDE) modeling the deformations as the evolution of a time-varying velocity field leads to a prior density over deformations in the form of a Gaussian process. This permits estimating the full posterior distribution in order to represent uncertainty, in contrast to methods in which the posterior is approximated via Monte Carlo sampling or maximized in maximum a-posteriori (MAP) estimation. The frame-work is demonstrated in the case of landmark-based image registration, including simulated data and annotated pre and intra-operative 3D images. version:1
arxiv-1701-02291 | QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures | http://arxiv.org/abs/1701.02291 | id:1701.02291 author:Tapabrata Ghosh category:cs.LG stat.ML  published:2017-01-09 summary:We present QuickNet, a fast and accurate network architecture that is both faster and significantly more accurate than other fast deep architectures like SqueezeNet. Furthermore, it uses less parameters than previous networks, making it more memory efficient. We do this by making two major modifications to the reference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable convolutions and 2) The use of parametric rectified linear units. We make the observation that parametric rectified linear units are computationally equivalent to leaky rectified linear units at test time and the observation that separable convolutions can be interpreted as a compressed Inception network (Chollet, 2016). Using these observations, we derive a network architecture, which we call QuickNet, that is both faster and more accurate than previous models. Our architecture provides at least four major advantages: (1) A smaller model size, which is more tenable on memory constrained systems; (2) A significantly faster network which is more tenable on computationally constrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10 Dataset which outperforms all but one result published so far, although we note that our works are orthogonal approaches and can be combined (4) Orthogonality to previous model compression approaches allowing for further speed gains to be realized. version:2
arxiv-1701-03246 | Ordered Pooling of Optical Flow Sequences for Action Recognition | http://arxiv.org/abs/1701.03246 | id:1701.03246 author:Jue Wang, Anoop Cherian, Fatih Porikli category:cs.CV  published:2017-01-12 summary:Training of Convolutional Neural Networks (CNNs) on long video sequences is computationally expensive due to the substantial memory requirements and the massive number of parameters that deep architectures demand. Early fusion of video frames is thus a standard technique, in which several consecutive frames are first agglomerated into a compact representation, and then fed into the CNN as an input sample. For this purpose, a summarization approach that represents a set of consecutive RGB frames by a single dynamic image to capture pixel dynamics is proposed recently. In this paper, we introduce a novel ordered representation of consecutive optical flow frames as an alternative and argue that this representation captures the action dynamics more effectively than RGB frames. We provide intuitions on why such a representation is better for action recognition. We validate our claims on standard benchmark datasets and demonstrate that using summaries of flow images lead to significant improvements over RGB frames while achieving accuracy comparable to the state-of-the-art on UCF101 and HMDB datasets. version:1
arxiv-1701-03244 | Light Source Point Cluster Selection Based Atmosphere Light Estimation | http://arxiv.org/abs/1701.03244 | id:1701.03244 author:Wenbo Zhang, Xiaorong Hou category:cs.CV  published:2017-01-12 summary:Atmosphere light value is a highly critical parameter in defogging algorithms that are based on an atmosphere scattering model. Any error in atmosphere light value will produce a direct impact on the accuracy of scattering computation and thus bring chromatic distortion to restored images. To address this problem, this paper propose a method that relies on clustering statistics to estimate atmosphere light value. It starts by selecting in the original image some potential atmosphere light source points, which are grouped into point clusters by means of clustering technique. From these clusters, a number of clusters containing candidate atmosphere light source points are selected, the points are then analyzed statistically, and the cluster containing the most candidate points is used for estimating atmosphere light value. The mean brightness vector of the candidate atmosphere light points in the chosen point cluster is taken as the estimate of atmosphere light value, while their geometric center in the image is accepted as the location of atmosphere light. Experimental results suggest that this statistics clustering method produces more accurate atmosphere brightness vectors and light source locations. This accuracy translates to, from a subjective perspective, more natural defogging effect on the one hand and to the improvement in various objective image quality indicators on the other hand. version:1
arxiv-1701-03458 | An Asynchronous Parallel Approach to Sparse Recovery | http://arxiv.org/abs/1701.03458 | id:1701.03458 author:Deanna Needell, Tina Woolf category:cs.LG cs.DC  published:2017-01-12 summary:Asynchronous parallel computing and sparse recovery are two areas that have received recent interest. Asynchronous algorithms are often studied to solve optimization problems where the cost function takes the form $\sum_{i=1}^M f_i(x)$, with a common assumption that each $f_i$ is sparse; that is, each $f_i$ acts only on a small number of components of $x\in\mathbb{R}^n$. Sparse recovery problems, such as compressed sensing, can be formulated as optimization problems, however, the cost functions $f_i$ are dense with respect to the components of $x$, and instead the signal $x$ is assumed to be sparse, meaning that it has only $s$ non-zeros where $s\ll n$. Here we address how one may use an asynchronous parallel architecture when the cost functions $f_i$ are not sparse in $x$, but rather the signal $x$ is sparse. We propose an asynchronous parallel approach to sparse recovery via a stochastic greedy algorithm, where multiple processors asynchronously update a vector in shared memory containing information on the estimated signal support. We include numerical simulations that illustrate the potential benefits of our proposed asynchronous method. version:1
arxiv-1701-03231 | Single-Pass, Adaptive Natural Language Filtering: Measuring Value in User Generated Comments on Large-Scale, Social Media News Forums | http://arxiv.org/abs/1701.03231 | id:1701.03231 author:Manuel Amunategui category:cs.CL  published:2017-01-12 summary:There are large amounts of insight and social discovery potential in mining crowd-sourced comments left on popular news forums like Reddit.com, Tumblr.com, Facebook.com and Hacker News. Unfortunately, due the overwhelming amount of participation with its varying quality of commentary, extracting value out of such data isn't always obvious nor timely. By designing efficient, single-pass and adaptive natural language filters to quickly prune spam, noise, copy-cats, marketing diversions, and out-of-context posts, we can remove over a third of entries and return the comments with a higher probability of relatedness to the original article in question. The approach presented here uses an adaptive, two-step filtering process. It first leverages the original article posted in the thread as a starting corpus to parse comments by matching intersecting words and term-ratio balance per sentence then grows the corpus by adding new words harvested from high-matching comments to increase filtering accuracy over time. version:1
arxiv-1701-03227 | Promoting Domain-Specific Terms in Topic Models with Informative Priors | http://arxiv.org/abs/1701.03227 | id:1701.03227 author:Angela Fan, Finale Doshi-Velez, Luke Miratrix category:cs.CL cs.IR cs.LG  published:2017-01-12 summary:Latent Dirichlet Allocation (LDA) models trained without stopword removal often produce topics with high posterior probabilities on uninformative words, obscuring the underlying corpus content. Even when canonical stopwords are manually removed, uninformative words common in that corpus will still dominate the most probable words in a topic. We propose a simple strategy for automatically promoting terms with domain relevance and demoting these domain-specific stop words. Our approach is easily applied within any existing LDA framework and increases the amount of domain-relevant content and reduces the appearance of canonical and human-evaluated stopwords in three very different domains: Department of Labor accident reports, online health forum posts, and NIPS abstracts. Along the way, we show that standard topic quality measures such as coherence and pointwise mutual information act counter-intuitively in presence of common but irrelevant words. We also explain why these standard metrics fall short, propose an additional topic quality metric that targets the stopword problem, and show that it correlates with our human subject experiments. version:1
arxiv-1701-03212 | Sparse-TDA: Sparse Realization of Topological Data Analysis for Multi-Way Classification | http://arxiv.org/abs/1701.03212 | id:1701.03212 author:Wei Guo, Krithika Manohar, Steven L. Brunton, Ashis G. Banerjee category:stat.ML cs.LG  published:2017-01-12 summary:Topological data analysis (TDA) has emerged as one of the most promising techniques to reconstruct the unknown shapes of high-dimensional spaces from observed data samples. TDA, thus, yields key shape descriptors in the form of persistent topological features that can be used for any supervised or unsupervised learning task, including multi-way classification. Sparse sampling, on the other hand, provides a highly efficient technique to reconstruct signals in the spatial-temporal domain from just a few carefully-chosen samples. Here, we present a new method, referred to as the Sparse-TDA algorithm, that combines favorable aspects of the two techniques. This combination is realized by selecting an optimal set of sparse pixel samples from the persistent features generated by a vector-based TDA algorithm. These sparse samples are selected using a low-rank matrix representation of persistent features. We show that the Sparse-TDA method demonstrates promising performance on three benchmark problems related to human posture recognition and image texture classification. version:1
arxiv-1701-02490 | Real-Time Bidding by Reinforcement Learning in Display Advertising | http://arxiv.org/abs/1701.02490 | id:1701.02490 author:Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, Defeng Guo category:cs.LG cs.AI cs.GT  published:2017-01-10 summary:The majority of online display ads are served through real-time bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. version:2
arxiv-1701-03198 | Unsupervised Latent Behavior Manifold Learning from Acoustic Features: audio2behavior | http://arxiv.org/abs/1701.03198 | id:1701.03198 author:Haoqi Li, Brian Baucom, Panayiotis Georgiou category:cs.LG cs.SD  published:2017-01-12 summary:Behavioral annotation using signal processing and machine learning is highly dependent on training data and manual annotations of behavioral labels. Previous studies have shown that speech information encodes significant behavioral information and be used in a variety of automated behavior recognition tasks. However, extracting behavior information from speech is still a difficult task due to the sparseness of training data coupled with the complex, high-dimensionality of speech, and the complex and multiple information streams it encodes. In this work we exploit the slow varying properties of human behavior. We hypothesize that nearby segments of speech share the same behavioral context and hence share a similar underlying representation in a latent space. Specifically, we propose a Deep Neural Network (DNN) model to connect behavioral context and derive the behavioral manifold in an unsupervised manner. We evaluate the proposed manifold in the couples therapy domain and also provide examples from publicly available data (e.g. stand-up comedy). We further investigate training within the couples' therapy domain and from movie data. The results are extremely encouraging and promise improved behavioral quantification in an unsupervised manner and warrants further investigation in a range of applications. version:1
arxiv-1701-03185 | Generating Long and Diverse Responses with Neural Conversation Models | http://arxiv.org/abs/1701.03185 | id:1701.03185 author:Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray Kurzweil category:cs.CL  published:2017-01-11 summary:Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths. version:1
arxiv-1701-03163 | Parsing Universal Dependencies without training | http://arxiv.org/abs/1701.03163 | id:1701.03163 author:Héctor Martínez Alonso, Željko Agić, Barbara Plank, Anders Søgaard category:cs.CL  published:2017-01-11 summary:We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. version:1
arxiv-1701-03153 | Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in Re-identification | http://arxiv.org/abs/1701.03153 | id:1701.03153 author:Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander Rognhaugen, Theoharis Theoharis category:cs.CV I.2.10; I.4.8  published:2017-01-11 summary:Re-identification is generally carried out by encoding the appearance of a subject in terms of outfit, suggesting scenarios where people do not change their attire. In this paper we overcome this restriction, by proposing a framework based on a deep convolutional neural network, SOMAnet, that additionally models other discriminative aspects, namely, structural attributes of the human figure (e.g. height, obesity, gender). Our method is unique in many respects. First, SOMAnet is based on the Inception architecture, departing from the usual siamese framework. This spares expensive data preparation (pairing images across cameras) and allows the understanding of what the network learned. Second, and most notably, the training data consists of a synthetic 100K instance dataset, SOMAset, created by photorealistic human body generation software. Synthetic data represents a good compromise between realistic imagery, usually not required in re-identification since surveillance cameras capture low-resolution silhouettes, and complete control of the samples, which is useful in order to customize the data w.r.t. the surveillance scenario at-hand, e.g. ethnicity. SOMAnet, trained on SOMAset and fine-tuned on recent re-identification benchmarks, outperforms all competitors, matching subjects even with different apparel. The combination of synthetic data with Inception architectures opens up new research avenues in re-identification. version:1
arxiv-1701-03151 | Guaranteed Parameter Estimation for Discrete Energy Minimization | http://arxiv.org/abs/1701.03151 | id:1701.03151 author:Mengtian Li, Daniel Huber category:cs.CV  published:2017-01-11 summary:Structural learning, a method to estimate the parameters for discrete energy minimization, has been proven to be effective in solving computer vision problems, especially in 3D scene parsing. As the complexity of the models increases, structural learning algorithms turn to approximate inference to retain tractability. Unfortunately, such methods often fail because the approximation can be arbitrarily poor. In this work, we propose a method to overcome this limitation through exploiting the properties of the joint problem of training time inference and learning. With the help of the learning framework, we transform the inapproximable inference problem into a polynomial time solvable one, thereby enabling tractable exact inference while still allowing an arbitrary graph structure and full potential interactions. Our learning algorithm is guaranteed to return a solution with a bounded error to the global optimal within the feasible parameter space. We demonstrate the effectiveness of this method on two point cloud scene parsing datasets. Our approach runs much faster and solves a problem that is intractable for previous, well-known approaches. version:1
arxiv-1701-03129 | De-identification In practice | http://arxiv.org/abs/1701.03129 | id:1701.03129 author:Besat Kassaie category:cs.CL  published:2017-01-11 summary:We report our effort to identify the sensitive information, subset of data items listed by HIPAA (Health Insurance Portability and Accountability), from medical text using the recent advances in natural language processing and machine learning techniques. We represent the words with high dimensional continuous vectors learned by a variant of Word2Vec called Continous Bag Of Words (CBOW). We feed the word vectors into a simple neural network with a Long Short-Term Memory (LSTM) architecture. Without any attempts to extract manually crafted features and considering that our medical dataset is too small to be fed into neural network, we obtained promising results. The results thrilled us to think about the larger scale of the project with precise parameter tuning and other possible improvements. version:1
arxiv-1701-03126 | Attention-Based Multimodal Fusion for Video Description | http://arxiv.org/abs/1701.03126 | id:1701.03126 author:Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R. Hershey, Tim K. Marks category:cs.CV cs.CL cs.MM  published:2017-01-11 summary:Currently successful methods for video description are based on encoder-decoder sentence generation using recur-rent neural networks (RNNs). Recent work has shown the advantage of integrating temporal and/or spatial attention mechanisms into these models, in which the decoder net-work predicts each word in the description by selectively giving more weight to encoded features from specific time frames (temporal attention) or to features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model to selectively attend not just to specific times or spatial regions, but to specific modalities of input such as image features, motion features, and audio features. Our new modality-dependent attention mechanism, which we call multimodal attention, provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state of the art. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention significantly outperforms the model that uses temporal attention alone. version:1
arxiv-1701-03092 | Job Detection in Twitter | http://arxiv.org/abs/1701.03092 | id:1701.03092 author:Besat Kassaie category:cs.CL  published:2017-01-11 summary:In this report, we propose a new application for twitter data called \textit{job detection}. We identify people's job category based on their tweets. As a preliminary work, we limited our task to identify only IT workers from other job holders. We have used and compared both simple bag of words model and a document representation based on Skip-gram model. Our results show that the model based on Skip-gram, achieves a 76\% precision and 82\% recall. version:1
arxiv-1701-03079 | RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems | http://arxiv.org/abs/1701.03079 | id:1701.03079 author:Chongyang Tao, Lili Mou, Dongyan Zhao, Rui Yan category:cs.CL cs.HC cs.IR  published:2017-01-11 summary:Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is time- and labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has high correlation with human annotation. version:1
arxiv-1701-03056 | CNN-based Segmentation of Medical Imaging Data | http://arxiv.org/abs/1701.03056 | id:1701.03056 author:Baris Kayalibay, Grady Jensen, Patrick van der Smagt category:cs.CV  published:2017-01-11 summary:Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand. version:1
arxiv-1701-03051 | Efficient Twitter Sentiment Classification using Subjective Distant Supervision | http://arxiv.org/abs/1701.03051 | id:1701.03051 author:Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, Manish Singh category:cs.SI cs.CL cs.IR  published:2017-01-11 summary:As microblogging services like Twitter are becoming more and more influential in today's globalised world, its facets like sentiment analysis are being extensively studied. We are no longer constrained by our own opinion. Others opinions and sentiments play a huge role in shaping our perspective. In this paper, we build on previous works on Twitter sentiment analysis using Distant Supervision. The existing approach requires huge computation resource for analysing large number of tweets. In this paper, we propose techniques to speed up the computation process for sentiment analysis. We use tweet subjectivity to select the right training samples. We also introduce the concept of EFWS (Effective Word Score) of a tweet that is derived from polarity scores of frequently used words, which is an additional heuristic that can be used to speed up the sentiment classification with standard machine learning algorithms. We performed our experiments using 1.6 million tweets. Experimental evaluations show that our proposed technique is more efficient and has higher accuracy compared to previously proposed methods. We achieve overall accuracies of around 80% (EFWS heuristic gives an accuracy around 85%) on a training dataset of 100K tweets, which is half the size of the dataset used for the baseline model. The accuracy of our proposed model is 2-3% higher than the baseline model, and the model effectively trains at twice the speed of the baseline model. version:1
arxiv-1701-03102 | Linear Disentangled Representation Learning for Facial Actions | http://arxiv.org/abs/1701.03102 | id:1701.03102 author:Xiang Xiang, Trac D. Tran category:cs.CV cs.AI cs.LG stat.ML  published:2017-01-11 summary:Limited annotated data available for the recognition of facial expression and action units embarrasses the training of deep networks, which can learn disentangled invariant features. However, a linear model with just several parameters normally is not demanding in terms of training data. In this paper, we propose an elegant linear model to untangle confounding factors in challenging realistic multichannel signals such as 2D face videos. The simple yet powerful model does not rely on huge training data and is natural for recognizing facial actions without explicitly disentangling the identity. Base on well-understood intuitive linear models such as Sparse Representation based Classification (SRC), previous attempts require a prepossessing of explicit decoupling which is practically inexact. Instead, we exploit the low-rank property across frames to subtract the underlying neutral faces which are modeled jointly with sparse representation on the action components with group sparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot automatic method on raw face videos performs as competitive as SRC applied on manually prepared action components and performs even better than SRC in terms of true positive rate. We apply the model to the even more challenging task of facial action unit recognition, verified on the MPI Face Video Database (MPI-VDB) achieving a decent performance. All the programs and data have been made publicly available. version:1
arxiv-1701-03041 | Modeling Grasp Motor Imagery through Deep Conditional Generative Models | http://arxiv.org/abs/1701.03041 | id:1701.03041 author:Matthew Veres, Medhat Moussa, Graham W. Taylor category:cs.RO stat.ML  published:2017-01-11 summary:Grasping is a complex process involving knowledge of the object, the surroundings, and of oneself. While humans are able to integrate and process all of the sensory information required for performing this task, equipping machines with this capability is an extremely challenging endeavor. In this paper, we investigate how deep learning techniques can allow us to translate high-level concepts such as motor imagery to the problem of robotic grasp synthesis. We explore a paradigm based on generative models for learning integrated object-action representations, and demonstrate its capacity for capturing and generating multimodal, multi-finger grasp configurations on a simulated grasping dataset. version:1
arxiv-1701-03006 | Compressive Sensing via Convolutional Factor Analysis | http://arxiv.org/abs/1701.03006 | id:1701.03006 author:Xin Yuan, Yunchen Pu, Lawrence Carin category:stat.ML cs.LG  published:2017-01-11 summary:We solve the compressive sensing problem via convolutional factor analysis, where the convolutional dictionaries are learned {\em in situ} from the compressed measurements. An alternating direction method of multipliers (ADMM) paradigm for compressive sensing inversion based on convolutional factor analysis is developed. The proposed algorithm provides reconstructed images as well as features, which can be directly used for recognition ($e.g.$, classification) tasks. When a deep (multilayer) model is constructed, a stochastic unpooling process is employed to build a generative model. During reconstruction and testing, we project the upper layer dictionary to the data level and only a single layer deconvolution is required. We demonstrate that using $\sim30\%$ (relative to pixel numbers) compressed measurements, the proposed model achieves the classification accuracy comparable to the original data on MNIST. We also observe that when the compressed measurements are very limited ($e.g.$, $<10\%$), the upper layer dictionary can provide better reconstruction results than the bottom layer. version:1
arxiv-1701-02967 | A Large Dimensional Analysis of Least Squares Support Vector Machines | http://arxiv.org/abs/1701.02967 | id:1701.02967 author:Zhenyu Liao, Romain Couillet category:stat.ML  published:2017-01-11 summary:In this article, a large dimensional performance analysis of kernel least squares support vector machines (LS-SVMs) is provided under the assumption of a two-class Gaussian mixture model for the input data. Building upon recent random matrix advances, when both the dimension of data $p$ and their number $n$ grow large at the same rate, we show that the LS-SVM decision function converges to a normal-distributed variable, the mean and variance of which depend explicitly on a local behavior of the kernel function. This theoretical result is then applied to the MNIST data sets which, despite their non-Gaussianity, exhibit a surprisingly similar behavior. Our analysis provides a deeper understanding of the mechanism into play in SVM-type methods and in particular of the impact on the choice of the kernel function as well as some of their theoretical limits. version:1
arxiv-1701-02965 | Revisiting Deep Image Smoothing and Intrinsic Image Decomposition | http://arxiv.org/abs/1701.02965 | id:1701.02965 author:Qingnan Fan, David Wipf, Gang Hua, Baoquan Chen category:cs.CV  published:2017-01-11 summary:We propose an image smoothing approximation and intrinsic image decomposition method based on a modified convolutional neural network architecture applied directly to the original color image. Our network has a very large receptive field equipped with at least 20 convolutional layers and 8 residual units. When training such a deep model however, it is quite difficult to generate edge-preserving images without undesirable color differences. To overcome this obstacle, we apply both image gradient supervision and a channel-wise rescaling layer that computes a minimum mean-squared error color correction. Additionally, to enhance piece-wise constant effects for image smoothing, we append a domain transform filter with a predicted refined edge map. The resulting deep model, which can be trained end-to-end, directly learns edge-preserving smooth images and intrinsic decompositions without any special design or input scaling/size requirements. Moreover, our method shows much better numerical and visual results on both tasks and runs in comparable test time to existing deep methods. version:1
arxiv-1701-02962 | Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network | http://arxiv.org/abs/1701.02962 | id:1701.02962 author:Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu category:cs.CL  published:2017-01-11 summary:Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods. version:1
arxiv-1701-02960 | Slow mixing for Latent Dirichlet allocation | http://arxiv.org/abs/1701.02960 | id:1701.02960 author:Johan Jonasson category:cs.LG stat.ML G.3  published:2017-01-11 summary:Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability theory in general and in machine learning in particular. A Markov chain is devised so that its stationary distribution is some probability distribution of interest. Then one samples from the given distribution by running the Markov chain for a "long time" until it appears to be stationary and then collects the sample. However these chains are often very complex and there are no theoretical guarantees that stationarity is actually reached. In this paper we study the Gibbs sampler of the posterior distribution of a very simple case of Latent Dirichlet Allocation, an attractive Bayesian unsupervised learning model for text generation and text classification. It turns out that in some situations, the mixing time of the Gibbs sampler is exponential in the length of documents and so it is practically impossible to properly sample from the posterior when documents are sufficiently long. version:1
arxiv-1701-02946 | Cross-lingual RST Discourse Parsing | http://arxiv.org/abs/1701.02946 | id:1701.02946 author:Chloé Braud, Maximin Coavoux, Anders Søgaard category:cs.CL  published:2017-01-11 summary:Discourse parsing is an integral part of understanding information flow and argumentative structure in documents. Most previous research has focused on inducing and evaluating models from the English RST Discourse Treebank. However, discourse treebanks for other languages exist, including Spanish, German, Basque, Dutch and Brazilian Portuguese. The treebanks share the same underlying linguistic theory, but differ slightly in the way documents are annotated. In this paper, we present (a) a new discourse parser which is simpler, yet competitive (significantly better on 2/3 metrics) to state of the art for English, (b) a harmonization of discourse treebanks across languages, enabling us to present (c) what to the best of our knowledge are the first experiments on cross-lingual discourse parsing. version:1
arxiv-1701-02925 | Question Analysis for Arabic Question Answering Systems | http://arxiv.org/abs/1701.02925 | id:1701.02925 author:Waheeb Ahmed, Dr. Anto P Babu category:cs.CL  published:2017-01-11 summary:The first step of processing a question in Question Answering(QA) Systems is to carry out a detailed analysis of the question for the purpose of determining what it is asking for and how to perfectly approach answering it. Our Question analysis uses several techniques to analyze any question given in natural language: a Stanford POS Tagger & parser for Arabic language, a named entity recognizer, tokenizer,Stop-word removal, Question expansion, Question classification and Question focus extraction components. We employ numerous detection rules and trained classifier using features from this analysis to detect important elements of the question, including: 1) the portion of the question that is a referring to the answer (the focus); 2) different terms in the question that identify what type of entity is being asked for (the lexical answer types); 3) Question expansion ; 4) a process of classifying the question into one or more of several and different types; and We describe how these elements are identified and evaluate the effect of accurate detection on our question-answering system using the Mean Reciprocal Rank(MRR) accuracy measure. version:1
arxiv-1701-02901 | A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions | http://arxiv.org/abs/1701.02901 | id:1701.02901 author:Antonio Toral, Víctor M. Sánchez-Cartagena category:cs.CL  published:2017-01-11 summary:We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences. version:1
arxiv-1701-02892 | Multivariate Regression with Grossly Corrupted Observations: A Robust Approach and its Applications | http://arxiv.org/abs/1701.02892 | id:1701.02892 author:Xiaowei Zhang, Chi Xu, Yu Zhang, Tingshao Zhu, Li Cheng category:stat.ML cs.CV cs.LG  published:2017-01-11 summary:This paper studies the problem of multivariate linear regression where a portion of the observations is grossly corrupted or is missing, and the magnitudes and locations of such occurrences are unknown in priori. To deal with this problem, we propose a new approach by explicitly consider the error source as well as its sparseness nature. An interesting property of our approach lies in its ability of allowing individual regression output elements or tasks to possess their unique noise levels. Moreover, despite working with a non-smooth optimization problem, our approach still guarantees to converge to its optimal solution. Experiments on synthetic data demonstrate the competitiveness of our approach compared with existing multivariate regression models. In addition, empirically our approach has been validated with very promising results on two exemplar real-world applications: The first concerns the prediction of \textit{Big-Five} personality based on user behaviors at social network sites (SNSs), while the second is 3D human hand pose estimation from depth images. The implementation of our approach and comparison methods as well as the involved datasets are made publicly available in support of the open-source and reproducible research initiatives. version:1
arxiv-1701-02886 | The empirical Christoffel function in Statistics and Machine Learning | http://arxiv.org/abs/1701.02886 | id:1701.02886 author:Jean-Bernard Lasserre, Edouard Pauwels category:cs.LG  published:2017-01-11 summary:We illustrate the potential in statistics and machine learning of the Christoffel function, or more precisely, its empirical counterpart associated with a counting measure uniformly supported on a finite set of points. Firstly, we provide a thresholding scheme which allows to approximate the support of a measure from a finite subset of its moments with strong asymptotic guaranties. Secondly, we provide a consistency result which relates the empirical Christoffel function and its population counterpart in the limit of large samples. Finally, we illustrate the relevance of our results on simulated and real world datasets for several applications in statistics and machine learning: (a) density and support estimation from finite samples, (b) outlier and novelty detection and (c) affine matching. version:1
arxiv-1701-02877 | Generalisation in Named Entity Recognition: A Quantitative Analysis | http://arxiv.org/abs/1701.02877 | id:1701.02877 author:Isabelle Augenstein, Leon Derczynski, Kalina Bontcheva category:cs.CL  published:2017-01-11 summary:Named Entity Recognition (NER) is a key NLP task, which is all the more challenging on Web and user-generated content with their diverse and continuously changing language. This paper aims to quantify how this diversity impacts state-of-the-art NER methods, by measuring named entity (NE) and context variability, feature sparsity, and their effects on precision and recall. In particular, our findings indicate that NER approaches struggle to generalise in diverse genres with limited training data. Unseen NEs, in particular, play an important role, which have a higher incidence in diverse genres such as social media than in more regular genres such as newswire. Coupled with a higher incidence of unseen features more generally and the lack of large training corpora, this leads to significantly lower F1 scores for diverse genres as compared to more regular ones. We also find that leading systems rely heavily on surface forms found in training data, having problems generalising beyond these, and offer explanations for this observation. version:1
arxiv-1701-02870 | Context-aware Captions from Context-agnostic Supervision | http://arxiv.org/abs/1701.02870 | id:1701.02870 author:Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik category:cs.CV cs.AI  published:2017-01-11 summary:We introduce a technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of "siamese cat" and "tiger cat", our system generates language that describes the "siamese cat" in a way that distinguishes it from "tiger cat". We start with a generic language model that is context-agnostic and add a listener to discriminate between closely-related concepts. Our approach offers two key advantages over previous work: 1) our listener does not need separate training, and 2) allows joint inference to decode sentences that satisfy both the speaker and listener -- yielding an introspective speaker. We first apply our introspective speaker to a justification task, i.e. to describe why an image contains a particular fine-grained category as opposed to another closely related category in the CUB-200-2011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one out of two semantically similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination. version:1
arxiv-1701-02829 | A Unified RGB-T Saliency Detection Benchmark: Dataset, Baselines, Analysis and A Novel Approach | http://arxiv.org/abs/1701.02829 | id:1701.02829 author:Chenglong Li, Guizhao Wang, Yunpeng Ma, Aihua Zheng, Bin Luo, Jin Tang category:cs.CV  published:2017-01-11 summary:Despite significant progress, image saliency detection still remains a challenging task in complex scenes and environments. Integrating multiple different but complementary cues, like RGB and Thermal (RGB-T), may be an effective way for boosting saliency detection performance. The current research in this direction, however, is limited by the lack of a comprehensive benchmark. This work contributes such a RGB-T image dataset, which includes 821 spatially aligned RGB-T image pairs and their ground truth annotations for saliency detection purpose. The image pairs are with high diversity recorded under different scenes and environmental conditions, and we annotate 11 challenges on these image pairs for performing the challenge-sensitive analysis for different saliency detection algorithms. We also implement 3 kinds of baseline methods with different modality inputs to provide a comprehensive comparison platform. With this benchmark, we propose a novel approach, multi-task manifold ranking with cross-modality consistency, for RGB-T saliency detection. In particular, we introduce a weight for each modality to describe the reliability, and integrate them into the graph-based manifold ranking algorithm to achieve adaptive fusion of different source data. Moreover, we incorporate the cross-modality consistent constraints to integrate different modalities collaboratively. For the optimization, we design an efficient algorithm to iteratively solve several subproblems with closed-form solutions. Extensive experiments against other baseline methods on the newly created benchmark demonstrate the effectiveness of the proposed approach, and we also provide basic insights and potential future research directions for RGB-T saliency detection. version:1
arxiv-1701-02815 | Stochastic Generative Hashing | http://arxiv.org/abs/1701.02815 | id:1701.02815 author:Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song category:cs.LG cs.CV stat.ML  published:2017-01-11 summary:Learning to hash plays a fundamentally important role in the efficient image and video retrieval and many other computer vision problems. However, due to the binary outputs of the hash functions, the learning of hash functions is very challenging. In this paper, we propose a novel approach to learn stochastic hash functions such that the learned hashing codes can be used to regenerate the inputs. We develop an efficient stochastic gradient learning algorithm which can avoid the notorious difficulty caused by binary output constraint, and directly optimize the parameters of the hash functions and the associated generative model jointly. The proposed method can be applied to both $L2$ approximate nearest neighbor search (L2NNS) and maximum inner product search (MIPS). Extensive experiments on a variety of large-scale datasets show that the proposed method achieves significantly better retrieval results than previous state-of-the-arts. version:1
arxiv-1701-02810 | OpenNMT: Open-Source Toolkit for Neural Machine Translation | http://arxiv.org/abs/1701.02810 | id:1701.02810 author:Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M. Rush category:cs.CL cs.AI cs.NE  published:2017-01-10 summary:We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques. version:1
arxiv-1701-02795 | Bidirectional American Sign Language to English Translation | http://arxiv.org/abs/1701.02795 | id:1701.02795 author:Hardie Cate, Zeshan Hussain category:cs.CL  published:2017-01-10 summary:We outline a bidirectional translation system that converts sentences from American Sign Language (ASL) to English, and vice versa. To perform machine translation between ASL and English, we utilize a generative approach. Specifically, we employ an adjustment to the IBM word-alignment model 1 (IBM WAM1), where we define language models for English and ASL, as well as a translation model, and attempt to generate a translation that maximizes the posterior distribution defined by these models. Then, using these models, we are able to quantify the concepts of fluency and faithfulness of a translation between languages. version:1
arxiv-1701-02789 | Causal Best Intervention Identification via Importance Sampling | http://arxiv.org/abs/1701.02789 | id:1701.02789 author:Rajat Sen, Karthikeyan Shanmugam, Alexandros G. Dimakis, Sanjay Shakkottai category:stat.ML cs.IT cs.LG math.IT  published:2017-01-10 summary:Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node $V$ in a causal DAG, to maximize the expected value of a target node $Y$ (downstream of $V$). There is a fixed total budget for sampling under various interventions. Also, there are cost constraints on different types of interventions. We pose this as a best arm identification problem with $K$ arms, where each arm is a soft intervention at $V$. The key difference from the classical setting is that there is information leakage among the arms. Each soft intervention is a distinct known conditional probability distribution between $V$ and its parents $pa(V)$. We propose an efficient algorithm that uses importance sampling to adaptively sample using different interventions and leverage information leakage to choose the best. We provide the first gap dependent simple regret and best arm mis-identification error bounds for this problem. This generalizes the prior bounds available for the simpler case of no information leakage. In the case of no leakage, the number of samples required for identification is (upto polylog factors) $\tilde{O} (\max_i \frac{i}{\Delta_i^2})$ where $\Delta_i$ is the gap between the optimal and the $i$-th best arm. We generalize the previous result for the causal setting and show that $\tilde{O}(\max_i \frac{\sigma_i^2}{\Delta_i^2})$ is sufficient where $\sigma_i^2$ is the effective variance of an importance sampling estimator that eliminates the $i$-th best arm out of a set of arms with gaps roughly at most twice $\Delta_i$. We also show that $\sigma_i^2 << i$ in many cases. Our result also recovers (up to constants) prior gap independent bounds for this setting. We demonstrate that our algorithm empirically outperforms the state of the art, through synthetic experiments. version:1
arxiv-1701-02776 | Universal Joint Image Clustering and Registration using Partition Information | http://arxiv.org/abs/1701.02776 | id:1701.02776 author:Ravi Kiran Raman, Lav R. Varshney category:cs.IT math.IT stat.ML  published:2017-01-10 summary:The problem of joint clustering and registration of images is studied in a universal setting. We define universal joint clustering and registration algorithms using multivariate information functionals. We first study the problem of registering two images using maximum mutual information and prove its asymptotic optimality. We then show the shortcomings of pairwise registration in multi-image registration, and design an asymptotically optimal algorithm based on multiinformation. Finally, we define a novel multivariate information functional to perform joint clustering and registration of images, and prove consistency of the algorithm. version:1
arxiv-1701-03360 | Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition | http://arxiv.org/abs/1701.03360 | id:1701.03360 author:Jaeyoung Kim, Mostafa El-Khamy, Jungwon Lee category:cs.LG cs.AI cs.SD  published:2017-01-10 summary:In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The proposed residual LSTM architecture provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus indicates that the performance of plain and highway LSTM networks degrades with increasing network depth. For example, 10-layer plain and highway LSTM networks showed 13.7% and 6.2% increase in WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and highway LSTM networks, respectively. Training with both the IHM and SDM corpora, the residual LSTM architecture provided larger gain from increasing depth: a 10-layer residual LSTM showed 3.0% WER reduction over the corresponding 5-layer one. version:1
arxiv-1701-02720 | Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks | http://arxiv.org/abs/1701.02720 | id:1701.02720 author:Ying Zhang, Mohammad Pezeshki, Philemon Brakel, Saizheng Zhang, Cesar Laurent Yoshua Bengio, Aaron Courville category:cs.CL cs.LG stat.ML  published:2017-01-10 summary:Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information. version:1
arxiv-1701-02718 | See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content | http://arxiv.org/abs/1701.02718 | id:1701.02718 author:Roozbeh Mottaghi, Connor Schenck, Dieter Fox, Ali Farhadi category:cs.CV  published:2017-01-10 summary:Humans have rich understanding of liquid containers and their contents; for example, we can effortlessly pour water from a pitcher to a cup. Doing so requires estimating the volume of the cup, approximating the amount of water in the pitcher, and predicting the behavior of water when we tilt the pitcher. Very little attention in computer vision has been made to liquids and their containers. In this paper, we study liquid containers and their contents, and propose methods to estimate the volume of containers, approximate the amount of liquid in them, and perform comparative volume estimations all from a single RGB image. Furthermore, we show that our proposed model can predict the behavior of liquids inside containers when one tilts the containers. We also introduce a new dataset of Containers Of liQuid contEnt (COQE) that contains 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models. version:1
arxiv-1701-02704 | Clicktionary: A Web-based Game for Exploring the Atoms of Object Recognition | http://arxiv.org/abs/1701.02704 | id:1701.02704 author:Drew Linsley, Sven Eberhardt, Tarun Sharma, Pankaj Gupta, Thomas Serre category:cs.CV  published:2017-01-10 summary:Understanding what visual features and representations contribute to human object recognition may provide scaffolding for more effective artificial vision systems. While recent advances in Deep Convolutional Networks (DCNs) have led to systems approaching human accuracy, it is unclear if they leverage the same visual features as humans for object recognition. We introduce Clicktionary, a competitive web-based game for discovering features that humans use for object recognition: One participant from a pair sequentially reveals parts of an object in an image until the other correctly identifies its category. Scoring image regions according to their proximity to correct recognition yields maps of visual feature importance for individual images. We find that these "realization" maps exhibit only weak correlation with relevance maps derived from DCNs or image salience algorithms. Cueing DCNs to attend to features emphasized by these maps improves their object recognition accuracy. Our results thus suggest that realization maps identify visual features that humans deem important for object recognition but are not adequately captured by DCNs. To rectify this shortcoming, we propose a novel web-based application for acquiring realization maps at scale, with the aim of improving the state-of-the-art in object recognition. version:1
arxiv-1701-02676 | Unsupervised Image-to-Image Translation with Generative Adversarial Networks | http://arxiv.org/abs/1701.02676 | id:1701.02676 author:Hao Dong, Paarth Neekhara, Chao Wu, Yike Guo category:cs.CV cs.LG  published:2017-01-10 summary:It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the "image-to-image translation" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation version:1
arxiv-1701-02664 | ChaLearn Looking at People: Events and Resources | http://arxiv.org/abs/1701.02664 | id:1701.02664 author:Sergio Escalera, Xavier Baró, Hugo Jair Escalante, Isabelle Guyon category:cs.CV  published:2017-01-10 summary:This paper reviews the historic of ChaLearn Looking at People (LAP) events. We started in 2011 (with the release of the first Kinect device) to run challenges related to human action/activity and gesture recognition. Since then we have regularly organized events in a series of competitions covering all aspects of visual analysis of humans. So far we have organized more than 10 international challenges and events in this field. This paper reviews associated events, and introduces the ChaLearn LAP platform where public resources (including code, data and preprints of papers) related to the organized events are available. We also provide a discussion on perspectives of ChaLearn LAP activities. version:1
arxiv-1701-01302 | Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making | http://arxiv.org/abs/1701.01302 | id:1701.01302 author:Andrew Critch category:cs.AI cs.GT cs.LG  published:2017-01-05 summary:Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\"{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs. version:2
arxiv-1701-02632 | Midgar: Detection of people through computer vision in the Internet of Things scenarios to improve the security in Smart Cities, Smart Towns, and Smart Homes | http://arxiv.org/abs/1701.02632 | id:1701.02632 author:Cristian González García, Daniel Meana-Llorián, B. Cristina Pelayo G-Bustelo, Juan Manuel Cueva Lovelle, Néstor Garcia-Fernandez category:cs.CV  published:2017-01-10 summary:Could we use Computer Vision in the Internet of Things for using pictures as sensors? This is the principal hypothesis that we want to resolve. Currently, in order to create safety areas, cities, or homes, people use IP cameras. Nevertheless, this system needs people who watch the camera images, watch the recording after something occurred, or watch when the camera notifies them of any movement. These are the disadvantages. Furthermore, there are many Smart Cities and Smart Homes around the world. This is why we thought of using the idea of the Internet of Things to add a way of automating the use of IP cameras. In our case, we propose the analysis of pictures through Computer Vision to detect people in the analysed pictures. With this analysis, we are able to obtain if these pictures contain people and handle the pictures as if they were sensors with two possible states. Notwithstanding, Computer Vision is a very complicated field. This is why we needed a second hypothesis: Could we work with Computer Vision in the Internet of Things with a good accuracy to automate or semi-automate this kind of events? The demonstration of these hypotheses required a testing over our Computer Vision module to check the possibilities that we have to use this module in a possible real environment with a good accuracy. Our proposal, as a possible solution, is the analysis of entire sequence instead of isolated pictures for using pictures as sensors in the Internet of Things. version:1
arxiv-1701-02620 | Deep Learning for Logo Recognition | http://arxiv.org/abs/1701.02620 | id:1701.02620 author:Simone Bianco, Marco Buzzelli, Davide Mazzini, Raimondo Schettini category:cs.CV  published:2017-01-10 summary:In this paper we propose a method for logo recognition using deep learning. Our recognition pipeline is composed of a logo region proposal followed by a Convolutional Neural Network (CNN) specifically trained for logo classification, even if they are not precisely localized. Experiments are carried out on the FlickrLogos-32 database, and we evaluate the effect on recognition performance of synthetic versus real data augmentation, and image pre-processing. Moreover, we systematically investigate the benefits of different training choices such as class-balancing, sample-weighting and explicit modeling the background class (i.e. no-logo regions). Experimental results confirm the feasibility of the proposed method, that outperforms the methods in the state of the art. version:1
arxiv-1701-02610 | SubCMap: Subject and Condition Specific Effect Maps | http://arxiv.org/abs/1701.02610 | id:1701.02610 author:Ender Konukoglu, Ben Glocker category:cs.CV  published:2017-01-10 summary:Current methods for statistical analysis of neuroimaging data identify condition related structural alterations in the human brain by detecting group differences. They construct detailed maps showing population-wide changes due to a condition of interest. Although extremely useful, methods do not provide information on the subject-specific structural alterations and they have limited diagnostic value because group assignments for each subject are required for the analysis. In this article, we propose SubCMap, a novel method to detect subject and condition specific structural alterations. SubCMap is designed to work without the group assignment information in order to provide diagnostic value. Unlike outlier detection methods, SubCMap detections are condition-specific and can be used to study the effects of various conditions or for diagnosing diseases. The method combines techniques from classification, generalization error estimation and image restoration to the identify the condition-related alterations. Experimental evaluation is performed on synthetically generated data as well as data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on synthetic data demonstrate the advantages of SubCMap compared to population-wide techniques and higher detection accuracy compared to outlier detection. Analysis with the ADNI dataset show that SubCMap detections on cortical thickness data well correlate with non-imaging markers of Alzheimer's Disease (AD), the Mini Mental State Examination Score and Cerebrospinal Fluid amyloid-$\beta$ levels, suggesting the proposed method well captures the inter-subject variation of AD effects. version:1
arxiv-1701-02593 | A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling | http://arxiv.org/abs/1701.02593 | id:1701.02593 author:Diego Marcheggiani, Anton Frolov, Ivan Titov category:cs.CL cs.AI  published:2017-01-10 summary:We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves respectable performance on English even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the CoNLL-2009 dataset. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e. syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on the standard out-of-domain test set. version:1
arxiv-1701-02485 | Efficient Image Set Classification using Linear Regression based Image Reconstruction | http://arxiv.org/abs/1701.02485 | id:1701.02485 author:Syed Afaq Ali Shah, Uzair Nadeem, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri category:cs.CV  published:2017-01-10 summary:We propose a novel image set classification technique using linear regression models. Downsampled gallery image sets are interpreted as subspaces of a high dimensional space to avoid the computationally expensive training step. We estimate regression models for each test image using the class specific gallery subspaces. Images of the test set are then reconstructed using the regression models. Based on the minimum reconstruction error between the reconstructed and the original images, a weighted voting strategy is used to classify the test set. We performed extensive evaluation on the benchmark UCSD/Honda, CMU Mobo and YouTube Celebrity datasets for face classification, and ETH-80 dataset for object classification. The results demonstrate that by using only a small amount of training data, our technique achieved competitive classification accuracy and superior computational speed compared with the state-of-the-art methods. version:1
arxiv-1701-02477 | Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition | http://arxiv.org/abs/1701.02477 | id:1701.02477 author:Abhinav Thanda, Shankar M Venkatesan category:cs.CL cs.AI cs.CV cs.LG  published:2017-01-10 summary:Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\% relative improvement in WER is reported at -3 SNR dB version:1
arxiv-1701-02468 | Unite the People: Closing the Loop Between 3D and 2D Human Representations | http://arxiv.org/abs/1701.02468 | id:1701.02468 author:Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, Peter V. Gehler category:cs.CV  published:2017-01-10 summary:3D models provide the common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits "in-the-wild". However, depending on the level of detail, it can be hard to impossible to obtain labeled representations on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits to the core human pose datasets. Human annotators solely sort good and bad fits. This enables us to efficiently build a large dataset with a rich representation. In a comprehensive set of experiments, we show how we can make use of this data to push the limits of discriminative models. With segmentation into 31 body parts and keypoint detection with 91 landmarks, we present compelling results for human analysis at an unprecedented level of detail. Using our dense landmark set, we present state-of-the art results for 3D human pose and shape estimation, while having used an order of magnitude less training data and making no assumptions about gender or pose in the fitting procedure. We show that the initial dataset can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. version:1
arxiv-1701-02440 | Machine Learning of Linear Differential Equations using Gaussian Processes | http://arxiv.org/abs/1701.02440 | id:1701.02440 author:Maziar Raissi, George Em. Karniadakis category:cs.LG math.NA stat.ML  published:2017-01-10 summary:This work leverages recent advances in probabilistic machine learning to discover conservation laws expressed by parametric linear equations. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or "black-box" computer simulations. version:1
arxiv-1701-01821 | Unsupervised Learning of Long-Term Motion Dynamics for Videos | http://arxiv.org/abs/1701.01821 | id:1701.01821 author:Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei category:cs.CV  published:2017-01-07 summary:We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, Depth, and RGB-D videos. version:2
arxiv-1701-02426 | Scene Graph Generation by Iterative Message Passing | http://arxiv.org/abs/1701.02426 | id:1701.02426 author:Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei category:cs.CV  published:2017-01-10 summary:Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods on generating scene graphs using Visual Genome dataset and inferring support relations with NYU Depth v2 dataset. version:1
arxiv-1701-02392 | Reinforcement Learning via Recurrent Convolutional Neural Networks | http://arxiv.org/abs/1701.02392 | id:1701.02392 author:Tanmay Shankar, Santosha K. Dwivedy, Prithwijit Guha category:cs.LG cs.AI 68T05  published:2017-01-09 summary:Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies. version:1
arxiv-1701-02386 | AdaGAN: Boosting Generative Models | http://arxiv.org/abs/1701.02386 | id:1701.02386 author:Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, Bernhard Schölkopf category:stat.ML cs.LG  published:2017-01-09 summary:Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes. version:1
arxiv-1701-02377 | The principle of cognitive action - Preliminary experimental analysis | http://arxiv.org/abs/1701.02377 | id:1701.02377 author:Marco Gori, Marco Maggini, Alessandro Rossi category:cs.LG  published:2017-01-09 summary:In this document we shows a first implementation and some preliminary results of a new theory, facing Machine Learning problems in the frameworks of Classical Mechanics and Variational Calculus. We give a general formulation of the problem and then we studies basic behaviors of the model on simple practical implementations. version:1
arxiv-1701-00160 | NIPS 2016 Tutorial: Generative Adversarial Networks | http://arxiv.org/abs/1701.00160 | id:1701.00160 author:Ian Goodfellow category:cs.LG  published:2016-12-31 summary:This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises. version:3
arxiv-1701-02362 | Visualizing Residual Networks | http://arxiv.org/abs/1701.02362 | id:1701.02362 author:Brian Chu, Daylen Yang, Ravi Tadinada category:cs.CV  published:2017-01-09 summary:Residual networks are the current state of the art on ImageNet. Similar work in the direction of utilizing shortcut connections has been done extremely recently with derivatives of residual networks and with highway networks. This work potentially challenges our understanding that CNNs learn layers of local features that are followed by increasingly global features. Through qualitative visualization and empirical analysis, we explore the purpose that residual skip connections serve. Our assessments show that the residual shortcut connections force layers to refine features, as expected. We also provide alternate visualizations that confirm that residual networks learn what is already intuitively known about CNNs in general. version:1
arxiv-1701-02357 | Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware Semantic Segmentation | http://arxiv.org/abs/1701.02357 | id:1701.02357 author:Carlos Castillo, Soham De, Xintong Han, Bharat Singh, Abhay Kumar Yadav, Tom Goldstein category:cs.CV cs.GR  published:2017-01-09 summary:Style transfer is an important task in which the style of a source image is mapped onto that of a target image. The method is useful for synthesizing derivative works of a particular artist or specific painting. This work considers targeted style transfer, in which the style of a template image is used to alter only part of a target image. For example, an artist may wish to alter the style of only one particular object in a target image without altering the object's general morphology or surroundings. This is useful, for example, in augmented reality applications (such as the recently released Pokemon GO), where one wants to alter the appearance of a single real-world object in an image frame to make it appear as a cartoon. Most notably, the rendering of real-world objects into cartoon characters has been used in a number of films and television show, such as the upcoming series Son of Zorn. We present a method for targeted style transfer that simultaneously segments and stylizes single objects selected by the user. The method uses a Markov random field model to smooth and anti-alias outlier pixels near object boundaries, so that stylized objects naturally blend into their surroundings. version:1
arxiv-1701-02354 | MonoCap: Monocular Human Motion Capture using a CNN Coupled with a Geometric Prior | http://arxiv.org/abs/1701.02354 | id:1701.02354 author:Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon Leonardos, Kostantinos G. Derpanis, Kostas Daniilidis category:cs.CV  published:2017-01-09 summary:Recovering 3D full-body human pose is a challenging problem with many applications. It has been successfully addressed by motion capture systems with body worn markers and multiple cameras. In this paper, we address the more challenging case of not only using a single camera but also not leveraging markers: going directly from 2D appearance to 3D geometry. Deep learning approaches have shown remarkable abilities to discriminatively learn 2D appearance features. The missing piece is how to integrate 2D, 3D and temporal information to recover 3D geometry and account for the uncertainties arising from the discriminative model. We introduce a novel approach that treats 2D joint locations as latent variables, whose uncertainty distributions are given by a deep fully convolutional network. The unknown 3D poses are modeled by a sparse representation and the 3D parameter estimates are realized via an Expectation-Maximization algorithm, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Extensive evaluation on benchmark datasets shows that the proposed approach achieves greater accuracy over state-of-the-art baselines. Notably, the proposed approach does not require synchronized 2D-3D data for training and is applicable to "in-the-wild" images, which is demonstrated with the MPII dataset. version:1
arxiv-1701-02349 | MEBoost: Variable Selection in the Presence of Measurement Error | http://arxiv.org/abs/1701.02349 | id:1701.02349 author:Benjamin Brown, Timothy Weaver, Julian Wolfson category:stat.CO stat.ML  published:2017-01-09 summary:We present a novel method for variable selection in regression models when covariates are measured with error. The iterative algorithm we propose, MEBoost, follows a path defined by estimating equations that correct for covariate measurement error. Via simulation, we evaluated our method and compare its performance to the recently-proposed Convex Conditioned Lasso (CoCoLasso) and to the "naive" Lasso which does not correct for measurement error. Increasing the degree of measurement error increased prediction error and decreased the probability of accurate covariate selection, but this loss of accuracy was least pronounced when using MEBoost. We illustrate the use of MEBoost in practice by analyzing data from the Box Lunch Study, a clinical trial in nutrition where several variables are based on self-report and hence measured with error. version:1
arxiv-1701-02343 | Information Pursuit: A Bayesian Framework for Sequential Scene Parsing | http://arxiv.org/abs/1701.02343 | id:1701.02343 author:Ehsan Jahangiri, Erdem Yoruk, Rene Vidal, Laurent Younes, Donald Geman category:cs.CV cs.AI stat.ML  published:2017-01-09 summary:Despite enormous progress in object detection and classification, the problem of incorporating expected contextual relationships among object instances into modern recognition systems remains a key challenge. In this work we propose Information Pursuit, a Bayesian framework for scene parsing that combines prior models for the geometry of the scene and the spatial arrangement of objects instances with a data model for the output of high-level image classifiers trained to answer specific questions about the scene. In the proposed framework, the scene interpretation is progressively refined as evidence accumulates from the answers to a sequence of questions. At each step, we choose the question to maximize the mutual information between the new answer and the full interpretation given the current evidence obtained from previous inquiries. We also propose a method for learning the parameters of the model from synthesized, annotated scenes obtained by top-down sampling from an easy-to-learn generative scene model. Finally, we introduce a database of annotated indoor scenes of dining room tables, which we use to evaluate the proposed approach. version:1
arxiv-1701-02284 | DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning | http://arxiv.org/abs/1701.02284 | id:1701.02284 author:Tian Zhao, Xiaobing Huang, Yu Cao category:cs.PL cs.LG  published:2017-01-09 summary:In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides (1) intuitive constructs to support compact encoding of deep networks; (2) symbolic gradient derivation of the networks; (3) static analysis for memory consumption and error detection; and (4) DSL-level optimization to improve memory and runtime efficiency. DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries. version:1
arxiv-1701-02273 | Visual Multiple-Object Tracking for Unknown Clutter Rate | http://arxiv.org/abs/1701.02273 | id:1701.02273 author:Du Yong Kim category:cs.CV  published:2017-01-09 summary:In most multi-object tracking algorithms, tuning of model parameters is of critical importance for reliable performance. In particular, we are interested in designing a robust tracking algorithm that is able to handle unknown false measurement rate. The proposed algorithm is based on coupling of two random finite set filters that share tracking parameters. Performance evaluation with visual surveillance and cell microscopy images demonstrates the effectiveness of the tracking algorithm for real-world scenarios. version:1
arxiv-1701-00495 | Vid2speech: Speech Reconstruction from Silent Video | http://arxiv.org/abs/1701.00495 | id:1701.00495 author:Ariel Ephrat, Shmuel Peleg category:cs.CV cs.SD  published:2017-01-02 summary:Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words. version:2
arxiv-1701-02265 | On Reject and Refine Options in Multicategory Classification | http://arxiv.org/abs/1701.02265 | id:1701.02265 author:Chong Zhang, Wenbo Wang, Xingye Qiao category:stat.ML math.ST stat.CO stat.TH 62H30  published:2017-01-09 summary:In many real applications of statistical learning, a decision made from misclassification can be too costly to afford; in this case, a reject option, which defers the decision until further investigation is conducted, is often preferred. In recent years, there has been much development for binary classification with a reject option. Yet, little progress has been made for the multicategory case. In this article, we propose margin-based multicategory classification methods with a reject option. In addition, and more importantly, we introduce a new and unique refine option for the multicategory problem, where the class of an observation is predicted to be from a set of class labels, whose cardinality is not necessarily one. The main advantage of both options lies in their capacity of identifying error-prone observations. Moreover, the refine option can provide more constructive information for classification by effectively ruling out implausible classes. Efficient implementations have been developed for the proposed methods. On the theoretical side, we offer a novel statistical learning theory and show a fast convergence rate of the excess $\ell$-risk of our methods with emphasis on diverging dimensionality and number of classes. The results can be further improved under a low noise assumption. A set of comprehensive simulation and real data studies has shown the usefulness of the new learning tools compared to regular multicategory classifiers. Detailed proofs of theorems and extended numerical results are included in the supplemental materials available online. version:1
arxiv-1701-02258 | Multiple Instance Hybrid Estimator for Learning Target Signatures | http://arxiv.org/abs/1701.02258 | id:1701.02258 author:Changzhe Jiao, Alina Zare category:cs.CV  published:2017-01-09 summary:Signature-based detectors for hyperspectral target detection rely on knowing the specific target signature in advance. However, target signature are often difficult or impossible to obtain. Furthermore, common methods for obtaining target signatures, such as from laboratory measurements or manual selection from an image scene, usually do not capture the discriminative features of target class. In this paper, an approach for estimating a discriminative target signature from imprecise labels is presented. The proposed approach maximizes the response of the hybrid sub-pixel detector within a multiple instance learning framework and estimates a set of discriminative target signatures. After learning target signatures, any signature based detector can then be applied on test data. Both simulated and real hyperspectral target detection experiments are shown to illustrate the effectiveness of the method. version:1
arxiv-1701-01081 | SalGAN: Visual Saliency Prediction with Generative Adversarial Networks | http://arxiv.org/abs/1701.01081 | id:1701.01081 author:Junting Pan, Cristian Canton Ferrer, Kevin McGuinness, Noel E. O'Connor, Jordi Torres, Elisa Sayrol, Xavier Giro-i-Nieto category:cs.CV  published:2017-01-04 summary:We introduce SalGAN, a deep convolutional neural network for visual saliency prediction trained with adversarial examples. The first stage of the network consists of a generator model whose weights are learned by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency maps. The resulting prediction is processed by a discriminator network trained to solve a binary classification task between the saliency maps generated by the generative stage and the ground truth ones. Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a widely-used loss function like BCE. version:2
arxiv-1701-00311 | Bayesian model selection consistency and oracle inequality with intractable marginal likelihood | http://arxiv.org/abs/1701.00311 | id:1701.00311 author:Yun Yang, Debdeep Pati category:math.ST stat.ME stat.ML stat.TH  published:2017-01-02 summary:In this article, we investigate large sample properties of model selection procedures in a general Bayesian framework when a closed form expression of the marginal likelihood function is not available or a local asymptotic quadratic approximation of the log-likelihood function does not exist. Under appropriate identifiability assumptions on the true model, we provide sufficient conditions for a Bayesian model selection procedure to be consistent and obey the Occam's razor phenomenon, i.e., the probability of selecting the "smallest" model that contains the truth tends to one as the sample size goes to infinity. In order to show that a Bayesian model selection procedure selects the smallest model containing the truth, we impose a prior anti-concentration condition, requiring the prior mass assigned by large models to a neighborhood of the truth to be sufficiently small. In a more general setting where the strong model identifiability assumption may not hold, we introduce the notion of local Bayesian complexity and develop oracle inequalities for Bayesian model selection procedures. Our Bayesian oracle inequality characterizes a trade-off between the approximation error and a Bayesian characterization of the local complexity of the model, illustrating the adaptive nature of averaging-based Bayesian procedures towards achieving an optimal rate of posterior convergence. Specific applications of the model selection theory are discussed in the context of high-dimensional nonparametric regression and density regression where the regression function or the conditional density is assumed to depend on a fixed subset of predictors. As a result of independent interest, we propose a general technique for obtaining upper bounds of certain small ball probability of stationary Gaussian processes. version:2
arxiv-1701-02185 | Crowdsourcing Ground Truth for Medical Relation Extraction | http://arxiv.org/abs/1701.02185 | id:1701.02185 author:Anca Dumitrache, Lora Aroyo, Chris Welty category:cs.CL cs.HC  published:2017-01-09 summary:Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the $cause$ and $treat$ relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task. version:1
arxiv-1701-02166 | A Learning-based Variable Size Part Extraction Architecture for 6D Object Pose Recovery in Depth | http://arxiv.org/abs/1701.02166 | id:1701.02166 author:Caner Sahin, Rigas Kouskouridas, Tae-Kyun Kim category:cs.CV  published:2017-01-09 summary:State-of-the-art techniques for 6D object pose recovery depend on occlusion-free point clouds to accurately register objects in 3D space. To deal with this shortcoming, we introduce a novel architecture called Iterative Hough Forest with Histogram of Control Points that is capable of estimating the 6D pose of occluded and cluttered objects given a candidate 2D bounding box. Our Iterative Hough Forest (IHF) is learnt using parts extracted only from the positive samples. These parts are represented with Histogram of Control Points (HoCP), a "scale-variant" implicit volumetric description, which we derive from recently introduced Implicit B-Splines (IBS). The rich discriminative information provided by the scale-variant HoCP features is leveraged during inference. An automatic variable size part extraction framework iteratively refines the object's initial pose that is roughly aligned due to the extraction of coarsest parts, the ones occupying the largest area in image pixels. The iterative refinement is accomplished based on finer (smaller) parts that are represented with more discriminative control point descriptors by using our Iterative Hough Forest. Experiments conducted on a publicly available dataset report that our approach show better registration performance than the state-of-the-art methods. version:1
arxiv-1701-02163 | Just an Update on PMING Distance for Web-based Semantic Similarity in Artificial Intelligence and Data Mining | http://arxiv.org/abs/1701.02163 | id:1701.02163 author:Valentina Franzoni category:cs.AI cs.CL cs.IR math.PR 68T20 I.2.8; I.5.3; I.7.1  published:2017-01-09 summary:One of the main problems that emerges in the classic approach to semantics is the difficulty in acquisition and maintenance of ontologies and semantic annotations. On the other hand, the Internet explosion and the massive diffusion of mobile smart devices lead to the creation of a worldwide system, which information is daily checked and fueled by the contribution of millions of users who interacts in a collaborative way. Search engines, continually exploring the Web, are a natural source of information on which to base a modern approach to semantic annotation. A promising idea is that it is possible to generalize the semantic similarity, under the assumption that semantically similar terms behave similarly, and define collaborative proximity measures based on the indexing information returned by search engines. The PMING Distance is a proximity measure used in data mining and information retrieval, which collaborative information express the degree of relationship between two terms, using only the number of documents returned as result for a query on a search engine. In this work, the PMINIG Distance is updated, providing a novel formal algebraic definition, which corrects previous works. The novel point of view underlines the features of the PMING to be a locally normalized linear combination of the Pointwise Mutual Information and Normalized Google Distance. The analyzed measure dynamically reflects the collaborative change made on the web resources. version:1
arxiv-1701-02149 | Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching | http://arxiv.org/abs/1701.02149 | id:1701.02149 author:Wenpeng Yin, Hinrich Schütze category:cs.CL  published:2017-01-09 summary:This work studies comparatively two typical sentence matching tasks: textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final classifier. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts alignment at word and phrase levels by handcrafted features or (iii) utilizes a single framework of alignment without considering the characteristics of specific tasks, which limits the framework's effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach. version:1
arxiv-1701-02145 | Shallow and Deep Networks Intrusion Detection System: A Taxonomy and Survey | http://arxiv.org/abs/1701.02145 | id:1701.02145 author:Elike Hodo, Xavier Bellekens, Andrew Hamilton, Christos Tachtatzis, Robert Atkinson category:cs.CR cs.LG  published:2017-01-09 summary:Intrusion detection has attracted a considerable interest from researchers and industries. The community, after many years of research, still faces the problem of building reliable and efficient IDS that are capable of handling large quantities of data, with changing patterns in real time situations. The work presented in this manuscript classifies intrusion detection systems (IDS). Moreover, a taxonomy and survey of shallow and deep networks intrusion detection systems is presented based on previous and current works. This taxonomy and survey reviews machine learning techniques and their performance in detecting anomalies. Feature selection which influences the effectiveness of machine learning (ML) IDS is discussed to explain the role of feature selection in the classification and training phase of ML IDS. Finally, a discussion of the false and true positive alarm rates is presented to help researchers model reliable and efficient machine learning based intrusion detection systems. version:1
arxiv-1701-02141 | Light Field Super-Resolution Via Graph-Based Regularization | http://arxiv.org/abs/1701.02141 | id:1701.02141 author:Mattia Rossi, Pascal Frossard category:cs.CV cs.MM  published:2017-01-09 summary:Light field cameras can capture the 3D information in a scene with a single shot. This special feature makes light field cameras very appealing for a variety of applications: from the popular post-capture refocus, to depth estimation and image-based rendering. However, light field cameras suffer by design from strong limitations in their spatial resolution, which should therefore be augmented by computational methods. On the one hand, off-the-shelf single-frame and multi-frame super-resolution algorithms are not ideal for light field data, as they do not consider its particular structure. On the other hand, the few super-resolution algorithms explicitly tailored for light field data exhibit significant limitations, such as the need to estimate an explicit disparity map at each view. In this work we propose a new light field super-resolution algorithm meant to address these limitations. We adopt a multi-frame alike super-resolution approach, where the complementary information in the different light field views is used to augment the spatial resolution of the whole light field. We show that coupling the multi-frame approach with a graph regularizer, that enforces the light field structure via non local self similarities, permits to avoid the costly and challenging disparity estimation step for all the views. Extensive experiments show that the proposed algorithm compares favorably to the other state-of-the-art methods for light field super-resolution, both in terms of PSNR and in terms of visual quality. Moreover, differently from the other light field super-resolution methods, the new algorithm provides reconstructed light field views with uniform quality, which happens to be an important feature for any light field application. version:1
arxiv-1701-02133 | Deep driven fMRI decoding of visual categories | http://arxiv.org/abs/1701.02133 | id:1701.02133 author:Michele Svanera, Sergio Benini, Gal Raz, Talma Hendler, Rainer Goebel, Giancarlo Valente category:stat.ML cs.LG q-bio.NC  published:2017-01-09 summary:Deep neural networks have been developed drawing inspiration from the brain visual pathway, implementing an end-to-end approach: from image data to video object classes. However building an fMRI decoder with the typical structure of Convolutional Neural Network (CNN), i.e. learning multiple level of representations, seems impractical due to lack of brain data. As a possible solution, this work presents the first hybrid fMRI and deep features decoding approach: collected fMRI and deep learnt representations of video object classes are linked together by means of Kernel Canonical Correlation Analysis. In decoding, this allows exploiting the discriminatory power of CNN by relating the fMRI representation to the last layer of CNN (fc7). We show the effectiveness of embedding fMRI data onto a subspace related to deep features in distinguishing semantic visual categories based solely on brain imaging data. version:1
arxiv-1701-02127 | Discrete approximations of affine Gaussian receptive fields | http://arxiv.org/abs/1701.02127 | id:1701.02127 author:Tony Lindeberg category:cs.CV  published:2017-01-09 summary:This paper presents a theory for discretizing the affine Gaussian scale-space concept so that scale-space properties hold also for the discrete implementation. Two ways of discretizing spatial smoothing with affine Gaussian kernels are presented: (i) by solving semi-discretized affine diffusion equation as derived by necessity from the requirement of a semi-group structure over a continuum of scale parameters as parameterized by a family of spatial covariance matrices and obeying non-creation of new structures from any finer to any coarser scale as formalized by the requirement of non-enhancement of local extrema and (ii) a set of parameterized 3x3-kernels as derived from an additional discretization of the above theory along the scale direction and with the parameters of the kernels having a direct interpretation in terms of the covariance matrix of the composed discrete smoothing operation. We show how convolutions with the first family of kernels can be implemented in terms of a closed form expression for the Fourier transform and analyse how a remaining degree of freedom in the theory can be explored to ensure a positive discretization and optionally also achieve higher-order discrete approximation of the angular dependency of the shapes of the affine Gaussian kernels. We do also show how discrete directional derivative approximations can be efficiently implemented to approximate affine Gaussian derivatives as constituting a canonical model for receptive fields over a purely spatial image domain and with close relations to receptive fields in biological vision. version:1
arxiv-1701-02123 | Green-Blue Stripe Pattern for Range Sensing from a Single Image | http://arxiv.org/abs/1701.02123 | id:1701.02123 author:Changsoo Je, Kyuhyoung Choi, Sang Wook Lee category:cs.CV cs.GR I.2.10; I.4.8  published:2017-01-09 summary:In this paper, we present a novel method for rapid high-resolution range sensing using green-blue stripe pattern. We use green and blue for designing high-frequency stripe projection pattern. For accurate and reliable range recovery, we identify the stripe patterns by our color-stripe segmentation and unwrapping algorithms. The experimental result for a naked human face shows the effectiveness of our method. version:1
arxiv-1701-01582 | Learning Sparse Structural Changes in High-dimensional Markov Networks: A Review on Methodologies and Theories | http://arxiv.org/abs/1701.01582 | id:1701.01582 author:Song Liu, Kenji Fukumizu, Taiji Suzuki category:stat.ML  published:2017-01-06 summary:Recent years have seen an increasing popularity of learning the sparse \emph{changes} in Markov Networks. Changes in the structure of Markov Networks reflect alternations of interactions between random variables under different regimes and provide insights into the underlying system. While each individual network structure can be complicated and difficult to learn, the overall change from one network to another can be simple. This intuition gave birth to an approach that \emph{directly} learns the sparse changes without modelling and learning the individual (possibly dense) networks. In this paper, we review such a direct learning method with some latest developments along this line of research. version:2
arxiv-1701-02110 | Transformation Forests | http://arxiv.org/abs/1701.02110 | id:1701.02110 author:Torsten Hothorn, Achim Zeileis category:stat.ME stat.ML  published:2017-01-09 summary:Regression models for supervised learning problems with a continuous target are commonly understood as models for the conditional mean of the target given predictors. This notion is simple and therefore appealing for interpretation and visualisation. Information about the whole underlying conditional distribution is, however, not available from these models. A more general understanding of regression models as models for conditional distributions allows much broader inference from such models, for example the computation of prediction intervals. Several random forest-type algorithms aim at estimating conditional distributions, most prominently quantile regression forests (Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric family of distributions characterised by their transformation function. A dedicated novel "transformation tree" algorithm able to detect distributional changes is developed. Based on these transformation trees, we introduce "transformation forests" as an adaptive local likelihood estimator of conditional distribution functions. The resulting models are fully parametric yet very general and allow broad inference procedures, such as the model-based bootstrap, to be applied in a straightforward way. version:1
arxiv-1701-05088 | Temporal scale selection in time-causal scale space | http://arxiv.org/abs/1701.05088 | id:1701.05088 author:Tony Lindeberg category:cs.CV  published:2017-01-09 summary:When designing and developing scale selection mechanisms for generating hypotheses about characteristic scales in signals, it is essential that the selected scale levels reflect the extent of the underlying structures in the signal. This paper presents a theory and in-depth theoretical analysis about the scale selection properties of methods for automatically selecting local temporal scales in time-dependent signals based on local extrema over temporal scales of scale-normalized temporal derivative responses. Specifically, this paper develops a novel theoretical framework for performing such temporal scale selection over a time-causal and time-recursive temporal domain as is necessary when processing continuous video or audio streams in real time or when modelling biological perception. For a recently developed time-causal and time-recursive scale-space concept defined by convolution with a scale-invariant limit kernel, we show that it is possible to transfer a large number of the desirable scale selection properties that hold for the Gaussian scale-space concept over a non-causal temporal domain to this temporal scale-space concept over a truly time-causal domain. Specifically, we show that for this temporal scale-space concept, it is possible to achieve true temporal scale invariance although the temporal scale levels have to be discrete, which is a novel theoretical construction. version:1
arxiv-1701-02096 | Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis | http://arxiv.org/abs/1701.02096 | id:1701.02096 author:Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky category:cs.CV  published:2017-01-09 summary:The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage. version:1
arxiv-1701-02073 | Neural Personalized Response Generation as Domain Adaptation | http://arxiv.org/abs/1701.02073 | id:1701.02073 author:Weinan Zhang, Ting Liu, Yifa Wang, Qingfu Zhu category:cs.CL  published:2017-01-09 summary:In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems. version:1
arxiv-1701-02071 | Optimal statistical decision for Gaussian graphical model selection | http://arxiv.org/abs/1701.02071 | id:1701.02071 author:Valery A. Kalyagin, Alexander P. Koldanov, Petr A. Koldanov, Panos M. Pardalos category:stat.ML 62-09  62H15  62C05  published:2017-01-09 summary:Gaussian graphical model is a graphical representation of the dependence structure for a Gaussian random vector. It is recognized as a powerful tool in different applied fields such as bioinformatics, error-control codes, speech language, information retrieval and others. Gaussian graphical model selection is a statistical problem to identify the Gaussian graphical model from a sample of a given size. Different approaches for Gaussian graphical model selection are suggested in the literature. One of them is based on considering the family of individual conditional independence tests. The application of this approach leads to the construction of a variety of multiple testing statistical procedures for Gaussian graphical model selection. An important characteristic of these procedures is its error rate for a given sample size. In existing literature great attention is paid to the control of error rates for incorrect edge inclusion (Type I error). However, in graphical model selection it is also important to take into account error rates for incorrect edge exclusion (Type II error). To deal with this issue we consider the graphical model selection problem in the framework of the multiple decision theory. The quality of statistical procedures is measured by a risk function with additive losses. Additive losses allow both types of errors to be taken into account. We construct the tests of a Neyman structure for individual hypotheses and combine them to obtain a multiple decision statistical procedure. We show that the obtained procedure is optimal in the sense that it minimizes the linear combination of expected numbers of Type I and Type II errors in the class of unbiased multiple decision procedures. version:1
arxiv-1701-02058 | Coupled Compound Poisson Factorization | http://arxiv.org/abs/1701.02058 | id:1701.02058 author:Mehmet E. Basbug, Barbara E. Engelhardt category:cs.LG cs.AI stat.ML  published:2017-01-09 summary:We present a general framework, the coupled compound Poisson factorization (CCPF), to capture the missing-data mechanism in extremely sparse data sets by coupling a hierarchical Poisson factorization with an arbitrary data-generating model. We derive a stochastic variational inference algorithm for the resulting model and, as examples of our framework, implement three different data-generating models---a mixture model, linear regression, and factor analysis---to robustly model non-random missing data in the context of clustering, prediction, and matrix factorization. In all three cases, we test our framework against models that ignore the missing-data mechanism on large scale studies with non-random missing data, and we show that explicitly modeling the missing-data mechanism substantially improves the quality of the results, as measured using data log likelihood on a held-out test set. version:1
arxiv-1701-02046 | Tunable GMM Kernels | http://arxiv.org/abs/1701.02046 | id:1701.02046 author:Ping Li category:stat.ML cs.LG  published:2017-01-09 summary:The recently proposed "generalized min-max" (GMM) kernel can be efficiently linearized via hashing or the Nystrom method, with direct applications in large-scale machine learning and fast near neighbor search. As a tuning-free kernel, the linearized GMM kernel has been extensively compared with the normalized random Fourier features (NRFF). On classification tasks, the tuning-free GMM kernel performs (surprisingly) well compared to the best-tuned RBF kernel. Nevertheless, one would naturally expect that the GMM kernel ought to be further improved if we introduce tuning parameters. In this paper, we study two simple constructions of tunable GMM kernels: (i) the exponentiated-GMM (or eGMM) kernel, and (ii) the powered-GMM (or pGMM) kernel. One can of course introduce additional (and more complex) kernels by (e.g.,) combining eGMM and pGMM kernels. The pGMM kernel can still be efficiently linearized by modifying the original hashing procedure for the GMM kernel. On 47 publicly available classification datasets from the UCI repository, we verify that the eGMM and pGMM kernels (typically) improve over the original GMM kernel. On a fraction of datasets, the improvements can be astonishingly significant. We hope our introduction of tunable kernels could offer practitioners the flexibility of choosing appropriate kernels and methods for large-scale search \& learning for their specific applications. version:1
arxiv-1701-02026 | Large-Scale Network Motif Learning with Compression | http://arxiv.org/abs/1701.02026 | id:1701.02026 author:Peter Bloem, Steven de Rooij category:cs.LG  published:2017-01-08 summary:We introduce a new learning method for network motifs: interesting or informative subgraph patterns in a network. Current methods for finding motifs rely on the frequency of the motif: specifically, subgraphs are motifs when their frequency in the data is high compared to the expected frequency under a null model. To compute this expectation, the search for motifs is normally repeated on as many as 1000 random graphs sampled from the null model, a prohibitively expensive step. We use ideas from the Minimum Description Length (MDL) literature to define a new measure of motif relevance. This has several advantages: the subgraph count on samples from the null model can be eliminated, and the search for motif candidates within the data itself can be greatly simplified. Our method allows motif analysis to scale to networks with billions of links, provided that a fast null model is used. version:1
arxiv-1701-01996 | MS and PAN image fusion by combining Brovey and wavelet methods | http://arxiv.org/abs/1701.01996 | id:1701.01996 author:Hamid Reza Shahdoosti category:cs.CV 68U10  published:2017-01-08 summary:Among the existing fusion algorithms, the wavelet fusion method is the most frequently discussed one in recent publications because the wavelet approach preserves the spectral characteristics of the multispectral image better than other methods. The Brovey is also a popular fusion method used for its ability in preserving the spatial information of the PAN image. This study presents a new fusion approach that integrates the advantages of both the Brovey (which preserves a high degree of spatial information) and the wavelet (which preserves a high degree of spectral information) techniques to reduce the colour distortion of fusion results. Visual and statistical analyzes show that the proposed algorithm clearly improves the merging quality in terms of: correlation coefficient and UIQI; compared to fusion methods including, IHS, Brovey, PCA , HPF, discrete wavelet transform (DWT), and a-trous wavelet. version:1
arxiv-1701-01945 | A Framework for Wasserstein-1-Type Metrics | http://arxiv.org/abs/1701.01945 | id:1701.01945 author:Bernhard Schmitzer, Benedikt Wirth category:math.OC cs.CV math.NA  published:2017-01-08 summary:We propose a unifying framework for generalising the Wasserstein-1 metric to a discrepancy measure between nonnegative measures of different mass. This generalization inherits the convexity and computational efficiency from the Wasserstein-1 metric, and it includes several previous approaches from the literature as special cases. For various specific instances of the generalized Wasserstein-1 metric we furthermore demonstrate their usefulness in applications by numerical experiments. version:1
arxiv-1701-01942 | Multi-spectral Image Panchromatic Sharpening, Outcome and Process Quality Assessment Protocol | http://arxiv.org/abs/1701.01942 | id:1701.01942 author:Andrea Baraldi, Francesca Despini, Sergio Teggi category:cs.CV  published:2017-01-08 summary:Multispectral (MS) image panchromatic (PAN) sharpening algorithms proposed to the remote sensing community are ever increasing in number and variety. Their aim is to sharpen a coarse spatial resolution MS image with a fine spatial resolution PAN image acquired simultaneously by a spaceborne or airborne Earth observation (EO) optical imaging sensor pair. Unfortunately, to date, no standard evaluation procedure for MS image PAN sharpening outcome and process is community agreed upon, in contrast with the Quality Assurance Framework for Earth Observation (QA4EO) guidelines proposed by the intergovernmental Group on Earth Observations (GEO). In general, process is easier to measure, outcome is more important. The original contribution of the present study is fourfold. First, existing procedures for quantitative quality assessment (Q2A) of the (sole) PAN sharpened MS product are critically reviewed. Their conceptual and implementation drawbacks are highlighted to be overcome for quality improvement. Second, a novel (to the best of these authors' knowledge, the first) protocol for Q2A of MS image PAN sharpening product and process is designed, implemented and validated by independent means. Third, within this protocol, an innovative categorization of spectral and spatial image quality indicators and metrics is presented. Fourth, according to this new taxonomy, an original third order isotropic multi scale gray level co occurrence matrix (TIMS GLCM) calculator and a TIMS GLCM texture feature extractor are proposed to replace popular second order GLCMs. version:1
arxiv-1701-01940 | Automated Linear-Time Detection and Quality Assessment of Superpixels in Uncalibrated True- or False-Color RGB Images | http://arxiv.org/abs/1701.01940 | id:1701.01940 author:Andrea Baraldi, Dirk Tiede, Stefan Lang category:cs.CV  published:2017-01-08 summary:Capable of automated near real time superpixel detection and quality assessment in an uncalibrated monitor typical red green blue (RGB) image, depicted in either true or false colors, an original low level computer vision (CV) lightweight computer program, called RGB Image Automatic Mapper (RGBIAM), is designed and implemented. Constrained by the Calibration Validation (CalVal) requirements of the Quality Assurance Framework for Earth Observation (QA4EO) guidelines, RGBIAM requires as mandatory an uncalibrated RGB image pre processing first stage, consisting of an automated statistical model based color constancy algorithm. The RGBIAM hybrid inference pipeline comprises: (I) a direct quantitative to nominal (QN) RGB variable transform, where RGB pixel values are mapped onto a prior dictionary of color names, equivalent to a static polyhedralization of the RGB cube. Prior color naming is the deductive counterpart of inductive vector quantization (VQ), whose typical VQ error function to minimize is a root mean square error (RMSE). In the output multi level color map domain, superpixels are automatically detected in linear time as connected sets of pixels featuring the same color label. (II) An inverse nominal to quantitative (NQ) RGB variable transform, where a superpixelwise constant RGB image approximation is generated in linear time to assess a VQ error image. The hybrid direct and inverse RGBIAM QNQ transform is: (i) general purpose, data and application independent. (ii) Automated, i.e., it requires no user machine interaction. (iii) Near real time, with a computational complexity increasing linearly with the image size. (iv) Implemented in tile streaming mode, to cope with massive images. Collected outcome and process quality indicators, including degree of automation, computational efficiency, VQ rate and VQ error, are consistent with theoretical expectations. version:1
arxiv-1701-01932 | Stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for Earth observation Level 2 product generation, Part 2 Validation | http://arxiv.org/abs/1701.01932 | id:1701.01932 author:Andrea Baraldi, Michael Laurence Humber, Dirk Tiede, Stefan Lang category:cs.CV  published:2017-01-08 summary:The European Space Agency (ESA) defines an Earth Observation (EO) Level 2 product as a multispectral (MS) image corrected for geometric, atmospheric, adjacency and topographic effects, stacked with its scene classification map (SCM) whose legend includes quality layers such as cloud and cloud-shadow. No ESA EO Level 2 product has ever been systematically generated at the ground segment. To contribute toward filling an information gap from EO big sensory data to the ESA EO Level 2 product, a Stage 4 validation (Val) of an off the shelf Satellite Image Automatic Mapper (SIAM) lightweight computer program for prior knowledge based MS color naming was conducted by independent means. A time-series of annual Web Enabled Landsat Data (WELD) image composites of the conterminous U.S. (CONUS) was selected as input dataset. The annual SIAM WELD maps of the CONUS were validated in comparison with the U.S. National Land Cover Data (NLCD) 2006 map. These test and reference maps share the same spatial resolution and spatial extent, but their map legends are not the same and must be harmonized. For the sake of readability this paper is split into two. The previous Part 1 Theory provided the multidisciplinary background of a priori color naming. The present Part 2 Validation presents and discusses Stage 4 Val results collected from the test SIAM WELD map time series and the reference NLCD map by an original protocol for wall to wall thematic map quality assessment without sampling, where the test and reference map legends can differ in agreement with the Part 1. Conclusions are that the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the FAO Land Cover Classification System (LCCS) taxonomy at the Dichotomous Phase (DP) Level 1 vegetation/nonvegetation, Level 2 terrestrial/aquatic or superior LCCS level. version:1
arxiv-1701-01930 | Stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for Earth observation Level 2 product generation, Part 1 Theory | http://arxiv.org/abs/1701.01930 | id:1701.01930 author:Andrea Baraldi, Michael Laurence Humber, Dirk Tiede, Stefan Lang category:cs.CV  published:2017-01-08 summary:The European Space Agency (ESA) defines an Earth Observation (EO) Level 2 product as a multispectral (MS) image corrected for geometric, atmospheric, adjacency and topographic effects, stacked with its scene classification map (SCM), whose legend includes quality layers such as cloud and cloud-shadow. No ESA EO Level 2 product has ever been systematically generated at the ground segment. To contribute toward filling an information gap from EO big data to the ESA EO Level 2 product, an original Stage 4 validation (Val) of the Satellite Image Automatic Mapper (SIAM) lightweight computer program was conducted by independent means on an annual Web-Enabled Landsat Data (WELD) image composite time-series of the conterminous U.S. The core of SIAM is a one pass prior knowledge based decision tree for MS reflectance space hyperpolyhedralization into static color names presented in literature in recent years. For the sake of readability this paper is split into two. The present Part 1 Theory provides the multidisciplinary background of a priori color naming in cognitive science, from linguistics to computer vision. To cope with dictionaries of MS color names and land cover class names that do not coincide and must be harmonized, an original hybrid guideline is proposed to identify a categorical variable pair relationship. An original quantitative measure of categorical variable pair association is also proposed. The subsequent Part 2 Validation discusses Stage 4 Val results collected by an original protocol for wall-to-wall thematic map quality assessment without sampling where the test and reference map legends can differ. Conclusions are that the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the 4 class taxonomy of the FAO Land Cover Classification System at the Dichotomous Phase Level 1 vegetation/nonvegetation and Level 2 terrestrial/aquatic. version:1
arxiv-1701-01924 | On Classification of Distorted Images with Deep Convolutional Neural Networks | http://arxiv.org/abs/1701.01924 | id:1701.01924 author:Yiren Zhou, Sibo Song, Ngai-Man Cheung category:cs.CV  published:2017-01-08 summary:Image blur and image noise are common distortions during image acquisition. In this paper, we systematically study the effect of image distortions on the deep neural network (DNN) image classifiers. First, we examine the DNN classifier performance under four types of distortions. Second, we propose two approaches to alleviate the effect of image distortion: re-training and fine-tuning with noisy images. Our results suggest that, under certain conditions, fine-tuning with noisy images can alleviate much effect due to distorted inputs, and is more practical than re-training. version:1
arxiv-1701-01917 | See the Near Future: A Short-Term Predictive Methodology to Traffic Load in ITS | http://arxiv.org/abs/1701.01917 | id:1701.01917 author:Xun Zhou, Changle Li, Zhe Liu, Tom H. Luan, Zhifang Miao, Lina Zhu, Lei Xiong category:cs.LG stat.AP  published:2017-01-08 summary:The Intelligent Transportation System (ITS) targets to a coordinated traffic system by applying the advanced wireless communication technologies for road traffic scheduling. Towards an accurate road traffic control, the short-term traffic forecasting to predict the road traffic at the particular site in a short period is often useful and important. In existing works, Seasonal Autoregressive Integrated Moving Average (SARIMA) model is a popular approach. The scheme however encounters two challenges: 1) the analysis on related data is insufficient whereas some important features of data may be neglected; and 2) with data presenting different features, it is unlikely to have one predictive model that can fit all situations. To tackle above issues, in this work, we develop a hybrid model to improve accuracy of SARIMA. In specific, we first explore the autocorrelation and distribution features existed in traffic flow to revise structure of the time series model. Based on the Gaussian distribution of traffic flow, a hybrid model with a Bayesian learning algorithm is developed which can effectively expand the application scenarios of SARIMA. We show the efficiency and accuracy of our proposal using both analysis and experimental studies. Using the real-world trace data, we show that the proposed predicting approach can achieve satisfactory performance in practice. version:1
arxiv-1701-00573 | Robust method for finding sparse solutions to linear inverse problems using an L2 regularization | http://arxiv.org/abs/1701.00573 | id:1701.00573 author:Gonzalo H Otazu category:cs.NA cs.LG stat.ML  published:2017-01-03 summary:We analyzed the performance of a biologically inspired algorithm called the Corrected Projections Algorithm (CPA) when a sparseness constraint is required to unambiguously reconstruct an observed signal using atoms from an overcomplete dictionary. By changing the geometry of the estimation problem, CPA gives an analytical expression for a binary variable that indicates the presence or absence of a dictionary atom using an L2 regularizer. The regularized solution can be implemented using an efficient real-time Kalman-filter type of algorithm. The smoother L2 regularization of CPA makes it very robust to noise, and CPA outperforms other methods in identifying known atoms in the presence of strong novel atoms in the signal. version:2
arxiv-1701-01911 | Random Sampling and Locality Constraint for Face Sketch | http://arxiv.org/abs/1701.01911 | id:1701.01911 author:Nannan Wang, Xinbo Gao, Jie Li category:cs.CV  published:2017-01-08 summary:Face sketch synthesis plays an important role in both digital entertainment and law enforcement. It generally consists of two parts: neighbor selection and reconstruction weight representation. State-of-the-art face sketch synthesis methods perform neighbor selection online in a data-driven manner by $K$ nearest neighbor ($K$-NN) searching. However, the online search increases the time consuming for synthesis. Moreover, since these methods need to traverse the whole training dataset for neighbor selection, the computational complexity increases with the scale of the training database and hence these methods have limited scalability. In addition, state-of-the-art methods consider that all selected neighbors contribute equally to the reconstruction weight computation process while the distinct similarity between the test patch and these neighbors are neglected. In this paper, we employ offline random sampling in place of online $K$-NN search to improve the synthesis efficiency. Locality constraint is introduced to model the distinct correlations between the test patch and random sampled patches. Extensive experiments on public face sketch databases demonstrate the superiority of the proposed method in comparison to state-of-the-art methods, in terms of both synthesis quality and time consumption. The proposed method could be extended to other heterogeneous face image transformation problems such as face hallucination. We will release the source codes of our proposed methods and the evaluation metrics for future study online: http://www.ihitworld.com/RSLCR.html. version:1
arxiv-1701-01909 | Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies | http://arxiv.org/abs/1701.01909 | id:1701.01909 author:Amir Sadeghian, Alexandre Alahi, Silvio Savarese category:cs.CV  published:2017-01-08 summary:We present a multi-cue metric learning framework to tackle the popular yet unsolved Multi-Object Tracking (MOT) problem. One of the key challenges of tracking methods is to effectively compute a similarity score that models multiple cues from the past such as object appearance, motion, or even interactions. This is particularly challenging when objects get occluded or share similar appearance properties with surrounding objects. To address this challenge, we cast the problem as a metric learning task that jointly reasons on multiple cues across time. Our framework learns to encode long-term temporal dependencies across multiple cues with a hierarchical Recurrent Neural Network. We demonstrate the strength of our approach by tracking multiple objects using their appearance, motion, and interactions. Our method outperforms previous works by a large margin on multiple publicly available datasets including the challenging MOT benchmark. version:1
arxiv-1701-01908 | Sentence-level dialects identification in the greater China region | http://arxiv.org/abs/1701.01908 | id:1701.01908 author:Fan Xu, Mingwen Wang, Maoxi Li category:cs.CL  published:2017-01-08 summary:Identifying the different varieties of the same language is more challenging than unrelated languages identification. In this paper, we propose an approach to discriminate language varieties or dialects of Mandarin Chinese for the Mainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore, a.k.a., the Greater China Region (GCR). When applied to the dialects identification of the GCR, we find that the commonly used character-level or word-level uni-gram feature is not very efficient since there exist several specific problems such as the ambiguity and context-dependent characteristic of words in the dialects of the GCR. To overcome these challenges, we use not only the general features like character-level n-gram, but also many new word-level features, including PMI-based and word alignment-based features. A series of evaluation results on both the news and open-domain dataset from Wikipedia show the effectiveness of the proposed approach. version:1
arxiv-1701-01892 | Urban Scene Segmentation with Laser-Constrained CRFs | http://arxiv.org/abs/1701.01892 | id:1701.01892 author:Charika De Alvis, Lionel Ott, Fabio Ramos category:cs.CV  published:2017-01-07 summary:Robots typically possess sensors of different modalities, such as colour cameras, inertial measurement units, and 3D laser scanners. Often, solving a particular problem becomes easier when more than one modality is used. However, while there are undeniable benefits to combine sensors of different modalities the process tends to be complicated. Segmenting scenes observed by the robot into a discrete set of classes is a central requirement for autonomy as understanding the scene is the first step to reason about future situations. Scene segmentation is commonly performed using either image data or 3D point cloud data. In computer vision many successful methods for scene segmentation are based on conditional random fields (CRF) where the maximum a posteriori (MAP) solution to the segmentation can be obtained by inference. In this paper we devise a new CRF inference method for scene segmentation that incorporates global constraints, enforcing the sets of nodes are assigned the same class label. To do this efficiently, the CRF is formulated as a relaxed quadratic program whose MAP solution is found using a gradient-based optimisation approach. The proposed method is evaluated on images and 3D point cloud data gathered in urban environments where image data provides the appearance features needed by the CRF, while the 3D point cloud data provides global spatial constraints over sets of nodes. Comparisons with belief propagation, conventional quadratic programming relaxation, and higher order potential CRF show the benefits of the proposed method. version:1
arxiv-1701-01887 | Deep Learning for Time-Series Analysis | http://arxiv.org/abs/1701.01887 | id:1701.01887 author:John Cristian Borges Gamboa category:cs.LG  published:2017-01-07 summary:In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field. version:1
arxiv-1701-01885 | Group Visual Sentiment Analysis | http://arxiv.org/abs/1701.01885 | id:1701.01885 author:Zeshan Hussain, Tariq Patanam, Hardie Cate category:cs.CV  published:2017-01-07 summary:In this paper, we introduce a framework for classifying images according to high-level sentiment. We subdivide the task into three primary problems: emotion classification on faces, human pose estimation, and 3D estimation and clustering of groups of people. We introduce novel algorithms for matching body parts to a common individual and clustering people in images based on physical location and orientation. Our results outperform several baseline approaches. version:1
arxiv-1701-01879 | Greedy Search for Descriptive Spatial Face Features | http://arxiv.org/abs/1701.01879 | id:1701.01879 author:Caner Gacav, Burak Benligiray, Cihan Topal category:cs.CV  published:2017-01-07 summary:Facial expression recognition methods use a combination of geometric and appearance-based features. Spatial features are derived from displacements of facial landmarks, and carry geometric information. These features are either selected based on prior knowledge, or dimension-reduced from a large pool. In this study, we produce a large number of potential spatial features using two combinations of facial landmarks. Among these, we search for a descriptive subset of features using sequential forward selection. The chosen feature subset is used to classify facial expressions in the extended Cohn-Kanade dataset (CK+), and delivered 88.7% recognition accuracy without using any appearance-based features. version:1
arxiv-1701-01876 | DeepFace: Face Generation using Deep Learning | http://arxiv.org/abs/1701.01876 | id:1701.01876 author:Hardie Cate, Fahim Dalvi, Zeshan Hussain category:cs.CV  published:2017-01-07 summary:We use CNNs to build a system that both classifies images of faces based on a variety of different facial attributes and generates new faces given a set of desired facial characteristics. After introducing the problem and providing context in the first section, we discuss recent work related to image generation in Section 2. In Section 3, we describe the methods used to fine-tune our CNN and generate new images using a novel approach inspired by a Gaussian mixture model. In Section 4, we discuss our working dataset and describe our preprocessing steps and handling of facial attributes. Finally, in Sections 5, 6 and 7, we explain our experiments and results and conclude in the following section. Our classification system has 82\% test accuracy. Furthermore, our generation pipeline successfully creates well-formed faces. version:1
arxiv-1701-01875 | Sign Language Recognition Using Temporal Classification | http://arxiv.org/abs/1701.01875 | id:1701.01875 author:Hardie Cate, Fahim Dalvi, Zeshan Hussain category:cs.CV  published:2017-01-07 summary:Devices like the Myo armband available in the market today enable us to collect data about the position of a user's hands and fingers over time. We can use these technologies for sign language translation since each sign is roughly a combination of gestures across time. In this work, we utilize a dataset collected by a group at the University of South Wales, which contains parameters, such as hand position, hand rotation, and finger bend, for 95 unique signs. For each input stream representing a sign, we predict which sign class this stream falls into. We begin by implementing baseline SVM and logistic regression models, which perform reasonably well on high quality data. Lower quality data requires a more sophisticated approach, so we explore different methods in temporal classification, including long short term memory architectures and sequential pattern mining methods. version:1
arxiv-1701-01854 | Neural Machine Translation on Scarce-Resource Condition: A case-study on Persian-English | http://arxiv.org/abs/1701.01854 | id:1701.01854 author:Mohaddeseh Bastan, Shahram Khadivi, Mohammad Mehdi Homayounpour category:cs.CL  published:2017-01-07 summary:Neural Machine Translation (NMT) is a new approach for Machine Translation (MT), and due to its success, it has absorbed the attention of many researchers in the field. In this paper, we study NMT model on Persian-English language pairs, to analyze the model and investigate the appropriateness of the model for scarce-resourced scenarios, the situation that exists for Persian-centered translation systems. We adjust the model for the Persian language and find the best parameters and hyper parameters for two tasks: translation and transliteration. We also apply some preprocessing task on the Persian dataset which yields to increase for about one point in terms of BLEU score. Also, we have modified the loss function to enhance the word alignment of the model. This new loss function yields a total of 1.87 point improvements in terms of BLEU score in the translation quality. version:1
arxiv-1701-01833 | Oriented Response Networks | http://arxiv.org/abs/1701.01833 | id:1701.01833 author:Yanzhao Zhou, Qixiang Ye, Qiang Qiu, Jianbin Jiao category:cs.CV  published:2017-01-07 summary:Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks. version:1
arxiv-1701-01814 | Large-scale Isolated Gesture Recognition Using Convolutional Neural Networks | http://arxiv.org/abs/1701.01814 | id:1701.01814 author:Pichao Wang, Wanqing Li, Song Liu, Zhimin Gao, Chang Tang, Philip Ogunbona category:cs.CV  published:2017-01-07 summary:This paper proposes three simple, compact yet effective representations of depth sequences, referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images (DDMNI). These dynamic images are constructed from a sequence of depth maps using bidirectional rank pooling to effectively capture the spatial-temporal information. Such image-based representations enable us to fine-tune the existing ConvNets models trained on image data for classification of depth sequences, without introducing large parameters to learn. Upon the proposed representations, a convolutional Neural networks (ConvNets) based method is developed for gesture recognition and evaluated on the Large-scale Isolated Gesture Recognition at the ChaLearn Looking at People (LAP) challenge 2016. The method achieved 55.57\% classification accuracy and ranked $2^{nd}$ place in this challenge but was very close to the best performance even though we only used depth data. version:1
arxiv-1701-01811 | Structural Attention Neural Networks for improved sentiment analysis | http://arxiv.org/abs/1701.01811 | id:1701.01811 author:Filippos Kokkinos, Alexandros Potamianos category:cs.CL cs.NE  published:2017-01-07 summary:We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset. version:1
arxiv-1701-01791 | Classification Accuracy Improvement for Neuromorphic Computing Systems with One-level Precision Synapses | http://arxiv.org/abs/1701.01791 | id:1701.01791 author:Yandan Wang, Wei Wen, Linghao Song, Hai Li category:cs.NE I.2.6  I.5.1  published:2017-01-07 summary:Brain inspired neuromorphic computing has demonstrated remarkable advantages over traditional von Neumann architecture for its high energy efficiency and parallel data processing. However, the limited resolution of synaptic weights degrades system accuracy and thus impedes the use of neuromorphic systems. In this work, we propose three orthogonal methods to learn synapses with one-level precision, namely, distribution-aware quantization, quantization regularization and bias tuning, to make image classification accuracy comparable to the state-of-the-art. Experiments on both multi-layer perception and convolutional neural networks show that the accuracy drop can be well controlled within 0.19% (5.53%) for MNIST (CIFAR-10) database, compared to an ideal system without quantization. version:1
arxiv-1701-02804 | Similarity Function Tracking using Pairwise Comparisons | http://arxiv.org/abs/1701.02804 | id:1701.02804 author:Kristjan Greenewald, Stephen Kelley, Brandon Oselio, Alfred O. Hero III category:stat.ML cs.LG  published:2017-01-07 summary:Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retro-initialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, and demonstrate parameter-free RICE-OCELAD metric learning on both synthetic data and a highly nonstationary Twitter dataset. We show significant performance improvements and increased robustness to nonstationary effects relative to previously proposed batch and online distance metric learning algorithms. version:1
arxiv-1701-01779 | Towards Accurate Multi-person Pose Estimation in the Wild | http://arxiv.org/abs/1701.01779 | id:1701.01779 author:George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Kevin Murphy category:cs.CV  published:2017-01-06 summary:We propose a method for multi-person detection and 2-D keypoint localization (human pose estimation) that achieves state-of-the-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector with an Inception-ResNet architecture. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Our final system achieves average precision of 0.636 on the COCO test-dev set and the 0.628 test-standard sets, outperforming the CMU-Pose winner of the 2016 COCO keypoints challenge. Further, by using additional labeled data we obtain an even higher average precision of 0.668 on the test-dev set and 0.658 on the test-standard set, thus achieving a roughly 10% improvement over the previous best performing method on the same challenge. version:1
arxiv-1701-01772 | Estimation of Graphlet Statistics | http://arxiv.org/abs/1701.01772 | id:1701.01772 author:Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed category:cs.SI cs.DC math.CO stat.ML  published:2017-01-06 summary:Graphlets are induced subgraphs of a large network and are important for understanding and modeling complex networks. Despite their practical importance, graphlets have been severely limited to applications and domains with relatively small graphs. Most previous work has focused on exact algorithms, however, it is often too expensive to compute graphlets exactly in massive networks with billions of edges, and finding an approximate count is usually sufficient for many applications. In this work, we propose an unbiased graphlet estimation framework that is (a) fast with significant speedups compared to the state-of-the-art, (b) parallel with nearly linear-speedups, (c) accurate with <1% relative error, (d) scalable and space-efficient for massive networks with billions of edges, and (e) flexible for a variety of real-world settings, as well as estimating macro and micro-level graphlet statistics (e.g., counts) of both connected and disconnected graphlets. In addition, an adaptive approach is introduced that finds the smallest sample size required to obtain estimates within a given user-defined error bound. On 300 networks from 20 domains, we obtain <1% relative error for all graphlets. This is significantly more accurate than existing methods while using less data. Moreover, it takes a few seconds on billion edge graphs (as opposed to days/weeks). These are by far the largest graphlet computations to date. version:1
arxiv-1701-01745 | Map-guided Hyperspectral Image Superpixel Segmentation Using Proportion Maps | http://arxiv.org/abs/1701.01745 | id:1701.01745 author:Hao Sun, Alina Zare category:cs.CV  published:2017-01-06 summary:A map-guided superpixel segmentation method for hyperspectral imagery is developed and introduced. The proposed approach develops a hyperspectral-appropriate version of the SLIC superpixel segmentation algorithm, leverages map information to guide segmentation, and incorporates the semi-supervised Partial Membership Latent Dirichlet Allocation (sPM-LDA) to obtain a final superpixel segmentation. The proposed method is applied to two real hyperspectral data sets and quantitative cluster validity metrics indicate that the proposed approach outperforms existing hyperspectral superpixel segmentation methods. version:1
arxiv-1701-01722 | Follow the Compressed Leader: Faster Algorithms for Matrix Multiplicative Weight Updates | http://arxiv.org/abs/1701.01722 | id:1701.01722 author:Zeyuan Allen-Zhu, Yuanzhi Li category:cs.LG cs.DS math.OC stat.ML  published:2017-01-06 summary:Matrix multiplicative weight update (MMWU) is an extremely powerful algorithmic tool for computer science and related fields. However, it comes with a slow running time due to the matrix exponential and eigendecomposition computations. For this reason, many researchers studied the followed-the-perturbed-leader (FTPL) framework which is faster, but a factor $\sqrt{d}$ worse than the optimal regret of MMWU for dimension-$d$ matrices. In this paper, we propose a $\textit{followed-the-compressed-leader}$ framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs $\textit{even faster}$ than FTPL. Our main idea is to "compress" the matrix exponential computation to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem. version:1
arxiv-1701-00145 | Expanding Subjective Lexicons for Social Media Mining with Embedding Subspaces | http://arxiv.org/abs/1701.00145 | id:1701.00145 author:Silvio Amir, Rámon Astudillo, Wang Ling, Paula C. Carvalho, Mário J. Silva category:cs.CL  published:2016-12-31 summary:Recent approaches for sentiment lexicon induction have capitalized on pre-trained word embeddings that capture latent semantic properties. However, embeddings obtained by optimizing performance of a given task (e.g. predicting contextual words) are sub-optimal for other applications. In this paper, we address this problem by exploiting task-specific representations, induced via embedding sub-space projection. This allows us to expand lexicons describing multiple semantic properties. For each property, our model jointly learns suitable representations and the concomitant predictor. Experiments conducted over multiple subjective lexicons, show that our model outperforms previous work and other baselines; even in low training data regimes. Furthermore, lexicon-based sentiment classifiers built on top of our lexicons outperform similar resources and yield performances comparable to those of supervised models. version:2
arxiv-1701-01698 | Deep Class Aware Denoising | http://arxiv.org/abs/1701.01698 | id:1701.01698 author:Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein category:cs.CV  published:2017-01-06 summary:The increasing demand for high image quality in mobile devices brings forth the need for better computational enhancement techniques, and image denoising in particular. At the same time, the images captured by these devices can be categorized into a small set of semantic classes. However simple, this observation has not been exploited in image denoising until now. In this paper, we demonstrate how the reconstruction quality improves when a denoiser is aware of the type of content in the image. To this end, we first propose a new fully convolutional deep neural network architecture which is simple yet powerful as it achieves state-of-the-art performance even without being class-aware. We further show that a significant boost in performance of up to $0.4$ dB PSNR can be achieved by making our network class-aware, namely, by fine-tuning it for images belonging to a specific semantic class. Relying on the hugely successful existing image classifiers, this research advocates for using a class-aware approach in all image enhancement tasks. version:1
arxiv-1701-01692 | To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection | http://arxiv.org/abs/1701.01692 | id:1701.01692 author:Eshed Ohn-Bar, Mohan M. Trivedi category:cs.CV  published:2017-01-06 summary:We aim to study the modeling limitations of the commonly employed boosted decision trees classifier. Inspired by the success of large, data-hungry visual recognition models (e.g. deep convolutional neural networks), this paper focuses on the relationship between modeling capacity of the weak learners, dataset size, and dataset properties. A set of novel experiments on the Caltech Pedestrian Detection benchmark results in the best known performance among non-CNN techniques while operating at fast run-time speed. Furthermore, the performance is on par with deep architectures (9.71% log-average miss rate), while using only HOG+LUV channels as features. The conclusions from this study are shown to generalize over different object detection domains as demonstrated on the FDDB face detection benchmark (93.37% accuracy). Despite the impressive performance, this study reveals the limited modeling capacity of the common boosted trees model, motivating a need for architectural changes in order to compete with multi-level and very deep architectures. version:1
arxiv-1701-01687 | Deep Convolutional Denoising of Low-Light Images | http://arxiv.org/abs/1701.01687 | id:1701.01687 author:Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein category:cs.CV  published:2017-01-06 summary:Poisson distribution is used for modeling noise in photon-limited imaging. While canonical examples include relatively exotic types of sensing like spectral imaging or astronomy, the problem is relevant to regular photography now more than ever due to the booming market for mobile cameras. Restricted form factor limits the amount of absorbed light, thus computational post-processing is called for. In this paper, we make use of the powerful framework of deep convolutional neural networks for Poisson denoising. We demonstrate how by training the same network with images having a specific peak value, our denoiser outperforms previous state-of-the-art by a large margin both visually and quantitatively. Being flexible and data-driven, our solution resolves the heavy ad hoc engineering used in previous methods and is an order of magnitude faster. We further show that by adding a reasonable prior on the class of the image being processed, another significant boost in performance is achieved. version:1
arxiv-1701-01437 | NIPS 2016 Workshop on Representation Learning in Artificial and Biological Neural Networks (MLINI 2016) | http://arxiv.org/abs/1701.01437 | id:1701.01437 author:Leila Wehbe, Anwar Nunez-Elizalde, Marcel van Gerven, Irina Rish, Brian Murphy, Moritz Grosse-Wentrup, Georg Langs, Guillermo Cecchi category:stat.ML  published:2017-01-06 summary:This workshop explores the interface between cognitive neuroscience and recent advances in AI fields that aim to reproduce human performance such as natural language processing and computer vision, and specifically deep learning approaches to such problems. When studying the cognitive capabilities of the brain, scientists follow a system identification approach in which they present different stimuli to the subjects and try to model the response that different brain areas have of that stimulus. The goal is to understand the brain by trying to find the function that expresses the activity of brain areas in terms of different properties of the stimulus. Experimental stimuli are becoming increasingly complex with more and more people being interested in studying real life phenomena such as the perception of natural images or natural sentences. There is therefore a need for a rich and adequate vector representation of the properties of the stimulus, that we can obtain using advances in machine learning. In parallel, new ML approaches, many of which in deep learning, are inspired to a certain extent by human behavior or biological principles. Neural networks for example were originally inspired by biological neurons. More recently, processes such as attention are being used which have are inspired by human behavior. However, the large bulk of these methods are independent of findings about brain function, and it is unclear whether it is at all beneficial for machine learning to try to emulate brain function in order to achieve the same tasks that the brain achieves. version:1
arxiv-1701-01623 | Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages | http://arxiv.org/abs/1701.01623 | id:1701.01623 author:Michael Sejr Schlichtkrull, Anders Søgaard category:cs.CL  published:2017-01-06 summary:In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art. version:1
arxiv-1701-01619 | Learning From Noisy Large-Scale Datasets With Minimal Supervision | http://arxiv.org/abs/1701.01619 | id:1701.01619 author:Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, Serge Belongie category:cs.CV  published:2017-01-06 summary:We present an approach to effectively use millions of images with noisy annotations in conjunction with a small subset of cleanly-annotated images to learn powerful image representations. One common approach to combine clean and noisy data is to first pre-train a network using the large noisy dataset and then fine-tune with the clean dataset. We show this approach does not fully leverage the information contained in the clean set. Thus, we demonstrate how to use the clean annotations to reduce the noise in the large dataset before fine-tuning the network using both the clean set and the full set with reduced noise. The approach comprises a multi-task network that jointly learns to clean noisy annotations and to accurately classify images. We evaluate our approach on the recently released Open Images dataset, containing ~9 million images, multiple annotations per image and over 6000 unique classes. For the small clean set of annotations we use a quarter of the validation set with ~40k images. Our results demonstrate that the proposed approach clearly outperforms direct fine-tuning across all major categories of classes in the Open Image dataset. Further, our approach is particularly effective for a large number of classes with medium level of noise in annotations (20-80% false positive annotations). version:1
arxiv-1701-01614 | Enumeration of Extractive Oracle Summaries | http://arxiv.org/abs/1701.01614 | id:1701.01614 author:Tsutomu Hirao, Masaaki Nishino, Jun Suzuki, Masaaki Nagata category:cs.CL  published:2017-01-06 summary:To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the oracle summaries for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. version:1
arxiv-1701-01574 | Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word Representation | http://arxiv.org/abs/1701.01574 | id:1701.01574 author:Haoyue Shi, Caihua Li, Junfeng Hu category:cs.CL  published:2017-01-06 summary:Previous researches have shown that learning multiple representations for polysemous words can improve the performance of word embeddings on many tasks. However, this leads to another problem. Several vectors of a word may actually point to the same meaning, namely pseudo multi-sense. In this paper, we introduce the concept of pseudo multi-sense, and then propose an algorithm to detect such cases. With the consideration of the detected pseudo multi-sense cases, we try to refine the existing word embeddings to eliminate the influence of pseudo multi-sense. Moreover, we apply our algorithm on previous released multi-sense word embeddings and tested it on artificial word similarity tasks and the analogy task. The result of the experiments shows that diminishing pseudo multi-sense can improve the quality of word representations. Thus, our method is actually an efficient way to reduce linguistic complexity. version:1
arxiv-1701-01565 | Replication issues in syntax-based aspect extraction for opinion mining | http://arxiv.org/abs/1701.01565 | id:1701.01565 author:Edison Marrese-Taylor, Yutaka Matsuo category:cs.CL  published:2017-01-06 summary:Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances. version:1
arxiv-1701-01546 | Abnormal Event Detection in Videos using Spatiotemporal Autoencoder | http://arxiv.org/abs/1701.01546 | id:1701.01546 author:Yong Shean Chong, Yong Haur Tay category:cs.CV  published:2017-01-06 summary:We present an efficient method for detecting anomalies in videos. Recent applications of convolutional neural networks have shown promises of convolutional layers for object detection and recognition, especially in images. However, convolutional neural networks are supervised and require labels as learning signals. We propose a spatiotemporal architecture for anomaly detection in videos including crowded scenes. Our architecture includes two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features. Experimental results on Avenue, Subway and UCSD benchmarks confirm that the detection accuracy of our method is comparable to state-of-the-art methods at a considerable speed of up to 140 fps. version:1
arxiv-1701-01096 | Probabilistic Multigraph Modeling for Improving the Quality of Crowdsourced Affective Data | http://arxiv.org/abs/1701.01096 | id:1701.01096 author:Jianbo Ye, Jia Li, Michelle G. Newman, Reginald B. Adams Jr., James Z. Wang category:stat.ML cs.HC  published:2017-01-04 summary:We proposed a probabilistic approach to joint modeling of participants' reliability and humans' regularity in crowdsourced affective studies. Reliability measures how likely a subject will respond to a question seriously; and regularity measures how often a human will agree with other seriously-entered responses coming from a targeted population. Crowdsourcing-based studies or experiments, which rely on human self-reported affect, pose additional challenges as compared with typical crowdsourcing studies that attempt to acquire concrete non-affective labels of objects. The reliability of participants has been massively pursued for typical non-affective crowdsourcing studies, whereas the regularity of humans in an affective experiment in its own right has not been thoroughly considered. It has been often observed that different individuals exhibit different feelings on the same test question, which does not have a sole correct response in the first place. High reliability of responses from one individual thus cannot conclusively result in high consensus across individuals. Instead, globally testing consensus of a population is of interest to investigators. Built upon the agreement multigraph among tasks and workers, our probabilistic model differentiates subject regularity from population reliability. We demonstrate the method's effectiveness for in-depth robust analysis of large-scale crowdsourced affective data, including emotion and aesthetic assessments collected by presenting visual stimuli to human subjects. version:2
arxiv-1701-01505 | Crime Topic Modeling | http://arxiv.org/abs/1701.01505 | id:1701.01505 author:Da Kuang, P. Jeffrey Brantingham, Andrea L. Bertozzi category:cs.CL  published:2017-01-05 summary:The classification of crime into discrete categories entails a massive loss of information. Crimes emerge out of a complex mix of behaviors and situations, yet most of these details cannot be captured by singular crime type labels. This information loss impacts our ability to not only understand the causes of crime, but also how to develop optimal crime prevention strategies. We apply machine learning methods to short narrative text descriptions accompanying crime records with the goal of discovering ecologically more meaningful latent crime classes. We term these latent classes "crime topics" in reference to text-based topic modeling methods that produce them. We use topic distributions to measure clustering among formally recognized crime types. Crime topics replicate broad distinctions between violent and property crime, but also reveal nuances linked to target characteristics, situational conditions and the tools and methods of attack. Formal crime types are not discrete in topic space. Rather, crime types are distributed across a range of crime topics. Similarly, individual crime topics are distributed across a range of formal crime types. Key ecological groups include identity theft, shoplifting, burglary and theft, car crimes and vandalism, criminal threats and confidence crimes, and violent crimes. Crime topic modeling positions behavioral situations as the focal unit of analysis for crime events. Though unlikely to replace formal legal crime classifications, crime topics provide a unique window into the heterogeneous causal processes underlying crime. We discuss whether automated procedures could be used to cross-check the quality of official crime classifications. version:1
arxiv-1701-01497 | Learning local trajectories for high precision robotic tasks : application to KUKA LBR iiwa Cartesian positioning | http://arxiv.org/abs/1701.01497 | id:1701.01497 author:Joris Guerin, Olivier Gibaru, Eric Nyiri, Stephane Thiery category:cs.AI cs.LG cs.RO  published:2017-01-05 summary:To ease the development of robot learning in industry, two conditions need to be fulfilled. Manipulators must be able to learn high accuracy and precision tasks while being safe for workers in the factory. In this paper, we extend previously submitted work which consists in rapid learning of local high accuracy behaviors. By exploration and regression, linear and quadratic models are learnt for respectively the dynamics and cost function. Iterative Linear Quadratic Gaussian Regulator combined with cost quadratic regression can converge rapidly in the final stages towards high accuracy behavior as the cost function is modelled quite precisely. In this paper, both a different cost function and a second order improvement method are implemented within this framework. We also propose an analysis of the algorithm parameters through simulation for a positioning task. Finally, an experimental validation on a KUKA LBR iiwa robot is carried out. This collaborative robot manipulator can be easily programmed into safety mode, which makes it qualified for the second industry constraint stated above. version:1
arxiv-1701-01495 | Membrane-Dependent Neuromorphic Learning Rule for Unsupervised Spike Pattern Detection | http://arxiv.org/abs/1701.01495 | id:1701.01495 author:Sadique Sheik, Somnath Paul, Charles Augustine, Gert Cauwenberghs category:cs.NE  published:2017-01-05 summary:Several learning rules for synaptic plasticity, that depend on either spike timing or internal state variables, have been proposed in the past imparting varying computational capabilities to Spiking Neural Networks. Due to design complications these learning rules are typically not implemented on neuromorphic devices leaving the devices to be only capable of inference. In this work we propose a unidirectional post-synaptic potential dependent learning rule that is only triggered by pre-synaptic spikes, and easy to implement on hardware. We demonstrate that such a learning rule is functionally capable of replicating computational capabilities of pairwise STDP. Further more, we demonstrate that this learning rule can be used to learn and classify spatio-temporal spike patterns in an unsupervised manner using individual neurons. We argue that this learning rule is computationally powerful and also ideal for hardware implementations due to its unidirectional memory access. version:1
arxiv-1701-01486 | Motion Deblurring in the Wild | http://arxiv.org/abs/1701.01486 | id:1701.01486 author:Mehdi Noroozi, Paramanand Chandramouli, Paolo Favaro category:cs.CV  published:2017-01-05 summary:The task of image deblurring is a very ill-posed problem as both the image and the blur are unknown. Moreover, when pictures are taken in the wild, this task becomes even more challenging due to the blur varying spatially and the occlusions between the object. Due to the complexity of the general image model we propose a novel convolutional network architecture which directly generates the sharp image.This network is built in three stages, and exploits the benefits of pyramid schemes often used in blind deconvolution. One of the main difficulties in training such a network is to design a suitable dataset. While useful data can be obtained by synthetically blurring a collection of images, more realistic data must be collected in the wild. To obtain such data we use a high frame rate video camera and keep one frame as the sharp image and frame average as the corresponding blurred image. We show that this realistic dataset is key in achieving state-of-the-art performance and dealing with occlusions. version:1
arxiv-1701-01480 | Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study | http://arxiv.org/abs/1701.01480 | id:1701.01480 author:Yi-Ling Chen, Tzu-Wei Huang, Kai-Han Chang, Yu-Chen Tsai, Hwann-Tzong Chen, Bing-Yu Chen category:cs.CV  published:2017-01-05 summary:Automatic photo cropping is an important tool for improving visual quality of digital photos without resorting to tedious manual selection. Traditionally, photo cropping is accomplished by determining the best proposal window through visual quality assessment or saliency detection. In essence, the performance of an image cropper highly depends on the ability to correctly rank a number of visually similar proposal windows. Despite the ranking nature of automatic photo cropping, little attention has been paid to learning-to-rank algorithms in tackling such a problem. In this work, we conduct an extensive study on traditional approaches as well as ranking-based croppers trained on various image features. In addition, a new dataset consisting of high quality cropping and pairwise ranking annotations is presented to evaluate the performance of various baselines. The experimental results on the new dataset provide useful insights into the design of better photo cropping algorithms. version:1
arxiv-1701-01470 | Graph Structure Learning from Unlabeled Data for Event Detection | http://arxiv.org/abs/1701.01470 | id:1701.01470 author:Sriram Somanchi, Daniel B. Neill category:stat.ML cs.SI  published:2017-01-05 summary:Processes such as disease propagation and information diffusion often spread over some latent network structure which must be learned from observation. Given a set of unlabeled training examples representing occurrences of an event type of interest (e.g., a disease outbreak), our goal is to learn a graph structure that can be used to accurately detect future events of that type. Motivated by new theoretical results on the consistency of constrained and unconstrained subset scans, we propose a novel framework for learning graph structure from unlabeled data by comparing the most anomalous subsets detected with and without the graph constraints. Our framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure, and efficiently searches for the highest-scoring graph structure. Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County, we show that our method learns a structure similar to the true underlying graph, but enables faster and more accurate detection. version:1
arxiv-1701-01417 | Exploration of Proximity Heuristics in Length Normalization | http://arxiv.org/abs/1701.01417 | id:1701.01417 author:Pranav Agrawal category:cs.IR cs.CL  published:2017-01-05 summary:Ranking functions used in information retrieval are primarily used in the search engines and they are often adopted for various language processing applications. However, features used in the construction of ranking functions should be analyzed before applying it on a data set. This paper gives guidelines on construction of generalized ranking functions with application-dependent features. The paper prescribes a specific case of a generalized function for recommendation system using feature engineering guidelines on the given data set. The behavior of both generalized and specific functions are studied and implemented on the unstructured textual data. The proximity feature based ranking function has outperformed by 52% from regular BM25. version:1
arxiv-1701-01394 | Signed Laplacian for spectral clustering revisited | http://arxiv.org/abs/1701.01394 | id:1701.01394 author:Andrew V. Knyazev category:cs.DS cs.LG math.NA stat.ML H.3.3; I.5.3  published:2017-01-05 summary:Classical spectral clustering is based on a spectral decomposition of a graph Laplacian, obtained from a graph adjacency matrix representing positive graph edge weights describing similarities of graph vertices. In signed graphs, the graph edge weights can be negative to describe disparities of graph vertices, for example, negative correlations in the data. Negative weights lead to possible negative spectrum of the standard graph Laplacian, which is cured by defining a signed Laplacian. We revisit comparing the standard and signed Laplacians and argue that the former is more natural than the latter, also showing that the negative spectrum is actually beneficial, for spectral clustering of signed graphs. version:1
arxiv-1701-01370 | Learning from Synthetic Humans | http://arxiv.org/abs/1701.01370 | id:1701.01370 author:Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael Black, Ivan Laptev, Cordelia Schmid category:cs.CV  published:2017-01-05 summary:Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data. version:1
arxiv-1701-01358 | NeuroRule: A Connectionist Approach to Data Mining | http://arxiv.org/abs/1701.01358 | id:1701.01358 author:Hongjun Lu, Rudy Setiono, Huan Liu category:cs.LG  published:2017-01-05 summary:Classification, which involves finding rules that partition a given data set into disjoint groups, is one class of data mining problems. Approaches proposed so far for mining classification rules for large databases are mainly decision tree based symbolic learning methods. The connectionist approach based on neural networks has been thought not well suited for data mining. One of the major reasons cited is that knowledge generated by neural networks is not explicitly represented in the form of rules suitable for verification or interpretation by humans. This paper examines this issue. With our newly developed algorithms, rules which are similar to, or more concise than those generated by the symbolic methods can be extracted from the neural networks. The data mining process using neural networks with the emphasis on rule extraction is described. Experimental results and comparison with previously published works are presented. version:1
arxiv-1701-01356 | Gaussian Process Quadrature Moment Transform | http://arxiv.org/abs/1701.01356 | id:1701.01356 author:Jakub Prüher, Ondřej Straka category:stat.ME stat.ML  published:2017-01-05 summary:Computation of moments of transformed random variables is a problem appearing in many engineering applications. The current methods for moment transformation are mostly based on the classical quadrature rules which cannot account for the approximation errors. Our aim is to design a method for moment transformation for Gaussian random variables which accounts for the error in the numerically computed mean. We employ an instance of Bayesian quadrature, called Gaussian process quadrature (GPQ), which allows us to treat the integral itself as a random variable, where the integral variance informs about the incurred integration error. Experiments on the coordinate transformation and nonlinear filtering examples show that the proposed GPQ moment transform performs better than the classical transforms. version:1
arxiv-1701-01064 | Optimal Low-Rank Dynamic Mode Decomposition | http://arxiv.org/abs/1701.01064 | id:1701.01064 author:Patrick Héas, Cédric Herzet category:stat.ML  published:2017-01-04 summary:Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of non-linear systems from experimental datasets. Recently, several attempts have extended DMD to the context of low-rank approximations. This extension is of particular interest for reduced-order modeling in various applicative domains, e.g. for climate prediction, to study molecular dynamics or micro-electromechanical devices. This low-rank extension takes the form of a non-convex optimization problem. To the best of our knowledge, only sub-optimal algorithms have been proposed in the literature to compute the solution of this problem. In this paper, we prove that there exists a closed-form optimal solution to this problem and design an effective algorithm to compute it based on Singular Value Decomposition (SVD). A toy-example illustrates the gain in performance of the proposed algorithm compared to state-of-the-art techniques. version:2
arxiv-1701-01329 | Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks | http://arxiv.org/abs/1701.01329 | id:1701.01329 author:Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, Mark P. Waller category:cs.NE cs.AI cs.LG physics.chem-ph stat.ML  published:2017-01-05 summary:In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery. version:1
arxiv-1701-01325 | Outlier Detection for Text Data : An Extended Version | http://arxiv.org/abs/1701.01325 | id:1701.01325 author:Ramakrishnan Kannan, Hyenkyun Woo, Charu C. Aggarwal, Haesun Park category:cs.IR cs.LG stat.ML  published:2017-01-05 summary:The problem of outlier detection is extremely challenging in many domains such as text, in which the attribute values are typically non-negative, and most values are zero. In such cases, it often becomes difficult to separate the outliers from the natural variations in the patterns in the underlying data. In this paper, we present a matrix factorization method, which is naturally able to distinguish the anomalies with the use of low rank approximations of the underlying data. Our iterative algorithm TONMF is based on block coordinate descent (BCD) framework. We define blocks over the term-document matrix such that the function becomes solvable. Given most recently updated values of other matrix blocks, we always update one block at a time to its optimal. Our approach has significant advantages over traditional methods for text outlier detection. Finally, we present experimental results illustrating the effectiveness of our method over competing methods. version:1
arxiv-1701-01293 | OpenML: An R Package to Connect to the Networked Machine Learning Platform OpenML | http://arxiv.org/abs/1701.01293 | id:1701.01293 author:Giuseppe Casalicchio, Jakob Bossek, Michel Lang, Dominik Kirchhoff, Pascal Kerschke, Benjamin Hofner, Heidi Seibold, Joaquin Vanschoren, Bernd Bischl category:stat.ML cs.LG  published:2017-01-05 summary:OpenML is an online machine learning platform where researchers can easily share data, machine learning tasks and experiments as well as organize them online to work and collaborate more efficiently. In this paper, we present an R package to interface with the OpenML platform and illustrate its usage in combination with the machine learning R package mlr. We show how the OpenML package allows R users to easily search, download and upload data sets and machine learning tasks. Furthermore, we also show how to upload results of experiments, share them with others and download results from other users. Beyond ensuring reproducibility of results, the OpenML platform automates much of the drudge work, speeds up research, facilitates collaboration and increases the users' visibility online. version:1
arxiv-1701-01272 | Autoencoder Regularized Network For Driving Style Representation Learning | http://arxiv.org/abs/1701.01272 | id:1701.01272 author:Weishan Dong, Ting Yuan, Kai Yang, Changsheng Li, Shilei Zhang category:cs.CV cs.AI cs.NE  published:2017-01-05 summary:In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the driver identification problem show that ARNet can learn a good generalized driving style representation: It significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods. version:1
arxiv-1701-01271 | Subpopulation Diversity Based Selecting Migration Moment in Distributed Evolutionary Algorithms | http://arxiv.org/abs/1701.01271 | id:1701.01271 author:Chengjun Li, Jia Wu category:cs.NE  published:2017-01-05 summary:In distributed evolutionary algorithms, migration interval is used to decide migration moments. Nevertheless, migration moments predetermined by intervals cannot match the dynamic situation of evolution. In this paper, a scheme of setting the success rate of migration based on subpopulation diversity at each interval is proposed. With the scheme, migration still occurs at intervals, but the probability of immigrants entering the target subpopulation will be determined by the diversity of this subpopulation according to a proposed formula. An analysis shows that the time consumption of our scheme is acceptable. In our experiments, the basement of parallelism is an evolutionary algorithm for the traveling salesman problem. Under different value combinations of parameters for the formula, outcomes for eight benchmark instances of the distributed evolutionary algorithm with the proposed scheme are compared with those of a traditional one, respectively. Results show that the distributed evolutionary algorithm based on our scheme has a significant advantage on solutions especially for high difficulty instances. Moreover, it can be seen that the algorithm with the scheme has the most outstanding performance under three value combinations of above-mentioned parameters for the formula. version:1
arxiv-1701-01231 | Adaptive Questionnaires for Direct Identification of Optimal Product Design | http://arxiv.org/abs/1701.01231 | id:1701.01231 author:Max Yi Ren, Clayton Scott category:stat.ML cs.IR  published:2017-01-05 summary:We consider the problem of identifying the most profitable product design from a finite set of candidates under unknown consumer preference. A standard approach to this problem follows a two-step strategy: First, estimate the preference of the consumer population, represented as a point in part-worth space, using an adaptive discrete-choice questionnaire. Second, integrate the estimated part-worth vector with engineering feasibility and cost models to determine the optimal design. In this work, we (1) demonstrate that accurate preference estimation is neither necessary nor sufficient for identifying the optimal design, (2) introduce a novel adaptive questionnaire that leverages knowledge about engineering feasibility and manufacturing costs to directly determine the optimal design, and (3) interpret product design in terms of a nonlinear segmentation of part-worth space, and use this interpretation to illuminate the intrinsic difficulty of optimal design in the presence of noisy questionnaire responses. We establish the superiority of the proposed approach using a well-documented optimal product design task. This study demonstrates how the identification of optimal product design can be accelerated by integrating marketing and manufacturing knowledge into the adaptive questionnaire. version:1
arxiv-1701-01218 | Overlapping Cover Local Regression Machines | http://arxiv.org/abs/1701.01218 | id:1701.01218 author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.LG cs.CV  published:2017-01-05 summary:We present the Overlapping Domain Cover (ODC) notion for kernel machines, as a set of overlapping subsets of the data that covers the entire training set and optimized to be spatially cohesive as possible. We show how this notion benefit the speed of local kernel machines for regression in terms of both speed while achieving while minimizing the prediction error. We propose an efficient ODC framework, which is applicable to various regression models and in particular reduces the complexity of Twin Gaussian Processes (TGP) regression from cubic to quadratic. Our notion is also applicable to several kernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as shown in our experiments). We also theoretically justified the idea behind our method to improve local prediction by the overlapping cover. We validated and analyzed our method on three benchmark human pose estimation datasets and interesting findings are discussed. version:1
arxiv-1701-01214 | A Review of Neural Network Based Machine Learning Approaches for Rotor Angle Stability Control | http://arxiv.org/abs/1701.01214 | id:1701.01214 author:Reza Yousefian, Sukumar Kamalasadan category:cs.SY cs.NE  published:2017-01-05 summary:This paper reviews the current status and challenges of Neural Networks (NNs) based machine learning approaches for modern power grid stability control including their design and implementation methodologies. NNs are widely accepted as Artificial Intelligence (AI) approaches offering an alternative way to control complex and ill-defined problems. In this paper various application of NNs for power system rotor angle stabilization and control problem is discussed. The main focus of this paper is on the use of Reinforcement Learning (RL) and Supervised Learning (SL) algorithms in power system wide-area control (WAC). Generally, these algorithms due to their capability in modeling nonlinearities and uncertainties are used for transient classification, neuro-control, wide-area monitoring and control, renewable energy management and control, and so on. The works of researchers in the field of conventional and renewable energy systems are reported and categorized. Paper concludes by presenting, comparing and evaluating various learning techniques and infrastructure configurations based on efficiency. version:1
arxiv-1701-01207 | A Matrix Factorization Approach for Learning Semidefinite-Representable Regularizers | http://arxiv.org/abs/1701.01207 | id:1701.01207 author:Yong Sheng Soh, Venkat Chandrasekaran category:math.OC cs.IT math.IT stat.ML  published:2017-01-05 summary:Regularization techniques are widely employed in optimization-based approaches for solving ill-posed inverse problems in data analysis and scientific computing. These methods are based on augmenting the objective with a penalty function, which is specified based on prior domain-specific expertise to induce a desired structure in the solution. We consider the problem of learning suitable regularization functions from data in settings in which precise domain knowledge is not directly available. Previous work under the title of `dictionary learning' or `sparse coding' may be viewed as learning a regularization function that can be computed via linear programming. We describe generalizations of these methods to learn regularizers that can be computed and optimized via semidefinite programming. Our framework for learning such semidefinite regularizers is based on obtaining structured factorizations of data matrices, and our algorithmic approach for computing these factorizations combines recent techniques for rank minimization problems along with an operator analog of Sinkhorn scaling. Under suitable conditions on the input data, our algorithm provides a locally linearly convergent method for identifying the correct regularizer that promotes the type of structure contained in the data. Our analysis is based on the stability properties of Operator Sinkhorn scaling and their relation to geometric aspects of determinantal varieties (in particular tangent spaces with respect to these varieties). The regularizers obtained using our framework can be employed effectively in semidefinite programming relaxations for solving inverse problems. version:1
arxiv-1701-01140 | Learning causal effects from many randomized experiments using regularized instrumental variables | http://arxiv.org/abs/1701.01140 | id:1701.01140 author:Alexander Peysakhovich, Dean Eckles category:stat.ML stat.AP  published:2017-01-04 summary:We study how to meta-analyze a large collection of randomized experiments (eg. those done during routine improvements of an online service) to learn general causal relationships. We focus on the case where the number of tests is large, the analyst has no metadata about the context of the test and only has access to summary statistics (and not the raw data). We apply instrumental variable analysis in the form of two stage least squares regression (TSLS). We show that a form of L0 regularization in the first stage can help improve learning by reducing bias in some situations. This is even true in some situations where standard tests (eg. first stage F > 10) would suggest that two stage least squares is hopeless. We propose a cross-validation procedure to set the regularization parameter. version:1
arxiv-1701-01126 | Textual Entailment with Structured Attentions and Composition | http://arxiv.org/abs/1701.01126 | id:1701.01126 author:Kai Zhao, Liang Huang, Mingbo Ma category:cs.CL  published:2017-01-04 summary:Deep learning techniques are increasingly popular in the textual entailment task, overcoming the fragility of traditional discrete models with hard alignments and logics. In particular, the recently proposed attention models (Rockt\"aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art accuracy by computing soft word alignments between the premise and hypothesis sentences. However, there remains a major limitation: this line of work completely ignores syntax and recursion, which is helpful in many traditional efforts. We show that it is beneficial to extend the attention model to tree nodes between premise and hypothesis. More importantly, this subtree-level attention reveals information about entailment relation. We study the recursive composition of this subtree-level entailment relation, which can be viewed as a soft version of the Natural Logic framework (MacCartney and Manning, 2009). Experiments show that our structured attention and entailment composition model can correctly identify and infer entailment relations from the bottom up, and bring significant improvements in accuracy. version:1
arxiv-1701-00008 | Deep Neural Networks to Enable Real-time Multimessenger Astrophysics | http://arxiv.org/abs/1701.00008 | id:1701.00008 author:Daniel George, E. A. Huerta category:astro-ph.IM astro-ph.GA astro-ph.HE cs.LG gr-qc  published:2016-12-30 summary:We introduce a new method for time-domain signal processing, based on deep learning neural networks, which has the potential to revolutionize data analysis in engineering and science. To demonstrate how this enables real-time multimessenger astrophysics, we designed two deep convolutional neural networks that can analyze time-series data from observatories including advanced LIGO. The first neural network recognizes the presence of gravitational waves from binary black hole mergers, and the second one estimates the mass of each black hole, given weak signals hidden in extremely noisy time-series inputs. We highlight the advantages offered by this novel method, which outperforms matched-filtering or conventional machine learning techniques, and propose strategies to extend our implementation for simultaneously targeting different classes of gravitational wave sources while ignoring anomalous noise transients. Our results strongly indicate that deep neural networks are highly efficient and versatile tools for directly processing any raw noisy data streams. We also pioneer a new paradigm to accelerate scientific discovery by combining high-performance simulations on traditional supercomputers and artificial intelligence algorithms that exploit innovative hardware architectures such as deep-learning-optimized GPUs. This unique approach immediately provides a natural framework to unify multi-spectrum observations in real-time thus enabling coincident detection campaigns of gravitational waves sources and their electromagnetic counterparts. version:2
arxiv-1701-01095 | Estimating Quality in User-Guided Multi-Objective Bandits Optimization | http://arxiv.org/abs/1701.01095 | id:1701.01095 author:Audrey Durand, Christian Gagné category:cs.LG stat.ML  published:2017-01-04 summary:Many real-world applications are characterized by a number of conflicting performance measures. As optimizing in a multi-objective setting leads to a set of non-dominated solutions, a preference function is required for selecting the solution with the appropriate trade-off between the objectives. This preference function is often unknown, especially when it comes from an expert human user. However, if we could provide the expert user with a proper estimation for each action, she would be able to pick her best choice. The question is: how good do these estimations have to be in order for her choice to remain the same as if she had access to the exact values? In this paper, we introduce the concept of preference radius to characterize the robustness of the preference function and provide guidelines for controlling the quality of estimations in the multi-objective setting. More specifically, we provide a general formulation of multi-objective optimization under the bandits setting and the pure exploration setting with user feedback for articulating the preferences. We show how the preference radius relates to the optimal gap and how it can be used to analyze algorithms in the bandits and pure exploration settings. We finally present experiments in the bandits setting, where we evaluate the impact of noise and delayed expert user feedback, and in the pure exploration setting, where we compare multi-objective Thompson sampling with uniform sampling. version:1
arxiv-1701-01093 | Private Incremental Regression | http://arxiv.org/abs/1701.01093 | id:1701.01093 author:Shiva Prasad Kasiviswanathan, Kobbi Nissim, Hongxia Jin category:cs.DS cs.CR stat.ML  published:2017-01-04 summary:Data is continuously generated by modern data sources, and a recent challenge in machine learning has been to develop techniques that perform well in an incremental (streaming) setting. In this paper, we investigate the problem of private machine learning, where as common in practice, the data is not given at once, but rather arrives incrementally over time. We introduce the problems of private incremental ERM and private incremental regression where the general goal is to always maintain a good empirical risk minimizer for the history observed under differential privacy. Our first contribution is a generic transformation of private batch ERM mechanisms into private incremental ERM mechanisms, based on a simple idea of invoking the private batch ERM procedure at some regular time intervals. We take this construction as a baseline for comparison. We then provide two mechanisms for the private incremental regression problem. Our first mechanism is based on privately constructing a noisy incremental gradient function, which is then used in a modified projected gradient procedure at every timestep. This mechanism has an excess empirical risk of $\approx\sqrt{d}$, where $d$ is the dimensionality of the data. While from the results of [Bassily et al. 2014] this bound is tight in the worst-case, we show that certain geometric properties of the input and constraint set can be used to derive significantly better results for certain interesting regression problems. version:1
arxiv-1701-01077 | Transforming Sensor Data to the Image Domain for Deep Learning - an Application to Footstep Detection | http://arxiv.org/abs/1701.01077 | id:1701.01077 author:Monit Shah Singh, Vinaychandran Pondenkandath, Bo Zhou, Paul Lukowicz, Marcus Liwicki category:cs.CV  published:2017-01-04 summary:Convolutional Neural Networks (CNNs) have become the state-of-the-art in various computer vision tasks, but they are still premature for most sensor data, especially in pervasive and wearable computing. A major reason for this is the limited amount of annotated training data. In this paper, we propose the idea of leveraging the discriminative power of pre-trained deep CNNs on 2-dimensional sensor data by transforming the sensor modality to the visual domain. By three proposed strategies, 2D sensor output is converted into pressure distribution imageries. Then we utilize a pre-trained CNN for transfer learning on the converted imagery data. We evaluate our method on a gait dataset of floor surface pressure mapping. We obtain a classification accuracy of 87.66%, which outperforms the conventional machine learning methods by over 10%. version:1
arxiv-1701-01037 | Tensor-on-tensor regression | http://arxiv.org/abs/1701.01037 | id:1701.01037 author:Eric F. Lock category:stat.ME stat.ML  published:2017-01-04 summary:We propose a framework for the linear prediction of a multi-way array (i.e., a tensor) from another multi-way array of arbitrary dimension, using the contracted tensor product. This framework generalizes several existing approaches, including methods to predict a scalar outcome from a tensor, a matrix from a matrix, or a tensor from a scalar. We describe an approach that exploits the multiway structure of both the predictors and the outcomes by restricting the coefficients to have reduced CP-rank. We propose a general and efficient algorithm for penalized least-squares estimation, which allows for a ridge (L_2) penalty on the coefficients. The objective is shown to give the mode of a Bayesian posterior, which motivates a Gibbs sampling algorithm for inference. We illustrate the approach with an application to facial image data. An R package is available at https://github.com/lockEF/MultiwayRegression . version:1
arxiv-1701-01036 | Demystifying Neural Style Transfer | http://arxiv.org/abs/1701.01036 | id:1701.01036 author:Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou category:cs.CV cs.LG cs.NE  published:2017-01-04 summary:Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches. version:1
arxiv-1701-01035 | Path-following based Point Matching using Similarity Transformation | http://arxiv.org/abs/1701.01035 | id:1701.01035 author:Wei Lian category:cs.CV  published:2017-01-04 summary:To address the problem of 3D point matching where the poses of two point sets are unknown, we adapt a recently proposed path following based method to use similarity transformation instead of the original affine transformation. The reduced number of transformation parameters leads to more constrained and desirable matching results. Experimental results demonstrate better robustness of the proposed method over state-of-the-art methods. version:1
arxiv-1701-00485 | Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices | http://arxiv.org/abs/1701.00485 | id:1701.00485 author:Wenjia Meng, Zonghua Gu, Ming Zhang, Zhaohui Wu category:cs.LG cs.CV  published:2017-01-02 summary:With the rapid proliferation of Internet of Things and intelligent edge devices, there is an increasing need for implementing machine learning algorithms, including deep learning, on resource-constrained mobile embedded devices with limited memory and computation power. Typical large Convolutional Neural Networks (CNNs) need large amounts of memory and computational power, and cannot be deployed on embedded devices efficiently. We present Two-Bit Networks (TBNs) for model compression of CNNs with edge weights constrained to (-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the memory usage and improve computational efficiency significantly while achieving good performance in terms of classification accuracy, thus representing a reasonable tradeoff between model size and performance. version:2
arxiv-1701-01000 | Joint Optimization Projection Matrix and Sparsifying Dictionary via Stochastic Method | http://arxiv.org/abs/1701.01000 | id:1701.01000 author:Tao Hong category:cs.LG  published:2017-01-04 summary:An efficient algorithm is proposed in this letter to optimize the Projection Matrix and Sparsifying Dictionary (PMSD) simultaneously on a large training dataset through stochastic method. A closed-from solution is derived for optimizing the projection matrix with a fixed sparsifying dictionary and the stochastic method is used to optimize the sparsifying dictionary with a fixed optimized projection matrix on a large training dataset. Benefiting from training on a large dataset, the proposed method yields a much better performance in terms of signal recovery accuracy than the existing ones. The simulation results on natural images demonstrate its effectiveness and efficiency. version:1
arxiv-1701-00995 | An Evaluation Framework and Database for MoCap-Based Gait Recognition Methods | http://arxiv.org/abs/1701.00995 | id:1701.00995 author:Michal Balazia, Petr Sojka category:cs.CV 68T05  68T10 I.5  published:2017-01-04 summary:As a contribution to reproducible research, this paper presents a framework and a database to improve the development, evaluation and comparison of methods for gait recognition from motion capture (MoCap) data. The evaluation framework provides implementation details and source codes of state-of-the-art human-interpretable geometric features as well as our own approaches where gait features are learned by a modification of Fisher's Linear Discriminant Analysis with the Maximum Margin Criterion, and by a combination of Principal Component Analysis and Linear Discriminant Analysis. It includes a description and source codes of a mechanism for evaluating four class separability coefficients of feature space and four rank-based classifier performance metrics. This framework also contains a tool for learning a custom classifier and for classifying a custom query on a custom gallery. We provide an experimental database along with source codes for its extraction from the general CMU MoCap database. version:1
arxiv-1701-00991 | World Literature According to Wikipedia: Introduction to a DBpedia-Based Framework | http://arxiv.org/abs/1701.00991 | id:1701.00991 author:Christoph Hube, Frank Fischer, Robert Jäschke, Gerhard Lauer, Mads Rosendahl Thomsen category:cs.IR cs.CL  published:2017-01-04 summary:Among the manifold takes on world literature, it is our goal to contribute to the discussion from a digital point of view by analyzing the representation of world literature in Wikipedia with its millions of articles in hundreds of languages. As a preliminary, we introduce and compare three different approaches to identify writers on Wikipedia using data from DBpedia, a community project with the goal of extracting and providing structured information from Wikipedia. Equipped with our basic set of writers, we analyze how they are represented throughout the 15 biggest Wikipedia language versions. We combine intrinsic measures (mostly examining the connectedness of articles) with extrinsic ones (analyzing how often articles are frequented by readers) and develop methods to evaluate our results. The better part of our findings seems to convey a rather conservative, old-fashioned version of world literature, but a version derived from reproducible facts revealing an implicit literary canon based on the editing and reading behavior of millions of people. While still having to solve some known issues, the introduced methods will help us build an observatory of world literature to further investigate its representativeness and biases. version:1
arxiv-1701-00951 | A Concave Optimization Algorithm for Matching Partially Overlapping Point Sets | http://arxiv.org/abs/1701.00951 | id:1701.00951 author:Wei Lian, Lei Zhang category:cs.CV  published:2017-01-04 summary:Point matching refers to the process of finding spatial transformation and correspondences between two sets of points. In this paper, we focus on the case that there is only partial overlap between two point sets. Following the approach of the robust point matching method, we model point matching as a mixed linear assignment-least square problem and show that after eliminating the transformation variable, the resulting problem of minimization with respect to point correspondence is a concave optimization problem. Furthermore, this problem has the property that the objective function can be converted into a form with few nonlinear terms via a linear transformation. Based on these properties, we employ the branch-and-bound (BnB) algorithm to optimize the resulting problem where the dimension of the search space is small. To further improve efficiency of the BnB algorithm where computation of the lower bound is the bottleneck, we propose a new lower bounding scheme which has a k-cardinality linear assignment formulation and can be efficiently solved. Experimental results show that the proposed algorithm outperforms state-of-the-art methods in terms of robustness to disturbances and point matching accuracy. version:1
arxiv-1701-02359 | Playtime Measurement with Survival Analysis | http://arxiv.org/abs/1701.02359 | id:1701.02359 author:Markus Viljanen, Antti Airola, Jukka Heikkonen, Tapio Pahikkala category:stat.AP cs.AI stat.ML  published:2017-01-04 summary:Maximizing product use is a central goal of many businesses, which makes retention and monetization two central analytics metrics in games. Player retention may refer to various duration variables quantifying product use: total playtime or session playtime are popular research targets, and active playtime is well-suited for subscription games. Such research often has the goal of increasing player retention or conversely decreasing player churn. Survival analysis is a framework of powerful tools well suited for retention type data. This paper contributes new methods to game analytics on how playtime can be analyzed using survival analysis without covariates. Survival and hazard estimates provide both a visual and an analytic interpretation of the playtime phenomena as a funnel type nonparametric estimate. Metrics based on the survival curve can be used to aggregate this playtime information into a single statistic. Comparison of survival curves between cohorts provides a scientific AB-test. All these methods work on censored data and enable computation of confidence intervals. This is especially important in time and sample limited data which occurs during game development. Throughout this paper, we illustrate the application of these methods to real world game development problems on the Hipster Sheep mobile game. version:1
arxiv-1701-00946 | Joint Semantic Synthesis and Morphological Analysis of the Derived Word | http://arxiv.org/abs/1701.00946 | id:1701.00946 author:Ryan Cotterell, Hinrich Schütze category:cs.CL  published:2017-01-04 summary:Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word's meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituents segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DerivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist's notion of morphological productivity. version:1
arxiv-1701-00939 | Dense Associative Memory is Robust to Adversarial Inputs | http://arxiv.org/abs/1701.00939 | id:1701.00939 author:Dmitry Krotov, John J Hopfield category:cs.LG cs.CR cs.CV q-bio.NC stat.ML  published:2017-01-04 summary:Deep neural networks (DNN) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation, so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNN and humans classify patterns, and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. Our paper examines these questions within the framework of Dense Associative Memory (DAM) models. These models are defined by the energy function, with higher order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units (ReLU), fail to transfer to and fool the models with higher order interactions. This opens up a possibility to use higher order models for detecting and stopping malicious adversarial attacks. The presented results suggest that DAM with higher order energy functions are closer to human visual perception than DNN with ReLUs. version:1
arxiv-1701-00903 | An Interval-Based Bayesian Generative Model for Human Complex Activity Recognition | http://arxiv.org/abs/1701.00903 | id:1701.00903 author:Li Liu, Yongzhong Yang, Lakshmi Narasimhan Govindarajan, Shu Wang, Bin Hu, Li Cheng, David S. Rosenblum category:stat.ML cs.LG  published:2017-01-04 summary:Complex activity recognition is challenging due to the inherent uncertainty and diversity of performing a complex activity. Normally, each instance of a complex activity has its own configuration of atomic actions and their temporal dependencies. We propose in this paper an atomic action-based Bayesian model that constructs Allen's interval relation networks to characterize complex activities with structural varieties in a probabilistic generative way: By introducing latent variables from the Chinese restaurant process, our approach is able to capture all possible styles of a particular complex activity as a unique set of distributions over atomic actions and relations. We also show that local temporal dependencies can be retained and are globally consistent in the resulting interval network. Moreover, network structure can be learned from empirical data. A new dataset of complex hand activities has been constructed and made publicly available, which is much larger in size than any existing datasets. Empirical evaluations on benchmark datasets as well as our in-house dataset demonstrate the competitiveness of our approach. version:1
arxiv-1701-00599 | AENet: Learning Deep Audio Features for Video Analysis | http://arxiv.org/abs/1701.00599 | id:1701.00599 author:Naoya Takahashi, Michael Gygli, Luc Van Gool category:cs.MM cs.CV cs.SD  published:2017-01-03 summary:We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone. version:2
arxiv-1701-00892 | Automated Blood Vessel Segmentation of Fundus Images Based on Region Features and Hierarchical Growth Algorithm | http://arxiv.org/abs/1701.00892 | id:1701.00892 author:Zhun Fan, Jiewei Lu, Wenji Li category:cs.CV  published:2017-01-04 summary:Automated blood vessel segmentation plays an important role in the diagnosis and treatment of various cardiovascular and ophthalmologic diseases. In this paper, a novel unsupervised segmentation algorithm is proposed to extract blood vessels from fundus images. At first, an enhanced vessel image is generated by morphological reconstruction, and then divided into three parts: preliminary vessel regions, background regions and undetermined regions. Secondly, a list of region features of blood vessels are defined and used to remove the noise regions in preliminary vessel regions and extract the skeletons of blood vessels. An intermediate vessel image is obtained by combing the denoised preliminary vessel regions with vessel skeletons. After that, a novel hierarchical growth algorithm, namely HGA, is proposed to label each pixel in undetermined regions as vessel or not in an incremental way, which takes the advantage of color and spatial information from the intermediate vessel image and background regions. Finally, postprocessing is performed to remove the nonvessel regions. The proposed algorithm has low computational complexity and outperforms many other state-of-art supervised and unsupervised methods in terms of accuracy. It achieves a vessel segmentation accuracy of 96.0%, 95.8% and 95.1% in an average time of 10.72s, 15.74s and 50.71s on images from three publicly available retinal image datasets DRIVE, STARE, and CHASE DB1, respectively. version:1
arxiv-1701-00879 | PlatEMO: A MATLAB Platform for Evolutionary Multi-Objective Optimization | http://arxiv.org/abs/1701.00879 | id:1701.00879 author:Ye Tian, Ran Cheng, Xingyi Zhang, Yaochu Jin category:cs.NE 90C29  published:2017-01-04 summary:Over the last three decades, a large number of evolutionary algorithms have been developed for solving multiobjective optimization problems. However, there lacks an up-to-date and comprehensive software platform for researchers to properly benchmark existing algorithms and for practitioners to apply selected algorithms to solve their real-world problems. The demand of such a common tool becomes even more urgent, when the source code of many proposed algorithms has not been made publicly available. To address these issues, we have developed a MATLAB platform for evolutionary multi-objective optimization in this paper, called PlatEMO, which includes more than 50 multi-objective evolutionary algorithms and more than 100 multi-objective test problems, along with several widely used performance indicators. With a user-friendly graphical user interface, PlatEMO enables users to easily compare several evolutionary algorithms at one time and collect statistical results in Excel or LaTeX files. More importantly, PlatEMO is completely open source, such that users are able to develop new algorithms on the basis of it. This paper introduces the main features of PlatEMO and illustrates how to use it for performing comparative experiments, embedding new algorithms, creating new test problems, and developing performance indicators. Source code of PlatEMO is now available at: http://bimk.ahu.edu.cn/index.php?s=/Index/Software/index.html. version:1
arxiv-1701-00851 | Unsupervised neural and Bayesian models for zero-resource speech processing | http://arxiv.org/abs/1701.00851 | id:1701.00851 author:Herman Kamper category:cs.CL cs.LG  published:2017-01-03 summary:In settings where only unlabelled speech data is available, zero-resource speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. There are two central problems in zero-resource speech processing: (i) finding frame-level feature representations which make it easier to discriminate between linguistic units (phones or words), and (ii) segmenting and clustering unlabelled speech into meaningful units. In this thesis, we argue that a combination of top-down and bottom-up modelling is advantageous in tackling these two problems. To address the problem of frame-level representation learning, we present the correspondence autoencoder (cAE), a neural network trained with weak top-down supervision from an unsupervised term discovery system. By combining this top-down supervision with unsupervised bottom-up initialization, the cAE yields much more discriminative features than previous approaches. We then present our unsupervised segmental Bayesian model that segments and clusters unlabelled speech into hypothesized words. By imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, our system outperforms several others on multi-speaker conversational English and Xitsonga speech data. Finally, we show that the clusters discovered by the segmental Bayesian model can be made less speaker- and gender-specific by using features from the cAE instead of traditional acoustic features. In summary, the different models and systems presented in this thesis show that both top-down and bottom-up modelling can improve representation learning, segmentation and clustering of unlabelled speech data. version:1
arxiv-1701-00831 | Collapsing of dimensionality | http://arxiv.org/abs/1701.00831 | id:1701.00831 author:Marco Gori, Marco Maggini, Alessandro Rossi category:cs.LG  published:2017-01-03 summary:We analyze a new approach to Machine Learning coming from a modification of classical regularization networks by casting the process in the time dimension, leading to a sort of collapse of dimensionality in the problem of learning the model parameters. This approach allows the definition of a online learning algorithm that progressively accumulates the knowledge provided in the input trajectory. The regularization principle leads to a solution based on a dynamical system that is paired with a procedure to develop a graph structure that stores the input regularities acquired from the temporal evolution. We report an extensive experimental exploration on the behavior of the parameter of the proposed model and an evaluation on artificial dataset. version:1
arxiv-1701-00823 | Learning a Mixture of Deep Networks for Single Image Super-Resolution | http://arxiv.org/abs/1701.00823 | id:1701.00823 author:Ding Liu, Zhaowen Wang, Nasser Nasrabadi, Thomas Huang category:cs.CV  published:2017-01-03 summary:Single image super-resolution (SR) is an ill-posed problem which aims to recover high-resolution (HR) images from their low-resolution (LR) observations. The crux of this problem lies in learning the complex mapping between low-resolution patches and the corresponding high-resolution patches. Prior arts have used either a mixture of simple regression models or a single non-linear neural network for this propose. This paper proposes the method of learning a mixture of SR inference modules in a unified framework to tackle this problem. Specifically, a number of SR inference modules specialized in different image local patterns are first independently applied on the LR image to obtain various HR estimates, and the resultant HR estimates are adaptively aggregated to form the final HR image. By selecting neural networks as the SR inference module, the whole procedure can be incorporated into a unified network and be optimized jointly. Extensive experiments are conducted to investigate the relation between restoration performance and different network architectures. Compared with other current image SR approaches, our proposed method achieves state-of-the-arts restoration results on a wide range of images consistently while allowing more flexible design choices. The source codes are available in http://www.ifp.illinois.edu/~dingliu2/accv2016. version:1
arxiv-1701-00804 | Semi-Supervised Endmember Identification In Nonlinear Spectral Mixtures Via Semantic Representation | http://arxiv.org/abs/1701.00804 | id:1701.00804 author:Yuki Itoh, Siwei Feng, Marco F. Duarte, Mario Parente category:cs.CV  published:2017-01-03 summary:This paper proposes a new hyperspectral unmixing method for nonlinearly mixed hyperspectral data using a semantic representation in a semi-supervised fashion, assuming the availability of a spectral reference library. Existing semi-supervised unmixing algorithms select members from an endmember library that are present at each of the pixels; most such methods assume a linear mixing model. However, those methods will fail in the presence of nonlinear mixing among the observed spectra. To address this issue, we develop an endmember selection method using a recently proposed semantic spectral representation obtained via non-homogeneous hidden Markov chain (NHMC) model for a wavelet transform of the spectra. The semantic representation can encode spectrally discriminative features for any observed spectrum and, therefore, our proposed method can perform endmember selection without any assumption on the mixing model. Experimental results show that in the presence of sufficiently nonlinear mixing our proposed method outperforms dictionary-based sparse unmixing approaches based on linear models. version:1
arxiv-1701-00798 | Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences | http://arxiv.org/abs/1701.00798 | id:1701.00798 author:Amir Hossein Yazdavar, Monireh Ebrahimi, Naomie Salim category:cs.CL  published:2017-01-03 summary:With the rapid growth of social media on the web, emotional polarity computation has become a flourishing frontier in the text mining community. However, it is challenging to understand the latest trends and summarize the state or general opinions about products due to the big diversity and size of social media data and this creates the need of automated and real time opinion extraction and mining. On the other hand, the bulk of current research has been devoted to study the subjective sentences which contain opinion keywords and limited work has been reported for objective statements that imply sentiment. In this paper, fuzzy based knowledge engineering model has been developed for sentiment classification of special group of such sentences including the change or deviation from desired range or value. Drug reviews are the rich source of such statements. Therefore, in this research, some experiments were carried out on patient's reviews on several different cholesterol lowering drugs to determine their sentiment polarity. The main conclusion through this study is, in order to increase the accuracy level of existing drug opinion mining systems, objective sentences which imply opinion should be taken into account. Our experimental results demonstrate that our proposed model obtains over 72 percent F1 value. version:1
arxiv-1701-00794 | Constrained Deep Weak Supervision for Histopathology Image Segmentation | http://arxiv.org/abs/1701.00794 | id:1701.00794 author:Zhipeng Jia, Xingyi Huang, Eric I-Chao Chang, Yan Xu category:cs.CV  published:2017-01-03 summary:In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-to-image weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images. version:1
arxiv-1701-00757 | Clustering Signed Networks with the Geometric Mean of Laplacians | http://arxiv.org/abs/1701.00757 | id:1701.00757 author:Pedro Mercado, Francesco Tudisco, Matthias Hein category:stat.ML cs.LG math.NA  published:2017-01-03 summary:Signed networks allow to model positive and negative relationships. We analyze existing extensions of spectral clustering to signed networks. It turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise. Our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the Laplacians of the positive and negative part. As a solution we propose to use the geometric mean of the Laplacians of positive and negative part and show that it outperforms the existing approaches. While the geometric mean of matrices is computationally expensive, we show that eigenvectors of the geometric mean can be computed efficiently, leading to a numerical scheme for sparse matrices which is of independent interest. version:1
arxiv-1612-09030 | Meta-Unsupervised-Learning: A supervised approach to unsupervised learning | http://arxiv.org/abs/1612.09030 | id:1612.09030 author:Vikas K. Garg, Adam Tauman Kalai category:cs.LG cs.AI cs.CV  published:2016-12-29 summary:We introduce a new paradigm to investigate unsupervised learning, reducing unsupervised learning to supervised learning. Specifically, we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior, possibly heterogeneous, supervised learning tasks. We demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as (a) clustering, (b) outlier detection, and (c) similarity prediction under a common umbrella of meta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to establish the theoretical foundations of our framework, and show that our framing of meta-clustering circumvents Kleinberg's impossibility theorem for clustering. version:2
arxiv-1701-00749 | Pyndri: a Python Interface to the Indri Search Engine | http://arxiv.org/abs/1701.00749 | id:1701.00749 author:Christophe Van Gysel, Evangelos Kanoulas, Maarten de Rijke category:cs.IR cs.CL  published:2017-01-03 summary:We introduce pyndri, a Python interface to the Indri search engine. Pyndri allows to access Indri indexes from Python at two levels: (1) dictionary and tokenized document collection, (2) evaluating queries on the index. We hope that with the release of pyndri, we will stimulate reproducible, open and fast-paced IR research. version:1
arxiv-1701-00737 | Deterministic and Probabilistic Conditions for Finite Completability of Low-rank Multi-View Data | http://arxiv.org/abs/1701.00737 | id:1701.00737 author:Morteza Ashraphijuo, Xiaodong Wang, Vaneet Aggarwal category:cs.IT cs.LG math.AG math.IT  published:2017-01-03 summary:We consider the multi-view data completion problem, i.e., to complete a matrix $\mathbf{U}=[\mathbf{U}_1 \mathbf{U}_2]$ where the ranks of $\mathbf{U},\mathbf{U}_1$, and $\mathbf{U}_2$ are given. In particular, we investigate the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries for finite completability of such a multi-view data given the corresponding rank constraints. In contrast with the existing analysis on Grassmannian manifold for a single-view matrix, i.e., conventional matrix completion, we propose a geometric analysis on the manifold structure for multi-view data to incorporate more than one rank constraint. We provide a deterministic necessary and sufficient condition on the sampling pattern for finite completability. We also give a probabilistic condition in terms of the number of samples per column that guarantees finite completability with high probability. Finally, using the developed tools, we derive the deterministic and probabilistic guarantees for unique completability. version:1
arxiv-1701-00728 | On (Commercial) Benefits of Automatic Text Summarization Systems in the News Domain: A Case of Media Monitoring and Media Response Analysis | http://arxiv.org/abs/1701.00728 | id:1701.00728 author:Pashutan Modaresi, Philipp Gross, Siavash Sefidrodi, Mirja Eckhof, Stefan Conrad category:cs.CL  published:2017-01-03 summary:In this work, we present the results of a systematic study to investigate the (commercial) benefits of automatic text summarization systems in a real world scenario. More specifically, we define a use case in the context of media monitoring and media response analysis and claim that even using a simple query-based extractive approach can dramatically save the processing time of the employees without significantly reducing the quality of their work. version:1
arxiv-1701-00723 | Image denoising using group sparsity residual and external nonlocal self-similarity prior | http://arxiv.org/abs/1701.00723 | id:1701.00723 author:Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang category:cs.CV  published:2017-01-03 summary:Nonlocal image representation has been successfully used in many image-related inverse problems including denoising, deblurring and deblocking. However, a majority of reconstruction methods only exploit the nonlocal self-similarity (NSS) prior of the degraded observation image, it is very challenging to reconstruct the latent clean image. In this paper we propose a novel model for image denoising via group sparsity residual and external NSS prior. To boost the performance of image denoising, the concept of group sparsity residual is proposed, and thus the problem of image denoising is transformed into one that reduces the group sparsity residual. Due to the fact that the groups contain a large amount of NSS information of natural images, we obtain a good estimation of the group sparse coefficients of the original image by the external NSS prior based on Gaussian Mixture model (GMM) learning and the group sparse coefficients of noisy image is used to approximate the estimation. Experimental results have demonstrated that the proposed method not only outperforms many state-of-the-art methods, but also delivers the best qualitative denoising results with finer details and less ringing artifacts. version:1
arxiv-1701-00694 | Mixed one-bit compressive sensing with applications to overexposure correction for CT reconstruction | http://arxiv.org/abs/1701.00694 | id:1701.00694 author:Xiaolin Huang, Yan Xia, Lei Shi, Yixing Huang, Ming Yan, Joachim Hornegger, Andreas Maier category:cs.CV cs.IR cs.NA math.NA  published:2017-01-03 summary:When a measurement falls outside the quantization or measurable range, it becomes saturated and cannot be used in classical reconstruction methods. For example, in C-arm angiography systems, which provide projection radiography, fluoroscopy, digital subtraction angiography, and are widely used for medical diagnoses and interventions, the limited dynamic range of C-arm flat detectors leads to overexposure in some projections during an acquisition, such as imaging relatively thin body parts (e.g., the knee). Aiming at overexposure correction for computed tomography (CT) reconstruction, we in this paper propose a mixed one-bit compressive sensing (M1bit-CS) to acquire information from both regular and saturated measurements. This method is inspired by the recent progress on one-bit compressive sensing, which deals with only sign observations. Its successful applications imply that information carried by saturated measurements is useful to improve recovery quality. For the proposed M1bit-CS model, alternating direction methods of multipliers is developed and an iterative saturation detection scheme is established. Then we evaluate M1bit-CS on one-dimensional signal recovery tasks. In some experiments, the performance of the proposed algorithms on mixed measurements is almost the same as recovery on unsaturated ones with the same amount of measurements. Finally, we apply the proposed method to overexposure correction for CT reconstruction on a phantom and a simulated clinical image. The results are promising, as the typical streaking artifacts and capping artifacts introduced by saturated projection data are effectively reduced, yielding significant error reduction compared with existing algorithms based on extrapolation. version:1
arxiv-1701-00677 | New Methods of Enhancing Prediction Accuracy in Linear Models with Missing Data | http://arxiv.org/abs/1701.00677 | id:1701.00677 author:Mohammad Amin Fakharian, Ashkan Esmaeili, Farokh Marvasti category:stat.ML cs.LG  published:2017-01-03 summary:In this paper, prediction for linear systems with missing information is investigated. New methods are introduced to improve the Mean Squared Error (MSE) on the test set in comparison to state-of-the-art methods, through appropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft Weighted Prediction (SWP) algorithm and its efficacy are depicted and compared to previous works for non-missing scenarios. The algorithm is then modified and optimized for missing scenarios. It is shown that controlled over-fitting by suggested algorithms will improve prediction accuracy in various cases. Simulation results approve our heuristics in enhancing the prediction accuracy. version:1
arxiv-1701-00669 | Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space | http://arxiv.org/abs/1701.00669 | id:1701.00669 author:Matthias Vestner, Roee Litman, Emanuele Rodolà, Alex Bronstein, Daniel Cremers category:cs.CV  published:2017-01-03 summary:Many algorithms for the computation of correspondences between deformable shapes rely on some variant of nearest neighbor matching in a descriptor space. Such are, for example, various point-wise correspondence recovery algorithms used as a post-processing stage in the functional correspondence framework. Such frequently used techniques implicitly make restrictive assumptions (e.g., near-isometry) on the considered shapes and in practice suffer from lack of accuracy and result in poor surjectivity. We propose an alternative recovery technique capable of guaranteeing a bijective correspondence and producing significantly higher accuracy and smoothness. Unlike other methods our approach does not depend on the assumption that the analyzed shapes are isometric. We derive the proposed method from the statistical framework of kernel density estimation and demonstrate its performance on several challenging deformable 3D shape matching datasets. version:1
arxiv-1701-00660 | Ambiguity and Incomplete Information in Categorical Models of Language | http://arxiv.org/abs/1701.00660 | id:1701.00660 author:Dan Marsden category:cs.LO cs.CL math.CT  published:2017-01-03 summary:We investigate notions of ambiguity and partial information in categorical distributional models of natural language. Probabilistic ambiguity has previously been studied using Selinger's CPM construction. This construction works well for models built upon vector spaces, as has been shown in quantum computational applications. Unfortunately, it doesn't seem to provide a satisfactory method for introducing mixing in other compact closed categories such as the category of sets and binary relations. We therefore lack a uniform strategy for extending a category to model imprecise linguistic information. In this work we adopt a different approach. We analyze different forms of ambiguous and incomplete information, both with and without quantitative probabilistic data. Each scheme then corresponds to a suitable enrichment of the category in which we model language. We view different monads as encapsulating the informational behaviour of interest, by analogy with their use in modelling side effects in computation. Previous results of Jacobs then allow us to systematically construct suitable bases for enrichment. We show that we can freely enrich arbitrary dagger compact closed categories in order to capture all the phenomena of interest, whilst retaining the important dagger compact closed structure. This allows us to construct a model with real convex combination of binary relations that makes non-trivial use of the scalars. Finally we relate our various different enrichments, showing that finite subconvex algebra enrichment covers all the effects under consideration. version:1
arxiv-1701-00652 | Semidefinite tests for latent causal structures | http://arxiv.org/abs/1701.00652 | id:1701.00652 author:Aditya Kela, Kai von Prillwitz, Johan Aberg, Rafael Chaves, David Gross category:stat.ML math.ST quant-ph stat.TH  published:2017-01-03 summary:Testing whether a probability distribution is compatible with a given Bayesian network is a fundamental task in the field of causal inference, where Bayesian networks model causal relations. Here we consider the class of causal structures where all correlations between observed quantities are solely due to the influence from latent variables. We show that each model of this type imposes a certain signature on the observable covariance matrix in terms of a particular decomposition into positive semidefinite components. This signature, and thus the underlying hypothetical latent structure, can be tested in a computationally efficient manner via semidefinite programming. This stands in stark contrast with the algebraic geometric tools required if the full observable probability distribution is taken into account. The semidefinite test is compared with tests based on entropic inequalities. version:1
arxiv-1701-04398 | Automatic sleep monitoring using ear-EEG | http://arxiv.org/abs/1701.04398 | id:1701.04398 author:Takashi Nakamura, Valentin Goverdovsky, Mary J. Morrell, Danilo P. Mandic category:physics.med-ph stat.ML  published:2017-01-03 summary:The monitoring of sleep patterns without patient's inconvenience or involvement of a medical specialist is a clinical question of significant importance. To this end, we propose an automatic sleep stage monitoring system based on an affordable, unobtrusive, discreet, and long-term wearable in-ear sensor for recording the Electroencephalogram (ear-EEG). The selected features for sleep pattern classification from a single ear-EEG channel include the spectral edge frequency (SEF) and multi- scale fuzzy entropy (MSFE), a structural complexity feature. In this preliminary study, the manually scored hypnograms from simultaneous scalp-EEG and ear-EEG recordings of four subjects are used as labels for two analysis scenarios: 1) classification of ear-EEG hypnogram labels from ear-EEG recordings and 2) prediction of scalp-EEG hypnogram labels from ear-EEG recordings. We consider both 2-class and 4-class sleep scoring, with the achieved accuracies ranging from 78.5 % to 95.2 % for ear-EEG labels predicted from ear-EEG, and 76.8 % to 91.8 % for scalp-EEG labels predicted from ear-EEG. The corresponding kappa coefficients, which range from 0.64 to 0.83 for Scenario 1 and from 0.65 to 0.80 for Scenario 2, indicate a Substantial to Almost Perfect agreement, thus proving the feasibility of in-ear sensing for sleep monitoring in the community. version:1
arxiv-1701-00609 | Akid: A Library for Neural Network Research and Production from a Dataism Approach | http://arxiv.org/abs/1701.00609 | id:1701.00609 author:Shuai Li category:cs.LG cs.DC  published:2017-01-03 summary:Neural networks are a revolutionary but immature technique that is fast evolving and heavily relies on data. To benefit from the newest development and newly available data, we want the gap between research and production as small as possibly. On the other hand, differing from traditional machine learning models, neural network is not just yet another statistic model, but a model for the natural processing engine --- the brain. In this work, we describe a neural network library named {\texttt akid}. It provides higher level of abstraction for entities (abstracted as blocks) in nature upon the abstraction done on signals (abstracted as tensors) by Tensorflow, characterizing the dataism observation that all entities in nature processes input and emit out in some ways. It includes a full stack of software that provides abstraction to let researchers focus on research instead of implementation, while at the same time the developed program can also be put into production seamlessly in a distributed environment, and be production ready. At the top application stack, it provides out-of-box tools for neural network applications. Lower down, akid provides a programming paradigm that lets user easily build customized models. The distributed computing stack handles the concurrency and communication, thus letting models be trained or deployed to a single GPU, multiple GPUs, or a distributed environment without affecting how a model is specified in the programming paradigm stack. Lastly, the distributed deployment stack handles how the distributed computing is deployed, thus decoupling the research prototype environment with the actual production environment, and is able to dynamically allocate computing resources, so development (Devs) and operations (Ops) could be separated. Please refer to http://akid.readthedocs.io/en/latest/ for documentation. version:1
arxiv-1701-00422 | Towards multiple kernel principal component analysis for integrative analysis of tumor samples | http://arxiv.org/abs/1701.00422 | id:1701.00422 author:Nora K. Speicher, Nico Pfeifer category:stat.ML  published:2017-01-02 summary:Personalized treatment of patients based on tissue-specific cancer subtypes has strongly increased the efficacy of the chosen therapies. Even though the amount of data measured for cancer patients has increased over the last years, most cancer subtypes are still diagnosed based on individual data sources (e.g. gene expression data). We propose an unsupervised data integration method based on kernel principal component analysis. Principal component analysis is one of the most widely used techniques in data analysis. Unfortunately, the straight-forward multiple-kernel extension of this method leads to the use of only one of the input matrices, which does not fit the goal of gaining information from all data sources. Therefore, we present a scoring function to determine the impact of each input matrix. The approach enables visualizing the integrated data and subsequent clustering for cancer subtype identification. Due to the nature of the method, no free parameters have to be set. We apply the methodology to five different cancer data sets and demonstrate its advantages in terms of results and usability. version:2
arxiv-1701-00597 | Deep Convolutional Neural Networks for Pairwise Causality | http://arxiv.org/abs/1701.00597 | id:1701.00597 author:Karamjit Singh, Garima Gupta, Lovekesh Vig, Gautam Shroff, Puneet Agarwal category:cs.LG  published:2017-01-03 summary:Discovering causal models from observational and interventional data is an important first step preceding what-if analysis or counterfactual reasoning. As has been shown before, the direction of pairwise causal relations can, under certain conditions, be inferred from observational data via standard gradient-boosted classifiers (GBC) using carefully engineered statistical features. In this paper we apply deep convolutional neural networks (CNNs) to this problem by plotting attribute pairs as 2-D scatter plots that are fed to the CNN as images. We evaluate our approach on the 'Cause- Effect Pairs' NIPS 2013 Data Challenge. We observe that a weighted ensemble of CNN with the earlier GBC approach yields significant improvement. Further, we observe that when less training data is available, our approach performs better than the GBC based approach suggesting that CNN models pre-trained to determine the direction of pairwise causal direction could have wider applicability in causal discovery and enabling what-if or counterfactual analysis. version:1
arxiv-1701-00593 | HLA class I binding prediction via convolutional neural networks | http://arxiv.org/abs/1701.00593 | id:1701.00593 author:Yeeleng Scott Vang, Xiaohui Xie category:q-bio.QM cs.LG  published:2017-01-03 summary:Many biological processes are governed by protein-ligand interactions. Of such is the recognition of self and nonself cells by the immune system. This immune response process is regulated by the major histocompatibility complex (MHC) protein which is encoded by the human leukocyte antigen (HLA) complex. Understanding the binding potential between MHC and peptides is crucial to our understanding of the functioning of the immune system, which in turns will broaden our understanding of autoimmune diseases and vaccine design. We introduce a new distributed representation of amino acids, named HLA-Vec, that can be used for a variety of downstream proteomic machine learning tasks. We then propose a deep convolutional neurerror can be used only in preambleal network architecture, named HLA-CNN, for the task of HLA class I-peptide binding prediction. Experimental results show combining the new distributed representation with our HLA-CNN architecture acheives state-of-the-art results in the vast majority of the latest two Immune Epitope Database (IEDB) weekly automated benchmark datasets. version:1
arxiv-1701-00576 | Shortcut Sequence Tagging | http://arxiv.org/abs/1701.00576 | id:1701.00576 author:Huijia Wu, Jiajun Zhang, Chengqing Zong category:cs.CL  published:2017-01-03 summary:Deep stacked RNNs are usually hard to train. Adding shortcut connections across different layers is a common way to ease the training of stacked networks. However, extra shortcuts make the recurrent step more complicated. To simply the stacked architecture, we propose a framework called shortcut block, which is a marriage of the gating mechanism and shortcuts, while discarding the self-connected part in LSTM cell. We present extensive empirical experiments showing that this design makes training easy and improves generalization. We propose various shortcut block topologies and compositions to explore its effectiveness. Based on this architecture, we obtain a 6% relatively improvement over the state-of-the-art on CCGbank supertagging dataset. We also get comparable results on POS tagging task. version:1
arxiv-1701-00562 | End-to-End Attention based Text-Dependent Speaker Verification | http://arxiv.org/abs/1701.00562 | id:1701.00562 author:Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li, Yifan Gong category:cs.CL stat.ML  published:2017-01-03 summary:A new type of End-to-End system for text-dependent speaker verification is presented in this paper. Previously, using the phonetically discriminative/speaker discriminative DNNs as feature extractors for speaker verification has shown promising results. The extracted frame-level (DNN bottleneck, posterior or d-vector) features are equally weighted and aggregated to compute an utterance-level speaker representation (d-vector or i-vector). In this work we use speaker discriminative CNNs to extract the noise-robust frame-level features. These features are smartly combined to form an utterance-level speaker vector through an attention mechanism. The proposed attention model takes the speaker discriminative information and the phonetic information to learn the weights. The whole system, including the CNN and attention model, is joint optimized using an end-to-end criterion. The training algorithm imitates exactly the evaluation process --- directly mapping a test utterance and a few target speaker utterances into a single verification score. The algorithm can automatically select the most similar impostor for each target speaker to train the network. We demonstrated the effectiveness of the proposed end-to-end system on Windows $10$ "Hey Cortana" speaker verification task. version:1
arxiv-1701-00561 | Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation | http://arxiv.org/abs/1701.00561 | id:1701.00561 author:Xinyu Wang, Hanxi Li, Yi Li, Fumin Shen, Fatih Porikli category:cs.CV  published:2017-01-03 summary:Visual tracking is a fundamental problem in computer vision. Recently, some deep-learning-based tracking algorithms have been achieving record-breaking performances. However, due to the high complexity of deep learning, most deep trackers suffer from low tracking speed, and thus are impractical in many real-world applications. Some new deep trackers with smaller network structure achieve high efficiency while at the cost of significant decrease on precision. In this paper, we propose to transfer the feature for image classification to the visual tracking domain via convolutional channel reductions. The channel reduction could be simply viewed as an additional convolutional layer with the specific task. It not only extracts useful information for object tracking but also significantly increases the tracking speed. To better accommodate the useful feature of the target in different scales, the adaptation filters are designed with different sizes. The yielded visual tracker is real-time and also illustrates the state-of-the-art accuracies in the experiment involving two well-adopted benchmarks with more than 100 test videos. version:1
arxiv-1612-09413 | Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression | http://arxiv.org/abs/1612.09413 | id:1612.09413 author:Quan Zhang, Mingyuan Zhou category:stat.ME stat.ML  published:2016-12-30 summary:To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and appealing performance. version:2
arxiv-1701-00504 | Stance detection in online discussions | http://arxiv.org/abs/1701.00504 | id:1701.00504 author:Peter Krejzl, Barbora Hourová, Josef Steinberger category:cs.CL  published:2017-01-02 summary:This paper describes our system created to detect stance in online discussions. The goal is to identify whether the author of a comment is in favor of the given target or against. Our approach is based on a maximum entropy classifier, which uses surface-level, sentiment and domain-specific features. The system was originally developed to detect stance in English tweets. We adapted it to process Czech news commentaries. version:1
arxiv-1701-00458 | Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient Detection | http://arxiv.org/abs/1701.00458 | id:1701.00458 author:Guillermo Cabrera-Vives, Ignacio Reyes, Francisco Förster, Pablo A. Estévez, Juan-Carlos Maureira category:astro-ph.IM cs.CV  published:2017-01-02 summary:We introduce Deep-HiTS, a rotation invariant convolutional neural network (CNN) model for classifying images of transients candidates into artifacts or real sources for the High cadence Transient Survey (HiTS). CNNs have the advantage of learning the features automatically from the data while achieving high performance. We compare our CNN model against a feature engineering approach using random forests (RF). We show that our CNN significantly outperforms the RF model reducing the error by almost half. Furthermore, for a fixed number of approximately 2,000 allowed false transient candidates per night we are able to reduce the miss-classified real transients by approximately 1/5. To the best of our knowledge, this is the first time CNNs have been used to detect astronomical transient events. Our approach will be very useful when processing images from next generation instruments such as the Large Synoptic Survey Telescope (LSST). We have made all our code and data available to the community for the sake of allowing further developments and comparisons at https://github.com/guille-c/Deep-HiTS. version:1
arxiv-1701-00449 | Retrieving Similar X-Ray Images from Big Image Data Using Radon Barcodes with Single Projections | http://arxiv.org/abs/1701.00449 | id:1701.00449 author:Morteza Babaie, H. R. Tizhoosh, Shujin Zhu, M. E. Shiri category:cs.CV  published:2017-01-02 summary:The idea of Radon barcodes (RBC) has been introduced recently. In this paper, we propose a content-based image retrieval approach for big datasets based on Radon barcodes. Our method (Single Projection Radon Barcode, or SP-RBC) uses only a few Radon single projections for each image as global features that can serve as a basis for weak learners. This is our most important contribution in this work, which improves the results of the RBC considerably. As a matter of fact, only one projection of an image, as short as a single SURF feature vector, can already achieve acceptable results. Nevertheless, using multiple projections in a long vector will not deliver anticipated improvements. To exploit the information inherent in each projection, our method uses the outcome of each projection separately and then applies more precise local search on the small subset of retrieved images. We have tested our method using IRMA 2009 dataset a with 14,400 x-ray images as part of imageCLEF initiative. Our approach leads to a substantial decrease in the error rate in comparison with other non-learning methods. version:1
arxiv-1701-00405 | Adversarially Tuned Scene Generation | http://arxiv.org/abs/1701.00405 | id:1701.00405 author:V S R Veeravasarapu, Constantin Rothkopf, Ramesh Visvanathan category:cs.CV  published:2017-01-02 summary:Generalization performance of trained computer vision systems that use computer graphics (CG) generated data is not yet effective due to the concept of 'domain-shift' between virtual and real data. Although simulated data augmented with a few real world samples has been shown to mitigate domain shift and improve transferability of trained models, guiding or bootstrapping the virtual data generation with the distributions learnt from target real world domain is desired, especially in the fields where annotating even few real images is laborious (such as semantic labeling, and intrinsic images etc.). In order to address this problem in an unsupervised manner, our work combines recent advances in CG (which aims to generate stochastic scene layouts coupled with large collections of 3D object models) and generative adversarial training (which aims train generative models by measuring discrepancy between generated and real data in terms of their separability in the space of a deep discriminatively-trained classifier). Our method uses iterative estimation of the posterior density of prior distributions for a generative graphical model. This is done within a rejection sampling framework. Initially, we assume uniform distributions as priors on the parameters of a scene described by a generative graphical model. As iterations proceed the prior distributions get updated to distributions that are closer to the (unknown) distributions of target data. We demonstrate the utility of adversarially tuned scene generation on two real-world benchmark datasets (CityScapes and CamVid) for traffic scene semantic labeling with a deep convolutional net (DeepLab). We realized performance improvements by 2.28 and 3.14 points (using the IoU metric) between the DeepLab models trained on simulated sets prepared from the scene generation models before and after tuning to CityScapes and CamVid respectively. version:1
arxiv-1701-00352 | Weakly Supervised Semantic Segmentation using Web-Crawled Videos | http://arxiv.org/abs/1701.00352 | id:1701.00352 author:Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, Bohyung Han category:cs.CV  published:2017-01-02 summary:We propose a novel algorithm for weakly supervised semantic segmentation based on image-level class labels only. In weakly supervised setting, it is commonly observed that trained model overly focuses on discriminative parts rather than the entire object area. Our goal is to overcome this limitation with no additional human intervention by retrieving videos relevant to target class labels from web repository, and generating segmentation labels from the retrieved videos to simulate strong supervision for semantic segmentation. During this process, we take advantage of image classification with discriminative localization technique to reject false alarms in retrieved videos and identify relevant spatio-temporal volumes within retrieved videos. Although the entire procedure does not require any additional supervision, the segmentation annotations obtained from videos are sufficiently strong to learn a model for semantic segmentation. The proposed algorithm substantially outperforms existing methods based on the same level of supervision and is even as competitive as the approaches relying on extra annotations. version:1
arxiv-1701-00338 | Assessing Uncertainties in X-ray Single-particle Three-dimensional reconstructions | http://arxiv.org/abs/1701.00338 | id:1701.00338 author:Stefan Engblom, Carl Nettelblad, Jing Liu category:stat.ME cs.CV physics.data-an  published:2017-01-02 summary:Modern technology for producing extremely bright and coherent X-ray laser pulses provides the possibility to acquire a large number of diffraction patterns from individual biological nanoparticles, including proteins, viruses, and DNA. These two-dimensional diffraction patterns can be practically reconstructed and retrieved down to a resolution of a few \angstrom. In principle, a sufficiently large collection of diffraction patterns will contain the required information for a full three-dimensional reconstruction of the biomolecule. The computational methodology for this reconstruction task is still under development and highly resolved reconstructions have not yet been produced. We analyze the Expansion-Maximization-Compression scheme, the current state of the art approach for this very challenging application, by isolating different sources of uncertainty. Through numerical experiments on synthetic data we evaluate their respective impact. We reach conclusions of relevance for handling actual experimental data, as well as pointing out certain improvements to the underlying estimation algorithm. We also introduce a practically applicable computational methodology in the form of bootstrap procedures for assessing reconstruction uncertainty in the real data case. We evaluate the sharpness of this approach and argue that this type of procedure will be critical in the near future when handling the increasing amount of data. version:1
arxiv-1701-00326 | Challenges ahead Electron Microscopy for Structural Biology from the Image Processing point of view | http://arxiv.org/abs/1701.00326 | id:1701.00326 author:Carlos Oscar S. Sorzano, Jose Maria Carazo category:cs.CV q-bio.QM I.4  published:2017-01-02 summary:Since the introduction of Direct Electron Detectors (DEDs), the resolution and range of macromolecules amenable to this technique has significantly widened, generating a broad interest that explains the well over a dozen reviews in top journal in the last two years. Similarly, the number of job offers to lead EM groups and/or coordinate EM facilities has exploded, and FEI (the main microscope manufacturer for Life Sciences) has received more than 100 orders of high-end electron microscopes by summer 2016. Strategic corporate movements are also happening, with very big players entering the market through key acquisitions (Thermo Fisher has recently bought FEI for \$4.2B), partly attracted by new Pharma interest in the field, now perceived to be in a position to impact structure-based drug design. The scientific perspectives are indeed extremely positive but, in these moments of well-founded generalized optimists, we want to make a reflection on some of the hurdles ahead us, since they certainly exist and they indeed limit the informational content of cryoEM projects. Here we focus on image processing aspects, particularly in the so-called area of Single Particle Analysis, discussing some of the current resolution and high-throughput limiting factors. version:1
arxiv-1701-00322 | Deep learning for plasma tomography using the bolometer system at JET | http://arxiv.org/abs/1701.00322 | id:1701.00322 author:Francisco A. Matos, Diogo R. Ferreira, Pedro J. Carvalho, JET Contributors category:stat.ML physics.plasm-ph  published:2017-01-02 summary:Deep learning is having a profound impact in many fields, especially those that involve some form of image processing. Deep neural networks excel in turning an input image into a set of high-level features. On the other hand, tomography deals with the inverse problem of recreating an image from a number of projections. In plasma diagnostics, tomography aims at reconstructing the cross-section of the plasma from radiation measurements. This reconstruction can be computed with neural networks. However, previous attempts have focused on learning a parametric model of the plasma profile. In this work, we use a deep neural network to produce a full, pixel-by-pixel reconstruction of the plasma profile. For this purpose, we use the overview bolometer system at JET, and we introduce an up-convolutional network that has been trained and tested on a large set of sample tomograms. We show that this network is able to reproduce existing reconstructions with a high level of accuracy, as measured by several metrics. version:1
arxiv-1701-00299 | Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution | http://arxiv.org/abs/1701.00299 | id:1701.00299 author:Lanlan Liu, Jia Deng category:cs.LG stat.ML  published:2017-01-02 summary:We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward deep neural network that allow selective execution. Given an input, only a subset of D2NN neurons are executed, and the particular subset is determined by the D2NN itself. By pruning unnecessary computation depending on input, D2NNs provide a way to improve computational efficiency. To achieve dynamic selective execution, a D2NN augments a regular feed-forward deep neural network (directed acyclic graph of differentiable modules) with one or more controller modules. Each controller module is a sub-network whose output is a decision that controls whether other modules can execute. A D2NN is trained end to end. Both regular modules and controller modules in a D2NN are learnable and are jointly trained to optimize both accuracy and efficiency. Such training is achieved by integrating backpropagation with reinforcement learning. With extensive experiments of various D2NN architectures on image classification tasks, we demonstrate that D2NNs are general and flexible, and can effectively optimize accuracy-efficiency trade-offs. version:1
arxiv-1701-00294 | The Geodesic Distance between $\mathcal{G}_I^0$ Models and its Application to Region Discrimination | http://arxiv.org/abs/1701.00294 | id:1701.00294 author:José Naranjo-Torres, Juliana Gambini, Alejandro C. Frery category:cs.CV stat.ML  published:2017-01-01 summary:The $\mathcal{G}_I^0$ distribution is able to characterize different regions in monopolarized SAR imagery. It is indexed by three parameters: the number of looks (which can be estimated in the whole image), a scale parameter and a texture parameter. This paper presents a new proposal for feature extraction and region discrimination in SAR imagery, using the geodesic distance as a measure of dissimilarity between $\mathcal{G}_I^0$ models. We derive geodesic distances between models that describe several practical situations, assuming the number of looks known, for same and different texture and for same and different scale. We then apply this new tool to the problems of (i)~identifying edges between regions with different texture, and (ii)~quantify the dissimilarity between pairs of samples in actual SAR data. We analyze the advantages of using the geodesic distance when compared to stochastic distances. version:1
arxiv-1701-00289 | Integrating sentiment and social structure to determine preference alignments: The Irish Marriage Referendum | http://arxiv.org/abs/1701.00289 | id:1701.00289 author:David J. P. O'Sullivan, Guillermo Garduño-Hernández, James P. Gleeson, Mariano Beguerisse-Díaz category:cs.SI cs.CL cs.IR physics.soc-ph  published:2017-01-01 summary:We investigate the relationship between social structure and sentiment through the analysis of half a million tweets about the Irish Marriage Referendum of 2015. We obtain the sentiment of every tweet with the hashtags #marref and #marriageref posted in the days leading to the referendum, and construct networks to aggregate sentiment and study the interactions among users. The sentiment of the mention tweets that a user sends is correlated with the sentiment of the mentions received, and there are significantly more connections between users with similar sentiment scores than among users with opposite scores. We combine the community structure of the follower and mention networks, the activity level of the users, and sentiment scores to find groups of users who support voting 'yes' or 'no' on the referendum. We find that many conversations between users on opposing sides of the debate occurred in the absence of follower connections, suggesting that there were efforts by some users to establish dialogue and debate across ideological divisions. These results show that social structures can be successfully integrated with sentiment to analyse and understand the disposition of social media users. We discuss the implications of our work for the integration of data and meta-data, opinion dynamics, public opinion modelling and polling. version:1
arxiv-1701-00285 | High Dimensional Multi-Level Covariance Estimation and Kriging | http://arxiv.org/abs/1701.00285 | id:1701.00285 author:Julio E. Castrillon-Candas category:stat.CO stat.ML  published:2017-01-01 summary:With the advent of big data sets much of the computational science and engineering communities have been moving toward data-driven approaches to regression and classification. However, they present a significant challenge due to the increasing size, complexity and dimensionality of the problems. In this paper a multi-level kriging method that scales well with dimensions is developed. A multi-level basis is constructed that is adapted to a random projection tree (or kD-tree) partitioning of the observations and a sparse grid approximation. This approach identifies the high dimensional underlying phenomena from the noise in an accurate and numerically stable manner. Furthermore, numerically unstable covariance matrices are transformed into well conditioned multi-level matrices without compromising accuracy. A-posteriori error estimates are derived, such as the sub-exponential decay of the coefficients of the multi-level covariance matrix. The multi-level method is tested on numerically unstable problems of up to 50 dimensions. Accurate solutions with feasible computational cost are obtained. version:1
arxiv-1612-09296 | Symmetry, Saddle Points, and Global Geometry of Nonconvex Matrix Factorization | http://arxiv.org/abs/1612.09296 | id:1612.09296 author:Xingguo Li, Zhaoran Wang, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Tuo Zhao category:cs.LG math.OC stat.ML  published:2016-12-29 summary:We propose a general theory for studying the geometry of nonconvex objective functions with underlying symmetric structures. In specific, we characterize the locations of stationary points and the null space of the associated Hessian matrices via the lens of invariant groups. As a major motivating example, we apply the proposed general theory to characterize the global geometry of the low-rank matrix factorization problem. In particular, we illustrate how the rotational symmetry group gives rise to infinitely many non-isolated strict saddle points and equivalent global minima of the objective function. By explicitly identifying all stationary points, we divide the entire parameter space into three regions: ($\cR_1$) the region containing the neighborhoods of all strict saddle points, where the objective has negative curvatures; ($\cR_2$) the region containing neighborhoods of all global minima, where the objective enjoys strong convexity along certain directions; and ($\cR_3$) the complement of the above regions, where the gradient has sufficiently large magnitudes. We further extend our result to the matrix sensing problem. This allows us to establish strong global convergence guarantees for popular iterative algorithms with arbitrary initial solutions. version:2
arxiv-1701-00251 | Outlier Robust Online Learning | http://arxiv.org/abs/1701.00251 | id:1701.00251 author:Jiashi Feng, Huan Xu, Shie Mannor category:cs.LG stat.ML  published:2017-01-01 summary:We consider the problem of learning from noisy data in practical settings where the size of data is too large to store on a single machine. More challenging, the data coming from the wild may contain malicious outliers. To address the scalability and robustness issues, we present an online robust learning (ORL) approach. ORL is simple to implement and has provable robustness guarantee -- in stark contrast to existing online learning approaches that are generally fragile to outliers. We specialize the ORL approach for two concrete cases: online robust principal component analysis and online linear regression. We demonstrate the efficiency and robustness advantages of ORL through comprehensive simulations and predicting image tags on a large-scale data set. We also discuss extension of the ORL to distributed learning and provide experimental evaluations. version:1
arxiv-1612-07139 | Deep-learning in Mobile Robotics - from Perception to Control Systems: A Survey on Why and Why not | http://arxiv.org/abs/1612.07139 | id:1612.07139 author:Lei Tai, Ming Liu category:cs.RO cs.AI cs.LG cs.SY  published:2016-12-21 summary:Deep-learning has dramatically changed the world overnight. It greatly boosted the development of visual perception, object detection, and speech recognition, etc. That was attributed to the multiple convolutional processing layers for abstraction of learning representations from massive data. The advantages of deep convolutional structures in data processing motivated the applications of artificial intelligence methods in robotic problems, especially perception and control system, the two typical and challenging problems in robotics. This paper presents a survey of the deep-learning research landscape in mobile robotics. We start with introducing the definition and development of deep-learning in related fields, especially the essential distinctions between image processing and robotic tasks. We described and discussed several typical applications and related works in this domain, followed by the benefits from deep-learning, and related existing frameworks. Besides, operation in the complex dynamic environment is regarded as a critical bottleneck for mobile robots, such as that for autonomous driving. We thus further emphasize the recent achievement on how deep-learning contributes to navigation and control systems for mobile robots. At the end, we discuss the open challenges and research frontiers. version:3
arxiv-1701-00220 | Classification of Smartphone Users Using Internet Traffic | http://arxiv.org/abs/1701.00220 | id:1701.00220 author:Andrey Finkelstein, Ron Biton, Rami Puzis, Asaf Shabtai category:cs.LG cs.CR  published:2017-01-01 summary:Today, smartphone devices are owned by a large portion of the population and have become a very popular platform for accessing the Internet. Smartphones provide the user with immediate access to information and services. However, they can easily expose the user to many privacy risks. Applications that are installed on the device and entities with access to the device's Internet traffic can reveal private information about the smartphone user and steal sensitive content stored on the device or transmitted by the device over the Internet. In this paper, we present a method to reveal various demographics and technical computer skills of smartphone users by their Internet traffic records, using machine learning classification models. We implement and evaluate the method on real life data of smartphone users and show that smartphone users can be classified by their gender, smoking habits, software programming experience, and other characteristics. version:1
arxiv-1701-00754 | Using Artificial Neural Networks (ANN) to Control Chaos | http://arxiv.org/abs/1701.00754 | id:1701.00754 author:Ibrahim Ighneiwaa, Salwa Hamidatoua, Fadia Ben Ismaela category:cs.LG nlin.CD  published:2017-01-01 summary:Controlling Chaos could be a big factor in getting great stable amounts of energy out of small amounts of not necessarily stable resources. By definition, Chaos is getting huge changes in the system's output due to unpredictable small changes in initial conditions, and that means we could take advantage of this fact and select the proper control system to manipulate system's initial conditions and inputs in general and get a desirable output out of otherwise a Chaotic system. That was accomplished by first building some known chaotic circuit (Chua circuit) and the NI's MultiSim was used to simulate the ANN control system. It was shown that this technique can also be used to stabilize some hard to stabilize electronic systems. version:1
arxiv-1701-00198 | A robust approach for tree segmentation in deciduous forests using small-footprint airborne LiDAR data | http://arxiv.org/abs/1701.00198 | id:1701.00198 author:Hamid Hamraz, Marco A. Contreras, Jun Zhang category:cs.CV cs.CE cs.CG  published:2017-01-01 summary:This paper presents a non-parametric approach for segmenting trees from airborne LiDAR data in deciduous forests. Based on the LiDAR point cloud, the approach collects crown information such as steepness and height on-the-fly to delineate crown boundaries, and most importantly, does not require a priori assumptions of crown shape and size. The approach segments trees iteratively starting from the tallest within a given area to the smallest until all trees have been segmented. To evaluate its performance, the approach was applied to the University of Kentucky Robinson Forest, a deciduous closed-canopy forest with complex terrain and vegetation conditions. The approach identified 94% of dominant and co-dominant trees with a false detection rate of 13%. About 62% of intermediate, overtopped, and dead trees were also detected with a false detection rate of 15%. The overall segmentation accuracy was 77%. Correlations of the segmentation scores of the proposed approach with local terrain and stand metrics was not significant, which is likely an indication of the robustness of the approach as results are not sensitive to the differences in terrain and stand structures. version:1
arxiv-1701-00193 | Video-based Person Re-identification with Accumulative Motion Context | http://arxiv.org/abs/1701.00193 | id:1701.00193 author:Hao Liu, Zequn Jie, Karlekar Jayashree, Meibin Qi, Jianguo Jiang, Shuicheng Yan, Jiashi Feng category:cs.CV  published:2017-01-01 summary:Video based person re-identification plays a central role in realistic security and video surveillance. In this paper we propose a novel Accumulative Motion Context (AMOC) network for addressing this important problem, which effectively exploits the long-range motion context for robustly identifying the same person under challenging conditions. Given a video sequence of the same or different persons, the proposed AMOC network jointly learns appearance representation and motion context from a collection of adjacent frames using a two-stream convolutional architecture. Then AMOC accumulates clues from motion context by recurrent aggregation, allowing effective information flow among adjacent frames and capturing dynamic gist of the persons. The architecture of AMOC is end-to-end trainable and thus motion context can be adapted to complement appearance clues under unfavorable conditions (\textit{e.g.}, occlusions). Extensive experiments are conduced on two public benchmark datasets, \textit{i.e.}, the iLIDS-VID and PRID-2011 datasets, to investigate the performance of AMOC. The experimental results demonstrate that the proposed AMOC network outperforms state-of-the-arts for video-based re-identification significantly and confirm the advantage of exploiting long-range motion context for video based person re-identification, validating our motivation evidently. version:1
arxiv-1701-00188 | Aspect-augmented Adversarial Networks for Domain Adaptation | http://arxiv.org/abs/1701.00188 | id:1701.00188 author:Yuan Zhang, Regina Barzilay, Tommi Jaakkola category:cs.CL  published:2017-01-01 summary:We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Instead of target labels, we assume a few keywords pertaining to source and target aspects indicating sentence relevance rather than document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 24% on a pathology dataset and 5% on a review dataset. version:1
arxiv-1701-00185 | Self-Taught Convolutional Neural Networks for Short Text Clustering | http://arxiv.org/abs/1701.00185 | id:1701.00185 author:Jiaming Xu, Bo XuPeng Wang, Suncong Zheng, Guanhua Tian, Jun Zhao, Bo Xu category:cs.IR cs.CL  published:2017-01-01 summary:Short text clustering is a challenging problem due to its sparseness of text representation. Here we propose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering (dubbed STC^2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets. version:1
arxiv-1701-00178 | Lazily Adapted Constant Kinky Inference for Nonparametric Regression and Model-Reference Adaptive Control | http://arxiv.org/abs/1701.00178 | id:1701.00178 author:Jan-Peter Calliess category:math.OC cs.AI cs.LG cs.SY stat.ML  published:2016-12-31 summary:Techniques known as Nonlinear Set Membership prediction, Lipschitz Interpolation or Kinky Inference are approaches to machine learning that utilise presupposed Lipschitz properties to compute inferences over unobserved function values. Provided a bound on the true best Lipschitz constant of the target function is known a priori they offer convergence guarantees as well as bounds around the predictions. Considering a more general setting that builds on Hoelder continuity relative to pseudo-metrics, we propose an online method for estimating the Hoelder constant online from function value observations that possibly are corrupted by bounded observational errors. Utilising this to compute adaptive parameters within a kinky inference rule gives rise to a nonparametric machine learning method, for which we establish strong universal approximation guarantees. That is, we show that our prediction rule can learn any continuous function in the limit of increasingly dense data to within a worst-case error bound that depends on the level of observational uncertainty. We apply our method in the context of nonparametric model-reference adaptive control (MRAC). Across a range of simulated aircraft roll-dynamics and performance metrics our approach outperforms recently proposed alternatives that were based on Gaussian processes and RBF-neural networks. For discrete-time systems, we provide stability guarantees for our learning-based controllers both for the batch and the online learning setting. version:1
arxiv-1701-00169 | Tree segmentation in multi-story stands within small-footprint airborne LiDAR data | http://arxiv.org/abs/1701.00169 | id:1701.00169 author:Hamid Hamraz, Marco A. Contreras, Jun Zhang category:cs.CV cs.CE cs.CG  published:2016-12-31 summary:Airborne LiDAR point cloud of a forest contains three dimensional data, from which vertical stand structure (including information about under-story trees) can be derived. This paper presents a segmentation approach for multi-story stands that strips the point cloud to its canopy layers, identifies individual tree segments within each layer using a DSM-based tree identification method as a building block, and combines the segments of immediate layers in order to fix potential over-segmentation of tree crowns across the layers. We introduce local layering that analyzes the vertical distributions of LiDAR points within their local neighborhoods in order to locally determine the height thresholds for layering the canopy. Unlike the previous work that stripped stiff layers within constrained areas, the local layering method strips flexible (in thickness and elevation) and narrower canopy layers within unconstrained areas. Statistical analyses showed that layering in general strongly improves identifying (specifically under-story) trees for the cost of moderately increasing over-segmentation rate of the identified trees. Combining tree segments across the immediate layers did not seem to improve tree identification accuracy remarkably, suggesting that layers separated canopy layers rather precisely. version:1
arxiv-1701-00168 | Social Media Argumentation Mining: The Quest for Deliberateness in Raucousness | http://arxiv.org/abs/1701.00168 | id:1701.00168 author:Jan Šnajder category:cs.CL  published:2016-12-31 summary:Argumentation mining from social media content has attracted increasing attention. The task is both challenging and rewarding. The informal nature of user-generated content makes the task dauntingly difficult. On the other hand, the insights that could be gained by a large-scale analysis of social media argumentation make it a very worthwhile task. In this position paper I discuss the motivation for social media argumentation mining, as well as the tasks and challenges involved. version:1
arxiv-1701-00167 | Very Fast Kernel SVM under Budget Constraints | http://arxiv.org/abs/1701.00167 | id:1701.00167 author:David Picard category:stat.ML cs.LG  published:2016-12-31 summary:In this paper we propose a fast online Kernel SVM algorithm under tight budget constraints. We propose to split the input space using LVQ and train a Kernel SVM in each cluster. To allow for online training, we propose to limit the size of the support vector set of each cluster using different strategies. We show in the experiment that our algorithm is able to achieve high accuracy while having a very high number of samples processed per second both in training and in the evaluation. version:1
arxiv-1701-00165 | Improved Stereo Matching with Constant Highway Networks and Reflective Confidence Learning | http://arxiv.org/abs/1701.00165 | id:1701.00165 author:Amit Shaked, Lior Wolf category:cs.CV  published:2016-12-31 summary:We present an improved three-step pipeline for the stereo matching problem and introduce multiple novelties at each stage. We propose a new highway network architecture for computing the matching cost at each possible disparity, based on multilevel weighted residual shortcuts, trained with a hybrid loss that supports multilevel comparison of image patches. A novel post-processing step is then introduced, which employs a second deep convolutional neural network for pooling global information from multiple disparities. This network outputs both the image disparity map, which replaces the conventional "winner takes all" strategy, and a confidence in the prediction. The confidence score is achieved by training the network with a new technique that we call the reflective loss. Lastly, the learned confidence is employed in order to better detect outliers in the refinement step. The proposed pipeline achieves state of the art accuracy on the largest and most competitive stereo benchmarks, and the learned confidence is shown to outperform all existing alternatives. version:1
arxiv-1701-00142 | EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras (Extended Abstract) | http://arxiv.org/abs/1701.00142 | id:1701.00142 author:Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov, Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, Christian Theobalt category:cs.CV  published:2016-12-31 summary:Marker-based and marker-less optical skeletal motion-capture methods use an outside-in arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort by possibly needed marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds. We therefore propose a new method for real-time, marker-less and egocentric motion capture which estimates the full-body skeleton pose from a lightweight stereo pair of fisheye cameras that are attached to a helmet or virtual-reality headset. It combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a new automatically annotated and augmented dataset. Our inside-in method captures full-body motion in general indoor and outdoor scenes, and also crowded scenes. version:1
arxiv-1701-00138 | RNN-based Encoder-decoder Approach with Word Frequency Estimation | http://arxiv.org/abs/1701.00138 | id:1701.00138 author:Jun Suzuki, Masaaki Nagata category:cs.CL cs.AI stat.ML  published:2016-12-31 summary:This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. version:1
arxiv-1701-00736 | Simulated Tornado Optimization | http://arxiv.org/abs/1701.00736 | id:1701.00736 author:S. Hossein Hosseini, Tohid Nouri, Afshin Ebrahimi, S. Ali Hosseini category:math.OC cs.AI cs.NE  published:2016-12-31 summary:We propose a swarm-based optimization algorithm inspired by air currents of a tornado. Two main air currents - spiral and updraft - are mimicked. Spiral motion is designed for exploration of new search areas and updraft movements is deployed for exploitation of a promising candidate solution. Assignment of just one search direction to each particle at each iteration, leads to low computational complexity of the proposed algorithm respect to the conventional algorithms. Regardless of the step size parameters, the only parameter of the proposed algorithm, called tornado diameter, can be efficiently adjusted by randomization. Numerical results over six different benchmark cost functions indicate comparable and, in some cases, better performance of the proposed algorithm respect to some other metaheuristics. version:1
arxiv-1701-00066 | A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP Tools Contest Entry from Surukam | http://arxiv.org/abs/1701.00066 | id:1701.00066 author:Sree Harsha Ramesh, Raveena R Kumar category:cs.CL  published:2016-12-31 summary:Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a particularly challenging problem in computational linguistics due to a dearth of accurately annotated training corpora. ICON, as part of its NLP tools contest has organized this challenge as a shared task for the second consecutive year to improve the state-of-the-art. This paper describes the POS tagger built at Surukam to predict the coarse-grained and fine-grained POS tags for three language pairs - Bengali-English, Telugu-English and Hindi-English, with the text spanning three popular social media platforms - Facebook, WhatsApp and Twitter. We employed Conditional Random Fields as the sequence tagging algorithm and used a library called sklearn-crfsuite - a thin wrapper around CRFsuite for training our model. Among the features we used include - character n-grams, language information and patterns for emoji, number, punctuation and web-address. Our submissions in the constrained environment,i.e., without making any use of monolingual POS taggers or the like, obtained an overall average F1-score of 76.45%, which is comparable to the 2015 winning score of 76.79%. version:1
arxiv-1701-00040 | p-DLA: A Predictive System Model for Onshore Oil and Gas Pipeline Dataset Classification and Monitoring - Part 1 | http://arxiv.org/abs/1701.00040 | id:1701.00040 author:E. N. Osegi category:cs.CV  published:2016-12-31 summary:With the rise in militant activity and rogue behaviour in oil and gas regions around the world, oil pipeline disturbances is on the increase leading to huge losses to multinational operators and the countries where such facilities exist. However, this situation can be averted if adequate predictive monitoring schemes are put in place. We propose in the first part of this paper, an artificial intelligence predictive monitoring system capable of predictive classification and pattern recognition of pipeline datasets. The predictive system is based on a highly sparse predictive Deviant Learning Algorithm (p-DLA) designed to synthesize a sequence of memory predictive clusters for eventual monitoring, control and decision making. The DLA (p-DLA) is compared with a popular machine learning algorithm, the Long Short-Term Memory (LSTM) which is based on a temporal version of the standard feed-forward back-propagation trained artificial neural networks (ANNs). The results of simulations study show impressive results and validates the sparse memory predictive approach which favours the sub-synthesis of a highly compressed and low dimensional knowledge discovery and information prediction scheme. It also shows that the proposed new approach is competitive with a well-known and proven AI approach such as the LSTM. version:1
arxiv-1701-00031 | Super-Resolution Reconstruction of Electrical Impedance Tomography Images | http://arxiv.org/abs/1701.00031 | id:1701.00031 author:R. A. Borsoi, J. C. C. Aya, G. H. Costa, J. C. M. Bermudez category:cs.CV  published:2016-12-30 summary:Electrical Impedance Tomography (EIT) systems are becoming popular because they present several advantages over competing systems. However, EIT leads to images with very low resolution. Moreover, the nonuniform sampling characteristic of EIT precludes the straightforward application of traditional image ruper-resolution techniques. In this work, we propose a resampling based Super-Resolution method for EIT image quality improvement. Preliminary results show that the proposed technique can lead to substantial improvements in EIT image resolution, making it more competitive with other technologies. version:1
arxiv-1612-09596 | Counterfactual Prediction with Deep Instrumental Variables Networks | http://arxiv.org/abs/1612.09596 | id:1612.09596 author:Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy category:stat.AP cs.LG stat.ML  published:2016-12-30 summary:We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine. version:1
arxiv-1612-09574 | Automatic Data Deformation Analysis on Evolving Folksonomy Driven Environment | http://arxiv.org/abs/1612.09574 | id:1612.09574 author:Massimiliano Dal Mas category:cs.IR cs.CL cs.CY cs.SI  published:2016-12-30 summary:The Folksodriven framework makes it possible for data scientists to define an ontology environment where searching for buried patterns that have some kind of predictive power to build predictive models more effectively. It accomplishes this through an abstractions that isolate parameters of the predictive modeling process searching for patterns and designing the feature set, too. To reflect the evolving knowledge, this paper considers ontologies based on folksonomies according to a new concept structure called "Folksodriven" to represent folksonomies. So, the studies on the transformational regulation of the Folksodriven tags are regarded to be important for adaptive folksonomies classifications in an evolving environment used by Intelligent Systems to represent the knowledge sharing. Folksodriven tags are used to categorize salient data points so they can be fed to a machine-learning system and "featurizing" the data. version:1
arxiv-1612-09548 | A Unified Tensor-based Active Appearance Face Model | http://arxiv.org/abs/1612.09548 | id:1612.09548 author:Zhen-Hua Feng, Josef Kittler, William Christmas, Xiao-Jun Wu category:cs.CV  published:2016-12-30 summary:Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. In contrast with the classical Tensor-based AAM (T-AAM), the proposed UT-AAM has four advantages: First, for each type of face information, namely shape and texture, we construct a tensor model capturing all relevant appearance variations. This unified tensor model contrasts with the variation-specific models of T-AAM. Second, a strategy for dealing with self-occluded faces is proposed to obtain consistent shape and texture representations of faces across large pose variations. Third, our UT-AAM is capable of constructing the model from an incomplete training dataset, using tensor completion methods. Last, we use an effective cascaded-regression-based method for UT-AAM fitting. With these improvements, the utility of UT-AAM in practice is considerably enhanced in comparison with the classical T-AAM. As an example, we demonstrate the improvements in training facial landmark detectors through the use of UT-AAM to synthesise a large number of virtual samples. Experimental results obtained using the Multi-PIE and 300-W face datasets demonstrate the merits of the proposed approach. version:1
arxiv-1612-08714 | Clustering with Confidence: Finding Clusters with Statistical Guarantees | http://arxiv.org/abs/1612.08714 | id:1612.08714 author:Andreas Henelius, Kai Puolamäki, Henrik Boström, Panagiotis Papapetrou category:stat.ML cs.LG  published:2016-12-27 summary:Clustering is a widely used unsupervised learning method for finding structure in the data. However, the resulting clusters are typically presented without any guarantees on their robustness; slightly changing the used data sample or re-running a clustering algorithm involving some stochastic component may lead to completely different clusters. There is, hence, a need for techniques that can quantify the instability of the generated clusters. In this study, we propose a technique for quantifying the instability of a clustering solution and for finding robust clusters, termed core clusters, which correspond to clusters where the co-occurrence probability of each data item within a cluster is at least $1 - \alpha$. We demonstrate how solving the core clustering problem is linked to finding the largest maximal cliques in a graph. We show that the method can be used with both clustering and classification algorithms. The proposed method is tested on both simulated and real datasets. The results show that the obtained clusters indeed meet the guarantees on robustness. version:2
arxiv-1612-09542 | A Joint Speaker-Listener-Reinforcer Model for Referring Expressions | http://arxiv.org/abs/1612.09542 | id:1612.09542 author:Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg category:cs.CV cs.AI cs.CL  published:2016-12-30 summary:Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer version:1
arxiv-1612-09535 | PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese | http://arxiv.org/abs/1612.09535 | id:1612.09535 author:Conceição Rocha, Alípio Jorge, Roberta Sionara, Paula Brito, Carlos Pimenta, Solange Rezende category:cs.IR cs.CL  published:2016-12-30 summary:This paper deals with the entity extraction task (named entity recognition) of a text mining process that aims at unveiling non-trivial semantic structures, such as relationships and interaction between entities or communities. In this paper we present a simple and efficient named entity extraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging based algorithm for NER), relies on flexible pattern matching, part-of-speech tagging and lexical-based rules. It was developed to process texts written in Portuguese, however it is potentially applicable to other languages as well. We compare our approach with current alternatives that support Named Entity Recognition (NER) for content written in Portuguese. These are Alchemy, Zemanta and Rembrandt. Evaluation of the efficacy of the entity extraction method on several texts written in Portuguese indicates a considerable improvement on $recall$ and $F_1$ measures. version:1
arxiv-1612-09506 | Adult Content Recognition from Images Using a Mixture of Convolutional Neural Networks | http://arxiv.org/abs/1612.09506 | id:1612.09506 author:Mundher Al-Shabi, Tee Connie, Andrew Beng Jin Teoh category:stat.ML cs.CV cs.NE  published:2016-12-30 summary:With rapid development of the Internet, the web contents become huge. Most of the websites are publicly available and anyone can access the contents everywhere such as workplace, home and even schools. Nev-ertheless, not all the web contents are appropriate for all users, especially children. An example of these contents is pornography images which should be restricted to certain age group. Besides, these images are not safe for work (NSFW) in which employees should not be seen accessing such contents. Recently, convolutional neural networks have been successfully applied to many computer vision problems. Inspired by these successes, we propose a mixture of convolutional neural networks for adult content recognition. Unlike other works, our method is formulated on a weighted sum of multiple deep neural network models. The weights of each CNN models are expressed as a linear regression problem learnt using Ordinary Least Squares (OLS). Experimental results demonstrate that the proposed model outperforms both single CNN model and the average sum of CNN models in adult content recognition. version:1
arxiv-1612-08185 | Deep Probabilistic Modeling of Natural Images using a Pyramid Decomposition | http://arxiv.org/abs/1612.08185 | id:1612.08185 author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV  published:2016-12-24 summary:We introduce a new technique for probabilistic modeling of natural images that combines the advantages of classic multi-scale and modern deep learning models. By explicitly representing natural images at different scales we derive a model that can capture high level image structure in a computationally efficient way. We show experimentally that our model achieves new state-of-the-art image modeling performance on the CIFAR-10 dataset and at the same time is much faster than competitive models. We also evaluate the proposed technique on a human faces dataset and demonstrate the potential of our model to generate nearly photorealistic face samples. version:2
arxiv-1612-09466 | Double Coupled Canonical Polyadic Decomposition for Joint Blind Source Separation | http://arxiv.org/abs/1612.09466 | id:1612.09466 author:Xiao-Feng Gong, Qiu-Hua Lin, Feng-Yu Cong, Lieven De Lathauwer category:stat.ML  published:2016-12-30 summary:Joint blind source separation (J-BSS) is an emerging data-driven technique for multi-set data-fusion. In this paper, J-BSS is addressed from a tensorial perspective. We show how, by using second-order multi-set statistics in J-BSS, a specific double coupled canonical polyadic decomposition (DC-CPD) problem can be formulated. We propose an algebraic DC-CPD algorithm based on a coupled rank-1 detection mapping. This algorithm converts a possibly underdetermined DC-CPD to a set of overdetermined CPDs. The latter can be solved algebraically via a generalized eigenvalue decomposition based scheme. Therefore, this algorithm is deterministic and returns the exact solution in the noiseless case. In the noisy case, it can be used to effectively initialize optimization based DC-CPD algorithms. In addition, we obtain the determini- stic and generic uniqueness conditions for DC-CPD, which are shown to be more relaxed than their CPD counterparts. Experiment results are given to illustrate the superiority of DC- CPD over standard CPD with regards to uniqueness and accuracy. version:1
arxiv-1612-09465 | Adaptive Lambda Least-Squares Temporal Difference Learning | http://arxiv.org/abs/1612.09465 | id:1612.09465 author:Timothy A. Mann, Hugo Penedones, Shie Mannor, Todd Hester category:cs.LG cs.AI stat.ML  published:2016-12-30 summary:Temporal Difference learning or TD($\lambda$) is a fundamental algorithm in the field of reinforcement learning. However, setting TD's $\lambda$ parameter, which controls the timescale of TD updates, is generally left up to the practitioner. We formalize the $\lambda$ selection problem as a bias-variance trade-off where the solution is the value of $\lambda$ that leads to the smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the space of $\lambda$ values. Unfortunately, this approach is too computationally expensive for most practical applications. For Least Squares TD (LSTD) we show that LOTO-CV can be implemented efficiently to automatically tune $\lambda$ and apply function optimization methods to efficiently search the space of $\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our experiments demonstrate that ALLSTD is significantly computationally faster than the na\"{i}ve LOTO-CV implementation while achieving similar performance. version:1
arxiv-1701-05084 | Analysis of the noise in back-projection light field acquisition and its optimization | http://arxiv.org/abs/1701.05084 | id:1701.05084 author:Ni Chen, Zhenbo Ren, Dayan Li, Edmund Y. Lam, Guohai Situ category:cs.CV physics.optics  published:2016-12-30 summary:Light field reconstruction from images captured by focal plane sweeping can achieve high lateral resolution comparable to the modern camera sensor. This is impossible for the conventional micro-lenslet based light field capture systems. However, the severe defocus noise and the low depth resolution limit its applications. In this paper, we analyze the defocus noise and the depth resolution in the focal plane sweeping based light field reconstruction technique, and propose a method to reduce the defocus noise and improve the depth resolution. Both numerical and experimental results verify the proposed method. version:1
