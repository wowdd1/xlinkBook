arxiv-1511-03776 | ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural Networks | http://arxiv.org/abs/1511.03776 | id:1511.03776 author:Chen Sun, Manohar Paluri, Ronan Collobert, Ram Nevatia, Lubomir Bourdev category:cs.CV  published:2015-11-12 summary:This paper aims to classify and locate objects accurately and efficiently, without using bounding box annotations. It is challenging as objects in the wild could appear at arbitrary locations and in different scales. In this paper, we propose a novel classification architecture ProNet based on convolutional neural networks. It uses computationally efficient neural networks to propose image regions that are likely to contain objects, and applies more powerful but slower networks on the proposed regions. The basic building block is a multi-scale fully-convolutional network which assigns object confidence scores to boxes at different locations and scales. We show that such networks can be trained effectively using image-level annotations, and can be connected into cascades or trees for efficient object classification. ProNet outperforms previous state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets for object classification and point-based localization. version:3
arxiv-1604-03635 | Online Multi-target Tracking using Recurrent Neural Networks | http://arxiv.org/abs/1604.03635 | id:1604.03635 author:Anton Milan, Seyed Hamid Rezatofighi, Anthony Dick, Konrad Schindler, Ian Reid category:cs.CV  published:2016-04-13 summary:We present a novel approach to online multi-target tracking based on recurrent neural networks (RNNs). Tracking multiple objects in real-world scenes involves many challenges, including a) an a-priori unknown and time-varying number of targets, b) a continuous state estimation of all present targets, and c) a discrete combinatorial problem of data association. Most previous methods involve complex models that require tedious tuning of parameters. Here, we propose for the first time, a full end-to-end learning approach for online multi-target tracking based on deep learning. Existing deep learning methods are not designed for the above challenges and cannot be trivially applied to the task. Our solution addresses all of the above points in a principled way. Experiments on both synthetic and real data show competitive results obtained at 300 Hz on a standard CPU, and pave the way towards future research in this direction. version:1
arxiv-1604-03629 | Quantifying mesoscale neuroanatomy using X-ray microtomography | http://arxiv.org/abs/1604.03629 | id:1604.03629 author:Eva L. Dyer, William Gray Roncal, Hugo L. Fernandes, Doga Gürsoy, Xianghui Xiao, Joshua T. Vogelstein, Chris Jacobsen, Konrad P. Körding, Narayanan Kasthuri category:q-bio.QM cs.CV  published:2016-04-13 summary:Common methods for imaging the 3D microstructure of the brain often require slicing the brain, imaging these slices, and stitching the images back together. In contrast, X-rays allow access into centimeter-thick samples without sectioning, providing an unique and largely untapped approach for producing large 3D mesoscale brain maps. Here we demonstrate the use of synchrotron X-ray microtomography ($\mu$CT) for brain science and provide a much needed toolkit for analyzing the large datasets afforded by this approach. We introduce methods for sample preparation, imaging, and analyzing the resultant neural structures. This pipeline provides methods for automated cell detection and segmentation of the vasculature, in addition to large-scale analyses of the spatial statistics of cells and blood vessels. We applied our methods to produce dense micron-scale maps of the cells and blood vessels in a millimeter-scale volume of mouse brain with $\mu$CT. Our results demonstrate that X-ray tomography promises rapid reconstructions over brain large volumes, in a way that is complementary to other brain mapping and connectomics efforts. version:1
arxiv-1604-03627 | Dissecting a Social Botnet: Growth, Content and Influence in Twitter | http://arxiv.org/abs/1604.03627 | id:1604.03627 author:Norah Abokhodair, Daisy Yoo, David W. McDonald category:cs.CY cs.CL cs.SI  published:2016-04-13 summary:Social botnets have become an important phenomenon on social media. There are many ways in which social bots can disrupt or influence online discourse, such as, spam hashtags, scam twitter users, and astroturfing. In this paper we considered one specific social botnet in Twitter to understand how it grows over time, how the content of tweets by the social botnet differ from regular users in the same dataset, and lastly, how the social botnet may have influenced the relevant discussions. Our analysis is based on a qualitative coding for approximately 3000 tweets in Arabic and English from the Syrian social bot that was active for 35 weeks on Twitter before it was shutdown. We find that the growth, behavior and content of this particular botnet did not specifically align with common conceptions of botnets. Further we identify interesting aspects of the botnet that distinguish it from regular users. version:1
arxiv-1509-08970 | Energy-Efficient Object Detection using Semantic Decomposition | http://arxiv.org/abs/1509.08970 | id:1509.08970 author:Priyadarshini Panda, Swagath Venkataramani, Abhronil Sengupta, Anand Raghunathan, Kaushik Roy category:cs.CV  published:2015-09-29 summary:Machine-learning algorithms offer immense possibilities in the development of several cognitive applications. In fact, large scale machine-learning classifiers now represent the state-of-the-art in a wide range of object detection/classification problems. However, the network complexities of large-scale classifiers present them as one of the most challenging and energy intensive workloads across the computing spectrum. In this paper, we present a new approach to optimize energy efficiency of object detection tasks using semantic decomposition to build a hierarchical classification framework. We observe that certain semantic information like color/texture are common across various images in real-world datasets for object detection applications. We exploit these common semantic features to distinguish the objects of interest from the remaining inputs (non-objects of interest) in a dataset at a lower computational effort. We propose a 2-stage hierarchical classification framework, with increasing levels of complexity, wherein the first stage is trained to recognize the broad representative semantic features relevant to the object of interest. The first stage rejects the input instances that do not have the representative features and passes only the relevant instances to the second stage. Our methodology thus allows us to reject certain information at lower complexity and utilize the full computational effort of a network only on a smaller fraction of inputs to perform detection. We use color and texture as distinctive traits to carry out several experiments for object detection. Our experiments on the Caltech101/CIFAR10 dataset show that the proposed method yields 1.93x/1.46x improvement in average energy, respectively, over the traditional single classifier model. version:2
arxiv-1603-09742 | Object Boundary Guided Semantic Segmentation | http://arxiv.org/abs/1603.09742 | id:1603.09742 author:Qin Huang, Chunyang Xia, Wenchao Zheng, Yuhang Song, Hao Xu, C. -C. Jay Kuo category:cs.CV  published:2016-03-31 summary:Semantic segmentation is crucial for understanding image contents and object localizations. Recent development in fully-convolutional neural network (FCN) has enabled accurate pixel-level labeling with finer results. Although different constraints have been applied to help delineate segmentation details, previous methods majorly focus on differentiating the surface patterns of different object classes and merge those with similar properties, which consequently deprive the FCN based segmentation of its ability to understand and recognize the object. To tackle with this major shortage, we introduce a double branch network, which not only learns about the object class for each region, but also acquires the knowledge to set up object boundaries so that a more accurate segmentation of object class and finer details could be achieved . To this end, we relabel the object contours based on ground truth of object labeling and use the FCN network to specially learn a three-class branch, which is later used as a mask layer combing with original 21-class FCN. This network, called object boundary guided FCN (OBG-FCN) is then going through an end-to-end training, which refines the details of segmentation boundaries and class accuracies. We apply the proposed method in the PASCAL VOC segmentation benchmark, and have achieved state-of-the-art performance. Our pre-trained edge model has shown to be stable and accurate even at accuracy level of FCN-2s. version:3
arxiv-1604-03605 | What do different evaluation metrics tell us about saliency models? | http://arxiv.org/abs/1604.03605 | id:1604.03605 author:Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba, Frédo Durand category:cs.CV  published:2016-04-12 summary:How best to evaluate a saliency model's ability to predict where humans look in images is an open research question. The choice of evaluation metric depends on how saliency is defined and how the ground truth is represented. Metrics differ in how they rank saliency models, and this results from how false positives and false negatives are treated, whether viewing biases are accounted for, whether spatial deviations are factored in, and how the saliency maps are pre-processed. In this paper, we provide an analysis of 8 different evaluation metrics and their properties. With the help of systematic experiments and visualizations of metric computations, we add interpretability to saliency scores and more transparency to the evaluation of saliency models. Building off the differences in metric properties and behaviors, we make recommendations for metric selections under specific assumptions and for specific applications. version:1
arxiv-1604-03601 | Community Detection with Node Attributes and its Generalization | http://arxiv.org/abs/1604.03601 | id:1604.03601 author:Yuan Li category:cs.SI physics.soc-ph stat.ML  published:2016-04-12 summary:Community detection algorithms are fundamental tools to understand organizational principles in social networks. With the increasing power of social media platforms, when detecting communities there are two possi- ble sources of information one can use: the structure of social network and node attributes. However structure of social networks and node attributes are often interpreted separately in the research of community detection. When these two sources are interpreted simultaneously, one common as- sumption shared by previous studies is that nodes attributes are correlated with communities. In this paper, we present a model that is capable of combining topology information and nodes attributes information with- out assuming correlation. This new model can recover communities with higher accuracy even when node attributes and communities are uncorre- lated. We derive the detectability threshold for this model and use Belief Propagation (BP) to make inference. This algorithm is optimal in the sense that it can recover community all the way down to the threshold. This new model is also with the potential to handle edge content and dynamic settings. version:1
arxiv-1511-07111 | Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments | http://arxiv.org/abs/1511.07111 | id:1511.07111 author:Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, Trevor Darrell category:cs.CV  published:2015-11-23 summary:We address the problem of adapting robotic perception from simulated to real-world environments. For many robotic control tasks, real training imagery is expensive to obtain, but a large number of synthetic images is easy to generate through simulation. We propose a method that adapts visual representations using a small number of paired synthetic and real views of the same scene. Our model generalizes prior approaches and combines a standard in-domain loss, a cross-domain adaptation loss, and a contrastive loss explicitly designed to align pairs of images in feature space. We presume a synthetic dataset comprised of views that are a superset of a small number of real views, where the alignment may be either explicit or latent. We evaluate our approach on a manipulation task and show that by exploiting the presence of synthetic-real image pairs, our model is able to compensate for domain shift more effectively than conventional initialization techniques. Our results serve as an initial step toward pretraining deep visuomotor policies entirely in simulation, significantly reducing physical demands when learning complex policies. version:3
arxiv-1511-01966 | Enhanced Low-Rank Matrix Approximation | http://arxiv.org/abs/1511.01966 | id:1511.01966 author:Ankit Parekh, Ivan W. Selesnick category:cs.CV cs.LG math.OC  published:2015-11-06 summary:This letter proposes to estimate low-rank matrices by formulating a convex optimization problem with non-convex regularization. We employ parameterized non-convex penalty functions to estimate the non-zero singular values more accurately than the nuclear norm. A closed-form solution for the global optimum of the proposed objective function (sum of data fidelity and the non-convex regularizer) is also derived. The solution reduces to singular value thresholding method as a special case. The proposed method is demonstrated for image denoising. version:4
arxiv-1512-06974 | Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels | http://arxiv.org/abs/1512.06974 | id:1512.06974 author:Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, Ross Girshick category:cs.CV  published:2015-12-22 summary:When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy "human-centric" annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting "what's in the image" versus "what's worth saying." We demonstrate the algorithm's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases. version:2
arxiv-1604-03540 | Training Region-based Object Detectors with Online Hard Example Mining | http://arxiv.org/abs/1604.03540 | id:1604.03540 author:Abhinav Shrivastava, Abhinav Gupta, Ross Girshick category:cs.CV cs.LG  published:2016-04-12 summary:The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been -- detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively. version:1
arxiv-1604-03539 | Cross-stitch Networks for Multi-task Learning | http://arxiv.org/abs/1604.03539 | id:1604.03539 author:Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert category:cs.CV cs.LG  published:2016-04-12 summary:Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multi-task learning. Specifically, we propose a new sharing unit: "cross-stitch" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples. version:1
arxiv-1603-09025 | Recurrent Batch Normalization | http://arxiv.org/abs/1603.09025 | id:1603.09025 author:Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville category:cs.LG  published:2016-03-30 summary:We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization. version:4
arxiv-1604-03519 | Contextual Deep CNN Based Hyperspectral Classification | http://arxiv.org/abs/1604.03519 | id:1604.03519 author:Hyungtae Lee, Heesung Kwon category:cs.CV cs.LG  published:2016-04-12 summary:In this paper, we describe a novel deep convolutional neural networks (CNN) based approach called contextual deep CNN that can jointly exploit spatial and spectral features for hyperspectral image classification. The contextual deep CNN first concurrently applies multiple 3-dimensional local convolutional filters with different sizes jointly exploiting spatial and spectral features of a hyperspectral image. The initial spatial and spectral feature maps obtained from applying the variable size convolutional filters are then combined together to form a joint spatio-spectral feature map. The joint feature map representing rich spectral and spatial properties of the hyperspectral image is then fed through fully convolutional layers that eventually predict the corresponding label of each pixel vector. The proposed approach is tested on the Indian Pines data and performance comparison shows enhanced classification performance of the proposed approach over the current state of the art. version:1
arxiv-1604-03518 | DTM: Deformable Template Matching | http://arxiv.org/abs/1604.03518 | id:1604.03518 author:Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William D. Nothwang category:cs.CV  published:2016-04-12 summary:A novel template matching algorithm that can incorporate the concept of deformable parts, is presented in this paper. Unlike the deformable part model (DPM) employed in object recognition, the proposed template-matching approach called Deformable Template Matching (DTM) does not require a training step. Instead, deformation is achieved by a set of predefined basic rules (e.g. the left sub-patch cannot pass across the right patch). Experimental evaluation of this new method using the PASCAL VOC 07 dataset demonstrated substantial performance improvement over conventional template matching algorithms. Additionally, to confirm the applicability of DTM, the concept is applied to the generation of a rotation-invariant SIFT descriptor. Experimental evaluation employing deformable matching of SIFT features shows an increased number of matching features compared to a conventional SIFT matching. version:1
arxiv-1604-03517 | Fast Object Localization Using a CNN Feature Map Based Multi-Scale Search | http://arxiv.org/abs/1604.03517 | id:1604.03517 author:Hyungtae Lee, Heesung Kwon, Archith J. Bency, William D. Nothwang category:cs.CV  published:2016-04-12 summary:Object localization is an important task in computer vision but requires a large amount of computational power due mainly to an exhaustive multiscale search on the input image. In this paper, we describe a near real-time multiscale search on a deep CNN feature map that does not use region proposals. The proposed approach effectively exploits local semantic information preserved in the feature map of the outermost convolutional layer. A multi-scale search is performed on the feature map by processing all the sub-regions of different sizes using separate expert units of fully connected layers. Each expert unit receives as input local semantic features only from the corresponding sub-regions of a specific geometric shape. Therefore, it contains more nearly optimal parameters tailored to the corresponding shape. This multi-scale and multi-aspect ratio scanning strategy can effectively localize a potential object of an arbitrary size. The proposed approach is fast and able to localize objects of interest with a frame rate of 4 fps while providing improved detection performance over the state-of-the art on the PASCAL VOC 12 and MSCOCO data sets. version:1
arxiv-1604-03513 | Full Flow: Optical Flow Estimation By Global Optimization over Regular Grids | http://arxiv.org/abs/1604.03513 | id:1604.03513 author:Qifeng Chen, Vladlen Koltun category:cs.CV  published:2016-04-12 summary:We present a global optimization approach to optical flow estimation. The approach optimizes a classical optical flow objective over the full space of mappings between discrete grids. No descriptor matching is used. The highly regular structure of the space of mappings enables optimizations that reduce the computational complexity of the algorithm's inner loop from quadratic to linear and support efficient matching of tens of thousands of nodes to tens of thousands of displacements. We show that one-shot global optimization of a classical Horn-Schunck-type objective over regular grids at a single resolution is sufficient to initialize continuous interpolation and achieve state-of-the-art performance on challenging modern benchmarks. version:1
arxiv-1604-03506 | An Unbiased Data Collection and Content Exploitation/Exploration Strategy for Personalization | http://arxiv.org/abs/1604.03506 | id:1604.03506 author:Liangjie Hong, Adnan Boz category:cs.IR cs.LG  published:2016-04-12 summary:One of missions for personalization systems and recommender systems is to show content items according to users' personal interests. In order to achieve such goal, these systems are learning user interests over time and trying to present content items tailoring to user profiles. Recommending items according to users' preferences has been investigated extensively in the past few years, mainly thanks for the popularity of Netflix competition. In a real setting, users may be attracted by a subset of those items and interact with them, only leaving partial feedbacks to the system to learn in the next cycle, which leads to significant biases into systems and hence results in a situation where user engagement metrics cannot be improved over time. The problem is not just for one component of the system. The data collected from users is usually used in many different tasks, including learning ranking functions, building user profiles and constructing content classifiers. Once the data is biased, all these downstream use cases would be impacted as well. Therefore, it would be beneficial to gather unbiased data through user interactions. Traditionally, unbiased data collection is done through showing items uniformly sampling from the content pool. However, this simple scheme is not feasible as it risks user engagement metrics and it takes long time to gather user feedbacks. In this paper, we introduce a user-friendly unbiased data collection framework, by utilizing methods developed in the exploitation and exploration literature. We discuss how the framework is different from normal multi-armed bandit problems and why such method is needed. We layout a novel Thompson sampling for Bernoulli ranked-list to effectively balance user experiences and data collection. The proposed method is validated from a real bucket test and we show strong results comparing to old algorithms version:1
arxiv-1604-03505 | Counting Everyday Objects in Everyday Scenes | http://arxiv.org/abs/1604.03505 | id:1604.03505 author:Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath RS, Dhruv Batra, Devi Parikh category:cs.CV  published:2016-04-12 summary:We introduce the problem of counting everyday objects in everyday scenes. While previous works have studied specific counting problems such as pedestrian counting in surveillance videos, or biological cell counting, we are interested in counting common objects in natural scenes. We study this problem in a setup similar to traditional scene understanding problems. Given an image, we consider the task of predicting the counts (or the numerosity) of categories of interest. We study some simple approaches and applications for this counting problem. Our detect approach adapts an object detector to perform counting, while our glance approach regresses to ground truth counts. Our associative subitizing (aso-sub) approach divides an image into regions and regresses to fractional object counts in each region. We create an ensemble (ens) of these counting methods which improves performance. We demonstrate counting performance on the PASCAL and MS COCO datasets. We show proof-of-concept applications of our automatic counting methods to 1) improve object detection performance, and 2) visual question answering (on VQA and COCO-QA). Our code and datasets will be publicly available. version:1
arxiv-1604-03498 | GPU-FV: Realtime Fisher Vector and Its Applications in Video Monitoring | http://arxiv.org/abs/1604.03498 | id:1604.03498 author:Wenying Ma, Liangliang Cao, Lei Yu, Guoping Long, Yucheng Li category:cs.CV  published:2016-04-12 summary:Fisher vector has been widely used in many multimedia retrieval and visual recognition applications with good performance. However, the computation complexity prevents its usage in real-time video monitoring. In this work, we proposed and implemented GPU-FV, a fast Fisher vector extraction method with the help of modern GPUs. The challenge of implementing Fisher vector on GPUs lies in the data dependency in feature extraction and expensive memory access in Fisher vector computing. To handle these challenges, we carefully designed GPU-FV in a way that utilizes the computing power of GPU as much as possible, and applied optimizations such as loop tiling to boost the performance. GPU-FV is about 12 times faster than the CPU version, and 50\% faster than a non-optimized GPU implementation. For standard video input (320*240), GPU-FV can process each frame within 34ms on a model GPU. Our experiments show that GPU-FV obtains a similar recognition accuracy as traditional FV on VOC 2007 and Caltech 256 image sets. We also applied GPU-FV for realtime video monitoring tasks and found that GPU-FV outperforms a number of previous works. Especially, when the number of training examples are small, GPU-FV outperforms the recent popular deep CNN features borrowed from ImageNet. The code can be downloaded from the following link https://bitbucket.org/mawenjing/gpu-fv. version:1
arxiv-1604-03492 | Structured Matrix Recovery via the Generalized Dantzig Selector | http://arxiv.org/abs/1604.03492 | id:1604.03492 author:Sheng Chen, Arindam Banerjee category:stat.ML  published:2016-04-12 summary:In recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure, and limited progress has been made matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector under generic sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures of suitable sets which only depend on the structure of the underlying true matrix. In addition, we derive the general bounds on these geometric measures for structures characterized by unitarily invariant norms, which is a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development. version:1
arxiv-1512-06927 | A C++ library for Multimodal Deep Learning | http://arxiv.org/abs/1512.06927 | id:1512.06927 author:Jian Jin category:cs.LG  published:2015-12-22 summary:MDL, Multimodal Deep Learning Library, is a deep learning framework that supports multiple models, and this document explains its philosophy and functionality. MDL runs on Linux, Mac, and Unix platforms. It depends on OpenCV. version:4
arxiv-1511-04196 | Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition | http://arxiv.org/abs/1511.04196 | id:1511.04196 author:Zhiwei Deng, Arash Vahdat, Hexiang Hu, Greg Mori category:cs.CV  published:2015-11-13 summary:Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks. version:2
arxiv-1604-03489 | From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment Prediction | http://arxiv.org/abs/1604.03489 | id:1604.03489 author:Victor Campos, Brendan Jou, Xavier Giro-i-Nieto category:cs.CV cs.MM  published:2016-04-12 summary:Visual media have become a crucial part of our social lives. The throughput of generated multimedia content, together with its richness for conveying sentiments and feelings, highlights the need of automated visual sentiment analysis tools. We explore how Convolutional Neural Networks (CNNs), a computational learning paradigm that has shown outstanding performance in several vision tasks, can be applied to the task of visual sentiment prediction by fine-tuning a state-of-the-art CNN. We analyze its architecture, studying several performance boosting techniques, which led to a network tuned to achieve a 6.1 % absolute accuracy improvement over the previous state-of-the-art on a dataset of images from a popular social media platform. Finally, we present visualizations of local patterns that the network associates to each image's sentiment. version:1
arxiv-1511-05197 | Visualizing and Understanding Deep Texture Representations | http://arxiv.org/abs/1511.05197 | id:1511.05197 author:Tsung-Yu Lin, Subhransu Maji category:cs.CV  published:2015-11-16 summary:A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these models represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model [25] is an excellent general-purpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work [13, 28] we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at http://vis-www.cs.umass.edu/texture version:2
arxiv-1604-03463 | The Matrix Generalized Inverse Gaussian Distribution: Properties and Applications | http://arxiv.org/abs/1604.03463 | id:1604.03463 author:Farideh Fazayeli, Arindam Banerjee category:stat.ML  published:2016-04-12 summary:While the Matrix Generalized Inverse Gaussian ($\mathcal{MGIG}$) distribution arises naturally in some settings as a distribution over symmetric positive semi-definite matrices, certain key properties of the distribution and effective ways of sampling from the distribution have not been carefully studied. In this paper, we show that the $\mathcal{MGIG}$ is unimodal, and the mode can be obtained by solving an Algebraic Riccati Equation (ARE) equation [7]. Based on the property, we propose an importance sampling method for the $\mathcal{MGIG}$ where the mode of the proposal distribution matches that of the target. The proposed sampling method is more efficient than existing approaches [32, 33], which use proposal distributions that may have the mode far from the $\mathcal{MGIG}$'s mode. Further, we illustrate that the the posterior distribution in latent factor models, such as probabilistic matrix factorization (PMF) [25], when marginalized over one latent factor has the $\mathcal{MGIG}$ distribution. The characterization leads to a novel Collapsed Monte Carlo (CMC) inference algorithm for such latent factor models. We illustrate that CMC has a lower log loss or perplexity than MCMC, and needs fewer samples. version:1
arxiv-1604-03443 | Multi-modal Fusion for Diabetes Mellitus and Impaired Glucose Regulation Detection | http://arxiv.org/abs/1604.03443 | id:1604.03443 author:Jinxing Li, David Zhang, Yongcheng Li, Jian Wu category:cs.CV  published:2016-04-12 summary:Effective and accurate diagnosis of Diabetes Mellitus (DM), as well as its early stage Impaired Glucose Regulation (IGR), has attracted much attention recently. Traditional Chinese Medicine (TCM) [3], [5] etc. has proved that tongue, face and sublingual diagnosis as a noninvasive method is a reasonable way for disease detection. However, most previous works only focus on a single modality (tongue, face or sublingual) for diagnosis, although different modalities may provide complementary information for the diagnosis of DM and IGR. In this paper, we propose a novel multi-modal classification method to discriminate between DM (or IGR) and healthy controls. Specially, the tongue, facial and sublingual images are first collected by using a non-invasive capture device. The color, texture and geometry features of these three types of images are then extracted, respectively. Finally, our so-called multi-modal similar and specific learning (MMSSL) approach is proposed to combine features of tongue, face and sublingual, which not only exploits the correlation but also extracts individual components among them. Experimental results on a dataset consisting of 192 Healthy, 198 DM and 114 IGR samples (all samples were obtained from Guangdong Provincial Hospital of Traditional Chinese Medicine) substantiate the effectiveness and superiority of our proposed method for the diagnosis of DM and IGR, compared to the case of using a single modality. version:1
arxiv-1604-03390 | Video Description using Bidirectional Recurrent Neural Networks | http://arxiv.org/abs/1604.03390 | id:1604.03390 author:Álvaro Peris, Marc Bolaños, Petia Radeva, Francisco Casacuberta category:cs.CV cs.CL cs.LG  published:2016-04-12 summary:Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames. version:1
arxiv-1604-03373 | A Convex Surrogate Operator for General Non-Modular Loss Functions | http://arxiv.org/abs/1604.03373 | id:1604.03373 author:Jiaqian Yu, Matthew Blaschko category:stat.ML cs.LG  published:2016-04-12 summary:Empirical risk minimization frequently employs convex surrogates to underlying discrete loss functions in order to achieve computational tractability during optimization. However, classical convex surrogates can only tightly bound modular loss functions, sub-modular functions or supermodular functions separately while maintaining polynomial time computation. In this work, a novel generic convex surrogate for general non-modular loss functions is introduced, which provides for the first time a tractable solution for loss functions that are neither super-modular nor submodular. This convex surro-gate is based on a submodular-supermodular decomposition for which the existence and uniqueness is proven in this paper. It takes the sum of two convex surrogates that separately bound the supermodular component and the submodular component using slack-rescaling and the Lov{\'a}sz hinge, respectively. It is further proven that this surrogate is convex , piecewise linear, an extension of the loss function, and for which subgradient computation is polynomial time. Empirical results are reported on a non-submodular loss based on the S{{\o}}rensen-Dice difference function, and a real-world face track dataset with tens of thousands of frames, demonstrating the improved performance, efficiency, and scalabil-ity of the novel convex surrogate. version:1
arxiv-1510-06767 | Order-Fractal transition in abstract paintings | http://arxiv.org/abs/1510.06767 | id:1510.06767 author:E. M. De la Calleja, F. Cervantes, J. De la Calleja category:cs.CV  published:2015-10-22 summary:We report the degree of order of twenty-two Jackson Pollock's paintings using \emph{Hausdorff-Besicovitch fractal dimension}. Through the maximum value of each multi-fractal spectrum, the artworks are classify by the year in which they were painted. It has been reported that Pollock's paintings are fractal and it increased on his latest works. However our results show that fractal dimension of the paintings are on a range of fractal dimension with values close to two. We identify this behavior as a fractal-order transition. Based on the study of disorder-order transition in physical systems, we interpreted the fractal-order transition through its dark paint strokes in Pollocks' paintings, as structured lines following a power law measured by fractal dimension. We obtain self-similarity in some specific Pollock's paintings, that reveal an important dependence on the scale of observation. We also characterize by its fractal spectrum, the called \emph{Teri's Find}. We obtained similar spectrums between \emph{Teri's Find} and \emph{Number 5} from Pollock, suggesting that fractal dimension cannot be completely rejected as a quantitative parameter to authenticate this kind of artworks. version:3
arxiv-1604-03357 | Improving sentence compression by learning to predict gaze | http://arxiv.org/abs/1604.03357 | id:1604.03357 author:Sigrid Klerke, Yoav Goldberg, Anders Søgaard category:cs.CL  published:2016-04-12 summary:We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches. version:1
arxiv-1604-03351 | Orientation-boosted Voxel Nets for 3D Object Recognition | http://arxiv.org/abs/1604.03351 | id:1604.03351 author:Nima Sedaghat, Mohammadreza Zolfaghari, Thomas Brox category:cs.CV cs.NE  published:2016-04-12 summary:Recent work has shown good recognition results in 3D data using 3D convolutional networks. In this paper, we argue that the object orientation plays an important role in 3D recognition. To this end, we approach the category-level classification task as a multi-task problem, in which the network is forced to predict the pose of the object in addition to the class label. We show that this yields significant improvements in the classification results. We implemented different network architectures for this purpose and tested them on different datasets representing various 3D data sources: LiDAR data, CAD models and RGBD images. We report state-of-the-art results on classification, and analyze the effects of orientation-boosting on the dominant signal paths in the network. version:1
arxiv-1604-03348 | Optimal Margin Distribution Machine | http://arxiv.org/abs/1604.03348 | id:1604.03348 author:Teng Zhang, Zhi-Hua Zhou category:cs.LG  published:2016-04-12 summary:Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be used in any place where SVM can be applied, and their superiority is verified both theoretically and empirically in this paper. version:1
arxiv-1604-03346 | An incremental linear-time learning algorithm for the Optimum-Path Forest classifier | http://arxiv.org/abs/1604.03346 | id:1604.03346 author:Mateus Riva, Moacir Ponti category:cs.LG cs.CV  published:2016-04-12 summary:We present a classification method with linear-time incremental capabilities based on the Optimum-Path Forest (OPF) classifier. The OPF considers instances as nodes of a fully-connected training graph, where the edges' weights are the distances between two nodes' feature vectors. Upon this graph, a minimum spanning tree is built, and every edge connecting instances of different classes is removed, with those nodes becoming prototypes or roots of a tree. A new instance is classified by discovering which tree it would conquer. In this paper we describe a new training algorithm with incremental capabilities while keeping the properties of the OPF. New instances can be inserted in the model into one of the existing trees; substitute the prototype of a tree; or split a tree. This incremental method was tested for accuracy and running time against both full retraining using the original OPF and an adaptation of the Differential Image Foresting Transform. The method updates the training model in linear-time, while keeping similar accuracies when compared with the original model, which runs in quadratic-time. version:1
arxiv-1604-03343 | Loss Bounds and Time Complexity for Speed Priors | http://arxiv.org/abs/1604.03343 | id:1604.03343 author:Daniel Filan, Marcus Hutter, Jan Leike category:cs.LG stat.ML  published:2016-04-12 summary:This paper establishes for the first time the predictive performance of speed priors and their computational complexity. A speed prior is essentially a probability distribution that puts low probability on strings that are not efficiently computable. We propose a variant to the original speed prior (Schmidhuber, 2002), and show that our prior can predict sequences drawn from probability measures that are estimable in polynomial time. Our speed prior is computable in doubly-exponential time, but not in polynomial time. On a polynomial time computable sequence our speed prior is computable in exponential time. We show better upper complexity bounds for Schmidhuber's speed prior under the same conditions, and that it predicts deterministic sequences that are computable in polynomial time; however, we also show that it is not computable in polynomial time, and the question of its predictive properties for stochastic sequences remains open. version:1
arxiv-1604-03336 | Typicality-Based Stability and Privacy | http://arxiv.org/abs/1604.03336 | id:1604.03336 author:Raef Bassily, Yoav Freund category:cs.LG cs.DS  published:2016-04-12 summary:In this paper, we introduce a new notion of algorithmic stability called typical stability. When our goal is to release real-valued queries (statistics) computed over a dataset, this notion does not require the queries to be of bounded sensitivity -- a condition that is generally assumed under a standard notion of algorithmic stability known as differential privacy [DMNS06, Dwo06]. Instead, typical stability requires the output of the query, when computed on a dataset drawn from the underlying distribution, to be "well-concentrated" around its expected value with respect to that distribution. Typical stability can also be motivated as an alternative definition for database privacy (in such case, we call it typical privacy). Like differential privacy, this notion enjoys several important properties including robustness to post-processing and adaptive composition. We also discuss the guarantees of typical stability on the generalization error for a broader class of queries than that of bounded-sensitivity queries. This class contains all queries whose output distributions have a "light" tail, e.g., subgaussian and subexponential queries. In particular, we show that if a typically stable interaction with a dataset yields a query from that class, then this query when evaluated on the same dataset will have small generalization error with high probability (i.e., it will not overfit to the dataset). We discuss the composition guarantees of typical stability and prove a composition theorem that gives a characterization of the degradation of the parameters of typical stability/privacy under $k$-fold adaptive composition. We also give simple noise-addition algorithms that achieve this notion. These algorithms are similar to their differentially private counterparts, however, the added noise is calibrated differently. version:1
arxiv-1604-03334 | Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation | http://arxiv.org/abs/1604.03334 | id:1604.03334 author:Qi Ye, Shanxin Yuan, Tae-Kyun Kim category:cs.CV  published:2016-04-12 summary:Discriminative methods often generate hand poses kinematically implausible, then generative methods are used to correct (or verify) these results in a hybrid method. Estimating 3D hand pose in a hierarchy, where the high-dimensional output space is decomposed into smaller ones, has been shown effective. Existing hierarchical methods mainly focus on the decomposition of the output space while the input space remains almost the same along the hierarchy. In this paper, a hybrid hand pose estimation method is proposed by applying the kinematic hierarchy strategy to the input space (as well as the output space) of the discriminative method by a spatial attention mechanism and to the optimization of the generative method by hierarchical Particle Swarm Optimization (PSO). The spatial attention mechanism integrates cascaded and hierarchical regression into a CNN framework by transforming both the input(and feature space) and the output space, which greatly reduces the viewpoint and articulation variations. Between the levels in the hierarchy, the hierarchical PSO forces the kinematic constraints to the results of the CNNs. The experimental results show that our method significantly outperforms four state-of-the-art methods and three baselines on three public benchmarks. version:1
arxiv-1603-05027 | Identity Mappings in Deep Residual Networks | http://arxiv.org/abs/1603.05027 | id:1603.05027 author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV cs.LG  published:2016-03-16 summary:Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers. version:2
arxiv-1604-03318 | Applying Ontological Modeling on Quranic Nature Domain | http://arxiv.org/abs/1604.03318 | id:1604.03318 author:A. B. M. Shamsuzzaman Sadi, Towfique Anam, Mohamed Abdirazak, Abdillahi Hasan Adnan, Sazid Zaman Khan, Mohamed Mahmudur Rahman, Ghassan Samara category:cs.AI cs.CL  published:2016-04-12 summary:The holy Quran is the holy book of the Muslims. It contains information about many domains. Often people search for particular concepts of holy Quran based on the relations among concepts. An ontological modeling of holy Quran can be useful in such a scenario. In this paper, we have modeled nature related concepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource Description Framework). Our methodology involves identifying nature related concepts mentioned in holy Quran and identifying relations among those concepts. These concepts and relations are represented as classes/instances and properties of an OWL ontology. Later, in the result section it is shown that, using the Ontological model, SPARQL queries can retrieve verses and concepts of interest. Thus, this modeling helps semantic search and query on the holy Quran. In this work, we have used English translation of the holy Quran by Sahih International, Protege OWL Editor and for querying we have used SPARQL. version:1
arxiv-1604-02703 | Synthesizing Training Images for Boosting Human 3D Pose Estimation | http://arxiv.org/abs/1604.02703 | id:1604.02703 author:Wenzheng Chen, Huan Wang, Yangyan Li, Hao Su, Changhe Tu, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen category:cs.CV  published:2016-04-10 summary:Human 3D pose estimation from a single image is a challenging task with numerous applications. Convolutional Neural Networks (CNNs) have recently achieved superior performance on the task of 2D pose estimation from a single image, by training on images with 2D annotations collected by crowd sourcing. This suggests that similar success could be achieved for direct estimation of 3D poses. However, 3D poses are much harder to annotate, and the lack of suitable annotated training images hinders attempts towards end-to-end solutions. To address this issue, we opt to automatically synthesize training images with ground truth pose annotations. We find that pose space coverage and texture diversity are the key ingredients for the effectiveness of synthetic training data. We present a fully automatic, scalable approach that samples the human pose space for guiding the synthesis procedure and extracts clothing textures from real images. We demonstrate that CNNs trained with our synthetic images out-perform those trained with real photos on 3D pose estimation tasks. version:2
arxiv-1604-03278 | Confidence Decision Trees via Online and Active Learning for Streaming (BIG) Data | http://arxiv.org/abs/1604.03278 | id:1604.03278 author:Rocco De Rosa category:stat.ML cs.LG  published:2016-04-12 summary:Decision tree classifiers are a widely used tool in data stream mining. The use of confidence intervals to estimate the gain associated with each split leads to very effective methods, like the popular Hoeffding tree algorithm. From a statistical viewpoint, the analysis of decision tree classifiers in a streaming setting requires knowing when enough new information has been collected to justify splitting a leaf. Although some of the issues in the statistical analysis of Hoeffding trees have been already clarified, a general and rigorous study of confidence intervals for splitting criteria is missing. We fill this gap by deriving accurate confidence intervals to estimate the splitting gain in decision tree learning with respect to three criteria: entropy, Gini index, and a third index proposed by Kearns and Mansour. Our confidence intervals depend in a more detailed way on the tree parameters. We also extend our confidence analysis to a selective sampling setting, in which the decision tree learner adaptively decides which labels to query in the stream. We furnish theoretical guarantee bounding the probability that the classification is non-optimal learning the decision tree via our selective sampling strategy. Experiments on real and synthetic data in a streaming setting show that our trees are indeed more accurate than trees with the same number of leaves generated by other techniques and our active learning module permits to save labeling cost. In addition, comparing our labeling strategy with recent methods, we show that our approach is more robust and consistent respect all the other techniques applied to incremental decision trees. version:1
arxiv-1604-03277 | The Right Mutation Strength for Multi-Valued Decision Variables | http://arxiv.org/abs/1604.03277 | id:1604.03277 author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE cs.DS F.2.2  published:2016-04-12 summary:The most common representation in evolutionary computation are bit strings. This is ideal to model binary decision variables, but less useful for variables taking more values. With very little theoretical work existing on how to use evolutionary algorithms for such optimization problems, we study the run time of simple evolutionary algorithms on some OneMax-like functions defined over $\Omega = \{0, 1, \dots, r-1\}^n$. More precisely, we regard a variety of problem classes requesting the component-wise minimization of the distance to an unknown target vector $z \in \Omega$. For such problems we see a crucial difference in how we extend the standard-bit mutation operator to these multi-valued domains. While it is natural to select each position of the solution vector to be changed independently with probability $1/n$, there are various ways to then change such a position. If we change each selected position to a random value different from the original one, we obtain an expected run time of $\Theta(nr \log n)$. If we change each selected position by either $+1$ or $-1$ (random choice), the optimization time reduces to $\Theta(nr + n\log n)$. If we use a random mutation strength $i \in \{0,1,\ldots,r-1\}^n$ with probability inversely proportional to $i$ and change the selected position by either $+i$ or $-i$ (random choice), then the optimization time becomes $\Theta(n \log(r)(\log(n)+\log(r)))$, bringing down the dependence on $r$ from linear to polylogarithmic. One of our results depends on a new variant of the lower bounding multiplicative drift theorem. version:1
arxiv-1604-03266 | Online Learning of Portfolio Ensembles with Sector Exposure Regularization | http://arxiv.org/abs/1604.03266 | id:1604.03266 author:Guy Uziel, Ran El-Yaniv category:cs.LG  published:2016-04-12 summary:We consider online learning of ensembles of portfolio selection algorithms and aim to regularize risk by encouraging diversification with respect to a predefined risk-driven grouping of stocks. Our procedure uses online convex optimization to control capital allocation to underlying investment algorithms while encouraging non-sparsity over the given grouping. We prove a logarithmic regret for this procedure with respect to the best-in-hindsight ensemble. We applied the procedure with known mean-reversion portfolio selection algorithms using the standard GICS industry sector grouping. Empirical Experimental results showed an impressive percentage increase of risk-adjusted return (Sharpe ratio). version:1
arxiv-1511-03240 | Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer | http://arxiv.org/abs/1511.03240 | id:1511.03240 author:Jun Xie, Martin Kiefel, Ming-Ting Sun, Andreas Geiger category:cs.CV  published:2015-11-10 summary:Semantic annotations are vital for training models for object recognition, semantic segmentation or scene understanding. Unfortunately, pixelwise annotation of images at very large scale is labor-intensive and only little labeled data is available, particularly at instance level and for street scenes. In this paper, we propose to tackle this problem by lifting the semantic instance labeling task from 2D into 3D. Given reconstructions from stereo or laser data, we annotate static 3D scene elements with rough bounding primitives and develop a model which transfers this information into the image domain. We leverage our method to obtain 2D labels for a novel suburban video dataset which we have collected, resulting in 400k semantic and instance image annotations. A comparison of our method to state-of-the-art label transfer baselines reveals that 3D information enables more efficient annotation while at the same time resulting in improved accuracy and time-coherent labels. version:2
arxiv-1604-03249 | Attributes as Semantic Units between Natural Language and Visual Recognition | http://arxiv.org/abs/1604.03249 | id:1604.03249 author:Marcus Rohrbach category:cs.CV cs.CL  published:2016-04-12 summary:Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images. version:1
arxiv-1604-03248 | The Univariate Flagging Algorithm (UFA): a Fully-Automated Approach for Identifying Optimal Thresholds in Data | http://arxiv.org/abs/1604.03248 | id:1604.03248 author:Mallory Sheth, Roy Welsch, Natasha Markuzon category:cs.LG stat.AP  published:2016-04-12 summary:In many data classification problems, there is no linear relationship between an explanatory and the dependent variables. Instead, there may be ranges of the input variable for which the observed outcome is signficantly more or less likely. This paper describes an algorithm for automatic detection of such thresholds, called the Univariate Flagging Algorithm (UFA). The algorithm searches for a separation that optimizes the difference between separated areas while providing the maximum support. We evaluate its performance using three examples and demonstrate that thresholds identified by the algorithm align well with visual inspection and subject matter expertise. We also introduce two classification approaches that use UFA and show that the performance attained on unseen test data is equal to or better than that of more traditional classifiers. We demonstrate that the proposed algorithm is robust against missing data and noise, is scalable, and is easy to interpret and visualize. It is also well suited for problems where incidence of the target is low. version:1
arxiv-1604-03247 | Thesis: Multiple Kernel Learning for Object Categorization | http://arxiv.org/abs/1604.03247 | id:1604.03247 author:Dinesh Govindaraj category:cs.CV cs.LG  published:2016-04-12 summary:Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels. Since essentially a single descriptor is selected, the existing formulations maybe sub- optimal for object categorization. A MKL formulation based on block l-infinity norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alter- native algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations. version:1
arxiv-1604-03239 | CRAFT Objects from Images | http://arxiv.org/abs/1604.03239 | id:1604.03239 author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV  published:2016-04-12 summary:Object detection is a fundamental problem in image understanding. One popular solution is the R-CNN framework and its fast versions. They decompose the object detection problem into two cascaded easier tasks: 1) generating object proposals from images, 2) classifying proposals into various object categories. Despite that we are handling with two relatively easier tasks, they are not solved perfectly and there's still room for improvement. In this paper, we push the "divide and conquer" solution even further by dividing each task into two sub-tasks. We call the proposed method "CRAFT" (Cascade Region-proposal-network And FasT-rcnn), which tackles each task with a carefully designed network cascade. We show that the cascade structure helps in both tasks: in proposal generation, it provides more compact and better localized object proposals; in object classification, it reduces false positives (mainly between ambiguous categories) by capturing both inter- and intra-category variances. CRAFT achieves consistent and considerable improvement over the state-of-the-art on object detection benchmarks like PASCAL VOC 07/12 and ILSVRC. version:1
arxiv-1602-00134 | Convolutional Pose Machines | http://arxiv.org/abs/1602.00134 | id:1602.00134 author:Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh category:cs.CV  published:2016-01-30 summary:Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets. version:4
arxiv-1604-03227 | Recurrent Attentional Networks for Saliency Detection | http://arxiv.org/abs/1604.03227 | id:1604.03227 author:Jason Kuen, Zhenhua Wang, Gang Wang category:cs.CV cs.LG stat.ML  published:2016-04-12 summary:Convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection. But, they do not work well with objects of multiple scales. To overcome such a limitation, in this work, we propose a recurrent attentional convolutional-deconvolution network (RACDNN). Using spatial transformer and recurrent network units, RACDNN is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively. Besides tackling the scale problem, RACDNN can also learn context-aware features from past iterations to enhance saliency refinement in future iterations. Experiments on several challenging saliency detection datasets validate the effectiveness of RACDNN, and show that RACDNN outperforms state-of-the-art saliency detection methods. version:1
arxiv-1604-03225 | Geometric Feature-Based Facial Expression Recognition in Image Sequences Using Multi-Class AdaBoost and Support Vector Machines | http://arxiv.org/abs/1604.03225 | id:1604.03225 author:Deepak Ghimire, Joonwhoan Lee category:cs.CV 68T01 I.4; I.5  published:2016-04-12 summary:Facial expressions are widely used in the behavioral interpretation of emotions, cognitive science, and social interactions. In this paper, we present a novel method for fully automatic facial expression recognition in facial image sequences. As the facial expression evolves over time facial landmarks are automatically tracked in consecutive video frames, using displacements based on elastic bunch graph matching displacement estimation. Feature vectors from individual landmarks, as well as pairs of landmarks tracking results are extracted, and normalized, with respect to the first frame in the sequence. The prototypical expression sequence for each class of facial expression is formed, by taking the median of the landmark tracking results from the training facial expression sequences. Multi-class AdaBoost with dynamic time warping similarity distance between the feature vector of input facial expression and prototypical facial expression, is used as a weak classifier to select the subset of discriminative feature vectors. Finally, two methods for facial expression recognition are presented, either by using multi-class AdaBoost with dynamic time warping, or by using support vector machine on the boosted feature vectors. The results on the Cohn-Kanade (CK+) facial expression database show a recognition accuracy of 95.17% and 97.35% using multi-class AdaBoost and support vector machines, respectively. version:1
arxiv-1604-03209 | Disfluency Detection using a Bidirectional LSTM | http://arxiv.org/abs/1604.03209 | id:1604.03209 author:Vicky Zayats, Mari Ostendorf, Hannaneh Hajishirzi category:cs.CL  published:2016-04-12 summary:We introduce a new approach for disfluency detection using a Bidirectional Long-Short Term Memory neural network (BLSTM). In addition to the word sequence, the model takes as input pattern match features that were developed to reduce sensitivity to vocabulary size in training, which lead to improved performance over the word sequence alone. The BLSTM takes advantage of explicit repair states in addition to the standard reparandum states. The final output leverages integer linear programming to incorporate constraints of disfluency structure. In experiments on the Switchboard corpus, the model achieves state-of-the-art performance for both the standard disfluency detection task and the correction detection task. Analysis shows that the model has better detection of non-repetition disfluencies, which tend to be much harder to detect. version:1
arxiv-1511-06062 | Compact Bilinear Pooling | http://arxiv.org/abs/1511.06062 | id:1511.06062 author:Yang Gao, Oscar Beijbom, Ning Zhang, Trevor Darrell category:cs.CV  published:2015-11-19 summary:Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets. version:2
arxiv-1604-03200 | Efficient Classification of Multi-Labelled Text Streams by Clashing | http://arxiv.org/abs/1604.03200 | id:1604.03200 author:Ricardo Ñanculef, Ilias Flaounas, Nello Cristianini category:cs.AI cs.LG  published:2016-04-12 summary:We present a method for the classification of multi-labelled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labelled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labelled streams. version:1
arxiv-1604-02748 | TGIF: A New Dataset and Benchmark on Animated GIF Description | http://arxiv.org/abs/1604.02748 | id:1604.02748 author:Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, Jiebo Luo category:cs.CV  published:2016-04-10 summary:With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich metadata. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowdworkers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation, and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description. version:2
arxiv-1604-03196 | Privacy-Preserving Egocentric Activity Recognition from Extreme Low Resolution | http://arxiv.org/abs/1604.03196 | id:1604.03196 author:Michael S. Ryoo, Brandon Rothrock, Charles Fleming category:cs.CV  published:2016-04-12 summary:Privacy protection from video taken by wearable cameras is an important societal challenge. We desire a wearable vision system that can recognize human activities, yet not disclose the identity of the participants. Video anonymization is typically handled by decimating the image to a very low resolution. Activity recognition, however, generally requires resolution high enough that features such as faces are identifiable. In this paper, we propose a new approach to address such contradicting objectives: human activity recognition while only using extreme low-resolution (e.g., 16x12) anonymized videos. We introduce the paradigm of inverse super resolution (ISR), the concept of learning the optimal set of image transformations to generate multiple low-resolution videos from a single video. Our ISR learns different types of sub-pixel transformations optimized for the activity classification, allowing the classifier to best take advantage of existing high-resolution videos (e.g., YouTube videos) by generating multiple LR training videos tailored for the problem. We experimentally confirm that the paradigm of inverse super resolution is able to benefit activity recognition from extreme low-resolution videos (e.g., 16x12 and 32x24), particularly in first-person scenarios. version:1
arxiv-1507-02558 | Multi-Type Activity Recognition in Robot-Centric Scenarios | http://arxiv.org/abs/1507.02558 | id:1507.02558 author:Ilaria Gori, J. K. Aggarwal, Larry Matthies, Michael S. Ryoo category:cs.CV  published:2015-07-09 summary:Activity recognition is very useful in scenarios where robots interact with, monitor or assist humans. In the past years many types of activities -- single actions, two persons interactions or ego-centric activities, to name a few -- have been analyzed. Whereas traditional methods treat such types of activities separately, an autonomous robot should be able to detect and recognize multiple types of activities to effectively fulfill its tasks. We propose a method that is intrinsically able to detect and recognize activities of different types that happen in sequence or concurrently. We present a new unified descriptor, called Relation History Image (RHI), which can be extracted from all the activity types we are interested in. We then formulate an optimization procedure to detect and recognize activities of different types. We apply our approach to a new dataset recorded from a robot-centric perspective and systematically evaluate its quality compared to multiple baselines. Finally, we show the efficacy of the RHI descriptor on publicly available datasets performing extensive comparisons. version:2
arxiv-1604-02181 | A Unified Bayesian Framework for Sparse Non-negative Matrix Factorization | http://arxiv.org/abs/1604.02181 | id:1604.02181 author:Igor Fedorov, Alican Nalci, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen, H. Garudadri category:stat.ML  published:2016-04-07 summary:In this work, we study the sparse non-negative matrix factorization (Sparse NMF or S-NMF) problem. NMF and S-NMF are popular machine learning tools which decompose a given non-negative dataset into a dictionary and an activation matrix, where both are constrained to be non-negative. We review how common concave sparsity measures from the compressed sensing literature can be extended to the S-NMF problem. Furthermore, we show that these sparsity measures have a Bayesian interpretation and each one corresponds to a specific prior on the activations. We present a comprehensive Sparse Bayesian Learning (SBL) framework for modeling non-negative data and provide details for Type I and Type II inference procedures. We show that efficient multiplicative update rules can be employed to solve the S-NMF problem for the penalty functions discussed and present experimental results validating our assertions. version:2
arxiv-1604-03193 | Application of the Second-Order Statistics for Estimation of the Pure Spectra of Individual Components from the Visible Hyperspectral Images of Their Mixture | http://arxiv.org/abs/1604.03193 | id:1604.03193 author:Sung-Ho Jong, Yong-U Ri, Kye-Ryong Sin category:cs.CV physics.chem-ph  published:2016-04-12 summary:The second-order statistics (SOS) can be applied in estimation of the pure spectra of chemical components from the spectrum of their mixture, when SOS seems to be good at estimation of spectral patterns, but their peak directions are opposite in some cases. In this paper, one method for judgment of the peak direction of the pure spectra was proposed, where the base line of the pure spectra was drawn by using their histograms and the peak directions were chosen so as to make all of the pure spectra located upwards over the base line. Results of the SOS analysis on the visible hyperspectral images of the mixture composed of two or three chemical components showed that the present method offered the reasonable shape and direction of the pure spectra of its components. version:1
arxiv-1511-06065 | Deep Learning for Tactile Understanding From Visual and Haptic Data | http://arxiv.org/abs/1511.06065 | id:1511.06065 author:Yang Gao, Lisa Anne Hendricks, Katherine J. Kuchenbecker, Trevor Darrell category:cs.RO cs.CV cs.LG  published:2015-11-19 summary:Robots which interact with the physical world will benefit from a fine-grained tactile understanding of objects and surfaces. Additionally, for certain tasks, robots may need to know the haptic properties of an object before touching it. To enable better tactile understanding for robots, we propose a method of classifying surfaces with haptic adjectives (e.g., compressible or smooth) from both visual and physical interaction data. Humans typically combine visual predictions and feedback from physical interactions to accurately predict haptic properties and interact with the world. Inspired by this cognitive pattern, we propose and explore a purely visual haptic prediction model. Purely visual models enable a robot to "feel" without physical interaction. Furthermore, we demonstrate that using both visual and physical interaction signals together yields more accurate haptic classification. Our models take advantage of recent advances in deep neural networks by employing a unified approach to learning features for physical interaction and visual observations. Even though we employ little domain specific knowledge, our model still achieves better results than methods based on hand-designed features. version:2
arxiv-1604-03171 | Learning Simple Auctions | http://arxiv.org/abs/1604.03171 | id:1604.03171 author:Jamie Morgenstern, Tim Roughgarden category:cs.LG cs.GT  published:2016-04-11 summary:We present a general framework for proving polynomial sample complexity bounds for the problem of learning from samples the best auction in a class of "simple" auctions. Our framework captures all of the most prominent examples of "simple" auctions, including anonymous and non-anonymous item and bundle pricings, with either a single or multiple buyers. The technique we propose is to break the analysis of auctions into two natural pieces. First, one shows that the set of allocation rules have large amounts of structure; second, fixing an allocation on a sample, one shows that the set of auctions agreeing with this allocation on that sample have revenue functions with low dimensionality. Our results effectively imply that whenever it's possible to compute a near-optimal simple auction with a known prior, it is also possible to compute such an auction with an unknown prior (given a polynomial number of samples). version:1
arxiv-1512-02767 | Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding | http://arxiv.org/abs/1512.02767 | id:1512.02767 author:Michael Maire, Takuya Narihira, Stella X. Yu category:cs.CV cs.LG cs.NE  published:2015-12-09 summary:Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization. From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer. We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix. Spectral embedding then resolves these predictions into a globally-consistent segmentation and figure/ground organization of the scene. Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities. Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks. version:2
arxiv-1604-03136 | Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text | http://arxiv.org/abs/1604.03136 | id:1604.03136 author:Arnav Sharma, Sakshi Gupta, Raveesh Motlani, Piyush Bansal, Manish Srivastava, Radhika Mamidi, Dipti M. Sharma category:cs.CL  published:2016-04-11 summary:In this study, the problem of shallow parsing of Hindi-English code-mixed social media text (CSMT) has been addressed. We have annotated the data, developed a language identifier, a normalizer, a part-of-speech tagger and a shallow parser. To the best of our knowledge, we are the first to attempt shallow parsing on CSMT. The pipeline developed has been made available to the research community with the goal of enabling better text analysis of Hindi English CSMT. The pipeline is accessible at http://bit.ly/csmt-parser-api . version:1
arxiv-1604-03114 | Conversational flow in Oxford-style debates | http://arxiv.org/abs/1604.03114 | id:1604.03114 author:Justine Zhang, Ravi Kumar, Sujith Ravi, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML  published:2016-04-11 summary:Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study of Oxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation. version:1
arxiv-1604-03075 | Fully-Automatic Synapse Prediction and Validation on a Large Data Set | http://arxiv.org/abs/1604.03075 | id:1604.03075 author:Gary B. Huang, Louis K. Scheffer, Stephen M. Plaza category:cs.CV  published:2016-04-11 summary:Extracting a connectome from an electron microscopy (EM) data set requires identification of neurons and determination of synapses between neurons. As manual extraction of this information is very time-consuming, there has been extensive research effort to automatically segment the neurons to help guide and eventually replace manual tracing. Until recently, there has been comparatively less research on automatically detecting the actual synapses between neurons. This discrepancy can, in part, be attributed to several factors: obtaining neuronal shapes is a prerequisite first step in extracting a connectome, manual tracing is much more time-consuming than annotating synapses, and neuronal contact area can be used as a proxy for synapses in determining connections. However, recent research has demonstrated that contact area alone is not a sufficient predictor of synaptic connection. Moreover, as segmentation has improved, we have observed that synapse annotation is consuming a more significant fraction of overall reconstruction time. This ratio will only get worse as segmentation improves, gating overall possible speed-up. Therefore, we address this problem by developing algorithms that automatically detect pre-synaptic neurons and their post-synaptic partners. In particular, pre-synaptic structures are detected using a Deep and Wide Multiscale Recursive Network, and post-synaptic partners are detected using a MLP with features conditioned on the local segmentation. This work is novel because it requires minimal amount of training, leverages advances in image segmentation directly, and provides a complete solution for polyadic synapse detection. We further introduce novel metrics to evaluate our algorithm on connectomes of meaningful size. These metrics demonstrate that complete automatic prediction can be used to effectively characterize most connectivity correctly. version:1
arxiv-1604-03073 | Reservoir computing for spatiotemporal signal classification without trained output weights | http://arxiv.org/abs/1604.03073 | id:1604.03073 author:Ashley Prater category:cs.NE cs.CV cs.LG  published:2016-04-11 summary:Reservoir computing is a recently introduced machine learning paradigm that has been shown to be well-suited for the processing of spatiotemporal data. Rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks, reservoirs instead have fixed connections and weights among the `hidden layer' nodes, and traditionally only the weights to the output layer of neurons are trained using linear regression. We claim that for signal classification tasks, one may forgo the weight training step entirely and instead use a simple supervised clustering method. The proposed method is analyzed theoretically and explored through numerical experiments on real-world data. The examples demonstrate that the proposed clustering method outperforms the traditional trained output weight approach in terms of speed, accuracy, and sensitivity to reservoir parameters. version:1
arxiv-1511-05298 | Structural-RNN: Deep Learning on Spatio-Temporal Graphs | http://arxiv.org/abs/1511.05298 | id:1511.05298 author:Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.LG cs.NE cs.RO  published:2015-11-17 summary:Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks. version:3
arxiv-1604-00644 | An electronic-game framework for evaluating coevolutionary algorithms | http://arxiv.org/abs/1604.00644 | id:1604.00644 author:Karine da Silva Miras de Araújo, Fabrício Olivetti de França category:cs.NE cs.AI  published:2016-04-03 summary:One of the common artificial intelligence applications in electronic games consists of making an artificial agent learn how to execute some determined task successfully in a game environment. One way to perform this task is through machine learning algorithms capable of learning the sequence of actions required to win in a given game environment. There are several supervised learning techniques able to learn the correct answer for a problem through examples. However, when learning how to play electronic games, the correct answer might only be known by the end of the game, after all the actions were already taken. Thus, not being possible to measure the accuracy of each individual action to be taken at each time step. A way for dealing with this problem is through Neuroevolution, a method which trains Artificial Neural Networks using evolutionary algorithms. In this article, we introduce a framework for testing optimization algorithms with artificial agent controllers in electronic games, called EvoMan, which is inspired in the action-platformer game Mega Man II. The environment can be configured to run in different experiment modes, as single evolution, coevolution and others. To demonstrate some challenges regarding the proposed platform, as initial experiments we applied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the context of competitively coevolving two distinct agents in this game. version:2
arxiv-1508-02933 | No Regret Bound for Extreme Bandits | http://arxiv.org/abs/1508.02933 | id:1508.02933 author:Robert Nishihara, David Lopez-Paz, Léon Bottou category:stat.ML cs.LG math.OC math.ST stat.TH  published:2015-08-12 summary:Algorithms for hyperparameter optimization abound, all of which work well under different and often unverifiable assumptions. Motivated by the general challenge of sequentially choosing which algorithm to use, we study the more specific task of choosing among distributions to use for random hyperparameter optimization. This work is naturally framed in the extreme bandit setting, which deals with sequentially choosing which distribution from a collection to sample in order to minimize (maximize) the single best cost (reward). Whereas the distributions in the standard bandit setting are primarily characterized by their means, a number of subtleties arise when we care about the minimal cost as opposed to the average cost. For example, there may not be a well-defined "best" distribution as there is in the standard bandit setting. The best distribution depends on the rewards that have been obtained and on the remaining time horizon. Whereas in the standard bandit setting, it is sensible to compare policies with an oracle which plays the single best arm, in the extreme bandit setting, there are multiple sensible oracle models. We define a sensible notion of "extreme regret" in the extreme bandit setting, which parallels the concept of regret in the standard bandit setting. We then prove that no policy can asymptotically achieve no extreme regret. version:3
arxiv-1604-03035 | Learning Global Features for Coreference Resolution | http://arxiv.org/abs/1604.03035 | id:1604.03035 author:Sam Wiseman, Alexander M. Rush, Stuart M. Shieber category:cs.CL  published:2016-04-11 summary:There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search. version:1
arxiv-1604-03034 | M3: Scaling Up Machine Learning via Memory Mapping | http://arxiv.org/abs/1604.03034 | id:1604.03034 author:Dezhi Fang, Duen Horng Chau category:cs.LG cs.DC  published:2016-04-11 summary:To process data that do not fit in RAM, conventional wisdom would suggest using distributed approaches. However, recent research has demonstrated virtual memory's strong potential in scaling up graph mining algorithms on a single machine. We propose to use a similar approach for general machine learning. We contribute: (1) our latest finding that memory mapping is also a feasible technique for scaling up general machine learning algorithms like logistic regression and k-means, when data fits in or exceeds RAM (we tested datasets up to 190GB); (2) an approach, called M3, that enables existing machine learning algorithms to work with out-of-core datasets through memory mapping, achieving a speed that is significantly faster than a 4-instance Spark cluster, and comparable to an 8-instance cluster. version:1
arxiv-1604-03427 | In the mood: the dynamics of collective sentiments on Twitter | http://arxiv.org/abs/1604.03427 | id:1604.03427 author:Nathaniel Charlton, Colin Singleton, Danica Vukadinović Greetham category:cs.SI stat.ML  published:2016-04-11 summary:We study the relationship between the sentiment levels of Twitter users and the evolving network structure that the users created by @-mentioning each other. We use a large dataset of tweets to which we apply three sentiment scoring algorithms, including the open source SentiStrength program. Specifically we make three contributions. Firstly we find that people who have potentially the largest communication reach (according to a dynamic centrality measure) use sentiment differently than the average user: for example they use positive sentiment more often and negative sentiment less often. Secondly we find that when we follow structurally stable Twitter communities over a period of months, their sentiment levels are also stable, and sudden changes in community sentiment from one day to the next can in most cases be traced to external events affecting the community. Thirdly, based on our findings, we create and calibrate a simple agent-based model that is capable of reproducing measures of emotive response comparable to those obtained from our empirical dataset. version:1
arxiv-1508-03390 | Doubly Stochastic Primal-Dual Coordinate Method for Empirical Risk Minimization and Bilinear Saddle-Point Problem | http://arxiv.org/abs/1508.03390 | id:1508.03390 author:Adams Wei Yu, Qihang Lin, Tianbao Yang category:cs.LG stat.ML  published:2015-08-14 summary:We proposed a doubly stochastic primal-dual coordinate (DSPDC) optimization algorithm for empirical risk minimization, which can be formulated as a bilinear saddle-point problem. In each iteration, our method randomly samples a block of coordinates of the primal and dual solutions to update. The convergence of our method is established in both the distance from the current iterate to the optimal solution and the primal-dual objective gap. We show that the proposed method has a lower overall complexity than existing coordinate methods when either the data matrix has a factorized structure or the proximal mapping on each block is computationally expensive, e.g., involves an eigenvalue decomposition. Furthermore, we give a theoretical lower bound on the iteration complexity of a general family of primal-dual (block) coordinate methods for bilinear saddle-point problems, which also includes DSPDC. version:2
arxiv-1604-03010 | Semi-supervised learning of local structured output predictors | http://arxiv.org/abs/1604.03010 | id:1604.03010 author:Xin Du category:cs.LG cs.CV  published:2016-04-11 summary:In this paper, we study the problem of semi-supervised structured output prediction, which aims to learn predictors for structured outputs, such as sequences, tree nodes, vectors, etc., from a set of data points of both input-output pairs and single inputs without outputs. The traditional methods to solve this problem usually learns one single predictor for all the data points, and ignores the variety of the different data points. Different parts of the data set may have different local distributions, and requires different optimal local predictors. To overcome this disadvantage of existing methods, we propose to learn different local predictors for neighborhoods of different data points, and the missing structured outputs simultaneously. In the neighborhood of each data point, we proposed to learn a linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization is conducted by gradient descent algorithms. Experiments over four benchmark data sets, including DDSM mammography medical images, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set, show the advantages of the proposed method. version:1
arxiv-1604-03006 | Demystifying Fixed k-Nearest Neighbor Information Estimators | http://arxiv.org/abs/1604.03006 | id:1604.03006 author:Weihao Gao, Sewoong Oh, Pramod Viswanath category:cs.LG cs.IT math.IT stat.ML  published:2016-04-11 summary:Estimating mutual information from i.i.d. samples drawn from an unknown joint density function is a basic statistical problem of broad interest with multitudinous applications. The most popular estimator is one proposed by Kraskov and St\"ogbauer and Grassberger (KSG) in 2004, and is nonparametric and based on the distances of each sample to its $k^{\rm th}$ nearest neighboring sample, where $k$ is a fixed small integer. Despite its widespread use (part of scientific software packages), theoretical properties of this estimator have been largely unexplored. In this paper we demonstrate that the estimator is consistent and also identify an upper bound on the rate of convergence of the bias as a function of number of samples. We argue that the superior performance benefits of the KSG estimator stems from a curious "correlation boosting" effect and build on this intuition to modify the KSG estimator in novel ways to construct a superior estimator. As a byproduct of our investigations, we obtain nearly tight rates of convergence of the $\ell_2$ error of the well known fixed $k$ nearest neighbor estimator of differential entropy by Kozachenko and Leonenko. version:1
arxiv-1504-01046 | Graph Connectivity in Noisy Sparse Subspace Clustering | http://arxiv.org/abs/1504.01046 | id:1504.01046 author:Yining Wang, Yu-Xiang Wang, Aarti Singh category:stat.ML cs.LG  published:2015-04-04 summary:Subspace clustering is the problem of clustering data points into a union of low-dimensional linear/affine subspaces. It is the mathematical abstraction of many important problems in computer vision, image processing and machine learning. A line of recent work (4, 19, 24, 20) provided strong theoretical guarantee for sparse subspace clustering (4), the state-of-the-art algorithm for subspace clustering, on both noiseless and noisy data sets. It was shown that under mild conditions, with high probability no two points from different subspaces are clustered together. Such guarantee, however, is not sufficient for the clustering to be correct, due to the notorious "graph connectivity problem" (15). In this paper, we investigate the graph connectivity problem for noisy sparse subspace clustering and show that a simple post-processing procedure is capable of delivering consistent clustering under certain "general position" or "restricted eigenvalue" assumptions. We also show that our condition is almost tight with adversarial noise perturbation by constructing a counter-example. These results provide the first exact clustering guarantee of noisy SSC for subspaces of dimension greater then 3. version:2
arxiv-1604-02975 | CP-mtML: Coupled Projection multi-task Metric Learning for Large Scale Face Retrieval | http://arxiv.org/abs/1604.02975 | id:1604.02975 author:Binod Bhattarai, Gaurav Sharma, Frederic Jurie category:cs.CV  published:2016-04-11 summary:We propose a novel Coupled Projection multi-task Metric Learning (CP-mtML) method for large scale face retrieval. In contrast to previous works which were limited to low dimensional features and small datasets, the proposed method scales to large datasets with high dimensional face descriptors. It utilises pairwise (dis-)similarity constraints as supervision and hence does not require exhaustive class annotation for every training image. While, traditionally, multi-task learning methods have been validated on same dataset but different tasks, we work on the more challenging setting with heterogeneous datasets and different tasks. We show empirical validation on multiple face image datasets of different facial traits, e.g. identity, age and expression. We use classic Local Binary Pattern (LBP) descriptors along with the recent Deep Convolutional Neural Network (CNN) features. The experiments clearly demonstrate the scalability and improved performance of the proposed method on the tasks of identity and age based face image retrieval compared to competitive existing methods, on the standard datasets and with the presence of a million distractor face images. version:1
arxiv-1511-04273 | Learning to Assign Orientations to Feature Points | http://arxiv.org/abs/1511.04273 | id:1511.04273 author:Kwang Moo Yi, Yannick Verdie, Pascal Fua, Vincent Lepetit category:cs.CV  published:2015-11-13 summary:We show how to train a Convolutional Neural Network to assign a canonical orientation to feature points given an image patch centered on the feature point. Our method improves feature point matching upon the state-of-the art and can be used in conjunction with any existing rotation sensitive descriptors. To avoid the tedious and almost impossible task of finding a target orientation to learn, we propose to use Siamese networks which implicitly find the optimal orientations during training. We also propose a new type of activation function for Neural Networks that generalizes the popular ReLU, maxout, and PReLU activation functions. This novel activation performs better for our task. We validate the effectiveness of our method extensively with four existing datasets, including two non-planar datasets, as well as our own dataset. We show that we outperform the state-of-the-art without the need of retraining for each dataset. version:2
arxiv-1604-02946 | Kernel-based Sensor Fusion with Application to Audio-Visual Voice Activity Detection | http://arxiv.org/abs/1604.02946 | id:1604.02946 author:David Dov, Ronen Talmon, Israel Cohen category:cs.CV  published:2016-04-11 summary:In this paper, we address the problem of multiple view data fusion in the presence of noise and interferences. Recent studies have approached this problem using kernel methods, by relying particularly on a product of kernels constructed separately for each view. From a graph theory point of view, we analyze this fusion approach in a discrete setting. More specifically, based on a statistical model for the connectivity between data points, we propose an algorithm for the selection of the kernel bandwidth, a parameter, which, as we show, has important implications on the robustness of this fusion approach to interferences. Then, we consider the fusion of audio-visual speech signals measured by a single microphone and by a video camera pointed to the face of the speaker. Specifically, we address the task of voice activity detection, i.e., the detection of speech and non-speech segments, in the presence of structured interferences such as keyboard taps and office noise. We propose an algorithm for voice activity detection based on the audio-visual signal. Simulation results show that the proposed algorithm outperforms competing fusion and voice activity detection approaches. In addition, we demonstrate that a proper selection of the kernel bandwidth indeed leads to improved performance. version:1
arxiv-1512-08347 | Communicating with sentences: A multi-word naming game model | http://arxiv.org/abs/1512.08347 | id:1512.08347 author:Yang Lou, Guanrong Chen, Jianwei Hu category:cs.CL physics.soc-ph  published:2015-12-28 summary:Naming game simulates the process of naming a single object by a single word, in which a population of communicating agents can reach global consensus asymptotically through iteratively pair-wise conversations. In this paper, we propose an extension of the single-word naming game, to a multi-word naming game (MWNG), which simulates the naming game process when agents name an object by a sentence (i.e., a series of multiple words) for describing a complex object such as an opinion or an event. We first define several categories of words, and then organize sentences by combining words from different word categories. We refer to a formatted combination of several words as a pattern. In such an MWNG, through a pair-wise conversation, it requires the hearer to achieve consensus with the speaker with respect to both every single word in the sentence as well as the sentence pattern, so as to guarantee the correct meaning of the saying; otherwise, they fail reaching consensus in the interaction. We employ three typical topologies used for the underlying communication network, namely random-graph, small-world and scale-free networks. We validate the model by using both conventional English language patterns and man-made test sentence patterns in performing the MWNG. Our simulation results show that: 1) the new sentence sharing model is an extension of the classical lexicon sharing model; 2) the propagating, learning and converging processes are more complicated than that in the conventional naming game; 3) the convergence time is non-decreasing as the network becomes better connected; 4) the agents are prone to accept short sentence patterns. These new findings may help deepen our understanding of the human language development from a network science perspective. version:3
arxiv-1603-09170 | Improving and Scaling Trans-dimensional Random Field Language Models | http://arxiv.org/abs/1603.09170 | id:1603.09170 author:Bin Wang, Zhijian Ou, Yong He, Akinori Kawamura category:cs.CL cs.LG stat.ML  published:2016-03-30 summary:The dominant language models (LMs) such as n-gram and neural network (NN) models represent sentence probabilities in terms of conditionals. In contrast, a new trans-dimensional random field (TRF) LM has been recently introduced to show superior performances, where the whole sentence is modeled as a random field. In this paper, we further develop the TDF LMs with two technical improvements, which are a new method of exploiting Hessian information in parameter optimization to further enhance the convergence of the training algorithm and an enabling method for training the TRF LMs on large corpus which may contain rare very long sentences. Experiments show that the TRF LMs can scale to using training data of up to 32 million words, consistently achieve 10% relative perplexity reductions over 5-gram LMs, and perform as good as NN LMs but with much faster speed in calculating sentence probabilities. version:2
arxiv-1603-03911 | Optical Flow with Semantic Segmentation and Localized Layers | http://arxiv.org/abs/1603.03911 | id:1603.03911 author:Laura Sevilla-Lara, Deqing Sun, Varun Jampani, Michael J. Black category:cs.CV  published:2016-03-12 summary:Existing optical flow methods make generic, spatially homogeneous, assumptions about the spatial structure of the flow. In reality, optical flow varies across an image depending on object class. Simply put, different objects move differently. Here we exploit recent advances in static semantic scene segmentation to segment the image into objects of different types. We define different models of image motion in these regions depending on the type of object. For example, we model the motion on roads with homographies, vegetation with spatially smooth flow, and independently moving objects like cars and planes with affine motion plus deviations. We then pose the flow estimation problem using a novel formulation of localized layers, which addresses limitations of traditional layered models for dealing with complex scene motion. Our semantic flow method achieves the lowest error of any published monocular method in the KITTI-2015 flow benchmark and produces qualitatively better flow and segmentation than recent top methods on a wide range of natural videos. version:2
arxiv-1604-02902 | Statistics of RGBD Images | http://arxiv.org/abs/1604.02902 | id:1604.02902 author:Dan Rosenbaum, Yair Weiss category:cs.CV  published:2016-04-11 summary:Cameras that can measure the depth of each pixel in addition to its color have become easily available and are used in many consumer products worldwide. Often the depth channel is captured at lower quality compared to the RGB channels and different algorithms have been proposed to improve the quality of the D channel given the RGB channels. Typically these approaches work by assuming that edges in RGB are correlated with edges in D. In this paper we approach this problem from the standpoint of natural image statistics. We obtain examples of high quality RGBD images from a computer graphics generated movie (MPI-Sintel) and we use these examples to compare different probabilistic generative models of RGBD image patches. We then use the generative models together with a degradation model and obtain a Bayes Least Squares (BLS) estimator of the D channel given the RGB channels. Our results show that learned generative models outperform the state-of-the-art in improving the quality of depth channels given the color channels in natural images even when training is performed on artificially generated images. version:1
arxiv-1604-02898 | Sparse Coding for Alpha Matting | http://arxiv.org/abs/1604.02898 | id:1604.02898 author:Jubin Johnson, Ehsan Shahrian Varnousfaderani, Hisham Cholakkal, Deepu Rajan category:cs.CV  published:2016-04-11 summary:Existing color sampling based alpha matting methods use the compositing equation to estimate alpha at a pixel from pairs of foreground (F) and background (B) samples. The quality of the matte depends on the selected (F,B) pairs. In this paper, the matting problem is reinterpreted as a sparse coding of pixel features, wherein the sum of the codes gives the estimate of the alpha matte from a set of unpaired F and B samples. A non-parametric probabilistic segmentation provides a certainty measure on the pixel belonging to foreground or background, based on which a dictionary is formed for use in sparse coding. By removing the restriction to conform to (F,B) pairs, this method allows for better alpha estimation from multiple F and B samples. The same framework is extended to videos, where the requirement of temporal coherence is handled effectively. Here, the dictionary is formed by samples from multiple frames. A multi-frame graph model, as opposed to a single image as for image matting, is proposed that can be solved efficiently in closed form. Quantitative and qualitative evaluations on a benchmark dataset are provided to show that the proposed method outperforms current state-of-the-art in image and video matting. version:1
arxiv-1604-03392 | A statistical learning strategy for closed-loop control of fluid flows | http://arxiv.org/abs/1604.03392 | id:1604.03392 author:Florimond Guéniat, Lionel Mathelin, M. Yousuff Hussaini category:stat.ML math.OC physics.flu-dyn  published:2016-04-11 summary:This work discusses a closed-loop control strategy for complex systems utilizing scarce and streaming data. A discrete embedding space is first built using hash functions applied to the sensor measurements from which a Markov process model is derived, approximating the complex system's dynamics. A control strategy is then learned using reinforcement learning once rewards relevant with respect to the control objective are identified. This method is designed for experimental configurations, requiring no computations nor prior knowledge of the system, and enjoys intrinsic robustness. It is illustrated on two systems: the control of the transitions of a Lorenz 63 dynamical system, and the control of the drag of a cylinder flow. The method is shown to perform well. version:1
arxiv-1402-5876 | Manifold Gaussian Processes for Regression | http://arxiv.org/abs/1402.5876 | id:1402.5876 author:Roberto Calandra, Jan Peters, Carl Edward Rasmussen, Marc Peter Deisenroth category:stat.ML cs.LG  published:2014-02-24 summary:Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts. version:4
arxiv-1604-02878 | Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks | http://arxiv.org/abs/1604.02878 | id:1604.02878 author:Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao category:cs.CV  published:2016-04-11 summary:Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance. version:1
arxiv-1601-06070 | Efficient Globally Optimal 2D-to-3D Deformable Shape Matching | http://arxiv.org/abs/1601.06070 | id:1601.06070 author:Zorah Lähner, Emanuele Rodolà, Frank R. Schmidt, Michael M. Bronstein, Daniel Cremers category:cs.CV  published:2016-01-22 summary:We propose the first algorithm for non-rigid 2D-to-3D shape matching, where the input is a 2D shape represented as a planar curve and a 3D shape represented as a surface; the output is a continuous curve on the surface. We cast the problem as finding the shortest circular path on the prod- uct 3-manifold of the surface and the curve. We prove that the optimal matching can be computed in polynomial time with a (worst-case) complexity of $O(mn^2\log(n))$, where $m$ and $n$ denote the number of vertices on the template curve and the 3D shape respectively. We also demonstrate that in practice the runtime is essentially linear in $m\!\cdot\! n$ making it an efficient method for shape analysis and shape retrieval. Quantitative evaluation confirms that the method provides excellent results for sketch-based deformable 3D shape re- trieval. version:2
arxiv-1604-02855 | Active Learning for Online Recognition of Human Activities from Streaming Videos | http://arxiv.org/abs/1604.02855 | id:1604.02855 author:Rocco De Rosa, Ilaria Gori, Fabio Cuzzolin, Barbara Caputo, Nicolò Cesa-Bianchi category:stat.ML cs.CV cs.LG  published:2016-04-11 summary:Recognising human activities from streaming videos poses unique challenges to learning algorithms: predictive models need to be scalable, incrementally trainable, and must remain bounded in size even when the data stream is arbitrarily long. Furthermore, as parameter tuning is problematic in a streaming setting, suitable approaches should be parameterless, and make no assumptions on what class labels may occur in the stream. We present here an approach to the recognition of human actions from streaming data which meets all these requirements by: (1) incrementally learning a model which adaptively covers the feature space with simple local classifiers; (2) employing an active learning strategy to reduce annotation requests; (3) achieving promising accuracy within a fixed model size. Extensive experiments on standard benchmarks show that our approach is competitive with state-of-the-art non-incremental methods, and outperforms the existing active incremental baselines. version:1
arxiv-1512-00933 | Probabilistic Integration: A Role for Statisticians in Numerical Analysis? | http://arxiv.org/abs/1512.00933 | id:1512.00933 author:François-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A. Osborne, Dino Sejdinovic category:stat.ML cs.NA math.NA math.ST stat.CO stat.TH  published:2015-12-03 summary:A research frontier has emerged in scientific computation, founded on the principle that numerical error entails epistemic uncertainty that ought to be subjected to statistical analysis. This viewpoint raises several interesting challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational pipeline. This paper examines thoroughly the case for probabilistic numerical methods in statistical computation and a specific case study is presented for Markov chain and Quasi Monte Carlo methods. A probabilistic integrator is equipped with a full distribution over its output, providing a measure of epistemic uncertainty that is shown to be statistically valid at finite computational levels, as well as in asymptotic regimes. The approach is motivated by expensive integration problems, where, as in krigging, one is willing to expend, at worst, cubic computational effort in order to gain uncertainty quantification. There, probabilistic integrators enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assessment of the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and uncertainty quantification in oil reservoir modelling. version:4
arxiv-1604-02843 | Method of Tibetan Person Knowledge Extraction | http://arxiv.org/abs/1604.02843 | id:1604.02843 author:Yuan Sun, Zhen Zhu category:cs.CL  published:2016-04-11 summary:Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction. version:1
arxiv-1510-02516 | Differential Evolution with Generalized Mutation Operator for Parameters Optimization in Gene Selection for Cancer Classification | http://arxiv.org/abs/1510.02516 | id:1510.02516 author:H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei category:cs.NE  published:2015-10-08 summary:Differential Evolution (DE) proved to be one of the most successful evolutionary algorithms for global optimization purposes in continuous problems. The core operator in DE is mutation which can provide the algorithm with both exploration and exploitation. In this article, a new notation for DE is proposed which has a formula that can be utilized for generating and extracting novel mutations and by applying this new notation, four novel mutations are proposed. More importantly, by combining these novel trial vector generation strategies and four other well-known ones, we proposed Generalized Mutation Differential Evolution (GMDE) that takes advantage of two mutation pools that have both explorative and exploitative strategies inside them. Results and experimental analysis are performed on CEC2005 benchmarks and the results stated that GMDE is surprisingly competitive and significantly improved the performance of this algorithm. Finally, GMDE is also applied to parameters optimization, modification and improvement of a feature selection method for cancer classification purposes over gene expression microarray profiles. version:2
arxiv-1604-02815 | Beyond Brightness Constancy: Learning Noise Models for Optical Flow | http://arxiv.org/abs/1604.02815 | id:1604.02815 author:Dan Rosenbaum, Yair Weiss category:cs.CV  published:2016-04-11 summary:Optical flow is typically estimated by minimizing a "data cost" and an optional regularizer. While there has been much work on different regularizers many modern algorithms still use a data cost that is not very different from the ones used over 30 years ago: a robust version of brightness constancy or gradient constancy. In this paper we leverage the recent availability of ground-truth optical flow databases in order to learn a data cost. Specifically we take a generative approach in which the data cost models the distribution of noise after warping an image according to the flow and we measure the "goodness" of a data cost by how well it matches the true distribution of flow warp error. Consistent with current practice, we find that robust versions of gradient constancy are better models than simple brightness constancy but a learned GMM that models the density of patches of warp error gives a much better fit than any existing assumption of constancy. This significant advantage of the GMM is due to an explicit modeling of the spatial structure of warp errors, a feature which is missing from almost all existing data costs in optical flow. Finally, we show how a good density model of warp error patches can be used for optical flow estimation on whole images. We replace the data cost by the expected patch log-likelihood (EPLL), and show how this cost can be optimized iteratively using an additional step of denoising the warp error image. The results of our experiments are promising and show that patch models with higher likelihood lead to better optical flow estimation. version:1
arxiv-1604-02808 | NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis | http://arxiv.org/abs/1604.02808 | id:1604.02808 author:Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang category:cs.CV  published:2016-04-11 summary:Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis. version:1
arxiv-1604-02801 | Capturing Dynamic Textured Surfaces of Moving Targets | http://arxiv.org/abs/1604.02801 | id:1604.02801 author:Ruizhe Wang, Lingyu Wei, Etienne Vouga, Qixing Huang, Duygu Ceylan, Gerard Medioni, Hao Li category:cs.CV  published:2016-04-11 summary:We present an end-to-end system for reconstructing complete watertight and textured models of moving subjects such as clothed humans and animals, using only three or four handheld sensors. The heart of our framework is a new pairwise registration algorithm that minimizes, using a particle swarm strategy, an alignment error metric based on mutual visibility and occlusion. We show that this algorithm reliably registers partial scans with as little as 15% overlap without requiring any initial correspondences, and outperforms alternative global registration algorithms. This registration algorithm allows us to reconstruct moving subjects from free-viewpoint video produced by consumer-grade sensors, without extensive sensor calibration, constrained capture volume, expensive arrays of cameras, or templates of the subject geometry. version:1
arxiv-1512-07711 | Adaptive Object Detection Using Adjacency and Zoom Prediction | http://arxiv.org/abs/1512.07711 | id:1512.07711 author:Yongxi Lu, Tara Javidi, Svetlana Lazebnik category:cs.CV  published:2015-12-24 summary:State-of-the-art object detection systems rely on an accurate set of region proposals. Several recent methods use a neural network architecture to hypothesize promising object locations. While these approaches are computationally efficient, they rely on fixed image regions as anchors for predictions. In this paper we propose to use a search strategy that adaptively directs computational resources to sub-regions likely to contain objects. Compared to methods based on fixed anchor locations, our approach naturally adapts to cases where object instances are sparse and small. Our approach is comparable in terms of accuracy to the state-of-the-art Faster R-CNN approach while using two orders of magnitude fewer anchors on average. Code is publicly available. version:2
arxiv-1604-03099 | Symbolic Knowledge Extraction using Łukasiewicz Logics | http://arxiv.org/abs/1604.03099 | id:1604.03099 author:Carlos Leandro category:cs.AI cs.LG 03B52  92B20  68T05  published:2016-04-11 summary:This work describes a methodology that combines logic-based systems and connectionist systems. Our approach uses finite truth-valued {\L}ukasiewicz logic, wherein every connective can be defined by a neuron in an artificial network. This allowed the injection of first-order formulas into a network architecture, and also simplified symbolic rule extraction. For that we trained a neural networks using the Levenderg-Marquardt algorithm, where we restricted the knowledge dissemination in the network structure. This procedure reduces neural network plasticity without drastically damaging the learning performance, thus making the descriptive power of produced neural networks similar to the descriptive power of {\L}ukasiewicz logic language and simplifying the translation between symbolic and connectionist structures. We used this method for reverse engineering truth table and in extraction of formulas from real data sets. version:1
arxiv-1411-0023 | Validation of Matching | http://arxiv.org/abs/1411.0023 | id:1411.0023 author:Ya Le, Eric Bax, Nicola Barbieri, David Garcia Soriano, Jitesh Mehta, James Li category:cs.LG stat.ML  published:2014-10-31 summary:We introduce a technique to compute probably approximately correct (PAC) bounds on precision and recall for matching algorithms. The bounds require some verified matches, but those matches may be used to develop the algorithms. The bounds can be applied to network reconciliation or entity resolution algorithms, which identify nodes in different networks or values in a data set that correspond to the same entity. For network reconciliation, the bounds do not require knowledge of the network generation process. version:2
arxiv-1512-05227 | Fine-grained Categorization and Dataset Bootstrapping using Deep Metric Learning with Humans in the Loop | http://arxiv.org/abs/1512.05227 | id:1512.05227 author:Yin Cui, Feng Zhou, Yuanqing Lin, Serge Belongie category:cs.CV  published:2015-12-16 summary:Existing fine-grained visual categorization methods often suffer from three challenges: lack of training data, large number of fine-grained categories, and high intraclass vs. low inter-class variance. In this work we propose a generic iterative framework for fine-grained categorization and dataset bootstrapping that handles these three challenges. Using deep metric learning with humans in the loop, we learn a low dimensional feature embedding with anchor points on manifolds for each category. These anchor points capture intra-class variances and remain discriminative between classes. In each round, images with high confidence scores from our model are sent to humans for labeling. By comparing with exemplar images, labelers mark each candidate image as either a "true positive" or a "false positive". True positives are added into our current dataset and false positives are regarded as "hard negatives" for our metric learning model. Then the model is retrained with an expanded dataset and hard negatives for the next round. To demonstrate the effectiveness of the proposed framework, we bootstrap a fine-grained flower dataset with 620 categories from Instagram images. The proposed deep metric learning scheme is evaluated on both our dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show significant performance gain using dataset bootstrapping and demonstrate state-of-the-art results achieved by the proposed deep metric learning methods. version:2
arxiv-1602-07807 | Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection | http://arxiv.org/abs/1602.07807 | id:1602.07807 author:Michael Bloodgood, Benjamin Strauss category:cs.DB cs.CL stat.ML  published:2016-02-25 summary:Many important forms of data are stored digitally in XML format. Errors can occur in the textual content of the data in the fields of the XML. Fixing these errors manually is time-consuming and expensive, especially for large amounts of data. There is increasing interest in the research, development, and use of automated techniques for assisting with data cleaning. Electronic dictionaries are an important form of data frequently stored in XML format that frequently have errors introduced through a mixture of manual typographical entry errors and optical character recognition errors. In this paper we describe methods for flagging statistical anomalies as likely errors in electronic dictionaries stored in XML format. We describe six systems based on different sources of information. The systems detect errors using various signals in the data including uncommon characters, text length, character-based language models, word-based language models, tied-field length ratios, and tied-field transliteration models. Four of the systems detect errors based on expectations automatically inferred from content within elements of a single field type. We call these single-field systems. Two of the systems detect errors based on correspondence expectations automatically inferred from content within elements of multiple related field types. We call these tied-field systems. For each system, we provide an intuitive analysis of the type of error that it is successful at detecting. Finally, we describe two larger-scale evaluations using crowdsourcing with Amazon's Mechanical Turk platform and using the annotations of a domain expert. The evaluations consistently show that the systems are useful for improving the efficiency with which errors in XML electronic dictionaries can be detected. version:2
arxiv-1511-04164 | Natural Language Object Retrieval | http://arxiv.org/abs/1511.04164 | id:1511.04164 author:Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell category:cs.CV cs.CL  published:2015-11-13 summary:In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer. version:3
arxiv-1603-07057 | Do We Really Need to Collect Millions of Faces for Effective Face Recognition? | http://arxiv.org/abs/1603.07057 | id:1603.07057 author:Iacopo Masi, Anh Tuan Tran, Jatuporn Toy Leksut, Tal Hassner, Gerard Medioni category:cs.CV  published:2016-03-23 summary:Face recognition capabilities have recently made extraordinary leaps. Though this progress is at least partially due to ballooning training set sizes -- huge numbers of face images downloaded and labeled for identity -- it is not clear if the formidable task of collecting so many images is truly necessary. We propose a far more accessible means of increasing training data sizes for face recognition systems. Rather than manually harvesting and labeling more faces, we simply synthesize them. We describe novel methods of enriching an existing dataset with important facial appearance variations by manipulating the faces it contains. We further apply this synthesis approach when matching query images represented using a standard convolutional neural network. The effect of training and testing with synthesized images is extensively tested on the LFW and IJB-A (verification and identification) benchmarks and Janus CS2. The performances obtained by our approach match state of the art results reported by systems trained on millions of downloaded images. version:2
arxiv-1510-08174 | Visual Quality Enhancement in Optoacoustic Tomography using Active Contour Segmentation Priors | http://arxiv.org/abs/1510.08174 | id:1510.08174 author:Subhamoy Mandal, Xosé Luís Deán-Ben, Daniel Razansky category:physics.med-ph cs.CV physics.optics  published:2015-10-28 summary:Segmentation of biomedical images is essential for studying and characterizing anatomical structures, detection and evaluation of pathological tissues. Segmentation has been further shown to enhance the reconstruction performance in many tomographic imaging modalities by accounting for heterogeneities of the excitation field and tissue properties in the imaged region. This is particularly relevant in optoacoustic tomography, where discontinuities in the optical and acoustic tissue properties, if not properly accounted for, may result in deterioration of the imaging performance. Efficient segmentation of optoacoustic images is often hampered by the relatively low intrinsic contrast of large anatomical structures, which is further impaired by the limited angular coverage of some commonly employed tomographic imaging configurations. Herein, we analyze the performance of active contour models for boundary segmentation in cross-sectional optoacoustic tomography. The segmented mask is employed to construct a two compartment model for the acoustic and optical parameters of the imaged tissues, which is subsequently used to improve accuracy of the image reconstruction routines. The performance of the suggested segmentation and modeling approach are showcased in tissue-mimicking phantoms and small animal imaging experiments. version:3
arxiv-1604-02774 | Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz Fuzzy Logics using Linear Neural Networks | http://arxiv.org/abs/1604.02774 | id:1604.02774 author:Carlos Leandro category:cs.AI cs.NE 94D04  published:2016-04-11 summary:This work describes a methodology to combine logic-based systems and connectionist systems. Our approach uses finite truth valued {\L}ukasiewicz logic, where we take advantage of fact what in this type of logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one. This allowed the injection of first-order formulas in a network architecture, and also simplified symbolic rule extraction. Our method trains a neural network using Levenderg-Marquardt algorithm, where we restrict the knowledge dissemination in the network structure. We show how this reduces neural networks plasticity without damage drastically the learning performance. Making the descriptive power of produced neural networks similar to the descriptive power of {\L}ukasiewicz logic language, simplifying the translation between symbolic and connectionist structures. This method is used in the reverse engineering problem of finding the formula used on generation of a truth table for a multi-valued {\L}ukasiewicz logic. For real data sets the method is particularly useful for attribute selection, on binary classification problems defined using nominal attribute. After attribute selection and possible data set completion in the resulting connectionist model: neurons are directly representable using a disjunctive or conjunctive formulas, in the {\L}ukasiewicz logic, or neurons are interpretations which can be approximated by symbolic rules. This fact is exemplified, extracting symbolic knowledge from connectionist models generated for the data set Mushroom from UCI Machine Learning Repository. version:1
arxiv-1511-02283 | Generation and Comprehension of Unambiguous Object Descriptions | http://arxiv.org/abs/1511.02283 | id:1511.02283 author:Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy category:cs.CV cs.CL cs.LG cs.RO  published:2015-11-07 summary:We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/mjhucla/Google_Refexp_toolbox version:3
arxiv-1511-06653 | Semi-supervised Learning with Encoder-Decoder Recurrent Neural Networks: Experiments with Motion Capture Sequences | http://arxiv.org/abs/1511.06653 | id:1511.06653 author:Félix G. Harvey, Christopher Pal category:cs.CV cs.LG  published:2015-11-20 summary:Recent work on sequence to sequence translation using Recurrent Neural Networks (RNNs) based on Long Short Term Memory (LSTM) architectures has shown great potential for learning useful representations of sequential data. A one-to-many encoder-decoder(s) scheme allows for a single encoder to provide representations serving multiple purposes. In our case, we present an LSTM encoder network able to produce representations used by two decoders: one that reconstructs, and one that classifies if the training sequence has an associated label. This allows the network to learn representations that are useful for both discriminative and reconstructive tasks at the same time. This paradigm is well suited for semi-supervised learning with sequences and we test our proposed approach on an action recognition task using motion capture (MOCAP) sequences. We find that semi-supervised feature learning can improve state-of-the-art movement classification accuracy on the HDM05 action dataset. Further, we find that even when using only labeled data and a primarily discriminative objective the addition of a reconstructive decoder can serve as a form of regularization that reduces over-fitting and improves test set accuracy. version:2
arxiv-1604-02752 | Performance Trade-Offs in Multi-Processor Approximate Message Passing | http://arxiv.org/abs/1604.02752 | id:1604.02752 author:Junan Zhu, Ahmad Beirami, Dror Baron category:cs.IT cs.DC cs.LG math.IT  published:2016-04-10 summary:We consider large-scale linear inverse problems in Bayesian settings. Our general approach follows a recent line of work that applies the approximate message passing (AMP) framework in multi-processor (MP) computational systems by storing and processing a subset of rows of the measurement matrix along with corresponding measurements at each MP node. In each MP-AMP iteration, nodes of the MP system and its fusion center exchange lossily compressed messages pertaining to their estimates of the input. There is a trade-off between the physical costs of the reconstruction process including computation time, communication loads, and the reconstruction quality, and it is impossible to simultaneously minimize all the costs. We pose this minimization as a multi-objective optimization problem (MOP), and study the properties of the best trade-offs (Pareto optimality) in this MOP. We prove that the achievable region of this MOP is convex, and conjecture how the combined cost of computation and communication scales with the desired mean squared error. These properties are verified numerically. version:1
arxiv-1511-04166 | Unsupervised Learning of Edges | http://arxiv.org/abs/1511.04166 | id:1511.04166 author:Yin Li, Manohar Paluri, James M. Rehg, Piotr Dollár category:cs.CV  published:2015-11-13 summary:Data-driven approaches for edge detection have proven effective and achieve top results on modern benchmarks. However, all current data-driven edge detectors require manual supervision for training in the form of hand-labeled region segments or object boundaries. Specifically, human annotators mark semantically meaningful edges which are subsequently used for training. Is this form of strong, high-level supervision actually necessary to learn to accurately detect edges? In this work we present a simple yet effective approach for training edge detectors without human supervision. To this end we utilize motion, and more specifically, the only input to our method is noisy semi-dense matches between frames. We begin with only a rudimentary knowledge of edges (in the form of image gradients), and alternate between improving motion estimation and edge detection in turn. Using a large corpus of video data, we show that edge detectors trained using our unsupervised scheme approach the performance of the same methods trained with full supervision (within 3-5%). Finally, we show that when using a deep network for the edge detector, our approach provides a novel pre-training scheme for object detection. version:2
arxiv-1604-02737 | Correlated Equilibria for Approximate Variational Inference in MRFs | http://arxiv.org/abs/1604.02737 | id:1604.02737 author:Luis E. Ortiz, Ze Gong category:cs.AI cs.GT stat.ML  published:2016-04-10 summary:Almost all of the work in graphical models for game theory has mirrored previous work in probabilistic graphical models. Our work considers the opposite direction: Taking advantage of recent advances in equilibrium computation for belief inference. In particular, we present formulations of inference problems in Markov random fields (MRFs) as computation of equilibria in a certain class of game-theoretic graphical models. While some previous work explores this direction, none of that work concretely establishes the precise connection between variational probabilistic inference in MRFs and correlated equilibria. There is no work that exploits recent theoretical and empirical results from the literature on algorithmic and computational game theory on the tractable, polynomial-time computation of exact or approximate correlated equilibria in graphical games with arbitrary, loopy graph structure. Our work discusses how to design new algorithms with equally tractable guarantees for the computation of approximate variational inference in MRFs. In addition, inspired by a previously stated game-theoretic view of state-of-the-art tree-reweighed (TRW) message-passing techniques for belief inference as zero-sum game, we propose a different, general-sum potential game to design approximate fictitious-play techniques. We perform synthetic experiments evaluating our proposed approximation algorithms with standard methods and TRW on several classes of classical Ising models. Our experiments show that our global approach is competitive, particularly shinning in a class of Ising models with constant, "highly attractive" edge-weights, in which it is often better than all other alternatives we evaluated. While our local approach was not as effective as our global approach or TRW, almost all of the alternatives are often no better than a simple baseline: estimate the marginal probability to be 0.5. version:1
arxiv-1603-07593 | Evaluating the Performance of Offensive Linemen in the NFL | http://arxiv.org/abs/1603.07593 | id:1603.07593 author:Nikhil Byanna, Diego Klabjan category:stat.ML  published:2016-03-24 summary:How does one objectively measure the performance of an individual offensive lineman in the NFL? The existing literature proposes various measures that rely on subjective assessments of game film, but has yet to develop an objective methodology to evaluate performance. Using a variety of statistics related to an offensive lineman's performance, we develop a framework to objectively analyze the overall performance of an individual offensive lineman and determine specific linemen who are overvalued or undervalued relative to their salary. We identify eight players across the 2013-2014 and 2014-2015 NFL seasons that are considered to be overvalued or undervalued and corroborate the results with existing metrics that are based on subjective evaluation. To the best of our knowledge, the techniques set forth in this work have not been utilized in previous works to evaluate the performance of NFL players at any position, including offensive linemen. version:2
arxiv-1604-02715 | Soccer Field Localization from a Single Image | http://arxiv.org/abs/1604.02715 | id:1604.02715 author:Namdar Homayounfar, Sanja Fidler, Raquel Urtasun category:cs.CV  published:2016-04-10 summary:In this work, we propose a novel way of efficiently localizing a soccer field from a single broadcast image of the game. Related work in this area relies on manually annotating a few key frames and extending the localization to similar images, or installing fixed specialized cameras in the stadium from which the layout of the field can be obtained. In contrast, we formulate this problem as a branch and bound inference in a Markov random field where an energy function is defined in terms of field cues such as grass, lines and circles. Moreover, our approach is fully automatic and depends only on single images from the broadcast video of the game. We demonstrate the effectiveness of our method by applying it to various games and obtain promising results. Finally, we posit that our approach can be applied easily to other sports such as hockey and basketball. version:1
arxiv-1512-01815 | PatchBatch: a Batch Augmented Loss for Optical Flow | http://arxiv.org/abs/1512.01815 | id:1512.01815 author:David Gadot, Lior Wolf category:cs.CV  published:2015-12-06 summary:We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks. version:2
arxiv-1602-08753 | Stability and Structural Properties of Gene Regulation Networks with Coregulation Rules | http://arxiv.org/abs/1602.08753 | id:1602.08753 author:Jonathan H. Warrell, Musa M. Mhlanga category:stat.ML q-bio.MN q-bio.QM  published:2016-02-28 summary:Coregulation of the expression of groups of genes has been extensively demonstrated empirically in bacterial and eukaryotic systems. Such coregulation can arise through the use of shared regulatory motifs, which allow the coordinated expression of modules (and module groups) of functionally related genes across the genome. Coregulation can also arise through the physical association of multi-gene complexes through chromosomal looping, which are then transcribed together. We present a general formalism for modeling coregulation rules in the framework of Random Boolean Networks (RBN), and develop specific models for transcription factor networks with modular structure (including module groups, and multi-input modules (MIM) with autoregulation) and multi-gene complexes (including hierarchical differentiation between multi-gene complex members). We develop a mean-field approach to analyse the stability of large networks incorporating coregulation, and show that autoregulated MIM and hierarchical gene-complex models can achieve greater stability than networks without coregulation whose rules have matching activation frequency. We provide further analysis of the stability of small networks of both kinds through simulations. We also characterize several general properties of the transients and attractors in the hierarchical coregulation model, and show using simulations that the steady-state distribution factorizes hierarchically as a Bayesian network in a Markov Jump Process analogue of the RBN model. version:2
arxiv-1602-06429 | Generalized Statistical Tests for mRNA and Protein Subcellular Spatial Patterning against Complete Spatial Randomness | http://arxiv.org/abs/1602.06429 | id:1602.06429 author:Jonathan H. Warrell, Anca F. Savulescu, Robyn Brackin, Musa M. Mhlanga category:stat.ML q-bio.QM stat.AP  published:2016-02-20 summary:We derive generalized estimators for a number of spatial statistics that have been used in the analysis of spatially resolved omics data, such as Ripley's K, H and L functions, clustering index, and degree of clustering, which allow these statistics to be calculated on data modelled by arbitrary random measures (RMs). Our estimators generalize those typically used to calculate these statistics on point process data, allowing them to be calculated on RMs which assign continuous values to spatial regions, for instance to model protein intensity. The clustering index (H*) compares Ripley's H function calculated empirically to its distribution under complete spatial randomness (CSR), leading us to consider CSR null hypotheses for RMs which are not point-processes when generalizing this statistic. We thus consider restricted classes of completely random measures which can be simulated directly (Gamma processes and Marked Poisson Processes), as well as the general class of all CSR RMs, for which we derive an exact permutation-based H* estimator. We establish several properties of the estimators, including bounds on the accuracy of our general Ripley K estimator, its relationship to a previous estimator for the cross-correlation measure, and the relationship of our generalized H* estimator to previous statistics. To test the ability of our approach to identify spatial patterning, we use Fluorescent In Situ Hybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA and protein subcellular localization patterns respectively in polarizing mouse fibroblasts on micropattened cells. We observe correlated patterns of clustering over time for corresponding mRNAs and proteins, suggesting a deterministic effect of mRNA localization on protein localization for several pairs tested, including one case in which spatial patterning at the mRNA level has not been previously demonstrated. version:3
arxiv-1604-02677 | DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation | http://arxiv.org/abs/1604.02677 | id:1604.02677 author:Hao Chen, Xiaojuan Qi, Lequan Yu, Pheng-Ann Heng category:cs.CV  published:2016-04-10 summary:The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin. version:1
arxiv-1604-02668 | Distance for Functional Data Clustering Based on Smoothing Parameter Commutation | http://arxiv.org/abs/1604.02668 | id:1604.02668 author:ShengLi Tzeng, Christian Hennig, Yu-Fen Li, Chien-Ju Lin category:stat.ME stat.AP stat.ML  published:2016-04-10 summary:We propose a novel method to determine the dissimilarity between subjects for functional data clustering. Spline smoothing or interpolation is common to deal with data of such type. Instead of estimating the best-representing curve for each subject as fixed during clustering, we measure the dissimilarity between subjects based on varying curve estimates with commutation of smoothing parameters pair-by-pair (of subjects). The intuitions are that smoothing parameters of smoothing splines reflect inverse signal-to-noise ratios and that applying an identical smoothing parameter the smoothed curves for two similar subjects are expected to be close. The effectiveness of our proposal is shown through simulations comparing to other dissimilarity measures. It also has several pragmatic advantages. First, missing values or irregular time points can be handled directly, thanks to the nature of smoothing splines. Second, conventional clustering method based on dissimilarity can be employed straightforward, and the dissimilarity also serves as a useful tool for outlier detection. Third, the implementation is almost handy since subroutines for smoothing splines and numerical integration are widely available. Fourth, the computational complexity does not increase and is parallel with that in calculating Euclidean distance between curves estimated by smoothing splines. version:1
arxiv-1603-08895 | Latent Embeddings for Zero-shot Classification | http://arxiv.org/abs/1603.08895 | id:1603.08895 author:Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, Bernt Schiele category:cs.CV  published:2016-03-29 summary:We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps. version:2
arxiv-1604-02657 | Direction matters: hand pose estimation from local surface normals | http://arxiv.org/abs/1604.02657 | id:1604.02657 author:Chengde Wan, Angela Yao, Luc Van Gool category:cs.CV  published:2016-04-10 summary:We present a hierarchical regression framework for estimating hand joint positions from single depth images based on local surface normals. The hierarchical regression follows the tree structured topology of hand from wrist to finger tips. We propose a conditional regression forest, i.e., the Frame Conditioned Regression Forest (FCRF) which uses a new normal difference feature. At each stage of the regression, the frame of reference is established from either the local surface normal or previously estimated hand joints. By making the regression with respect to the local frame, the pose estimation is more robust to rigid transformations. We also introduce a new efficient approximation to estimate surface normals. We verify the effectiveness of our method by conducting experiments on two challenging real-world datasets and show consistent improvements over previous discriminative pose estimation methods. version:1
arxiv-1604-02647 | Real-Time Facial Segmentation and Performance Capture from RGB Input | http://arxiv.org/abs/1604.02647 | id:1604.02647 author:Shunsuke Saito, Tianye Li, Hao Li category:cs.CV  published:2016-04-10 summary:We introduce the concept of unconstrained real-time 3D facial performance capture through explicit semantic segmentation in the RGB input. To ensure robustness, cutting edge supervised learning approaches rely on large training datasets of face images captured in the wild. While impressive tracking quality has been demonstrated for faces that are largely visible, any occlusion due to hair, accessories, or hand-to-face gestures would result in significant visual artifacts and loss of tracking accuracy. The modeling of occlusions has been mostly avoided due to its immense space of appearance variability. To address this curse of high dimensionality, we perform tracking in unconstrained images assuming non-face regions can be fully masked out. Along with recent breakthroughs in deep learning, we demonstrate that pixel-level facial segmentation is possible in real-time by repurposing convolutional neural networks designed originally for general semantic segmentation. We develop an efficient architecture based on a two-stream deconvolution network with complementary characteristics, and introduce carefully designed training samples and data augmentation strategies for improved segmentation accuracy and robustness. We adopt a state-of-the-art regression-based facial tracking framework with segmented face images as training, and demonstrate accurate and uninterrupted facial performance capture in the presence of extreme occlusion and even side views. Furthermore, the resulting segmentation can be directly used to composite partial 3D face models on the input images and enable seamless facial manipulation tasks, such as virtual make-up or face replacement. version:1
arxiv-1604-02634 | Online Nonnegative Matrix Factorization with Outliers | http://arxiv.org/abs/1604.02634 | id:1604.02634 author:Renbo Zhao, Vincent Y. F. Tan category:stat.ML cs.LG math.OC stat.ME  published:2016-04-10 summary:We propose a unified and systematic framework for performing online nonnegative matrix factorization in the presence of outliers that is particularly suited to large datasets. Within this framework, we propose two solvers based on proximal gradient descent and alternating direction method of multipliers. We prove that the objective function converges almost surely by appealing to the quasi-martingale convergence theorem. We also show the learned basis matrix converges to the set of local minimizers of the objective function almost surely. In addition, we extend our basic problem formulation to various settings with different constraints and regularizers, and adapt the solvers and analyses to each setting. We perform extensive experiments on both synthetic and image datasets. These experiments demonstrate the efficiency and efficacy of our algorithm on tasks such as basis learning, image denoising and shadow removal. version:1
arxiv-1604-02631 | Grid Based Nonlinear Filtering Revisited: Recursive Estimation & Asymptotic Optimality | http://arxiv.org/abs/1604.02631 | id:1604.02631 author:Dionysios S. Kalogerias, Athina P. Petropulu category:math.ST cs.IT math.IT math.OC stat.ME stat.ML stat.TH  published:2016-04-10 summary:We revisit the development of grid based recursive approximate filtering of general Markov processes in discrete time, partially observed in conditionally Gaussian noise. The grid based filters considered rely on two types of state quantization: The \textit{Markovian} type and the \textit{marginal} type. We propose a set of novel, relaxed sufficient conditions, ensuring strong and fully characterized pathwise convergence of these filters to the respective MMSE state estimator. In particular, for marginal state quantizations, we introduce the notion of \textit{conditional regularity of stochastic kernels}, which, to the best of our knowledge, constitutes the most relaxed condition proposed, under which asymptotic optimality of the respective grid based filters is guaranteed. Further, we extend our convergence results, including filtering of bounded and continuous functionals of the state, as well as recursive approximate state prediction. For both Markovian and marginal quantizations, the whole development of the respective grid based filters relies more on linear-algebraic techniques and less on measure theoretic arguments, making the presentation considerably shorter and technically simpler. version:1
arxiv-1604-02612 | Fusing Audio, Textual and Visual Features for Sentiment Analysis of News Videos | http://arxiv.org/abs/1604.02612 | id:1604.02612 author:Moisés H. R. Pereira, Flávio L. C. Pádua, Adriano C. M. Pereira, Fabrício Benevenuto, Daniel H. Dalip category:cs.CL  published:2016-04-09 summary:This paper presents a novel approach to perform sentiment analysis of news videos, based on the fusion of audio, textual and visual clues extracted from their contents. The proposed approach aims at contributing to the semiodiscoursive study regarding the construction of the ethos (identity) of this media universe, which has become a central part of the modern-day lives of millions of people. To achieve this goal, we apply state-of-the-art computational methods for (1) automatic emotion recognition from facial expressions, (2) extraction of modulations in the participants' speeches and (3) sentiment analysis from the closed caption associated to the videos of interest. More specifically, we compute features, such as, visual intensities of recognized emotions, field sizes of participants, voicing probability, sound loudness, speech fundamental frequencies and the sentiment scores (polarities) from text sentences in the closed caption. Experimental results with a dataset containing 520 annotated news videos from three Brazilian and one American popular TV newscasts show that our approach achieves an accuracy of up to 84% in the sentiments (tension levels) classification task, thus demonstrating its high potential to be used by media analysts in several applications, especially, in the journalistic domain. version:1
arxiv-1604-02606 | A General Retraining Framework for Scalable Adversarial Classification | http://arxiv.org/abs/1604.02606 | id:1604.02606 author:Bo Li, Yevgeniy Vorobeychik, Xinyun Chen category:cs.GT cs.LG stat.ML  published:2016-04-09 summary:Traditional classification algorithms assume that training and test data come from the same or similar distribution. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the first systematic and general-purpose retraining framework which can: a) boost robustness of an arbitrary learning algorithm, and b) incorporate a broad class of adversarial models. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. We also offer a very general adversarial evasion model and algorithmic framework based on coordinate greedy local search. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but far more scalable and general. The experiments also confirm that without retraining, our adversarial framework is extremely effective in dramatically reducing the effectiveness of learning. In contrast, retraining significantly boosts robustness to evasion attacks without compromising much overall accuracy. version:1
arxiv-1601-04149 | $\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images | http://arxiv.org/abs/1601.04149 | id:1601.04149 author:Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, Thomas S. Huang category:cs.CV cs.AI cs.LG  published:2016-01-16 summary:In this paper, we design a Deep Dual-Domain ($\mathbf{D^3}$) based fast restoration model to remove artifacts of JPEG compressed images. It leverages the large learning capacity of deep networks, as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures. For the latter, we take into consideration both the prior knowledge of the JPEG compression scheme, and the successful practice of the sparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and light-weighted feed-forward approximation of sparse coding. Extensive experiments verify the superiority of the proposed $D^3$ model over several state-of-the-art methods. Specifically, our best model is capable of outperforming the latest deep model for around 1 dB in PSNR, and is 30 times faster. version:3
arxiv-1604-02594 | Learning Compact Recurrent Neural Networks | http://arxiv.org/abs/1604.02594 | id:1604.02594 author:Zhiyun Lu, Vikas Sindhwani, Tara N. Sainath category:cs.LG cs.CL cs.NE  published:2016-04-09 summary:Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, have produced state-of-the-art results on a variety of speech recognition tasks. However, these models are often too large in size for deployment on mobile devices with memory and latency constraints. In this work, we study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. Our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance. A hybrid strategy of using structured matrices in the bottom layers and shared low-rank factors on the top layers is found to be particularly effective, reducing the parameters of a standard LSTM by 75%, at a small cost of 0.3% increase in WER, on a 2,000-hr English Voice Search task. version:1
arxiv-1512-05193 | ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs | http://arxiv.org/abs/1512.05193 | id:1512.05193 author:Wenpeng Yin, Hinrich Schütze, Bing Xiang, Bowen Zhou category:cs.CL  published:2015-12-16 summary:How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence's representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNN; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNN achieves state-of-the-art performance on AS, PI and TE tasks. version:3
arxiv-1604-02546 | Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic Deep Features | http://arxiv.org/abs/1604.02546 | id:1604.02546 author:Lorenzo Baraldi, Costantino Grana, Rita Cucchiara category:cs.CV cs.IR cs.MM  published:2016-04-09 summary:This paper presents a novel retrieval pipeline for video collections, which aims to retrieve the most significant parts of an edited video for a given query, and represent them with thumbnails which are at the same time semantically meaningful and aesthetically remarkable. Videos are first segmented into coherent and story-telling scenes, then a retrieval algorithm based on deep learning is proposed to retrieve the most significant scenes for a textual query. A ranking strategy based on deep features is finally used to tackle the problem of visualizing the best thumbnail. Qualitative and quantitative experiments are conducted on a collection of edited videos to demonstrate the effectiveness of our approach. version:1
arxiv-1511-03416 | Visual7W: Grounded Question Answering in Images | http://arxiv.org/abs/1511.03416 | id:1511.03416 author:Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei category:cs.CV cs.LG cs.NE  published:2015-11-11 summary:We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks. version:4
arxiv-1604-02531 | Person Re-identification in the Wild | http://arxiv.org/abs/1604.02531 | id:1604.02531 author:Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Qi Tian category:cs.CV  published:2016-04-09 summary:We present a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification accuracy and assessing the effectiveness of different detectors for re-identification. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-identification through two simple yet effective improvements: a discriminatively trained ID-discriminative Embedding (IDE) in the person subspace using convolutional neural network (CNN) features and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-identification. version:1
arxiv-1407-6810 | Dissimilarity-based Sparse Subset Selection | http://arxiv.org/abs/1407.6810 | id:1407.6810 author:Ehsan Elhamifar, Guillermo Sapiro, S. Shankar Sastry category:cs.LG stat.ML  published:2014-07-25 summary:Finding an informative subset of a large collection of data points or models is at the center of many problems in computer vision, recommender systems, bio/health informatics as well as image and natural language processing. Given pairwise dissimilarities between the elements of a `source set' and a `target set,' we consider the problem of finding a subset of the source set, called representatives or exemplars, that can efficiently describe the target set. We formulate the problem as a row-sparsity regularized trace minimization problem. Since the proposed formulation is, in general, NP-hard, we consider a convex relaxation. The solution of our optimization finds representatives and the assignment of each element of the target set to each representative, hence, obtaining a clustering. We analyze the solution of our proposed optimization as a function of the regularization parameter. We show that when the two sets jointly partition into multiple groups, our algorithm finds representatives from all groups and reveals clustering of the sets. In addition, we show that the proposed framework can effectively deal with outliers. Our algorithm works with arbitrary dissimilarities, which can be asymmetric or violate the triangle inequality. To efficiently implement our algorithm, we consider an Alternating Direction Method of Multipliers (ADMM) framework, which results in quadratic complexity in the problem size. We show that the ADMM implementation allows to parallelize the algorithm, hence further reducing the computational time. Finally, by experiments on real-world datasets, we show that our proposed algorithm improves the state of the art on the two problems of scene categorization using representative images and time-series modeling and segmentation using representative~models. version:2
arxiv-1604-02506 | Higher order features and recurrent neural networks based on Long-Short Term Memory nodes in supervised biomedical word sense disambiguation | http://arxiv.org/abs/1604.02506 | id:1604.02506 author:Antonio Jimeno Yepes category:cs.CL cs.LG  published:2016-04-09 summary:Word sense disambiguation helps identifying the proper sense of ambiguous words in text. With large terminologies such as the UMLS Metathesaurus ambiguities appear and highly effective disambiguation methods are required. Supervised learning algorithm methods are used as one of the approaches to perform disambiguation. Features extracted from the context of an ambiguous word are used to identify the proper sense of such a word. The type of features have an impact on machine learning methods, thus affect disambiguation performance. In this work, we have evaluated several types of features derived from the context of the ambiguous word and we have explored as well more global features derived from MEDLINE using word embeddings. Results show that word embeddings improve the performance of more traditional features and allow as well using recurrent neural networks based on Long-Short Term Memory (LSTM) nodes, which further improve the disambiguation performance. The combination of unigrams and word embeddings set a new state of the art performance with an accuracy of 95.97 in the MSH WSD data set. version:1
arxiv-1505-05901 | Randomized Robust Subspace Recovery for High Dimensional Data Matrices | http://arxiv.org/abs/1505.05901 | id:1505.05901 author:Mostafa Rahmani, George Atia category:stat.ML cs.CV  published:2015-05-21 summary:This paper explores and analyzes two randomized designs for robust Principal Component Analysis (PCA) employing low-dimensional data sketching. In one design, a data sketch is constructed using random column sampling followed by low dimensional embedding, while in the other, sketching is based on random column and row sampling. Both designs are shown to bring about substantial savings in complexity and memory requirements for robust subspace learning over conventional approaches that use the full scale data. A characterization of the sample and computational complexity of both designs is derived in the context of two distinct outlier models, namely, sparse and independent outlier models. The proposed randomized approach can provably recover the correct subspace with computational and sample complexity that are almost independent of the size of the data. The results of the mathematical analysis are confirmed through numerical simulations using both synthetic and real data. version:2
arxiv-1508-06585 | Towards universal neural nets: Gibbs machines and ACE | http://arxiv.org/abs/1508.06585 | id:1508.06585 author:Galin Georgiev category:cs.CV cs.LG cs.NE  published:2015-08-26 summary:We study a class of neural nets - \emph{Gibbs machines} - which are a type of variational auto-encoders, designed for gradual learning. They offer an universal platform for incrementally adding newly learned features, including physical symmetries, and are directly connected to information geometry and thermodynamics. Combining them with classifiers, gives rise to a brand of universal generative neural nets - stochastic auto-classifier-encoders (ACE). ACE have state-of-the-art performance in their class, both for classification and density estimation for the MNIST data set. version:4
arxiv-1604-02492 | Challenges in Bayesian Adaptive Data Analysis | http://arxiv.org/abs/1604.02492 | id:1604.02492 author:Sam Elder category:cs.LG stat.ML  published:2016-04-08 summary:Traditional statistical analysis requires that the analysis process and data are independent. By contrast, the new field of adaptive data analysis hopes to understand and provide algorithms and accuracy guarantees for research as it is commonly performed in practice, as an iterative process of proposing hypotheses and interacting with the data set. Previous work has established a model with a rather strong lower bound on sample complexity in terms of the number of queries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harder than static data analysis, where $n\sim\log q$ is possible. Instead, we argue that those strong lower bounds point to a shortcoming in the model, an informational asymmetry with no basis in applications. In its place, we propose a new Bayesian version of the problem without this unnecessary asymmetry. The previous lower bounds are no longer valid, which offers the possibility for stronger results. However, we show that a large family of methods, including all previously proposed algorithms, cannot achieve the static dependence of $n\sim\log q$ even in this regime, establishing polylogarithmic lower bounds with a new family of lower bounds. These preliminary results suggest that adaptive data analysis is harder than static data analysis even without this information asymmetry, but still leave wide open the possibility that new algorithms can be developed to work with fewer samples than the previous best known algorithms. version:1
arxiv-1511-02841 | Symmetries and control in generative neural nets | http://arxiv.org/abs/1511.02841 | id:1511.02841 author:Galin Georgiev category:cs.CV cs.LG  published:2015-11-09 summary:We study generative nets which can control and modify observations, after being trained on real-life datasets. In order to zoom-in on an object, some spatial, color and other attributes are learned by classifiers in specialized attention nets. In field-theoretical terms, these learned symmetry statistics form the gauge group of the data set. Plugging them in the generative layers of auto-classifiers-encoders (ACE) appears to be the most direct way to simultaneously: i) generate new observations with arbitrary attributes, from a given class, ii) describe the low-dimensional manifold encoding the "essence" of the data, after superfluous attributes are factored out, and iii) organically control, i.e., move or modify objects within given observations. We demonstrate the sharp improvement of the generative qualities of shallow ACE, with added spatial and color symmetry statistics, on the distorted MNIST and CIFAR10 datasets. version:3
arxiv-1604-02488 | Application of Multifractal Analysis to Segmentation of Water Bodies in Optical and Synthetic Aperture Radar Satellite Images | http://arxiv.org/abs/1604.02488 | id:1604.02488 author:Victor Manuel San Martin, Alejandra Figliola category:cs.CV physics.data-an  published:2016-04-08 summary:A method for segmenting water bodies in optical and synthetic aperture radar (SAR) satellite images is proposed. It makes use of the textural features of the different regions in the image for segmentation. The method consists in a multiscale analysis of the images, which allows us to study the images regularity both, locally and globally. As results of the analysis, coarse multifractal spectra of studied images and a group of images that associates each position (pixel) with its corresponding value of local regularity (or singularity) spectrum are obtained. Thresholds are then applied to the multifractal spectra of the images for the classification. These thresholds are selected after studying the characteristics of the spectra under the assumption that water bodies have larger local regularity than other soil types. Classifications obtained by the multifractal method are compared quantitatively with those obtained by neural networks trained to classify the pixels of the images in covered against uncovered by water. In optical images, the classifications are also compared with those derived using the so-called Normalized Differential Water Index (NDWI). version:1
arxiv-1604-02485 | Machine Learning for Visual Navigation of Unmanned Ground Vehicles | http://arxiv.org/abs/1604.02485 | id:1604.02485 author:Artem A. Lenskiy, Jong-Soo Lee category:cs.CV I.4.7  I.4.8  I.5.1  published:2016-04-08 summary:The use of visual information for the navigation of unmanned ground vehicles in a cross-country environment recently received great attention. However, until now, the use of textural information has been somewhat less effective than color or laser range information. This manuscript reviews the recent achievements in cross-country scene segmentation and addresses their shortcomings. It then describes a problem related to classification of high dimensional texture features. Finally, it compares three machine learning algorithms aimed at resolving this problem. The experimental results for each machine learning algorithm with the discussion of comparisons are given at the end of the manuscript. version:1
arxiv-1604-03221 | Leveraging Network Dynamics for Improved Link Prediction | http://arxiv.org/abs/1604.03221 | id:1604.03221 author:Alireza Hajibagheri, Gita Sukthankar, Kiran Lakkaraju category:cs.SI cs.LG  published:2016-04-08 summary:The aim of link prediction is to forecast connections that are most likely to occur in the future, based on examples of previously observed links. A key insight is that it is useful to explicitly model network dynamics, how frequently links are created or destroyed when doing link prediction. In this paper, we introduce a new supervised link prediction framework, RPM (Rate Prediction Model). In addition to network similarity measures, RPM uses the predicted rate of link modifications, modeled using time series data; it is implemented in Spark-ML and trained with the original link distribution, rather than a small balanced subset. We compare the use of this network dynamics model to directly creating time series of network similarity measures. Our experiments show that RPM, which leverages predicted rates, outperforms the use of network similarity measures, either individually or within a time series. version:1
arxiv-1604-02477 | One-class classifiers based on entropic spanning graphs | http://arxiv.org/abs/1604.02477 | id:1604.02477 author:Lorenzo Livi, Cesare Alippi category:cs.LG cs.CV cs.IT math.IT  published:2016-04-08 summary:One-class classifiers offer valuable tools to assess the presence of outliers in data. In this paper, we propose a design methodology for one-class classifiers based on entropic spanning graphs. The spanning graph is learned on the embedded input data, with the aim to generate a partition of the vertices. The final partition is derived by exploiting a criterion based on mutual information minimization. Here, we compute the mutual information by using a convenient formulation provided in terms of the $\alpha$-Jensen difference. Once training is completed, in order to associate a confidence level with the classifier decision, a graph-based fuzzy model is constructed. The fuzzification process is based only on topological information of the vertices of the entropic spanning graph. As such, the proposed one-class classifier is suitable also for datasets with complex geometric structures. We provide experiments on well-known benchmarking datasets containing both feature vectors and labeled graphs. In addition, we apply the method on the problem of protein solubility recognition by considering several data representations for the samples. Experimental results demonstrate the effectiveness and versatility of the proposed method with respect to other state-of-the-art approaches. version:1
arxiv-1604-02469 | Image segmentation of cross-country scenes captured in IR spectrum | http://arxiv.org/abs/1604.02469 | id:1604.02469 author:Artem Lenskiy category:cs.CV 68T10 I.4.7; I.4.8; I.5.1  published:2016-04-08 summary:Computer vision has become a major source of information for autonomous navigation of robots of various types, self-driving cars, military robots and mars/lunar rovers are some examples. Nevertheless, the majority of methods focus on analysing images captured in visible spectrum. In this manuscript we elaborate on the problem of segmenting cross-country scenes captured in IR spectrum. For this purpose we proposed employing salient features. Salient features are robust to variations in scale, brightness and view angle. We suggest the Speeded-Up Robust Features as a basis for our salient features for a number of reasons discussed in the paper. We also provide a comparison of two SURF implementations. The SURF features are extracted from images of different terrain types. For every feature we estimate a terrain class membership function. The membership values are obtained by means of either the multi-layer perceptron or nearest neighbours. The features' class membership values and their spatial positions are then applied to estimate class membership values for all pixels in the image. To decrease the effect of segmentation blinking that is caused by rapid switching between different terrain types and to speed up segmentation, we are tracking camera position and predict features' positions. The comparison of the multi-layer perception and the nearest neighbour classifiers is presented in the paper. The error rate of the terrain segmentation using the nearest neighbours obtained on the testing set is 16.6+-9.17%. version:1
arxiv-1602-03903 | Wavelet-Based Semantic Features for Hyperspectral Signature Discrimination | http://arxiv.org/abs/1602.03903 | id:1602.03903 author:Siwei Feng, Yuki Itoh, Mario Parente, Marco F. Duarte category:cs.CV cs.LG  published:2016-02-11 summary:Hyperspectral signature classification is a quantitative analysis approach for hyperspectral imagery which performs detection and classification of the constituent materials at the pixel level in the scene. The classification procedure can be operated directly on hyperspectral data or performed by using some features extracted from the corresponding hyperspectral signatures containing information like the signature's energy or shape. In this paper, we describe a technique that applies non-homogeneous hidden Markov chain (NHMC) models to hyperspectral signature classification. The basic idea is to use statistical models (such as NHMC) to characterize wavelet coefficients which capture the spectrum semantics (i.e., structural information) at multiple levels. Experimental results show that the approach based on NHMC models can outperform existing approaches relevant in classification tasks. version:2
arxiv-1604-02426 | CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples | http://arxiv.org/abs/1604.02426 | id:1604.02426 author:Filip Radenović, Giorgos Tolias, Ondřej Chum category:cs.CV  published:2016-04-08 summary:Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes. version:1
arxiv-1602-00216 | Feature Selection for Regression Problems Based on the Morisita Estimator of Intrinsic Dimension | http://arxiv.org/abs/1602.00216 | id:1602.00216 author:Jean Golay, Michael Leuenberger, Mikhail Kanevski category:stat.ML cs.LG  published:2016-01-31 summary:Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed. version:5
arxiv-1604-02355 | The (1+1) Elitist Black-Box Complexity of LeadingOnes | http://arxiv.org/abs/1604.02355 | id:1604.02355 author:Carola Doerr, Johannes Lengler category:cs.NE cs.CC F.2.2  published:2016-04-08 summary:One important goal of black-box complexity theory is the development of complexity models allowing to derive meaningful lower bounds for whole classes of randomized search heuristics. Complementing classical runtime analysis, black-box models help us understand how algorithmic choices such as the population size, the variation operators, or the selection rules influence the optimization time. One example for such a result is the $\Omega(n \log n)$ lower bound for unary unbiased algorithms on functions with a unique global optimum [Lehre/Witt, GECCO 2010], which tells us that higher arity operators or biased sampling strategies are needed when trying to beat this bound. In lack of analyzing techniques, almost no non-trivial bounds are known for other restricted models. Proving such bounds therefore remains to be one of the main challenges in black-box complexity theory. With this paper we contribute to our technical toolbox for lower bound computations by proposing a new type of information-theoretic argument. We regard the permutation- and bit-invariant version of \textsc{LeadingOnes} and prove that its (1+1) elitist black-box complexity is $\Omega(n^2)$, a bound that is matched by (1+1)-type evolutionary algorithms. The (1+1) elitist complexity of \textsc{LeadingOnes} is thus considerably larger than its unrestricted one, which is known to be of order $n\log\log n$ [Afshani et al., 2013]. version:1
arxiv-1604-02354 | Bayesian Neighbourhood Component Analysis | http://arxiv.org/abs/1604.02354 | id:1604.02354 author:Dong Wang, Xiaoyang Tan category:cs.CV cs.LG  published:2016-04-08 summary:Learning a good distance metric in feature space potentially improves the performance of the KNN classifier and is useful in many real-world applications. Many metric learning algorithms are however based on the point estimation of a quadratic optimization problem, which is time-consuming, susceptible to overfitting, and lack a natural mechanism to reason with parameter uncertainty, an important property useful especially when the training set is small and/or noisy. To deal with these issues, we present a novel Bayesian metric learning method, called Bayesian NCA, based on the well-known Neighbourhood Component Analysis method, in which the metric posterior is characterized by the local label consistency constraints of observations, encoded with a similarity graph instead of independent pairwise constraints. For efficient Bayesian optimization, we explore the variational lower bound over the log-likelihood of the original NCA objective. Experiments on several publicly available datasets demonstrate that the proposed method is able to learn robust metric measures from small size dataset and/or from challenging training set with labels contaminated by errors. The proposed method is also shown to outperform a previous pairwise constrained Bayesian metric learning method. version:1
arxiv-1602-02499 | The "Sprekend Nederland" project and its application to accent location | http://arxiv.org/abs/1602.02499 | id:1602.02499 author:David A. van Leeuwen, Rosemary Orr category:stat.ML cs.CL  published:2016-02-08 summary:This paper describes the data collection effort that is part of the project Sprekend Nederland (The Netherlands Talking), and discusses its potential use in Automatic Accent Location. We define Automatic Accent Location as the task to describe the accent of a speaker in terms of the location of the speaker and its history. We discuss possible ways of describing accent location, the consequence these have for the task of automatic accent location, and potential evaluation metrics. version:2
arxiv-1603-03236 | Pymanopt: A Python Toolbox for Manifold Optimization using Automatic Differentiation | http://arxiv.org/abs/1603.03236 | id:1603.03236 author:James Townsend, Niklas Koep, Sebastian Weichwald category:cs.MS cs.LG math.OC stat.ML  published:2016-03-10 summary:Manifold optimization is a method for (non-convex) optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with manifold optimization techniques. We introduce Pymanopt (available at https://pymanopt.github.io), a manifold optimization toolbox implemented in Python that - similarly to the Manopt Matlab toolbox - implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors. version:2
arxiv-1604-02316 | Free-Space Detection with Self-Supervised and Online Trained Fully Convolutional Networks | http://arxiv.org/abs/1604.02316 | id:1604.02316 author:Willem P. Sanberg, Gijs Dubbelman, Peter H. N. de With category:cs.CV  published:2016-04-08 summary:Recently, vision-based Advanced Driver Assist Systems have gained broad interest. In this work, we investigate free-space detection, for which we propose to employ a Fully Convolutional Network (FCN). We show that this FCN can be trained in a self-supervised manner and achieve similar results compared to training on manually annotated data, thereby reducing the need for large manually annotated training sets. To this end, our self-supervised training relies on a stereo-vision disparity system, to automatically generate (weak) training labels for the color-based FCN. Additionally, our self-supervised training facilitates online training of the FCN instead of offline. Consequently, given that the applied FCN is relatively small, the free-space analysis becomes highly adaptive to any traffic scene that the vehicle encounters. We have validated our algorithm using publicly available data and on a new challenging benchmark dataset that is released with this paper. Experiments show that the online training boosts performance with 5% when compared to offline training, both for Fmax and AP. version:1
arxiv-1511-04033 | Block-diagonal covariance selection for high-dimensional Gaussian graphical models | http://arxiv.org/abs/1511.04033 | id:1511.04033 author:Emilie Devijver, Mélina Gallopin category:math.ST cs.LG stat.ML stat.TH  published:2015-11-12 summary:Gaussian graphical models are widely utilized to infer and visualize networks of dependencies between continuous variables. However, inferring the graph is difficult when the sample size is small compared to the number of variables. To reduce the number of parameters to estimate in the model, we propose a non-asymptotic model selection procedure supported by strong theoretical guarantees based on an oracle inequality and a minimax lower bound. The covariance matrix of the model is approximated by a block-diagonal matrix. The structure of this matrix is detected by thresholding the sample covariance matrix, where the threshold is selected using the slope heuristic. Based on the block-diagonal structure of the covariance matrix, the estimation problem is divided into several independent problems: subsequently, the network of dependencies between variables is inferred using the graphical lasso algorithm in each block. The performance of the procedure is illustrated on simulated data. An application to a real gene expression dataset with a limited sample size is also presented: the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes, leading to a more parsimonious and interpretable modular network. version:2
arxiv-1604-02292 | A method for locally approximating regularized iterative tomographic reconstruction methods | http://arxiv.org/abs/1604.02292 | id:1604.02292 author:D. M. Pelt, K. J. Batenburg category:math.NA cs.CV  published:2016-04-08 summary:In many applications of tomography, the acquired projections are either limited in number or contain a significant amount of noise. In these cases, standard reconstruction methods tend to produce artifacts that can make further analysis difficult. Advanced regularized iterative methods, such as total variation minimization, are often able to achieve a higher reconstruction quality by exploiting prior knowledge about the scanned object. In practice, however, these methods often have prohibitively long computation times or large memory requirements. Furthermore, since they are based on minimizing a global objective function, regularized iterative methods need to reconstruct the entire scanned object, even when one is only interested in a (small) region of the reconstructed image. In this paper, we present a method to approximate regularized iterative reconstruction methods inside a (small) region of the scanned object. The method only performs computations inside the region of interest, ensuring low computational requirements. Reconstruction results for different phantom images and types of regularization are given, showing that reconstructions of the proposed local method are almost identical to those of the global regularized iterative methods that are approximated, even for relatively small regions of interest. Furthermore, we show that larger regions can be reconstructed efficiently by reconstructing several small regions in parallel and combining them into a single reconstruction afterwards. version:1
arxiv-1506-02247 | Well-posedness of a nonlinear integro-differential problem and its rearranged formulation | http://arxiv.org/abs/1506.02247 | id:1506.02247 author:Gonzalo Galiano, Emanuele Schiavi, Julián Velasco category:cs.CV  published:2015-06-07 summary:We study the existence and uniqueness of solutions of a nonlinear integro-differential problem which we reformulate introducing the notion of the decreasing rearrangement of the solution. A dimensional reduction of the problem is obtained and a detailed analysis of the properties of the solutions of the model is provided. Finally, a fast numerical method is devised and implemented to show the performance of the model when typical image processing tasks such as filtering and segmentation are performed. version:2
arxiv-1604-01602 | Manifold unwrapping using density ridges | http://arxiv.org/abs/1604.01602 | id:1604.01602 author:Jonas Nordhaug Myhre, Matineh Shaker, Devrim Kaba, Robert Jenssen, Deniz Erdogmus category:stat.ML  published:2016-04-06 summary:Research on manifold learning within a density ridge estimation framework has shown great potential in recent work for both estimation and de-noising of manifolds, building on the intuitive and well-defined notion of principal curves and surfaces. However, the problem of unwrapping or unfolding manifolds has received relatively little attention within the density ridge approach, despite being an integral part of manifold learning in general. This paper proposes two novel algorithms for unwrapping manifolds based on estimated principal curves and surfaces for one- and multi-dimensional manifolds respectively. The methods of unwrapping are founded in the realization that both principal curves and principal surfaces will have inherent local maxima of the probability density function. Following this observation, coordinate systems that follow the shape of the manifold can be computed by following the integral curves of the gradient flow of a kernel density estimate on the manifold. Furthermore, since integral curves of the gradient flow of a kernel density estimate is inherently local, we propose to stitch together local coordinate systems using parallel transport along the manifold. We provide numerical experiments on both real and synthetic data that illustrates clear and intuitive unwrapping results comparable to state-of-the-art manifold learning algorithms. version:2
arxiv-1604-02275 | Online Open World Recognition | http://arxiv.org/abs/1604.02275 | id:1604.02275 author:Rocco De Rosa, Thomas Mensink, Barbara Caputo category:cs.CV cs.LG stat.ML  published:2016-04-08 summary:As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. For all these scenarios our proposed methods outperform their non-online counterparts. We conclude that local and online learning is important to capture the full dynamics of open world recognition. version:1
arxiv-1604-02271 | Deep Structured Scene Parsing by Learning with Image Descriptions | http://arxiv.org/abs/1604.02271 | id:1604.02271 author:Liang Lin, Guangrun Wang, Rui Zhang, Ruimao Zhang, Xiaodan Liang, Wangmeng Zuo category:cs.CV 68U10 I.4.8; I.5  published:2016-04-08 summary:This paper addresses a fundamental problem of scene understanding: How to parse the scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations) that finely accords with human perception. We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixelwise object labeling and ii) a recursive neural network (RNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative user annotations (e.g., manually labeling semantic maps and relations), we train our deep model in a weakly-supervised manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and facilitate these trees discovering the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments suggest that our model is capable of producing meaningful and structured scene configurations and achieving more favorable scene labeling performance on PASCAL VOC 2012 over other state-of-the-art weakly-supervised methods. version:1
arxiv-1604-02270 | Single-Molecule Protein Identification by Sub-Nanopore Sensors | http://arxiv.org/abs/1604.02270 | id:1604.02270 author:Mikhail Kolmogorov, Eamonn Kennedy, Zhuxin Dong, Gregory Timp, Pavel Pevzner category:q-bio.QM cs.LG  published:2016-04-08 summary:Recent advances in top-down mass spectrometry enabled identification of intact proteins, but this technology still faces challenges. For example, top-down mass spectrometry suffers from a lack of sensitivity since the ion counts for a single fragmentation event are often low. In contrast, nanopore technology is exquisitely sensitive to single intact molecules, but it has only been successfully applied to DNA sequencing, so far. Here, we explore the potential of sub-nanopores for single-molecule protein identification (SMPI) and describe an algorithm for analyzing the electrical current blockade signal (nanospectrum) resulting from the translocation of a denaturated, linearly charged protein through a sub-nanopore. We further describe the first SMPI algorithm, compute the p-values of Protein-Nanospectrum Matches, and discuss the promise and computational limitations of the current SMPI technology. version:1
arxiv-1604-02264 | Probabilistic classifiers with low rank indefinite kernels | http://arxiv.org/abs/1604.02264 | id:1604.02264 author:Frank-Michael Schleif, Andrej Gisbrecht, Peter Tino category:cs.LG  published:2016-04-08 summary:Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores, but are also common in other fields like shape measures in image retrieval. Lacking an underlying vector space, the data are given as pairwise similarities only. The few algorithms available for such data do not scale to larger datasets. Focusing on probabilistic batch classifiers, the Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic Classification Vector Machine (PCVM) are both effective algorithms for this type of data but, with cubic complexity. Here we propose an extension of iKFD and PCVM such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Employing the Nystr\"om approximation for indefinite kernels, we also propose a new almost parameter free approach to identify the landmarks, restricted to a supervised learning problem. Evaluations at several larger similarity data from various domains show that the proposed methods provides similar generalization capabilities while being easier to parametrize and substantially faster for large scale data. version:1
arxiv-1511-02853 | Weakly Supervised Deep Detection Networks | http://arxiv.org/abs/1511.02853 | id:1511.02853 author:Hakan Bilen, Andrea Vedaldi category:cs.CV  published:2015-11-09 summary:Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well. version:3
arxiv-1604-01931 | Geometric Scene Parsing with Hierarchical LSTM | http://arxiv.org/abs/1604.01931 | id:1604.01931 author:Zhanglin Peng, Ruimao Zhang, Xiaodan Liang, Xiaobai Liu, Liang Lin category:cs.CV  published:2016-04-07 summary:This paper addresses the problem of geometric scene parsing, i.e. simultaneously labeling geometric surfaces (e.g. sky, ground and vertical plane) and determining the interaction relations (e.g. layering, supporting, siding and affinity) between main regions. This problem is more challenging than the traditional semantic scene labeling, as recovering geometric structures necessarily requires the rich and diverse contextual information. To achieve these goals, we propose a novel recurrent neural network model, named Hierarchical Long Short-Term Memory (H-LSTM). It contains two coupled sub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM (MS-LSTM) for handling the surface labeling and relation prediction, respectively. The two sub-networks provide complementary information to each other to exploit hierarchical scene contexts, and they are jointly optimized for boosting the performance. Our extensive experiments show that our model is capable of parsing scene geometric structures and outperforming several state-of-the-art methods by large margins. In addition, we show promising 3D reconstruction results from the still images based on the geometric parsing. version:2
arxiv-1604-02038 | Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves | http://arxiv.org/abs/1604.02038 | id:1604.02038 author:Fei Tian, Bin Gao, Di He, Tie-Yan Liu category:cs.LG cs.CL cs.IR  published:2016-04-07 summary:We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks (RNN) based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic (i.e., topics to sentences), which is a key technology for real world applications such as personalized short text conversation. version:2
arxiv-1510-08568 | Feature-Based Diversity Optimization for Problem Instance Classification | http://arxiv.org/abs/1510.08568 | id:1510.08568 author:Wanru Gao, Samadhi Nallaperuma, Frank Neumann category:cs.NE cs.AI  published:2015-10-29 summary:Understanding the behaviour of heuristic search methods is a challenge. This even holds for simple local search methods such as 2-OPT for the Traveling Salesperson problem. In this paper, we present a general framework that is able to construct a diverse set of instances that are hard or easy for a given search heuristic. Such a diverse set is obtained by using an evolutionary algorithm for constructing hard or easy instances that are diverse with respect to different features of the underlying problem. Examining the constructed instance sets, we show that many combinations of two or three features give a good classification of the TSP instances in terms of whether they are hard to be solved by 2-OPT. version:2
arxiv-1511-05616 | Learning Structured Inference Neural Networks with Label Relations | http://arxiv.org/abs/1511.05616 | id:1511.05616 author:Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, Greg Mori category:cs.CV cs.LG  published:2015-11-17 summary:Images of scenes have various objects as well as abundant attributes, and diverse levels of visual categorization are possible. A natural image could be assigned with fine-grained labels that describe major components, coarse-grained labels that depict high level abstraction or a set of labels that reveal attributes. Such categorization at different concept layers can be modeled with label graphs encoding label information. In this paper, we exploit this rich information with a state-of-art deep learning framework, and propose a generic structured model that leverages diverse label relations to improve image classification performance. Our approach employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. We evaluate our method on benchmark image datasets, and empirical results illustrate the efficacy of our model. version:3
arxiv-1603-03235 | UTSig: A Persian Offline Signature Dataset | http://arxiv.org/abs/1603.03235 | id:1603.03235 author:Amir Soleimani, Kazim Fouladi, Babak N. Araabi category:cs.CV  published:2016-03-10 summary:The crucial role of datasets in signature verification systems has motivated researchers to collect signature samples. However, with regard to the distinct characteristics of Persian signature, existing offline signature datasets cannot be used in Persian systems. This paper presents a new and public Persian offline signature dataset, UTSig, which consists of 8280 images from 115 classes that each class has 27 genuine, 3 opposite-hand signatures of the genuine signer, and 42 skilled forgeries made by 6 forgers from 230 people. Compared to the other public datasets, UTSig has larger number of samples, classes, and forgers. Meanwhile its samples were collected by considering variables such as signing period, writing instrument, signature box size, and number of observable samples for forgers. Reviewing the main characteristics of offline signature datasets, we statistically show that Persian signatures has fewer number of branch points and end points. We propose and test four different training and testing setups for UTSig. Results of our experiments show that training genuine samples along with opposite-hand signed samples and random forgeries can improve the performance in terms of equal error rate and minimum cost of log likelihood ratio which is an information theoretic criterion. version:3
arxiv-1511-03816 | Characterizing Concept Drift | http://arxiv.org/abs/1511.03816 | id:1511.03816 author:Geoffrey I. Webb, Roy Hyde, Hong Cao, Hai Long Nguyen, Francois Petitjean category:cs.LG cs.AI  published:2015-11-12 summary:Most machine learning models are static, but the world is dynamic, and increasing online deployment of learned models gives increasing urgency to the development of efficient and effective mechanisms to address learning in the context of non-stationary distributions, or as it is commonly called concept drift. However, the key issue of characterizing the different types of drift that can occur has not previously been subjected to rigorous definition and analysis. In particular, while some qualitative drift categorizations have been proposed, few have been formally defined, and the quantitative descriptions required for precise and objective understanding of learner performance have not existed. We present the first comprehensive framework for quantitative analysis of drift. This supports the development of the first comprehensive set of formal definitions of types of concept drift. The formal definitions clarify ambiguities and identify gaps in previous definitions, giving rise to a new comprehensive taxonomy of concept drift types and a solid foundation for research into mechanisms to detect and address concept drift. version:6
arxiv-1409-5616 | A Survey on Soft Subspace Clustering | http://arxiv.org/abs/1409.5616 | id:1409.5616 author:Zhaohong Deng, Kup-Sze Choi, Yizhang Jiang, Jun Wang, Shitong Wang category:cs.LG  published:2014-09-19 summary:Subspace clustering (SC) is a promising clustering technology to identify clusters based on their associations with subspaces in high dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been extensively studied and well accepted by the scientific community, SSC algorithms are relatively new but gaining more attention in recent years due to better adaptability. In the paper, a comprehensive survey on existing SSC algorithms and the recent development are presented. The SSC algorithms are classified systematically into three main categories, namely, conventional SSC (CSSC), independent SSC (ISSC) and extended SSC (XSSC). The characteristics of these algorithms are highlighted and the potential future development of SSC is also discussed. version:2
arxiv-1509-01851 | Deep Online Convex Optimization by Putting Forecaster to Sleep | http://arxiv.org/abs/1509.01851 | id:1509.01851 author:David Balduzzi category:cs.LG cs.GT cs.NE  published:2015-09-06 summary:Methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since neural networks are not convex and standard guarantees do not apply. This paper develops the first rigorous link between online convex optimization and error backpropagation on convolutional networks. The first step is to introduce circadian games, a mild generalization of convex games with similar convergence properties. The main result is that error backpropagation on a convolutional network is equivalent to playing out a circadian game. It follows immediately that the waking-regret of players in the game (the units in the neural network) controls the overall rate of convergence of the network. Finally, we explore some implications of the results: (i) we describe the representations learned by a neural network game-theoretically, (ii) propose a learning setting at the level of individual units that can be plugged into deep architectures, and (iii) propose a new approach to adaptive model selection by applying bandit algorithms to choose which players to wake on each round. version:2
arxiv-1603-06265 | Collaborative prediction with expert advice | http://arxiv.org/abs/1603.06265 | id:1603.06265 author:Paul Christiano category:cs.LG  published:2016-03-20 summary:Many practical learning systems aggregate data across many users, while learning theory traditionally considers a single learner who trusts all of their observations. A case in point is the foundational learning problem of prediction with expert advice. To date, there has been no theoretical study of the general collaborative version of prediction with expert advice, in which many users face a similar problem and would like to share their experiences in order to learn faster. A key issue in this collaborative framework is robustness: generally algorithms that aggregate data are vulnerable to manipulation by even a small number of dishonest users. We exhibit the first robust collaborative algorithm for prediction with expert advice. When all users are honest and have similar tastes our algorithm matches the performance of pooling data and using a traditional algorithm. But our algorithm also guarantees that adding users never significantly degrades performance, even if the additional users behave adversarially. We achieve strong guarantees even when the overwhelming majority of users behave adversarially. As a special case, our algorithm is extremely robust to variation amongst the users. version:3
arxiv-1604-02201 | Transfer Learning for Low-Resource Neural Machine Translation | http://arxiv.org/abs/1604.02201 | id:1604.02201 author:Barret Zoph, Deniz Yuret, Jonathan May, Kevin Knight category:cs.CL  published:2016-04-08 summary:The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation. version:1
arxiv-1604-00466 | Automatic Annotation of Structured Facts in Images | http://arxiv.org/abs/1604.00466 | id:1604.00466 author:Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal category:cs.CL cs.CV  published:2016-04-02 summary:Motivated by the application of fact-level image understanding, we present an automatic method for data collection of structured visual facts from images with captions. Example structured facts include attributed objects (e.g., <flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man, walking, dog>), and positional information (e.g., <vase, on, table>). The collected annotations are in the form of fact-image pairs (e.g.,<man, walking, dog> and an image region containing this fact). With a language approach, the proposed method is able to collect hundreds of thousands of visual fact annotations with accuracy of 83% according to human judgment. Our method automatically collected more than 380,000 visual fact annotations and more than 110,000 unique visual facts from images with captions and localized them in images in less than one day of processing time on standard CPU platforms. version:3
arxiv-1604-02182 | Family in the Wild (FIW): A Large-scale Kinship Recognition Database | http://arxiv.org/abs/1604.02182 | id:1604.02182 author:Joseph P Robinson, Ming Shao, Yue Wu, Yun Fu category:cs.CV  published:2016-04-07 summary:We introduce a large-scale dataset for visual kin-based problems, the Family in the Wild (FIW) dataset. Motivated by the lack of a single, unified image dataset available for kinship tasks, our goal is to provide a dataset that captivates the interest of the research community, i.e., large enough to support multiple tasks for evaluation. For this, we collected and labelled the largest set of family images to date, with only a small team and an efficient labelling tool that was designed to optimize the process of marking complex hierarchical relationships, attributes, and local label information in family photos. We experimentally compare our dataset the existing kinship image datasets, and demonstrate the practical value of the newly collected FIW dataset. We also demonstrate that using a pre-trained convolutional neural network (CNN) as an off-the-shelf feature extractor as performing better than traditional feature types used for kinship based tasks in the visual domain. We also measure human performance and show their performance does not match up to that of machine vision algorithms. version:1
arxiv-1512-06238 | The Limitations of Optimization from Samples | http://arxiv.org/abs/1512.06238 | id:1512.06238 author:Eric Balkanski, Aviad Rubinstein, Yaron Singer category:cs.DS cs.DM cs.LG  published:2015-12-19 summary:In this paper we consider the following question: can we optimize decisions on models learned from data and be guaranteed that we achieve desirable outcomes? We formalize this question through a novel framework called optimization from samples (OPS). In the OPS framework, we are given sampled values of a function drawn from some distribution and the objective is to optimize the function under some constraint. We show that there are classes of functions which have desirable learnability and optimizability guarantees and for which no reasonable approximation for optimization from samples is achievable. In particular, our main result shows that even for maximization of coverage functions under a cardinality constraint $k$, there exists a hypothesis class of functions that cannot be approximated within a factor of $n^{-1/4 + \epsilon}$ (for any constant $\epsilon > 0$) of the optimal solution, from samples drawn from the uniform distribution over all sets of size at most $k$. In the general case of monotone submodular functions, we show an $n^{-1/3 + \epsilon}$ lower bound and an almost matching $\tilde{\Omega}(n^{-1/3})$-optimization from samples algorithm. On the positive side, if a monotone subadditive function has bounded curvature we obtain desirable guarantees. We also show that additive and unit-demand functions can be optimized from samples to within arbitrarily good precision, and that budget additive functions can be optimized from samples to a factor of 1/2. version:2
arxiv-1404-0298 | A Kernel-Based Nonparametric Test for Anomaly Detection over Line Networks | http://arxiv.org/abs/1404.0298 | id:1404.0298 author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor category:cs.IT math.IT stat.ML  published:2014-04-01 summary:The nonparametric problem of detecting existence of an anomalous interval over a one dimensional line network is studied. Nodes corresponding to an anomalous interval (if exists) receive samples generated by a distribution q, which is different from the distribution p that generates samples for other nodes. If anomalous interval does not exist, then all nodes receive samples generated by p. It is assumed that the distributions p and q are arbitrary, and are unknown. In order to detect whether an anomalous interval exists, a test is built based on mean embeddings of distributions into a reproducing kernel Hilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It is shown that as the network size n goes to infinity, if the minimum length of candidate anomalous intervals is larger than a threshold which has the order O(log n), the proposed test is asymptotically successful, i.e., the probability of detection error approaches zero asymptotically. An efficient algorithm to perform the test with substantial computational complexity reduction is proposed, and is shown to be asymptotically successful if the condition on the minimum length of candidate anomalous interval is satisfied. Numerical results are provided, which are consistent with the theoretical results. version:2
arxiv-1604-02153 | A Semi-Lagrangian two-level preconditioned Newton-Krylov solver for constrained diffeomorphic image registration | http://arxiv.org/abs/1604.02153 | id:1604.02153 author:Andreas Mang, George Biros category:math.OC cs.CV  published:2016-04-07 summary:We propose an efficient numerical algorithm for the solution of diffeomorphic image registration problems. We use an optimization formulation constrained by a partial differential equation (PDE), where the constraints are a scalar transport equation. We use a pseudospectral discretization in space and second-order accurate semi-Lagrangian time stepping scheme for the transport PDE. We solve for a stationary velocity field using a preconditioned, globalized, matrix-free Newton-Krylov scheme. We propose and test a two-level Hessian preconditioner. We consider two strategies for inverting the preconditioner on the coarse grid: a nested preconditioned conjugate gradient method (exact solve) and a nested Chebyshev iterative method (inexact solve) with a fixed number of iterations. We test the performance of our solver in different synthetic and real-world two-dimensional application scenarios. We study grid convergence and computational efficiency of our new scheme. We compare the performance of our solver against our initial implementation that uses the same spatial discretization but a standard, explicit, second-order Runge-Kutta scheme for the numerical time integration of the transport equations and a single-level preconditioner. Our improved scheme delivers significant speedups over our original implementation. As a highlight, we observe a 20$\times$ speedup for a two dimensional, real world multi-subject medical image registration problem. version:1
arxiv-1604-02135 | A MultiPath Network for Object Detection | http://arxiv.org/abs/1604.02135 | id:1604.02135 author:Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O. Pinheiro, Sam Gross, Soumith Chintala, Piotr Dollár category:cs.CV  published:2016-04-07 summary:The recent MS COCO object detection dataset presents several new challenges for object detection. In particular, it contains objects at a broad range of scales, less prototypical images, and requires more precise localization. To address these challenges, we test three modifications to the standard Fast R-CNN object detector: (1) skip connections that give the detector access to features at multiple network layers, (2) a foveal structure to exploit object context at multiple object resolutions, and (3) an integral loss function and corresponding network adjustment that improve localization. The result of these modifications is that information can flow along multiple paths in our network, including through features from multiple network layers and from multiple object views. We refer to our modified classifier as a "MultiPath" network. We couple our MultiPath network with DeepMask object proposals, which are well suited for localization and small objects, and adapt our pipeline to predict segmentation masks in addition to bounding boxes. The combined system improves results over the baseline Fast R-CNN detector with Selective Search by 66% overall and by 4x on small objects. It placed second in both the COCO 2015 detection and segmentation challenges. version:1
arxiv-1604-02129 | Horizon Lines in the Wild | http://arxiv.org/abs/1604.02129 | id:1604.02129 author:Scott Workman, Menghua Zhai, Nathan Jacobs category:cs.CV  published:2016-04-07 summary:The horizon line is an important property for a wide variety of image understanding tasks. As such, many methods have been introduced to estimate the horizon line from a single image, primarily geometric methods which assume the presence of specific cues in the scene (e.g., vanishing points). These purely geometric methods are limited in their real-world capability, require extensive tuning, and are tested on benchmark datasets designed to showcase their ability. We introduce a large, realistic evaluation dataset, Horizon Lines in the Wild (HLW), containing natural images with labeled horizon lines. version:1
arxiv-1604-02123 | Multilevel Weighted Support Vector Machine for Classification on Healthcare Data with Missing Values | http://arxiv.org/abs/1604.02123 | id:1604.02123 author:Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nicholas Marko category:stat.ML cs.LG stat.AP  published:2016-04-07 summary:This work is motivated by the needs of predictive analytics on healthcare data as represented by Electronic Medical Records. Such data is invariably problematic: noisy, with missing entries, with imbalance in classes of interests, leading to serious bias in predictive modeling. Since standard data mining methods often produce poor performance measures, we argue for development of specialized techniques of data-preprocessing and classification. In this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. It is based on a multilevel framework of the cost-sensitive SVM and the expected maximization imputation method for missing values, which relies on iterated regression analyses. We compare classification results of multilevel SVM-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel SVM-based method produces fast, and more accurate and robust classification results. version:1
arxiv-1602-08486 | A Single Model Explains both Visual and Auditory Precortical Coding | http://arxiv.org/abs/1602.08486 | id:1602.08486 author:Honghao Shan, Matthew H. Tong, Garrison W. Cottrell category:q-bio.NC cs.CV cs.LG cs.NE  published:2016-02-26 summary:Precortical neural systems encode information collected by the senses, but the driving principles of the encoding used have remained a subject of debate. We present a model of retinal coding that is based on three constraints: information preservation, minimization of the neural wiring, and response equalization. The resulting novel version of sparse principal components analysis successfully captures a number of known characteristics of the retinal coding system, such as center-surround receptive fields, color opponency channels, and spatiotemporal responses that correspond to magnocellular and parvocellular pathways. Furthermore, when trained on auditory data, the same model learns receptive fields well fit by gammatone filters, commonly used to model precortical auditory coding. This suggests that efficient coding may be a unifying principle of precortical encoding across modalities. version:2
arxiv-1604-02115 | Trajectory Aligned Features For First Person Action Recognition | http://arxiv.org/abs/1604.02115 | id:1604.02115 author:Suriya Singh, Chetan Arora, C. V. Jawahar category:cs.CV  published:2016-04-07 summary:Egocentric videos are characterised by their ability to have the first person view. With the popularity of Google Glass and GoPro, use of egocentric videos is on the rise. Recognizing action of the wearer from egocentric videos is an important problem. Unstructured movement of the camera due to natural head motion of the wearer causes sharp changes in the visual field of the egocentric camera causing many standard third person action recognition techniques to perform poorly on such videos. Objects present in the scene and hand gestures of the wearer are the most important cues for first person action recognition but are difficult to segment and recognize in an egocentric video. We propose a novel representation of the first person actions derived from feature trajectories. The features are simple to compute using standard point tracking and does not assume segmentation of hand/objects or recognizing object or hand pose unlike in many previous approaches. We train a bag of words classifier with the proposed features and report a performance improvement of more than 11% on publicly available datasets. Although not designed for the particular case, we show that our technique can also recognize wearer's actions when hands or objects are not visible. version:1
arxiv-1604-02929 | Solving Optimization Problems by the Spatial Public Goods Game | http://arxiv.org/abs/1604.02929 | id:1604.02929 author:Marco Alberto Javarone category:physics.soc-ph cs.GT cs.NE  published:2016-04-07 summary:We introduce a method based on the spatial Public Goods Game for solving optimization tasks. In particular, we focus on the Traveling Salesman Problem, i.e., a problem whose search space exponentially grows increasing the number of cities, then becoming NP-hard. The proposed method considers a population whose agents are provided with a random solution to the given problem. Then, agents interact by playing the Public Goods Game using the fitness of their solution as currency of the game. In doing so, agents with better solutions provide higher contributions, while agents with lower ones tend to imitate the solution of richer agents to increase their fitness. Numerical simulations show that the proposed method allows to compute exact solutions, and suboptimal ones, in the considered search spaces. As result, beyond to propose a new heuristic for combinatorial optimization tasks, our work aims to highlight the potentiality of evolutionary game theory outside its current horizons. version:1
arxiv-1604-02085 | A robust autoassociative memory with coupled networks of Kuramoto-type oscillators | http://arxiv.org/abs/1604.02085 | id:1604.02085 author:Daniel Heger, Katharina Krischer category:nlin.AO cs.CV cs.NE  published:2016-04-07 summary:Uncertain recognition success, unfavorable scaling of connection complexity or dependence on complex external input impair the usefulness of current oscillatory neural networks for pattern recognition or restrict technical realizations to small networks. We propose a new network architecture of coupled oscillators for pattern recognition which shows none of the mentioned aws. Furthermore we illustrate the recognition process with simulation results and analyze the new dynamics analytically: Possible output patterns are isolated attractors of the system. Additionally, simple criteria for recognition success are derived from a lower bound on the basins of attraction. version:1
arxiv-1604-01685 | The Cityscapes Dataset for Semantic Urban Scene Understanding | http://arxiv.org/abs/1604.01685 | id:1604.01685 author:Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele category:cs.CV  published:2016-04-06 summary:Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark. version:2
arxiv-1604-02032 | 3-D Hand Pose Estimation from Kinect's Point Cloud Using Appearance Matching | http://arxiv.org/abs/1604.02032 | id:1604.02032 author:Pasquale Coscia, Francesco A. N. Palmieri, Francesco Castaldo, Alberto Cavallo category:cs.CV  published:2016-04-07 summary:We present a novel appearance-based approach for pose estimation of a human hand using the point clouds provided by the low-cost Microsoft Kinect sensor. Both the free-hand case, in which the hand is isolated from the surrounding environment, and the hand-object case, in which the different types of interactions are classified, have been considered. The hand-object case is clearly the most challenging task having to deal with multiple tracks. The approach proposed here belongs to the class of partial pose estimation where the estimated pose in a frame is used for the initialization of the next one. The pose estimation is obtained by applying a modified version of the Iterative Closest Point (ICP) algorithm to synthetic models to obtain the rigid transformation that aligns each model with respect to the input data. The proposed framework uses a "pure" point cloud as provided by the Kinect sensor without any other information such as RGB values or normal vector components. For this reason, the proposed method can also be applied to data obtained from other types of depth sensor, or RGB-D camera. version:1
arxiv-1603-01360 | Neural Architectures for Named Entity Recognition | http://arxiv.org/abs/1603.01360 | id:1603.01360 author:Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer category:cs.CL  published:2016-03-04 summary:State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers. version:3
arxiv-1511-07763 | LocNet: Improving Localization Accuracy for Object Detection | http://arxiv.org/abs/1511.07763 | id:1511.07763 author:Spyros Gidaris, Nikos Komodakis category:cs.CV cs.LG cs.NE  published:2015-11-24 summary:We propose a novel object localization methodology with the purpose of boosting the localization accuracy of state-of-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework. For implementing our localization model, we make use of a convolutional neural network architecture that is properly adapted for this task, called LocNet. We show experimentally that LocNet achieves a very significant improvement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and that it can be very easily coupled with recent state-of-the-art object detection systems, helping them to boost their performance. Finally, we demonstrate that our detection approach can achieve high detection accuracy even when it is given as input a set of sliding windows, thus proving that it is independent of box proposal methods. version:2
arxiv-1604-02030 | Edge Detection Based Shape Identification | http://arxiv.org/abs/1604.02030 | id:1604.02030 author:Vivek Kumar, Sumit Pandey, Amrindra Pal, Sandeep Sharma category:cs.CV  published:2016-04-07 summary:Image recognition is the need of the hour. In order to be able to recognize an image, it is of immense importance that the image should be distinguishable from the background. In the present work, an approach is presented for automatic detection and recognition of regular 2D shapes in low noise environments. The work has a large number of direct applications in the real world. The algorithm proposed is based on locating the edges and thus, in turn calculating the area of the object helps in identification of a specified shape. The results were simulated using MATLAB tool are encouraging and validate the proposed algorithm. Index Terms: Edge Detection, Area Calculation, Shape Detection, Object Recognition version:1
arxiv-1603-08458 | Longitudinal Analysis of Discussion Topics in an Online Breast Cancer Community using Convolutional Neural Networks | http://arxiv.org/abs/1603.08458 | id:1603.08458 author:Shaodian Zhang, Edouard Grave, Elizabeth Sklar, Noemie Elhadad category:cs.CL cs.CY cs.SI  published:2016-03-28 summary:Identifying topics of discussions in online health communities (OHC) is critical to various applications, but can be difficult because topics of OHC content are usually heterogeneous and domain-dependent. In this paper, we provide a multi-class schema, an annotated dataset, and supervised classifiers based on convolutional neural network (CNN) and other models for the task of classifying discussion topics. We apply the CNN classifier to the most popular breast cancer online community, and carry out a longitudinal analysis to show topic distributions and topic changes throughout members' participation. Our experimental results suggest that CNN outperforms other classifiers in the task of topic classification, and that certain trajectories can be detected with respect to topic changes. version:3
arxiv-1602-04605 | Distributed Information-Theoretic Biclustering | http://arxiv.org/abs/1602.04605 | id:1602.04605 author:Georg Pichler, Pablo Piantanida, Gerald Matz category:cs.IT cs.LG math.IT  published:2016-02-15 summary:We study a novel multi-terminal source coding setup motivated by the biclustering problem. Two separate encoders observe two stationary, memoryless sources $X^n$ and $Z^n$, respectively. The goal is to find rate-limited encodings $f(x^n)$ and $g(z^n)$ that maximize the mutual information $I(f(X^n);g(Z^n))/n$. We present non-trivial outer and inner bounds on the achievable region for this problem. These bounds are also generalized to an arbitrary collection of stationary, memoryless sources. The considered problem is intimately connected to distributed hypothesis testing against independence under communication constraints, and hence our results are expected to apply to that setting as well. version:2
arxiv-1511-06860 | Convex Sparse Spectral Clustering: Single-view to Multi-view | http://arxiv.org/abs/1511.06860 | id:1511.06860 author:Canyi Lu, Shuicheng Yan, Zhouchen Lin category:cs.CV  published:2015-11-21 summary:Spectral Clustering (SC) is one of the most widely used methods for data clustering. It first finds a low-dimensonal embedding $\U$ of data by computing the eigenvectors of the normalized Laplacian matrix, and then performs k-means on $\U^\top$ to get the final clustering result. In this work, we observe that, in the ideal case, $\U\U^\top$ should be block diagonal and thus sparse. Therefore we propose the Sparse Spectral Clustering (SSC) method which extends SC with sparse regularization on $\U\U^\top$. To address the computational issue of the nonconvex SSC model, we propose a novel convex relaxation of SSC based on the convex hull of the fixed rank projection matrices. Then the convex SSC model can be efficiently solved by the Alternating Direction Method of \canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse Spectral Clustering (PSSC) which extends SSC to boost the clustering performance by using the multi-view information of data. Experimental comparisons with several baselines on real-world datasets testify to the efficacy of our proposed methods. version:2
arxiv-1604-01980 | Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies | http://arxiv.org/abs/1604.01980 | id:1604.01980 author:Emanuel Laude, Thomas Möllenhoff, Michael Moeller, Jan Lellmann, Daniel Cremers category:cs.CV math.OC  published:2016-04-07 summary:Convex relaxations of nonconvex multilabel problems have been demonstrated to produce superior (provably optimal or near-optimal) solutions to a variety of classical computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. The key idea is that we approximate the dataterm of the vectorial labeling problem in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for the fractional solutions of the relaxed convex problem. In numerous experiments on large-displacement optical flow estimation and on color image denoising we demonstrate that the computed solutions have superior quality while requiring much lower memory and runtime. version:1
arxiv-1604-01972 | An Adaptive Resample-Move Algorithm for Estimating Normalizing Constants | http://arxiv.org/abs/1604.01972 | id:1604.01972 author:Marco Fraccaro, Ulrich Paquet, Ole Winther category:stat.ML  published:2016-04-07 summary:The estimation of normalizing constants is a fundamental step in probabilistic model comparison. Sequential Monte Carlo methods may be used for this task and have the advantage of being inherently parallelizable. However, the standard choice of using a fixed number of particles at each iteration is suboptimal because some steps will contribute disproportionately to the variance of the estimate. We introduce an adaptive version of the Resample-Move algorithm, in which the particle set is adaptively expanded whenever a better approximation of an intermediate distribution is needed. The algorithm builds on the expression for the optimal number of particles and the corresponding minimum variance found under ideal conditions. Benchmark results on challenging Gaussian Process Classification and Restricted Boltzmann Machine applications show that Adaptive Resample-Move (ARM) estimates the normalizing constant with a smaller variance, using less computational resources, than either Resample-Move with a fixed number of particles or Annealed Importance Sampling. A further advantage over Annealed Importance Sampling is that ARM is easier to tune. version:1
arxiv-1604-01655 | Correlated and Individual Multi-Modal Deep Learning for RGB-D Object Recognition | http://arxiv.org/abs/1604.01655 | id:1604.01655 author:Ziyan Wang, Ruogu Lin, Jiwen Lu, Jianjiang Feng, Jie zhou category:cs.CV  published:2016-04-06 summary:In this paper, we propose a new correlated and individual multi-modal deep learning (CIMDL) method for RGB-D object recognition. Unlike most conventional RGB-D object recognition methods which extract features from the RGB and depth channels individually, our CIMDL jointly learns feature representations from raw RGB-D data with a pair of deep neural networks, so that the sharable and modal-specific information can be simultaneously exploited. Specifically, we construct a pair of deep convolutional neural networks (CNNs) for the RGB and depth data, and concatenate them at the top layer of the network with a loss function which learns a new feature space where both correlated part and the individual part of the RGB-D information are well modelled. The parameters of the whole networks are updated by using the back-propagation criterion. Experimental results on two widely used RGB-D object image benchmark datasets clearly show that our method outperforms state-of-the-arts. version:2
arxiv-1510-02824 | On the Complexity of Inner Product Similarity Join | http://arxiv.org/abs/1510.02824 | id:1510.02824 author:Thomas D. Ahle, Rasmus Pagh, Ilya Razenshteyn, Francesco Silvestri category:cs.DS cs.DB cs.LG  published:2015-10-09 summary:A number of tasks in classification, information retrieval, recommendation systems, and record linkage reduce to the core problem of inner product similarity join (IPS join): identifying pairs of vectors in a collection that have a sufficiently large inner product. IPS join is well understood when vectors are normalized and some approximation of inner products is allowed. However, the general case where vectors may have any length appears much more challenging. Recently, new upper bounds based on asymmetric locality-sensitive hashing (ALSH) and asymmetric embeddings have emerged, but little has been known on the lower bound side. In this paper we initiate a systematic study of inner product similarity join, showing new lower and upper bounds. Our main results are: * Approximation hardness of IPS join in subquadratic time, assuming the strong exponential time hypothesis. * New upper and lower bounds for (A)LSH-based algorithms. In particular, we show that asymmetry can be avoided by relaxing the LSH definition to only consider the collision probability of distinct elements. * A new indexing method for IPS based on linear sketches, implying that our hardness results are not far from being tight. Our technical contributions include new asymmetric embeddings that may be of independent interest. At the conceptual level we strive to provide greater clarity, for example by distinguishing among signed and unsigned variants of IPS join and shedding new light on the effect of asymmetry. version:3
arxiv-1604-01955 | Monitoring Chinese Population Migration in Consecutive Weekly Basis from Intra-city scale to Inter-province scale by Didi's Bigdata | http://arxiv.org/abs/1604.01955 | id:1604.01955 author:Renyu Zhao category:stat.ML  published:2016-04-07 summary:Population migration is valuable information which leads to proper decision in urban-planning strategy, massive investment, and many other fields. For instance, inter-city migration is a posterior evidence to see if the government's constrain of population works, and inter-community immigration might be a prior evidence of real estate price hike. With timely data, it is also impossible to compare which city is more favorable for the people, suppose the cities release different new regulations, we could also compare the customers of different real estate development groups, where they come from, where they probably will go. Unfortunately these data was not available. In this paper, leveraging the data generated by positioning team in Didi, we propose a novel approach that timely monitoring population migration from community scale to provincial scale. Migration can be detected as soon as in a week. It could be faster, the setting of a week is for statistical purpose. A monitoring system is developed, then applied nation wide in China, some observations derived from the system will be presented in this paper. This new method of migration perception is origin from the insight that nowadays people mostly moving with their personal Access Point (AP), also known as WiFi hotspot. Assume that the ratio of AP moving to the migration of population is constant, analysis of comparative population migration would be feasible. More exact quantitative research would also be done with few sample research and model regression. The procedures of processing data includes many steps: eliminating the impact of pseudo-migration AP, for instance pocket WiFi, and second-hand traded router; distinguishing moving of population with moving of companies; identifying shifting of AP by the finger print clusters, etc.. version:1
arxiv-1604-01952 | Deep Online Convex Optimization with Gated Games | http://arxiv.org/abs/1604.01952 | id:1604.01952 author:David Balduzzi category:cs.LG cs.GT cs.NE stat.ML  published:2016-04-07 summary:Methods from convex optimization are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since modern convolutional networks (convnets), incorporating rectifier units and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. This paper provides the first convergence rates for gradient descent on rectifier convnets. The proof utilizes the particular structure of rectifier networks which consists in binary active/inactive gates applied on top of an underlying linear network. The approach generalizes to max-pooling, dropout and maxout. In other words, to precisely the neural networks that perform best empirically. The key step is to introduce gated games, an extension of convex games with similar convergence properties that capture the gating function of rectifiers. The main result is that rectifier convnets converge to a critical point at a rate controlled by the gated-regret of the units in the network. Corollaries of the main result include: (i) a game-theoretic description of the representations learned by a neural network; (ii) a logarithmic-regret algorithm for training neural nets; and (iii) a formal setting for analyzing conditional computation in neural nets that can be applied to recently developed models of attention. version:1
arxiv-1604-01946 | Optimizing Performance of Recurrent Neural Networks on GPUs | http://arxiv.org/abs/1604.01946 | id:1604.01946 author:Jeremy Appleyard, Tomas Kocisky, Phil Blunsom category:cs.LG cs.NE  published:2016-04-07 summary:As recurrent neural networks become larger and deeper, training times for single networks are rising into weeks or even months. As such there is a significant incentive to improve the performance and scalability of these networks. While GPUs have become the hardware of choice for training and deploying recurrent models, the implementations employed often make use of only basic optimizations for these architectures. In this article we demonstrate that by exposing parallelism between operations within the network, an order of magnitude speedup across a range of network sizes can be achieved over a naive implementation. We describe three stages of optimization that have been incorporated into the fifth release of NVIDIA's cuDNN: firstly optimizing a single cell, secondly a single layer, and thirdly the entire network. version:1
arxiv-1503-06379 | Relaxed Leverage Sampling for Low-rank Matrix Completion | http://arxiv.org/abs/1503.06379 | id:1503.06379 author:Abhisek Kundu category:cs.IT cs.LG math.IT stat.ML  published:2015-03-22 summary:We consider the problem of exact recovery of any $m\times n$ matrix of rank $\varrho$ from a small number of observed entries via the standard nuclear norm minimization framework. Such low-rank matrices have degrees of freedom $(m+n)\varrho - \varrho^2$. We show that any arbitrary low-rank matrices can be recovered exactly from a $\Theta\left(((m+n)\varrho - \varrho^2)\log^2(m+n)\right)$ randomly sampled entries, thus matching the lower bound on the required number of entries (in terms of degrees of freedom), with an additional factor of $O(\log^2(m+n))$. To achieve this bound on sample size we observe each entry with probabilities proportional to the sum of corresponding row and column leverage scores, minus their product. We show that this relaxation in sampling probabilities (as opposed to sum of leverage scores in Chen et al, 2014) can give us an $O(\varrho^2\log^2(m+n))$ additive improvement on the (best known) sample size obtained by Chen et al, 2014, for the nuclear norm minimization. Experiments on real data corroborate the theoretical improvement on sample size. Further, exact recovery of $(a)$ incoherent matrices (with restricted leverage scores), and $(b)$ matrices with only one of the row or column spaces to be incoherent, can be performed using our relaxed leverage score sampling, via nuclear norm minimization, without knowing the leverage scores a priori. In such settings also we can achieve improvement on sample size. version:3
arxiv-1604-01904 | Neural Headline Generation with Minimum Risk Training | http://arxiv.org/abs/1604.01904 | id:1604.01904 author:Ayana, Shiqi Shen, Zhiyuan Liu, Maosong Sun category:cs.CL  published:2016-04-07 summary:Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters with respect to evaluation metrics and statistically leads to significant improvements for headline generation. Experiment results show that our approach outperforms state-of-the-art systems on both English and Chinese headline generation tasks. version:1
arxiv-1604-01894 | A Novel Scene Text Detection Algorithm Based On Convolutional Neural Network | http://arxiv.org/abs/1604.01894 | id:1604.01894 author:Xiaohang Ren, Kai Chen, Jun Sun category:cs.CV  published:2016-04-07 summary:Candidate text region extraction plays a critical role in convolutional neural network (CNN) based text detection from natural images. In this paper, we propose a CNN based scene text detection algorithm with a new text region extractor. The so called candidate text region extractor I-MSER is based on Maximally Stable Extremal Region (MSER), which can improve the independency and completeness of the extracted candidate text regions. Design of I-MSER is motivated by the observation that text MSERs have high similarity and are close to each other. The independency of candidate text regions obtained by I-MSER is guaranteed by selecting the most representative regions from a MSER tree which is generated according to the spatial overlapping relationship among the MSERs. A multi-layer CNN model is trained to score the confidence value of the extracted regions extracted by the I-MSER for text detection. The new text detection algorithm based on I-MSER is evaluated with wide-used ICDAR 2011 and 2013 datasets and shows improved detection performance compared to the existing algorithms. version:1
arxiv-1511-03683 | Generative Concatenative Nets Jointly Learn to Write and Classify Reviews | http://arxiv.org/abs/1511.03683 | id:1511.03683 author:Zachary C. Lipton, Sharad Vikram, Julian McAuley category:cs.CL cs.LG  published:2015-11-11 summary:A recommender system's basic task is to estimate how users will respond to unseen items. This is typically modeled in terms of how a user might rate a product, but here we aim to extend such approaches to model how a user would write about the product. To do so, we design a character-level Recurrent Neural Network (RNN) that generates personalized product reviews. The network convincingly learns styles and opinions of nearly 1000 distinct authors, using a large corpus of reviews from BeerAdvocate.com. It also tailors reviews to describe specific items, categories, and star ratings. Using a simple input replication strategy, the Generative Concatenative Network (GCN) preserves the signal of static auxiliary inputs across wide sequence intervals. Without any additional training, the generative model can classify reviews, identifying the author of the review, the product category, and the sentiment (rating), with remarkable accuracy. Our evaluation shows the GCN captures complex dynamics in text, such as the effect of negation, misspellings, slang, and large vocabularies gracefully absent any machinery explicitly dedicated to the purpose. version:5
arxiv-1604-01891 | A CNN Based Scene Chinese Text Recognition Algorithm With Synthetic Data Engine | http://arxiv.org/abs/1604.01891 | id:1604.01891 author:Xiaohang Ren, Kai Chen, Jun Sun category:cs.CV  published:2016-04-07 summary:Scene text recognition plays an important role in many computer vision applications. The small size of available public available scene text datasets is the main challenge when training a text recognition CNN model. In this paper, we propose a CNN based Chinese text recognition algorithm. To enlarge the dataset for training the CNN model, we design a synthetic data engine for Chinese scene character generation, which generates representative character images according to the fonts use frequency of Chinese texts. As the Chinese text is more complex, the English text recognition CNN architecture is modified for Chinese text. To ensure the small size nature character dataset and the large size artificial character dataset are comparable in training, the CNN model are trained progressively. The proposed Chinese text recognition algorithm is evaluated with two Chinese text datasets. The algorithm achieves better recognize accuracy compared to the baseline methods. version:1
arxiv-1604-01889 | Reinterpreting the Transformation Posterior in Probabilistic Image Registration | http://arxiv.org/abs/1604.01889 | id:1604.01889 author:Jie Luo, Karteek Popuri, Dana Cobzas, Hongyi Ding, Masashi Sugiyama category:cs.CV  published:2016-04-07 summary:Probabilistic image registration methods estimate the posterior distribution of transformation. The conventional way of interpreting the transformation posterior is to use the mode as the most likely transformation and assign its corresponding intensity to the registered voxel. Meanwhile, summary statistics of the posterior are employed to evaluate the registration uncertainty, that is the trustworthiness of the registered image. Despite the wide acceptance, this convention has never been justified. In this paper, based on illustrative examples, we question the correctness and usefulness of conventional methods. In order to faithfully translate the transformation posterior, we propose to encode the variability of values into a novel data type called ensemble fields. Ensemble fields can serve as a complement to the registered image and a foundation for developing advanced methods to characterize the uncertainty in registration-based tasks. We demonstrate the potential of ensemble fields by pilot examples version:1
arxiv-1604-01662 | Towards Bayesian Deep Learning: A Survey | http://arxiv.org/abs/1604.01662 | id:1604.01662 author:Hao Wang, Dit-Yan Yeung category:stat.ML cs.AI cs.CV cs.LG cs.NE  published:2016-04-06 summary:While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks. version:2
arxiv-1503-08395 | Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition | http://arxiv.org/abs/1503.08395 | id:1503.08395 author:Shusen Wang, Zhihua Zhang, Tong Zhang category:cs.LG  published:2015-03-29 summary:Symmetric positive semi-definite (SPSD) matrix approximation methods have been extensively used to speed up large-scale eigenvalue computation and kernel learning methods. The sketching based method, which we call the prototype model, produces relatively accurate approximations. The prototype model is computationally efficient on skinny matrices where one of the matrix dimensions is relatively small, but it is inefficient on large square matrices. The Nystr\"om method is highly efficient on SPSD matrices, but can only achieve low matrix approximation accuracy. In this paper we propose novel model which we call the {\it faster SPSD matrix approximation model}. The faster model is nearly as efficient as the Nystr\"om method and as accurate as the prototype model. We show that the faster model can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size to achieve $1+\epsilon$ relative-error, whereas the prototype model and the Nystr\"om method cost at least quadratic time to attain comparable error bound. We also contribute new understandings of the Nystro\"om method. The Nystr\"om method is a special instance of our faster SPSD matrix approximation model, and it is approximation to the prototype model. Our technique can be straightforwardly applied to make the CUR matrix decomposition more efficiently computed without much affecting the accuracy. Empirical experiments demonstrate the effectiveness of the proposed methods. version:5
arxiv-1602-00354 | Active Learning Algorithms for Graphical Model Selection | http://arxiv.org/abs/1602.00354 | id:1602.00354 author:Gautam Dasarathy, Aarti Singh, Maria-Florina Balcan, Jong Hyuk Park category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH  published:2016-02-01 summary:The problem of learning the structure of a high dimensional graphical model from data has received considerable attention in recent years. In many applications such as sensor networks and proteomics it is often expensive to obtain samples from all the variables involved simultaneously. For instance, this might involve the synchronization of a large number of sensors or the tagging of a large number of proteins. To address this important issue, we initiate the study of a novel graphical model selection problem, where the goal is to optimize the total number of scalar samples obtained by allowing the collection of samples from only subsets of the variables. We propose a general paradigm for graphical model selection where feedback is used to guide the sampling to high degree vertices, while obtaining only few samples from the ones with the low degrees. We instantiate this framework with two specific active learning algorithms, one of which makes mild assumptions but is computationally expensive, while the other is more computationally efficient but requires stronger (nevertheless standard) assumptions. Whereas the sample complexity of passive algorithms is typically a function of the maximum degree of the graph, we show that the sample complexity of our algorithms is provable smaller and that it depends on a novel local complexity measure that is akin to the average degree of the graph. We finally demonstrate the efficacy of our framework via simulations. version:2
arxiv-1604-01879 | GIFT: A Real-time and Scalable 3D Shape Search Engine | http://arxiv.org/abs/1604.01879 | id:1604.01879 author:Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki category:cs.CV  published:2016-04-07 summary:Projective analysis is an important solution for 3D shape retrieval, since human visual perceptions of 3D shapes rely on various 2D observations from different view points. Although multiple informative and discriminative views are utilized, most projection-based retrieval systems suffer from heavy computational cost, thus cannot satisfy the basic requirement of scalability for search engines. In this paper, we present a real-time 3D shape search engine based on the projective images of 3D shapes. The real-time property of our search engine results from the following aspects: (1) efficient projection and view feature extraction using GPU acceleration; (2) the first inverted file, referred as F-IF, is utilized to speed up the procedure of multi-view matching; (3) the second inverted file (S-IF), which captures a local distribution of 3D shapes in the feature manifold, is adopted for efficient context-based re-ranking. As a result, for each query the retrieval task can be finished within one second despite the necessary cost of IO overhead. We name the proposed 3D shape search engine, which combines GPU acceleration and Inverted File Twice, as GIFT. Besides its high efficiency, GIFT also outperforms the state-of-the-art methods significantly in retrieval accuracy on various shape benchmarks and competitions. version:1
arxiv-1604-01871 | When is Nontrivial Estimation Possible for Graphons and Stochastic Block Models? | http://arxiv.org/abs/1604.01871 | id:1604.01871 author:Audra McMillan, Adam Smith category:math.ST cs.LG stat.TH  published:2016-04-07 summary:Block graphons (also called stochastic block models) are an important and widely-studied class of models for random networks. We provide a lower bound on the accuracy of estimators for block graphons with a large number of blocks. We show that, given only the number $k$ of blocks and an upper bound $\rho$ on the values (connection probabilities) of the graphon, every estimator incurs error at least on the order of $\min(\rho, \sqrt{\rho k^2/n^2})$ in the $\delta_2$ metric with constant probability, in the worst case over graphons. In particular, our bound rules out any nontrivial estimation (that is, with $\delta_2$ error substantially less than $\rho$) when $k\geq n\sqrt{\rho}$. Combined with previous upper and lower bounds, our results characterize, up to logarithmic terms, the minimax accuracy of graphon estimation in the $\delta_2$ metric. A similar lower bound to ours was obtained independently by Klopp, Tsybakov and Verzelen (2016). version:1
arxiv-1604-01854 | Building Ensembles of Adaptive Nested Dichotomies with Random-Pair Selection | http://arxiv.org/abs/1604.01854 | id:1604.01854 author:Tim Leathart, Bernhard Pfahringer, Eibe Frank category:stat.ML cs.LG  published:2016-04-07 summary:A system of nested dichotomies is a method of decomposing a multi-class problem into a collection of binary problems. Such a system recursively splits the set of classes into two subsets, and trains a binary classifier to distinguish between each subset. Even though ensembles of nested dichotomies with random structure have been shown to perform well in practice, using a more sophisticated class subset selection method can be used to improve classification accuracy. We investigate an approach to this problem called random-pair selection, and evaluate its effectiveness compared to other published methods of subset selection. We show that our method outperforms other methods in many cases, and is at least on par in almost all other cases. version:1
arxiv-1604-01850 | End-to-End Deep Learning for Person Search | http://arxiv.org/abs/1604.01850 | id:1604.01850 author:Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, Xiaogang Wang category:cs.CV  published:2016-04-07 summary:Existing person re-identification (re-id) benchmarks and algorithms mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be found from whole images. To close the gap, we investigate how to localize and match query persons from the scene images without relying on the annotations of candidate boxes. Instead of breaking it down into two separate tasks---pedestrian detection and person re-id, we propose an end-to-end deep learning framework to jointly handle both tasks. A random sampling softmax loss is proposed to effectively train the model under the supervision of sparse and unbalanced labels. On the other hand, existing benchmarks are small in scale and the samples are collected from a few fixed camera views with low scene diversities. To address this issue, we collect a large-scale and scene-diversified person search dataset, which contains 18,184 images, 8,432 persons, and 99,809 annotated bounding boxes\footnote{\url{http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html}}. We evaluate our approach and other baselines on the proposed dataset, and study the influence of various factors. Experiments show that our method achieves the best result. version:1
arxiv-1409-2558 | Penalty methods for a class of non-Lipschitz optimization problems | http://arxiv.org/abs/1409.2558 | id:1409.2558 author:Xiaojun Chen, Zhaosong Lu, Ting Kei Pong category:math.OC stat.ML  published:2014-09-09 summary:We consider a class of constrained optimization problems with a possibly nonconvex non-Lipschitz objective and a convex feasible set being the intersection of a polyhedron and a possibly degenerate ellipsoid. Such problems have a wide range of applications in data science, where the objective is used for inducing sparsity in the solutions while the constraint set models the noise tolerance and incorporates other prior information for data fitting. To solve this class of constrained optimization problems, a common approach is the penalty method. However, there is little theory on exact penalization for problems with nonconvex and non-Lipschitz objective functions. In this paper, we study the existence of exact penalty parameters regarding local minimizers, stationary points and $\epsilon$-minimizers under suitable assumptions. Moreover, we discuss a penalty method whose subproblems are solved via a nonmonotone proximal gradient method with a suitable update scheme for the penalty parameters, and prove the convergence of the algorithm to a KKT point of the constrained problem. Preliminary numerical results demonstrate the efficiency of the penalty method for finding sparse solutions of underdetermined linear systems. version:3
arxiv-1512-03384 | VRFP: On-the-fly Video Retrieval using Web Images and Fast Fisher Vector Products | http://arxiv.org/abs/1512.03384 | id:1512.03384 author:Xintong Han, Bharat Singh, Vlad I. Morariu, Larry S. Davis category:cs.CV  published:2015-12-10 summary:VRFP is a real-time video retrieval framework based on short text input queries in which weakly labeled training samples from the web are obtained, after the query is known. Our experiments show that a Fisher Vector is robust to noise present in web-images and compares favorably in terms of accuracy to other standard representations. While a Fisher Vector for a new query can be constructed efficiently, matching against the test set is slow due to its high dimensionality. To perform matching in real-time, we present a lossless algorithm for accelerating the computation of dot product between high dimensional Fisher Vectors. We prove that the expected number of multiplications required is quadratic in terms of sparsity in Fisher Vectors. We are not only able to construct and apply query models in real-time, but with the help of a simple re-ranking scheme, we also outperform state-of-the-art automatic retrieval methods by a significant margin on TRECVID MED13 (3.5%), MED14 (1.3%) and CCV datasets (5.2%). version:2
arxiv-1604-01841 | A Classification Leveraged Object Detector | http://arxiv.org/abs/1604.01841 | id:1604.01841 author:Miao Sun, Tony X. Han, Zhihai He category:cs.CV  published:2016-04-07 summary:Currently, the state-of-the-art image classification algorithms outperform the best available object detector by a big margin in terms of average precision. We, therefore, propose a simple yet principled approach that allows us to leverage object detection through image classification on supporting regions specified by a preliminary object detector. Using a simple bag-of- words model based image classification algorithm, we leveraged the performance of the deformable model objector from 35.9% to 39.5% in average precision over 20 categories on standard PASCAL VOC 2007 detection dataset. version:1
arxiv-1604-01839 | Clustering Via Crowdsourcing | http://arxiv.org/abs/1604.01839 | id:1604.01839 author:Arya Mazumdar, Barna Saha category:cs.DS cs.IT cs.LG math.IT  published:2016-04-07 summary:In recent years, crowdsourcing, aka human aided computation has emerged as an effective platform for solving problems that are considered complex for machines alone. Using human is time-consuming and costly due to monetary compensations. Therefore, a crowd based algorithm must judiciously use any information computed through an automated process, and ask minimum number of questions to the crowd adaptively. One such problem which has received significant attention is {\em entity resolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$ where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$, for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal is to retrieve the sets $V_i$s by making minimum number of pair-wise queries $V \times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query is correct, e.g. via resampling, then this reduces to finding connected components in a graph. On the other hand, when crowd answers may be incorrect, it corresponds to clustering over minimum number of noisy inputs. Even, with perfect answers, a simple lower and upper bound of $\Theta(nk)$ on query complexity can be shown. A major contribution of this paper is to reduce the query complexity to linear or even sublinear in $n$ when mild side information is provided by a machine, and even in presence of crowd errors which are not correctable via resampling. We develop new information theoretic lower bounds on the query complexity of clustering with side information and errors, and our upper bounds closely match with them. Our algorithms are naturally parallelizable, and also give near-optimal bounds on the number of adaptive rounds required to match the query complexity. version:1
arxiv-1603-05350 | Self-organization of vocabularies under different interaction orders | http://arxiv.org/abs/1603.05350 | id:1603.05350 author:Javier Vera category:cs.CL physics.soc-ph  published:2016-03-17 summary:Traditionally, the formation of vocabularies has been studied by agent-based models (specially, the Naming Game) in which random pairs of agents negotiate word-meaning associations at each discrete time step. This paper proposes a first approximation to a novel question: To what extent the negotiation of word-meaning associations is influenced by the order in which the individuals interact? Automata Networks provide the adequate mathematical framework to explore this question. Computer simulations suggest that on two-dimensional lattices the typical features of the formation of word-meaning associations are recovered under random schemes that update small fractions of the population at the same time. version:2
arxiv-1508-01577 | Automata networks model for alignment and least effort on vocabulary formation | http://arxiv.org/abs/1508.01577 | id:1508.01577 author:Javier Vera, Felipe Urbina, Eric Goles category:cs.CL physics.soc-ph  published:2015-08-07 summary:Can artificial communities of agents develop language with scaling relations close to the Zipf law? As a preliminary answer to this question, we propose an Automata Networks model of the formation of a vocabulary on a population of individuals, under two in principle opposite strategies: the alignment and the least effort principle. Within the previous account to the emergence of linguistic conventions (specially, the Naming Game), we focus on modeling speaker and hearer efforts as actions over their vocabularies and we study the impact of these actions on the formation of a shared language. The numerical simulations are essentially based on an energy function, that measures the amount of local agreement between the vocabularies. The results suggests that on one dimensional lattices the best strategy to the formation of shared languages is the one that minimizes the efforts of speakers on communicative tasks. version:2
arxiv-1510-02358 | Automata networks for multi-party communication in the Naming Game | http://arxiv.org/abs/1510.02358 | id:1510.02358 author:Javier Vera, Pedro Montealegre, Eric Goles category:cs.CL  published:2015-10-08 summary:The Naming Game has been studied to explore the role of self-organization in the development and negotiation of linguistic conventions. In this paper, we define an automata networks approach to the Naming Game. Two problems are faced: (1) the definition of an automata networks for multi-party communicative interactions; and (2) the proof of convergence for three different orders in which the individuals are updated (updating schemes). Finally, computer simulations are explored in two-dimensional lattices with the purpose to recover the main features of the Naming Game and to describe the dynamics under different updating schemes. version:2
arxiv-1507-04808 | Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models | http://arxiv.org/abs/1507.04808 | id:1507.04808 author:Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE I.5.1; I.2.7  published:2015-07-17 summary:We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings. version:3
arxiv-1510-03820 | A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification | http://arxiv.org/abs/1510.03820 | id:1510.03820 author:Ye Zhang, Byron Wallace category:cs.CL cs.LG cs.NE  published:2015-10-13 summary:Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (kim 2014, kalchbrenner 2014, johnson 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings. version:4
arxiv-1412-2404 | Dimensionality Reduction with Subspace Structure Preservation | http://arxiv.org/abs/1412.2404 | id:1412.2404 author:Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju category:cs.LG stat.ML  published:2014-12-07 summary:Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that $2K$ projection vectors are sufficient for the independence preservation of any $K$ class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving \textit{state-of-the-art} results compared to popular dimensionality reduction techniques. version:3
arxiv-1604-01828 | Differential TD Learning for Value Function Approximation | http://arxiv.org/abs/1604.01828 | id:1604.01828 author:Adithya M. Devraj, Sean P. Meyn category:cs.SY cs.LG math.OC 93E20  93E35  60J20  published:2016-04-06 summary:Value functions arise as a component of algorithms as well as performance metrics in statistics and engineering applications. Computation of the associated Bellman equations is numerically challenging in all but a few special cases. A popular approximation technique is known as Temporal Difference (TD) learning. The algorithm introduced in this paper is intended to resolve two well-known problems with this approach:In the discounted-cost setting, the variance of the algorithm diverges as the discount factor approaches unity. Second, for the average cost setting, unbiased algorithms exist only in special cases. It is shown that the gradient of any of these value functions admits a representation that lends itself to algorithm design. Based on this result, the new differential TD method is obtained for Markovian models on Euclidean space with smooth dynamics. Numerical examples show remarkable improvements in performance. In application to speed scaling, variance is reduced by two orders of magnitude. version:1
arxiv-1604-01827 | Deep Semantic Matching for Optical Flow | http://arxiv.org/abs/1604.01827 | id:1604.01827 author:Min Bai, Wenjie Luo, Kaustav Kundu, Raquel Urtasun category:cs.CV  published:2016-04-06 summary:We tackle the problem of estimating optical flow from a monocular camera in the context of autonomous driving. We build on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. We propose to estimate the traffic participants using instance-level segmentation. For each traffic participant, we use the epipolar constraints that govern each independent motion for faster and more accurate estimation. Our second contribution is a new convolutional net that learns to perform flow matching, and is able to estimate the uncertainty of its matches. This is a core element of our flow estimation pipeline. We demonstrate the effectiveness of our approach in the challenging KITTI 2015 flow benchmark, and show that our approach outperforms published approaches by a large margin. version:1
arxiv-1604-01818 | R-FUSE: Robust Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation | http://arxiv.org/abs/1604.01818 | id:1604.01818 author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret, Jose Bioucas-Dias, Simon Godsill category:cs.CV  published:2016-04-06 summary:This paper proposes a robust fast multi-band image fusion method to merge a high-spatial low-spectral resolution image and a low-spatial high-spectral resolution image. Following the method recently developed in [1], the generalized Sylvester matrix equation associated with the multi-band image fusion problem is solved in a more robust and efficient way by exploiting the Woodbury formula, avoiding any permutation operation in the frequency domain as well as the blurring kernel invertibility assumption required in [1]. Thanks to this improvement, the proposed algorithm requires fewer computational operations and is also more robust with respect to the blurring kernel compared with the one in [1]. The proposed new algorithm is tested with different priors considered in [1]. Our conclusion is that the proposed fusion algorithm is more robust than the one in [1] with a reduced computational cost. version:1
arxiv-1604-01806 | Generalising the Discriminative Restricted Boltzmann Machine | http://arxiv.org/abs/1604.01806 | id:1604.01806 author:Srikanth Cherla, Son N Tran, Tillman Weyde, Artur d'Avila Garcez category:cs.LG  published:2016-04-06 summary:We present a novel theoretical result that generalises the Discriminative Restricted Boltzmann Machine (DRBM). While originally the DRBM was defined assuming the {0, 1}-Bernoulli distribution in each of its hidden units, this result makes it possible to derive cost functions for variants of the DRBM that utilise other distributions, including some that are often encountered in the literature. This is illustrated with the Binomial and {-1, +1}-Bernoulli distributions here. We evaluate these two DRBM variants and compare them with the original one on three benchmark datasets, namely the MNIST and USPS digit classification datasets, and the 20 Newsgroups document classification dataset. Results show that each of the three compared models outperforms the remaining two in one of the three datasets, thus indicating that the proposed theoretical generalisation of the DRBM may be valuable in practice. version:1
arxiv-1506-02216 | A Recurrent Latent Variable Model for Sequential Data | http://arxiv.org/abs/1506.02216 | id:1506.02216 author:Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio category:cs.LG  published:2015-06-07 summary:In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state. version:6
arxiv-1604-01802 | Learning to Track at 100 FPS with Deep Regression Networks | http://arxiv.org/abs/1604.01802 | id:1604.01802 author:David Held, Sebastian Thrun, Silvio Savarese category:cs.CV cs.AI cs.LG cs.RO  published:2016-04-06 summary:Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for using neural networks to track generic objects in a way that allows them to improve performance by training on labeled videos. Previous attempts to use neural networks for tracking are very slow to run and not practical for real-time applications. In contrast, our tracker uses a simple feed-forward network with no online training required, allowing our tracker to run at 100 fps during test time. Our tracker trains from both labeled video as well as a large collection of images, which helps prevent overfitting. The tracker learns generic object motion and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Our network learns to track generic objects in real-time as they move throughout the world. version:1
arxiv-1604-01792 | Advances in Very Deep Convolutional Neural Networks for LVCSR | http://arxiv.org/abs/1604.01792 | id:1604.01792 author:Tom Sercu, Vaibhava Goel category:cs.CL cs.LG cs.NE  published:2016-04-06 summary:Very deep CNNs with small 3x3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper, we demonstrate that the accuracy gains of these deep CNNs are retained both on larger scale data, and after sequence training. We show this by carrying out sequence training on both the 300h switchboard-1 and the 2000h switchboard dataset. Furthermore, we investigate how pooling and padding in time influences performance, both in terms of word error rate and computational cost. We argue that designing CNNs without timepadding and without timepooling, though slightly suboptimal for accuracy, has two significant consequences. Firstly, the proposed design allows for efficient evaluation at sequence training and test (deployment) time. Secondly, this design principle allows for batch normalization to be adopted to CNNs on sequence data. Our very deep CNN model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching with a single model the performance of 2015 IBM system combination, which was the previous best published result. version:1
arxiv-1604-01787 | A Subpath Kernel for Learning Hierarchical Image Representations | http://arxiv.org/abs/1604.01787 | id:1604.01787 author:Yanwei Cui, Laetitia Chapel, Sébastien Lefèvre category:cs.CV  published:2016-04-06 summary:Tree kernels have demonstrated their ability to deal with hierarchical data, as the intrinsic tree structure often plays a discriminative role. While such kernels have been successfully applied to various domains such as nature language processing and bioinformatics, they mostly concentrate on ordered trees and whose nodes are described by symbolic data. Meanwhile, hierarchical representations have gained increasing interest to describe image content. This is particularly true in remote sensing, where such representations allow for revealing different objects of interest at various scales through a tree structure. However, the induced trees are unordered and the nodes are equipped with numerical features. In this paper, we propose a new structured kernel for hierarchical image representations which is built on the concept of subpath kernel. Experimental results on both artificial and remote sensing datasets show that the proposed kernel manages to deal with the hierarchical nature of the data, leading to better classification rates. version:1
arxiv-1604-01785 | Safe Probability | http://arxiv.org/abs/1604.01785 | id:1604.01785 author:Peter Grünwald category:stat.ME cs.AI cs.LG math.ST stat.TH 62A01  published:2016-04-06 summary:We formalize the idea of probability distributions that lead to reliable predictions about some, but not all aspects of a domain. The resulting notion of `safety' provides a fresh perspective on foundational issues in statistics, providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly Bayesian approaches on the other. It also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict, thus taking some of the sting out of the fiducial idea. By restricting probabilistic inference to safe uses, one also automatically avoids paradoxes such as the Monty Hall problem. Safety comes in a variety of degrees, such as "validity" (the strongest notion), "calibration", "confidence safety" and "unbiasedness" (almost the weakest notion). version:1
arxiv-1604-01753 | Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding | http://arxiv.org/abs/1604.01753 | id:1604.01753 author:Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta category:cs.CV  published:2016-04-06 summary:Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,850 annotated videos with an average length of 30 seconds, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 37,972 temporally localized intervals for 160 action classes and 24,623 labels for 40 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community. version:1
arxiv-1602-06632 | Denoising and Covariance Estimation of Single Particle Cryo-EM Images | http://arxiv.org/abs/1602.06632 | id:1602.06632 author:Tejal Bhamre, Teng Zhang, Amit Singer category:cs.CV q-bio.BM stat.ML  published:2016-02-22 summary:The problem of image restoration in cryo-EM entails correcting for the effects of the Contrast Transfer Function (CTF) and noise. Popular methods for image restoration include `phase flipping', which corrects only for the Fourier phases but not amplitudes, and Wiener filtering, which requires the spectral signal to noise ratio. We propose a new image restoration method which we call `Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the projection images is used within the classical Wiener filtering framework for solving the image restoration deconvolution problem. Our estimation procedure for the covariance matrix is new and successfully corrects for the CTF. We demonstrate the efficacy of CWF by applying it to restore both simulated and experimental cryo-EM images. Results with experimental datasets demonstrate that CWF provides a good way to evaluate the particle images and to see what the dataset contains even without 2D classification and averaging. version:3
arxiv-1604-01733 | A U-statistic Approach to Hypothesis Testing for Structure Discovery in Undirected Graphical Models | http://arxiv.org/abs/1604.01733 | id:1604.01733 author:Wacha Bounliphone, Matthew Blaschko category:stat.ML math.ST stat.TH  published:2016-04-06 summary:Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution, but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of distributions than considered in previous works. The resulting test enables one to answer with statistical significance whether an edge is present in the graph, and convergence results are known for a wide range of distributions. The computational complexities is linear in the sample size enabling the application of the test to large data samples for which computation time becomes a limiting factor. We experimentally validate the correctness and scalability of the test on multivariate distributions for which the distributional assumptions of competing tests result in underestimates of the false positive ratio. By contrast, the proposed test remains sound, promising to be a useful tool for hypothesis testing for diverse real-world problems. version:1
arxiv-1604-01729 | Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text | http://arxiv.org/abs/1604.01729 | id:1604.01729 author:Subhashini Venugopalan, Lisa Anne Hendricks, Raymond Mooney, Kate Saenko category:cs.CL cs.CV  published:2016-04-06 summary:This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while maintaining or modestly improving descriptive quality. Further, we show that such techniques can be beneficial for describing unseen object classes with no paired training data (zeroshot captioning). version:1
arxiv-1604-01720 | Reading Between the Pixels: Photographic Steganography for Camera Display Messaging | http://arxiv.org/abs/1604.01720 | id:1604.01720 author:Eric Wengrowski, Kristin Dana, Marco Gruteser, Narayan Mandayam category:cs.CV cs.GR cs.MM cs.NI I.4.8  published:2016-04-06 summary:We exploit human color metamers to send light-modulated messages less visible to the human eye, but recoverable by cameras. These messages are a key component to camera-display messaging, such as handheld smartphones capturing information from electronic signage. Each color pixel in the display image is modified by a particular color gradient vector. The challenge is to find the color gradient that maximizes camera response, while minimizing human response. The mismatch in human spectral and camera sensitivity curves creates an opportunity for hidden messaging. Our approach does not require knowledge of these sensitivity curves, instead we employ a data-driven method. We learn an ellipsoidal partitioning of the six-dimensional space of colors and color gradients. This partitioning creates metamer sets defined by the base color at the display pixel and the color gradient direction for message encoding. We sample from the resulting metamer sets to find color steps for each base color to embed a binary message into an arbitrary image with reduced visible artifacts. Unlike previous methods that rely on visually obtrusive intensity modulation, we embed with color so that the message is more hidden. Ordinary displays and cameras are used without the need for expensive LEDs or high speed devices. The primary contribution of this work is a framework to map the pixels in an arbitrary image to a metamer pair for steganographic photo messaging. version:1
arxiv-1511-01411 | Learning in Auctions: Regret is Hard, Envy is Easy | http://arxiv.org/abs/1511.01411 | id:1511.01411 author:Constantinos Daskalakis, Vasilis Syrgkanis category:cs.GT cs.AI cs.CC cs.LG  published:2015-11-04 summary:A line of recent work provides welfare guarantees of simple combinatorial auction formats, such as selling m items via simultaneous second price auctions (SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et al. 2013). These guarantees hold even when the auctions are repeatedly executed and players use no-regret learning algorithms. Unfortunately, off-the-shelf no-regret algorithms for these auctions are computationally inefficient as the number of actions is exponential. We show that this obstacle is insurmountable: there are no polynomial-time no-regret algorithms for SiSPAs, unless RP$\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises the question of how good outcomes polynomially-bounded bidders may discover in such auctions. To answer this question, we propose a novel concept of learning in auctions, termed "no-envy learning." This notion is founded upon Walrasian equilibrium, and we show that it is both efficiently implementable and results in approximately optimal welfare, even when the bidders have fractionally subadditive (XOS) valuations (assuming demand oracles) or coverage valuations (without demand oracles). No-envy learning outcomes are a relaxation of no-regret outcomes, which maintain their approximate welfare optimality while endowing them with computational tractability. Our results extend to other auction formats that have been studied in the literature via the smoothness paradigm. Our results for XOS valuations are enabled by a novel Follow-The-Perturbed-Leader algorithm for settings where the number of experts is infinite, and the payoff function of the learner is non-linear. This algorithm has applications outside of auction settings, such as in security games. Our result for coverage valuations is based on a novel use of convex rounding schemes and a reduction to online convex optimization. version:6
arxiv-1603-09279 | On the Geometry of Message Passing Algorithms for Gaussian Reciprocal Processes | http://arxiv.org/abs/1603.09279 | id:1603.09279 author:Francesca Paola Carli category:stat.ML math.OC  published:2016-03-30 summary:Reciprocal processes are acausal generalizations of Markov processes introduced by Bernstein in 1932. In the literature, a significant amount of attention has been focused on developing dynamical models for reciprocal processes. Recently, probabilistic graphical models for reciprocal processes have been provided. This opens the way to the application of efficient inference algorithms in the machine learning literature to solve the smoothing problem for reciprocal processes. Such algorithms are known to converge if the underlying graph is a tree. This is not the case for a reciprocal process, whose associated graphical model is a single loop network. The contribution of this paper is twofold. First, we introduce belief propagation for Gaussian reciprocal processes. Second, we establish a link between convergence analysis of belief propagation for Gaussian reciprocal processes and stability theory for differentially positive systems. version:2
arxiv-1604-01696 | A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories | http://arxiv.org/abs/1604.01696 | id:1604.01696 author:Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen category:cs.CL cs.AI  published:2016-04-06 summary:Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding. version:1
arxiv-1604-01692 | An Ensemble Method to Produce High-Quality Word Embeddings | http://arxiv.org/abs/1604.01692 | id:1604.01692 author:Robert Speer, Joshua Chin category:cs.CL I.2.7  published:2016-04-06 summary:A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of $\rho = .596$ on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system. version:1
arxiv-1604-01686 | Equivalence Among Different Variants of One-Class Nearest Neighbours and Creating Their Accurate Ensembles | http://arxiv.org/abs/1604.01686 | id:1604.01686 author:Shehroz S. Khan, Amir Ahmad category:cs.LG  published:2016-04-06 summary:In one-class classification (OCC) problems, only the data for the target class is available, whereas the data for the non-target class may be completely absent. In this paper, we study one-class nearest neighbour (OCNN) classifiers and their different variants for the OCC problem. We present a theoretical analysis to show the equivalence among different variants of OCNN that may use different neighbours or thresholds to identify unseen examples of the non-target class. We also present a method based on inter-quartile range for optimizing parameters used in OCNN in the absence of non-target data during training. Then, we propose to use two ensemble approaches based on random sub-space and random projection approaches to create accurate ensemble that significantly outperforms the baseline OCNN. We tested the proposed methods on various benchmark and real word domain-specific datasets to show their superior performance. The results give strong evidence that the random projection ensemble of the proposed OCNN with optimized parameters variants perform significantly and consistently better than the single OCC on all the tested datasets. version:1
arxiv-1510-04189 | Improving Back-Propagation by Adding an Adversarial Gradient | http://arxiv.org/abs/1510.04189 | id:1510.04189 author:Arild Nøkland category:stat.ML cs.LG  published:2015-10-14 summary:The back-propagation algorithm is widely used for learning in artificial neural networks. A challenge in machine learning is to create models that generalize to new data samples not seen in the training data. Recently, a common flaw in several machine learning algorithms was discovered: small perturbations added to the input data lead to consistent misclassification of data samples. Samples that easily mislead the model are called adversarial examples. Training a "maxout" network on adversarial examples has shown to decrease this vulnerability, but also increase classification performance. This paper shows that adversarial training has a regularizing effect also in networks with logistic, hyperbolic tangent and rectified linear units. A simple extension to the back-propagation method is proposed, that adds an adversarial gradient to the training. The extension requires an additional forward and backward pass to calculate a modified input sample, or mini batch, used as input for standard back-propagation learning. The first experimental results on MNIST show that the "adversarial back-propagation" method increases the resistance to adversarial examples and boosts the classification performance. The extension reduces the classification error on the permutation invariant MNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a network with rectified linear units. Results on CIFAR-10 indicate that the method has a regularizing effect similar to dropout in fully connected networks. Based on these promising results, adversarial back-propagation is proposed as a stand-alone regularizing method that should be further investigated. version:2
arxiv-1601-00917 | DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks | http://arxiv.org/abs/1601.00917 | id:1601.00917 author:Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, Tat-Seng Chua category:cs.LG cs.NE  published:2016-01-05 summary:The performance of deep neural networks is well-known to be sensitive to the setting of their hyperparameters. Recent advances in reverse-mode automatic differentiation allow for optimizing hyperparameters with gradients. The standard way of computing these gradients involves a forward and backward pass of computations. However, the backward pass usually needs to consume unaffordable memory to store all the intermediate variables to exactly reverse the forward training procedure. In this work we propose a simple but effective method, DrMAD, to distill the knowledge of the forward pass into a shortcut path, through which we approximately reverse the training trajectory. Experiments on several image benchmark datasets show that DrMAD is at least 45 times faster and consumes 100 times less memory compared to state-of-the-art methods for optimizing hyperparameters with minimal compromise to its effectiveness. To the best of our knowledge, DrMAD is the first research attempt to make it practical to automatically tune thousands of hyperparameters of deep neural networks. The code can be downloaded from https://github.com/bigaidream-projects/drmad version:5
arxiv-1602-02009 | Computing with hardware neurons: spiking or classical? Perspectives of applied Spiking Neural Networks from the hardware side | http://arxiv.org/abs/1602.02009 | id:1602.02009 author:Sergei Dytckov, Masoud Daneshtalab category:cs.NE  published:2016-02-05 summary:While classical neural networks take a position of a leading method in the machine learning community, spiking neuromorphic systems bring attention and large projects in neuroscience. Spiking neural networks were shown to be able to substitute networks of classical neurons in applied tasks. This work explores recent hardware designs focusing on perspective applications (like convolutional neural networks) for both neuron types from the energy efficiency side to analyse whether there is a possibility for spiking neuromorphic hardware to grow up for a wider use. Our comparison shows that spiking hardware is at least on the same level of energy efficiency or even higher than non-spiking on a level of basic operations. However, on a system level, spiking systems are outmatched and consume much more energy due to inefficient data representation with a long series of spikes. If spike-driven applications, minimizing an amount of spikes, are developed, spiking neural systems may reach the energy efficiency level of classical neural systems. However, in the near future, both type of neuromorphic systems may benefit from emerging memory technologies, minimizing the energy consumption of computation and memory for both neuron types. That would make infrastructure and data transfer energy dominant on the system level. We expect that spiking neurons have some benefits, which would allow achieving better energy results. Still the problem of an amount of spikes will still be the major bottleneck for spiking hardware systems. version:2
arxiv-1511-04986 | A genetic algorithm to discover flexible motifs with support | http://arxiv.org/abs/1511.04986 | id:1511.04986 author:Joan Serrà, Aleksandar Matic, Josep Luis Arcos, Alexandros Karatzoglou category:cs.LG cs.NE  published:2015-11-16 summary:Finding repeated patterns or motifs in a time series is an important unsupervised task that has still a number of open issues, starting by the definition of motif. In this paper, we revise the notion of motif support, characterizing it as the number of patterns or repetitions that define a motif. We then propose GENMOTIF, a genetic algorithm to discover motifs with support which, at the same time, is flexible enough to accommodate other motif specifications and task characteristics. GENMOTIF is an anytime algorithm that easily adapts to many situations: searching in a range of segment lengths, applying uniform scaling, dealing with multiple dimensions, using different similarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: it has only two intuitive parameters which, if set within reasonable bounds, do not substantially affect its performance. We demonstrate the value of our approach in a number of synthetic and real-world settings, considering traffic volume measurements, accelerometer signals, and telephone call records. version:2
arxiv-1603-05354 | Modeling self-organization of vocabularies under phonological similarity effects | http://arxiv.org/abs/1603.05354 | id:1603.05354 author:Javier Vera category:cs.CL physics.soc-ph  published:2016-03-17 summary:This work develops a computational model (by Automata Networks) of phonological similarity effects involved in the formation of word-meaning associations on artificial populations of speakers. Classical studies show that in recalling experiments memory performance was impaired for phonologically similar words versus dissimilar ones. Here, the individuals confound phonologically similar words according to a predefined parameter. The main hypothesis is that there is a critical range of the parameter, and with this, of working-memory mechanisms, which implies drastic changes in the final consensus of the entire population. Theoretical results present proofs of convergence for a particular case of the model within a worst-case complexity framework. Computer simulations describe the evolution of an energy function that measures the amount of local agreement between individuals. The main finding is the appearance of sudden changes in the energy function at critical parameters. version:2
arxiv-1603-06708 | A Self-Paced Regularization Framework for Multi-Label Learning | http://arxiv.org/abs/1603.06708 | id:1603.06708 author:Changsheng Li, Fan Wei, Junchi Yan, Weishan Dong, Qingshan Liu, Xiaoyu Zhang, Hongyuan Zha category:cs.LG  published:2016-03-22 summary:In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach. version:2
arxiv-1604-01643 | Information Utilization Ratio in Heuristic Optimization Algorithms | http://arxiv.org/abs/1604.01643 | id:1604.01643 author:Junzhi Li, Ying Tan category:cs.NE  published:2016-04-06 summary:Heuristic algorithms are able to optimize objective functions efficiently because they use intelligently the information of the objective functions. Thus information utilization is vital to the performance of heuristics. However, the concept of information utilization has remained vague and abstract because there is no reliable metric to reflect the extent to which the information of the objective function is utilized in heuristic algorithms. In this paper, the metric of information utilization ratio (IUR) is defined, which is the ratio between the utilized information quantity and the acquired information quantity in the searching process. IUR proves to be well-defined. Several examples of typical heuristic algorithms are given to demonstrate the procedure of calculating IUR. The results also reveal that elevating IUR is the potential cause of many algorithmic improving works. IUR can be an index of how exquisite an algorithm is designed and guide the design of new heuristics and the improvement of existing ones. version:1
arxiv-1604-01351 | Nonparametric Detection of Geometric Structures over Networks | http://arxiv.org/abs/1604.01351 | id:1604.01351 author:Shaofeng Zou, Yingbin Liang, H. Vincent Poor category:stat.ML  published:2016-04-05 summary:Nonparametric detection of existence of an anomalous structure over a network is investigated. Nodes corresponding to the anomalous structure (if one exists) receive samples generated by a distribution q, which is different from a distribution p generating samples for other nodes. If an anomalous structure does not exist, all nodes receive samples generated by p. It is assumed that the distributions p and q are arbitrary and unknown. The goal is to design statistically consistent tests with probability of errors converging to zero as the network size becomes asymptotically large. Kernel-based tests are proposed based on maximum mean discrepancy that measures the distance between mean embeddings of distributions into a reproducing kernel Hilbert space. Detection of an anomalous interval over a line network is first studied. Sufficient conditions on minimum and maximum sizes of candidate anomalous intervals are characterized in order to guarantee the proposed test to be consistent. It is also shown that certain necessary conditions must hold to guarantee any test to be universally consistent. Comparison of sufficient and necessary conditions yields that the proposed test is order-level optimal and nearly optimal respectively in terms of minimum and maximum sizes of candidate anomalous intervals. Generalization of the results to other networks is further developed. Numerical results are provided to demonstrate the performance of the proposed tests. version:2
arxiv-1506-02465 | ASlib: A Benchmark Library for Algorithm Selection | http://arxiv.org/abs/1506.02465 | id:1506.02465 author:Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Frechette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown, Kevin Tierney, Joaquin Vanschoren category:cs.AI cs.LG  published:2015-06-08 summary:The task of algorithm selection involves choosing an algorithm from a set of algorithms on a per-instance basis in order to exploit the varying performance of algorithms over a set of instances. The algorithm selection problem is attracting increasing attention from researchers and practitioners in AI. Years of fruitful applications in a number of domains have resulted in a large amount of data, but the community lacks a standard format or repository for this data. This situation makes it difficult to share and compare different approaches effectively, as is done in other, more established fields. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we introduce a standardized format for representing algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format has been designed to be able to express a wide variety of different scenarios. Demonstrating the breadth and power of our platform, we describe a set of example experiments that build and evaluate algorithm selection models through a common interface. The results display the potential of algorithm selection to achieve significant performance improvements across a broad range of problems and algorithms. version:3
arxiv-1604-02100 | Hankel Matrix Nuclear Norm Regularized Tensor Completion for N-dimensional Exponential Signals | http://arxiv.org/abs/1604.02100 | id:1604.02100 author:Jiaxi Ying, Hengfa Lu, Qingtao Wei, Jian-Feng Cai, Di Guo, Jihui Wu, Zhong Chen, Xiaobo Qu category:stat.ML cs.NA  published:2016-04-06 summary:Signals are usually modeled as a superposition of exponential functions in spectroscopy of chemistry, biology and medical imaging. However, for fast data acquisition or other inevitable reasons, only a small amount of samples may be acquired. How to recover the full signal is then of great interest. Existing approaches can not efficiently recover N-dimensional exponential signals with N>=3. This paper studies the problem of recovering N-dimensional (particularly $N\geq 3$) exponential signals from partial observations, and we formulate this problem as a low-rank tensor completion problem with exponential factors. The full signal is reconstructed by simultaneously exploiting the CANDECOMP/PARAFAC (CP) tensor decomposition and the exponential structure of the associated factors, of which the latter is promoted by minimizing an objective function involving the nuclear norm of Hankel matrices. Experimental results on simulated and real magnetic resonance spectroscopy data show that the proposed approach can successfully recover full signals from very limited samples and is robust to the estimated tensor rank. version:1
arxiv-1604-01592 | Fast $(1+ε)$-approximation of the Löwner extremal matrices of high-dimensional symmetric matrices | http://arxiv.org/abs/1604.01592 | id:1604.01592 author:Frank Nielsen, Richard Nock category:cs.CG cs.CV  published:2016-04-06 summary:Matrix data sets are common nowadays like in biomedical imaging where the Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) modality produces data sets of 3D symmetric positive definite matrices anchored at voxel positions capturing the anisotropic diffusion properties of water molecules in biological tissues. The space of symmetric matrices can be partially ordered using the L\"owner ordering, and computing extremal matrices dominating a given set of matrices is a basic primitive used in matrix-valued signal processing. In this letter, we design a fast and easy-to-implement iterative algorithm to approximate arbitrarily finely these extremal matrices. Finally, we discuss on extensions to matrix clustering. version:1
arxiv-1508-07544 | Computational Sociolinguistics: A Survey | http://arxiv.org/abs/1508.07544 | id:1508.07544 author:Dong Nguyen, A. Seza Doğruöz, Carolyn P. Rosé, Franciska de Jong category:cs.CL  published:2015-08-30 summary:Language is a social phenomenon and variation is inherent to its social nature. Recently, there has been a surge of interest within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of "Computational Sociolinguistics" that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions employed in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges. version:2
arxiv-1511-00540 | Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem Solvers | http://arxiv.org/abs/1511.00540 | id:1511.00540 author:Jonathan Binas, Giacomo Indiveri, Michael Pfeiffer category:cs.NE  published:2015-11-02 summary:Solving constraint satisfaction problems (CSPs) is a notoriously expensive computational task. Recently, it has been proposed that efficient stochastic solvers can be obtained through appropriately configured spiking neural networks performing Markov Chain Monte Carlo (MCMC) sampling. The possibility to run such models on massively parallel, low-power neuromorphic hardware holds great promise; however, previously proposed networks are based on probabilistically spiking neurons, and thus rely on random number generators or external noise sources to achieve the necessary stochasticity, leading to significant overhead in the implementation. Here we show how stochasticity can be achieved by implementing deterministic models of integrate and fire neurons using subthreshold analog circuits that are affected by thermal noise. We present an efficient implementation of spike-based CSP solvers using a reconfigurable neural network VLSI device, and the device's intrinsic noise as a source of randomness. To illustrate the overall concept, we implement a generic Sudoku solver based on our approach and demonstrate its operation. We establish a link between the neuron parameters and the system dynamics, allowing for a simple temperature control mechanism. version:2
arxiv-1604-01545 | Training Constrained Deconvolutional Networks for Road Scene Semantic Segmentation | http://arxiv.org/abs/1604.01545 | id:1604.01545 author:German Ros, Simon Stent, Pablo F. Alcantarilla, Tomoki Watanabe category:cs.CV  published:2016-04-06 summary:In this work we investigate the problem of road scene semantic segmentation using Deconvolutional Networks (DNs). Several constraints limit the practical performance of DNs in this context: firstly, the paucity of existing pixel-wise labelled training data, and secondly, the memory constraints of embedded hardware, which rule out the practical use of state-of-the-art DN architectures such as fully convolutional networks (FCN). To address the first constraint, we introduce a Multi-Domain Road Scene Semantic Segmentation (MDRS3) dataset, aggregating data from six existing densely and sparsely labelled datasets for training our models, and two existing, separate datasets for testing their generalisation performance. We show that, while MDRS3 offers a greater volume and variety of data, end-to-end training of a memory efficient DN does not yield satisfactory performance. We propose a new training strategy to overcome this, based on (i) the creation of a best-possible source network (S-Net) from the aggregated data, ignoring time and memory constraints; and (ii) the transfer of knowledge from S-Net to the memory-efficient target network (T-Net). We evaluate different techniques for S-Net creation and T-Net transferral, and demonstrate that training a constrained deconvolutional network in this manner can unlock better performance than existing training approaches. Specifically, we show that a target network can be trained to achieve improved accuracy versus an FCN despite using less than 1\% of the memory. We believe that our approach can be useful beyond automotive scenarios where labelled data is similarly scarce or fragmented and where practical constraints exist on the desired model size. We make available our network models and aggregated multi-domain dataset for reproducibility. version:1
arxiv-1604-01537 | Generating Chinese Classical Poems with RNN Encoder-Decoder | http://arxiv.org/abs/1604.01537 | id:1604.01537 author:Xiaoyuan Yi, Ruoyu Li, Maosong Sun category:cs.CL cs.NE  published:2016-04-06 summary:We take the generation of Chinese classical poem lines as a sequence-to-sequence learning problem, and build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a topic word as input. Our system can jointly learn semantic meaning within a single line, semantic relevance among lines in a poem, and the use of structural, rhythmical and tonal patterns, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems. We also find that the attention mechanism can capture the word associations in Chinese classical poetry and inverting target lines in training can improve performance. version:1
arxiv-1604-01170 | Accurate and scalable social recommendation using mixed-membership stochastic block models | http://arxiv.org/abs/1604.01170 | id:1604.01170 author:Antonia Godoy-Lorite, Roger Guimera, Cristopher Moore, Marta Sales-Pardo category:cs.SI cs.IR cs.LG physics.soc-ph  published:2016-04-05 summary:With ever-increasing amounts of online information available, modeling and predicting individual preferences-for books or articles, for example-is becoming more and more important. Good predictions enable us to improve advice to users, and obtain a better understanding of the socio-psychological processes that determine those preferences. We have developed a collaborative filtering model, with an associated scalable algorithm, that makes accurate predictions of individuals' preferences. Our approach is based on the explicit assumption that there are groups of individuals and of items, and that the preferences of an individual for an item are determined only by their group memberships. Importantly, we allow each individual and each item to belong simultaneously to mixtures of different groups and, unlike many popular approaches, such as matrix factorization, we do not assume implicitly or explicitly that individuals in each group prefer items in a single group of items. The resulting overlapping groups and the predicted preferences can be inferred with a expectation-maximization algorithm whose running time scales linearly (per iteration). Our approach enables us to predict individual preferences in large datasets, and is considerably more accurate than the current algorithms for such large datasets. version:2
arxiv-1604-01518 | Simple and Efficient Learning using Privileged Information | http://arxiv.org/abs/1604.01518 | id:1604.01518 author:Xinxing Xu, Joey Tianyi Zhou, IvorW. Tsang, Zheng Qin, Rick Siow Mong Goh, Yong Liu category:cs.LG  published:2016-04-06 summary:The Support Vector Machine using Privileged Information (SVM+) has been proposed to train a classifier to utilize the additional privileged information that is only available in the training phase but not available in the test phase. In this work, we propose an efficient solution for SVM+ by simply utilizing the squared hinge loss instead of the hinge loss as in the existing SVM+ formulation, which interestingly leads to a dual form with less variables and in the same form with the dual of the standard SVM. The proposed algorithm is utilized to leverage the additional web knowledge that is only available during training for the image categorization tasks. The extensive experimental results on both Caltech101 andWebQueries datasets show that our proposed method can achieve a factor of up to hundred times speedup with the comparable accuracy when compared with the existing SVM+ method. version:1
arxiv-1604-01515 | Comments on: "A Random Forest Guided Tour" by G. Biau and E. Scornet | http://arxiv.org/abs/1604.01515 | id:1604.01515 author:Sylvain Arlot, Robin Genuer category:math.ST stat.ME stat.ML stat.TH  published:2016-04-06 summary:This paper is a comment on the survey paper by Biau and Scornet (2016) about random forests. We focus on the problem of quantifying the impact of each ingredient of random forests on their performance. We show that such a quantification is possible for a simple pure forest , leading to conclusions that could apply more generally. Then, we consider "hold-out" random forests, which are a good middle point between "toy" pure forests and Breiman's original random forests. version:1
arxiv-1602-07360 | SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size | http://arxiv.org/abs/1602.07360 | id:1602.07360 author:Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer category:cs.CV cs.AI  published:2016-02-24 summary:Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet version:3
arxiv-1604-01500 | LOMo: Latent Ordinal Model for Facial Analysis in Videos | http://arxiv.org/abs/1604.01500 | id:1604.01500 author:Karan Sikka, Gaurav Sharma, Marian Bartlett category:cs.CV  published:2016-04-06 summary:We study the problem of facial analysis in videos. We propose a novel weakly supervised learning method that models the video event (expression, pain etc.) as a sequence of automatically mined, discriminative sub-events (eg. onset and offset phase for smile, brow lower and cheek raise for pain). The proposed model is inspired by the recent works on Multiple Instance Learning and latent SVM/HCRF- it extends such frameworks to model the ordinal or temporal aspect in the videos, approximately. We obtain consistent improvements over relevant competitive baselines on four challenging and publicly available video based facial analysis datasets for prediction of expression, clinical pain and intent in dyadic conversations. In combination with complimentary features, we report state-of-the-art results on these datasets. version:1
arxiv-1604-01497 | How does the Low-Rank Matrix Completion Help Internal and External Learnings for Super-Resolution | http://arxiv.org/abs/1604.01497 | id:1604.01497 author:Shuang Wang, Bo Yue, Xuefeng Liang, Peiyuan Ji, Licheng Jiao category:cs.CV  published:2016-04-06 summary:A new challenge in Super-resolution (SR) problem is how to utilize the pros of internal and external learnings to further enhance the result. To address this issue, we analyze the attributes of these two kinds of methods, and find they are complementary in the feature space and image plan, meanwhile, have sparse estimation error. This finding inspires us to propose a low-rank solution which effectively integrates them together. We then tailor an internal prior learning and an external dictionary learning to fit the solution. With a theoretical analysis on the algorithm, we also prove that the low-rank solution does not require massive input to guarantee the performance. This simplifies the design of the internal and external learning methods for the solution, and reduces the computation cost. Unlike other methods, the proposed solution is a parameter free integration, and can be generalized with more recent internal and external learning methods. Intensive experiments show the proposed solution reconstructs image details effectively, also outperforms state-of-the-arts in both visual and quantitative assessments, especially for the noisy images. version:1
arxiv-1604-01495 | Parameterized Analysis of Multi-objective Evolutionary Algorithms and the Weighted Vertex Cover Problem | http://arxiv.org/abs/1604.01495 | id:1604.01495 author:Mojgan Pourhassan, Feng Shi, Frank Neumann category:cs.NE cs.DS  published:2016-04-06 summary:A rigorous runtime analysis of evolutionary multi-objective optimization for the classical vertex cover problem in the context of parameterized complexity analysis has been presented by Kratsch and Neumann (2013). In this paper, we extend the analysis to the weighted vertex cover problem and provide a fixed parameter evolutionary algorithm with respect to OPT, the cost of the the optimal solution for the problem. Moreover, using a diversity mechanisms, we present a multi-objective evolutionary algorithm that finds a 2-approximation in expected polynomial time and introduce a population-based evolutionary algorithm which finds a $(1+\varepsilon)$-approximation in expected time $O(n\cdot 2^{\min \{n,2(1- \varepsilon)OPT \}} + n^3)$. version:1
arxiv-1603-09446 | Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs | http://arxiv.org/abs/1603.09446 | id:1603.09446 author:Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang, Xiang Bai category:cs.CV  published:2016-03-31 summary:Object skeleton is a useful cue for object detection, complementary to the object contour, as it provides a structural representation to describe the relationship among object parts. While object skeleton extraction in natural images is a very challenging problem, as it requires the extractor to be able to capture both local and global image context to determine the intrinsic scale of each skeleton pixel. Existing methods rely on per-pixel based multi-scale feature computation, which results in difficult modeling and high time consumption. In this paper, we present a fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the sequential stages in the network and the skeleton scales they can capture, we introduce a scale-associated side output to each stage. We impose supervision to different stages by guiding the scale-associated side outputs toward groundtruth skeletons of different scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to localize skeleton pixels with multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors. version:2
arxiv-1604-02013 | Keyboard Based Control of Four Dimensional Rotations | http://arxiv.org/abs/1604.02013 | id:1604.02013 author:Akira Kageyama category:cs.GR cs.CV  published:2016-04-06 summary:Aiming at applications to the scientific visualization of three dimensional simulations with time evolution, a keyboard based control method to specify rotations in four dimensions is proposed. It is known that four dimensional rotations are generally so-called double rotations, and a double rotation is a combination of simultaneously applied two simple rotations. The proposed method can specify both the simple and double rotations by single key typings of the keyboard. The method is tested in visualizations of a regular pentachoron in four dimensional space by a hyperplane slicing. version:1
arxiv-1604-01485 | A Focused Dynamic Attention Model for Visual Question Answering | http://arxiv.org/abs/1604.01485 | id:1604.01485 author:Ilija Ilievski, Shuicheng Yan, Jiashi Feng category:cs.CV cs.CL cs.NE  published:2016-04-06 summary:Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects. Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question. In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions. Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit. Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers. Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines. version:1
arxiv-1501-00630 | Non-iterative rigid 2D/3D point-set registration using semidefinite programming | http://arxiv.org/abs/1501.00630 | id:1501.00630 author:Yuehaw Khoo, Ankur Kapoor category:cs.CV math.OC 90C22  92C55 G.1.6; I.4.9  published:2015-01-04 summary:We describe a convex programming framework for pose estimation in 2D/3D point-set registration with unknown point correspondences. We give two mixed-integer nonlinear program (MINP) formulations of the 2D/3D registration problem when there are multiple 2D images, and propose convex relaxations for both of the MINPs to semidefinite programs (SDP) that can be solved efficiently by interior point methods. Our approach to the 2D/3D registration problem is non-iterative in nature as we jointly solve for pose and correspondence. Furthermore, these convex programs can readily incorporate feature descriptors of points to enhance registration results. We prove that the convex programs exactly recover the solution to the original nonconvex 2D/3D registration problem under noiseless condition. We apply these formulations to the registration of 3D models of coronary vessels to their 2D projections obtained from multiple intra-operative fluoroscopic images. For this application, we experimentally corroborate the exact recovery property in the absence of noise and further demonstrate robustness of the convex programs in the presence of noise. version:3
arxiv-1604-01475 | Learning A Deep $\ell_\infty$ Encoder for Hashing | http://arxiv.org/abs/1604.01475 | id:1604.01475 author:Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, Thomas S. Huang category:cs.LG cs.CV  published:2016-04-06 summary:We investigate the $\ell_\infty$-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named \textit{Deep $\ell_\infty$ Encoder}, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a \textit{Deep Siamese $\ell_\infty$ Network}, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors. version:1
arxiv-1604-01474 | Self-Paced Multi-Task Learning | http://arxiv.org/abs/1604.01474 | id:1604.01474 author:Changsheng Li, Fan Wei, Junchi Yan, Weishan Dong, Qingshan Liu, Hongyuan Zha category:cs.LG  published:2016-04-06 summary:In this paper, we propose a novel multi-task learning (MTL) framework, called Self-Paced Multi-Task Learning (SPMTL). Different from previous works treating all tasks and instances equally when training, SPMTL attempts to jointly learn the tasks by taking into consideration the complexities of both tasks and instances. This is inspired by the cognitive process of human brain that often learns from the easy to the hard. We construct a compact SPMTL formulation by proposing a new task-oriented regularizer that can jointly prioritize the tasks and the instances. Thus it can be interpreted as a self-paced learner for MTL. A simple yet effective algorithm is designed for optimizing the proposed objective function. An error bound for a simplified formulation is also analyzed theoretically. Experimental results on toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the state-of-the-art methods. version:1
arxiv-1510-07712 | Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks | http://arxiv.org/abs/1510.07712 | id:1510.07712 author:Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu category:cs.CV  published:2015-10-26 summary:We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal- and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively. version:2
arxiv-1603-03958 | Template Adaptation for Face Verification and Identification | http://arxiv.org/abs/1603.03958 | id:1603.03958 author:Nate Crosswhite, Jeffrey Byrne, Omkar M. Parkhi, Chris Stauffer, Qiong Cao, Andrew Zisserman category:cs.CV  published:2016-03-12 summary:Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild dataset for imagery and the YouTubeFaces dataset for videos. In contrast, the newly released IJB-A face recognition dataset unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for template-based face verification and identification. version:3
arxiv-1511-08250 | Recurrent Instance Segmentation | http://arxiv.org/abs/1511.08250 | id:1511.08250 author:Bernardino Romera-Paredes, Philip H. S. Torr category:cs.CV cs.AI  published:2015-11-25 summary:Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing learning opportunities. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows handling occlusion. In order to train the model we designed a new principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting. version:2
arxiv-1604-01433 | Collaborative Representation Learning | http://arxiv.org/abs/1604.01433 | id:1604.01433 author:Matías Vera, Leonardo Rey Vega, Pablo Piantanida category:cs.IT math.IT stat.ML  published:2016-04-05 summary:This paper investigates an information-theoretic approach to the problem of collaborative representation learning: how to extract salient features of statistical relationships in order to build cooperatively meaningful representations of some relevant content. Modeling the structure of data and its hidden representations by independently identically distributed samples, our goal is to study fundamental limits of the so-called Two-way Collaborative Representation Learning (TW-CRL) and the Collaborative Distributed Representation Learning (CDRL) problems. The TW-CRL problem consists of two distant encoders that separately observe marginal (dependent) components $X_1$ and $X_2$ and can cooperate through multiple exchanges of limited information with the aim of learning hidden representations $(Y_1,Y_2)$, which can be arbitrarily dependent on $(X_1,X_2)$. On the other hand, in CDRL there are two cooperating encoders and the learner of the hidden representation $Y$ is a third node which can listen the exchanges between the two encoders. The relevance (figure-of-merit) of such learned representations is measured in terms of a normalized (per-sample) multi-letter mutual information metric. Inner and outer bounds to the complexity-relevance region of these problems are derived from which optimality is characterized for several cases of interest. Our resulting complexity-relevance regions are finally evaluated for binary symmetric and Gaussian statistical models showing how to identify comparatively random features that represent complexity-constrained statistics for the inference of the hidden representations. version:1
arxiv-1604-01420 | Highly accurate gaze estimation using a consumer RGB-depth sensor | http://arxiv.org/abs/1604.01420 | id:1604.01420 author:Reza Shoja Ghiass, Ognjen Arandjelovic category:cs.CV  published:2016-04-05 summary:Determining the direction in which a person is looking is an important problem in a wide range of HCI applications. In this paper we describe a highly accurate algorithm that performs gaze estimation using an affordable and widely available device such as Kinect. The method we propose starts by performing accurate head pose estimation achieved by fitting a person specific morphable model of the face to depth data. The ordinarily competing requirements of high accuracy and high speed are met concurrently by formulating the fitting objective function as a combination of terms which excel either in accurate or fast fitting, and then by adaptively adjusting their relative contributions throughout fitting. Following pose estimation, pose normalization is done by re-rendering the fitted model as a frontal face. Finally gaze estimates are obtained through regression from the appearance of the eyes in synthetic, normalized images. Using EYEDIAP, the standard public dataset for the evaluation of gaze estimation algorithms from RGB-D data, we demonstrate that our method greatly outperforms the state of the art. version:1
arxiv-1511-06040 | A Hierarchical Deep Temporal Model for Group Activity Recognition | http://arxiv.org/abs/1511.06040 | id:1511.06040 author:Moustafa Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori category:cs.CV  published:2015-11-19 summary:In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of in- dividual people in a sequence and another LSTM model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods. version:2
arxiv-1604-01416 | dMath: A Scalable Linear Algebra and Math Library for Heterogeneous GP-GPU Architectures | http://arxiv.org/abs/1604.01416 | id:1604.01416 author:Steven Eliuk, Cameron Upright, Anthony Skjellum category:cs.NE cs.DC cs.MS  published:2016-04-05 summary:A new scalable parallel math library, dMath, is presented in this paper that demonstrates leading scaling when using intranode, or internode, hybrid-parallelism for deep-learning. dMath provides easy-to-use distributed base primitives and a variety of domain-specific algorithms. These include matrix multiplication, convolutions, and others allowing for rapid development of highly scalable applications, including Deep Neural Networks (DNN), whereas previously one was restricted to libraries that provided effective primitives for only a single GPU, like Nvidia cublas and cudnn or DNN primitives from Nervana neon framework. Development of HPC software is difficult, labor-intensive work, requiring a unique skill set. dMath allows a wide range of developers to utilize parallel and distributed hardware easily. One contribution of this approach is that data is stored persistently on the GPU hardware, avoiding costly transfers between host and device. Advanced memory management techniques are utilized, including caching of transferred data and memory reuse through pooling. A key contribution of dMath is that it delivers performance, portability, and productivity to its specific domain of support. It enables algorithm and application programmers to quickly solve problems without managing the significant complexity associated with multi-level parallelism. version:1
arxiv-1604-01360 | The Curious Robot: Learning Visual Representations via Physical Interactions | http://arxiv.org/abs/1604.01360 | id:1604.01360 author:Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, Abhinav Gupta category:cs.CV cs.AI cs.RO  published:2016-04-05 summary:What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require semantic labels. In fact, we argue that biological agents use active exploration and physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and actively observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing backprops to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Finally, we evaluate our learned ConvNet on different image classification tasks and show improvements compared to learning without external data. version:1
arxiv-1604-01357 | Heavy hitters via cluster-preserving clustering | http://arxiv.org/abs/1604.01357 | id:1604.01357 author:Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen, Mikkel Thorup category:cs.DS cs.LG  published:2016-04-05 summary:In turnstile $\ell_p$ $\varepsilon$-heavy hitters, one maintains a high-dimensional $x\in\mathbb{R}^n$ subject to $\texttt{update}(i,\Delta)$ causing $x_i\leftarrow x_i + \Delta$, where $i\in[n]$, $\Delta\in\mathbb{R}$. Upon receiving a query, the goal is to report a small list $L\subset[n]$, $ L = O(1/\varepsilon^p)$, containing every "heavy hitter" $i\in[n]$ with $ x_i \ge \varepsilon \ x_{\overline{1/\varepsilon^p}}\ _p$, where $x_{\overline{k}}$ denotes the vector obtained by zeroing out the largest $k$ entries of $x$ in magnitude. For any $p\in(0,2]$ the CountSketch solves $\ell_p$ heavy hitters using $O(\varepsilon^{-p}\log n)$ words of space with $O(\log n)$ update time, $O(n\log n)$ query time to output $L$, and whose output after any query is correct with high probability (whp) $1 - 1/poly(n)$. Unfortunately the query time is very slow. To remedy this, the work [CM05] proposed for $p=1$ in the strict turnstile model, a whp correct algorithm achieving suboptimal space $O(\varepsilon^{-1}\log^2 n)$, worse update time $O(\log^2 n)$, but much better query time $O(\varepsilon^{-1}poly(\log n))$. We show this tradeoff between space and update time versus query time is unnecessary. We provide a new algorithm, ExpanderSketch, which in the most general turnstile model achieves optimal $O(\varepsilon^{-p}\log n)$ space, $O(\log n)$ update time, and fast $O(\varepsilon^{-p}poly(\log n))$ query time, and whp correctness. Our main innovation is an efficient reduction from the heavy hitters to a clustering problem in which each heavy hitter is encoded as some form of noisy spectral cluster in a much bigger graph, and the goal is to identify every cluster. Since every heavy hitter must be found, correctness requires that every cluster be found. We then develop a "cluster-preserving clustering" algorithm, partitioning the graph into clusters without destroying any original cluster. version:1
arxiv-1604-01354 | Radiometric Scene Decomposition: Scene Reflectance, Illumination, and Geometry from RGB-D Images | http://arxiv.org/abs/1604.01354 | id:1604.01354 author:Stephen Lombardi, Ko Nishino category:cs.CV  published:2016-04-05 summary:Recovering the radiometric properties of a scene (i.e., the reflectance, illumination, and geometry) is a long-sought ability of computer vision that can provide invaluable information for a wide range of applications. Deciphering the radiometric ingredients from the appearance of a real-world scene, as opposed to a single isolated object, is particularly challenging as it generally consists of various objects with different material compositions exhibiting complex reflectance and light interactions that are also part of the illumination. We introduce the first method for radiometric scene decomposition that handles those intricacies. We use RGB-D images to bootstrap geometry recovery and simultaneously recover the complex reflectance and natural illumination while refining the noisy initial geometry and segmenting the scene into different material regions. Most important, we handle real-world scenes consisting of multiple objects of unknown materials, which necessitates the modeling of spatially-varying complex reflectance, natural illumination, texture, interreflection and shadows. We systematically evaluate the effectiveness of our method on synthetic scenes and demonstrate its application to real-world scenes. The results show that rich radiometric information can be recovered from RGB-D images and demonstrate a new role RGB-D sensors can play for general scene understanding tasks. version:1
arxiv-1604-01350 | Bounded Optimal Exploration in MDP | http://arxiv.org/abs/1604.01350 | id:1604.01350 author:Kenji Kawaguchi category:cs.AI cs.LG  published:2016-04-05 summary:Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and non-Bayesian methods. version:1
arxiv-1604-01348 | Bayesian Optimization with Exponential Convergence | http://arxiv.org/abs/1604.01348 | id:1604.01348 author:Kenji Kawaguchi, Leslie Pack Kaelbling, Tomás Lozano-Pérez category:stat.ML cs.LG  published:2016-04-05 summary:This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate. version:1
arxiv-1604-01347 | Marr Revisited: 2D-3D Alignment via Surface Normal Prediction | http://arxiv.org/abs/1604.01347 | id:1604.01347 author:Aayush Bansal, Bryan Russell, Abhinav Gupta category:cs.CV  published:2016-04-05 summary:We introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library. Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trained Oxford VGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods. Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval. When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input. Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches. version:1
arxiv-1603-08358 | Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs | http://arxiv.org/abs/1603.08358 | id:1603.08358 author:Siddhartha Chandra, Iasonas Kokkinos category:cs.CV cs.LG  published:2016-03-28 summary:In this work we propose a combination of the Gaussian Conditional Random Field (G-CRF) with Deep Learning for the task of structured prediction. Our method inherits several virtues of G-CRF and Deep Learning: (a) the structured prediction task has a unique global optimum that is obtained exactly from the solution of a linear system (b) structured prediction can be jointly trained in an end-to-end setting in general architectures and with arbitrary loss functions, (c) the pairwise terms do not have to be simple hand-crafted expressions, as in the line of works building on the DenseCRF, but can rather be `discovered' from data through deep architectures - in particular we use fully convolutional networks to obtain the unary and pairwise terms of our G-CRF. Building on standard tools from numerical analysis we develop very efficient algorithms for inference and learning. This efficiency allows us to explore more sophisticated architectures for structured prediction in deep learning: we introduce multi-resolution architectures to couple information across scales in a joint optimization framework, yielding systematic improvement. We demonstrate the utility of our approach on the challenging VOC PASCAL 2012 image segmentation benchmark, where an extensive ablation study indicates substantial improvements over strong baselines. version:2
arxiv-1604-01345 | Discovering Perceptual Attributes in a Deep Local Material Recognition Network | http://arxiv.org/abs/1604.01345 | id:1604.01345 author:Gabriel Schwartz, Ko Nishino category:cs.CV  published:2016-04-05 summary:Perceptual material attributes, intrinsic visual properties of materials, are being studied in parallel in computer vision and human vision. In neuroscience, perceptual attributes are shown to be an integral part of the human neural response during material recognition. In computer vision, however, they are merely an intermediate representation of materials and not integrated into the recognition process. In this paper, we show that perceptual material attributes can indeed be found inside a framework for local, patch-based, object-independent material recognition. We introduce a new CNN architecture, the material attribute-category CNN (MAC-CNN), that uses deep weak supervision to simultaneously classify materials and discover per-pixel perceptual attributes. We show that these attributes conform with past semantic material attributes and enhance recognition of novel materials. We also introduce an extensive new database for local material recognition. Our results show that the internal representation of the MAC-CNN generalizes well and agrees with human perception, which has potential implications for our understanding of human material perception as well as applications in object recognition. version:1
arxiv-1604-01335 | Deep Cross Residual Learning for Multitask Visual Recognition | http://arxiv.org/abs/1604.01335 | id:1604.01335 author:Brendan Jou, Shih-Fu Chang category:cs.CV cs.AI cs.MM  published:2016-04-05 summary:Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia and vision applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of in-network regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with >40% less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4% over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting. version:1
arxiv-1603-01913 | A Latent Variable Recurrent Neural Network for Discourse Relation Language Models | http://arxiv.org/abs/1603.01913 | id:1603.01913 author:Yangfeng Ji, Gholamreza Haffari, Jacob Eisenstein category:cs.CL cs.LG cs.NE stat.ML  published:2016-03-07 summary:This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. version:2
arxiv-1604-01325 | Deep Image Retrieval: Learning global representations for image search | http://arxiv.org/abs/1604.01325 | id:1604.01325 author:Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus category:cs.CV  published:2016-04-05 summary:We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we introduce a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we leverage a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. We intend to release our pre-trained model. version:1
arxiv-1604-01319 | Cohomology of Cryo-Electron Microscopy | http://arxiv.org/abs/1604.01319 | id:1604.01319 author:Ke Ye, Lek-Heng Lim category:cs.CV math.AT  published:2016-04-05 summary:The goal of cryo-electron microscopy (EM) is to reconstruct the 3-dimensional structure of a molecule from a collection of its 2-dimensional projected images. In this article, we show that the basic premise of cryo-EM --- patching together 2-dimensional projections to reconstruct a 3-dimensional object --- is naturally one of Cech cohomology with SO(2)-coefficients. We deduce that every cryo-EM reconstruction problem corresponds to an oriented circle bundle on a simplicial complex, allowing us to classify cryo-EM problems via principal bundles. In practice, the 2-dimensional images are noisy and a main task in cryo-EM is to denoise them. We will see how the aforementioned insights can be used towards this end. version:1
arxiv-1604-01304 | Towards Label Imbalance in Multi-label Classification with Many Labels | http://arxiv.org/abs/1604.01304 | id:1604.01304 author:Li Li, Houfeng Wang category:cs.LG  published:2016-04-05 summary:In multi-label classification, an instance may be associated with a set of labels simultaneously. Recently, the research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large. The existing works focus on how to design scalable algorithms that offer fast training procedures and have a small memory footprint. However they ignore and even compound another challenge - the label imbalance problem. To address this drawback, we propose a novel Representation-based Multi-label Learning with Sampling (RMLS) approach. To the best of our knowledge, we are the first to tackle the imbalance problem in multi-label classification with many labels. Our experimentations with real-world datasets demonstrate the effectiveness of the proposed approach. version:1
arxiv-1604-01278 | RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy | http://arxiv.org/abs/1604.01278 | id:1604.01278 author:Guntis Barzdins, Didzis Gosko category:cs.CL  published:2016-04-05 summary:Two extensions to the AMR smatch scoring script are presented. The first extension com-bines the smatch scoring script with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. This first extension results in 4% gain over the state-of-art CAMR baseline parser by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. The second extension combines a per-sentence smatch with an en-semble method for selecting the best AMR graph among the set of AMR graphs for the same sentence. This second modification au-tomatically yields further 0.4% gain when ap-plied to outputs of two nondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level neural translation AMR parser. For AMR parsing task the character-level neural translation attains surprising 7% gain over the carefully optimized word-level neural translation. Overall, we achieve smatch F1=62% on the SemEval-2016 official scor-ing set and F1=67% on the LDC2015E86 test set. version:1
arxiv-1604-01272 | Feature extraction using Latent Dirichlet Allocation and Neural Networks: A case study on movie synopses | http://arxiv.org/abs/1604.01272 | id:1604.01272 author:Despoina Christou category:cs.CL cs.AI cs.IR cs.LG stat.ML  published:2016-04-05 summary:Feature extraction has gained increasing attention in the field of machine learning, as in order to detect patterns, extract information, or predict future observations from big data, the urge of informative features is crucial. The process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space, to higher level meaningful abstractions. This dissertation employs Neural Networks for distributed paragraph representations, and Latent Dirichlet Allocation to capture higher level features of paragraph vectors. Although Neural Networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors, we show that a quick topic analysis model such as Latent Dirichlet Allocation can provide meaningful features too. We evaluate the two methods on the CMU Movie Summary Corpus, a collection of 25,203 movie plot summaries extracted from Wikipedia. Finally, for both approaches, we use K-Nearest Neighbors to discover similar movies, and plot the projected representations using T-Distributed Stochastic Neighbor Embedding to depict the context similarities. These similarities, expressed as movie distances, can be used for movies recommendation. The recommended movies of this approach are compared with the recommended movies from IMDB, which use a collaborative filtering recommendation approach, to show that our two models could constitute either an alternative or a supplementary recommendation approach. version:1
arxiv-1603-09509 | Learning Multiscale Features Directly From Waveforms | http://arxiv.org/abs/1603.09509 | id:1603.09509 author:Zhenyao Zhu, Jesse H. Engel, Awni Hannun category:cs.CL cs.LG cs.NE cs.SD  published:2016-03-31 summary:Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms. version:2
arxiv-1604-01252 | Comparative Deep Learning of Hybrid Representations for Image Recommendations | http://arxiv.org/abs/1604.01252 | id:1604.01252 author:Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li category:cs.CV  published:2016-04-05 summary:In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed \emph{hybrid} and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two sub-networks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other state-of-the-art image recommendation solutions. version:1
arxiv-1604-01243 | Mental Lexicon Growth Modelling Reveals the Multiplexity of the English Language | http://arxiv.org/abs/1604.01243 | id:1604.01243 author:Massimo Stella, Markus Brede category:physics.soc-ph cs.CL cs.SI  published:2016-04-05 summary:In this work we extend previous analyses of linguistic networks by adopting a multi-layer network framework for modelling the human mental lexicon, i.e. an abstract mental repository where words and concepts are stored together with their linguistic patterns. Across a three-layer linguistic multiplex, we model English words as nodes and connect them according to (i) phonological similarities, (ii) synonym relationships and (iii) free word associations. Our main aim is to exploit this multi-layered structure to explore the influence of phonological and semantic relationships on lexicon assembly over time. We propose a model of lexicon growth which is driven by the phonological layer: words are suggested according to different orderings of insertion (e.g. shorter word length, highest frequency, semantic multiplex features) and accepted or rejected subject to constraints. We then measure times of network assembly and compare these to empirical data about the age of acquisition of words. In agreement with empirical studies in psycholinguistics, our results provide quantitative evidence for the hypothesis that word acquisition is driven by features at multiple levels of organisation within language. version:1
arxiv-1604-01235 | A new TAG Formalism for Tamil and Parser Analytics | http://arxiv.org/abs/1604.01235 | id:1604.01235 author:Vijay Krishna Menon, S. Rajendran, M. Anand Kumar, K. P. Soman category:cs.CL  published:2016-04-05 summary:Tree adjoining grammar (TAG) is specifically suited for morph rich and agglutinated languages like Tamil due to its psycho linguistic features and parse time dependency and morph resolution. Though TAG and LTAG formalisms have been known for about 3 decades, efforts on designing TAG Syntax for Tamil have not been entirely successful due to the complexity of its specification and the rich morphology of Tamil language. In this paper we present a minimalistic TAG for Tamil without much morphological considerations and also introduce a parser implementation with some obvious variations from the XTAG system version:1
arxiv-1604-01221 | Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project | http://arxiv.org/abs/1604.01221 | id:1604.01221 author:Guntis Barzdins, Steve Renals, Didzis Gosko category:cs.CL  published:2016-04-05 summary:The paper steps outside the comfort-zone of the traditional NLP tasks like automatic speech recognition (ASR) and machine translation (MT) to addresses two novel problems arising in the automated multilingual news monitoring: segmentation of the TV and radio program ASR transcripts into individual stories, and clustering of the individual stories coming from various sources and languages into storylines. Storyline clustering of stories covering the same events is an essential task for inquisitorial media monitoring. We address these two problems jointly by engaging the low-dimensional semantic representation capabilities of the sequence to sequence neural translation models. To enable joint multi-task learning for multilingual neural translation of morphologically rich languages we replace the attention mechanism with the sliding-window mechanism and operate the sequence to sequence neural translation model on the character-level rather than on the word-level. The story segmentation and storyline clustering problem is tackled by examining the low-dimensional vectors produced as a side-product of the neural translation process. The results of this paper describe a novel approach to the automatic story segmentation and storyline clustering problem. version:1
arxiv-1602-06727 | Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training | http://arxiv.org/abs/1602.06727 | id:1602.06727 author:Zhizheng Wu, Simon King category:cs.SD cs.CL cs.NE  published:2016-02-22 summary:We propose two novel techniques --- stacking bottleneck features and minimum generation error training criterion --- to improve the performance of deep neural network (DNN)-based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNN-based synthesis frameworks. Stacking bottleneck features, which are an acoustically--informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The minimum generation error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems. version:3
arxiv-1604-01219 | Learning to Generate Posters of Scientific Papers | http://arxiv.org/abs/1604.01219 | id:1604.01219 author:Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal category:cs.AI cs.CL cs.HC cs.MM stat.ML  published:2016-04-05 summary:Researchers often summarize their work in the form of posters. Posters provide a coherent and efficient way to convey core ideas from scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including panel layout and attributes of each panel, are learned and inferred from data. Then, given inferred layout and attributes, composition of graphical elements within each panel is synthesized. To learn and validate our model, we collect and make public a Poster-Paper dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach. version:1
arxiv-1603-08865 | Compilation as a Typed EDSL-to-EDSL Transformation | http://arxiv.org/abs/1603.08865 | id:1603.08865 author:Emil Axelsson category:cs.CL  published:2016-03-29 summary:This article is about an implementation and compilation technique that is used in RAW-Feldspar which is a complete rewrite of the Feldspar embedded domain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-level functional language that generates efficient C code to run on embedded targets. The gist of the technique presented in this post is the following: rather writing a back end that converts pure Feldspar expressions directly to C, we translate them to a low-level monadic EDSL. From the low-level EDSL, C code is then generated. This approach has several advantages: 1. The translation is simpler to write than a complete C back end. 2. The translation is between two typed EDSLs, which rules out many potential errors. 3. The low-level EDSL is reusable and can be shared between several high-level EDSLs. Although the article contains a lot of code, most of it is in fact reusable. As mentioned in Discussion, we can write the same implementation in less than 50 lines of code using generic libraries that we have developed to support Feldspar. version:2
arxiv-1408-6027 | Label Distribution Learning | http://arxiv.org/abs/1408.6027 | id:1408.6027 author:Xin Geng category:cs.LG  published:2014-08-26 summary:Although multi-label learning can deal with many problems with label ambiguity, it does not fit some real applications well where the overall distribution of the importance of the labels matters. This paper proposes a novel learning paradigm named \emph{label distribution learning} (LDL) for such kind of applications. The label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is a more general learning framework which includes both single-label and multi-label learning as its special cases. This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design. In order to compare the performance of the LDL algorithms, six representative and diverse evaluation measures are selected via a clustering analysis, and the first batch of label distribution datasets are collected and made publicly available. Experimental results on one artificial and fifteen real-world datasets show clear advantages of the specialized algorithms, which indicates the importance of special design for the characteristics of the LDL problem. version:2
arxiv-1409-5686 | Transfer Prototype-based Fuzzy Clustering | http://arxiv.org/abs/1409.5686 | id:1409.5686 author:Zhaohong Deng, Yizhang Jiang, Fu-Lai Chung, Hisao Ishibuchi, Kup-Sze Choi, Shitong Wang category:cs.LG  published:2014-09-19 summary:The traditional prototype based clustering methods, such as the well-known fuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good clustering partition. If the available data is limited or scarce, most of the existing prototype based clustering algorithms will no longer be effective. While the data for the current clustering task may be scarce, there is usually some useful knowledge available in the related scenes/domains. In this study, the concept of transfer learning is applied to prototype based fuzzy clustering (PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to develop a set of transfer prototype based fuzzy clustering (TPFC) algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM, fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been chosen to incorporate with knowledge leveraging mechanism to develop the corresponding transfer clustering algorithms. Novel objective functions are proposed to integrate the knowledge of source domain with the data of target domain for clustering in the target domain. The proposed algorithms have been validated on different synthetic and real-world datasets and the results demonstrate their effectiveness when compared with both the original prototype based fuzzy clustering algorithms and the related clustering algorithms like multi-task clustering and co-clustering. version:2
arxiv-1511-06581 | Dueling Network Architectures for Deep Reinforcement Learning | http://arxiv.org/abs/1511.06581 | id:1511.06581 author:Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas category:cs.LG  published:2015-11-20 summary:In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain. version:3
arxiv-1604-01178 | Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks | http://arxiv.org/abs/1604.01178 | id:1604.01178 author:Aliaksei Severyn, Alessandro Moschitti category:cs.CL  published:2016-04-05 summary:In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art. version:1
arxiv-1603-09095 | Learning Local Descriptors by Optimizing the Keypoint-Correspondence Criterion | http://arxiv.org/abs/1603.09095 | id:1603.09095 author:Nenad Markuš, Igor S. Pandžić, Jörgen Ahlberg category:cs.CV  published:2016-03-30 summary:Current best local descriptors are learned on a large dataset of matching and non-matching keypoint pairs. However, data of this kind is not always available since detailed keypoint correspondences can be hard to establish. On the other hand, we can often obtain labels for pairs of keypoint bags. For example, keypoint bags extracted from two images of the same object under different views form a matching pair, and keypoint bags extracted from images of different objects form a non-matching pair. On average, matching pairs should contain more corresponding keypoints than non-matching pairs. We describe an end-to-end differentiable architecture that enables the learning of local keypoint descriptors from such weakly-labeled data. version:2
