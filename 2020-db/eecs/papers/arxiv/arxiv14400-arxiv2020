arxiv-1601-06630 | Bayesian Estimation of Bipartite Matchings for Record Linkage | http://arxiv.org/abs/1601.06630 | id:1601.06630 author:Mauricio Sadinle category:stat.ME stat.AP stat.ML  published:2016-01-25 summary:The bipartite record linkage task consists of merging two disparate datafiles containing information on two overlapping sets of entities. This is non-trivial in the absence of unique identifiers and it is important for a wide variety of applications given that it needs to be solved whenever we have to combine information from different sources. Most statistical techniques currently used for record linkage are derived from a seminal paper by Fellegi and Sunter (1969). These techniques usually assume independence in the matching statuses of record pairs to derive estimation procedures and optimal point estimators. We argue that this independence assumption is unreasonable and instead target a bipartite matching between the two datafiles as our parameter of interest. Bayesian implementations allow us to quantify uncertainty on the matching decisions and derive a variety of point estimators using different loss functions. We propose partial Bayes estimates that allow uncertain parts of the bipartite matching to be left unresolved. We evaluate our approach to record linkage using a variety of challenging scenarios and show that it outperforms the traditional methodology. We illustrate the advantages of our methods merging two datafiles on casualties from the civil war of El Salvador. version:1
arxiv-1510-01098 | Intensity-only optical compressive imaging using a multiply scattering material and a double phase retrieval approach | http://arxiv.org/abs/1510.01098 | id:1510.01098 author:Boshra Rajaei, Eric W. Tramel, Sylvain Gigan, Florent Krzakala, Laurent Daudet category:cs.CV  published:2015-10-05 summary:In this paper, the problem of compressive imaging is addressed using natural randomization by means of a multiply scattering medium. To utilize the medium in this way, its corresponding transmission matrix must be estimated. To calibrate the imager, we use a digital micromirror device (DMD) as a simple, cheap, and high-resolution binary intensity modulator. We propose a phase retrieval algorithm which is well adapted to intensity-only measurements on the camera, and to the input binary intensity patterns, both to estimate the complex transmission matrix as well as image reconstruction. We demonstrate promising experimental results for the proposed algorithm using the MNIST dataset of handwritten digits as example images. version:2
arxiv-1601-06615 | A Taxonomy of Deep Convolutional Neural Nets for Computer Vision | http://arxiv.org/abs/1601.06615 | id:1601.06615 author:Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita Prabhu, Srinivas S S Kruthiventi, R. Venkatesh Babu category:cs.CV cs.LG cs.MM  published:2016-01-25 summary:Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative -- that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e. deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision - convolutional neural networks (CNNs). We start with "AlexNet" as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision. version:1
arxiv-1601-06608 | An Unsupervised Method for Detection and Validation of The Optic Disc and The Fovea | http://arxiv.org/abs/1601.06608 | id:1601.06608 author:Mrinal Haloi, Samarendra Dandapat, Rohit Sinha category:cs.CV 68T45  published:2016-01-25 summary:In this work, we have presented a novel method for detection of retinal image features, the optic disc and the fovea, from colour fundus photographs of dilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map based method was used to detect the optic disc followed by an unsupervised probabilistic Latent Semantic Analysis for detection validation. The validation concept is based on distinct vessels structures in the optic disc. By using the clinical information of standard location of the fovea with respect to the optic disc, the macula region is estimated. Accuracy of 100\% detection is achieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\% detection accuracy on STARE dataset. version:1
arxiv-1601-06603 | Egocentric Activity Recognition with Multimodal Fisher Vector | http://arxiv.org/abs/1601.06603 | id:1601.06603 author:Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal, Jie Lin category:cs.MM cs.CV  published:2016-01-25 summary:With the increasing availability of wearable devices, research on egocentric activity recognition has received much attention recently. In this paper, we build a Multimodal Egocentric Activity dataset which includes egocentric videos and sensor data of 20 fine-grained and diverse activity categories. We present a novel strategy to extract temporal trajectory-like features from sensor data. We propose to apply the Fisher Kernel framework to fuse video and temporal enhanced sensor features. Experiment results show that with careful design of feature extraction and fusion algorithm, sensor data can enhance information-rich video data. We make publicly available the Multimodal Egocentric Activity dataset to facilitate future research. version:1
arxiv-1601-06580 | Is swarm intelligence able to create mazes? | http://arxiv.org/abs/1601.06580 | id:1601.06580 author:Dawid Polap, Marcin Wozniak, Christian Napoli, Emiliano Tramontana category:cs.NE cs.AI  published:2016-01-25 summary:In this paper, the idea of applying Computational Intelligence in the process of creation board games, in particular mazes, is presented. For two different algorithms the proposed idea has been examined. The results of the experiments are shown and discussed to present advantages and disadvantages. version:1
arxiv-1601-06579 | A Kernel Independence Test for Geographical Language Variation | http://arxiv.org/abs/1601.06579 | id:1601.06579 author:Dong Nguyen, Jacob Eisenstein category:cs.CL  published:2016-01-25 summary:Quantifying the degree of spatial dependence for linguistic variables is a key task for analyzing dialectal variation. However, existing approaches have important drawbacks. First, they make unjustified assumptions about the nature of spatial variation: some assume that the geographical distribution of linguistic variables is Gaussian, while others assume that linguistic variation is aligned to pre-defined geopolitical units such as states or counties. Second, they are not applicable to all types of linguistic data: some approaches apply only to frequencies, others to boolean indicators of whether a linguistic variable is present. We present a new method for measuring geographical language variation, which solves both of these problems. Our approach builds on reproducing kernel Hilbert space (RKHS) representations for nonparametric statistics, and takes the form of a test statistic that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real datasets: a corpus of Dutch tweets, a Dutch syntactic atlas, and a dataset of letters to the editor in North American newspapers. Our proposed test is shown to support robust inferences across a broad range of scenarios and types of data. version:1
arxiv-1512-07807 | Visualizations Relevant to The User By Multi-View Latent Variable Factorization | http://arxiv.org/abs/1512.07807 | id:1512.07807 author:Seppo Virtanen, Homayun Afrabandpey, Samuel Kaski category:cs.LG cs.IR  published:2015-12-24 summary:A main goal of data visualization is to find, from among all the available alternatives, mappings to the 2D/3D display which are relevant to the user. Assuming user interaction data, or other auxiliary data about the items or their relationships, the goal is to identify which aspects in the primary data support the user\'s input and, equally importantly, which aspects of the user\'s potentially noisy input have support in the primary data. For solving the problem, we introduce a multi-view embedding in which a latent factorization identifies which aspects in the two data views (primary data and user data) are related and which are specific to only one of them. The factorization is a generative model in which the display is parameterized as a part of the factorization and the other factors explain away the aspects not expressible in a two-dimensional display. Functioning of the model is demonstrated on several data sets. version:2
arxiv-1509-01509 | EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual Scene Analysis | http://arxiv.org/abs/1509.01509 | id:1509.01509 author:Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes, Radu Horaud category:cs.CV cs.LG stat.ML  published:2015-09-04 summary:Data clustering has received a lot of attention and numerous methods, algorithms and software packages are available. Among these techniques, parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization (EM). In this paper we propose a new mixture model that associates a weight with each observed point. We introduce the weighted-data Gaussian mixture and we derive two EM algorithms. The first one considers a fixed weight for each observation. The second one treats each weight as a random variable following a gamma distribution. We propose a model selection method based on a minimum message length criterion, provide a weight initialization strategy, and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques. We also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data, namely audio-visual scene analysis. version:2
arxiv-1511-09196 | Non-Adaptive Group Testing on Graphs | http://arxiv.org/abs/1511.09196 | id:1511.09196 author:Hamid Kameli category:cs.DS cs.LG  published:2015-11-30 summary:Grebinski and Kucherov (1998) and Alon et al. (2004-2005) study the problem of learning a hidden graph for some especial cases, such as hamiltonian cycle, cliques, stars, and matchings. This problem is motivated by problems in chemical reactions, molecular biology and genome sequencing. In this paper, we present a generalization of this problem. Precisely, we consider a graph G and a subgraph H of G and we assume that G contains exactly one defective subgraph isomorphic to H. The goal is to find the defective subgraph by testing whether an induced subgraph contains an edge of the defective subgraph, with the minimum number of tests. We present an upper bound for the number of tests to find the defective subgraph by using the symmetric and high probability variation of Lov\'asz Local Lemma. version:2
arxiv-1512-04792 | From One Point to A Manifold: Knowledge Graph Embedding For Precise Link Prediction | http://arxiv.org/abs/1512.04792 | id:1512.04792 author:Han Xiao, Minlie Huang, Xiaoyan Zhu category:cs.AI cs.LG  published:2015-12-15 summary:Knowledge graph embedding aims at offering a numerical knowledge representation paradigm by transforming the entities and relations into continuous vector space. However, existing methods could not characterize the knowledge graph in a fine degree to make a precise prediction. There are two reasons: being an ill-posed algebraic system and applying an overstrict geometric form. As precise prediction is critical, we propose an manifold-based embedding principle (\textbf{ManifoldE}) which could be treated as a well-posed algebraic system that expands the position of golden triples from one point in current models to a manifold in ours. Extensive experiments show that the proposed models achieve substantial improvements against the state-of-the-art baselines especially for the precise prediction task, and yet maintain high efficiency. version:3
arxiv-1505-06611 | Smooth PARAFAC Decomposition for Tensor Completion | http://arxiv.org/abs/1505.06611 | id:1505.06611 author:Tatsuya Yokota, Qibin Zhao, Andrzej Cichocki category:cs.CV  published:2015-05-25 summary:In recent years, low-rank based tensor completion, which is a higher-order extension of matrix completion, has received considerable attention. However, the low-rank assumption is not sufficient for the recovery of visual data, such as color and 3D images, where the ratio of missing data is extremely high. In this paper, we consider "smoothness" constraints as well as low-rank approximations, and propose an efficient algorithm for performing tensor completion that is particularly powerful regarding visual data. The proposed method admits significant advantages, owing to the integration of smooth PARAFAC decomposition for incomplete tensors and the efficient selection of models in order to minimize the tensor rank. Thus, our proposed method is termed as "smooth PARAFAC tensor completion (SPC)." In order to impose the smoothness constraints, we employ two strategies, total variation (SPC-TV) and quadratic variation (SPC-QV), and invoke the corresponding algorithms for model learning. Extensive experimental evaluations on both synthetic and real-world visual data illustrate the significant improvements of our method, in terms of both prediction performance and efficiency, compared with many state-of-the-art tensor completion methods. version:3
arxiv-1601-03821 | Learning Binary Features Online from Motion Dynamics for Incremental Loop-Closure Detection and Place Recognition | http://arxiv.org/abs/1601.03821 | id:1601.03821 author:Guangcong Zhang, Mason J. Lilly, Patricio A. Vela category:cs.CV  published:2016-01-15 summary:This paper proposes a simple yet effective approach to learn visual features online for improving loop-closure detection and place recognition, based on bag-of-words frameworks. The approach learns a codeword in bag-of-words model from a pair of matched features from two consecutive frames, such that the codeword has temporally-derived perspective invariance to camera motion. The learning algorithm is efficient: the binary descriptor is generated from the mean image patch, and the mask is learned based on discriminative projection by minimizing the intra-class distances among the learned feature and the two original features. A codeword for bag-of-words models is generated by packaging the learned descriptor and mask, with a masked Hamming distance defined to measure the distance between two codewords. The geometric properties of the learned codewords are then mathematically justified. In addition, hypothesis constraints are imposed through temporal consistency in matched codewords, which improves precision. The approach, integrated in an incremental bag-of-words system, is validated on multiple benchmark data sets and compared to state-of-the-art methods. Experiments demonstrate improved precision/recall outperforming state of the art with little loss in runtime. version:2
arxiv-1510-00084 | QUDA: A Direct Approach for Sparse Quadratic Discriminant Analysis | http://arxiv.org/abs/1510.00084 | id:1510.00084 author:Binyan Jiang, Xiangyu Wang, Chenlei Leng category:stat.ME stat.CO stat.ML  published:2015-10-01 summary:Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named QUDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, QUDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of QUDA is illustrated via extensive simulation studies and the analysis of two datasets. version:2
arxiv-1509-07859 | Information Limits for Recovering a Hidden Community | http://arxiv.org/abs/1509.07859 | id:1509.07859 author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT math.IT  published:2015-09-25 summary:We study the problem of recovering a hidden community of cardinality $K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$, $A_{ij} \sim P$ if $i, j$ both belong to the community and $A_{ij} \sim Q$ otherwise, for two known probability distributions $P$ and $Q$ depending on $n$. If $P={\rm Bern}(p)$ and $Q={\rm Bern}(q)$ with $p>q$, it reduces to the problem of finding a densely-connected $K$-subgraph planted in a large Erd\"os-R\'enyi graph; if $P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with $\mu>0$, it corresponds to the problem of locating a $K \times K$ principal submatrix of elevated means in a large Gaussian random matrix. We focus on two types of asymptotic recovery guarantees as $n \to \infty$: (1) weak recovery: expected number of classification errors is $o(K)$; (2) exact recovery: probability of classifying all indices correctly converges to one. Under mild assumptions on $P$ and $Q$, and allowing the community size to scale sublinearly with $n$, we derive a set of sufficient conditions and a set of necessary conditions for recovery, which are asymptotically tight with sharp constants. The results hold in particular for the Gaussian case, and for the case of bounded log likelihood ratio, including the Bernoulli case whenever $\frac{p}{q}$ and $\frac{1-p}{1-q}$ are bounded away from zero and infinity. An important algorithmic implication is that, whenever exact recovery is information theoretically possible, any algorithm that provides weak recovery when the community size is concentrated near $K$ can be upgraded to achieve exact recovery in linear additional time by a simple voting procedure. version:2
arxiv-1601-06476 | A new correlation clustering method for cancer mutation analysis | http://arxiv.org/abs/1601.06476 | id:1601.06476 author:Jack P. Hou, Amin Emad, Gregory J. Puleo, Jian Ma, Olgica Milenkovic category:cs.LG q-bio.QM  published:2016-01-25 summary:Cancer genomes exhibit a large number of different alterations that affect many genes in a diverse manner. It is widely believed that these alterations follow combinatorial patterns that have a strong connection with the underlying molecular interaction networks and functional pathways. A better understanding of the generative mechanisms behind the mutation rules and their influence on gene communities is of great importance for the process of driver mutations discovery and for identification of network modules related to cancer development and progression. We developed a new method for cancer mutation pattern analysis based on a constrained form of correlation clustering. Correlation clustering is an agnostic learning method that can be used for general community detection problems in which the number of communities or their structure is not known beforehand. The resulting algorithm, named $C^3$, leverages mutual exclusivity of mutations, patient coverage, and driver network concentration principles; it accepts as its input a user determined combination of heterogeneous patient data, such as that available from TCGA (including mutation, copy number, and gene expression information), and creates a large number of clusters containing mutually exclusive mutated genes in a particular type of cancer. The cluster sizes may be required to obey some useful soft size constraints, without impacting the computational complexity of the algorithm. To test $C^3$, we performed a detailed analysis on TCGA breast cancer and glioblastoma data and showed that our algorithm outperforms the state-of-the-art CoMEt method in terms of discovering mutually exclusive gene modules and identifying driver genes. Our $C^3$ method represents a unique tool for efficient and reliable identification of mutation patterns and driver pathways in large-scale cancer genomics studies. version:1
arxiv-1511-00725 | Galaxy-X: A Novel Approach for Multi-class Classification in an Open Universe | http://arxiv.org/abs/1511.00725 | id:1511.00725 author:Wajdi Dhifli, Abdoulaye Baniré Diallo category:cs.LG cs.AI cs.DB cs.IR  published:2015-11-02 summary:Classification is a fundamental task in machine learning and artificial intelligence. Existing classification methods are designed to classify unknown instances within a set of previously known classes that are seen in training. Such classification takes the form of prediction within a closed-set. However, a more realistic scenario that fits the ground truth of real world applications is to consider the possibility of encountering instances that do not belong to any of the classes that are seen in training, $i.e.$, an open-set classification. In such situation, existing closed-set classification methods will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification method for open-set problem. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unseen classes. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets as well as on synthetic datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes. version:2
arxiv-1511-00736 | ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space | http://arxiv.org/abs/1511.00736 | id:1511.00736 author:Wajdi Dhifli, Abdoulaye Baniré Diallo category:cs.LG cs.SI  published:2015-11-02 summary:Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein-graph pairwise similarities. Given a query protein, ProtNN finds the nearest neighbor reference proteins based on a graph representation model and a pairwise similarity between vector embedding of both query and reference protein-graphs in structural and topological spaces. ProtNN assigns to the query protein the function with the highest number of votes across the set of k nearest neighbor reference proteins, where k is a user-defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches. We further show that ProtNN is able to scale up to a whole PDB dataset in a single-process mode with no parallelization, with a gain of thousands order of magnitude of runtime compared to state-of-the-art approaches. version:2
arxiv-1506-06840 | On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants | http://arxiv.org/abs/1506.06840 | id:1506.06840 author:Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, Alex Smola category:cs.LG stat.ML  published:2015-06-23 summary:We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms---a crucial requirement for modern large-scale applications---have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG. version:2
arxiv-1411-5319 | Fashion Apparel Detection: The Role of Deep Convolutional Neural Network and Pose-dependent Priors | http://arxiv.org/abs/1411.5319 | id:1411.5319 author:Kota Hara, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV  published:2014-11-19 summary:In this work, we propose and address a new computer vision task, which we call fashion item detection, where the aim is to detect various fashion items a person in the image is wearing or carrying. The types of fashion items we consider in this work include hat, glasses, bag, pants, shoes and so on. The detection of fashion items can be an important first step of various e-commerce applications for fashion industry. Our method is based on state-of-the-art object detection method pipeline which combines object proposal methods with a Deep Convolutional Neural Network. Since the locations of fashion items are in strong correlation with the locations of body joints positions, we incorporate contextual information from body poses in order to improve the detection performance. Through the experiments, we demonstrate the effectiveness of the proposed method. version:2
arxiv-1505-05668 | Locally Adaptive Dynamic Networks | http://arxiv.org/abs/1505.05668 | id:1505.05668 author:Daniele Durante, David B. Dunson category:stat.AP stat.ML  published:2015-05-21 summary:Our focus is on realistically modeling and forecasting dynamic networks of face-to-face contacts among individuals. Important aspects of such data that lead to problems with current methods include the tendency to move between periods of slow and rapid changes and the dynamic heterogeneity in the connectivity behaviors across nodes. Motivated by this application, we develop a novel methodology for Locally Adaptive DYnamic (LADY) network inference. The proposed model relies on a dynamic latent space representation in which each subjects' position evolves over time via a stochastic differential equation. Using a state space representation for these stochastic processes and P\'olya-gamma data augmentation, we develop an efficient MCMC algorithm for posterior inference along with tractable online updating and prediction procedures for forecasting of future networks. We evaluate performance via simulation studies, and consider an application to face-to-face contacts among students in a primary school. version:2
arxiv-1601-04871 | Eye detection in digital images: challenges and solutions | http://arxiv.org/abs/1601.04871 | id:1601.04871 author:Mitra Montazeri, Mahdieh Montazeri, Saeid Saryazdi category:cs.CV  published:2016-01-19 summary:Eye Detection has an important role in the field of biometric identification and known as one method of person's identification. In recent years, many efforts have been done which can detect eye automatically and with different image conditions. However, each method has its own drawbacks which can control some of these conditions. In this paper, different methods of eye detection will be categorized and explained. In each category, the advantages and disadvantages of each method will be presented. version:2
arxiv-1512-05030 | Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised Learning | http://arxiv.org/abs/1512.05030 | id:1512.05030 author:Manaal Faruqui, Ryan McDonald, Radu Soricut category:cs.CL  published:2015-12-16 summary:Morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language. Such lexicons are not available for all languages and even when available, their coverage can be limited. We present a graph-based semi-supervised learning method that uses the morphological, syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets. Our method is language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. version:3
arxiv-1601-06342 | Fast Binary Embedding via Circulant Downsampled Matrix -- A Data-Independent Approach | http://arxiv.org/abs/1601.06342 | id:1601.06342 author:Sung-Hsien Hsieh, Chun-Shien Lu, Soo-Chang Pei category:cs.IT cs.CV cs.LG math.IT  published:2016-01-24 summary:Binary embedding of high-dimensional data aims to produce low-dimensional binary codes while preserving discriminative power. State-of-the-art methods often suffer from high computation and storage costs. We present a simple and fast embedding scheme by first downsampling N-dimensional data into M-dimensional data and then multiplying the data with an MxM circulant matrix. Our method requires O(N +M log M) computation and O(N) storage costs. We prove if data have sparsity, our scheme can achieve similarity-preserving well. Experiments further demonstrate that though our method is cost-effective and fast, it still achieves comparable performance in image applications. version:1
arxiv-1507-06217 | Persistent Images: A Stable Vector Representation of Persistent Homology | http://arxiv.org/abs/1507.06217 | id:1507.06217 author:Henry Adams, Sofya Chepushtanova, Tegan Emerson, Eric Hanson, Michael Kirby, Francis Motta, Rachel Neville, Chris Peterson, Patrick Shipman, Lori Ziegelmeier category:cs.CG math.AT stat.ML F.2.2; I.5.2  published:2015-07-22 summary:Many data sets can be viewed as a noisy sampling of an underlying topological space. A suite of tools in topological data analysis allows one to exploit this structure for the purpose of knowledge discovery. One such tool is persistent homology which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). The space of PDs can be given a metric structure allowing a given diagram to be used as a statistic for the purpose of comparison against other diagrams. We convert a PD to a persistence image (PI) and prove stability with respect to small perturbations in the inputs. The PI is a vector representation allowing the application of vector-based machine learning tools, such as linear and sparse support vector machines. These tools help to identify discriminatory features which can have a topological interpretation. The PIs and PDs derived from randomly sampled topological spaces are compared by applying the K-medoids clustering algorithm. To further illustrate the PI technique, linear and sparse support vector machines are implemented on this data set and classification is performed on additional data arising from a discrete dynamical system called the linked twist map. version:2
arxiv-1509-08967 | Very Deep Multilingual Convolutional Neural Networks for LVCSR | http://arxiv.org/abs/1509.08967 | id:1509.08967 author:Tom Sercu, Christian Puhrsch, Brian Kingsbury, Yann LeCun category:cs.CL cs.NE  published:2015-09-29 summary:Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far. version:2
arxiv-1601-06274 | Solving Dense Image Matching in Real-Time using Discrete-Continuous Optimization | http://arxiv.org/abs/1601.06274 | id:1601.06274 author:Alexander Shekhovtsov, Christian Reinbacher, Gottfried Graber, Thomas Pock category:cs.CV  published:2016-01-23 summary:Dense image matching is a fundamental low-level problem in Computer Vision, which has received tremendous attention from both discrete and continuous optimization communities. The goal of this paper is to combine the advantages of discrete and continuous optimization in a coherent framework. We devise a model based on energy minimization, to be optimized by both discrete and continuous algorithms in a consistent way. In the discrete setting, we propose a novel optimization algorithm that can be massively parallelized. In the continuous setting we tackle the problem of non-convex regularizers by a formulation based on differences of convex functions. The resulting hybrid discrete-continuous algorithm can be efficiently accelerated by modern GPUs and we demonstrate its real-time performance for the applications of dense stereo matching and optical flow. version:1
arxiv-1512-01322 | Fixed Point Performance Analysis of Recurrent Neural Networks | http://arxiv.org/abs/1512.01322 | id:1512.01322 author:Sungho Shin, Kyuyeon Hwang, Wonyong Sung category:cs.LG cs.NE  published:2015-12-04 summary:Recurrent neural networks have shown excellent performance in many applications, however they require increased complexity in hardware or software based implementations. The hardware complexity can be much lowered by minimizing the word-length of weights and signals. This work analyzes the fixed-point performance of recurrent neural networks using a retrain based quantization method. The quantization sensitivity of each layer in RNNs is studied, and the overall fixed-point optimization results minimizing the capacity of weights while not sacrificing the performance are presented. A language model and a phoneme recognition examples are used. version:2
arxiv-1601-06260 | Person Re-Identification by Discriminative Selection in Video Ranking | http://arxiv.org/abs/1601.06260 | id:1601.06260 author:Taiqing Wang, Shaogang Gong, Xiatian Zhu, Shengjin Wang category:cs.CV  published:2016-01-23 summary:Current person re-identification (ReID) methods typically rely on single-frame imagery features, whilst ignoring space-time information from image sequences often available in the practical surveillance scenarios. Single-frame (single-shot) based visual appearance matching is inherently limited for person ReID in public spaces due to the challenging visual ambiguity and uncertainty arising from non-overlapping camera views where viewing condition changes can cause significant people appearance variations. In this work, we present a novel model to automatically select the most discriminative video fragments from noisy/incomplete image sequences of people from which reliable space-time and appearance features can be computed, whilst simultaneously learning a video ranking function for person ReID. Using the PRID$2011$, iLIDS-VID, and HDA+ image sequence datasets, we extensively conducted comparative evaluations to demonstrate the advantages of the proposed model over contemporary gait recognition, holistic image sequence matching and state-of-the-art single-/multi-shot ReID methods. version:1
arxiv-1601-06259 | Minimax Lower Bounds for Linear Independence Testing | http://arxiv.org/abs/1601.06259 | id:1601.06259 author:Aaditya Ramdas, David Isenberg, Aarti Singh, Larry Wasserman category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH  published:2016-01-23 summary:Linear independence testing is a fundamental information-theoretic and statistical problem that can be posed as follows: given $n$ points $\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distribution where $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^T X$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in \mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n \to \infty$, $(p+q)/n \leq \kappa < \infty$, without sparsity assumptions). In summary, our results imply that $n$ must be at least as large as $\sqrt {pq}/\ \Sigma_{XY}\ _F^2$ for any procedure (test) to have non-trivial power, where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also provide some evidence that the lower bound is tight, by connections to two-sample testing and regression in specific settings. version:1
arxiv-1601-06251 | Using compatible shape descriptor for lexicon reduction of printed Farsi subwords | http://arxiv.org/abs/1601.06251 | id:1601.06251 author:Homa Davoudi, Ehsanollah Kabir category:cs.CV  published:2016-01-23 summary:This Paper presents a method for lexicon reduction of Printed Farsi subwords based on their holistic shape features. Because of the large number of Persian subwords variously shaped from a simple letter to a complex combination of several connected characters, it is not easy to find a fixed shape descriptor suitable for all subwords. In this paper, we propose to select the descriptor according to the input shape characteristics. To do this, a neural network is trained to predict the appropriate descriptor of the input image. This network is implemented in the proposed lexicon reduction system to decide on the descriptor used for comparison of the query image with the lexicon entries. Evaluating the proposed method on a dataset of Persian subwords allows one to attest the effectiveness of the proposed idea of dealing differently with various query shapes. version:1
arxiv-1601-06248 | Automatic recognition of element classes and boundaries in the birdsong with variable sequences | http://arxiv.org/abs/1601.06248 | id:1601.06248 author:Takuya Koumura, Kazuo Okanoya category:q-bio.NC cs.LG cs.SD  published:2016-01-23 summary:Researches on sequential vocalization often require analysis of vocalizations in long continuous sounds. In such studies as developmental ones or studies across generations in which days or months of vocalizations must be analyzed, methods for automatic recognition would be strongly desired. Although methods for automatic speech recognition for application purposes have been intensively studied, blindly applying them for biological purposes may not be an optimal solution. This is because, unlike human speech recognition, analysis of sequential vocalizations often requires accurate extraction of timing information. In the present study we propose automated systems suitable for recognizing birdsong, one of the most intensively investigated sequential vocalizations, focusing on the three properties of the birdsong. First, a song is a sequence of vocal elements, called notes, which can be grouped into categories. Second, temporal structure of birdsong is precisely controlled, meaning that temporal information is important in song analysis. Finally, notes are produced according to certain probabilistic rules, which may facilitate the accurate song recognition. We divided the procedure of song recognition into three sub-steps: local classification, boundary detection, and global sequencing, each of which corresponds to each of the three properties of birdsong. We compared the performances of several different ways to arrange these three steps. As results, we demonstrated a hybrid model of a deep neural network and a hidden Markov model is effective in recognizing birdsong with variable note sequences. We propose suitable arrangements of methods according to whether accurate boundary detection is needed. Also we designed the new measure to jointly evaluate the accuracy of note classification and boundary detection. Our methods should be applicable, with small modification and tuning, to the songs in other species that hold the three properties of the sequential vocalization. version:1
arxiv-1601-06243 | Super-resolution reconstruction of hyperspectral images via low rank tensor modeling and total variation regularization | http://arxiv.org/abs/1601.06243 | id:1601.06243 author:Shiying He, Haiwei Zhou, Yao Wang, Wenfei Cao, Zhi Han category:cs.CV  published:2016-01-23 summary:In this paper, we propose a novel approach to hyperspectral image super-resolution by modeling the global spatial-and-spectral correlation and local smoothness properties over hyperspectral images. Specifically, we utilize the tensor nuclear norm and tensor folded-concave penalty functions to describe the global spatial-and-spectral correlation hidden in hyperspectral images, and 3D total variation (TV) to characterize the local spatial-and-spectral smoothness across all hyperspectral bands. Then, we develop an efficient algorithm for solving the resulting optimization problem by combing the local linear approximation (LLA) strategy and alternative direction method of multipliers (ADMM). Experimental results on one hyperspectral image dataset illustrate the merits of the proposed approach. version:1
arxiv-1502-05689 | Unsupervised Network Pretraining via Encoding Human Design | http://arxiv.org/abs/1502.05689 | id:1502.05689 author:Ming-Yu Liu, Arun Mallya, Oncel C. Tuzel, Xi Chen category:cs.CV  published:2015-02-19 summary:Over the years, computer vision researchers have spent an immense amount of effort on designing image features for the visual object recognition task. We propose to incorporate this valuable experience to guide the task of training deep neural networks. Our idea is to pretrain the network through the task of replicating the process of hand-designed feature extraction. By learning to replicate the process, the neural network integrates previous research knowledge and learns to model visual objects in a way similar to the hand-designed features. In the succeeding finetuning step, it further learns object-specific representations from labeled data and this boosts its classification power. We pretrain two convolutional neural networks where one replicates the process of histogram of oriented gradients feature extraction, and the other replicates the process of region covariance feature extraction. After finetuning, we achieve substantially better performance than the baseline methods. version:2
arxiv-1601-06201 | Universal Collaboration Strategies for Signal Detection: A Sparse Learning Approach | http://arxiv.org/abs/1601.06201 | id:1601.06201 author:Prashant Khanduri, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Pramod K. Varshney category:cs.LG stat.ML  published:2016-01-22 summary:This paper considers the problem of high dimensional signal detection in a large distributed network. In contrast to conventional distributed detection, the nodes in the network can update their observations by combining observations from other one-hop neighboring nodes (spatial collaboration). Under the assumption that only a small subset of nodes are capable of communicating with the Fusion Center (FC), our goal is to design optimal collaboration strategies which maximize the detection performance at the FC. Note that, if one optimizes the system for the detection of a single known signal then the network cannot generalize well to other detection tasks. Hence, we propose to design optimal collaboration strategies which are universal for a class of equally probable deterministic signals. By establishing the equivalence between the collaboration strategy design problem and Sparse PCA, we seek the answers to the following questions: 1) How much do we gain from optimizing the collaboration strategy? 2) What is the effect of dimensionality reduction for different sparsity constraints? 3) How much do we lose in terms of detection performance by adopting a universal system (cost of universality)? version:1
arxiv-1601-06180 | On the Latent Variable Interpretation in Sum-Product Networks | http://arxiv.org/abs/1601.06180 | id:1601.06180 author:Robert Peharz, Robert Gens, Franz Pernkopf, Pedro Domingos category:cs.AI cs.LG 62  published:2016-01-22 summary:One of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent variables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and to efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables corresponding to the LVs' states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in SPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for introducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish the probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on these results, we find a sound derivation of the EM algorithm for SPNs, which was presented mistaken in literature. Furthermore, the Viterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm, when applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets. version:1
arxiv-1601-06105 | Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs | http://arxiv.org/abs/1601.06105 | id:1601.06105 author:Jonathan Root, Venkatesh Saligrama, Jing Qian category:stat.ML cs.LG  published:2016-01-22 summary:We propose a non-parametric anomaly detection algorithm for high dimensional data. We first rank scores derived from nearest neighbor graphs on $n$-point nominal training data. We then train limited complexity models to imitate these scores based on the max-margin learning-to-rank framework. A test-point is declared as an anomaly at $\alpha$-false alarm level if the predicted score is in the $\alpha$-percentile. The resulting anomaly detector is shown to be asymptotically optimal in that for any false alarm rate $\alpha$, its decision region converges to the $\alpha$-percentile minimum volume level set of the unknown underlying density. In addition, we test both the statistical performance and computational efficiency of our algorithm on a number of synthetic and real-data experiments. Our results demonstrate the superiority of our algorithm over existing $K$-NN based anomaly detection algorithms, with significant computational savings. version:1
arxiv-1601-06087 | Unsupervised convolutional neural networks for motion estimation | http://arxiv.org/abs/1601.06087 | id:1601.06087 author:Aria Ahmadi, Ioannis Patras category:cs.CV  published:2016-01-22 summary:Traditional methods for motion estimation estimate the motion field F between a pair of images as the one that minimizes a predesigned cost function. In this paper, we propose a direct method and train a Convolutional Neural Network (CNN) that when, at test time, is given a pair of images as input it produces a dense motion field F at its output layer. In the absence of large datasets with ground truth motion that would allow classical supervised training, we propose to train the network in an unsupervised manner. The proposed cost function that is optimized during training, is based on the classical optical flow constraint. The latter is differentiable with respect to the motion field and, therefore, allows backpropagation of the error to previous layers of the network. Our method is tested on both synthetic and real image sequences and performs similarly to the state-of-the-art methods. version:1
arxiv-1601-06081 | Why Do Urban Legends Go Viral? | http://arxiv.org/abs/1601.06081 | id:1601.06081 author:Marco Guerini, Carlo Strapparava category:cs.CL cs.CY cs.SI  published:2016-01-22 summary:Urban legends are a genre of modern folklore, consisting of stories about rare and exceptional events, just plausible enough to be believed, which tend to propagate inexorably across communities. In our view, while urban legends represent a form of "sticky" deceptive text, they are marked by a tension between the credible and incredible. They should be credible like a news article and incredible like a fairy tale to go viral. In particular we will focus on the idea that urban legends should mimic the details of news (who, where, when) to be credible, while they should be emotional and readable like a fairy tale to be catchy and memorable. Using NLP tools we will provide a quantitative analysis of these prototypical characteristics. We also lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these simple features. version:1
arxiv-1601-06071 | Bitwise Neural Networks | http://arxiv.org/abs/1601.06071 | id:1601.06071 author:Minje Kim, Paris Smaragdis category:cs.LG cs.AI cs.NE  published:2016-01-22 summary:Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings. version:1
arxiv-1601-06068 | Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing | http://arxiv.org/abs/1601.06068 | id:1601.06068 author:Shashi Narayan, Siva Reddy, Shay B. Cohen category:cs.CL  published:2016-01-22 summary:One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries -- there are many ways to ask a question, all with the same answer. In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query. We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser significantly improves over strong baselines. version:1
arxiv-1601-06062 | 3-D/2-D Registration of Cardiac Structures by 3-D Contrast Agent Distribution Estimation | http://arxiv.org/abs/1601.06062 | id:1601.06062 author:Matthias Hoffmann, Christopher Kowalewski, Andreas Maier, Klaus Kurzidim, Norbert Strobel, Joachim Hornegger category:cs.CV  published:2016-01-22 summary:For augmented fluoroscopy during cardiac catheter ablation procedures, a preoperatively acquired 3-D model of the left atrium of the patient can be registered to X-ray images. Therefore the 3D-model is matched with the contrast agent based appearance of the left atrium. Commonly, only small amounts of contrast agent (CA) are used to locate the left atrium. This is why we focus on robust registration methods that work also if the structure of interest is only partially contrasted. In particular, we propose two similarity measures for CA-based registration: The first similarity measure, explicit apparent edges, focuses on edges of the patient anatomy made visible by contrast agent and can be computed quickly on the GPU. The second novel similarity measure computes a contrast agent distribution estimate (CADE) inside the 3-D model and rates its consistency with the CA seen in biplane fluoroscopic images. As the CADE computation involves a reconstruction of CA in 3-D using the CA within the fluoroscopic images, it is slower. Using a combination of both methods, our evaluation on 11 well-contrasted clinical datasets yielded an error of 7.9+/-6.3 mm over all frames. For 10 datasets with little CA, we obtained an error of 8.8+/-6.7 mm. Our new methods outperform a registration based on the projected shadow significantly (p<0.05). version:1
arxiv-1601-06057 | Topological descriptors for 3D surface analysis | http://arxiv.org/abs/1601.06057 | id:1601.06057 author:Matthias Zeppelzauer, Bartosz Zieliński, Mateusz Juda, Markus Seidl category:cs.CV  published:2016-01-22 summary:We investigate topological descriptors for 3D surface analysis, i.e. the classification of surfaces according to their geometric fine structure. On a dataset of high-resolution 3D surface reconstructions we compute persistence diagrams for a 2D cubical filtration. In the next step we investigate different topological descriptors and measure their ability to discriminate structurally different 3D surface patches. We evaluate their sensitivity to different parameters and compare the performance of the resulting topological descriptors to alternative (non-topological) descriptors. We present a comprehensive evaluation that shows that topological descriptors are (i) robust, (ii) yield state-of-the-art performance for the task of 3D surface analysis and (iii) improve classification performance when combined with non-topological descriptors. version:1
arxiv-1601-06044 | Geometric-Algebra LMS Adaptive Filter and its Application to Rotation Estimation | http://arxiv.org/abs/1601.06044 | id:1601.06044 author:Wilder B. Lopes, Anas Al-Nuaimi, Cassio G. Lopes category:cs.CV cs.CG  published:2016-01-22 summary:This paper exploits Geometric (Clifford) Algebra (GA) theory in order to devise and introduce a new adaptive filtering strategy. From a least-squares cost function, the gradient is calculated following results from Geometric Calculus (GC), the extension of GA to handle differential and integral calculus. The novel GA least-mean-squares (GA-LMS) adaptive filter, which inherits properties from standard adaptive filters and from GA, is developed to recursively estimate a rotor (multivector), a hypercomplex quantity able to describe rotations in any dimension. The adaptive filter (AF) performance is assessed via a 3D point-clouds registration problem, which contains a rotation estimation step. Calculating the AF computational complexity suggests that it can contribute to reduce the cost of a full-blown 3D registration algorithm, especially when the number of points to be processed grows. Moreover, the employed GA/GC framework allows for easily applying the resulting filter to estimating rotors in higher dimensions. version:1
arxiv-1511-07386 | Pushing the Boundaries of Boundary Detection using Deep Learning | http://arxiv.org/abs/1511.07386 | id:1511.07386 author:Iasonas Kokkinos category:cs.CV cs.LG  published:2015-11-23 summary:In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 - while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-of-the-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second. version:2
arxiv-1601-06041 | Online Event Recognition from Moving Vessel Trajectories | http://arxiv.org/abs/1601.06041 | id:1601.06041 author:Kostas Patroumpas, Elias Alevizos, Alexander Artikis, Marios Vodas, Nikos Pelekis, Yannis Theodoridis category:cs.CV cs.AI  published:2016-01-22 summary:We present a system for online monitoring of maritime activity over streaming positions from numerous vessels sailing at sea. It employs an online tracking module for detecting important changes in the evolving trajectory of each vessel across time, and thus can incrementally retain concise, yet reliable summaries of its recent movement. In addition, thanks to its complex event recognition module, this system can also offer instant notification to marine authorities regarding emergency situations, such as risk of collisions, suspicious moves in protected zones, or package picking at open sea. Not only did our extensive tests validate the performance, efficiency, and robustness of the system against scalable volumes of real-world and synthetically enlarged datasets, but its deployment against online feeds from vessels has also confirmed its capabilities for effective, real-time maritime surveillance. version:1
arxiv-1601-06035 | Recommender systems inspired by the structure of quantum theory | http://arxiv.org/abs/1601.06035 | id:1601.06035 author:Cyril Stark category:cs.LG cs.IT math.IT math.OC quant-ph stat.ML  published:2016-01-22 summary:Physicists use quantum models to describe the behavior of physical systems. Quantum models owe their success to their interpretability, to their relation to probabilistic models (quantization of classical models) and to their high predictive power. Beyond physics, these properties are valuable in general data science. This motivates the use of quantum models to analyze general nonphysical datasets. Here we provide both empirical and theoretical insights into the application of quantum models in data science. In the theoretical part of this paper, we firstly show that quantum models can be exponentially more efficient than probabilistic models because there exist datasets that admit low-dimensional quantum models and only exponentially high-dimensional probabilistic models. Secondly, we explain in what sense quantum models realize a useful relaxation of compressed probabilistic models. Thirdly, we show that sparse datasets admit low-dimensional quantum models and finally, we introduce a method to compute hierarchical orderings of properties of users (e.g., personality traits) and items (e.g., genres of movies). In the empirical part of the paper, we evaluate quantum models in item recommendation and observe that the predictive power of quantum-inspired recommender systems can compete with state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore, we make use of the interpretability of quantum models by computing hierarchical orderings of properties of users and items. This work establishes a connection between data science (item recommendation), information theory (communication complexity), mathematical programming (positive semidefinite factorizations) and physics (quantum models). version:1
arxiv-1601-06032 | Learning Support Correlation Filters for Visual Tracking | http://arxiv.org/abs/1601.06032 | id:1601.06032 author:Wangmeng Zuo, Xiaohe Wu, Liang Lin, Lei Zhang, Ming-Hsuan Yang category:cs.CV  published:2016-01-22 summary:Sampling and budgeting training examples are two essential factors in tracking algorithms based on support vector machines (SVMs) as a trade-off between accuracy and efficiency. Recently, the circulant matrix formed by dense sampling of translated image patches has been utilized in correlation filters for fast tracking. In this paper, we derive an equivalent formulation of a SVM model with circulant matrix expression and present an efficient alternating optimization method for visual tracking. We incorporate the discrete Fourier transform with the proposed alternating optimization process, and pose the tracking problem as an iterative learning of support correlation filters (SCFs) which find the global optimal solution with real-time performance. For a given circulant data matrix with n^2 samples of size n*n, the computational complexity of the proposed algorithm is O(n^2*logn) whereas that of the standard SVM-based approaches is at least O(n^4). In addition, we extend the SCF-based tracking algorithm with multi-channel features, kernel functions, and scale-adaptive approaches to further improve the tracking performance. Experimental results on a large benchmark dataset show that the proposed SCF-based algorithms perform favorably against the state-of-the-art tracking methods in terms of accuracy and speed. version:1
arxiv-1601-06008 | A Robust Frame-based Nonlinear Prediction System for Automatic Speech Coding | http://arxiv.org/abs/1601.06008 | id:1601.06008 author:Mahmood Yousefi-Azar, Farbod Razzazi category:cs.SD cs.NE  published:2016-01-22 summary:In this paper, we propose a neural-based coding scheme in which an artificial neural network is exploited to automatically compress and decompress speech signals by a trainable approach. Having a two-stage training phase, the system can be fully specified to each speech frame and have robust performance across different speakers and wide range of spoken utterances. Indeed, Frame-based nonlinear predictive coding (FNPC) would code a frame in the procedure of training to predict the frame samples. The motivating objective is to analyze the system behavior in regenerating not only the envelope of spectra, but also the spectra phase. This scheme has been evaluated in time and discrete cosine transform (DCT) domains and the output of predicted phonemes show the potentiality of the FNPC to reconstruct complicated signals. The experiments were conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains versus the number of neurons in the hidden layer. Experiments approve the FNPC capability as an automatic coding system by which /b/d/g/ phonemes have been reproduced with a good accuracy. Evaluations revealed that the performance of FNPC system, trained to predict DCT coefficients is more desirable, particularly for frames with the wider distribution of energy, compared to time samples. version:1
arxiv-1411-0129 | The Latent Structure of Dictionaries | http://arxiv.org/abs/1411.0129 | id:1411.0129 author:Philippe Vincent-Lamarre, Alexandre Blondin Massé, Marcos Lopes, Mélanie Lord, Odile Marcotte, Stevan Harnad category:cs.CL cs.IR  published:2014-11-01 summary:How many words (and which ones) are sufficient to define all other words? When dictionaries are analyzed as directed graphs with links from defining words to defined words, they reveal a latent structure. Recursively removing all words that are reachable by definition but that do not define any further words reduces the dictionary to a Kernel of about 10%. This is still not the smallest number of words that can define all the rest. About 75% of the Kernel turns out to be its Core, a Strongly Connected Subset of words with a definitional path to and from any pair of its words and no word's definition depending on a word outside the set. But the Core cannot define all the rest of the dictionary. The 25% of the Kernel surrounding the Core consists of small strongly connected subsets of words: the Satellites. The size of the smallest set of words that can define all the rest (the graph's Minimum Feedback Vertex Set or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core, half-Satellite. But every dictionary has a huge number of MinSets. The Core words are learned earlier, more frequent, and less concrete than the Satellites, which in turn are learned earlier and more frequent but more concrete than the rest of the Dictionary. In principle, only one MinSet's words would need to be grounded through the sensorimotor capacity to recognize and categorize their referents. In a dual-code sensorimotor-symbolic model of the mental lexicon, the symbolic code could do all the rest via re-combinatory definition. version:2
arxiv-1601-05994 | Depth and Reflection Total Variation for Single Image Dehazing | http://arxiv.org/abs/1601.05994 | id:1601.05994 author:Wei Wang, Chuanjiang He category:cs.CV  published:2016-01-22 summary:Haze removal has been a very challenging problem due to its ill-posedness, which is more ill-posed if the input data is only a single hazy image. In this paper, we present a new approach for removing haze from a single input image. The proposed method combines the model widely used to describe the formation of a haze image with the assumption in Retinex that an image is the product of the illumination and the reflection. We assume that the depth and reflection functions are spatially piecewise smooth in the model, where the total variation is used for the regularization. The proposed model is defined as a constrained optimization problem, which is solved by an alternating minimization scheme and the fast gradient projection algorithm. Some theoretic analyses are given for the proposed model and algorithm. Finally, numerical examples are presented to demonstrate that our method can restore vivid and contrastive hazy images effectively. version:1
arxiv-1601-05936 | Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic Modeling in Speech Recognition | http://arxiv.org/abs/1601.05936 | id:1601.05936 author:Pranay Dighe, Gil Luyet, Afsaneh Asaei, Herve Bourlard category:cs.CL cs.LG stat.ML  published:2016-01-22 summary:We propose to model the acoustic space of deep neural network (DNN) class-conditional posterior probabilities as a union of low-dimensional subspaces. To that end, the training posteriors are used for dictionary learning and sparse coding. Sparse representation of the test posteriors using this dictionary enables projection to the space of training data. Relying on the fact that the intrinsic dimensions of the posterior subspaces are indeed very small and the matrix of all posteriors belonging to a class has a very low rank, we demonstrate how low-dimensional structures enable further enhancement of the posteriors and rectify the spurious errors due to mismatch conditions. The enhanced acoustic modeling method leads to improvements in continuous speech recognition task using hybrid DNN-HMM (hidden Markov model) framework in both clean and noisy conditions, where upto 15.4% relative reduction in word error rate (WER) is achieved. version:1
arxiv-1601-05900 | When is Clustering Perturbation Robust? | http://arxiv.org/abs/1601.05900 | id:1601.05900 author:Margareta Ackerman, Jarrod Moore category:cs.LG cs.CV  published:2016-01-22 summary:Clustering is a fundamental data mining tool that aims to divide data into groups of similar items. Generally, intuition about clustering reflects the ideal case -- exact data sets endowed with flawless dissimilarity between individual instances. In practice however, these cases are in the minority, and clustering applications are typically characterized by noisy data sets with approximate pairwise dissimilarities. As such, the efficacy of clustering methods in practical applications necessitates robustness to perturbations. In this paper, we perform a formal analysis of perturbation robustness, revealing that the extent to which algorithms can exhibit this desirable characteristic is inherently limited, and identifying the types of structures that allow popular clustering paradigms to discover meaningful clusters in spite of faulty data. version:1
arxiv-1601-05893 | GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach | http://arxiv.org/abs/1601.05893 | id:1601.05893 author:Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel category:cs.AI cs.CL cs.DB cs.IR  published:2016-01-22 summary:Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each block. Finally, one location is chosen for each block by assigning distance-based scores to each location and repeatedly selecting the location and block with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles' authors, where classification approaches have achieved median errors as low as 11 km, with attainable accuracy limited by the class size. Our approach achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When considering the five location tags with the greatest scores, 50% of articles were assigned at least one tag within 8.5 kilometres of the article's author-assigned true location. We also tested our approach on Twitter messages that are tagged with the location from which the message was sent. Twitter texts are challenging because they are short and unstructured and often do not contain words referring to the location they were sent from, but we obtain potentially useful results. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper accuracy limit, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall. version:1
arxiv-1601-03317 | Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model | http://arxiv.org/abs/1601.03317 | id:1601.03317 author:Shi Feng, Shujie Liu, Mu Li, Ming Zhou category:cs.CL  published:2016-01-13 summary:Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoder-decoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attention-based encoder-decoder. version:3
arxiv-1601-05861 | Manifold-Kernels Comparison in MKPLS for Visual Speech Recognition | http://arxiv.org/abs/1601.05861 | id:1601.05861 author:Amr Bakry, Ahmed Elgammal category:cs.CV  published:2016-01-22 summary:Speech recognition is a challenging problem. Due to the acoustic limitations, using visual information is essential for improving the recognition accuracy in real-life unconstraint situations. One common approach is to model the visual recognition as nonlinear optimization problem. Measuring the distances between visual units is essential for solving this problem. Embedding the visual units on a manifold and using manifold kernels is one way to measure these distances. This work is intended to evaluate the performance of several manifold kernels for solving the problem of visual speech recognition. We show the theory behind each kernel. We apply manifold kernel partial least squares framework to OuluVs and AvLetters databases, and show empirical comparison between all kernels. This framework provides convenient way to explore different kernels. version:1
arxiv-1506-02699 | Community detection in multi-relational data with restricted multi-layer stochastic blockmodel | http://arxiv.org/abs/1506.02699 | id:1506.02699 author:Subhadeep Paul, Yuguo Chen category:stat.ML  published:2015-06-08 summary:In recent years there has been an increased interest in statistical analysis of data with multiple types of relations among a set of entities. Such multi-relational data can be represented as multi-layer graphs where the set of vertices represents the entities and multiple types of edges represent the different relations among them. For community detection in multi-layer graphs, we consider two random graph models, the multi-layer stochastic blockmodel (MLSBM) and a model with a restricted parameter space, the restricted multi-layer stochastic blockmodel (RMLSBM). We derive consistency results for community assignments of the maximum likelihood estimators (MLEs) in both models where MLSBM is assumed to be the true model, and either the number of nodes or the number of types of edges or both grow. We compare MLEs in the two models with other baseline approaches, such as separate modeling of layers, aggregating the layers and majority voting. RMLSBM is shown to have advantage over MLSBM when either the growth rate of the number of communities is high or the growth rate of the average degree of the component graphs in the multi-graph is low. We also derive minimax rates of error and sharp thresholds for achieving consistency of community detection in both models, which are then used to compare the multi-layer models with a baseline model, the aggregate stochastic block model. The simulation studies and real data applications confirm the superior performance of the multi-layer approaches in comparison to the baseline procedures. version:2
arxiv-1507-03292 | Cluster-Aided Mobility Predictions | http://arxiv.org/abs/1507.03292 | id:1507.03292 author:Jaeseong Jeong, Mathieu Leconte, Alexandre Proutiere category:cs.LG  published:2015-07-12 summary:Predicting the future location of users in wireless net- works has numerous applications, and can help service providers to improve the quality of service perceived by their clients. The location predictors proposed so far estimate the next location of a specific user by inspecting the past individual trajectories of this user. As a consequence, when the training data collected for a given user is limited, the resulting prediction is inaccurate. In this paper, we develop cluster-aided predictors that exploit past trajectories collected from all users to predict the next location of a given user. These predictors rely on clustering techniques and extract from the training data similarities among the mobility patterns of the various users to improve the prediction accuracy. Specifically, we present CAMP (Cluster-Aided Mobility Predictor), a cluster-aided predictor whose design is based on recent non-parametric bayesian statistical tools. CAMP is robust and adaptive in the sense that it exploits similarities in users' mobility only if such similarities are really present in the training data. We analytically prove the consistency of the predictions provided by CAMP, and investigate its performance using two large-scale datasets. CAMP significantly outperforms existing predictors, and in particular those that only exploit individual past trajectories. version:4
arxiv-1601-05775 | Local community detection by seed expansion: from conductance to weighted kernel 1-mean optimization | http://arxiv.org/abs/1601.05775 | id:1601.05775 author:Twan van Laarhoven, Elena Marchiori category:cs.SI cs.LG stat.ML  published:2016-01-21 summary:In local community detection by seed expansion a single cluster concentrated around few given query nodes in a graph is discovered in a localized way. Conductance is a popular objective function used in many algorithms for local community detection. Algorithms that directly optimize conductance usually add or remove one node at a time to find a local optimum. This amounts to fix a specific neighborhood structure over clusters. A natural way to avoid the problem of choosing a specific neighborhood structure is to use a continuous relaxation of conductance. This paper studies such a continuous relaxation of conductance. We show that in this setting continuous optimization leads to hard clusters. We investigate the relation of conductance with weighted kernel k-means for a single cluster, which leads to the introduction of a weighted kernel 1-mean objective function, called \sigma-conductance, where {\sigma} is a parameter which influences the size of the community. Conductance is obtained by setting {\sigma} to 0. Two algorithms for local optimization of \sigma-conductance based on the expectation maximization and the projected gradient descend method are developed, called EMc and PGDc, respectively. We show that for \sigma=0 EMc corresponds to gradient descend with an infinite step size at each iteration. We design a procedure to automatically select a value for {\sigma}. Performance guarantee for these algorithms is proven for a class of dense communities centered around the seeds and well separated from the rest of the network. On this class we also prove that our algorithms stay localized. A comparative experimental analysis on networks with ground-truth communities is performed using state-of-the-art algorithms based on the graph diffusion method. Our experiments indicate that EMc and PGDc stay localized and produce communities most similar to the ground. version:1
arxiv-1303-5960 | SYNTAGMA. A Linguistic Approach to Parsing | http://arxiv.org/abs/1303.5960 | id:1303.5960 author:Daniel Christen category:cs.CL  published:2013-03-24 summary:SYNTAGMA is a rule-based parsing system, structured on two levels: a general parsing engine and a language specific grammar. The parsing engine is a language independent program, while grammar and language specific rules and resources are given as text files, consisting in a list of constituent structuresand a lexical database with word sense related features and constraints. Since its theoretical background is principally Tesniere's Elements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument structure (valency) in constraint satisfaction, and allows also horizontal bounds, for instance treating coordination. Notions such as Pro, traces, empty categories are derived from Generative Grammar and some solutions are close to Government&Binding Theory, although they are the result of an autonomous research. These properties allow SYNTAGMA to manage complex syntactic configurations and well known weak points in parsing engineering. An important resource is the semantic network, which is used in disambiguation tasks. Parsing process follows a bottom-up, rule driven strategy. Its behavior can be controlled and fine-tuned. version:3
arxiv-1601-05768 | Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA | http://arxiv.org/abs/1601.05768 | id:1601.05768 author:Daniel Christen category:cs.CL  published:2016-01-21 summary:This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky issues of syntactic ambiguity reduction and word sense disambiguation as well as providing innovative and original solutions for constituent generation and constraints management. To provide an insight into how it operates, the system's general architecture and components, as well as its lexical, syntactic and semantic resources are described. After that, the paper addresses the mechanism that performs selective parsing through an interaction between syntactic and semantic information, leading the parser to a coherent and accurate interpretation of the input text. version:1
arxiv-1601-05767 | Spatial Scaling of Satellite Soil Moisture using Temporal Correlations and Ensemble Learning | http://arxiv.org/abs/1601.05767 | id:1601.05767 author:Subit Chakrabarti, Jasmeet Judge, Tara Bongiovanni, Anand Rangarajan, Sanjay Ranka category:cs.CV  published:2016-01-21 summary:A novel algorithm is developed to downscale soil moisture (SM), obtained at satellite scales of 10-40 km by utilizing its temporal correlations to historical auxiliary data at finer scales. Including such correlations drastically reduces the size of the training set needed, accounts for time-lagged relationships, and enables downscaling even in the presence of short gaps in the auxiliary data. The algorithm is based upon bagged regression trees (BRT) and uses correlations between high-resolution remote sensing products and SM observations. The algorithm trains multiple regression trees and automatically chooses the trees that generate the best downscaled estimates. The algorithm was evaluated using a multi-scale synthetic dataset in north central Florida for two years, including two growing seasons of corn and one growing season of cotton per year. The time-averaged error across the region was found to be 0.01 $\mathrm{m}^3/\mathrm{m}^3$, with a standard deviation of 0.012 $\mathrm{m}^3/\mathrm{m}^3$ when 0.02% of the data were used for training in addition to temporal correlations from the past seven days, and all available data from the past year. The maximum spatially averaged errors obtained using this algorithm in downscaled SM were 0.005 $\mathrm{m}^3/\mathrm{m}^3$, for pixels with cotton land-cover. When land surface temperature~(LST) on the day of downscaling was not included in the algorithm to simulate "data gaps", the spatially averaged error increased minimally by 0.015 $\mathrm{m}^3/\mathrm{m}^3$ when LST is unavailable on the day of downscaling. The results indicate that the BRT-based algorithm provides high accuracy for downscaling SM using complex non-linear spatio-temporal correlations, under heterogeneous micro meteorological conditions. version:1
arxiv-1601-05764 | A Confidence-Based Approach for Balancing Fairness and Accuracy | http://arxiv.org/abs/1601.05764 | id:1601.05764 author:Benjamin Fish, Jeremy Kun, Ádám D. Lelkes category:cs.LG cs.CY  published:2016-01-21 summary:We study three classical machine learning algorithms in the context of algorithmic fairness: adaptive boosting, support vector machines, and logistic regression. Our goal is to maintain the high accuracy of these learning algorithms while reducing the degree to which they discriminate against individuals because of their membership in a protected group. Our first contribution is a method for achieving fairness by shifting the decision boundary for the protected group. The method is based on the theory of margins for boosting. Our method performs comparably to or outperforms previous algorithms in the fairness literature in terms of accuracy and low discrimination, while simultaneously allowing for a fast and transparent quantification of the trade-off between bias and error. Our second contribution addresses the shortcomings of the bias-error trade-off studied in most of the algorithmic fairness literature. We demonstrate that even hopelessly naive modifications of a biased algorithm, which cannot be reasonably said to be fair, can still achieve low bias and high accuracy. To help to distinguish between these naive algorithms and more sensible algorithms we propose a new measure of fairness, called resilience to random bias (RRB). We demonstrate that RRB distinguishes well between our naive and sensible fairness algorithms. RRB together with bias and accuracy provides a more complete picture of the fairness of an algorithm. version:1
arxiv-1506-02438 | High-Dimensional Continuous Control Using Generalized Advantage Estimation | http://arxiv.org/abs/1506.02438 | id:1506.02438 author:John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel category:cs.LG cs.RO cs.SY  published:2015-06-08 summary:Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time. version:4
arxiv-1601-04674 | A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure | http://arxiv.org/abs/1601.04674 | id:1601.04674 author:Peter Schulam, Suchi Saria category:stat.ML  published:2016-01-18 summary:For many complex diseases, there is a wide variety of ways in which an individual can manifest the disease. The challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual's disease, which can in turn enable clinicians to optimize treatments. We represent an individual's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time. We propose a hierarchical latent variable model that individualizes predictions of disease trajectories. This model shares statistical strength across observations at different resolutions--the population, subpopulation and the individual level. We describe an algorithm for learning population and subpopulation parameters offline, and an online procedure for dynamically learning individual-specific parameters. Finally, we validate our model on the task of predicting the course of interstitial lung disease, a leading cause of death among patients with the autoimmune disease scleroderma. We compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy. version:2
arxiv-1506-00438 | Network Topology Identification using PCA and its Graph Theoretic Interpretations | http://arxiv.org/abs/1506.00438 | id:1506.00438 author:Aravind Rajeswaran, Shankar Narasimhan category:cs.LG cs.DM cs.SY stat.ME  published:2015-06-01 summary:We solve the problem of identifying (reconstructing) network topology from steady state network measurements. Concretely, given only a data matrix $\mathbf{X}$ where the $X_{ij}$ entry corresponds to flow in edge $i$ in configuration (steady-state) $j$, we wish to find a network structure for which flow conservation is obeyed at all the nodes. This models many network problems involving conserved quantities like water, power, and metabolic networks. We show that identification is equivalent to learning a model $\mathbf{A_n}$ which captures the approximate linear relationships between the different variables comprising $\mathbf{X}$ (i.e. of the form $\mathbf{A_n X \approx 0}$) such that $\mathbf{A_n}$ is full rank (highest possible) and consistent with a network node-edge incidence structure. The problem is solved through a sequence of steps like estimating approximate linear relationships using Principal Component Analysis, obtaining f-cut-sets from these approximate relationships, and graph realization from f-cut-sets (or equivalently f-circuits). Each step and the overall process is polynomial time. The method is illustrated by identifying topology of a water distribution network. We also study the extent of identifiability from steady-state data. version:2
arxiv-1406-7728 | Sparse Recovery via Differential Inclusions | http://arxiv.org/abs/1406.7728 | id:1406.7728 author:Stanley Osher, Feng Ruan, Jiechao Xiong, Yuan Yao, Wotao Yin category:math.ST stat.ML stat.TH  published:2014-06-30 summary:In this paper, we recover sparse signals from their noisy linear measurements by solving nonlinear differential inclusions, which is based on the notion of inverse scale space (ISS) developed in applied mathematics. Our goal here is to bring this idea to address a challenging problem in statistics, \emph{i.e.} finding the oracle estimator which is unbiased and sign-consistent using dynamics. We call our dynamics \emph{Bregman ISS} and \emph{Linearized Bregman ISS}. A well-known shortcoming of LASSO and any convex regularization approaches lies in the bias of estimators. However, we show that under proper conditions, there exists a bias-free and sign-consistent point on the solution paths of such dynamics, which corresponds to a signal that is the unbiased estimate of the true signal and whose entries have the same signs as those of the true signs, \emph{i.e.} the oracle estimator. Therefore, their solution paths are regularization paths better than the LASSO regularization path, since the points on the latter path are biased when sign-consistency is reached. We also show how to efficiently compute their solution paths in both continuous and discretized settings: the full solution paths can be exactly computed piece by piece, and a discretization leads to \emph{Linearized Bregman iteration}, which is a simple iterative thresholding rule and easy to parallelize. Theoretical guarantees such as sign-consistency and minimax optimal $l_2$-error bounds are established in both continuous and discrete settings for specific points on the paths. Early-stopping rules for identifying these points are given. The key treatment relies on the development of differential inequalities for differential inclusions and their discretizations, which extends the previous results and leads to exponentially fast recovering of sparse signals before selecting wrong ones. version:5
arxiv-1601-05675 | Incremental Spectral Sparsification for Large-Scale Graph-Based Semi-Supervised Learning | http://arxiv.org/abs/1601.05675 | id:1601.05675 author:Daniele Calandriello, Alessandro Lazaric, Michal Valko, Ioannis Koutis category:stat.ML cs.LG  published:2016-01-21 summary:While the harmonic function solution performs well in many semi-supervised learning (SSL) tasks, it is known to scale poorly with the number of samples. Recent successful and scalable methods, such as the eigenfunction method focus on efficiently approximating the whole spectrum of the graph Laplacian constructed from the data. This is in contrast to various subsampling and quantization methods proposed in the past, which may fail in preserving the graph spectra. However, the impact of the approximation of the spectrum on the final generalization error is either unknown, or requires strong assumptions on the data. In this paper, we introduce Sparse-HFS, an efficient edge-sparsification algorithm for SSL. By constructing an edge-sparse and spectrally similar graph, we are able to leverage the approximation guarantees of spectral sparsification methods to bound the generalization error of Sparse-HFS. As a result, we obtain a theoretically-grounded approximation scheme for graph-based SSL that also empirically matches the performance of known large-scale methods. version:1
arxiv-1511-02025 | Finding structure in data using multivariate tree boosting | http://arxiv.org/abs/1511.02025 | id:1511.02025 author:Patrick J. Miller, Gitta H. Lubke, Daniel B. McArtor, C. S. Bergeman category:stat.ML cs.LG  published:2015-11-06 summary:Technology and collaboration enable dramatic increases in the size of psychological and psychiatric data collections, but finding structure in these large data sets with many collected variables is challenging. Decision tree ensembles like random forests (Strobl, Malley, and Tutz, 2009) are a useful tool for finding structure, but are difficult to interpret with multiple outcome variables which are often of interest in psychology. To find and interpret structure in data sets with multiple outcomes and many predictors (possibly exceeding the sample size), we introduce a multivariate extension to a decision tree ensemble method called Gradient Boosted Regression Trees (Friedman, 2001). Our method, multivariate tree boosting, can be used for identifying important predictors, detecting predictors with non-linear effects and interactions without specification of such effects, and for identifying predictors that cause two or more outcome variables to covary without parametric assumptions. We provide the R package 'mvtboost' to estimate, tune, and interpret the resulting model, which extends the implementation of univariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous, multivariate outcomes. To illustrate the approach, we analyze predictors of psychological well-being (Ryff and Keyes, 1995). Simulations verify that our approach identifies predictors with non-linear effects and achieves high prediction accuracy, exceeding or matching the performance of (penalized) multivariate multiple regression and multivariate decision trees over a wide range of conditions. version:2
arxiv-1601-05654 | Model-Coupled Autoencoder for Time Series Visualisation | http://arxiv.org/abs/1601.05654 | id:1601.05654 author:Nikolaos Gianniotis, Sven D. Kügler, Peter Tiňo, Kai L. Polsterer category:astro-ph.IM cs.NE  published:2016-01-21 summary:We present an approach for the visualisation of a set of time series that combines an echo state network with an autoencoder. For each time series in the dataset we train an echo state network, using a common and fixed reservoir of hidden neurons, and use the optimised readout weights as the new representation. Dimensionality reduction is then performed via an autoencoder on the readout weight representations. The crux of the work is to equip the autoencoder with a loss function that correctly interprets the reconstructed readout weights by associating them with a reconstruction error measured in the data space of sequences. This essentially amounts to measuring the predictive performance that the reconstructed readout weights exhibit on their corresponding sequences when plugged back into the echo state network with the same fixed reservoir. We demonstrate that the proposed visualisation framework can deal both with real valued sequences as well as binary sequences. We derive magnification factors in order to analyse distance preservations and distortions in the visualisation space. The versatility and advantages of the proposed method are demonstrated on datasets of time series that originate from diverse domains. version:1
arxiv-1601-05644 | B-spline Shape from Motion & Shading: An Automatic Free-form Surface Modeling for Face Reconstruction | http://arxiv.org/abs/1601.05644 | id:1601.05644 author:Weilong Peng, Zhiyong Feng, Chao Xu category:cs.CV cs.GR  published:2016-01-21 summary:Recently, many methods have been proposed for face reconstruction from multiple images, most of which involve fundamental principles of Shape from Shading and Structure from motion. However, a majority of the methods just generate discrete surface model of face. In this paper, B-spline Shape from Motion and Shading (BsSfMS) is proposed to reconstruct continuous B-spline surface for multi-view face images, according to an assumption that shading and motion information in the images contain 1st- and 0th-order derivative of B-spline face respectively. Face surface is expressed as a B-spline surface that can be reconstructed by optimizing B-spline control points. Therefore, normals and 3D feature points computed from shading and motion of images respectively are used as the 1st- and 0th- order derivative information, to be jointly applied in optimizing the B-spline face. Additionally, an IMLS (iterative multi-least-square) algorithm is proposed to handle the difficult control point optimization. Furthermore, synthetic samples and LFW dataset are introduced and conducted to verify the proposed approach, and the experimental results demonstrate the effectiveness with different poses, illuminations, expressions etc., even with wild images. version:1
arxiv-1601-05610 | Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs | http://arxiv.org/abs/1601.05610 | id:1601.05610 author:Hui Li, Chunhua Shen category:cs.CV  published:2016-01-21 summary:In this work, we tackle the problem of car license plate detection and recognition in natural scene images. Inspired by the success of deep neural networks (DNNs) in various vision applications, here we leverage DNNs to learn high-level features in a cascade framework, which lead to improved performance on both detection and recognition. Firstly, we train a $37$-class convolutional neural network (CNN) to detect all characters in an image, which results in a high recall, compared with conventional approaches such as training a binary text/non-text classifier. False positives are then eliminated by the second plate/non-plate CNN classifier. Bounding box refinement is then carried out based on the edge information of the license plates, in order to improve the intersection-over-union (IoU) ratio. The proposed cascade framework extracts license plates effectively with both high recall and precision. Last, we propose to recognize the license characters as a {sequence labelling} problem. A recurrent neural network (RNN) with long short-term memory (LSTM) is trained to recognize the sequential features extracted from the whole license plate via CNNs. The main advantage of this approach is that it is segmentation free. By exploring context information and avoiding errors caused by segmentation, the RNN method performs better than a baseline method of combining segmentation and deep CNN classification; and achieves state-of-the-art recognition accuracy. version:1
arxiv-1601-05593 | Automatic 3D modelling of craniofacial form | http://arxiv.org/abs/1601.05593 | id:1601.05593 author:Nick Pears, Christian Duncan category:cs.CV I.4.8  published:2016-01-21 summary:Three-dimensional models of craniofacial variation over the general population are useful for assessing pre- and post-operative head shape when treating various craniofacial conditions, such as craniosynostosis. We present a new method of automatically building both sagittal profile models and full 3D surface models of the human head using a range of techniques in 3D surface image analysis; in particular, automatic facial landmarking using supervised machine learning, global and local symmetry plane detection using a variant of trimmed iterative closest points, locally-affine template warping (for full 3D models) and a novel pose normalisation using robust iterative ellipse fitting. The PCA-based models built using the new pose normalisation are more compact than those using Generalised Procrustes Analysis and we demonstrate their utility in a clinical case study. version:1
arxiv-1512-00228 | MOCICE-BCubed F$_1$: A New Evaluation Measure for Biclustering Algorithms | http://arxiv.org/abs/1512.00228 | id:1512.00228 author:Henry Rosales-Méndez, Yunior Ramírez-Cruz category:cs.LG cs.IR I.5.3; H.3.3  published:2015-12-01 summary:The validation of biclustering algorithms remains a challenging task, even though a number of measures have been proposed for evaluating the quality of these algorithms. Although no criterion is universally accepted as the overall best, a number of meta-evaluation conditions to be satisfied by biclustering algorithms have been enunciated. In this work, we present MOCICE-BCubed F$_1$, a new external measure for evaluating biclusterings, in the scenario where gold standard annotations are available for both the object clusters and the associated feature subspaces. Our proposal relies on the so-called micro-objects transformation and satisfies the most comprehensive set of meta-evaluation conditions so far enunciated for biclusterings. Additionally, the proposed measure adequately handles the occurrence of overlapping in both the object and feature spaces. Moreover, when used for evaluating traditional clusterings, which are viewed as a particular case of biclustering, the proposed measure also satisfies the most comprehensive set of meta-evaluation conditions so far enunciated for this task. version:2
arxiv-1601-05535 | On the Diagnostic of Road Pathway Visibility | http://arxiv.org/abs/1601.05535 | id:1601.05535 author:Pierre Charbonnier, Jean-Philippe Tarel, Francois Goulette category:cs.CV  published:2016-01-21 summary:Visibility distance on the road pathway plays a significant role in road safety and in particular, has a clear impact on the choice of speed limits. Visibility distance is thus of importance for road engineers and authorities. While visibility distance criteria are routinely taken into account in road design, only a few systems exist for estimating it on existing road networks. Most existing systems comprise a target vehicle followed at a constant distance by an observer vehicle, which only allows to check if a given, fixed visibility distance is available. We propose two new approaches that allow estimating the maximum available visibility distance, involving only one vehicle and based on different sensor technologies, namely binocular stereovision and 3D range sensing (LIDAR). The first approach is based on the processing of two views taken by digital cameras onboard the diagnostic vehicle. The main stages of the process are: road segmentation, edge registration between the two views, road profile 3D reconstruction and finally, maximal road visibility distance estimation. The second approach involves the use of a Terrestrial LIDAR Mobile Mapping System. The triangulated 3D model of the road and its surroundings provided by the system is used to simulate targets at different distances, which allows estimating the maximum geometric visibility distance along the pathway. These approaches were developed in the context of the SARI-VIZIR PREDIT project. Both approaches are described, evaluated and compared. Their pros and cons with respect to vehicle following systems are also discussed. version:1
arxiv-1601-01006 | Space-Time Representation of People Based on 3D Skeletal Data: A Review | http://arxiv.org/abs/1601.01006 | id:1601.01006 author:Fei Han, Brian Reily, William Hoff, Hao Zhang category:cs.CV  published:2016-01-05 summary:Spatiotemporal human representation based on 3D visual perception data is a rapidly growing research area. Based on the information sources, these representations can be broadly categorized into two groups based on RGB-D information or 3D skeleton data. Recently, skeleton-based human representations have been intensively studied and kept attracting an increasing attention, due to their robustness to variations of viewpoint, human body scale and motion speed as well as the realtime, online performance. This paper presents a comprehensive survey of existing space-time representations of people based on 3D skeletal data, and provides an informative categorization and analysis of these methods from the perspectives, including information modality, representation encoding, structure and transition, and feature engineering. We also provide a brief overview of skeleton acquisition devices and construction methods, enlist a number of public benchmark datasets with skeleton data, and discuss potential future research directions. version:2
arxiv-1601-05511 | RGB-D-based Action Recognition Datasets: A Survey | http://arxiv.org/abs/1601.05511 | id:1601.05511 author:Jing Zhang, Wanqing Li, Philip O. Ogunbona, Pichao Wang, Chang Tang category:cs.CV  published:2016-01-21 summary:Human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. This raises the question of which dataset to select and how to use it in providing a fair and objective comparative evaluation against state-of-the-art methods. To address this issue, this paper provides a comprehensive review of the most commonly used action recognition related RGB-D video datasets, including 27 single-view datasets, 10 multi-view datasets, and 7 multi-person datasets. The detailed information and analysis of these datasets is a useful resource in guiding insightful selection of datasets for future research. In addition, the issues with current algorithm evaluation vis-\'{a}-vis limitations of the available datasets and evaluation protocols are also highlighted; resulting in a number of recommendations for collection of new datasets and use of evaluation protocols. version:1
arxiv-1601-04586 | Sparse Convex Clustering | http://arxiv.org/abs/1601.04586 | id:1601.04586 author:Binhuan Wang, Yilong Zhang, Wei Sun, Yixin Fang category:stat.ME cs.LG stat.ML  published:2016-01-18 summary:Convex clustering, a convex relaxation of k-means clustering and hierarchical clustering, has drawn recent attentions since it nicely addresses the instability issue of traditional nonconvex clustering methods. Although its computational and statistical properties have been recently studied, the performance of convex clustering has not yet been investigated in the high-dimensional clustering scenario, where the data contains a large number of features and many of them carry no information about the clustering structure. In this paper, we demonstrate that the performance of convex clustering could be distorted when the uninformative features are included in the clustering. To overcome it, we introduce a new clustering method, referred to as Sparse Convex Clustering, to simultaneously cluster observations and conduct feature selection. The key idea is to formulate convex clustering in a form of regularization, with an adaptive group-lasso penalty term on cluster centers. In order to optimally balance the tradeoff between the cluster fitting and sparsity, a tuning criterion based on clustering stability is developed. In theory, we provide an unbiased estimator for the degrees of freedom of the proposed sparse convex clustering method. Finally, the effectiveness of the sparse convex clustering is examined through a variety of numerical experiments and a real data application. version:2
arxiv-1512-00965 | Neural Enquirer: Learning to Query Tables with Natural Language | http://arxiv.org/abs/1512.00965 | id:1512.00965 author:Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao category:cs.AI cs.CL cs.LG cs.NE  published:2015-12-03 summary:We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully "neuralized": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures. version:2
arxiv-1601-05495 | Data-driven Rank Breaking for Efficient Rank Aggregation | http://arxiv.org/abs/1601.05495 | id:1601.05495 author:Ashish Khetan, Sewoong Oh category:cs.LG stat.ML  published:2016-01-21 summary:Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and unbiased estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph. version:1
arxiv-1504-00298 | Bayesian model comparison with un-normalised likelihoods | http://arxiv.org/abs/1504.00298 | id:1504.00298 author:Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina Evdemon-Hogan category:stat.CO physics.data-an stat.ME stat.ML  published:2015-04-01 summary:Models for which the likelihood function can be evaluated only up to a parameter-dependent unknown normalising constant, such as Markov random field models, are used widely in computer science, statistical physics, spatial statistics, and network analysis. However, Bayesian analysis of these models using standard Monte Carlo methods is not possible due to the intractability of their likelihood functions. Several methods that permit exact, or close to exact, simulation from the posterior distribution have recently been developed. However, estimating the evidence and Bayes' factors (BFs) for these models remains challenging in general. This paper describes new random weight importance sampling and sequential Monte Carlo methods for estimating BFs that use simulation to circumvent the evaluation of the intractable likelihood, and compares them to existing methods. In some cases we observe an advantage in the use of biased weight estimates. An initial investigation into the theoretical and empirical properties of this class of methods is presented. Some support for the use of biased estimates is presented, but we advocate caution in the use of such estimates. version:3
arxiv-1601-05472 | Hierarchical Latent Word Clustering | http://arxiv.org/abs/1601.05472 | id:1601.05472 author:Halid Ziya Yerebakan, Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa category:cs.CL  published:2016-01-20 summary:This paper presents a new Bayesian non-parametric model by extending the usage of Hierarchical Dirichlet Allocation to extract tree structured word clusters from text data. The inference algorithm of the model collects words in a cluster if they share similar distribution over documents. In our experiments, we observed meaningful hierarchical structures on NIPS corpus and radiology reports collected from public repositories. version:1
arxiv-1505-06378 | Monotonic Calibrated Interpolated Look-Up Tables | http://arxiv.org/abs/1505.06378 | id:1505.06378 author:Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojtek Moczydlowski, Alex van Esbroeck category:cs.LG  published:2015-05-23 summary:Real-world machine learning applications may require functions that are fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function can be critical to user trust. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic look-up tables by solving a convex problem with appropriate linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and propose random sampling of additive regularizer terms. Case studies with real-world problems with five to sixteen features and thousands to millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy on practical problems while providing greater transparency to users. version:3
arxiv-1503-00516 | Matrix Product State for Feature Extraction of Higher-Order Tensors | http://arxiv.org/abs/1503.00516 | id:1503.00516 author:Johann A. Bengua, Ho N. Phien, Hoang D. Tuan, Minh N. Do category:cs.CV cs.DS cs.LG  published:2015-03-02 summary:This paper introduces matrix product state (MPS) decomposition as a computational tool for extracting features of multidimensional data represented by higher-order tensors. Regardless of tensor order, MPS extracts its relevant features to the so-called core tensor of maximum order three which can be used for classification. Mainly based on a successive sequence of singular value decompositions (SVD), MPS is quite simple to implement without any recursive procedure needed for optimizing local tensors. Thus, it leads to substantial computational savings compared to other tensor feature extraction methods such as higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition (TD). Benchmark results show that MPS can reduce significantly the feature space of data while achieving better classification performance compared to HOOI. version:4
arxiv-1601-05447 | Detecting Temporally Consistent Objects in Videos through Object Class Label Propagation | http://arxiv.org/abs/1601.05447 | id:1601.05447 author:Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen category:cs.CV  published:2016-01-20 summary:Object proposals for detecting moving or static video objects need to address issues such as speed, memory complexity and temporal consistency. We propose an efficient Video Object Proposal (VOP) generation method and show its efficacy in learning a better video object detector. A deep-learning based video object detector learned using the proposed VOP achieves state-of-the-art detection performance on the Youtube-Objects dataset. We further propose a clustering of VOPs which can efficiently be used for detecting objects in video in a streaming fashion. As opposed to applying per-frame convolutional neural network (CNN) based object detection, our proposed method called Objects in Video Enabler thRough LAbel Propagation (OVERLAP) needs to classify only a small fraction of all candidate proposals in every video frame through streaming clustering of object proposals and class-label propagation. Source code will be made available soon. version:1
arxiv-1601-06440 | QUOTE: "Querying" Users as Oracles in Tag Engines - A Semi-Supervised Learning Approach to Personalized Image Tagging | http://arxiv.org/abs/1601.06440 | id:1601.06440 author:Amandianeze O. Nwana, Tsuhan Chen category:cs.IR cs.LG cs.MM cs.SI  published:2016-01-20 summary:One common trend in image tagging research is to focus on visually relevant tags, and this tends to ignore the personal and social aspect of tags, especially on photoblogging websites such as Flickr. Previous work has correctly identified that many of the tags that users provide on images are not visually relevant (i.e. representative of the salient content in the image) and they go on to treat such tags as noise, ignoring that the users chose to provide those tags over others that could have been more visually relevant. Another common assumption about user generated tags for images is that the order of these tags provides no useful information for the prediction of tags on future images. This assumption also tends to define usefulness in terms of what is visually relevant to the image. For general tagging or labeling applications that focus on providing visual information about image content, these assumptions are reasonable, but when considering personalized image tagging applications, these assumptions are at best too rigid, ignoring user choice and preferences. We challenge the aforementioned assumptions, and provide a machine learning approach to the problem of personalized image tagging with the following contributions: 1.) We reformulate the personalized image tagging problem as a search/retrieval ranking problem, 2.) We leverage the order of tags, which does not always reflect visual relevance, provided by the user in the past as a cue to their tag preferences, similar to click data, 3.) We propose a technique to augment sparse user tag data (semi-supervision), and 4.) We demonstrate the efficacy of our method on a subset of Flickr images, showing improvement over previous state-of-art methods. version:1
arxiv-1601-05409 | Selecting Efficient Features via a Hyper-Heuristic Approach | http://arxiv.org/abs/1601.05409 | id:1601.05409 author:Mitra Montazeri, Mahdieh Soleymani Baghshah, Aliakbar Niknafs category:cs.CV cs.NE  published:2016-01-20 summary:By Emerging huge databases and the need to efficient learning algorithms on these datasets, new problems have appeared and some methods have been proposed to solve these problems by selecting efficient features. Feature selection is a problem of finding efficient features among all features in which the final feature set can improve accuracy and reduce complexity. One way to solve this problem is to evaluate all possible feature subsets. However, evaluating all possible feature subsets is an exhaustive search and thus it has high computational complexity. Until now many heuristic algorithms have been studied for solving this problem. Hyper-heuristic is a new heuristic approach which can search the solution space effectively by applying local searches appropriately. Each local search is a neighborhood searching algorithm. Since each region of the solution space can have its own characteristics, it should be chosen an appropriate local search and apply it to current solution. This task is tackled to a supervisor. The supervisor chooses a local search based on the functional history of local searches. By doing this task, it can trade of between exploitation and exploration. Since the existing heuristic cannot trade of between exploration and exploitation appropriately, the solution space has not been searched appropriately in these methods and thus they have low convergence rate. For the first time, in this paper use a hyper-heuristic approach to find an efficient feature subset. In the proposed method, genetic algorithm is used as a supervisor and 16 heuristic algorithms are used as local searches. Empirical study of the proposed method on several commonly used data sets from UCI data sets indicates that it outperforms recent existing methods in the literature for feature selection. version:1
arxiv-1601-05403 | Semantic Word Clusters Using Signed Normalized Graph Cuts | http://arxiv.org/abs/1601.05403 | id:1601.05403 author:João Sedoc, Jean Gallier, Lyle Ungar, Dean Foster category:cs.CL cs.AI  published:2016-01-20 summary:Vector space representations of words capture many aspects of word similarity, but such methods tend to make vector spaces in which antonyms (as well as synonyms) are close to each other. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words which simultaneously capture distributional and synonym relations. We evaluate these clusters against the SimLex-999 dataset (Hill et al.,2014) of human judgments of word pair similarities, and also show the benefit of using our clusters to predict the sentiment of a given text. version:1
arxiv-1501-07680 | Disaggregation of Remotely Sensed Soil Moisture in Heterogeneous Landscapes using Holistic Structure based Models | http://arxiv.org/abs/1501.07680 | id:1501.07680 author:Subit Chakrabarti, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV 68  published:2015-01-30 summary:In this study, a novel machine learning algorithm is presented for disaggregation of satellite soil moisture (SM) based on self-regularized regressive models (SRRM) using high-resolution correlated information from auxiliary sources. It includes regularized clustering that assigns soft memberships to each pixel at fine-scale followed by a kernel regression that computes the value of the desired variable at all pixels. Coarse-scale remotely sensed SM were disaggregated from 10km to 1km using land cover, precipitation, land surface temperature, leaf area index, and in-situ observations of SM. This algorithm was evaluated using multi-scale synthetic observations in NC Florida for heterogeneous agricultural land covers. It was found that the root mean square error (RMSE) for 96% of the pixels was less than 0.02 $m^3/m^3$. The clusters generated represented the data well and reduced the RMSE by upto 40% during periods of high heterogeneity in land-cover and meteorological conditions. The Kullback Leibler divergence (KLD) between the true SM and the disaggregated estimates is close to 0, for both vegetated and baresoil landcovers. The disaggregated estimates were compared to those generated by the Principle of Relevant Information (PRI) method. The RMSE for the PRI disaggregated estimates is higher than the RMSE for the SRRM on each day of the season. The KLD of the disaggregated estimates generated by the SRRM is at least four orders of magnitude lower than those for the PRI disaggregated estimates, while the computational time needed was reduced by three times. The results indicate that the SRRM can be used for disaggregating SM with complex non-linear correlations on a grid with high accuracy. version:2
arxiv-1511-06418 | Binding via Reconstruction Clustering | http://arxiv.org/abs/1511.06418 | id:1511.06418 author:Klaus Greff, Rupesh Kumar Srivastava, Jürgen Schmidhuber category:cs.LG cs.NE  published:2015-11-19 summary:Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem. We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an unsupervised algorithm that uses denoising autoencoders to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process. The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training. version:4
arxiv-1601-05347 | Deep Perceptual Mapping for Cross-Modal Face Recognition | http://arxiv.org/abs/1601.05347 | id:1601.05347 author:M. Saquib Sarfraz, Rainer Stiefelhagen category:cs.CV  published:2016-01-20 summary:Cross modal face matching between the thermal and visible spectrum is a much desired capability for night-time surveillance and security applications. Due to a very large modality gap, thermal-to-visible face recognition is one of the most challenging face matching problem. In this paper, we present an approach to bridge this modality gap by a significant margin. Our approach captures the highly non-linear relationship between the two modalities by using a deep neural network. Our model attempts to learn a non-linear mapping from visible to thermal spectrum while preserving the identity information. We show substantive performance improvement on three difficult thermal-visible face datasets. The presented approach improves the state-of-the-art by more than 10\% on UND-X1 dataset and by more than 15-30\% on NVESD dataset in terms of Rank-1 identification. Our method bridges the drop in performance due to the modality gap by more than 40\%. version:1
arxiv-1601-03313 | Political Speech Generation | http://arxiv.org/abs/1601.03313 | id:1601.03313 author:Valentin Kassarnig category:cs.CL  published:2016-01-13 summary:In this report we present a system that can generate political speeches for a desired political party. Furthermore, the system allows to specify whether a speech should hold a supportive or opposing opinion. The system relies on a combination of several state-of-the-art NLP methods which are discussed in this report. These include n-grams, Justeson & Katz POS tag filter, recurrent neural networks, and latent Dirichlet allocation. Sequences of words are generated based on probabilities obtained from two underlying models: A language model takes care of the grammatical correctness while a topic model aims for textual consistency. Both models were trained on the Convote dataset which contains transcripts from US congressional floor debates. Furthermore, we present a manual and an automated approach to evaluate the quality of generated speeches. In an experimental evaluation generated speeches have shown very high quality in terms of grammatical correctness and sentence transitions. version:2
arxiv-1512-02011 | How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies | http://arxiv.org/abs/1512.02011 | id:1512.02011 author:Vincent François-Lavet, Raphael Fonteneau, Damien Ernst category:cs.LG cs.AI  published:2015-12-07 summary:Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma. version:2
arxiv-1601-05194 | Improved Spoken Document Summarization with Coverage Modeling Techniques | http://arxiv.org/abs/1601.05194 | id:1601.05194 author:Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang category:cs.CL cs.IR  published:2016-01-20 summary:Extractive summarization aims at selecting a set of indicative sentences from a source document as a summary that can express the major theme of the document. A general consensus on extractive summarization is that both relevance and coverage are critical issues to address. The existing methods designed to model coverage can be characterized by either reducing redundancy or increasing diversity in the summary. Maximal margin relevance (MMR) is a widely-cited method since it takes both relevance and redundancy into account when generating a summary for a given document. In addition to MMR, there is only a dearth of research concentrating on reducing redundancy or increasing diversity for the spoken document summarization task, as far as we are aware. Motivated by these observations, two major contributions are presented in this paper. First, in contrast to MMR, which considers coverage by reducing redundancy, we propose two novel coverage-based methods, which directly increase diversity. With the proposed methods, a set of representative sentences, which not only are relevant to the given document but also cover most of the important sub-themes of the document, can be selected automatically. Second, we make a step forward to plug in several document/sentence representation methods into the proposed framework to further enhance the summarization performance. A series of empirical evaluations demonstrate the effectiveness of our proposed methods. version:1
arxiv-1510-07786 | A Framework to Adjust Dependency Measure Estimates for Chance | http://arxiv.org/abs/1510.07786 | id:1510.07786 author:Simone Romano, Nguyen Xuan Vinh, James Bailey, Karin Verspoor category:stat.ML  published:2015-10-27 summary:Estimating the strength of dependency between two variables is fundamental for exploratory analysis and many other applications in data mining. For example: non-linear dependencies between two continuous variables can be explored with the Maximal Information Coefficient (MIC); and categorical variables that are dependent to the target class are selected using Gini gain in random forests. Nonetheless, because dependency measures are estimated on finite samples, the interpretability of their quantification and the accuracy when ranking dependencies become challenging. Dependency estimates are not equal to 0 when variables are independent, cannot be compared if computed on different sample size, and they are inflated by chance on variables with more categories. In this paper, we propose a framework to adjust dependency measure estimates on finite samples. Our adjustments, which are simple and applicable to any dependency measure, are helpful in improving interpretability when quantifying dependency and in improving accuracy on the task of ranking dependencies. In particular, we demonstrate that our approach enhances the interpretability of MIC when used as a proxy for the amount of noise between variables, and to gain accuracy when ranking variables during the splitting procedure in random forests. version:2
arxiv-1511-06380 | Unsupervised Learning of Visual Structure using Predictive Generative Networks | http://arxiv.org/abs/1511.06380 | id:1511.06380 author:William Lotter, Gabriel Kreiman, David Cox category:cs.LG cs.AI cs.CV q-bio.NC  published:2015-11-19 summary:The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features. version:2
arxiv-1601-04888 | A Closed-Form Solution to Tensor Voting: Theory and Applications | http://arxiv.org/abs/1601.04888 | id:1601.04888 author:Tai-Pang Wu, Sai-Kit Yeung, Jiaya Jia, Chi-Keung Tang, Gerard Medioni category:cs.CV  published:2016-01-19 summary:We prove a closed-form solution to tensor voting (CFTV): given a point set in any dimensions, our closed-form solution provides an exact, continuous and efficient algorithm for computing a structure-aware tensor that simultaneously achieves salient structure detection and outlier attenuation. Using CFTV, we prove the convergence of tensor voting on a Markov random field (MRF), thus termed as MRFTV, where the structure-aware tensor at each input site reaches a stationary state upon convergence in structure propagation. We then embed structure-aware tensor into expectation maximization (EM) for optimizing a single linear structure to achieve efficient and robust parameter estimation. Specifically, our EMTV algorithm optimizes both the tensor and fitting parameters and does not require random sampling consensus typically used in existing robust statistical techniques. We performed quantitative evaluation on its accuracy and robustness, showing that EMTV performs better than the original TV and other state-of-the-art techniques in fundamental matrix estimation for multiview stereo matching. The extensions of CFTV and EMTV for extracting multiple and nonlinear structures are underway. An addendum is included in this arXiv version. version:2
arxiv-1508-06538 | Causality, Information and Biological Computation: An algorithmic software approach to life, disease and the immune system | http://arxiv.org/abs/1508.06538 | id:1508.06538 author:Hector Zenil, Angelika Schmidt, Jesper Tegnér category:cs.NE cs.AI  published:2015-08-24 summary:Biology has taken strong steps towards becoming a computer science aiming at reprogramming nature after the realisation that nature herself has reprogrammed organisms by harnessing the power of natural selection and the digital prescriptive nature of replicating DNA. Here we further unpack ideas related to computability, algorithmic information theory and software engineering, in the context of the extent to which biology can be (re)programmed, and with how we may go about doing so in a more systematic way with all the tools and concepts offered by theoretical computer science in a translation exercise from computing to molecular biology and back. These concepts provide a means to a hierarchical organization thereby blurring previously clear-cut lines between concepts like matter and life, or between tumour types that are otherwise taken as different and may not have however a different cause. This does not diminish the properties of life or make its components and functions less interesting. On the contrary, this approach makes for a more encompassing and integrated view of nature, one that subsumes observer and observed within the same system, and can generate new perspectives and tools with which to view complex diseases like cancer, approaching them afresh from a software-engineering viewpoint that casts evolution in the role of programmer, cells as computing machines, DNA and genes as instructions and computer programs, viruses as hacking devices, the immune system as a software debugging tool, and diseases as an information-theoretic battlefield where all these forces deploy. We show how information theory and algorithmic programming may explain fundamental mechanisms of life and death. version:5
arxiv-1601-05141 | Habits vs Environment: What really causes asthma? | http://arxiv.org/abs/1601.05141 | id:1601.05141 author:Mengfan Tang, Pranav Agrawal, Ramesh Jain category:cs.CY cs.LG H.4; D.2.8  published:2016-01-20 summary:Despite considerable number of studies on risk factors for asthma onset, very little is known about their relative importance. To have a full picture of these factors, both categories, personal and environmental data, have to be taken into account simultaneously, which is missing in previous studies. We propose a framework to rank the risk factors from heterogeneous data sources of the two categories. Established on top of EventShop and Personal EventShop, this framework extracts about 400 features, and analyzes them by employing a gradient boosting tree. The features come from sources including personal profile and life-event data, and environmental data on air pollution, weather and PM2.5 emission sources. The top ranked risk factors derived from our framework agree well with the general medical consensus. Thus, our framework is a reliable approach, and the discovered rankings of relative importance of risk factors can provide insights for the prevention of asthma. version:1
arxiv-1512-04295 | Origami: A 803 GOp/s/W Convolutional Network Accelerator | http://arxiv.org/abs/1512.04295 | id:1512.04295 author:Lukas Cavigelli, Luca Benini category:cs.CV cs.AI cs.LG cs.NE B.7.1; I.2.6  published:2015-12-14 summary:An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance. version:2
arxiv-1601-05116 | A Theory of Local Matching: SIFT and Beyond | http://arxiv.org/abs/1601.05116 | id:1601.05116 author:Hossein Mobahi, Stefano Soatto category:cs.CV cs.LG  published:2016-01-19 summary:Why has SIFT been so successful? Why its extension, DSP-SIFT, can further improve SIFT? Is there a theory that can explain both? How can such theory benefit real applications? Can it suggest new algorithms with reduced computational complexity or new descriptors with better accuracy for matching? We construct a general theory of local descriptors for visual matching. Our theory relies on concepts in energy minimization and heat diffusion. We show that SIFT and DSP-SIFT approximate the solution the theory suggests. In particular, DSP-SIFT gives a better approximation to the theoretical solution; justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive new descriptors that have fewer parameters and are potentially better in handling affine deformations. version:1
arxiv-1411-2066 | Learning Theory for Distribution Regression | http://arxiv.org/abs/1411.2066 | id:1411.2066 author:Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, Arthur Gretton category:math.ST cs.LG math.FA stat.ML stat.TH 62G08  46E22  47B32 G.3; I.2.6  published:2014-11-08 summary:We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency tradeoff analysis showing that the studied estimator is able to match the one-stage sampled minimax optimal rate. This result answers a 16-year-old open question, establishing the consistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002] in regression. We also cover consistency for more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010]. version:3
arxiv-1511-07404 | Learning Visual Predictive Models of Physics for Playing Billiards | http://arxiv.org/abs/1511.07404 | id:1511.07404 author:Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, Jitendra Malik category:cs.CV  published:2015-11-23 summary:The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations ("visual imagination"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball. version:3
arxiv-1511-06747 | Data-Dependent Path Normalization in Neural Networks | http://arxiv.org/abs/1511.06747 | id:1511.06747 author:Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG  published:2015-11-20 summary:We propose a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we investigate issue of invariance of the optimization, data dependence and the connection with natural gradients. version:4
arxiv-1511-06456 | Task Loss Estimation for Sequence Prediction | http://arxiv.org/abs/1511.06456 | id:1511.06456 author:Dzmitry Bahdanau, Dmitriy Serdyuk, Philémon Brakel, Nan Rosemary Ke, Jan Chorowski, Aaron Courville, Yoshua Bengio category:cs.LG  published:2015-11-19 summary:Often, the performance on a supervised machine learning task is evaluated with a emph{task loss} function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a emph{surrogate loss} function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call emph{consistency with the task loss}. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error may be used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this property to design specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant ~13% relative improvement in terms of Character Error Rate (CER) in the case when no extra corpora are used for language modeling. version:4
arxiv-1511-02136 | Search-Convolutional Neural Networks | http://arxiv.org/abs/1511.02136 | id:1511.02136 author:James Atwood, Don Towsley category:cs.LG  published:2015-11-06 summary:We present a new deterministic relational model derived from convolutional neural networks. Search-Convolutional Neural Networks (SCNNs) extend the notion of convolution to graph search to construct a rich latent representation that extracts local behavior from general graph-structured data. Unlike other neural network models that take graph-structured data as input, SCNNs have a parameterization that is independent of input size, a property that enables transfer learning between datasets. SCNNs can be applied to a wide variety of prediction tasks, including node classification, community detection, and link prediction. Our results indicate that SCNNs can offer considerable lift over off-the-shelf classifiers and simple multilayer perceptrons, and comparable performance to state-of-the-art probabilistic graphical models at considerably lower computational cost. version:5
arxiv-1601-05030 | PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors | http://arxiv.org/abs/1601.05030 | id:1601.05030 author:Vassileios Balntas, Edward Johns, Lilian Tang, Krystian Mikolajczyk category:cs.CV  published:2016-01-19 summary:In this paper we propose a new approach for learning local descriptors for matching image patches. It has recently been demonstrated that descriptors based on convolutional neural networks (CNN) can significantly improve the matching performance. Unfortunately their computational complexity is prohibitive for any practical application. We address this problem and propose a CNN based descriptor with improved matching performance, significantly reduced training and execution time, as well as low dimensionality. We propose to train the network with triplets of patches that include a positive and negative pairs. To that end we introduce a new loss function that exploits the relations within the triplets. We compare our approach to recently introduced MatchNet and DeepCompare and demonstrate the advantages of our descriptor in terms of performance, memory footprint and speed i.e. when run in GPU, the extraction time of our 128 dimensional feature is comparable to the fastest available binary descriptors such as BRIEF and ORB. version:1
arxiv-1511-06292 | Foveation-based Mechanisms Alleviate Adversarial Examples | http://arxiv.org/abs/1511.06292 | id:1511.06292 author:Yan Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, Qi Zhao category:cs.LG cs.CV  published:2015-11-19 summary:We show that adversarial examples, i.e., the visually imperceptible perturbations that result in Convolutional Neural Networks (CNNs) fail, can be alleviated with a mechanism based on foveations---applying the CNN in different image regions. To see this, first, we report results in ImageNet that lead to a revision of the hypothesis that adversarial perturbations are a consequence of CNNs acting as a linear classifier: CNNs act locally linearly to changes in the image regions with objects recognized by the CNN, and in other regions the CNN may act non-linearly. Then, we corroborate that when the neural responses are linear, applying the foveation mechanism to the adversarial example tends to significantly reduce the effect of the perturbation. This is because, hypothetically, the CNNs for ImageNet are robust to changes of scale and translation of the object produced by the foveation, but this property does not generalize to transformations of the perturbation. As a result, the accuracy after a foveation is almost the same as the accuracy of the CNN without the adversarial perturbation, even if the adversarial perturbation is calculated taking into account a foveation. version:3
arxiv-1601-05011 | Variable projection without smoothness | http://arxiv.org/abs/1601.05011 | id:1601.05011 author:Aleksandr Aravkin, Dmitriy Drusvyatskiy, Tristan van Leeuwen category:math.OC stat.CO stat.ML 65K05  65K10  86-08  published:2016-01-19 summary:Variable projection is a powerful technique in optimization. Over the last 30 years, it has been applied broadly, with empirical and theoretical results demonstrating both greater efficacy and greater stability than competing approaches. In this paper, we illustrate the technique on a large class of structured nonsmooth optimization problems, with numerical examples in sparse deconvolution and machine learning applications. version:1
arxiv-1512-08425 | Convexified Modularity Maximization for Degree-corrected Stochastic Block Models | http://arxiv.org/abs/1512.08425 | id:1512.08425 author:Yudong Chen, Xiaodong Li, Jiaming Xu category:math.ST cs.LG cs.SI stat.ML stat.TH  published:2015-12-28 summary:The stochastic block model (SBM) is a popular framework for studying community detection in networks. This model is limited by the assumption that all nodes in the same community are statistically equivalent and have equal expected degrees. The degree-corrected stochastic block model (DCSBM) is a natural extension of SBM that allows for degree heterogeneity within communities. This paper proposes a convexified modularity maximization approach for estimating the hidden communities under DCSBM. Our approach is based on a convex programming relaxation of the classical (generalized) modularity maximization formulation, followed by a novel doubly-weighted $ \ell_1 $-norm $ k $-median procedure. We establish non-asymptotic theoretical guarantees for both approximate clustering and perfect clustering. Our approximate clustering results are insensitive to the minimum degree, and hold even in sparse regime with bounded average degrees. In the special case of SBM, these theoretical results match the best-known performance guarantees of computationally feasible algorithms. Numerically, we provide an efficient implementation of our algorithm, which is applied to both synthetic and real-world networks. Experiment results show that our method enjoys competitive performance compared to the state of the art in the literature. version:2
arxiv-1307-5944 | Online Optimization in Dynamic Environments | http://arxiv.org/abs/1307.5944 | id:1307.5944 author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.LG math.OC  published:2013-07-23 summary:High-velocity streams of high-dimensional data pose significant "big data" analysis challenges across a range of applications and settings. Online learning and online convex programming play a significant role in the rapid recovery of important or anomalous information from these large datastreams. While recent advances in online learning have led to novel and rapidly converging algorithms, these methods are unable to adapt to nonstationary environments arising in real-world problems. This paper describes a dynamic mirror descent framework which addresses this challenge, yielding low theoretical regret bounds and accurate, adaptive, and computationally efficient algorithms which are applicable to broad classes of problems. The methods are capable of learning and adapting to an underlying and possibly time-varying dynamical model. Empirical results in the context of dynamic texture analysis, solar flare detection, sequential compressed sensing of a dynamic scene, traffic surveillance,tracking self-exciting point processes and network behavior in the Enron email corpus support the core theoretical findings. version:3
arxiv-1508-01746 | Using Deep Learning for Detecting Spoofing Attacks on Speech Signals | http://arxiv.org/abs/1508.01746 | id:1508.01746 author:Alan Godoy, Flávio Simões, José Augusto Stuchi, Marcus de Assis Angeloni, Mário Uliani, Ricardo Violato category:cs.SD cs.CL cs.CR cs.LG stat.ML  published:2015-08-07 summary:It is well known that speaker verification systems are subject to spoofing attacks. The Automatic Speaker Verification Spoofing and Countermeasures Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing attacks based on synthetic speech, along with a protocol for experiments. This paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based on deep neural networks, working both as a classifier and as a feature extraction module for a GMM and a SVM classifier. Results show the validity of this approach, achieving less than 0.5\% EER for known attacks. version:2
arxiv-1510-06223 | Predicting popularity of online videos using Support Vector Regression | http://arxiv.org/abs/1510.06223 | id:1510.06223 author:Tomasz Trzcinski, Przemyslaw Rokita category:cs.SI cs.CV  published:2015-10-21 summary:In this work, we propose a regression method to predict the popularity of an online video based on temporal and visual cues. Our method uses Support Vector Regression with Gaussian Radial Basis Functions. We show that modelling popularity patterns with this approach provides higher and more stable prediction results, mainly thanks to the non-linearity character of the proposed method as well as its resistance against overfitting. We compare our method with the state of the art on datasets containing over 14,000 videos from YouTube and Facebook. Furthermore, we show that results obtained relying only on the early distribution patterns, can be improved by adding social and visual metadata. version:3
arxiv-1601-02644 | 3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye Trackers | http://arxiv.org/abs/1601.02644 | id:1601.02644 author:Mohsen Mansouryar, Julian Steil, Yusuke Sugano, Andreas Bulling category:cs.HC cs.CV  published:2016-01-11 summary:3D gaze information is important for scene-centric attention analysis but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures. version:2
arxiv-1601-04920 | Understanding Deep Convolutional Networks | http://arxiv.org/abs/1601.04920 | id:1601.04920 author:Stéphane Mallat category:stat.ML cs.CV cs.LG  published:2016-01-19 summary:Deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and non-linearities. A mathematical framework is introduced to analyze their properties. Computations of invariants involve multiscale contractions, the linearization of hierarchical symmetries, and sparse separations. Applications are discussed. version:1
arxiv-1506-05085 | Time Series Classification using the Hidden-Unit Logistic Model | http://arxiv.org/abs/1506.05085 | id:1506.05085 author:Wenjie Pei, Hamdi Dibeklioğlu, David M. J. Tax, Laurens van der Maaten category:cs.LG cs.CV  published:2015-06-16 summary:We present a new model for time series classification, called the hidden-unit logistic model, that uses binary stochastic hidden units to model latent structure in the data. The hidden units are connected in a chain structure that models temporal dependencies in the data. Compared to the prior models for time series classification such as the hidden conditional random field, our model can model very complex decision boundaries because the number of latent states grows exponentially with the number of hidden units. We demonstrate the strong performance of our model in experiments on a variety of (computer vision) tasks, including handwritten character recognition, speech recognition, facial expression, and action recognition. We also present a state-of-the-art system for facial action unit detection based on the hidden-unit logistic model. version:2
arxiv-1601-04902 | PupilNet: Convolutional Neural Networks for Robust Pupil Detection | http://arxiv.org/abs/1601.04902 | id:1601.04902 author:Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, Enkelejda Kasneci category:cs.CV  published:2016-01-19 summary:Real-time, accurate, and robust pupil detection is an essential prerequisite for pervasive video-based eye-tracking. However, automated pupil detection in real-world scenarios has proven to be an intricate challenge due to fast illumination changes, pupil occlusion, non centered and off-axis eye recording, and physiological eye characteristics. In this paper, we propose and evaluate a method based on a novel dual convolutional neural network pipeline. In its first stage the pipeline performs coarse pupil position identification using a convolutional neural network and subregions from a downscaled input image to decrease computational costs. Using subregions derived from a small window around the initial pupil position estimate, the second pipeline stage employs another convolutional neural network to refine this position, resulting in an increased pupil detection rate up to 25% in comparison with the best performing state-of-the-art algorithm. Annotated data sets can be made available upon request. version:1
arxiv-1601-04862 | Scalability in Neural Control of Musculoskeletal Robots | http://arxiv.org/abs/1601.04862 | id:1601.04862 author:Christoph Richter, Sören Jentzsch, Rafael Hostettler, Jesús A. Garrido, Eduardo Ros, Alois C. Knoll, Florian Röhrbein, Patrick van der Smagt, Jörg Conradt category:cs.RO cs.DC cs.NE cs.SY  published:2016-01-19 summary:Anthropomimetic robots are robots that sense, behave, interact and feel like humans. By this definition, anthropomimetic robots require human-like physical hardware and actuation, but also brain-like control and sensing. The most self-evident realization to meet those requirements would be a human-like musculoskeletal robot with a brain-like neural controller. While both musculoskeletal robotic hardware and neural control software have existed for decades, a scalable approach that could be used to build and control an anthropomimetic human-scale robot has not been demonstrated yet. Combining Myorobotics, a framework for musculoskeletal robot development, with SpiNNaker, a neuromorphic computing platform, we present the proof-of-principle of a system that can scale to dozens of neurally-controlled, physically compliant joints. At its core, it implements a closed-loop cerebellar model which provides real-time low-level neural control at minimal power consumption and maximal extensibility: higher-order (e.g., cortical) neural networks and neuromorphic sensors like silicon-retinae or -cochleae can naturally be incorporated. version:1
arxiv-1601-04800 | Top-N Recommender System via Matrix Completion | http://arxiv.org/abs/1601.04800 | id:1601.04800 author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.IR cs.AI cs.LG stat.ML  published:2016-01-19 summary:Top-N recommender systems have been investigated widely both in industry and academia. However, the recommendation quality is far from satisfactory. In this paper, we propose a simple yet promising algorithm. We fill the user-item matrix based on a low-rank assumption and simultaneously keep the original information. To do that, a nonconvex rank relaxation rather than the nuclear norm is adopted to provide a better rank approximation and an efficient optimization strategy is designed. A comprehensive set of experiments on real datasets demonstrates that our method pushes the accuracy of Top-N recommendation to a new level. version:1
arxiv-1404-6853 | One-bit compressive sensing with norm estimation | http://arxiv.org/abs/1404.6853 | id:1404.6853 author:Karin Knudson, Rayan Saab, Rachel Ward category:stat.ML math.NA math.OC math.PR 90C05  published:2014-04-28 summary:Consider the recovery of an unknown signal ${x}$ from quantized linear measurements. In the one-bit compressive sensing setting, one typically assumes that ${x}$ is sparse, and that the measurements are of the form $\operatorname{sign}(\langle {a}_i, {x} \rangle) \in \{\pm1\}$. Since such measurements give no information on the norm of ${x}$, recovery methods from such measurements typically assume that $\ {x} \ _2=1$. We show that if one allows more generally for quantized affine measurements of the form $\operatorname{sign}(\langle {a}_i, {x} \rangle + b_i)$, and if the vectors ${a}_i$ are random, an appropriate choice of the affine shifts $b_i$ allows norm recovery to be easily incorporated into existing methods for one-bit compressive sensing. Additionally, we show that for arbitrary fixed ${x}$ in the annulus $r \leq \ {x} \ _2 \leq R$, one may estimate the norm $\ {x} \ _2$ up to additive error $\delta$ from $m \gtrsim R^4 r^{-2} \delta^{-2}$ such binary measurements through a single evaluation of the inverse Gaussian error function. Finally, all of our recovery guarantees can be made universal over sparse vectors, in the sense that with high probability, one set of measurements and thresholds can successfully estimate all sparse vectors ${x}$ within a Euclidean ball of known radius. version:3
arxiv-1407-5245 | Feature and Region Selection for Visual Learning | http://arxiv.org/abs/1407.5245 | id:1407.5245 author:Ji Zhao, Liantao Wang, Ricardo Cabral, Fernando De la Torre category:cs.CV cs.LG  published:2014-07-20 summary:Visual learning problems such as object classification and action recognition are typically approached using extensions of the popular bag-of-words (BoW) model. Despite its great success, it is unclear what visual features the BoW model is learning: Which regions in the image or video are used to discriminate among classes? Which are the most discriminative visual words? Answering these questions is fundamental for understanding existing BoW models and inspiring better models for visual recognition. To answer these questions, this paper presents a method for feature selection and region selection in the visual BoW model. This allows for an intermediate visualization of the features and regions that are important for visual learning. The main idea is to assign latent weights to the features or regions, and jointly optimize these latent variables with the parameters of a classifier (e.g., support vector machine). There are four main benefits of our approach: (1) Our approach accommodates non-linear additive kernels such as the popular $\chi^2$ and intersection kernel; (2) our approach is able to handle both regions in images and spatio-temporal regions in videos in a unified way; (3) the feature selection problem is convex, and both problems can be solved using a scalable reduced gradient method; (4) we point out strong connections with multiple kernel learning and multiple instance learning approaches. Experimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTube illustrate the benefits of our approach. version:2
arxiv-1511-06104 | Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction | http://arxiv.org/abs/1511.06104 | id:1511.06104 author:Sheng-Yi Bai, Sebastian Agethen, Ting-Hsuan Chao, Winston Hsu category:cs.NE cs.CV cs.LG  published:2015-11-19 summary:The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand. version:2
arxiv-1601-04756 | Improved Sampling Techniques for Learning an Imbalanced Data Set | http://arxiv.org/abs/1601.04756 | id:1601.04756 author:Maureen Lyndel C. Lauron, Jaderick P. Pabico category:cs.LG  published:2016-01-18 summary:This paper presents the performance of a classifier built using the stackingC algorithm in nine different data sets. Each data set is generated using a sampling technique applied on the original imbalanced data set. Five new sampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax Random Oversampling, Lax Random Undersampling, Combined-Lax Random Oversampling Undersampling, and Combined-Lax Random Undersampling Oversampling) that were based on the three sampling techniques (i.e., Random Undersampling, Random Oversampling, and Synthetic Minority Oversampling Technique) usually used as solutions in imbalance learning. The metrics used to evaluate the classifier's performance were F-measure and G-mean. F-measure determines the performance of the classifier for every class, while G-mean measures the overall performance of the classifier. The results using F-measure showed that for the data without a sampling technique, the classifier's performance is good only for the majority class. It also showed that among the eight sampling techniques, RU and LRU have the worst performance while other techniques (i.e., RO, C-LRUO and C-LROU) performed well only on some classes. The best performing techniques in all data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measure values between 0.5 and 0.65. The results using G-mean showed that the oversampling technique that attained the highest G-mean value is LRO (0.86), next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83). Combining the result of the two metrics (F-measure and G-mean), only the three sampling techniques are considered as good performing (i.e., LRO, SMOTE, and SMOTERandRep). version:1
arxiv-1304-0682 | Sparse Signal Processing with Linear and Non-Linear Observations: A Unified Shannon Theoretic Approach | http://arxiv.org/abs/1304.0682 | id:1304.0682 author:Cem Aksoylar, George Atia, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2013-04-02 summary:We derive fundamental sample complexity bounds for recovering sparse and structured signals for linear and nonlinear observation models including sparse regression, group testing, multivariate regression and problems with missing features. In general, sparse signal processing problems can be characterized in terms of the following Markovian property. We are given a set of $N$ variables $X_1,X_2,\ldots,X_N$, and there is an unknown subset of variables $S \subset [N]$ that are \emph{relevant} for predicting outcomes $Y$. More specifically, when $Y$ is conditioned on $\{X_n\}_{n\in S}$ it is conditionally independent of the other variables, $\{X_n\}_{n \not \in S}$. Our goal is to identify the set $S$ from samples of the variables $X$ and the associated outcomes $Y$. We characterize this problem as a version of the noisy channel coding problem. Using asymptotic information theoretic analyses, we establish mutual information formulas that provide sufficient and necessary conditions on the number of samples required to successfully recover the salient variables. These mutual information expressions unify conditions for both linear and nonlinear observations. We then compute sample complexity bounds for the aforementioned models, based on the mutual information expressions in order to demonstrate the applicability and flexibility of our results in general sparse signal processing models. version:7
arxiv-1601-04692 | Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: a Survey | http://arxiv.org/abs/1601.04692 | id:1601.04692 author:Jean Gallier category:cs.LG cs.DS 68  published:2016-01-18 summary:This is a survey of the method of graph cuts and its applications to graph clustering of weighted unsigned and signed graphs. I provide a fairly thorough treatment of the method of normalized graph cuts, a deeply original method due to Shi and Malik, including complete proofs. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K > 2 clusters, based on the work of Yu and Shi. I also show how both graph drawing and normalized cut K-clustering can be easily generalized to handle signed graphs, which are weighted graphs in which the weight matrix W may have negative coefficients. Intuitively, negative coefficients indicate distance or dissimilarity. The solution is to replace the degree matrix by the matrix in which absolute values of the weights are used, and to replace the Laplacian by the Laplacian with the new degree matrix of absolute values. As far as I know, the generalization of K-way normalized clustering to signed graphs is new. Finally, I show how the method of ratio cuts, in which a cut is normalized by the size of the cluster rather than its volume, is just a special case of normalized cuts. version:1
arxiv-1506-02158 | Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference | http://arxiv.org/abs/1506.02158 | id:1506.02158 author:Yarin Gal, Zoubin Ghahramani category:stat.ML cs.LG  published:2015-06-06 summary:Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10. version:6
arxiv-1601-04669 | The Image Torque Operator for Contour Processing | http://arxiv.org/abs/1601.04669 | id:1601.04669 author:Morimichi Nishigaki, Cornelia Fermüller category:cs.CV  published:2016-01-18 summary:Contours are salient features for image description, but the detection and localization of boundary contours is still considered a challenging problem. This paper introduces a new tool for edge processing implementing the Gestaltism idea of edge grouping. This tool is a mid-level image operator, called the Torque operator, that is designed to help detect closed contours in images. The torque operator takes as input the raw image and creates an image map by computing from the image gradients within regions of multiple sizes a measure of how well the edges are aligned to form closed convex contours. Fundamental properties of the torque are explored and illustrated through examples. Then it is applied in pure bottom-up processing in a variety of applications, including edge detection, visual attention and segmentation and experimentally demonstrated a useful tool that can improve existing techniques. Finally, its extension as a more general grouping mechanism and application in object recognition is discussed. version:1
arxiv-1601-04667 | Proactive Message Passing on Memory Factor Networks | http://arxiv.org/abs/1601.04667 | id:1601.04667 author:Patrick Eschenfeldt, Dan Schmidt, Stark Draper, Jonathan Yedidia category:cs.AI cs.CV  published:2016-01-18 summary:We introduce a new type of graphical model that we call a "memory factor network" (MFN). We show how to use MFNs to model the structure inherent in many types of data sets. We also introduce an associated message-passing style algorithm called "proactive message passing"' (PMP) that performs inference on MFNs. PMP comes with convergence guarantees and is efficient in comparison to competing algorithms such as variants of belief propagation. We specialize MFNs and PMP to a number of distinct types of data (discrete, continuous, labelled) and inference problems (interpolation, hypothesis testing), provide examples, and discuss approaches for efficient implementation. version:1
arxiv-1511-06964 | Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and Denoising Autoencoders | http://arxiv.org/abs/1511.06964 | id:1511.06964 author:Alexander G. Ororbia II, C. Lee Giles, David Reitter category:cs.LG  published:2015-11-22 summary:Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-encoder, are proposed for handling semi-supervised learning problems. The models combine experts that model relevant distributions at different levels of abstraction to improve overall predictive performance on discriminative tasks. Theoretical motivations and algorithms for joint learning for each are presented. We apply the new models to the domain of data-streams in work towards life-long learning. The proposed architectures show improved performance compared to a pseudo-labeled, drop-out rectifier network. version:7
arxiv-1601-04621 | Detecting the Age of Twitter Users | http://arxiv.org/abs/1601.04621 | id:1601.04621 author:Benjamin Paul Chamberlain, Clive Humby, Marc Peter Deisenroth category:cs.SI stat.ML  published:2016-01-18 summary:Twitter provides an extremely rich and open source of data for studying human behaviour at scale. It has been used to advance our understanding of social network structure, the viral flow of information and how new ideas develop. Enriching Twitter with demographic information would permit more precise science and better generalisation to the real world. The only demographic indicators associated with a Twitter account are the free text name, location and description fields. We show how the age of most Twitter accounts can be inferred with high accuracy using the structure of the social graph. Besides classical social science applications, there are obvious privacy and child protection implications to this discovery. Previous work on Twitter age detection has focussed on either user-name or linguistic features of tweets. A shortcoming of the user-name approach is that it requires real names (Twitter names are often false) and census data from each user's (unknown) birth country. Problems with linguistic approaches are that most Twitter users do not tweet (the median number of Tweets is 4) and a different model must be learnt for each language. To address these issues, we devise a language-independent methodology for determining the age of Twitter users from data that is native to the Twitter ecosystem. Roughly 150,000 Twitter users specify an age in their free text description field. We generalize this to the entire Twitter network by showing that age can be predicted based on what or whom they follow. We adopt a Bayesian classification paradigm, which offers a consistent framework for handling uncertainty in our data, e.g., inaccurate age descriptions or spurious edges in the graph. Working within this paradigm we have successfully applied age detection to 700 million Twitter accounts with an F1 Score of 0.86. version:1
arxiv-1601-04619 | Comparison-based Image Quality Assessment for Parameter Selection | http://arxiv.org/abs/1601.04619 | id:1601.04619 author:Haoyi Liang, Daniel S. Weller category:cs.CV  published:2016-01-18 summary:Image quality assessment (IQA) is traditionally classified into full-reference (FR) IQA and no-reference (NR) IQA according to whether the original image is required. Although NR-IQA is widely used in practical applications, room for improvement still remains because of the lack of the reference image. Inspired by the fact that in many applications, such as parameter selection, a series of distorted images are available, the authors propose a novel comparison-based image quality assessment (C-IQA) method. The new comparison-based framework parallels FR-IQA by requiring two input images, and resembles NR-IQA by not using the original image. As a result, the new comparison-based approach has more application scenarios than FR-IQA does, and takes greater advantage of the accessible information than the traditional single-input NR-IQA does. Further, C-IQA is compared with other state-of-the-art NR-IQA methods on two widely used IQA databases. Experimental results show that C-IQA outperforms the other NR-IQA methods for parameter selection, and the parameter trimming framework combined with C-IQA saves the computation of iterative image reconstruction up to 80%. version:1
arxiv-1601-04589 | Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis | http://arxiv.org/abs/1601.04589 | id:1601.04589 author:Chuan Li, Michael Wand category:cs.CV  published:2016-01-18 summary:This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods. version:1
arxiv-1601-04580 | Nonparametric Bayesian Storyline Detection from Microtexts | http://arxiv.org/abs/1601.04580 | id:1601.04580 author:Vinodh Krishnan, Jacob Eisenstein category:cs.CL cs.LG  published:2016-01-18 summary:News events and social media are composed of evolving storylines, which capture public attention for a limited period of time. Identifying these storylines would enable many high-impact applications, such as tracking public interest and opinion in ongoing crisis events. However, this requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach. We present a novel online non-parametric Bayesian framework for storyline detection, using the distance-dependent Chinese Restaurant Process (dd-CRP). To ensure efficient linear-time inference, we employ a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate our baseline and proposed models on the TREC Twitter Timeline Generation task and show strong results. version:1
arxiv-1601-04568 | Content Aware Neural Style Transfer | http://arxiv.org/abs/1601.04568 | id:1601.04568 author:Rujie Yin category:cs.CV 68T10  published:2016-01-18 summary:This paper presents a content-aware style transfer algorithm for paintings and photos of similar content using pre-trained neural network, obtaining better results than the previous work. In addition, the numerical experiments show that the style pattern and the content information is not completely separated by neural network. version:1
arxiv-1601-04549 | Incremental Semiparametric Inverse Dynamics Learning | http://arxiv.org/abs/1601.04549 | id:1601.04549 author:Raffaello Camoriano, Silvio Traversaro, Lorenzo Rosasco, Giorgio Metta, Francesco Nori category:stat.ML cs.LG cs.RO  published:2016-01-18 summary:This paper presents a novel approach for incremental semiparametric inverse dynamics learning. In particular, we consider the mixture of two approaches: Parametric modeling based on rigid body dynamics equations and nonparametric modeling based on incremental kernel methods, with no prior information on the mechanical properties of the system. This yields to an incremental semiparametric approach, leveraging the advantages of both the parametric and nonparametric models. We validate the proposed technique learning the dynamics of one arm of the iCub humanoid robot. version:1
arxiv-1601-04530 | Domain based classification | http://arxiv.org/abs/1601.04530 | id:1601.04530 author:Robert P. W. Duin, Elzbieta Pekalska category:stat.ML cs.LG  published:2016-01-18 summary:The majority of traditional classification ru les minimizing the expected probability of error (0-1 loss) are inappropriate if the class probability distributions are ill-defined or impossible to estimate. We argue that in such cases class domains should be used instead of class distributions or densities to construct a reliable decision function. Proposals are presented for some evaluation criteria and classifier learning schemes, illustrated by an example. version:1
arxiv-1502-02846 | Probabilistic Line Searches for Stochastic Optimization | http://arxiv.org/abs/1502.02846 | id:1502.02846 author:Maren Mahsereci, Philipp Hennig category:cs.LG math.OC stat.ML  published:2015-02-10 summary:In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent. version:4
arxiv-1601-04468 | Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation | http://arxiv.org/abs/1601.04468 | id:1601.04468 author:Artem Sokolov, Stefan Riezler, Tanguy Urvoy category:cs.CL cs.LG  published:2016-01-18 summary:We present an approach to structured prediction from bandit feedback, called Bandit Structured Prediction, where only the value of a task loss function at a single predicted point, instead of a correct structure, is observed in learning. We present an application to discriminative reranking in Statistical Machine Translation (SMT) where the learning algorithm only has access to a 1-BLEU loss evaluation of a predicted translation instead of obtaining a gold standard reference translation. In our experiment bandit feedback is obtained by evaluating BLEU on reference translations without revealing them to the algorithm. This can be thought of as a simulation of interactive machine translation where an SMT system is personalized by a user who provides single point feedback to predicted translations. Our experiments show that our approach improves translation quality and is comparable to approaches that employ more informative feedback in learning. version:1
arxiv-1601-04451 | Zero-error dissimilarity based classifiers | http://arxiv.org/abs/1601.04451 | id:1601.04451 author:Robert P. W. Duin, Elzbieta Pekalska category:stat.ML cs.LG  published:2016-01-18 summary:We consider general non-Euclidean distance measures between real world objects that need to be classified. It is assumed that objects are represented by distances to other objects only. Conditions for zero-error dissimilarity based classifiers are derived. Additional conditions are given under which the zero-error decision boundary is a continues function of the distances to a finite set of training samples. These conditions affect the objects as well as the distance measure used. It is argued that they can be met in practice. version:1
arxiv-1512-05582 | Kauffman's adjacent possible in word order evolution | http://arxiv.org/abs/1512.05582 | id:1512.05582 author:Ramon Ferrer-i-Cancho category:cs.CL cs.IT math.IT physics.data-an physics.soc-ph  published:2015-12-17 summary:Word order evolution has been hypothesized to be constrained by a word order permutation ring: transitions involving orders that are closer in the permutation ring are more likely. The hypothesis can be seen as a particular case of Kauffman's adjacent possible in word order evolution. Here we consider the problem of the association of the six possible orders of S, V and O to yield a couple of primary alternating orders as a window to word order evolution. We evaluate the suitability of various competing hypotheses to predict one member of the couple from the other with the help of information theoretic model selection. Our ensemble of models includes a six-way model that is based on the word order permutation ring (Kauffman's adjacent possible) and another model based on the dual two-way of standard typology, that reduces word order to basic orders preferences (e.g., a preference for SV over VS and another for SO over OS). Our analysis indicates that the permutation ring yields the best model when favoring parsimony strongly, providing support for Kauffman's general view and a six-way typology. version:2
arxiv-1511-08136 | Unifying Decision Trees Split Criteria Using Tsallis Entropy | http://arxiv.org/abs/1511.08136 | id:1511.08136 author:Yisen Wang, Chaobing Song, Shu-Tao Xia category:stat.ML cs.AI cs.LG  published:2015-11-25 summary:The construction of efficient and effective decision trees remains a key topic in machine learning because of their simplicity and flexibility. A lot of heuristic algorithms have been proposed to construct near-optimal decision trees. ID3, C4.5 and CART are classical decision tree algorithms and the split criteria they used are Shannon entropy, Gain Ratio and Gini index respectively. All the split criteria seem to be independent, actually, they can be unified in a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon entropy and provides a new approach to enhance decision trees' performance with an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC) algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index, which generalizes the split criteria of decision trees. More importantly, we reveal the relations between Tsallis entropy with different $q$ and other split criteria. Experimental results on UCI data sets indicate that the TEC algorithm achieves statistically significant improvement over the classical algorithms. version:4
arxiv-1601-04406 | Discovering Picturesque Highlights from Egocentric Vacation Videos | http://arxiv.org/abs/1601.04406 | id:1601.04406 author:Vinay Bettadapura, Daniel Castro, Irfan Essa category:cs.CV  published:2016-01-18 summary:We present an approach for identifying picturesque highlights from large amounts of egocentric video data. Given a set of egocentric videos captured over the course of a vacation, our method analyzes the videos and looks for images that have good picturesque and artistic properties. We introduce novel techniques to automatically determine aesthetic features such as composition, symmetry and color vibrancy in egocentric videos and rank the video frames based on their photographic qualities to generate highlights. Our approach also uses contextual information such as GPS, when available, to assess the relative importance of each geographic location where the vacation videos were shot. Furthermore, we specifically leverage the properties of egocentric videos to improve our highlight detection. We demonstrate results on a new egocentric vacation dataset which includes 26.5 hours of videos taken over a 14 day vacation that spans many famous tourist destinations and also provide results from a user-study to access our results. version:1
arxiv-1601-04386 | A Comparative Study of Object Trackers for Infrared Flying Bird Tracking | http://arxiv.org/abs/1601.04386 | id:1601.04386 author:Ying Huang, Hong Zheng, Haibin Ling, Erik Blasch, Hao Yang category:cs.CV  published:2016-01-18 summary:Bird strikes present a huge risk for aircraft, especially since traditional airport bird surveillance is mainly dependent on inefficient human observation. Computer vision based technology has been proposed to automatically detect birds, determine bird flying trajectories, and predict aircraft takeoff delays. However, the characteristics of bird flight using imagery and the performance of existing methods applied to flying bird task are not well known. Therefore, we perform infrared flying bird tracking experiments using 12 state-of-the-art algorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommend feature analysis. We also develop a Struck-scale method to demonstrate the effectiveness of multiple scale sampling adaption in handling the object of flying bird with varying shape and scale. The general analysis can be used to develop specialized bird tracking methods for airport safety, wildness and urban bird population studies. version:1
arxiv-1511-02381 | Information Extraction Under Privacy Constraints | http://arxiv.org/abs/1511.02381 | id:1511.02381 author:Shahab Asoodeh, Mario Diaz, Fady Alajaji, Tamás Linder category:cs.IT math.IT math.ST stat.ML stat.TH  published:2015-11-07 summary:A privacy-constrained information extraction problem is considered where for a pair of correlated discrete random variables $(X,Y)$ governed by a given joint distribution, an agent observes $Y$ and wants to convey to a potentially public user as much information about $Y$ as possible without compromising the amount of information revealed about $X$. To this end, the so-called {\em rate-privacy function} is introduced to quantify the maximal amount of information (measured in terms of mutual information) that can be extracted from $Y$ under a privacy constraint between $X$ and the extracted information, where privacy is measured using either mutual information or maximal correlation. Properties of the rate-privacy function are analyzed and information-theoretic and estimation-theoretic interpretations of it are presented for both the mutual information and maximal correlation privacy measures. It is also shown that the rate-privacy function admits a closed-form expression for a large family of joint distributions of $(X,Y)$. Finally, the rate-privacy function under the mutual information privacy measure is considered for the case where $(X,Y)$ has a joint probability density function by studying the problem where the extracted information is a uniform quantization of $Y$ corrupted by additive Gaussian noise. The asymptotic behavior of the rate-privacy function is studied as the quantization resolution grows without bound and it is observed that not all of the properties of the rate-privacy function carry over from the discrete to the continuous case. version:3
arxiv-1512-02752 | A Novel Regularized Principal Graph Learning Framework on Explicit Graph Representation | http://arxiv.org/abs/1512.02752 | id:1512.02752 author:Qi Mao, Li Wang, Ivor W. Tsang, Yijun Sun category:cs.AI cs.LG stat.ML  published:2015-12-09 summary:Many scientific datasets are of high dimension, and the analysis usually requires visual manipulation by retaining the most important structures of data. Principal curve is a widely used approach for this purpose. However, many existing methods work only for data with structures that are not self-intersected, which is quite restrictive for real applications. A few methods can overcome the above problem, but they either require complicated human-made rules for a specific task with lack of convergence guarantee and adaption flexibility to different tasks, or cannot obtain explicit structures of data. To address these issues, we develop a new regularized principal graph learning framework that captures the local information of the underlying graph structure based on reversed graph embedding. As showcases, models that can learn a spanning tree or a weighted undirected $\ell_1$ graph are proposed, and a new learning algorithm is developed that learns a set of principal points and a graph structure from data, simultaneously. The new algorithm is simple with guaranteed convergence. We then extend the proposed framework to deal with large-scale data. Experimental results on various synthetic and six real world datasets show that the proposed method compares favorably with baselines and can uncover the underlying structure correctly. version:2
arxiv-1601-04296 | Building a Learning Database for the Neural Network Retrieval of Sea Surface Salinity from SMOS Brightness Temperatures | http://arxiv.org/abs/1601.04296 | id:1601.04296 author:Adel Ammar, Sylvie Labroue, Estelle Obligis, Michel Crépon, Sylvie Thiria category:cs.NE physics.ao-ph  published:2016-01-17 summary:This article deals with an important aspect of the neural network retrieval of sea surface salinity (SSS) from SMOS brightness temperatures (TBs). The neural network retrieval method is an empirical approach that offers the possibility of being independent from any theoretical emissivity model, during the in-flight phase. A Previous study [1] has proven that this approach is applicable to all pixels on ocean, by designing a set of neural networks with different inputs. The present study focuses on the choice of the learning database and demonstrates that a judicious distribution of the geophysical parameters allows to markedly reduce the systematic regional biases of the retrieved SSS, which are due to the high noise on the TBs. An equalization of the distribution of the geophysical parameters, followed by a new technique for boosting the learning process, makes the regional biases almost disappear for latitudes between 40{\deg}S and 40{\deg}N, while the global standard deviation remains between 0.6 psu (at the center of the of the swath) and 1 psu (at the edges). version:1
arxiv-1601-04293 | Face-space Action Recognition by Face-Object Interactions | http://arxiv.org/abs/1601.04293 | id:1601.04293 author:Amir Rosenfeld, Shimon Ullman category:cs.CV  published:2016-01-17 summary:Action recognition in still images has seen major improvement in recent years due to advances in human pose estimation, object recognition and stronger feature representations. However, there are still many cases in which performance remains far from that of humans. In this paper, we approach the problem by learning explicitly, and then integrating three components of transitive actions: (1) the human body part relevant to the action (2) the object being acted upon and (3) the specific form of interaction between the person and the object. The process uses class-specific features and relations not used in the past for action recognition and which use inherently two cycles in the process unlike most standard approaches. We focus on face-related actions (FRA), a subset of actions that includes several currently challenging categories. We present an average relative improvement of 52% over state-of-the art. We also make a new benchmark publicly available. version:1
arxiv-1509-08102 | Discriminative Learning of the Prototype Set for Nearest Neighbor Classification | http://arxiv.org/abs/1509.08102 | id:1509.08102 author:Shin Ando category:cs.LG  published:2015-09-27 summary:The nearest neighbor rule is one of the most widely used models for classification and selecting a compact set of prototype instances is an important problem for its applications. Many existing approaches on the prototype selection problem have relied on instance-based analyses of the class distribution, which can be computationally expensive for large datasets. In this paper, we revisit this problem to explore a parametric approach, which approximates the violation of the nearest neighbor rule over the training set and learns the prioritization of prototypes that minimizes the violation. We show that our approach reduces the problem to large-margin learning and demonstrate its advantage by empirical comparisons using public benchmark data. version:4
arxiv-1601-04251 | On-line Bayesian System Identification | http://arxiv.org/abs/1601.04251 | id:1601.04251 author:Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso category:cs.SY cs.LG stat.AP stat.ML  published:2016-01-17 summary:We consider an on-line system identification setting, in which new data become available at given time steps. In order to meet real-time estimation requirements, we propose a tailored Bayesian system identification procedure, in which the hyper-parameters are still updated through Marginal Likelihood maximization, but after only one iteration of a suitable iterative optimization algorithm. Both gradient methods and the EM algorithm are considered for the Marginal Likelihood optimization. We compare this "1-step" procedure with the standard one, in which the optimization method is run until convergence to a local minimum. The experiments we perform confirm the effectiveness of the approach we propose. version:1
arxiv-1511-02554 | Deep Recurrent Neural Networks for Sequential Phenotype Prediction in Genomics | http://arxiv.org/abs/1511.02554 | id:1511.02554 author:Farhad Pouladi, Hojjat Salehinejad, Amir Mohammad Gilani category:cs.NE cs.CE cs.LG  published:2015-11-09 summary:In analyzing of modern biological data, we are often dealing with ill-posed problems and missing data, mostly due to high dimensionality and multicollinearity of the dataset. In this paper, we have proposed a system based on matrix factorization (MF) and deep recurrent neural networks (DRNNs) for genotype imputation and phenotype sequences prediction. In order to model the long-term dependencies of phenotype data, the new Recurrent Linear Units (ReLU) learning strategy is utilized for the first time. The proposed model is implemented for parallel processing on central processing units (CPUs) and graphic processing units (GPUs). Performance of the proposed model is compared with other training algorithms for learning long-term dependencies as well as the sparse partial least square (SPLS) method on a set of genotype and phenotype data with 604 samples, 1980 single-nucleotide polymorphisms (SNPs), and two traits. The results demonstrate performance of the ReLU training algorithm in learning long-term dependencies in RNNs. version:3
arxiv-1505-05192 | Unsupervised Visual Representation Learning by Context Prediction | http://arxiv.org/abs/1505.05192 | id:1505.05192 author:Carl Doersch, Abhinav Gupta, Alexei A. Efros category:cs.CV  published:2015-05-19 summary:This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations. version:3
arxiv-1511-05432 | Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization | http://arxiv.org/abs/1511.05432 | id:1511.05432 author:Uri Shaham, Yutaro Yamada, Sahand Negahban category:stat.ML cs.LG cs.NE  published:2015-11-17 summary:We propose a general framework for increasing local stability of Artificial Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an alternating minimization-maximization procedure, in which the loss of the network is minimized over perturbed examples that are generated at each parameter update. We show that adversarial training of ANNs is in fact robustification of the network optimization, and that our proposed framework generalizes previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the network also on the original test data. version:3
arxiv-1601-04187 | Conversion of Artificial Recurrent Neural Networks to Spiking Neural Networks for Low-power Neuromorphic Hardware | http://arxiv.org/abs/1601.04187 | id:1601.04187 author:Peter U. Diehl, Guido Zarrella, Andrew Cassidy, Bruno U. Pedroni, Emre Neftci category:cs.NE  published:2016-01-16 summary:In recent years the field of neuromorphic low-power systems that consume orders of magnitude less power gained significant momentum. However, their wider use is still hindered by the lack of algorithms that can harness the strengths of such architectures. While neuromorphic adaptations of representation learning algorithms are now emerging, efficient processing of temporal sequences or variable length-inputs remain difficult. Recurrent neural networks (RNN) are widely used in machine learning to solve a variety of sequence learning tasks. In this work we present a train-and-constrain methodology that enables the mapping of machine learned (Elman) RNNs on a substrate of spiking neurons, while being compatible with the capabilities of current and near-future neuromorphic systems. This "train-and-constrain" method consists of first training RNNs using backpropagation through time, then discretizing the weights and finally converting them to spiking RNNs by matching the responses of artificial neurons with those of the spiking neurons. We demonstrate our approach by mapping a natural language processing task (question classification), where we demonstrate the entire mapping process of the recurrent layer of the network on IBM's Neurosynaptic System "TrueNorth", a spike-based digital neuromorphic hardware architecture. TrueNorth imposes specific constraints on connectivity, neural and synaptic parameters. To satisfy these constraints, it was necessary to discretize the synaptic weights and neural activities to 16 levels, and to limit fan-in to 64 inputs. We find that short synaptic delays are sufficient to implement the dynamical (temporal) aspect of the RNN in the question classification task. The hardware-constrained model achieved 74% accuracy in question classification while using less than 0.025% of the cores on one TrueNorth chip, resulting in an estimated power consumption of ~17 uW. version:1
arxiv-1601-04183 | TrueHappiness: Neuromorphic Emotion Recognition on TrueNorth | http://arxiv.org/abs/1601.04183 | id:1601.04183 author:Peter U. Diehl, Bruno U. Pedroni, Andrew Cassidy, Paul Merolla, Emre Neftci, Guido Zarrella category:q-bio.NC cs.NE  published:2016-01-16 summary:We present an approach to constructing a neuromorphic device that responds to language input by producing neuron spikes in proportion to the strength of the appropriate positive or negative emotional response. Specifically, we perform a fine-grained sentiment analysis task with implementations on two different systems: one using conventional spiking neural network (SNN) simulators and the other one using IBM's Neurosynaptic System TrueNorth. Input words are projected into a high-dimensional semantic space and processed through a fully-connected neural network (FCNN) containing rectified linear units trained via backpropagation. After training, this FCNN is converted to a SNN by substituting the ReLUs with integrate-and-fire neurons. We show that there is practically no performance loss due to conversion to a spiking network on a sentiment analysis test set, i.e. correlations between predictions and human annotations differ by less than 0.02 comparing the original DNN and its spiking equivalent. Additionally, we show that the SNN generated with this technique can be mapped to existing neuromorphic hardware -- in our case, the TrueNorth chip. Mapping to the chip involves 4-bit synaptic weight discretization and adjustment of the neuron thresholds. The resulting end-to-end system can take a user input, i.e. a word in a vocabulary of over 300,000 words, and estimate its sentiment on TrueNorth with a power consumption of approximately 50 uW. version:1
arxiv-1505-06389 | Image Segmentation Using Hierarchical Merge Tree | http://arxiv.org/abs/1505.06389 | id:1505.06389 author:Ting Liu, Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.CV  published:2015-05-24 summary:This paper investigates one of the most fundamental computer vision problems: image segmentation. We propose a supervised hierarchical approach to object-independent image segmentation. Starting with over-segmenting superpixels, we use a tree structure to represent the hierarchy of region merging, by which we reduce the problem of segmenting image regions to finding a set of label assignment to tree nodes. We formulate the tree structure as a constrained conditional model to associate region merging with likelihoods predicted using an ensemble boundary classifier. Final segmentations can then be inferred by finding globally optimal solutions to the model efficiently. We also present an iterative training and testing algorithm that generates various tree structures and combines them to emphasize accurate boundaries by segmentation accumulation. Experiment results and comparisons with other very recent methods on six public data sets demonstrate that our approach achieves the state-of-the-art region accuracy and is very competitive in image segmentation without semantic priors. version:2
arxiv-1512-08120 | Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning | http://arxiv.org/abs/1512.08120 | id:1512.08120 author:Fanhua Shang, James Cheng, Hong Cheng category:cs.LG cs.AI  published:2015-12-26 summary:Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0<p<\infty) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods. version:2
arxiv-1502-04874 | Regret bounds for Narendra-Shapiro bandit algorithms | http://arxiv.org/abs/1502.04874 | id:1502.04874 author:Sébastien Gadat, Fabien Panloup, Sofiane Saadane category:math.PR math.ST stat.ML stat.TH  published:2015-02-17 summary:Narendra-Shapiro (NS) algorithms are bandit-type algorithms that have been introduced in the sixties (with a view to applications in Psychology or learning automata), whose convergence has been intensively studied in the stochastic algorithm literature. In this paper, we adress the following question: are the Narendra-Shapiro (NS) bandit algorithms competitive from a \textit{regret} point of view? In our main result, we show that some competitive bounds can be obtained for such algorithms in their penalized version (introduced in \cite{Lamberton_Pages}). More precisely, up to an over-penalization modification, the pseudo-regret $\bar{R}_n$ related to the penalized two-armed bandit algorithm is uniformly bounded by $C \sqrt{n}$ (where $C$ is made explicit in the paper). \noindent We also generalize existing convergence and rates of convergence results to the multi-armed case of the over-penalized bandit algorithm, including the convergence toward the invariant measure of a Piecewise Deterministic Markov Process (PDMP) after a suitable renormalization. Finally, ergodic properties of this PDMP are given in the multi-armed case. version:2
arxiv-1601-04143 | Compositional Model based Fisher Vector Coding for Image Classification | http://arxiv.org/abs/1601.04143 | id:1601.04143 author:Lingqiao Liu, Peng Wang, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang, Heng Tao Shen category:cs.CV  published:2016-01-16 summary:Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to depict the generation process of local features. However, the representative power of the GMM could be limited because it essentially assumes that local features can be characterized by a fixed number of feature prototypes and the number of prototypes is usually small in FVC. To handle this limitation, in this paper we break the convention which assumes that a local feature is drawn from one of few Gaussian distributions. Instead, we adopt a compositional mechanism which assumes that a local feature is drawn from a Gaussian distribution whose mean vector is composed as the linear combination of multiple key components and the combination weight is a latent random variable. In this way, we can greatly enhance the representative power of the generative model of FVC. To implement our idea, we designed two particular generative models with such a compositional mechanism. version:1
arxiv-1502-06922 | Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval | http://arxiv.org/abs/1502.06922 | id:1502.06922 author:Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, Rabab Ward category:cs.CL cs.IR cs.LG cs.NE  published:2015-02-24 summary:This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task. version:3
arxiv-1601-04126 | Engineering Safety in Machine Learning | http://arxiv.org/abs/1601.04126 | id:1601.04126 author:Kush R. Varshney category:stat.ML cs.AI cs.CY cs.LG  published:2016-01-16 summary:Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software. version:1
arxiv-1506-02196 | Constrained Convex Neyman-Pearson Classification Using an Outer Approximation Splitting Method | http://arxiv.org/abs/1506.02196 | id:1506.02196 author:Michel Barlaud, Wafa Belhajali, Patrick L. Combettes, Lionel Fillatre category:math.ST stat.ML stat.TH  published:2015-06-06 summary:We propose an efficient splitting algorithm for solving Neyman-Pearson classification problems, which consist in minimizing the type II risk subject to an upper bound constraint on the type I risk. Since the 1/0 loss function is not convex, it is customary to replace it by convex surrogates that lead to manageable optimization problems. While statistical bounds have been be derived to quantify the cost of using such surrogates, no specific algorithm has yet been proposed to solve exactly the resulting constrained minimization problem and existing work has addressed only Langragian approximations. The contribution of this paper is to propose an efficient splitting algorithm to address this issue. Our method alternates a gradient step on the objective and a projection step onto the lower level set modeling the constraint. The projection step is implemented via an outer approximation scheme in which the constraint set is approximated by a sequence of simple convex sets consisting of the intersection of two half-spaces. Convergence of the iterates generated by the algorithm is established. Experiments on both synthetic and biological data show that our algorithm outperforms state of the art Lagrangian methods such as $\nu$-SVM. version:2
arxiv-1511-03034 | Learning with a Strong Adversary | http://arxiv.org/abs/1511.03034 | id:1511.03034 author:Ruitong Huang, Bing Xu, Dale Schuurmans, Csaba Szepesvari category:cs.LG  published:2015-11-10 summary:The robustness of neural networks to intended perturbations has recently attracted significant attention. In this paper, we propose a new method, \emph{learning with a strong adversary}, that learns robust classifiers from supervised data. The proposed method takes finding adversarial examples as an intermediate step. A new and simple way of finding adversarial examples is presented and experimentally shown to be efficient. Experimental results demonstrate that resulting learning method greatly improves the robustness of the classification models produced. version:6
arxiv-1601-04075 | Modification of Question Writing Style Influences Content Popularity in a Social Q&A System | http://arxiv.org/abs/1601.04075 | id:1601.04075 author:Igor A. Podgorny category:cs.IR cs.CL cs.SI  published:2016-01-15 summary:TurboTax AnswerXchange is a social Q&A system supporting users working on federal and state tax returns. Using 2015 data, we demonstrate that content popularity (or number of views per AnswerXchange question) can be predicted with reasonable accuracy based on attributes of the question alone. We also employ probabilistic topic analysis and uplift modeling to identify question features with the highest impact on popularity. We demonstrate that content popularity is driven by behavioral attributes of AnswerXchange users and depends on complex interactions between search ranking algorithms, psycholinguistic factors and question writing style. Our findings provide a rationale for employing popularity predictions to guide the users into formulating better questions and editing the existing ones. For example, starting question title with a question word or adding details to the question increase number of views per question. Similar approach can be applied to promoting AnswerXchange content indexed by Google to drive organic traffic to TurboTax. version:1
arxiv-1411-5620 | Maximum Entropy Kernels for System Identification | http://arxiv.org/abs/1411.5620 | id:1411.5620 author:Francesca Paola Carli, Tianshi Chen, Lennart Ljung category:math.OC cs.IT math.IT stat.ML  published:2014-11-20 summary:A new nonparametric approach for system identification has been recently proposed where the impulse response is modeled as the realization of a zero-mean Gaussian process whose covariance (kernel) has to be estimated from data. In this scheme, quality of the estimates crucially depends on the parametrization of the covariance of the Gaussian process. A family of kernels that have been shown to be particularly effective in the system identification framework is the family of Diagonal/Correlated (DC) kernels. Maximum entropy properties of a related family of kernels, the Tuned/Correlated (TC) kernels, have been recently pointed out in the literature. In this paper we show that maximum entropy properties indeed extend to the whole family of DC kernels. The maximum entropy interpretation can be exploited in conjunction with results on matrix completion problems in the graphical models literature to shed light on the structure of the DC kernel. In particular, we prove that the DC kernel admits a closed-form factorization, inverse and determinant. These results can be exploited both to improve the numerical stability and to reduce the computational complexity associated with the computation of the DC estimator. version:2
arxiv-1601-04033 | Faster Asynchronous SGD | http://arxiv.org/abs/1601.04033 | id:1601.04033 author:Augustus Odena category:stat.ML cs.LG  published:2016-01-15 summary:Asynchronous distributed stochastic gradient descent methods have trouble converging because of stale gradients. A gradient update sent to a parameter server by a client is stale if the parameters used to calculate that gradient have since been updated on the server. Approaches have been proposed to circumvent this problem that quantify staleness in terms of the number of elapsed updates. In this work, we propose a novel method that quantifies staleness in terms of moving averages of gradient statistics. We show that this method outperforms previous methods with respect to convergence speed and scalability to many clients. We also discuss how an extension to this method can be used to dramatically reduce bandwidth costs in a distributed training context. In particular, our method allows reduction of total bandwidth usage by a factor of 5 with little impact on cost convergence. We also describe (and link to) a software library that we have used to simulate these algorithms deterministically on a single machine. version:1
arxiv-1601-04012 | Detecting and Extracting Events from Text Documents | http://arxiv.org/abs/1601.04012 | id:1601.04012 author:Jugal Kalita category:cs.CL  published:2016-01-15 summary:Events of various kinds are mentioned and discussed in text documents, whether they are books, news articles, blogs or microblog feeds. The paper starts by giving an overview of how events are treated in linguistics and philosophy. We follow this discussion by surveying how events and associated information are handled in computationally. In particular, we look at how textual documents can be mined to extract events and ancillary information. These days, it is mostly through the application of various machine learning techniques. We also discuss applications of event detection and extraction systems, particularly in summarization, in the medical domain and in the context of Twitter posts. We end the paper with a discussion of challenges and future directions. version:1
arxiv-1601-03958 | Real-Time Association Mining in Large Social Networks | http://arxiv.org/abs/1601.03958 | id:1601.03958 author:Benjamin Paul Chamberlain, Josh Levy-Kramer, Clive Humby, Marc Peter Deisenroth category:cs.SI stat.ML  published:2016-01-15 summary:There is a growing realisation that to combat the waning effectiveness of traditional marketing, social media platform owners need to find new ways to monetise their data. Social media data contains rich information describing how real world entities relate to each other. Understanding the allegiances, communities and structure of key entities is of vital importance for decision support in a swathe of industries that have hitherto relied on expensive, small scale survey data. In this paper, we present a real-time method to query and visualise regions of networks that are closely related to a set of input vertices. The input vertices can define an industry, political party, sport etc. The key idea is that in large digital social networks measuring similarity via direct connections between nodes is not robust, but that robust similarities between nodes can be attained through the similarity of their neighbourhood graphs. We are able to achieve real-time performance by compressing the neighbourhood graphs using minhash signatures and facilitate rapid queries through Locality Sensitive Hashing. These techniques reduce query times from hours using industrial desktop machines to milliseconds on standard laptops. Our method allows analysts to interactively explore strongly associated regions of large networks in real time. Our work has been deployed in software that is actively used by analysts to understand social network structure. version:1
arxiv-1601-03945 | Improved graph-based SFA: Information preservation complements the slowness principle | http://arxiv.org/abs/1601.03945 | id:1601.03945 author:Alberto N. Escalante-B., Laurenz Wiskott category:cs.CV cs.LG stat.ML  published:2016-01-15 summary:Slow feature analysis (SFA) is an unsupervised-learning algorithm that extracts slowly varying features from a multi-dimensional time series. A supervised extension to SFA for classification and regression is graph-based SFA (GSFA). GSFA is based on the preservation of similarities, which are specified by a graph structure derived from the labels. It has been shown that hierarchical GSFA (HGSFA) allows learning from images and other high-dimensional data. The feature space spanned by HGSFA is complex due to the composition of the nonlinearities of the nodes in the network. However, we show that the network discards useful information prematurely before it reaches higher nodes, resulting in suboptimal global slowness and an under-exploited feature space. To counteract these problems, we propose an extension called hierarchical information-preserving GSFA (HiGSFA), where information preservation complements the slowness-maximization goal. We build a 10-layer HiGSFA network to estimate human age from facial photographs of the MORPH-II database, achieving a mean absolute error of 3.50 years, improving the state-of-the-art performance. HiGSFA and HGSFA support multiple-labels and offer a rich feature space, feed-forward training, and linear complexity in the number of samples and dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature slowness, estimation accuracy and input reconstruction, giving rise to a promising hierarchical supervised-learning approach. version:1
arxiv-1601-03896 | Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures | http://arxiv.org/abs/1601.03896 | id:1601.03896 author:Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara Plank category:cs.CL cs.CV  published:2016-01-15 summary:Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation. version:1
arxiv-1601-03855 | A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits | http://arxiv.org/abs/1601.03855 | id:1601.03855 author:Pratik Gajane, Tanguy Urvoy, Fabrice Clérot category:cs.LG  published:2016-01-15 summary:We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications. version:1
arxiv-1601-03822 | On the consistency of inversion-free parameter estimation for Gaussian random fields | http://arxiv.org/abs/1601.03822 | id:1601.03822 author:Hossein Keshavarz, Clayton Scott, XuanLong Nguyen category:math.ST cs.LG stat.ML stat.TH  published:2016-01-15 summary:Gaussian random fields are a powerful tool for modeling environmental processes. For high dimensional samples, classical approaches for estimating the covariance parameters require highly challenging and massive computations, such as the evaluation of the Cholesky factorization or solving linear systems. Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast and scalable algorithm which does not need such burdensome computations. The main focus of this article is to study the asymptotic behavior of the algorithm of Anitescu et al. (ACS) for regular and irregular grids in the increasing domain setting. Consistency, minimax optimality and asymptotic normality of this algorithm are proved under mild differentiability conditions on the covariance function. Despite the fact that ACS's method entails a non-concave maximization, our results hold for any stationary point of the objective function. A numerical study is presented to evaluate the efficiency of this algorithm for large data sets. version:1
arxiv-1502-04434 | Invariant backpropagation: how to train a transformation-invariant neural network | http://arxiv.org/abs/1502.04434 | id:1502.04434 author:Sergey Demyanov, James Bailey, Ramamohanarao Kotagiri, Christopher Leckie category:stat.ML cs.LG cs.NE  published:2015-02-16 summary:In many classification problems a classifier should be robust to small variations in the input vector. This is a desired property not only for particular transformations, such as translation and rotation in image classification problems, but also for all others for which the change is small enough to retain the object perceptually indistinguishable. We propose two extensions of the backpropagation algorithm that train a neural network to be robust to variations in the feature vector. While the first of them enforces robustness of the loss function to all variations, the second method trains the predictions to be robust to a particular variation which changes the loss function the most. The second methods demonstrates better results, but is slightly slower. We analytically compare the proposed algorithm with two the most similar approaches (Tangent BP and Adversarial Training), and propose their fast versions. In the experimental part we perform comparison of all algorithms in terms of classification accuracy and robustness to noise on MNIST and CIFAR-10 datasets. Additionally we analyze how the performance of the proposed algorithm depends on the dataset size and data augmentation. version:3
arxiv-1601-03805 | Matrix Neural Networks | http://arxiv.org/abs/1601.03805 | id:1601.03805 author:Junbin Gao, Yi Guo, Zhiyong Wang category:cs.LG  published:2016-01-15 summary:Traditional neural networks assume vectorial inputs as the network is arranged as layers of single line of computing units called neurons. This special structure requires the non-vectorial inputs such as matrices to be converted into vectors. This process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Secondly, the solution space becomes very large which demands very special treatments to the network parameters and high computational cost. To address these issues, we propose matrix neural networks (MatNet), which takes matrices directly as inputs. Each neuron senses summarised information through bilinear mapping from lower layer units in exactly the same way as the classic feed forward neural networks. Under this structure, back prorogation and gradient descent combination can be utilised to obtain network parameters efficiently. Furthermore, it can be conveniently extended for multimodal inputs. We apply MatNet to MNIST handwritten digits classification and image super resolution tasks to show its effectiveness. Without too much tweaking MatNet achieves comparable performance as the state-of-the-art methods in both tasks with considerably reduced complexity. version:1
arxiv-1601-03797 | ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models | http://arxiv.org/abs/1601.03797 | id:1601.03797 author:Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J. Franklin, Ken Goldberg category:cs.DB cs.LG  published:2016-01-15 summary:Data cleaning is often an important step to ensure that predictive models, such as regression and classification, are not affected by systematic errors such as inconsistent, out-of-date, or outlier data. Identifying dirty data is often a manual and iterative process, and can be challenging on large datasets. However, many data cleaning workflows can introduce subtle biases into the training processes due to violation of independence assumptions. We propose ActiveClean, a progressive cleaning approach where the model is updated incrementally instead of re-training and can guarantee accuracy on partially cleaned data. ActiveClean supports a popular class of models called convex loss models (e.g., linear regression and SVMs). ActiveClean also leverages the structure of a user's model to prioritize cleaning those records likely to affect the results. We evaluate ActiveClean on five real-world datasets UCI Adult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real and synthetic errors. Our results suggest that our proposed optimizations can improve model accuracy by up-to 2.5x for the same amount of data cleaned. Furthermore for a fixed cleaning budget and on all real dirty datasets, ActiveClean returns more accurate models than uniform sampling and Active Learning. version:1
arxiv-1601-03783 | Towards Turkish ASR: Anatomy of a rule-based Turkish g2p | http://arxiv.org/abs/1601.03783 | id:1601.03783 author:Duygu Altinok category:cs.CL  published:2016-01-15 summary:This paper describes the architecture and implementation of a rule-based grapheme to phoneme converter for Turkish. The system accepts surface form as input, outputs SAMPA mapping of the all parallel pronounciations according to the morphological analysis together with stress positions. The system has been implemented in Python version:1
arxiv-1403-2065 | Categorization Axioms for Clustering Results | http://arxiv.org/abs/1403.2065 | id:1403.2065 author:Jian Yu, Zongben Xu category:cs.LG  published:2014-03-09 summary:Cluster analysis has attracted more and more attention in the field of machine learning and data mining. Numerous clustering algorithms have been proposed and are being developed due to diverse theories and various requirements of emerging applications. Therefore, it is very worth establishing an unified axiomatic framework for data clustering. In the literature, it is an open problem and has been proved very challenging. In this paper, clustering results are axiomatized by assuming that an proper clustering result should satisfy categorization axioms. The proposed axioms not only introduce classification of clustering results and inequalities of clustering results, but also are consistent with prototype theory and exemplar theory of categorization models in cognitive science. Moreover, the proposed axioms lead to three principles of designing clustering algorithm and cluster validity index, which follow many popular clustering algorithms and cluster validity indices. version:8
arxiv-1408-0196 | A Blind Adaptive CDMA Receiver Based on State Space Structures | http://arxiv.org/abs/1408.0196 | id:1408.0196 author:Zaid Albataineh, Fathi M. Salem category:cs.IT cs.LG math.IT  published:2014-08-01 summary:Code Division Multiple Access (CDMA) is a channel access method, based on spread-spectrum technology, used by various radio technologies world-wide. In general, CDMA is used as an access method in many mobile standards such as CDMA2000 and WCDMA. We address the problem of blind multiuser equalization in the wideband CDMA system, in the noisy multipath propagation environment. Herein, we propose three new blind receiver schemes, which are based on state space structures and Independent Component Analysis (ICA). These blind state-space receivers (BSSR) do not require knowledge of the propagation parameters or spreading code sequences of the users they primarily exploit the natural assumption of statistical independence among the source signals. We also develop three semi blind adaptive detectors by incorporating the new adaptive methods into the standard RAKE receiver structure. Extensive comparative case study, based on Bit error rate (BER) performance of these methods, is carried out for different number of users, symbols per user, and signal to noise ratio (SNR) in comparison with conventional detectors, including the Blind Multiuser Detectors (BMUD) and Linear Minimum mean squared error (LMMSE). The results show that the proposed methods outperform the other detectors in estimating the symbol signals from the received mixed CDMA signals. Moreover, the new blind detectors mitigate the multi access interference (MAI) in CDMA. version:2
arxiv-1601-03769 | Generation of a Supervised Classification Algorithm for Time-Series Variable Stars with an Application to the LINEAR Dataset | http://arxiv.org/abs/1601.03769 | id:1601.03769 author:Kyle B Johnston, Hakeem M Oluseyi category:astro-ph.IM cs.LG  published:2016-01-14 summary:With the advent of digital astronomy, new benefits and new problems have been presented to the modern day astronomer. While data can be captured in a more efficient and accurate manor using digital means, the efficiency of data retrieval has led to an overload of scientific data for processing and storage. This paper will focus on the construction and application of a supervised pattern classification algorithm for the identification of variable stars. Given the reduction of a survey of stars into a standard feature space, the problem of using prior patterns to identify new observed patterns can be reduced to time tested classification methodologies and algorithms. Such supervised methods, so called because the user trains the algorithms prior to application using patterns with known classes or labels, provide a means to probabilistically determine the estimated class type of new observations. This paper will demonstrate the construction and application of a supervised classification algorithm on variable star data. The classifier is applied to a set of 192,744 LINEAR data points. Of the original samples, 34,451 unique stars were classified with high confidence (high level of probability of being the true class). version:1
arxiv-1601-03764 | Linear Algebraic Structure of Word Senses, with Applications to Polysemy | http://arxiv.org/abs/1601.03764 | id:1601.03764 author:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski category:cs.CL cs.LG stat.ML  published:2016-01-14 summary:Word embeddings are ubiquitous in NLP and information retrieval, but it's unclear what they represent when the word is polysemous, i.e., has multiple senses. Here it is shown that multiple word senses reside in linear superposition within the word embedding and can be recovered by simple sparse coding. The success of the method ---which applies to several embedding methods including word2vec--- is mathematically explained using the random walk on discourses model (Arora et al., 2015). A novel aspect of our technique is that each word sense is also accompanied by one of about 2000 "discourse atoms" that give a succinct description of which other words co-occur with that word sense. Discourse atoms seem of independent interest, and make the method potentially more useful than the traditional clustering-based approaches to polysemy. version:1
arxiv-1601-03754 | Dual-tree $k$-means with bounded iteration runtime | http://arxiv.org/abs/1601.03754 | id:1601.03754 author:Ryan R. Curtin category:cs.DS cs.LG  published:2016-01-14 summary:k-means is a widely used clustering algorithm, but for $k$ clusters and a dataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time. Although there are existing techniques to accelerate single Lloyd iterations, none of these are tailored to the case of large $k$, which is increasingly common as dataset sizes grow. We propose a dual-tree algorithm that gives the exact same results as standard $k$-means; when using cover trees, we use adaptive analysis techniques to, under some assumptions, bound the single-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledge these are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then show that this theoretically favorable algorithm performs competitively in practice, especially for large $N$ and $k$ in low dimensions. Further, the algorithm is tree-independent, so any type of tree may be used. version:1
arxiv-1510-01006 | Monitoring Potential Drug Interactions and Reactions via Network Analysis of Instagram User Timelines | http://arxiv.org/abs/1510.01006 | id:1510.01006 author:Rion Brattig Correia, Lang Li, Luis M. Rocha category:cs.SI cs.CY cs.IR q-bio.QM stat.ML  published:2015-10-05 summary:Much recent research aims to identify evidence for Drug-Drug Interactions (DDI) and Adverse Drug reactions (ADR) from the biomedical scientific literature. In addition to this "Bibliome", the universe of social media provides a very promising source of large-scale data that can help identify DDI and ADR in ways that have not been hitherto possible. Given the large number of users, analysis of social media data may be useful to identify under-reported, population-level pathology associated with DDI, thus further contributing to improvements in population health. Moreover, tapping into this data allows us to infer drug interactions with natural products--including cannabis--which constitute an array of DDI very poorly explored by biomedical research thus far. Our goal is to determine the potential of Instagram for public health monitoring and surveillance for DDI, ADR, and behavioral pathology at large. Using drug, symptom, and natural product dictionaries for identification of the various types of DDI and ADR evidence, we have collected ~7000 timelines. We report on 1) the development of a monitoring tool to easily observe user-level timelines associated with drug and symptom terms of interest, and 2) population-level behavior via the analysis of co-occurrence networks computed from user timelines at three different scales: monthly, weekly, and daily occurrences. Analysis of these networks further reveals 3) drug and symptom direct and indirect associations with greater support in user timelines, as well as 4) clusters of symptoms and drugs revealed by the collective behavior of the observed population. This demonstrates that Instagram contains much drug- and pathology specific data for public health monitoring of DDI and ADR, and that complex network analysis provides an important toolbox to extract health-related associations and their support from large-scale social media data. version:2
arxiv-1503-01626 | Inference of hidden structures in complex physical systems by multi-scale clustering | http://arxiv.org/abs/1503.01626 | id:1503.01626 author:Z. Nussinov, P. Ronhovde, Dandan Hu, S. Chakrabarty, M. Sahu, Bo Sun, N. A. Mauro, K. K. Sahu category:cond-mat.mtrl-sci cond-mat.stat-mech cs.CV physics.data-an  published:2015-03-05 summary:We survey the application of a relatively new branch of statistical physics--"community detection"-- to data mining. In particular, we focus on the diagnosis of materials and automated image segmentation. Community detection describes the quest of partitioning a complex system involving many elements into optimally decoupled subsets or communities of such elements. We review a multiresolution variant which is used to ascertain structures at different spatial and temporal scales. Significant patterns are obtained by examining the correlations between different independent solvers. Similar to other combinatorial optimization problems in the NP complexity class, community detection exhibits several phases. Typically, illuminating orders are revealed by choosing parameters that lead to extremal information theory correlations. version:2
arxiv-1601-03679 | Dynamic Concept Composition for Zero-Example Event Detection | http://arxiv.org/abs/1601.03679 | id:1601.03679 author:Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang, Alexander G. Hauptmann category:cs.CV  published:2016-01-14 summary:In this paper, we focus on automatically detecting events in unconstrained videos without the use of any visual training exemplars. In principle, zero-shot learning makes it possible to train an event detection model based on the assumption that events (e.g. \emph{birthday party}) can be described by multiple mid-level semantic concepts (e.g. "blowing candle", "birthday cake"). Towards this goal, we first pre-train a bundle of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept \wrt the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. While most existing systems combine the predictions of the concept classifiers with fixed weights, we propose to learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. To validate the effectiveness of the proposed approach, we have conducted extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset. The experimental results confirm the superiority of the proposed approach. version:1
arxiv-1510-00331 | Multimodal Hierarchical Dirichlet Process-based Active Perception | http://arxiv.org/abs/1510.00331 | id:1510.00331 author:Tadahiro Taniguchi, Toshiaki Takano, Ryo Yoshino category:cs.RO cs.AI stat.ML  published:2015-10-01 summary:In this paper, we propose an active perception method for recognizing object categories based on the multimodal hierarchical Dirichlet process (MHDP). The MHDP enables a robot to form object categories using multimodal information, e.g., visual, auditory, and haptic information, which can be observed by performing actions on an object. However, performing many actions on a target object requires a long time. In a real-time scenario, i.e., when the time is limited, the robot has to determine the set of actions that is most effective for recognizing a target object. We propose an MHDP-based active perception method that uses the information gain (IG) maximization criterion and lazy greedy algorithm. We show that the IG maximization criterion is optimal in the sense that the criterion is equivalent to a minimization of the expected Kullback--Leibler divergence between a final recognition state and the recognition state after the next set of actions. However, a straightforward calculation of IG is practically impossible. Therefore, we derive an efficient Monte Carlo approximation method for IG by making use of a property of the MHDP. We also show that the IG has submodular and non-decreasing properties as a set function because of the structure of the graphical model of the MHDP. Therefore, the IG maximization problem is reduced to a submodular maximization problem. This means that greedy and lazy greedy algorithms are effective and have a theoretical justification for their performance. We conducted an experiment using an upper-torso humanoid robot and a second one using synthetic data. The experimental results show that the method enables the robot to select a set of actions that allow it to recognize target objects quickly and accurately. The results support our theoretical outcomes. version:3
arxiv-1511-06381 | Manifold Regularized Deep Neural Networks using Adversarial Examples | http://arxiv.org/abs/1511.06381 | id:1511.06381 author:Taehoon Lee, Minsuk Choi, Sungroh Yoon category:cs.LG cs.CV  published:2015-11-19 summary:Learning meaningful representations using deep neural networks involves designing efficient training schemes and well-structured networks. Currently, the method of stochastic gradient descent that has a momentum with dropout is one of the most popular training protocols. Based on that, more advanced methods (i.e., Maxout and Batch Normalization) have been proposed in recent years, but most still suffer from performance degradation caused by small perturbations, also known as adversarial examples. To address this issue, we propose manifold regularized networks (MRnet) that utilize a novel training objective function that minimizes the difference between multi-layer embedding results of samples and those adversarial. Our experimental results demonstrated that MRnet is more resilient to adversarial examples and helps us to generalize representations on manifolds. Furthermore, combining MRnet and dropout allowed us to achieve competitive classification performances for three well-known benchmarks: MNIST, CIFAR-10, and SVHN. version:2
arxiv-1601-03651 | Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation | http://arxiv.org/abs/1601.03651 | id:1601.03651 author:Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin category:cs.CL cs.LG  published:2016-01-14 summary:Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent, compared with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolution neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluate our DRNNs on the SemEval-2010 Task 8, and achieve an $F_1$-score of 85.81%, outperforming state-of-the-art recorded results. version:1
arxiv-1601-03649 | Optimal Supervised Learning in Spiking Neural Networks for Precise Temporal Encoding | http://arxiv.org/abs/1601.03649 | id:1601.03649 author:Brian Gardner, André Grüning category:cs.NE q-bio.NC  published:2016-01-14 summary:Precise spike timing as a means to encode information in neural networks is biologically supported, and is advantageous over frequency-based codes by processing input features on a much shorter time-scale. For these reasons, much recent attention has been focused on the development of supervised learning rules for spiking neural networks that utilise a temporal coding scheme. However, despite significant progress in this area, there still lack rules that are theoretically justified, and yet can be considered biologically relevant. Here we examine the general conditions under which optimal synaptic plasticity takes place to support the supervised learning of a precise temporal code. As part of our analysis we introduce two analytically derived learning rules, one of which relies on an instantaneous error signal to optimise synaptic weights in a network (INST rule), and the other one relying on a filtered error signal to minimise the variance of synaptic weight modifications (FILT rule). We test the optimality of the solutions provided by each rule with respect to their temporal encoding precision, and then measure the maximum number of input patterns they can learn to memorise using the precise timings of individual spikes. Our results demonstrate the optimality of the FILT rule in most cases, underpinned by the rule's error-filtering mechanism which provides smooth convergence during learning. We also find the FILT rule to be most efficient at performing input pattern memorisations, and most noticeably when patterns are identified using spikes with sub-millisecond temporal precision. In comparison with existing work, we determine the performance of the FILT rule to be consistent with that of the highly efficient E-learning Chronotron rule, but with the distinct advantage that our FILT rule is also implementable as an online method for increased biological realism. version:1
arxiv-1511-04601 | Jointly Learning Non-negative Projection and Dictionary with Discriminative Graph Constraints for Classification | http://arxiv.org/abs/1511.04601 | id:1511.04601 author:Weiyang Liu, Zhiding Yu, Yingzhen Yang, Meng Yang, Thomas S. Huang category:cs.CV  published:2015-11-14 summary:Dictionary learning (DL) for sparse coding has shown impressive performance in classification tasks. But how to select a feature that can best work with the learned dictionary remains an open question. Current prevailing DL methods usually adopt existing well-performing features, ignoring the inner relationship between dictionaries and features. To address the problem, we propose a joint non-negative projection and dictionary learning (JNPDL) method. Non-negative projection learning and dictionary learning are complementary to each other, since the former leads to the intrinsic discriminative parts-based features for objects while the latter searches a suitable representation in the projected feature space. Specifically, discrimination of projection and dictionary is achieved by imposing to both projection and coding coefficients a graph constraint that maximizes the intra-class compactness and inter-class separability. Experimental results on both image classification and image set classification show the excellent performance of JNPDL by being comparable or outperforming many state-of-the-art approaches. version:3
arxiv-1510-03710 | Hybrid Dialog State Tracker | http://arxiv.org/abs/1510.03710 | id:1510.03710 author:Miroslav Vodolán, Rudolf Kadlec, Jan Kleindienst category:cs.CL  published:2015-10-13 summary:This paper presents a hybrid dialog state tracker that combines a rule based and a machine learning based approach to belief state tracking. Therefore, we call it a hybrid tracker. The machine learning in our tracker is realized by a Long Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker sets a new state-of-the-art result for the Dialog State Tracking Challenge (DSTC) 2 dataset when the system uses only live SLU as its input. version:3
arxiv-1601-03531 | Quantification of Ultrasonic Texture heterogeneity via Volumetric Stochastic Modeling for Tissue Characterization | http://arxiv.org/abs/1601.03531 | id:1601.03531 author:O. S. Al-Kadi, Daniel Y. F. Chung, Robert C. Carlisle, Constantin C. Coussios, J. Alison Noble category:cs.CV  published:2016-01-14 summary:Intensity variations in image texture can provide powerful quantitative information about physical properties of biological tissue. However, tissue patterns can vary according to the utilized imaging system and are intrinsically correlated to the scale of analysis. In the case of ultrasound, the Nakagami distribution is a general model of the ultrasonic backscattering envelope under various scattering conditions and densities where it can be employed for characterizing image texture, but the subtle intra-heterogeneities within a given mass are difficult to capture via this model as it works at a single spatial scale. This paper proposes a locally adaptive 3D multi-resolution Nakagami-based fractal feature descriptor that extends Nakagami-based texture analysis to accommodate subtle speckle spatial frequency tissue intensity variability in volumetric scans. Local textural fractal descriptors - which are invariant to affine intensity changes - are extracted from volumetric patches at different spatial resolutions from voxel lattice-based generated shape and scale Nakagami parameters. Using ultrasound radio-frequency datasets we found that after applying an adaptive fractal decomposition label transfer approach on top of the generated Nakagami voxels, tissue characterization results were superior to the state of art. Experimental results on real 3D ultrasonic pre-clinical and clinical datasets suggest that describing tumor intra-heterogeneity via this descriptor may facilitate improved prediction of therapy response and disease characterization. version:1
arxiv-1512-04009 | Quantum Privacy-Preserving Data Mining | http://arxiv.org/abs/1512.04009 | id:1512.04009 author:Shenggang Ying, Mingsheng Ying, Yuan Feng category:quant-ph cs.CR cs.DB cs.LG  published:2015-12-13 summary:Data mining is a key technology in big data analytics and it can discover understandable knowledge (patterns) hidden in large data sets. Association rule is one of the most useful knowledge patterns, and a large number of algorithms have been developed in the data mining literature to generate association rules corresponding to different problems and situations. Privacy becomes a vital issue when data mining is used to sensitive data sets like medical records, commercial data sets and national security. In this Letter, we present a quantum protocol for mining association rules on vertically partitioned databases. The quantum protocol can improve the privacy level preserved by known classical protocols and at the same time it can exponentially reduce the computational complexity and communication cost. version:2
arxiv-1504-03293 | Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction | http://arxiv.org/abs/1504.03293 | id:1504.03293 author:Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, Honglak Lee category:cs.CV  published:2015-04-13 summary:Object detection systems based on the deep convolutional neural network (CNN) have recently made ground- breaking advances on several object detection benchmarks. While the features learned by these high-capacity neural networks are discriminative for categorization, inaccurate localization is still a major source of error for detection. Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box, and 2) training the CNN with a structured loss that explicitly penalizes the localization inaccuracy. In experiments, we demonstrated that each of the proposed methods improves the detection performance over the baseline method on PASCAL VOC 2007 and 2012 datasets. Furthermore, two methods are complementary and significantly outperform the previous state-of-the-art when combined. version:3
arxiv-1512-02413 | Tracking Objects with Higher Order Interactions using Delayed Column Generation | http://arxiv.org/abs/1512.02413 | id:1512.02413 author:Steffen Wolf, Fred A. Hamprecht, Julian Yarkony category:cs.CV  published:2015-12-08 summary:We introduce a new approach to tracking a large number of objects. Specifically we consider tracking in the context of higher order Markov interactions which can not be modeled using network flow techniques as currently developed. Our approach relies on delayed column generation and is inspired by the corresponding approach to the cutting stock problem. Columns can be generated exactly using dynamic programming. version:2
arxiv-1506-08536 | A simple yet efficient algorithm for multiple kernel learning under elastic-net constraints | http://arxiv.org/abs/1506.08536 | id:1506.08536 author:Luca Citi category:stat.ML cs.LG  published:2015-06-29 summary:This report presents an algorithm for the solution of multiple kernel learning (MKL) problems with elastic-net constraints on the kernel weights. version:2
arxiv-1601-03375 | Multi-Atlas Segmentation with Joint Label Fusion of Osteoporotic Vertebral Compression Fractures on CT | http://arxiv.org/abs/1601.03375 | id:1601.03375 author:Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers category:cs.CV  published:2016-01-13 summary:The precise and accurate segmentation of the vertebral column is essential in the diagnosis and treatment of various orthopedic, neurological, and oncological traumas and pathologies. Segmentation is especially challenging in the presence of pathology such as vertebral compression fractures. In this paper, we propose a method to produce segmentations for osteoporotic compression fractured vertebrae by applying a multi-atlas joint label fusion technique for clinical CT images. A total of 170 thoracic and lumbar vertebrae were evaluated using atlases from five patients with varying degrees of spinal degeneration. In an osteoporotic cohort of bundled atlases, registration provided an average Dice coefficient and mean absolute surface distance of 2.7$\pm$4.5% and 0.32$\pm$0.13mm for osteoporotic vertebrae, respectively, and 90.9$\pm$3.0% and 0.36$\pm$0.11mm for compression fractured vertebrae. version:1
arxiv-1601-03348 | EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations | http://arxiv.org/abs/1601.03348 | id:1601.03348 author:Kayhan Moharreri, Minsu Ha, Ross H Nehm category:cs.CL  published:2016-01-13 summary:EvoGrader is a free, online, on-demand formative assessment service designed for use in undergraduate biology classrooms. EvoGrader's web portal is powered by Amazon's Elastic Cloud and run with LightSIDE Lab's open-source machine-learning tools. The EvoGrader web portal allows biology instructors to upload a response file (.csv) containing unlimited numbers of evolutionary explanations written in response to 86 different ACORNS (Assessing COntextual Reasoning about Natural Selection) instrument items. The system automatically analyzes the responses and provides detailed information about the scientific and naive concepts contained within each student's response, as well as overall student (and sample) reasoning model types. Graphs and visual models provided by EvoGrader summarize class-level responses; downloadable files of raw scores (in .csv format) are also provided for more detailed analyses. Although the computational machinery that EvoGrader employs is complex, using the system is easy. Users only need to know how to use spreadsheets to organize student responses, upload files to the web, and use a web browser. A series of experiments using new samples of 2,200 written evolutionary explanations demonstrate that EvoGrader scores are comparable to those of trained human raters, although EvoGrader scoring takes 99% less time and is free. EvoGrader will be of interest to biology instructors teaching large classes who seek to emphasize scientific practices such as generating scientific explanations, and to teach crosscutting ideas such as evolution and natural selection. The software architecture of EvoGrader is described as it may serve as a template for developing machine-learning portals for other core concepts within biology and across other disciplines. version:1
arxiv-1601-03333 | A Score-level Fusion Method for Eye Movement Biometrics | http://arxiv.org/abs/1601.03333 | id:1601.03333 author:Anjith George, Aurobinda Routray category:cs.CV  published:2016-01-13 summary:This paper proposes a novel framework for the use of eye movement patterns for biometric applications. Eye movements contain abundant information about cognitive brain functions, neural pathways, etc. In the proposed method, eye movement data is classified into fixations and saccades. Features extracted from fixations and saccades are used by a Gaussian Radial Basis Function Network (GRBFN) based method for biometric authentication. A score fusion approach is adopted to classify the data in the output layer. In the evaluation stage, the algorithm has been tested using two types of stimuli: random dot following on a screen and text reading. The results indicate the strength of eye movement pattern as a biometric modality. The algorithm has been evaluated on BioEye 2015 database and found to outperform all the other methods. Eye movements are generated by a complex oculomotor plant which is very hard to spoof by mechanical replicas. Use of eye movement dynamics along with iris recognition technology may lead to a robust counterfeit-resistant person identification system. version:1
arxiv-1601-03323 | Localized Dictionary design for Geometrically Robust Sonar ATR | http://arxiv.org/abs/1601.03323 | id:1601.03323 author:John McKay, Vishal Monga, Raghu Raj category:cs.CV  published:2016-01-13 summary:Advancements in Sonar image capture have opened the door to powerful classification schemes for automatic target recognition (ATR. Recent work has particularly seen the application of sparse reconstruction-based classification (SRC) to sonar ATR, which provides compelling accuracy rates even in the presence of noise and blur. Existing sparsity based sonar ATR techniques however assume that the test images exhibit geometric pose that is consistent with respect to the training set. This work addresses the outstanding open challenge of handling inconsistently posed test sonar images relative to training. We develop a new localized block-based dictionary design that can enable geometric, i.e. pose robustness. Further, a dictionary learning method is incorporated to increase performance and efficiency. The proposed SRC with Localized Pose Management (LPM), is shown to outperform the state of the art SIFT feature and SVM approach, due to its power to discern background clutter in Sonar images. version:1
arxiv-1601-01892 | Song Recommendation with Non-Negative Matrix Factorization and Graph Total Variation | http://arxiv.org/abs/1601.01892 | id:1601.01892 author:Kirell Benzi, Vassilis Kalofolias, Xavier Bresson, Pierre Vandergheynst category:stat.ML cs.IR cs.LG physics.data-an  published:2016-01-08 summary:This work formulates a novel song recommender system as a matrix completion problem that benefits from collaborative filtering through Non-negative Matrix Factorization (NMF) and content-based filtering via total variation (TV) on graphs. The graphs encode both playlist proximity information and song similarity, using a rich combination of audio, meta-data and social features. As we demonstrate, our hybrid recommendation system is very versatile and incorporates several well-known methods while outperforming them. Particularly, we show on real-world data that our model overcomes w.r.t. two evaluation metrics the recommendation of models solely based on low-rank information, graph-based information or a combination of both. version:2
arxiv-1601-03295 | Document image classification, with a specific view on applications of patent images | http://arxiv.org/abs/1601.03295 | id:1601.03295 author:Gabriela Csurka category:cs.CV  published:2016-01-13 summary:The main focus of this paper is document image classification and retrieval, where we analyze and compare different parameters for the RunLeght Histogram (RL) and Fisher Vector (FV) based image representations. We do an exhaustive experimental study using different document image datasets, including the MARG benchmarks, two datasets built on customer data and the images from the Patent Image Classification task of the Clef-IP 2011. The aim of the study is to give guidelines on how to best choose the parameters such that the same features perform well on different tasks. As an example of such need, we describe the Image-based Patent Retrieval task's of Clef-IP 2011, where we used the same image representation to predict the image type and retrieve relevant patents. version:1
arxiv-1601-03288 | Predicting the Effectiveness of Self-Training: Application to Sentiment Classification | http://arxiv.org/abs/1601.03288 | id:1601.03288 author:Vincent Van Asch, Walter Daelemans category:cs.CL  published:2016-01-13 summary:The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with. version:1
arxiv-1601-03239 | Digital Image Forensics vs. Image Composition: An Indirect Arms Race | http://arxiv.org/abs/1601.03239 | id:1601.03239 author:Victor Schetinger, Massimo Iuliani, Alessandro Piva, Manuel M. Oliveira category:cs.CV cs.MM  published:2016-01-13 summary:The field of image composition is constantly trying to improve the ways in which an image can be altered and enhanced. While this is usually done in the name of aesthetics and practicality, it also provides tools that can be used to maliciously alter images. In this sense, the field of digital image forensics has to be prepared to deal with the influx of new technology, in a constant arms-race. In this paper, the current state of this arms-race is analyzed, surveying the state-of-the-art and providing means to compare both sides. A novel scale to classify image forensics assessments is proposed, and experiments are performed to test composition techniques in regards to different forensics traces. We show that even though research in forensics seems unaware of the advanced forms of image composition, it possesses the basic tools to detect it. version:1
arxiv-1507-04576 | Multi-Face Tracking by Extended Bag-of-Tracklets in Egocentric Videos | http://arxiv.org/abs/1507.04576 | id:1507.04576 author:Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva category:cs.CV  published:2015-07-16 summary:Wearable cameras offer a hands-free way to record egocentric images of daily experiences, where social events are of special interest. The first step towards detection of social events is to track the appearance of multiple persons involved in it. In this paper, we propose a novel method to find correspondences of multiple faces in low temporal resolution egocentric videos acquired through a wearable camera. This kind of photo-stream imposes additional challenges to the multi-tracking problem with respect to conventional videos. Due to the free motion of the camera and to its low temporal resolution, abrupt changes in the field of view, in illumination condition and in the target location are highly frequent. To overcome such difficulties, we propose a multi-face tracking method that generates a set of tracklets through finding correspondences along the whole sequence for each detected face and takes advantage of the tracklets redundancy to deal with unreliable ones. Similar tracklets are grouped into the so called extended bag-of-tracklets (eBoT), which is aimed to correspond to a specific person. Finally, a prototype tracklet is extracted for each eBoT, where the occurred occlusions are estimated by relying on a new measure of confidence. We validated our approach over an extensive dataset of egocentric photo-streams and compared it to state of the art methods, demonstrating its effectiveness and robustness. version:2
arxiv-1601-03210 | The scarcity of crossing dependencies: a direct outcome of a specific constraint? | http://arxiv.org/abs/1601.03210 | id:1601.03210 author:Carlos Gómez-Rodríguez, Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph  published:2016-01-13 summary:Crossing syntactic dependencies have been observed to be infrequent in natural language, to the point that some syntactic theories and formalisms disregard them entirely. This leads to the question of whether the scarcity of crossings in languages arises from an independent and specific constraint on crossings. We provide statistical evidence suggesting that this is not the case, as the proportion of dependency crossings in a wide range of natural language treebanks can be accurately estimated by a simple predictor based on the local probability that two dependencies cross given their lengths. The relative error of this predictor never exceeds 5% on average, whereas a baseline predictor assuming a random ordering of the words of a sentence incurs a relative error that is at least 6 times greater. Our results suggest that the low frequency of crossings in natural languages is neither originated by hidden knowledge of language nor by the undesirability of crossings per se, but as a mere side effect of the principle of dependency length minimization. version:1
arxiv-1601-01799 | Dense Bag-of-Temporal-SIFT-Words for Time Series Classification | http://arxiv.org/abs/1601.01799 | id:1601.01799 author:Adeline Bailly, Simon Malinowski, Romain Tavenard, Thomas Guyet, Laetitia Chapel category:cs.LG  published:2016-01-08 summary:Time series classification is an application of particular interest with the increase of data to monitor. Classical techniques for time series classification rely on point-to-point distances. Recently, Bag-of-Words approaches have been used in this context. Words are quantized versions of simple features extracted from sliding windows. The SIFT framework has proved efficient for image classification. In this paper, we design a time series classification scheme that builds on the SIFT framework adapted to time series to feed a Bag-of-Words. We then refine our method by studying the impact of normalized Bag-of-Words, as well as densely extract point descriptors. Proposed adjustements achieve better performance. The evaluation shows that our method outperforms classical techniques in terms of classification. version:2
arxiv-1601-02093 | Group Invariant Deep Representations for Image Instance Retrieval | http://arxiv.org/abs/1601.02093 | id:1601.02093 author:Olivier Morère, Antoine Veillard, Jie Lin, Julie Petta, Vijay Chandrasekhar, Tomaso Poggio category:cs.CV cs.IR  published:2016-01-09 summary:Most image instance retrieval pipelines are based on comparison of vectors known as global image descriptors between a query image and the database images. Due to their success in large scale image classification, representations extracted from Convolutional Neural Networks (CNN) are quickly gaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors for image instance retrieval. While CNN-based descriptors are generally remarked for good retrieval performance at lower bitrates, they nevertheless present a number of drawbacks including the lack of robustness to common object transformations such as rotations compared with their interest point based FV counterparts. In this paper, we propose a method for computing invariant global descriptors from CNNs. Our method implements a recently proposed mathematical theory for invariance in a sensory cortex modeled as a feedforward neural network. The resulting global descriptors can be made invariant to multiple arbitrary transformation groups while retaining good discriminativeness. Based on a thorough empirical evaluation using several publicly available datasets, we show that our method is able to significantly and consistently improve retrieval results every time a new type of invariance is incorporated. We also show that our method which has few parameters is not prone to overfitting: improvements generalize well across datasets with different properties with regard to invariances. Finally, we show that our descriptors are able to compare favourably to other state-of-the-art compact descriptors in similar bitranges, exceeding the highest retrieval results reported in the literature on some datasets. A dedicated dimensionality reduction step --quantization or hashing-- may be able to further improve the competitiveness of the descriptors. version:2
arxiv-1601-03128 | Enhancing Energy Minimization Framework for Scene Text Recognition with Top-Down Cues | http://arxiv.org/abs/1601.03128 | id:1601.03128 author:Anand Mishra, Karteek Alahari, C. V. Jawahar category:cs.CV  published:2016-01-13 summary:Recognizing scene text is a challenging problem, even more so than the recognition of scanned documents. This problem has gained significant attention from the computer vision community in recent years, and several methods based on energy minimization frameworks and deep learning approaches have been proposed. In this work, we focus on the energy minimization framework and propose a model that exploits both bottom-up and top-down cues for recognizing cropped words extracted from street images. The bottom-up cues are derived from individual character detections from an image. We build a conditional random field model on these detections to jointly model the strength of the detections and the interactions between them. These interactions are top-down cues obtained from a lexicon-based prior, i.e., language statistics. The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model. We evaluate our proposed algorithm extensively on a number of cropped scene text benchmark datasets, namely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word, and show better performance than comparable methods. We perform a rigorous analysis of all the steps in our approach and analyze the results. We also show that state-of-the-art convolutional neural network features can be integrated in our framework to further improve the recognition performance. version:1
arxiv-1601-03124 | Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization | http://arxiv.org/abs/1601.03124 | id:1601.03124 author:Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng category:cs.LG stat.ML  published:2016-01-13 summary:Dyadic Data Prediction (DDP) is an important problem in many research areas. This paper develops a novel fully Bayesian nonparametric framework which integrates two popular and complementary approaches, discrete mixed membership modeling and continuous latent factor modeling into a unified Heterogeneous Matrix Factorization~(HeMF) model, which can predict the unobserved dyadics accurately. The HeMF can determine the number of communities automatically and exploit the latent linear structure for each bicluster efficiently. We propose a Variational Bayesian method to estimate the parameters and missing data. We further develop a novel online learning approach for Variational inference and use it for the online learning of HeMF, which can efficiently cope with the important large-scale DDP problem. We evaluate the performance of our method on the EachMoive, MovieLens and Netflix Prize collaborative filtering datasets. The experiment shows that, our model outperforms state-of-the-art methods on all benchmarks. Compared with Stochastic Gradient Method (SGD), our online learning approach achieves significant improvement on the estimation accuracy and robustness. version:1
arxiv-1601-03117 | Blind Image Denoising via Dependent Dirichlet Process Tree | http://arxiv.org/abs/1601.03117 | id:1601.03117 author:Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng category:cs.CV stat.ML  published:2016-01-13 summary:Most existing image denoising approaches assumed the noise to be homogeneous white Gaussian distributed with known intensity. However, in real noisy images, the noise models are usually unknown beforehand and can be much more complex. This paper addresses this problem and proposes a novel blind image denoising algorithm to recover the clean image from noisy one with the unknown noise model. To model the empirical noise of an image, our method introduces the mixture of Gaussian distribution, which is flexible enough to approximate different continuous distributions. The problem of blind image denoising is reformulated as a learning problem. The procedure is to first build a two-layer structural model for noisy patches and consider the clean ones as latent variable. To control the complexity of the noisy patch model, this work proposes a novel Bayesian nonparametric prior called "Dependent Dirichlet Process Tree" to build the model. Then, this study derives a variational inference algorithm to estimate model parameters and recover clean patches. We apply our method on synthesis and real noisy images with different noise models. Comparing with previous approaches, ours achieves better performance. The experimental results indicate the efficiency of the proposed algorithm to cope with practical image denoising tasks. version:1
arxiv-1412-3046 | Provable Tensor Methods for Learning Mixtures of Generalized Linear Models | http://arxiv.org/abs/1412.3046 | id:1412.3046 author:Hanie Sedghi, Majid Janzamin, Anima Anandkumar category:cs.LG stat.ML  published:2014-12-09 summary:We consider the problem of learning mixtures of generalized linear models (GLM) which arise in classification and regression problems. Typical learning approaches such as expectation maximization (EM) or variational Bayes can get stuck in spurious local optima. In contrast, we present a tensor decomposition method which is guaranteed to correctly recover the parameters. The key insight is to employ certain feature transformations of the input, which depend on the input generative model. Specifically, we employ score function tensors of the input and compute their cross-correlation with the response variable. We establish that the decomposition of this tensor consistently recovers the parameters, under mild non-degeneracy conditions. We demonstrate that the computational and sample complexity of our method is a low order polynomial of the input and the latent dimensions. version:4
arxiv-1601-02049 | A note on the sample complexity of the Er-SpUD algorithm by Spielman, Wang and Wright for exact recovery of sparsely used dictionaries | http://arxiv.org/abs/1601.02049 | id:1601.02049 author:Radosław Adamczak category:math.PR cs.LG math.ST stat.TH  published:2016-01-08 summary:We consider the problem of recovering an invertible $n \times n$ matrix $A$ and a sparse $n \times p$ random matrix $X$ based on the observation of $Y = AX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Using only elementary tools from the theory of empirical processes we show that a version of the Er-SpUD algorithm by Spielman, Wang and Wright with high probability recovers $A$ and $X$ exactly, provided that $p \ge Cn\log n$, which is optimal up to the constant $C$. version:2
arxiv-1601-03642 | Creativity in Machine Learning | http://arxiv.org/abs/1601.03642 | id:1601.03642 author:Martin Thoma category:cs.CV cs.LG  published:2016-01-12 summary:Recent machine learning techniques can be modified to produce creative results. Those results did not exist before; it is not a trivial combination of the data which was fed into the machine learning system. The obtained results come in multiple forms: As images, as text and as audio. This paper gives a high level overview of how they are created and gives some examples. It is meant to be a summary of the current work and give people who are new to machine learning some starting points. version:1
arxiv-1601-03094 | A metric for sets of trajectories that is practical and mathematically consistent | http://arxiv.org/abs/1601.03094 | id:1601.03094 author:José Bento category:cs.CV cs.SY math.OC  published:2016-01-12 summary:Metrics on the space of sets of trajectories are important for scientists in the field of computer vision, machine learning, robotics and general artificial intelligence. Yet existing notions of closeness are either mathematically inconsistent or of limited practical use. In this paper we outline the limitations in the existing mathematically-consistent metrics, which are based on Schuhmacher et al. 2008, and the inconsistencies in the heuristic notions of closeness used in practice, whose main ideas are common to the CLEAR MOT measures widely used in computer vision. In two steps we then propose a new intuitive metric between sets of trajectories and address these problems. First we explain a natural solution that leads to a metric that is hard to compute. Then we modify this formulation to obtain a metric that is easy to compute and keeps all the good properties of the previous metric. In particular, our notion of closeness is the first that has the following three properties: it can be quickly computed, it incorporates confusion of trajectories' identity in an optimal way and it is a metric in the mathematical sense. version:1
arxiv-1601-03073 | Infomax strategies for an optimal balance between exploration and exploitation | http://arxiv.org/abs/1601.03073 | id:1601.03073 author:Gautam Reddy, Antonio Celani, Massimo Vergassola category:cs.LG cs.IT math.IT physics.data-an q-bio.PE stat.ML  published:2016-01-12 summary:Proper balance between exploitation and exploration is what makes good decisions, which achieve high rewards like payoff or evolutionary fitness. The Infomax principle postulates that maximization of information directs the function of diverse systems, from living systems to artificial neural networks. While specific applications are successful, the validity of information as a proxy for reward remains unclear. Here, we consider the multi-armed bandit decision problem, which features arms (slot-machines) of unknown probabilities of success and a player trying to maximize cumulative payoff by choosing the sequence of arms to play. We show that an Infomax strategy (Info-p) which optimally gathers information on the highest mean reward among the arms saturates known optimal bounds and compares favorably to existing policies. The highest mean reward considered by Info-p is not the quantity actually needed for the choice of the arm to play, yet it allows for optimal tradeoffs between exploration and exploitation. version:1
arxiv-1506-08350 | Stochastic Gradient Made Stable: A Manifold Propagation Approach for Large-Scale Optimization | http://arxiv.org/abs/1506.08350 | id:1506.08350 author:Yadong Mu, Wei Liu, Wei Fan category:cs.LG cs.NA 90C06  published:2015-06-28 summary:Stochastic gradient descent (SGD) holds as a classical method to build large scale machine learning models over big data. A stochastic gradient is typically calculated from a limited number of samples (known as mini-batch), so it potentially incurs a high variance and causes the estimated parameters bounce around the optimal solution. To improve the stability of stochastic gradient, recent years have witnessed the proposal of several semi-stochastic gradient descent algorithms, which distinguish themselves from standard SGD by incorporating global information into gradient computation. In this paper we contribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm to this nascent research area, accelerating the optimization of a large family of composite convex functions. Though theoretically converging faster, prior semi-stochastic algorithms are found to suffer from high iteration complexity, which makes them even slower than SGD in practice on many datasets. In our proposed S3GD, the semi-stochastic gradient is calculated based on efficient manifold propagation, which can be numerically accomplished by sparse matrix multiplications. This way S3GD is able to generate a highly-accurate estimate of the exact gradient from each mini-batch with largely-reduced computational complexity. Theoretic analysis reveals that the proposed S3GD elegantly balances the geometric algorithmic convergence rate against the space and time complexities during the optimization. The efficacy of S3GD is also experimentally corroborated on several large-scale benchmark datasets. version:2
arxiv-1511-06444 | Universality in halting time and its applications in optimization | http://arxiv.org/abs/1511.06444 | id:1511.06444 author:Levent Sagun, Thomas Trogdon, Yann LeCun category:cs.LG math.NA math.PR 65K10  82D30  37E20  published:2015-11-19 summary:The authors present empirical universal distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main universality classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. version:2
arxiv-1601-02970 | Deep Neural Networks predict Hierarchical Spatio-temporal Cortical Dynamics of Human Visual Object Recognition | http://arxiv.org/abs/1601.02970 | id:1601.02970 author:Radoslaw M. Cichy, Aditya Khosla, Dimitrios Pantazis, Antonio Torralba, Aude Oliva category:cs.CV q-bio.NC  published:2016-01-12 summary:The complex multi-stage architecture of cortical visual pathways provides the neural basis for efficient visual object recognition in humans. However, the stage-wise computations therein remain poorly understood. Here, we compared temporal (magnetoencephalography) and spatial (functional MRI) visual brain representations with representations in an artificial deep neural network (DNN) tuned to the statistics of real-world visual recognition. We showed that the DNN captured the stages of human visual processing in both time and space from early visual areas towards the dorsal and ventral streams. Further investigation of crucial DNN parameters revealed that while model architecture was important, training on real-world categorization was necessary to enforce spatio-temporal hierarchical relationships with the brain. Together our results provide an algorithmically informed view on the spatio-temporal dynamics of visual object recognition in the human visual brain. version:1
arxiv-1601-02913 | Learning Subclass Representations for Visually-varied Image Classification | http://arxiv.org/abs/1601.02913 | id:1601.02913 author:Xinchao Li, Peng Xu, Yue Shi, Martha Larson, Alan Hanjalic category:cs.MM cs.CV  published:2016-01-12 summary:In this paper, we present a subclass-representation approach that predicts the probability of a social image belonging to one particular class. We explore the co-occurrence of user-contributed tags to find subclasses with a strong connection to the top level class. We then project each image on to the resulting subclass space to generate a subclass representation for the image. The novelty of the approach is that subclass representations make use of not only the content of the photos themselves, but also information on the co-occurrence of their tags, which determines membership in both subclasses and top-level classes. The novelty is also that the images are classified into smaller classes, which have a chance of being more visually stable and easier to model. These subclasses are used as a latent space and images are represented in this space by their probability of relatedness to all of the subclasses. In contrast to approaches directly modeling each top-level class based on the image content, the proposed method can exploit more information for visually diverse classes. The approach is evaluated on a set of $2$ million photos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale Flickr-tag Image Classification Grand Challenge. Experiments show that the proposed system delivers sound performance for visually diverse classes compared with methods that directly model top classes. version:1
arxiv-1601-02852 | Human Attention Estimation for Natural Images: An Automatic Gaze Refinement Approach | http://arxiv.org/abs/1601.02852 | id:1601.02852 author:Jinsoo Choi, Tae-Hyun Oh, In So Kweon category:cs.CV cs.HC cs.MM  published:2016-01-12 summary:Photo collections and its applications today attempt to reflect user interactions in various forms. Moreover, photo collections aim to capture the users' intention with minimum effort through applications capturing user intentions. Human interest regions in an image carry powerful information about the user's behavior and can be used in many photo applications. Research on human visual attention has been conducted in the form of gaze tracking and computational saliency models in the computer vision community, and has shown considerable progress. This paper presents an integration between implicit gaze estimation and computational saliency model to effectively estimate human attention regions in images on the fly. Furthermore, our method estimates human attention via implicit calibration and incremental model updating without any active participation from the user. We also present extensive analysis and possible applications for personal photo collections. version:1
arxiv-1601-03277 | Weightless neural network parameters and architecture selection in a quantum computer | http://arxiv.org/abs/1601.03277 | id:1601.03277 author:Adenilton J. da Silva, Wilson R. de Oliveira, Teresa B. Ludermir category:quant-ph cs.NE  published:2016-01-12 summary:Training artificial neural networks requires a tedious empirical evaluation to determine a suitable neural network architecture. To avoid this empirical process several techniques have been proposed to automatise the architecture selection process. In this paper, we propose a method to perform parameter and architecture selection for a quantum weightless neural network (qWNN). The architecture selection is performed through the learning procedure of a qWNN with a learning algorithm that uses the principle of quantum superposition and a non-linear quantum operator. The main advantage of the proposed method is that it performs a global search in the space of qWNN architecture and parameters rather than a local search. version:1
arxiv-1601-02828 | Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation | http://arxiv.org/abs/1601.02828 | id:1601.02828 author:Pawel Swietojanski, Jinyu Li, Steve Renals category:cs.CL cs.LG cs.SD  published:2016-01-12 summary:This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to more adaptable DNN acoustic model, which can work in both a speaker-dependent and a speaker-independent manner, without the requirement to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings and Aurora4) and over 270 test speakers we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition we have investigated the effect of the amount of adaptation data per speaker, the quality of adaptation targets when estimating transforms in an unsupervised manner, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner. version:1
arxiv-1511-04401 | Symbol Grounding Association in Multimodal Sequences with Missing Elements | http://arxiv.org/abs/1511.04401 | id:1511.04401 author:Federico Raue, Thomas M. Breuel, Andreas Dengel, Marcus Liwicki category:cs.CV cs.CL cs.LG cs.NE  published:2015-11-13 summary:In this paper, we extend a symbolic association framework to being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development on infants. This scenario has been long interested by Artificial Intelligence, Psychology and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our approach uses two parallel Long Short-Term Memory (LSTM) networks with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with max and mean operations for handling missing elements in the sequences. The intuition behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in three different scenarios: audio sequences with missing elements, visual sequences with missing elements, and sequences with missing elements in both modalities. The performance of our extension reaches better results than the original model and similar results to a unique LSTM trained in one modality, i.e., where the learning problem is less difficult. version:3
arxiv-1512-04077 | Learning the Correction for Multi-Path Deviations in Time-of-Flight Cameras | http://arxiv.org/abs/1512.04077 | id:1512.04077 author:Mojmir Mutny, Rahul Nair, Jens-Malte Gottfried category:cs.CV  published:2015-12-13 summary:The Multipath effect in Time-of-Flight(ToF) cameras still remains to be a challenging problem that hinders further processing of 3D data information. Based on the evidence from previous literature, we explored the possibility of using machine learning techniques to correct this effect. Firstly, we created two new datasets of of ToF images rendered via ToF simulator of LuxRender. These two datasets contain corners in multiple orientations and with different material properties. We chose scenes with corners as multipath effects are most pronounced in corners. Secondly, we used this dataset to construct a learning model to predict real valued corrections to the ToF data using Random Forests. We found out that in our smaller dataset we were able to predict real valued correction and improve the quality of depth images significantly by removing multipath bias. With our algorithm, we improved relative per-pixel error from average value of 19% to 3%. Additionally, variance of the error was lowered by an order of magnitude. version:2
arxiv-1601-02789 | Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking | http://arxiv.org/abs/1601.02789 | id:1601.02789 author:Krzysztof Wołk, Danijel Koržinek category:cs.CL stat.AP stat.ML  published:2016-01-12 summary:Re-speaking is a mechanism for obtaining high quality subtitles for use in live broadcast and other public events. Because it relies on humans performing the actual re-speaking, the task of estimating the quality of the results is non-trivial. Most organisations rely on humans to perform the actual quality assessment, but purely automatic methods have been developed for other similar problems, like Machine Translation. This paper will try to compare several of these methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will then be matched to the human-derived NER metric, commonly used in re-speaking. version:1
arxiv-1502-00598 | Lock in Feedback in Sequential Experiments | http://arxiv.org/abs/1502.00598 | id:1502.00598 author:Maurits Kaptein, Davide Iannuzzi category:cs.LG  published:2015-02-02 summary:We often encounter situations in which an experimenter wants to find, by sequential experimentation, $x_{max} = \arg\max_{x} f(x)$, where $f(x)$ is a (possibly unknown) function of a well controllable variable $x$. Taking inspiration from physics and engineering, we have designed a new method to address this problem. In this paper, we first introduce the method in continuous time, and then present two algorithms for use in sequential experiments. Through a series of simulation studies, we show that the method is effective for finding maxima of unknown functions by experimentation, even when the maximum of the functions drifts or when the signal to noise ratio is low. version:3
arxiv-1509-06003 | Robust Visual Tracking via Inverse Nonnegative Matrix Factorization | http://arxiv.org/abs/1509.06003 | id:1509.06003 author:Fanghui Liu, Tao Zhou, Keren Fu, Irene Y. H. Gu, Jie Yang category:cs.CV  published:2015-09-20 summary:The establishment of robust target appearance model over time is an overriding concern in visual tracking. In this paper, we propose an inverse nonnegative matrix factorization (NMF) method for robust appearance modeling. Rather than using a linear combination of nonnegative basis matrices for each target image patch in the conventional NMF, the proposed method is a reverse thought to conventional NMF tracker. It utilizes both the foreground and background information, and imposes a local coordinate constraint, where the basis matrix is sparse matrix from the linear combination of candidates with corresponding nonnegative coefficient vectors. Inverse NMF is used as a feature encoder, where the resulting coefficient vectors are fed into a SVM classifier for separating the target from the background. The proposed method is tested on several videos and compared with seven state-of-the-art methods. Our results have provided further support to the effectiveness and robustness of the proposed method. version:3
arxiv-1601-02748 | Robust Lineage Reconstruction from High-Dimensional Single-Cell Data | http://arxiv.org/abs/1601.02748 | id:1601.02748 author:Gregory Giecold, Eugenio Marco, Lorenzo Trippa, Guo-Cheng Yuan category:q-bio.QM stat.AP stat.CO stat.ML  published:2016-01-12 summary:Single-cell gene expression data provide invaluable resources for systematic characterization of cellular hierarchy in multi-cellular organisms. However, cell lineage reconstruction is still often associated with significant uncertainty due to technological constraints. Such uncertainties have not been taken into account in current methods. We present ECLAIR, a novel computational method for the statistical inference of cell lineage relationships from single-cell gene expression data. ECLAIR uses an ensemble approach to improve the robustness of lineage predictions, and provides a quantitative estimate of the uncertainty of lineage branchings. We show that the application of ECLAIR to published datasets successfully reconstructs known lineage relationships and significantly improves the robustness of predictions. In conclusion, ECLAIR is a powerful bioinformatics tool for single-cell data analysis. It can be used for robust lineage reconstruction with quantitative estimate of prediction accuracy. version:1
arxiv-1505-00521 | Reinforcement Learning Neural Turing Machines - Revised | http://arxiv.org/abs/1505.00521 | id:1505.00521 author:Wojciech Zaremba, Ilya Sutskever category:cs.LG  published:2015-05-04 summary:The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them. The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete. We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete. version:3
arxiv-1601-02733 | Deep Learning of Part-based Representation of Data Using Sparse Autoencoders with Nonnegativity Constraints | http://arxiv.org/abs/1601.02733 | id:1601.02733 author:Ehsan Hosseini-Asl, Jacek M. Zurada, Olfa Nasraoui category:cs.LG stat.ML  published:2016-01-12 summary:We demonstrate a new deep learning autoencoder network, trained by a nonnegativity constraint algorithm (NCAE), that learns features which show part-based representation of data. The learning algorithm is based on constraining negative weights. The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text dataset. The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data, while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and Nonnegative Matrix Factorization. It is also shown that this newly acquired representation improves the prediction performance of a deep neural network. version:1
arxiv-1506-08473 | Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods | http://arxiv.org/abs/1506.08473 | id:1506.08473 author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML  published:2015-06-28 summary:Training neural networks is a challenging non-convex optimization problem, and backpropagation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for guaranteed training of two-layer neural networks. We provide risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for learnability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we propose a computationally efficient method with guaranteed risk bounds for training neural networks with one hidden layer. version:3
arxiv-1601-02712 | IRLS and Slime Mold: Equivalence and Convergence | http://arxiv.org/abs/1601.02712 | id:1601.02712 author:Damian Straszak, Nisheeth K. Vishnoi category:cs.DS cs.ET cs.IT math.IT math.NA math.OC stat.ML  published:2016-01-12 summary:In this paper we present a connection between two dynamical systems arising in entirely different contexts: one in signal processing and the other in biology. The first is the famous Iteratively Reweighted Least Squares (IRLS) algorithm used in compressed sensing and sparse recovery while the second is the dynamics of a slime mold (Physarum polycephalum). Both of these dynamics are geared towards finding a minimum l1-norm solution in an affine subspace. Despite its simplicity the convergence of the IRLS method has been shown only for a certain regularization of it and remains an important open problem. Our first result shows that the two dynamics are projections of the same dynamical system in higher dimensions. As a consequence, and building on the recent work on Physarum dynamics, we are able to prove convergence and obtain complexity bounds for a damped version of the IRLS algorithm. version:1
arxiv-1512-09176 | Personalized Course Sequence Recommendations | http://arxiv.org/abs/1512.09176 | id:1512.09176 author:Jie Xu, Tianwei Xing, Mihaela van der Schaar category:cs.CY cs.LG  published:2015-12-30 summary:Given the variability in student learning it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multi-armed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and then recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations. version:2
arxiv-1601-02705 | Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding | http://arxiv.org/abs/1601.02705 | id:1601.02705 author:Jaeyong Sung, Seok Hyun Jin, Ian Lenz, Ashutosh Saxena category:cs.RO cs.AI cs.LG  published:2016-01-12 summary:There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before. version:1
arxiv-1601-02553 | Environmental Noise Embeddings for Robust Speech Recognition | http://arxiv.org/abs/1601.02553 | id:1601.02553 author:Suyoun Kim, Bhiksha Raj, Ian Lane category:cs.CL  published:2016-01-11 summary:We propose a novel deep neural network architecture for speech recognition that explicitly employs knowledge of the background environmental noise within a deep neural network acoustic model. A deep neural network is used to predict the acoustic environment in which the system in being used. The discriminative embedding generated at the bottleneck layer of this network is then concatenated with traditional acoustic features as input to a deep neural network acoustic model. Using simulated acoustic environments we show that the proposed approach significantly improves speech recognition accuracy in noisy and highly reverberant environments, outperforming multi-condition training and multi-task learning for this task. version:1
arxiv-1601-02543 | Evaluating the Performance of a Speech Recognition based System | http://arxiv.org/abs/1601.02543 | id:1601.02543 author:Vinod Kumar Pandey, Sunil Kumar Kopparapu category:cs.CL cs.AI cs.HC  published:2016-01-11 summary:Speech based solutions have taken center stage with growth in the services industry where there is a need to cater to a very large number of people from all strata of the society. While natural language speech interfaces are the talk in the research community, yet in practice, menu based speech solutions thrive. Typically in a menu based speech solution the user is required to respond by speaking from a closed set of words when prompted by the system. A sequence of human speech response to the IVR prompts results in the completion of a transaction. A transaction is deemed successful if the speech solution can correctly recognize all the spoken utterances of the user whenever prompted by the system. The usual mechanism to evaluate the performance of a speech solution is to do an extensive test of the system by putting it to actual people use and then evaluating the performance by analyzing the logs for successful transactions. This kind of evaluation could lead to dissatisfied test users especially if the performance of the system were to result in a poor transaction completion rate. To negate this the Wizard of Oz approach is adopted during evaluation of a speech system. Overall this kind of evaluations is an expensive proposition both in terms of time and cost. In this paper, we propose a method to evaluate the performance of a speech solution without actually putting it to people use. We first describe the methodology and then show experimentally that this can be used to identify the performance bottlenecks of the speech solution even before the system is actually used thus saving evaluation time and expenses. version:1
arxiv-1601-02539 | Investigating gated recurrent neural networks for speech synthesis | http://arxiv.org/abs/1601.02539 | id:1601.02539 author:Zhizheng Wu, Simon King category:cs.CL cs.NE  published:2016-01-11 summary:Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality. version:1
arxiv-1509-08144 | Optimal Copula Transport for Clustering Multivariate Time Series | http://arxiv.org/abs/1509.08144 | id:1509.08144 author:Gautier Marti, Frank Nielsen, Philippe Donnat category:cs.LG stat.ML  published:2015-09-27 summary:This paper presents a new methodology for clustering multivariate time series leveraging optimal transport between copulas. Copulas are used to encode both (i) intra-dependence of a multivariate time series, and (ii) inter-dependence between two time series. Then, optimal copula transport allows us to define two distances between multivariate time series: (i) one for measuring intra-dependence dissimilarity, (ii) another one for measuring inter-dependence dissimilarity based on a new multivariate dependence coefficient which is robust to noise, deterministic, and which can target specified dependencies. version:2
arxiv-1510-00878 | Client Profiling for an Anti-Money Laundering System | http://arxiv.org/abs/1510.00878 | id:1510.00878 author:Claudio Alexandre, João Balsa category:cs.LG cs.AI stat.ML  published:2015-10-03 summary:We present a data mining approach for profiling bank clients in order to support the process of detection of anti-money laundering operations. We first present the overall system architecture, and then focus on the relevant component for this paper. We detail the experiments performed on real world data from a financial institution, which allowed us to group clients in clusters and then generate a set of classification rules. We discuss the relevance of the founded client profiles and of the generated classification rules. According to the defined overall agent-based architecture, these rules will be incorporated in the knowledge base of the intelligent agents responsible for the signaling of suspicious transactions. version:2
arxiv-1601-02513 | How to learn a graph from smooth signals | http://arxiv.org/abs/1601.02513 | id:1601.02513 author:Vassilis Kalofolias category:stat.ML cs.LG physics.data-an  published:2016-01-11 summary:We propose a framework that learns the graph structure underlying a set of smooth signals. Given $X\in\mathbb{R}^{m\times n}$ whose rows reside on the vertices of an unknown graph, we learn the edge weights $w\in\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that $\text{tr}{X^\top LX}$ is small. We show that the problem is a weighted $\ell$-1 minimization that leads to naturally sparse solutions. We point out how known graph learning or construction techniques fall within our framework and propose a new model that performs better than the state of the art in many settings. We present efficient, scalable primal-dual based algorithms for both our model and the previous state of the art, and evaluate their performance on artificial and real data. version:1
arxiv-1601-02502 | Trans-gram, Fast Cross-lingual Word-embeddings | http://arxiv.org/abs/1601.02502 | id:1601.02502 author:Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, Amine Benhalloum category:cs.CL  published:2016-01-11 summary:We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks. version:1
arxiv-1601-02487 | Facial Expression Recognition in the Wild using Rich Deep Features | http://arxiv.org/abs/1601.02487 | id:1601.02487 author:Abubakrelsedik Karali, Ahmad Bassiouny, Motaz El-Saban category:cs.CV  published:2016-01-11 summary:Facial Expression Recognition is an active area of research in computer vision with a wide range of applications. Several approaches have been developed to solve this problem for different benchmark datasets. However, Facial Expression Recognition in the wild remains an area where much work is still needed to serve real-world applications. To this end, in this paper we present a novel approach towards facial expression recognition. We fuse rich deep features with domain knowledge through encoding discriminant facial patches. We conduct experiments on two of the most popular benchmark datasets; CK and TFE. Moreover, we present a novel dataset that, unlike its precedents, consists of natural - not acted - expression images. Experimental results show that our approach achieves state-of-the-art results over standard benchmarks and our own dataset version:1
arxiv-1601-02431 | The Effects of Age, Gender and Region on Non-standard Linguistic Variation in Online Social Networks | http://arxiv.org/abs/1601.02431 | id:1601.02431 author:Claudia Peersman, Walter Daelemans, Reinhild Vandekerckhove, Bram Vandekerckhove, Leona Van Vaerenbergh category:cs.CL  published:2016-01-11 summary:We present a corpus-based analysis of the effects of age, gender and region of origin on the production of both "netspeak" or "chatspeak" features and regional speech features in Flemish Dutch posts that were collected from a Belgian online social network platform. The present study shows that combining quantitative and qualitative approaches is essential for understanding non-standard linguistic variation in a CMC corpus. It also presents a methodology that enables the systematic study of this variation by including all non-standard words in the corpus. The analyses resulted in a convincing illustration of the Adolescent Peak Principle. In addition, our approach revealed an intriguing correlation between the use of regional speech features and chatspeak features. version:1
arxiv-1305-1422 | Somoclu: An Efficient Parallel Library for Self-Organizing Maps | http://arxiv.org/abs/1305.1422 | id:1305.1422 author:Peter Wittek, Shi Chao Gao, Ik Soo Lim, Li Zhao category:cs.DC cs.MS cs.NE  published:2013-05-07 summary:Somoclu is a massively parallel tool for training self-organizing maps on large data sets written in C++. It builds on OpenMP for multicore execution, and on MPI for distributing the workload across the nodes in a cluster. It is also able to boost training by using CUDA if graphics processing units are available. A sparse kernel is included, which is useful for high-dimensional but sparse data, such as the vector spaces common in text mining workflows. Python, R and MATLAB interfaces facilitate interactive use. Apart from fast execution, memory use is highly optimized, enabling training large emergent maps even on a single computer. version:3
arxiv-1507-05409 | A Parameter-free Affinity Based Clustering | http://arxiv.org/abs/1507.05409 | id:1507.05409 author:Bhaskar Mukhoty, Ruchir Gupta, Y. N. Singh category:cs.CV  published:2015-07-20 summary:Several methods have been proposed to estimate the number of clusters in a dataset; the basic ideal behind all of them has been to study an index that measures inter-cluster separation and intra-cluster cohesion over a range of cluster numbers and report the number which gives an optimum value of the index. In this paper we propose a simple, parameter free approach that is like human cognition to form clusters, where closely lying points are easily identified to form a cluster and total number of clusters are revealed. To identify closely lying points, affinity of two points is defined as a function of distance and a threshold affinity is identified, above which two points in a dataset are likely to be in the same cluster. Well separated clusters are identified even in the presence of outliers, whereas for not so well separated dataset, final number of clusters are estimated and the detected clusters are merged to produce the final clusters. Experiments performed with several large dimensional synthetic and real datasets show good results with robustness to noise and density variation within dataset. version:2
arxiv-1601-02377 | Implicit Look-alike Modelling in Display Ads: Transfer Collaborative Filtering to CTR Estimation | http://arxiv.org/abs/1601.02377 | id:1601.02377 author:Weinan Zhang, Lingxi Chen, Jun Wang category:cs.LG cs.IR  published:2016-01-11 summary:User behaviour targeting is essential in online advertising. Compared with sponsored search keyword targeting and contextual advertising page content targeting, user behaviour targeting builds users' interest profiles via tracking their online behaviour and then delivers the relevant ads according to each user's interest, which leads to higher targeting accuracy and thus more improved advertising performance. The current user profiling methods include building keywords and topic tags or mapping users onto a hierarchical taxonomy. However, to our knowledge, there is no previous work that explicitly investigates the user online visits similarity and incorporates such similarity into their ad response prediction. In this work, we propose a general framework which learns the user profiles based on their online browsing behaviour, and transfers the learned knowledge onto prediction of their ad response. Technically, we propose a transfer learning model based on the probabilistic latent factor graphic models, where the users' ad response profiles are generated from their online browsing profiles. The large-scale experiments based on real-world data demonstrate significant improvement of our solution over some strong baselines. version:1
arxiv-1601-02376 | Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction | http://arxiv.org/abs/1601.02376 | id:1601.02376 author:Weinan Zhang, Tianming Du, Jun Wang category:cs.LG cs.IR  published:2016-01-11 summary:Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models. version:1
arxiv-1510-08983 | Highway Long Short-Term Memory RNNs for Distant Speech Recognition | http://arxiv.org/abs/1510.08983 | id:1510.08983 author:Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, James Glass category:cs.NE cs.AI cs.CL cs.LG  published:2015-10-30 summary:In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and $5.3\%$ relative improvement respectively. version:2
arxiv-1509-00519 | Importance Weighted Autoencoders | http://arxiv.org/abs/1509.00519 | id:1509.00519 author:Yuri Burda, Roger Grosse, Ruslan Salakhutdinov category:cs.LG stat.ML  published:2015-09-01 summary:The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks. version:3
arxiv-1506-03039 | Measuring Sample Quality with Stein's Method | http://arxiv.org/abs/1506.03039 | id:1506.03039 author:Jackson Gorham, Lester Mackey category:stat.ML cs.LG math.PR stat.ME  published:2015-06-09 summary:To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference. version:4
arxiv-1601-02300 | Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering | http://arxiv.org/abs/1601.02300 | id:1601.02300 author:Young-Min Kim, Julien Velcin, Stéphane Bonnevay, Marian-Andrei Rizoiu category:cs.IR cs.LG stat.ML  published:2016-01-11 summary:Evolutionary clustering aims at capturing the temporal evolution of clusters. This issue is particularly important in the context of social media data that are naturally temporally driven. In this paper, we propose a new probabilistic model-based evolutionary clustering technique. The Temporal Multinomial Mixture (TMM) is an extension of classical mixture model that optimizes feature co-occurrences in the trade-off with temporal smoothness. Our model is evaluated for two recent case studies on opinion aggregation over time. We compare four different probabilistic clustering models and we show the superiority of our proposal in the task of instance-oriented clustering. version:1
arxiv-1601-02603 | How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions | http://arxiv.org/abs/1601.02603 | id:1601.02603 author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.LG cs.DS  published:2016-01-11 summary:In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance. version:1
arxiv-1511-03771 | Improving performance of recurrent neural network with relu nonlinearity | http://arxiv.org/abs/1511.03771 | id:1511.03771 author:Sachin S. Talathi, Aniket Vartak category:cs.NE cs.LG  published:2015-11-12 summary:In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem. version:2
arxiv-1504-03655 | Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients | http://arxiv.org/abs/1504.03655 | id:1504.03655 author:Bo Xie, Yingyu Liang, Le Song category:cs.LG  published:2015-04-14 summary:Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points. We propose a simple, computationally efficient, and memory friendly algorithm based on the "doubly stochastic gradients" to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the \emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate $\tilde{O}(1/t)$ to the global optimum, even for the top $k$ eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets. version:4
arxiv-1508-07643 | Directional Decision Lists | http://arxiv.org/abs/1508.07643 | id:1508.07643 author:Marc Goessling, Shan Kang category:stat.ML cs.LG stat.CO  published:2015-08-30 summary:In this paper we introduce a novel family of decision lists consisting of highly interpretable models which can be learned efficiently in a greedy manner. The defining property is that all rules are oriented in the same direction. Particular examples of this family are decision lists with monotonically decreasing (or increasing) probabilities. On simulated data we empirically confirm that the proposed model family is easier to train than general decision lists. We exemplify the practical usability of our approach by identifying problem symptoms in a manufacturing process. version:3
arxiv-1601-02257 | A Sufficient Statistics Construction of Bayesian Nonparametric Exponential Family Conjugate Models | http://arxiv.org/abs/1601.02257 | id:1601.02257 author:Robert Finn, Brian Kulis category:cs.LG stat.ML  published:2016-01-10 summary:Conjugate pairs of distributions over infinite dimensional spaces are prominent in statistical learning theory, particularly due to the widespread adoption of Bayesian nonparametric methodologies for a host of models and applications. Much of the existing literature in the learning community focuses on processes possessing some form of computationally tractable conjugacy as is the case for the beta and gamma processes (and, via normalization, the Dirichlet process). For these processes, proofs of conjugacy and requisite derivation of explicit computational formulae for posterior density parameters are idiosyncratic to the stochastic process in question. As such, Bayesian Nonparametric models are currently available for a limited number of conjugate pairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In each of these above cases the likelihood process belongs to the class of discrete exponential family distributions. The exclusion of continuous likelihood distributions from the known cases of Bayesian Nonparametric Conjugate models stands as a disparity in the researcher's toolbox. In this paper we first address the problem of obtaining a general construction of prior distributions over infinite dimensional spaces possessing distributional properties amenable to conjugacy. Second, we bridge the divide between the discrete and continuous likelihoods by illustrating a canonical construction for stochastic processes whose Levy measure densities are from positive exponential families, and then demonstrate that these processes in fact form the prior, likelihood, and posterior in a conjugate family. Our canonical construction subsumes known computational formulae for posterior density parameters in the cases where the likelihood is from a discrete distribution belonging to an exponential family. version:1
arxiv-1301-1942 | Bayesian Optimization in a Billion Dimensions via Random Embeddings | http://arxiv.org/abs/1301.1942 | id:1301.1942 author:Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, Nando de Freitas category:stat.ML cs.LG  published:2013-01-09 summary:Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver. version:2
arxiv-1601-02225 | Parallel Stroked Multi Line: a model-based method for compressing large fingerprint databases | http://arxiv.org/abs/1601.02225 | id:1601.02225 author:Hamid Mansouri, Hamid-Reza Pourreza category:cs.CV cs.DS  published:2016-01-10 summary:With increasing usage of fingerprints as an important biometric data, the need to compress the large fingerprint databases has become essential. The most recommended compression algorithm, even by standards, is JPEG2K. But at high compression rates, this algorithm is ineffective. In this paper, a model is proposed which is based on parallel lines with same orientations, arbitrary widths and same gray level values located on rectangle with constant gray level value as background. We refer to this algorithm as Parallel Stroked Multi Line (PSML). By using Adaptive Geometrical Wavelet and employing PSML, a compression algorithm is developed. This compression algorithm can preserve fingerprint structure and minutiae. The exact algorithm of computing the PSML model take exponential time. However, we have proposed an alternative approximation algorithm, which reduces the time complexity to $O(n^3)$. The proposed PSML alg. has significant advantage over Wedgelets Transform in PSNR value and visual quality in compressed images. The proposed method, despite the lower PSNR values than JPEG2K algorithm in common range of compression rates, in all compression rates have nearly equal or greater advantage over JPEG2K when used by Automatic Fingerprint Identification Systems (AFIS). At high compression rates, according to PSNR values, mean EER rate and visual quality, the encoded images with JPEG2K can not be identified from each other after compression. But, images encoded by the PSML alg. retained the sufficient information to maintain fingerprint identification performances similar to the ones obtained by raw images without compression. One the U.are.U 400 database, the mean EER rate for uncompressed images is 4.54%, while at 267:1 compression ratio, this value becomes 49.41% and 6.22% for JPEG2K and PSML, respectively. This result shows a significant improvement over the standard JPEG2K algorithm. version:1
arxiv-1601-02220 | Joint Object-Material Category Segmentation from Audio-Visual Cues | http://arxiv.org/abs/1601.02220 | id:1601.02220 author:Anurag Arnab, Michael Sapienza, Stuart Golodetz, Julien Valentin, Ondrej Miksik, Shahram Izadi, Philip Torr category:cs.CV cs.SD  published:2016-01-10 summary:It is not always possible to recognise objects and infer material properties for a scene from visual cues alone, since objects can look visually similar whilst being made of very different materials. In this paper, we therefore present an approach that augments the available dense visual cues with sparse auditory cues in order to estimate dense object and material labels. Since estimates of object class and material properties are mutually informative, we optimise our multi-output labelling jointly using a random-field framework. We evaluate our system on a new dataset with paired visual and auditory data that we make publicly available. We demonstrate that this joint estimation of object and material labels significantly outperforms the estimation of either category in isolation. version:1
arxiv-1511-07394 | Where To Look: Focus Regions for Visual Question Answering | http://arxiv.org/abs/1511.07394 | id:1511.07394 author:Kevin J. Shih, Saurabh Singh, Derek Hoiem category:cs.CV  published:2015-11-23 summary:We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method exhibits significant improvements in answering questions such as "what color," where it is necessary to evaluate a specific location, and "what room," where it selectively identifies informative image regions. Our model is tested on the VQA dataset which is the largest human-annotated visual question answering dataset to our knowledge. version:2
arxiv-1601-02213 | On Clustering Time Series Using Euclidean Distance and Pearson Correlation | http://arxiv.org/abs/1601.02213 | id:1601.02213 author:Michael R. Berthold, Frank Höppner category:cs.LG cs.AI stat.ML  published:2016-01-10 summary:For time series comparisons, it has often been observed that z-score normalized Euclidean distances far outperform the unnormalized variant. In this paper we show that a z-score normalized, squared Euclidean Distance is, in fact, equal to a distance based on Pearson Correlation. This has profound impact on many distance-based classification or clustering methods. In addition to this theoretically sound result we also show that the often used k-Means algorithm formally needs a mod ification to keep the interpretation as Pearson correlation strictly valid. Experimental results demonstrate that in many cases the standard k-Means algorithm generally produces the same results. version:1
arxiv-1601-02166 | Empirical Gaussian priors for cross-lingual transfer learning | http://arxiv.org/abs/1601.02166 | id:1601.02166 author:Anders Søgaard category:cs.CL  published:2016-01-09 summary:Sequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-of-speech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in $k$ languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the $k$ source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation. version:1
arxiv-1601-02124 | Kernelized LRR on Grassmann Manifolds for Subspace Clustering | http://arxiv.org/abs/1601.02124 | id:1601.02124 author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV  published:2016-01-09 summary:Low rank representation (LRR) has recently attracted great interest due to its pleasing efficacy in exploring low-dimensional sub- space structures embedded in data. One of its successful applications is subspace clustering, by which data are clustered according to the subspaces they belong to. In this paper, at a higher level, we intend to cluster subspaces into classes of subspaces. This is naturally described as a clustering problem on Grassmann manifold. The novelty of this paper is to generalize LRR on Euclidean space onto an LRR model on Grassmann manifold in a uniform kernelized LRR framework. The new method has many applications in data analysis in computer vision tasks. The proposed models have been evaluated on a number of practical data analysis applications. The experimental results show that the proposed models outperform a number of state-of-the-art subspace clustering methods. version:1
arxiv-1601-02098 | Supervised multiview learning based on simultaneous learning of multiview intact and single view classifier | http://arxiv.org/abs/1601.02098 | id:1601.02098 author:Qingjun Wang, Haiyan Lv, Jun Yue, Eugene Mitchell category:cs.CV  published:2016-01-09 summary:Multiview learning problem refers to the problem of learning a classifier from multiple view data. In this data set, each data points is presented by multiple different views. In this paper, we propose a novel method for this problem. This method is based on two assumptions. The first assumption is that each data point has an intact feature vector, and each view is obtained by a linear transformation from the intact vector. The second assumption is that the intact vectors are discriminative, and in the intact space, we have a linear classifier to separate the positive class from the negative class. We define an intact vector for each data point, and a view-conditional transformation matrix for each view, and propose to reconstruct the multiple view feature vectors by the product of the corresponding intact vectors and transformation matrices. Moreover, we also propose a linear classifier in the intact space, and learn it jointly with the intact vectors. The learning problem is modeled by a minimization problem, and the objective function is composed of a Cauchy error estimator-based view-conditional reconstruction term over all data points and views, and a classification error term measured by hinge loss over all the intact vectors of all the data points. Some regularization terms are also imposed to different variables in the objective function. The minimization problem is solve by an iterative algorithm using alternate optimization strategy and gradient descent algorithm. The proposed algorithm shows it advantage in the compression to other multiview learning algorithms on benchmark data sets. version:1
arxiv-1601-02088 | Multicuts and Perturb & MAP for Probabilistic Graph Clustering | http://arxiv.org/abs/1601.02088 | id:1601.02088 author:Jörg Hendrik Kappes, Paul Swoboda, Bogdan Savchynskyy, Tamir Hazan, Christoph Schnörr category:cs.CV  published:2016-01-09 summary:We present a probabilistic graphical model formulation for the graph clustering problem. This enables to locally represent uncertainty of image partitions by approximate marginal distributions in a mathematically substantiated way, and to rectify local data term cues so as to close contours and to obtain valid partitions. We exploit recent progress on globally optimal MAP inference by integer programming and on perturbation-based approximations of the log-partition function, in order to sample clusterings and to estimate marginal distributions of node-pairs both more accurately and more efficiently than state-of-the-art methods. Our approach works for any graphically represented problem instance. This is demonstrated for image segmentation and social network cluster analysis. Our mathematical ansatz should be relevant also for other combinatorial problems. version:1
arxiv-1509-00838 | What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment | http://arxiv.org/abs/1509.00838 | id:1509.00838 author:Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.CL cs.AI cs.LG cs.NE  published:2015-09-02 summary:We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved. version:2
arxiv-1507-03698 | Lifting GIS Maps into Strong Geometric Context for Scene Understanding | http://arxiv.org/abs/1507.03698 | id:1507.03698 author:Raúl Díaz, Minhaeng Lee, Jochen Schubert, Charless C. Fowlkes category:cs.CV  published:2015-07-14 summary:Contextual information can have a substantial impact on the performance of visual tasks such as semantic segmentation, object detection, and geometric estimation. Data stored in Geographic Information Systems (GIS) offers a rich source of contextual information that has been largely untapped by computer vision. We propose to leverage such information for scene understanding by combining GIS resources with large sets of unorganized photographs using Structure from Motion (SfM) techniques. We present a pipeline to quickly generate strong 3D geometric priors from 2D GIS data using SfM models aligned with minimal user input. Given an image resectioned against this model, we generate robust predictions of depth, surface normals, and semantic labels. We show that the precision of the predicted geometry is substantially more accurate other single-image depth estimation methods. We then demonstrate the utility of these contextual constraints for re-scoring pedestrian detections, and use these GIS contextual features alongside object detection score maps to improve a CRF-based semantic segmentation framework, boosting accuracy over baseline models. version:4
arxiv-1601-01974 | Scale-Free Online Learning | http://arxiv.org/abs/1601.01974 | id:1601.01974 author:Francesco Orabona, Dávid Pál category:cs.LG  published:2016-01-08 summary:We design algorithms for online linear optimization that have optimal regret and at the same time do not need to know any upper or lower bounds on the norm of the loss vectors. We achieve adaptiveness to the norms of the loss vectors by scale invariance, i.e., our algorithms make exactly the same decisions if the sequence of loss vectors is multiplied by any positive constant. One of our algorithms works for any decision set, bounded or unbounded. For unbounded decisions sets, this is the first adaptive algorithm for online linear optimization with a non-vacuous regret bound. We also study a popular scale-free variant of online mirror descent algorithm, and we show that in two natural settings it has linear or worse regret. version:1
arxiv-1601-01972 | Cox process representation and inference for stochastic reaction-diffusion processes | http://arxiv.org/abs/1601.01972 | id:1601.01972 author:David Schnoerr, Ramon Grima, Guido Sanguinetti category:cond-mat.stat-mech math.ST physics.data-an q-bio.QM stat.ML stat.TH  published:2016-01-08 summary:Complex behaviour in many systems arises from the stochastic interactions of spatially distributed particles or agents. Stochastic reaction-diffusion processes are widely used to model such behaviour in disciplines ranging from biology to the social sciences, yet they are notoriously difficult to simulate and calibrate to observational data. Here we use ideas from statistical physics and machine learning to provide a solution to the inverse problem of learning a stochastic reaction diffusion process to data. Our solution relies on a novel, non-trivial connection between stochastic reaction-diffusion processes and spatio-temporal Cox processes, a well-studied class of models from computational statistics. We develop an efficient and flexible algorithm which shows excellent accuracy on numeric and real data examples from systems biology and epidemiology. By using ideas from multiple disciplines, our approach provides both new and fundamental insights into spatio-temporal stochastic systems, and a practical solution to a long-standing problem in computational modelling. version:1
arxiv-1601-01966 | Numerical Coding of Nominal Data | http://arxiv.org/abs/1601.01966 | id:1601.01966 author:Zenon Gniazdowski, Michal Grabowski category:stat.ML  published:2016-01-08 summary:In this paper, a novel approach for coding nominal data is proposed. For the given nominal data, a rank in a form of complex number is assigned. The proposed method does not lose any information about the attribute and brings other properties previously unknown. The approach based on these knew properties can been used for classification. The analyzed example shows that classification with the use of coded nominal data or both numerical as well as coded nominal data is more effective than the classification, which uses only numerical data. version:1
arxiv-1506-01066 | Visualizing and Understanding Neural Models in NLP | http://arxiv.org/abs/1506.01066 | id:1506.01066 author:Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky category:cs.CL  published:2015-06-02 summary:While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets, version:2
arxiv-1206-6927 | Consistent Biclustering | http://arxiv.org/abs/1206.6927 | id:1206.6927 author:Cheryl J. Flynn, Patrick O. Perry category:stat.ME math.ST stat.ML stat.TH  published:2012-06-29 summary:Biclustering, the process of simultaneously clustering the rows and columns of a data matrix, is a popular and effective tool for finding structure in a high-dimensional dataset. Many biclustering procedures appear to work well in practice, but most do not have associated consistency guarantees. To address this shortcoming, we propose a new biclustering procedure based on profile likelihood. The procedure applies to a broad range of data modalities, including binary, count, and continuous observations. We prove that the procedure recovers the true row and column classes when the dimensions of the data matrix tend to infinity, even if the functional form of the data distribution is misspecified. The procedure requires computing a combinatorial search, which can be expensive in practice. Rather than performing this search directly, we propose a new heuristic optimization procedure based on the Kernighan-Lin heuristic, which has nice computational properties and performs well in simulations. We demonstrate our procedure with applications to congressional voting records, and microarray analysis. version:3
arxiv-1601-01944 | Nonparametric semi-supervised learning of class proportions | http://arxiv.org/abs/1601.01944 | id:1601.01944 author:Shantanu Jain, Martha White, Michael W. Trosset, Predrag Radivojac category:stat.ML cs.LG  published:2016-01-08 summary:The problem of developing binary classifiers from positive and unlabeled data is often encountered in machine learning. A common requirement in this setting is to approximate posterior probabilities of positive and negative classes for a previously unseen data point. This problem can be decomposed into two steps: (i) the development of accurate predictors that discriminate between positive and unlabeled data, and (ii) the accurate estimation of the prior probabilities of positive and negative examples. In this work we primarily focus on the latter subproblem. We study nonparametric class prior estimation and formulate this problem as an estimation of mixing proportions in two-component mixture models, given a sample from one of the components and another sample from the mixture itself. We show that estimation of mixing proportions is generally ill-defined and propose a canonical form to obtain identifiability while maintaining the flexibility to model any distribution. We use insights from this theory to elucidate the optimization surface of the class priors and propose an algorithm for estimating them. To address the problems of high-dimensional density estimation, we provide practical transformations to low-dimensional spaces that preserve class priors. Finally, we demonstrate the efficacy of our method on univariate and multivariate data. version:1
arxiv-1506-02554 | DUAL-LOCO: Distributing Statistical Estimation Using Random Projections | http://arxiv.org/abs/1506.02554 | id:1506.02554 author:Christina Heinze, Brian McWilliams, Nicolai Meinshausen category:stat.ML cs.DC cs.LG  published:2015-06-08 summary:We present DUAL-LOCO, a communication-efficient algorithm for distributed statistical estimation. DUAL-LOCO assumes that the data is distributed according to the features rather than the samples. It requires only a single round of communication where low-dimensional random projections are used to approximate the dependences between features available to different workers. We show that DUAL-LOCO has bounded approximation error which only depends weakly on the number of workers. We compare DUAL-LOCO against a state-of-the-art distributed optimization method on a variety of real world datasets and show that it obtains better speedups while retaining good accuracy. version:2
arxiv-1507-05371 | Regret Guarantees for Item-Item Collaborative Filtering | http://arxiv.org/abs/1507.05371 | id:1507.05371 author:Guy Bresler, Devavrat Shah, Luis F. Voloch category:cs.LG cs.IR cs.IT math.IT stat.ML  published:2015-07-20 summary:There is much empirical evidence that item-item collaborative filtering works well in practice. Motivated to understand this, we provide a framework to design and analyze various recommendation algorithms. The setup amounts to online binary matrix completion, where at each time a random user requests a recommendation and the algorithm chooses an entry to reveal in the user's row. The goal is to minimize regret, or equivalently to maximize the number of +1 entries revealed at any time. We analyze an item-item collaborative filtering algorithm that can achieve fundamentally better performance compared to user-user collaborative filtering. The algorithm achieves good "cold-start" performance (appropriately defined) by quickly making good recommendations to new users about whom there is little information. version:2
arxiv-1503-01313 | A Novel Performance Evaluation Methodology for Single-Target Trackers | http://arxiv.org/abs/1503.01313 | id:1503.01313 author:Matej Kristan, Jiri Matas, Ales Leonardis, Tomas Vojir, Roman Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli, Luka Cehovin category:cs.CV  published:2015-03-04 summary:This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical differences. A fully-annotated dataset with per-frame annotations with several visual attributes is introduced. The diversity of its visual properties is maximized in a novel way by clustering a large number of videos according to their visual attributes. This makes it the most sophistically constructed and annotated dataset to date. A multi-platform evaluation system allowing easy integration of third-party trackers is presented as well. The proposed evaluation methodology was tested on the VOT2014 challenge on the new dataset and 38 trackers, making it the largest benchmark to date. Most of the tested trackers are indeed state-of-the-art since they outperform the standard baselines, resulting in a highly-challenging benchmark. An exhaustive analysis of the dataset from the perspective of tracking difficulty is carried out. To facilitate tracker comparison a new performance visualization technique is proposed. version:3
arxiv-1412-4210 | Learning Precise Spike Train to Spike Train Transformations in Multilayer Feedforward Neuronal Networks | http://arxiv.org/abs/1412.4210 | id:1412.4210 author:Arunava Banerjee category:cs.NE  published:2014-12-13 summary:We derive a synaptic weight update rule for learning temporally precise spike train to spike train transformations in multilayer feedforward networks of spiking neurons. The framework, aimed at seamlessly generalizing error backpropagation to the deterministic spiking neuron setting, is based strictly on spike timing and avoids invoking concepts pertaining to spike rates or probabilistic models of spiking. The derivation is founded on two innovations. First, an error functional is proposed that compares the spike train emitted by the output neuron of the network to the desired spike train by way of their putative impact on a virtual postsynaptic neuron. This formulation sidesteps the need for spike alignment and leads to closed form solutions for all quantities of interest. Second, virtual assignment of weights to spikes rather than synapses enables a perturbation analysis of individual spike times and synaptic weights of the output as well as all intermediate neurons in the network, which yields the gradients of the error functional with respect to the said entities. Learning proceeds via a gradient descent mechanism that leverages these quantities. Simulation experiments demonstrate the efficacy of the proposed learning framework. The experiments also highlight asymmetries between synapses on excitatory and inhibitory neurons. version:2
arxiv-1510-01032 | Deep convolutional acoustic word embeddings using word-pair side information | http://arxiv.org/abs/1510.01032 | id:1510.01032 author:Herman Kamper, Weiran Wang, Karen Livescu category:cs.CL  published:2015-10-05 summary:Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task. version:2
arxiv-1601-01887 | Research Project: Text Engineering Tool for Ontological Scientometry | http://arxiv.org/abs/1601.01887 | id:1601.01887 author:Rustam Tagiew category:cs.CL cs.DL  published:2016-01-08 summary:The number of scientific papers grows exponentially in many disciplines. The share of online available papers grows as well. At the same time, the period of time for a paper to loose at chance to be cited anymore shortens. The decay of the citing rate shows similarity to ultradiffusional processes as for other online contents in social networks. The distribution of papers per author shows similarity to the distribution of posts per user in social networks. The rate of uncited papers for online available papers grows while some papers 'go viral' in terms of being cited. Summarized, the practice of scientific publishing moves towards the domain of social networks. The goal of this project is to create a text engineering tool, which can semi-automatically categorize a paper according to its type of contribution and extract relationships between them into an ontological database. Semi-automatic categorization means that the mistakes made by automatic pre-categorization and relationship-extraction will be corrected through a wikipedia-like front-end by volunteers from general public. This tool should not only help researchers and the general public to find relevant supplementary material and peers faster, but also provide more information for research funding agencies. version:1
arxiv-1601-01885 | Visual Script and Language Identification | http://arxiv.org/abs/1601.01885 | id:1601.01885 author:Anguelos Nicolaou, Andrew Bagdanov, Lluis Gomez-Bigorda, Dimosthenis Karatzas category:cs.CV  published:2016-01-08 summary:In this paper we introduce a script identification method based on hand-crafted texture features and an artificial neural network. The proposed pipeline achieves near state-of-the-art performance for script identification of video-text and state-of-the-art performance on visual language identification of handwritten text. More than using the deep network as a classifier, the use of its intermediary activations as a learned metric demonstrates remarkable results and allows the use of discriminative models on unknown classes. Comparative experiments in video-text and text in the wild datasets provide insights on the internals of the proposed deep network. version:1
arxiv-1601-01876 | Facial age estimation using BSIF and LBP | http://arxiv.org/abs/1601.01876 | id:1601.01876 author:Salah Eddine Bekhouche, Abdelkrim Ouafi, Abdelmalik Taleb-Ahmed, Abdenour Hadid, Azeddine Benlamoudi category:cs.CV  published:2016-01-08 summary:Human face aging is irreversible process causing changes in human face characteristics such us hair whitening, muscles drop and wrinkles. Due to the importance of human face aging in biometrics systems, age estimation became an attractive area for researchers. This paper presents a novel method to estimate the age from face images, using binarized statistical image features (BSIF) and local binary patterns (LBP)histograms as features performed by support vector regression (SVR) and kernel ridge regression (KRR). We applied our method on FG-NET and PAL datasets. Our proposed method has shown superiority to that of the state-of-the-art methods when using the whole PAL database. version:1
arxiv-1511-06739 | Superpixel Convolutional Networks using Bilateral Inceptions | http://arxiv.org/abs/1511.06739 | id:1511.06739 author:Raghudeep Gadde, Varun Jampani, Martin Kiefel, Peter V. Gehler category:cs.CV I.2.10; I.2.6  published:2015-11-20 summary:In this paper we propose a CNN architecture for image segmentation. We introduce a new "bilateral inception" layer that is used on top of a convolutional architecture. The bilateral inception performs a filtering between superpixels in an image. This addresses two problems that arise with CNN segmentation architectures. First, this layer propagates information between (super) pixels while respecting image edges, thus using the structured information of the problem for improved results. Second, the layer recovers a full resolution segmentation result from the lower resolution solution of a CNN. In the experiments we replace the deconvolution networks and Dense-CRF that have previously been proposed to address these problems with bilateral inception layers. The reduction to superpixels reduces the amount of computations and simplifies the network design. Further, we report better empirical results by replacing De-convolutional and CNN+Dense-CRF steps in four different semantic segmentation CNN architecutres, even with-out re-training their filter weights. version:3
arxiv-1511-05236 | Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets | http://arxiv.org/abs/1511.05236 | id:1511.05236 author:Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, Raquel Urtasun, Andreas Moshovos category:cs.LG cs.NE  published:2015-11-17 summary:This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%. version:4
arxiv-1508-01983 | Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance | http://arxiv.org/abs/1508.01983 | id:1508.01983 author:Amr Bakry, Mohamed Elhoseiny, Tarek El-Gaaly, Ahmed Elgammal category:cs.CV  published:2015-08-09 summary:This paper is focused on studying the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNN). There are several questions that this paper aims to answer: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can the structure of the view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions we propose a methodology to quantify the deformation and degeneracy of view manifolds in CNN layers. We apply this methodology and report interesting results in this paper that answer the aforementioned questions. version:3
arxiv-1511-00175 | FireCaffe: near-linear acceleration of deep neural network training on compute clusters | http://arxiv.org/abs/1511.00175 | id:1511.00175 author:Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, Kurt Keutzer category:cs.CV  published:2015-10-31 summary:Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -- Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup, respectively, when training on a cluster of 128 GPUs. version:2
arxiv-1506-08700 | Dropout as data augmentation | http://arxiv.org/abs/1506.08700 | id:1506.08700 author:Xavier Bouthillier, Kishore Konda, Pascal Vincent, Roland Memisevic category:stat.ML cs.LG  published:2015-06-29 summary:Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cost. version:4
arxiv-1511-04377 | Learning Dense Convolutional Embeddings for Semantic Segmentation | http://arxiv.org/abs/1511.04377 | id:1511.04377 author:Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos category:cs.CV  published:2015-11-13 summary:This paper proposes a new deep convolutional neural network (DCNN) architecture that learns pixel embeddings, such that pairwise distances between the embeddings can be used to infer whether or not the pixels lie on the same region. That is, for any two pixels on the same object, the embeddings are trained to be similar; for any pair that straddles an object boundary, the embeddings are trained to be dissimilar. Experimental results show that when this embedding network is used in conjunction with a DCNN trained on semantic segmentation, there is a systematic improvement in per-pixel classification accuracy. Our contributions are integrated in the popular Caffe deep learning framework, and consist in straightforward modifications to convolution routines. As such, they can be exploited for any task involving convolution layers. version:3
arxiv-1511-04397 | Similarity-based Text Recognition by Deeply Supervised Siamese Network | http://arxiv.org/abs/1511.04397 | id:1511.04397 author:Ehsan Hosseini-Asl, Angshuman Guha category:cs.CV cs.LG  published:2015-11-13 summary:In this paper, we propose a new text recognition model based on measuring the visual similarity of text and predicting the content of unlabeled texts. First a Siamese convolutional network is trained with deep supervision on a labeled training dataset. This network projects texts into a similarity manifold. The Deeply Supervised Siamese network learns visual similarity of texts. Then a K-nearest neighbor classifier is used to predict unlabeled text based on similarity distance to labeled texts. The performance of the model is evaluated on three datasets of machine-print and hand-written text combined. We demonstrate that the model reduces the cost of human estimation by $50\%-85\%$. The error of the system is less than $0.5\%$. The proposed model outperform conventional Siamese network by finding visually-similar barely-readable and readable text, e.g. machine-printed, handwritten, due to deep supervision. The results also demonstrate that the predicted labels are sometimes better than human labels e.g. spelling correction. version:3
arxiv-1511-06437 | A convnet for non-maximum suppression | http://arxiv.org/abs/1511.06437 | id:1511.06437 author:Jan Hosang, Rodrigo Benenson, Bernt Schiele category:cs.CV cs.LG  published:2015-11-19 summary:Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, and results on crowded pedestrian detection scenes. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision. version:3
arxiv-1511-06333 | Efficient Sum of Sparse Outer Products Dictionary Learning (SOUP-DIL) | http://arxiv.org/abs/1511.06333 | id:1511.06333 author:Saiprasad Ravishankar, Raj Rao Nadakuditi, Jeffrey A. Fessler category:cs.LG  published:2015-11-19 summary:The sparsity of natural signals in a transform domain or dictionary has been extensively exploited in several applications. More recently, the data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionaries. However, dictionary learning problems are typically non-convex and NP-hard, and the usual alternating minimization approaches for these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for $\ell_{0}$ "norm"-based dictionary learning by first approximating the training data set with a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the rank-one terms. The proposed algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent method. Our experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising. version:3
arxiv-1511-06434 | Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks | http://arxiv.org/abs/1511.06434 | id:1511.06434 author:Alec Radford, Luke Metz, Soumith Chintala category:cs.LG cs.CV  published:2015-11-19 summary:In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations. version:2
arxiv-1511-06423 | An Information Retrieval Approach to Finding Dependent Subspaces of Multiple Views | http://arxiv.org/abs/1511.06423 | id:1511.06423 author:Ziyuan Lin, Jaakko Peltonen category:stat.ML cs.LG  published:2015-11-19 summary:Finding relationships between multiple views of data is essential both for exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of \textit{neighbor retrieval between multiple views}. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, is able to measure dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments we show the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views. version:2
arxiv-1511-06297 | Conditional Computation in Neural Networks for faster models | http://arxiv.org/abs/1511.06297 | id:1511.06297 author:Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, Doina Precup category:cs.LG  published:2015-11-19 summary:Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation. version:2
arxiv-1511-06407 | Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition | http://arxiv.org/abs/1511.06407 | id:1511.06407 author:Suyoun Kim, Ian Lane category:cs.LG cs.CL  published:2015-11-19 summary:Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multi-channel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-to-sequence learning, no attention mechanisms have previously been applied to learn potentially asynchronous and non-stationary multiple inputs. We evaluate our neural attention model on the CHiME-3 challenge task, and show that the model achieves comparable performance to beamforming using a purely data-driven method. version:2
arxiv-1511-06855 | Discovering Internal Representations from Object-CNNs Using Population Encoding | http://arxiv.org/abs/1511.06855 | id:1511.06855 author:Jianyu Wang, Zhishuai Zhang, Vittal Premachandran, Alan Yuille category:cs.LG cs.CV  published:2015-11-21 summary:In this paper, we provide a method for understanding the internal representations of Convolutional Neural Networks (CNNs) trained on objects. We hypothesize that the information is distributed across multiple neuronal responses and propose a simple clustering technique to extract this information, which we call \emph{population encoding}. The population encoding technique looks into the entrails of an object-CNN at multiple layers of the network and shows the implicit presence of mid-level object part semantics distributed in the neuronal responses. Our qualitative visualizations show that population encoding can extract mid-level image patches that are visually tighter than the patches that produce high single-filter activations. Moreover, our comprehensive quantitative experiments using the object key point annotations from the PASCAL3D+ dataset corroborate the visualizations by demonstrating the superiority of population encoding over single-filter detectors, in the task of object-part detection. We also perform some preliminary experiments where we uncover the compositional relations between the adjacent layers using the parts detected by population encoding clusters. Finally, based on the insights gained from this work, we point to various new directions which will enable us to have a better understanding of the CNN's internal representations. version:2
arxiv-1511-06328 | Manifold Regularized Discriminative Neural Networks | http://arxiv.org/abs/1511.06328 | id:1511.06328 author:Shuangfei Zhai, Zhongfei Zhang category:cs.LG  published:2015-11-19 summary:Unregularized deep neural networks (DNNs) can be easily overfit with a limited sample size. We argue that this is mostly due to the disriminative nature of DNNs which directly model the conditional probability (or score) of labels given the input. The ignorance of input distribution makes DNNs difficult to generalize to unseen data. Recent advances in regularization techniques, such as pretraining and dropout, indicate that modeling input data distribution (either explicitly or implicitly) greatly improves the generalization ability of a DNN. In this work, we explore the manifold hypothesis which assumes that instances within the same class lie in a smooth manifold. We accordingly propose two simple regularizers to a standard discriminative DNN. The first one, named Label-Aware Manifold Regularization, assumes the availability of labels and penalizes large norms of the loss function w.r.t. data points. The second one, named Label-Independent Manifold Regularization, does not use label information and instead penalizes the Frobenius norm of the Jacobian matrix of prediction scores w.r.t. data points, which makes semi-supervised learning possible. We perform extensive control experiments on fully supervised and semi-supervised tasks using the MNIST, CIFAR10 and SVHN datasets and achieve excellent results. version:3
arxiv-1511-06348 | How much data is needed to train a medical image deep learning system to achieve necessary high accuracy? | http://arxiv.org/abs/1511.06348 | id:1511.06348 author:Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, Synho Do category:cs.LG cs.CV cs.NE  published:2015-11-19 summary:The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems. version:2
arxiv-1511-06841 | Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification | http://arxiv.org/abs/1511.06841 | id:1511.06841 author:Kyuyeon Hwang, Wonyong Sung category:cs.LG cs.NE  published:2015-11-21 summary:Connectionist temporal classification (CTC) based supervised sequence training of recurrent neural networks (RNNs) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition. For the CTC training, however, it is required to unroll (or unfold) the RNN by the length of an input sequence. This unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation. Furthermore, the length of training sequences is usually not uniform, which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs). In this work, we introduce an expectation-maximization (EM) based online CTC algorithm that enables unidirectional RNNs to learn sequences that are longer than the amount of unrolling. The RNNs can also be trained to process an infinitely long input sequence without pre-segmentation or external reset. Moreover, the proposed approach allows efficient parallel training on GPUs. For evaluation, phoneme recognition and end-to-end speech recognition examples are presented on the TIMIT and Wall Street Journal (WSJ) corpora, respectively. Our online model achieves 20.7% phoneme error rate (PER) on the very long input sequence that is generated by concatenating all 192 utterances in the TIMIT core test set. On WSJ, a network can be trained with only 64 times of unrolling while sacrificing 4.5% relative word error rate (WER). version:4
arxiv-1601-01660 | An Automaton Learning Approach to Solving Safety Games over Infinite Graphs | http://arxiv.org/abs/1601.01660 | id:1601.01660 author:Daniel Neider, Ufuk Topcu category:cs.FL cs.LG cs.LO 05C57 F.1.1; I.2.6  published:2016-01-07 summary:We propose a method to construct finite-state reactive controllers for systems whose interactions with their adversarial environment are modeled by infinite-duration two-player games over (possibly) infinite graphs. The proposed method targets safety games with infinitely many states or with such a large number of states that it would be impractical---if not impossible---for conventional synthesis techniques that work on the entire state space. We resort to constructing finite-state controllers for such systems through an automata learning approach, utilizing a symbolic representation of the underlying game that is based on finite automata. Throughout the learning process, the learner maintains an approximation of the winning region (represented as a finite automaton) and refines it using different types of counterexamples provided by the teacher until a satisfactory controller can be derived (if one exists). We present a symbolic representation of safety games (inspired by regular model checking), propose implementations of the learner and teacher, and evaluate their performance on examples motivated by robotic motion planning in dynamic environments. version:1
arxiv-1504-08362 | PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions | http://arxiv.org/abs/1504.08362 | id:1504.08362 author:Michael Figurnov, Dmitry Vetrov, Pushmeet Kohli category:cs.CV  published:2015-04-30 summary:We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. Our method allows to reduce the evaluation time of modern convolutional neural networks by 50% with a small decrease in accuracy. version:3
arxiv-1601-01653 | Large Collection of Diverse Gene Set Search Queries Recapitulate Known Protein-Protein Interactions and Gene-Gene Functional Associations | http://arxiv.org/abs/1601.01653 | id:1601.01653 author:Avi Ma'ayan, Neil R. Clark category:q-bio.MN cs.AI cs.SI q-bio.GN stat.ML  published:2016-01-07 summary:Popular online enrichment analysis tools from the field of molecular systems biology provide users with the ability to submit their experimental results as gene sets for individual analysis. Such queries are kept private, and have never before been considered as a resource for integrative analysis. By harnessing gene set query submissions from thousands of users, we aim to discover biological knowledge beyond the scope of an individual study. In this work, we investigated a large collection of gene sets submitted to the tool Enrichr by thousands of users. Based on co-occurrence, we constructed a global gene-gene association network. We interpret this inferred network as providing a summary of the structure present in this crowdsourced gene set library, and show that this network recapitulates known protein-protein interactions and functional associations between genes. This finding implies that this network also offers predictive value. Furthermore, we visualize this gene-gene association network using a new edge-pruning algorithm that retains both the local and global structures of large-scale networks. Our ability to make predictions for currently unknown gene associations, that may not be captured by individual researchers and data sources, is a demonstration of the potential of harnessing collective knowledge from users of popular tools in the field of molecular systems biology. version:1
