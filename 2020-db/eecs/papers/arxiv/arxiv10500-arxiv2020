arxiv-1503-01838 | Encoding Source Language with Convolutional Neural Network for Machine Translation | http://arxiv.org/abs/1503.01838 | id:1503.01838 author:Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, Qun Liu category:cs.CL cs.LG cs.NE  published:2015-03-06 summary:The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average version:5
arxiv-1506-04954 | A Tensor-Based Dictionary Learning Approach to Tomographic Image Reconstruction | http://arxiv.org/abs/1506.04954 | id:1506.04954 author:Sara Soltani, Misha E. Kilmer, Per Christian Hansen category:cs.CV cs.NA math.NA 15A69  65F22  65K10  published:2015-06-08 summary:We consider tomographic reconstruction using priors in the form of a dictionary learned from training images. The reconstruction has two stages: first we construct a tensor dictionary prior from our training data, and then we pose the reconstruction problem in terms of recovering the expansion coefficients in that dictionary. Our approach differs from past approaches in that a) we use a third-order tensor representation for our images and b) we recast the reconstruction problem using the tensor formulation. The dictionary learning problem is presented as a non-negative tensor factorization problem with sparsity constraints. The reconstruction problem is formulated in a convex optimization framework by looking for a solution with a sparse representation in the tensor dictionary. Numerical results show that our tensor formulation leads to very sparse representations of both the training images and the reconstructions due to the ability of representing repeated features compactly in the dictionary. version:1
arxiv-1406-3469 | LOCO: Distributing Ridge Regression with Random Projections | http://arxiv.org/abs/1406.3469 | id:1406.3469 author:Christina Heinze, Brian McWilliams, Nicolai Meinshausen, Gabriel Krummenacher category:stat.ML  published:2014-06-13 summary:We propose LOCO, an algorithm for large-scale ridge regression which distributes the features across workers on a cluster. Important dependencies between variables are preserved using structured random projections which are cheap to compute and must only be communicated once. We show that LOCO obtains a solution which is close to the exact ridge regression solution in the fixed design setting. We verify this experimentally in a simulation study as well as an application to climate prediction. Furthermore, we show that LOCO achieves significant speedups compared with a state-of-the-art distributed algorithm on a large-scale regression problem. version:4
arxiv-1503-05528 | Video Inpainting of Complex Scenes | http://arxiv.org/abs/1503.05528 | id:1503.05528 author:Alasdair Newson, Andrés Almansa, Matthieu Fradet, Yann Gousseau, Patrick Pérez category:cs.CV cs.MM math.NA  published:2015-03-18 summary:We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the inpainting mask, and can deal with a wider variety of situations than is handled by previous work. 1. Introduction. Advanced image and video editing techniques are increasingly common in the image processing and computer vision world, and are also starting to be used in media entertainment. One common and difficult task closely linked to the world of video editing is image and video " inpainting ". Generally speaking, this is the task of replacing the content of an image or video with some other content which is visually pleasing. This subject has been extensively studied in the case of images, to such an extent that commercial image inpainting products destined for the general public are available, such as Photoshop's " Content Aware fill " [1]. However, while some impressive results have been obtained in the case of videos, the subject has been studied far less extensively than image inpainting. This relative lack of research can largely be attributed to high time complexity due to the added temporal dimension. Indeed, it has only very recently become possible to produce good quality inpainting results on high definition videos, and this only in a semi-automatic manner. Nevertheless, high-quality video inpainting has many important and useful applications such as film restoration, professional post-production in cinema and video editing for personal use. For this reason, we believe that an automatic, generic video inpainting algorithm would be extremely useful for both academic and professional communities. version:2
arxiv-1506-02361 | Microscopic approach of a time elapsed neural model | http://arxiv.org/abs/1506.02361 | id:1506.02361 author:Julien Chevallier, Maria J. Caceres, Marie Doumic, Patricia Reynaud-Bouret category:cs.NE  published:2015-06-08 summary:The spike trains are the main components of the information processing in the brain. To model spike trains several point processes have been investigated in the literature. And more macroscopic approaches have also been studied, using partial differential equation models. The main aim of the present article is to build a bridge between several point processes models (Poisson, Wold, Hawkes) that have been proved to statistically fit real spike trains data and age-structured partial differential equations as introduced by Pakdaman, Perthame and Salort. version:1
arxiv-1406-7443 | Efficient Learning in Large-Scale Combinatorial Semi-Bandits | http://arxiv.org/abs/1406.7443 | id:1406.7443 author:Zheng Wen, Branislav Kveton, Azin Ashkan category:cs.LG cs.AI stat.ML  published:2014-06-28 summary:A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. version:3
arxiv-1502-03919 | Policy Gradient for Coherent Risk Measures | http://arxiv.org/abs/1502.03919 | id:1502.03919 author:Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor category:cs.AI cs.LG stat.ML  published:2015-02-13 summary:Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results. version:2
arxiv-1506-02348 | Convergence Rates of Active Learning for Maximum Likelihood Estimation | http://arxiv.org/abs/1506.02348 | id:1506.02348 author:Kamalika Chaudhuri, Sham Kakade, Praneeth Netrapalli, Sujay Sanghavi category:cs.LG stat.ML  published:2015-06-08 summary:An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting -- maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation. On the empirical side, the recent work in ~\cite{Zhang12} and~\cite{Zhang14} (on active linear and logistic regression) shows the promise of this approach. version:1
arxiv-1506-02345 | Wavelets and continuous wavelet transform for autostereoscopic multiview images | http://arxiv.org/abs/1506.02345 | id:1506.02345 author:Vladimir Saveljev category:cs.CV  published:2015-06-08 summary:Recently, the reference functions for the synthesis and analysis of the autostereoscopic multiview and integral images in three-dimensional displays we introduced. In the current paper, we propose the wavelets to analyze such images. The wavelets are built on the reference functions as on the scaling functions of the wavelet analysis. The continuous wavelet transform was successfully applied to the testing wireframe binary objects. The restored locations correspond to the structure of the testing wireframe binary objects. version:1
arxiv-1506-02327 | A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features | http://arxiv.org/abs/1506.02327 | id:1506.02327 author:Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Yuan-ming Liou, Yen-Chen Wu, Yen-Ju Lu, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG cs.NE  published:2015-06-07 summary:This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target DNN (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are used as feedback for the MAT and the MDNN itself. We call this iterative system the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge. version:1
arxiv-1506-02312 | A Framework for Constrained and Adaptive Behavior-Based Agents | http://arxiv.org/abs/1506.02312 | id:1506.02312 author:Renato de Pontes Pereira, Paulo Martins Engel category:cs.AI cs.LG cs.RO cs.SY  published:2015-06-07 summary:Behavior Trees are commonly used to model agents for robotics and games, where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions. In such application areas, learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment, but often discarded due to its unreliability. In this paper, we propose a framework that uses Reinforcement Learning nodes as part of Behavior Trees to address the problem of adding learning capabilities in constrained agents. We show how this framework relates to Options in Hierarchical Reinforcement Learning, ensuring convergence of nested learning nodes, and we empirically show that the learning nodes do not affect the execution of other nodes in the tree. version:1
arxiv-1506-02306 | SQUINKY! A Corpus of Sentence-level Formality, Informativeness, and Implicature | http://arxiv.org/abs/1506.02306 | id:1506.02306 author:Shibamouli Lahiri category:cs.CL  published:2015-06-07 summary:We introduce a corpus of 7,032 sentences rated by human annotators for formality, informativeness, and implicature on a 1-7 scale. The corpus was annotated using Amazon Mechanical Turk. Reliability in the obtained judgments was examined by comparing mean ratings across two MTurk experiments, and correlation with pilot annotations (on sentence formality) conducted in a more controlled setting. Despite the subjectivity and inherent difficulty of the annotation task, correlations between mean ratings were quite encouraging, especially on formality and informativeness. We further explored correlation between the three linguistic variables, genre-wise variation of ratings and correlations within genres, compatibility with automatic stylistic scoring, and sentential make-up of a document in terms of style. To date, our corpus is the largest sentence-level annotated corpus released for formality, informativeness, and implicature. version:1
arxiv-1409-2752 | Winner-Take-All Autoencoders | http://arxiv.org/abs/1409.2752 | id:1409.2752 author:Alireza Makhzani, Brendan Frey category:cs.LG cs.NE  published:2014-09-09 summary:In this paper, we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion. We first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units. We then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations. We describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner-take-all activation functions. We will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets, and achieve competitive classification performance. version:2
arxiv-1506-02278 | Optimal Ridge Detection using Coverage Risk | http://arxiv.org/abs/1506.02278 | id:1506.02278 author:Yen-Chi Chen, Christopher R. Genovese, Shirley Ho, Larry Wasserman category:stat.ME stat.ML  published:2015-06-07 summary:We introduce the concept of coverage risk as an error measure for density ridge estimation. The coverage risk generalizes the mean integrated square error to set estimation. We propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk. We study the rate of convergence for coverage risk and prove consistency of the risk estimators. We apply our method to three simulated datasets and to cosmology data. In all the examples, the proposed method successfully recover the underlying density structure. version:1
arxiv-1506-02265 | Randomized Structural Sparsity based Support Identification with Applications to Locating Activated or Discriminative Brain Areas: A Multi-center Reproducibility Study | http://arxiv.org/abs/1506.02265 | id:1506.02265 author:Yilun Wang, Sheng Zhang, Junjie Zheng, Heng Chen, Huafu Chen category:cs.CV 68T01 I.5.4  published:2015-06-07 summary:In this paper, we focus on how to locate the relevant or discriminative brain regions related with external stimulus or certain mental decease, which is also called support identification, based on the neuroimaging data. The main difficulty lies in the extremely high dimensional voxel space and relatively few training samples, easily resulting in an unstable brain region discovery (or called feature selection in context of pattern recognition). When the training samples are from different centers and have betweencenter variations, it will be even harder to obtain a reliable and consistent result. Corresponding, we revisit our recently proposed algorithm based on stability selection and structural sparsity. It is applied to the multi-center MRI data analysis for the first time. A consistent and stable result is achieved across different centers despite the between-center data variation while many other state-of-the-art methods such as two sample t-test fail. Moreover, we have empirically showed that the performance of this algorithm is robust and insensitive to several of its key parameters. In addition, the support identification results on both functional MRI and structural MRI are interpretable and can be the potential biomarkers. version:1
arxiv-1506-02256 | Knowledge Transfer Pre-training | http://arxiv.org/abs/1506.02256 | id:1506.02256 author:Zhiyuan Tang, Dong Wang, Yiqiao Pan, Zhiyong Zhang category:cs.LG cs.NE stat.ML  published:2015-06-07 summary:Pre-training is crucial for learning deep neural networks. Most of existing pre-training methods train simple models (e.g., restricted Boltzmann machines) and then stack them layer by layer to form the deep structure. This layer-wise pre-training has found strong theoretical foundation and broad empirical support. However, it is not easy to employ such method to pre-train models without a clear multi-layer structure,e.g., recurrent neural networks (RNNs). This paper presents a new pre-training approach based on knowledge transfer learning. In contrast to the layer-wise approach which trains model components incrementally, the new approach trains the entire model as a whole but with an easier objective function. This is achieved by utilizing soft targets produced by a prior trained model (teacher model). Compared to the conventional layer-wise methods, this new method does not care about the model structure, so can be used to pre-train very complex models. Experiments on a speech recognition task demonstrated that with this approach, complex RNNs can be well trained with a weaker deep neural network (DNN) model. Furthermore, the new method can be combined with conventional layer-wise pre-training to deliver additional gains. version:1
arxiv-1502-04993 | Reconstruction of recurrent synaptic connectivity of thousands of neurons from simulated spiking activity | http://arxiv.org/abs/1502.04993 | id:1502.04993 author:Yury V. Zaytsev, Abigail Morrison, Moritz Deger category:q-bio.NC q-bio.QM stat.ML  published:2015-02-17 summary:Dynamics and function of neuronal networks are determined by their synaptic connectivity. Current experimental methods to analyze synaptic network structure on the cellular level, however, cover only small fractions of functional neuronal circuits, typically without a simultaneous record of neuronal spiking activity. Here we present a method for the reconstruction of large recurrent neuronal networks from thousands of parallel spike train recordings. We employ maximum likelihood estimation of a generalized linear model of the spiking activity in continuous time. For this model the point process likelihood is concave, such that a global optimum of the parameters can be obtained by gradient ascent. Previous methods, including those of the same class, did not allow recurrent networks of that order of magnitude to be reconstructed due to prohibitive computational cost and numerical instabilities. We describe a minimal model that is optimized for large networks and an efficient scheme for its parallelized numerical optimization on generic computing clusters. For a simulated balanced random network of 1000 neurons, synaptic connectivity is recovered with a misclassification error rate of less than 1% under ideal conditions. We show that the error rate remains low in a series of example cases under progressively less ideal conditions. Finally, we successfully reconstruct the connectivity of a hidden synfire chain that is embedded in a random network, which requires clustering of the network connectivity to reveal the synfire groups. Our results demonstrate how synaptic connectivity could potentially be inferred from large-scale parallel spike train recordings. version:2
arxiv-1506-02239 | String Gaussian Process Kernels | http://arxiv.org/abs/1506.02239 | id:1506.02239 author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML  published:2015-06-07 summary:We introduce a new class of nonstationary kernels, which we derive as covariance functions of a novel family of stochastic processes we refer to as string Gaussian processes (string GPs). We construct string GPs to allow for multiple types of local patterns in the data, while ensuring a mild global regularity condition. In this paper, we illustrate the efficacy of the approach using synthetic data and demonstrate that the model outperforms competing approaches on well studied, real-life datasets that exhibit nonstationary features. version:1
arxiv-1506-02227 | Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex Losses | http://arxiv.org/abs/1506.02227 | id:1506.02227 author:Dominik Csiba, Peter Richtárik category:math.OC cs.DS cs.LG stat.ML  published:2015-06-07 summary:In this work we develop a new algorithm for regularized empirical risk minimization. Our method extends recent techniques of Shalev-Shwartz [02/2015], which enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes. Moreover, our method is able to better utilize the information in the data defining the ERM problem. For convex loss functions, our complexity results match those of QUARTZ, which is a primal-dual method also allowing for arbitrary mini-batching schemes. The advantage of a dual-free analysis comes from the fact that it guarantees convergence even for non-convex loss functions, as long as the average loss is convex. We illustrate through experiments the utility of being able to design arbitrary mini-batching schemes. version:1
arxiv-1410-4650 | Randomized Structural Sparsity via Constrained Block Subsampling for Improved Sensitivity of Discriminative Voxel Identification | http://arxiv.org/abs/1410.4650 | id:1410.4650 author:Yilun Wang, Junjie Zheng, Sheng Zhang, Xujun Duan, Huafu Chen category:cs.CV stat.ML G.3  I.5.2  published:2014-10-17 summary:In this paper, we consider voxel selection for functional Magnetic Resonance Imaging (fMRI) brain data with the aim of finding a more complete set of probably correlated discriminative voxels, thus improving interpretation of the discovered potential biomarkers. The main difficulty in doing this is an extremely high dimensional voxel space and few training samples, resulting in unreliable feature selection. In order to deal with the difficulty, stability selection has received a great deal of attention lately, especially due to its finite sample control of false discoveries and transparent principle for choosing a proper amount of regularization. However, it fails to make explicit use of the correlation property or structural information of these discriminative features and leads to large false negative rates. In other words, many relevant but probably correlated discriminative voxels are missed. Thus, we propose a new variant on stability selection "randomized structural sparsity", which incorporates the idea of structural sparsity. Numerical experiments demonstrate that our method can be superior in controlling for false negatives while also keeping the control of false positives inherited from stability selection. version:2
arxiv-1506-02211 | Boosting Optical Character Recognition: A Super-Resolution Approach | http://arxiv.org/abs/1506.02211 | id:1506.02211 author:Chao Dong, Ximei Zhu, Yubin Deng, Chen Change Loy, Yu Qiao category:cs.CV I.4.3; I.4.9  published:2015-06-07 summary:Text image super-resolution is a challenging yet open research problem in the computer vision community. In particular, low-resolution images hamper the performance of typical optical character recognition (OCR) systems. In this article, we summarize our entry to the ICDAR2015 Competition on Text Image Super-Resolution. Experiments are based on the provided ICDAR2015 TextSR dataset and the released Tesseract-OCR 3.02 system. We report that our winning entry of text image super-resolution framework has largely improved the OCR performance with low-resolution images used as input, reaching an OCR accuracy score of 77.19%, which is comparable with that of using the original high-resolution images 78.80%. version:1
arxiv-1506-02203 | Describing Common Human Visual Actions in Images | http://arxiv.org/abs/1506.02203 | id:1506.02203 author:Matteo Ruggero Ronchi, Pietro Perona category:cs.CV  published:2015-06-07 summary:Which common human actions and interactions are recognizable in monocular still images? Which involve objects and/or other people? How many is a person performing at a time? We address these questions by exploring the actions and interactions that are detectable in the images of the MS COCO dataset. We make two main contributions. First, a list of 140 common `visual actions', obtained by analyzing the largest on-line verb lexicon currently available for English (VerbNet) and human sentences used to describe images in MS COCO. Second, a complete set of annotations for those `visual actions', composed of subject-object and associated verb, which we call COCO-a (a for `actions'). COCO-a is larger than existing action datasets in terms of number of actions and instances of these actions, and is unique because it is data-driven, rather than experimenter-biased. Other unique features are that it is exhaustive, and that all subjects and objects are localized. A statistical analysis of the accuracy of our annotations and of each action, interaction and subject-object combination is provided. version:1
arxiv-1506-02194 | Fast Mixing for Discrete Point Processes | http://arxiv.org/abs/1506.02194 | id:1506.02194 author:Patrick Rebeschini, Amin Karbasi category:stat.ML math.ST stat.TH  published:2015-06-06 summary:We investigate the systematic mechanism for designing fast mixing Markov chain Monte Carlo algorithms to sample from discrete point processes under the Dobrushin uniqueness condition for Gibbs measures. Discrete point processes are defined as probability distributions $\mu(S)\propto \exp(\beta f(S))$ over all subsets $S\in 2^V$ of a finite set $V$ through a bounded set function $f:2^V\rightarrow \mathbb{R}$ and a parameter $\beta>0$. A subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage. We show that if the set function (not necessarily submodular) displays a natural notion of decay of correlation, then, for $\beta$ small enough, it is possible to design fast mixing Markov chain Monte Carlo methods that yield error bounds on marginal approximations that do not depend on the size of the set $V$. The sufficient conditions that we derive involve a control on the (discrete) Hessian of set functions, a quantity that has not been previously considered in the literature. We specialize our results for submodular functions, and we discuss canonical examples where the Hessian can be easily controlled. version:1
arxiv-1506-02184 | First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos | http://arxiv.org/abs/1506.02184 | id:1506.02184 author:Jun Ye, Hao Hu, Kai Li, Guo-Jun Qi, Kien A. Hua category:cs.CV  published:2015-06-06 summary:With the prevalence of the commodity depth cameras, the new paradigm of user interfaces based on 3D motion capturing and recognition have dramatically changed the way of interactions between human and computers. Human action recognition, as one of the key components in these devices, plays an important role to guarantee the quality of user experience. Although the model-driven methods have achieved huge success, they cannot provide a scalable solution for efficiently storing, retrieving and recognizing actions in the large-scale applications. These models are also vulnerable to the temporal translation and warping, as well as the variations in motion scales and execution rates. To address these challenges, we propose to treat the 3D human action recognition as a video-level hashing problem and propose a novel First-Take-All (FTA) Hashing algorithm capable of hashing the entire video into hash codes of fixed length. We demonstrate that this FTA algorithm produces a compact representation of the video invariant to the above mentioned variations, through which action recognition can be solved by an efficient nearest neighbor search by the Hamming distance between the FTA hash codes. Experiments on the public 3D human action datasets shows that the FTA algorithm can reach a recognition accuracy higher than 80%, with about 15 bits per frame considering there are 65 frames per video over the datasets. version:1
arxiv-1506-02170 | Hybridized Feature Extraction and Acoustic Modelling Approach for Dysarthric Speech Recognition | http://arxiv.org/abs/1506.02170 | id:1506.02170 author:Megha Rughani, D. Shivakrishna category:cs.SD cs.CL  published:2015-06-06 summary:Dysarthria is malfunctioning of motor speech caused by faintness in the human nervous system. It is characterized by the slurred speech along with physical impairment which restricts their communication and creates the lack of confidence and affects the lifestyle. This paper attempt to increase the efficiency of Automatic Speech Recognition (ASR) system for unimpaired speech signal. It describes state of art of research into improving ASR for speakers with dysarthria by means of incorporated knowledge of their speech production. Hybridized approach for feature extraction and acoustic modelling technique along with evolutionary algorithm is proposed for increasing the efficiency of the overall system. Here number of feature vectors are varied and tested the system performance. It is observed that system performance is boosted by genetic algorithm. System with 16 acoustic features optimized with genetic algorithm has obtained highest recognition rate of 98.28% with training time of 5:30:17. version:1
arxiv-1501-00102 | ModDrop: adaptive multi-modal gesture recognition | http://arxiv.org/abs/1501.00102 | id:1501.00102 author:Natalia Neverova, Christian Wolf, Graham W. Taylor, Florian Nebout category:cs.CV cs.HC cs.LG  published:2014-12-31 summary:We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio. version:2
arxiv-1307-5934 | A Near-Optimal Dynamic Learning Algorithm for Online Matching Problems with Concave Returns | http://arxiv.org/abs/1307.5934 | id:1307.5934 author:Xiao Alison Chen, Zizhuo Wang category:cs.DS cs.LG math.OC  published:2013-07-23 summary:We consider an online matching problem with concave returns. This problem is a significant generalization of the Adwords allocation problem and has vast applications in online advertising. In this problem, a sequence of items arrive sequentially and each has to be allocated to one of the bidders, who bid a certain value for each item. At each time, the decision maker has to allocate the current item to one of the bidders without knowing the future bids and the objective is to maximize the sum of some concave functions of each bidder's aggregate value. In this work, we propose an algorithm that achieves near-optimal performance for this problem when the bids arrive in a random order and the input data satisfies certain conditions. The key idea of our algorithm is to learn the input data pattern dynamically: we solve a sequence of carefully chosen partial allocation problems and use their optimal solutions to assist with the future decision. Our analysis belongs to the primal-dual paradigm, however, the absence of linearity of the objective function and the dynamic feature of the algorithm makes our analysis quite unique. version:3
arxiv-1502-06895 | On the consistency theory of high dimensional variable screening | http://arxiv.org/abs/1502.06895 | id:1502.06895 author:Xiangyu Wang, Chenlei Leng, David B. Dunson category:math.ST cs.LG stat.ML stat.TH  published:2015-02-24 summary:Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension $p$ is substantially larger than the sample size $n$, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods $SIS$ and $HOLP$ are both strong screening consistent (subject to additional constraints) with large probability if $n > O((\rho s + \sigma/\tau)^2\log p)$ under random designs. In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of $SIS$. version:3
arxiv-1506-02117 | Learning Multiple Tasks with Deep Relationship Networks | http://arxiv.org/abs/1506.02117 | id:1506.02117 author:Mingsheng Long, Jianmin Wang category:cs.LG  published:2015-06-06 summary:Deep neural networks trained on large-scale dataset can learn transferable features that promote learning multiple tasks for inductive transfer and labeling mitigation. As deep features eventually transition from general to specific along the network, a fundamental problem is how to exploit the relationship structure across different tasks while accounting for the feature transferability in the task-specific layers. In this work, we propose a novel Deep Relationship Network (DRN) architecture for multi-task learning by discovering correlated tasks based on multiple task-specific layers of a deep convolutional neural network. DRN models the task relationship by imposing matrix normal priors over the network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely. By jointly learning the transferable features and task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Empirical evidence shows that DRN yields state-of-the-art classification results on standard multi-domain object recognition datasets. version:1
arxiv-1506-02113 | Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks Using a Polynomial Number of Score Evaluations | http://arxiv.org/abs/1506.02113 | id:1506.02113 author:David Maxwell Chickering, Christopher Meek category:cs.LG cs.AI  published:2015-06-06 summary:We introduce Selective Greedy Equivalence Search (SGES), a restricted version of Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of GES but, unlike GES, has polynomial performance guarantees. In particular, we show that when data are sampled independently from a distribution that is perfect with respect to a DAG ${\cal G}$ defined over the observable variables then, in the limit of large data, SGES will identify ${\cal G}$'s equivalence class after a number of score evaluations that is (1) polynomial in the number of nodes and (2) exponential in various complexity measures including maximum-number-of-parents, maximum-clique-size, and a new measure called {\em v-width} that is at least as small as---and potentially much smaller than---the other two. More generally, we show that for any hereditary and equivalence-invariant property $\Pi$ known to hold in ${\cal G}$, we retain the large-sample optimality guarantees of GES even if we ignore any GES deletion operator during the backward phase that results in a state for which $\Pi$ does not hold in the common-descendants subgraph. version:1
arxiv-1502-05774 | Low-Cost Learning via Active Data Procurement | http://arxiv.org/abs/1502.05774 | id:1502.05774 author:Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner category:cs.GT cs.AI cs.LG stat.ML J.4; I.2.6  published:2015-02-20 summary:We design mechanisms for online procurement of data held by strategic agents for machine learning tasks. The challenge is to use past data to actively price future data and give learning guarantees even when an agent's cost for revealing her data may depend arbitrarily on the data itself. We achieve this goal by showing how to convert a large class of no-regret algorithms into online posted-price and learning mechanisms. Our results in a sense parallel classic sample complexity guarantees, but with the key resource being money rather than quantity of data: With a budget constraint $B$, we give robust risk (predictive error) bounds on the order of $1/\sqrt{B}$. Because we use an active approach, we can often guarantee to do significantly better by leveraging correlations between costs and data. Our algorithms and analysis go through a model of no-regret learning with $T$ arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds for this model are on the order of $T/\sqrt{B}$ and we give lower bounds on the same order. version:2
arxiv-1502-00702 | Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks | http://arxiv.org/abs/1502.00702 | id:1502.00702 author:Shiliang Zhang, Hui Jiang category:cs.LG cs.NE  published:2015-02-03 summary:In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-of-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning. version:2
arxiv-1506-01057 | A Hierarchical Neural Autoencoder for Paragraphs and Documents | http://arxiv.org/abs/1506.01057 | id:1506.01057 author:Jiwei Li, Minh-Thang Luong, Dan Jurafsky category:cs.CL  published:2015-06-02 summary:Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization\footnote{Code for the three models described in this paper can be found at www.stanford.edu/~jiweil/ . version:2
arxiv-1506-02087 | Global Gene Expression Analysis Using Machine Learning Methods | http://arxiv.org/abs/1506.02087 | id:1506.02087 author:Min Xu category:q-bio.QM cs.CE cs.LG stat.ML  published:2015-06-05 summary:Microarray is a technology to quantitatively monitor the expression of large number of genes in parallel. It has become one of the main tools for global gene expression analysis in molecular biology research in recent years. The large amount of expression data generated by this technology makes the study of certain complex biological problems possible and machine learning methods are playing a crucial role in the analysis process. At present, many machine learning methods have been or have the potential to be applied to major areas of gene expression analysis. These areas include clustering, classification, dynamic modeling and reverse engineering. In this thesis, we focus our work on using machine learning methods to solve the classification problems arising from microarray data. We first identify the major types of the classification problems; then apply several machine learning methods to solve the problems and perform systematic tests on real and artificial datasets. We propose improvement to existing methods. Specifically, we develop a multivariate and a hybrid feature selection method to obtain high classification performance for high dimension classification problems. Using the hybrid feature selection method, we are able to identify small sets of features that give predictive accuracy that is as good as that from other methods which require many more features. version:1
arxiv-1506-02085 | Gene selection for cancer classification using a hybrid of univariate and multivariate feature selection methods | http://arxiv.org/abs/1506.02085 | id:1506.02085 author:Min Xu, Rudy Setiono category:q-bio.QM cs.CE cs.LG stat.ML  published:2015-06-05 summary:Various approaches to gene selection for cancer classification based on microarray data can be found in the literature and they may be grouped into two categories: univariate methods and multivariate methods. Univariate methods look at each gene in the data in isolation from others. They measure the contribution of a particular gene to the classification without considering the presence of the other genes. In contrast, multivariate methods measure the relative contribution of a gene to the classification by taking the other genes in the data into consideration. Multivariate methods select fewer genes in general. However, the selection process of multivariate methods may be sensitive to the presence of irrelevant genes, noises in the expression and outliers in the training data. At the same time, the computational cost of multivariate methods is high. To overcome the disadvantages of the two types of approaches, we propose a hybrid method to obtain gene sets that are small and highly discriminative. We devise our hybrid method from the univariate Maximum Likelihood method (LIK) and the multivariate Recursive Feature Elimination method (RFE). We analyze the properties of these methods and systematically test the effectiveness of our proposed method on two cancer microarray datasets. Our experiments on a leukemia dataset and a small, round blue cell tumors dataset demonstrate the effectiveness of our hybrid method. It is able to discover sets consisting of fewer genes than those reported in the literature and at the same time achieve the same or better prediction accuracy. version:1
arxiv-1401-0689 | Machine Assisted Authentication of Paper Currency: an Experiment on Indian Banknotes | http://arxiv.org/abs/1401.0689 | id:1401.0689 author:Ankush Roy, Biswajit Halder, Utpal Garain, David S. Doermann category:cs.CV  published:2014-01-02 summary:Automatic authentication of paper money has been targeted. Indian bank notes are taken as reference to show how a system can be developed for discriminating fake notes from genuine ones. Image processing and pattern recognition techniques are used to design the overall approach. The ability of the embedded security aspects is thoroughly analysed for detecting fake currencies. Real forensic samples are involved in the experiment that shows a high precision machine can be developed for authentication of paper money. The system performance is reported in terms of both accuracy and processing speed. Comparison with human subjects namely forensic experts and bank staffs clearly shows its applicability for mass checking of currency notes in the real world. The analysis of security features to protect counterfeiting highlights some facts that should be taken care of in future designing of currency notes. version:5
arxiv-1506-02083 | Automatic tracking of protein vesicles | http://arxiv.org/abs/1506.02083 | id:1506.02083 author:Min Xu category:q-bio.QM cs.CV  published:2015-06-05 summary:With the advance of fluorescence imaging technologies, recently cell biologists are able to record the movement of protein vesicles within a living cell. Automatic tracking of the movements of these vesicles become key for qualitative analysis of dynamics of theses vesicles. In this thesis, we formulate such tracking problem as video object tracking problem, and design a dynamic programming method for tracking single object. Our experiments on simulation data show that the method can identify a track with high accuracy which is robust to the choose of tracking parameters and presence of high level noise. We then extend this method to the tracking multiple objects using the track elimination strategy. In multiple object tracking, the above approach often fails to correctly identify a track when two tracks cross. We solve this problem by incorporating the Kalman filter into the dynamic programming framework. Our experiments on simulated data show that the tracking accuracy is significantly improved. version:1
arxiv-1506-02080 | Local Nonstationarity for Efficient Bayesian Optimization | http://arxiv.org/abs/1506.02080 | id:1506.02080 author:Ruben Martinez-Cantin category:cs.LG stat.ML  published:2015-06-05 summary:Bayesian optimization has shown to be a fundamental global optimization algorithm in many applications: ranging from automatic machine learning, robotics, reinforcement learning, experimental design, simulations, etc. The most popular and effective Bayesian optimization relies on a surrogate model in the form of a Gaussian process due to its flexibility to represent a prior over function. However, many algorithms and setups relies on the stationarity assumption of the Gaussian process. In this paper, we present a novel nonstationary strategy for Bayesian optimization that is able to outperform the state of the art in Bayesian optimization both in stationary and nonstationary problems. version:1
arxiv-1506-02075 | Large-scale Simple Question Answering with Memory Networks | http://arxiv.org/abs/1506.02075 | id:1506.02075 author:Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston category:cs.LG cs.CL  published:2015-06-05 summary:Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance. version:1
arxiv-1504-02338 | Kernel Manifold Alignment | http://arxiv.org/abs/1504.02338 | id:1504.02338 author:Devis Tuia, Gustau Camps-Valls category:stat.ML cs.LG  published:2015-04-09 summary:We introduce a kernel method for manifold alignment (KEMA) and domain adaptation that can match an arbitrary number of data sources without needing corresponding pairs, just few labeled examples in all domains. KEMA has interesting properties: 1) it generalizes other manifold alignment methods, 2) it can align manifolds of very different complexities, performing a sort of manifold unfolding plus alignment, 3) it can define a domain-specific metric to cope with multimodal specificities, 4) it can align data spaces of different dimensionality, 5) it is robust to strong nonlinear feature deformations, and 6) it is closed-form invertible which allows transfer across-domains and data synthesis. We also present a reduced-rank version for computational efficiency and discuss the generalization performance of KEMA under Rademacher principles of stability. KEMA exhibits very good performance over competing methods in synthetic examples, visual object recognition and recognition of facial expressions tasks. version:3
arxiv-1503-01578 | Scalable Iterative Algorithm for Robust Subspace Clustering | http://arxiv.org/abs/1503.01578 | id:1503.01578 author:Sanghyuk Chun, Yung-Kyun Noh, Jinwoo Shin category:cs.DS cs.LG  published:2015-03-05 summary:Subspace clustering (SC) is a popular method for dimensionality reduction of high-dimensional data, where it generalizes Principal Component Analysis (PCA). Recently, several methods have been proposed to enhance the robustness of PCA and SC, while most of them are computationally very expensive, in particular, for high dimensional large-scale data. In this paper, we develop much faster iterative algorithms for SC, incorporating robustness using a {\em non-squared} $\ell_2$-norm objective. The known implementations for optimizing the objective would be costly due to the alternative optimization of two separate objectives: optimal cluster-membership assignment and robust subspace selection, while the substitution of one process to a faster surrogate can cause failure in convergence. To address the issue, we use a simplified procedure requiring efficient matrix-vector multiplications for subspace update instead of solving an expensive eigenvector problem at each iteration, in addition to release nested robust PCA loops. We prove that the proposed algorithm monotonically converges to a local minimum with approximation guarantees, e.g., it achieves 2-approximation for the robust PCA objective. In our experiments, the proposed algorithm is shown to converge at an order of magnitude faster than known algorithms optimizing the same objective, and have outperforms prior subspace clustering methods in accuracy and running time for MNIST dataset. version:2
arxiv-1506-01186 | No More Pesky Learning Rate Guessing Games | http://arxiv.org/abs/1506.01186 | id:1506.01186 author:Leslie N. Smith category:cs.CV cs.LG cs.NE  published:2015-06-03 summary:It is known that the learning rate is the most important hyper-parameter to tune for training deep convolutional neural networks (i.e., a "guessing game"). This report describes a new method for setting the learning rate, named cyclical learning rates, that eliminates the need to experimentally find the best values and schedule for the learning rates. Instead of setting the learning rate to fixed values, this method lets the learning rate cyclically vary within reasonable boundary values. This report shows that training with cyclical learning rates achieves near optimal classification accuracy without tuning and often in many fewer iterations. This report also describes a simple way to estimate "reasonable bounds" - by linearly increasing the learning rate in one training run of the network for only a few epochs. In addition, cyclical learning rates are demonstrated on training with the CIFAR-10 dataset and the AlexNet and GoogLeNet architectures on the ImageNet dataset. These methods are practical tools for everyone who trains convolutional neural networks. version:2
arxiv-1503-06450 | Multilingual Open Relation Extraction Using Cross-lingual Projection | http://arxiv.org/abs/1503.06450 | id:1503.06450 author:Manaal Faruqui, Shankar Kumar category:cs.CL  published:2015-03-22 summary:Open domain relation extraction systems identify relation and argument phrases in a sentence without relying on any underlying schema. However, current state-of-the-art relation extraction systems are available only for English because of their heavy reliance on linguistic tools such as part-of-speech taggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in 61 languages from Wikipedia. version:2
arxiv-1506-02004 | Sparse Overcomplete Word Vector Representations | http://arxiv.org/abs/1506.02004 | id:1506.02004 author:Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, Noah Smith category:cs.CL  published:2015-06-05 summary:Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks. version:1
arxiv-1503-00332 | JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes | http://arxiv.org/abs/1503.00332 | id:1503.00332 author:Jonathan H. Huggins, Karthik Narasimhan, Ardavan Saeedi, Vikash K. Mansinghka category:stat.ML cs.LG  published:2015-03-01 summary:Markov jump processes (MJPs) are used to model a wide range of phenomena from disease progression to RNA path folding. However, maximum likelihood estimation of parametric models leads to degenerate trajectories and inferential performance is poor in nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call \emph{JUMP-means}. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy. version:3
arxiv-1506-01597 | Abstractive Multi-Document Summarization via Phrase Selection and Merging | http://arxiv.org/abs/1506.01597 | id:1506.01597 author:Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca J. Passonneau category:cs.CL cs.AI  published:2015-06-04 summary:We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation. version:2
arxiv-1410-0870 | BayesPy: Variational Bayesian Inference in Python | http://arxiv.org/abs/1410.0870 | id:1410.0870 author:Jaakko Luttinen category:stat.ML stat.CO  published:2014-10-03 summary:BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference. version:3
arxiv-1502-03509 | MADE: Masked Autoencoder for Distribution Estimation | http://arxiv.org/abs/1502.03509 | id:1502.03509 author:Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle category:cs.LG cs.NE stat.ML  published:2015-02-12 summary:There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators. version:2
arxiv-1506-01914 | Content Translation: Computer-assisted translation tool for Wikipedia articles | http://arxiv.org/abs/1506.01914 | id:1506.01914 author:Niklas Laxström, Pau Giner, Santhosh Thottingal category:cs.CL  published:2015-06-05 summary:The quality and quantity of articles in each Wikipedia language varies greatly. Translating from another Wikipedia is a natural way to add more content, but the translation process is not properly supported in the software used by Wikipedia. Past computer-assisted translation tools built for Wikipedia are not commonly used. We created a tool that adapts to the specific needs of an open community and to the kind of content in Wikipedia. Qualitative and quantitative data indicates that the new tool helps users translate articles easier and faster. version:1
arxiv-1506-01906 | Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial Sentiment Analysis | http://arxiv.org/abs/1506.01906 | id:1506.01906 author:Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith category:cs.CL  published:2015-06-05 summary:Although, the fair amount of works in sentiment analysis (SA) and opinion mining (OM) systems in the last decade and with respect to the performance of these systems, but it still not desired performance, especially for morphologically-Rich Language (MRL) such as Arabic, due to the complexities and challenges exist in the nature of the languages itself. One of these challenges is the detection of idioms or proverbs phrases within the writer text or comment. An idiom or proverb is a form of speech or an expression that is peculiar to itself. Grammatically, it cannot be understood from the individual meanings of its elements and can yield different sentiment when treats as separate words. Consequently, In order to facilitate the task of detection and classification of lexical phrases for automated SA systems, this paper presents AIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic (MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence level with semantic orientation (positive or negative). The efforts of manually building and annotating the lexicon are reported. Moreover, we build a classifier that extracts idioms and proverbs, phrases from text using n-gram and similarity measure methods. Finally, several experiments were carried out on various data, including Arabic tweets and Arabic microblogs (hotel reservation, product reviews, and TV program comments) from publicly available Arabic online reviews websites (social media, blogs, forums, e-commerce web sites) to evaluate the coverage and accuracy of AIPSeLEX. version:1
arxiv-1506-01829 | Semidefinite and Spectral Relaxations for Multi-Label Classification | http://arxiv.org/abs/1506.01829 | id:1506.01829 author:Rémi Lajugie, Piotr Bojanowski, Sylvain Arlot, Francis Bach category:cs.LG  published:2015-06-05 summary:In this paper, we address the problem of multi-label classification. We consider linear classifiers and propose to learn a prior over the space of labels to directly leverage the performance of such methods. This prior takes the form of a quadratic function of the labels and permits to encode both attractive and repulsive relations between labels. We cast this problem as a structured prediction one aiming at optimizing either the accuracies of the predictors or the F 1-score. This leads to an optimization problem closely related to the max-cut problem, which naturally leads to semidefinite and spectral relaxations. We show on standard datasets how such a general prior can improve the performances of multi-label techniques. version:1
arxiv-1506-01782 | High-dimensional Ordinary Least-squares Projection for Screening Variables | http://arxiv.org/abs/1506.01782 | id:1506.01782 author:Xiangyu Wang, Chenlei Leng category:stat.ME math.ST stat.ML stat.TH  published:2015-06-05 summary:Variable selection is a challenging issue in statistical applications when the number of predictors $p$ far exceeds the number of observations $n$. In this ultra-high dimensional setting, the sure independence screening (SIS) procedure was introduced to significantly reduce the dimensionality by preserving the true model with overwhelming probability, before a refined second stage analysis. However, the aforementioned sure screening property strongly relies on the assumption that the important variables in the model have large marginal correlations with the response, which rarely holds in reality. To overcome this, we propose a novel and simple screening technique called the high-dimensional ordinary least-squares projection (HOLP). We show that HOLP possesses the sure screening property and gives consistent variable selection without the strong correlation assumption, and has a low computational complexity. A ridge type HOLP procedure is also discussed. Simulation study shows that HOLP performs competitively compared to many other marginal correlation based methods. An application to a mammalian eye disease data illustrates the attractiveness of HOLP. version:1
arxiv-1505-06289 | Text to 3D Scene Generation with Rich Lexical Grounding | http://arxiv.org/abs/1505.06289 | id:1505.06289 author:Angel Chang, Will Monroe, Manolis Savva, Christopher Potts, Christopher D. Manning category:cs.CL cs.GR  published:2015-05-23 summary:The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments. version:2
arxiv-1506-01744 | Spectral Learning of Large Structured HMMs for Comparative Epigenomics | http://arxiv.org/abs/1506.01744 | id:1506.01744 author:Chicheng Zhang, Jimin Song, Kevin C Chen, Kamalika Chaudhuri category:stat.ML cs.LG math.ST q-bio.GN stat.TH  published:2015-06-04 summary:We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure. The main challenge with learning parameters of such models is that iterative methods such as EM are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models. version:1
arxiv-1412-2309 | Visual Causal Feature Learning | http://arxiv.org/abs/1412.2309 | id:1412.2309 author:Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI cs.CV cs.LG  published:2014-12-07 summary:We provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans, animals, neurons, robots and other perceiving systems. Our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables. We prove the Causal Coarsening Theorem, which allows us to gain causal knowledge from observational data with minimal experimental effort. The theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with, but may not cause, the target behavior. Finally, we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior. We illustrate our inference and learning algorithms in experiments based on both synthetic and real data. version:2
arxiv-1506-01732 | Monocular SLAM Supported Object Recognition | http://arxiv.org/abs/1506.01732 | id:1506.01732 author:Sudeep Pillai, John Leonard category:cs.RO cs.CV  published:2015-06-04 summary:In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems. version:1
arxiv-1506-01709 | The Preference Learning Toolbox | http://arxiv.org/abs/1506.01709 | id:1506.01709 author:Vincent E. Farrugia, Héctor P. Martínez, Georgios N. Yannakakis category:stat.ML cs.IR cs.LG  published:2015-06-04 summary:Preference learning (PL) is a core area of machine learning that handles datasets with ordinal relations. As the number of generated data of ordinal nature is increasing, the importance and role of the PL field becomes central within machine learning research and practice. This paper introduces an open source, scalable, efficient and accessible preference learning toolbox that supports the key phases of the data training process incorporating various popular data preprocessing, feature selection and preference learning methods. version:1
arxiv-1506-01698 | The Long-Short Story of Movie Description | http://arxiv.org/abs/1506.01698 | id:1506.01698 author:Anna Rohrbach, Marcus Rohrbach, Bernt Schiele category:cs.CV cs.CL  published:2015-06-04 summary:Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII Movie Description allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task. version:1
arxiv-1402-0453 | Fine-Grained Visual Categorization via Multi-stage Metric Learning | http://arxiv.org/abs/1402.0453 | id:1402.0453 author:Qi Qian, Rong Jin, Shenghuo Zhu, Yuanqing Lin category:cs.CV cs.LG stat.ML  published:2014-02-03 summary:Fine-grained visual categorization (FGVC) is to categorize objects into subordinate classes instead of basic classes. One major challenge in FGVC is the co-occurrence of two issues: 1) many subordinate classes are highly correlated and are difficult to distinguish, and 2) there exists the large intra-class variation (e.g., due to object pose). This paper proposes to explicitly address the above two issues via distance metric learning (DML). DML addresses the first issue by learning an embedding so that data points from the same class will be pulled together while those from different classes should be pushed apart from each other; and it addresses the second issue by allowing the flexibility that only a portion of the neighbors (not all data points) from the same class need to be pulled together. However, feature representation of an image is often high dimensional, and DML is known to have difficulty in dealing with high dimensional feature vectors since it would require $\mathcal{O}(d^2)$ for storage and $\mathcal{O}(d^3)$ for optimization. To this end, we proposed a multi-stage metric learning framework that divides the large-scale high dimensional learning problem to a series of simple subproblems, achieving $\mathcal{O}(d)$ computational complexity. The empirical study with FVGC benchmark datasets verifies that our method is both effective and efficient compared to the state-of-the-art FGVC approaches. version:2
arxiv-1406-7130 | Density-Based Diffusion for Soft Clustering | http://arxiv.org/abs/1406.7130 | id:1406.7130 author:Thomas Bonis, Steve Oudot category:stat.ML  published:2014-06-27 summary:In this paper we advocate the use of diffusion processes guided by density to perform soft clustering tasks. Our approach interpolates between classical mode seeking and spectral clustering, being parametrized by a temperature parameter $\beta>0$ controlling the amount of random motion added to the gradient ascent. In practice we simulate the diffusion process in the continuous domain by random walks in neighborhood graphs built on the input data. We prove the convergence of this scheme under mild sampling conditions, and we derive guarantees for the clustering obtained in terms of the cluster membership distributions. Our theoretical results are cooroborated by preliminary experiments on manufactured data and on real data. version:2
arxiv-1503-03016 | Remarks on pointed digital homotopy | http://arxiv.org/abs/1503.03016 | id:1503.03016 author:Laurence Boxer, P. Christopher Staecker category:math.CO cs.CV math.GN 55P10  68R10 I.4.m  published:2015-03-10 summary:We present and explore in detail a pair of digital images with $c_u$-adjacencies that are homotopic but not pointed homotopic. For two digital loops $f,g: [0,m]_Z \rightarrow X$ with the same basepoint, we introduce the notion of {\em tight at the basepoint (TAB)} pointed homotopy, which is more restrictive than ordinary pointed homotopy and yields some different results. We present a variant form of the digital fundamental group. Based on what we call {\em eventually constant} loops, this version of the fundamental group is equivalent to that of Boxer (1999), but offers the advantage that eventually constant maps are often easier to work with than the trivial extensions that are key to the development of the fundamental group in Boxer (1999) and many subsequent papers. We show that homotopy equivalent digital images have isomorphic fundamental groups, even when the homotopy equivalence does not preserve the basepoint. This assertion appeared in Boxer (2005), but there was an error in the proof; here, we correct the error. version:2
arxiv-1506-01596 | Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images | http://arxiv.org/abs/1506.01596 | id:1506.01596 author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV  published:2015-06-04 summary:One of the challenges in hyperspectral data analysis is the presence of mixed pixels. Mixed pixels are the result of low spatial resolution of hyperspectral sensors. Spectral unmixing methods decompose a mixed pixel into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fraction values, NMF based methods are well suited to this problem. In this paper multilayer NMF has been used to improve the results of NMF methods for spectral unmixing of hyperspectral data under the linear mixing framework. Sparseness constraint on both spectral signatures and abundance fractions matrices are used in this paper. Evaluation of the proposed algorithm is done using synthetic and real datasets in terms of spectral angle and abundance angle distances. Results show that the proposed algorithm outperforms other previously proposed methods. version:1
arxiv-1506-01573 | Programs as Polypeptides | http://arxiv.org/abs/1506.01573 | id:1506.01573 author:Lance R. Williams category:cs.NE cs.ET cs.PL  published:2015-06-04 summary:We describe a visual programming language for defining behaviors manifested by reified actors in a 2D virtual world that can be compiled into programs comprised of sequences of combinators that are themselves reified as actors. This makes it possible to build programs that build programs from components of a few fixed types delivered by diffusion using processes that resemble chemistry as much as computation. version:1
arxiv-1506-01567 | Feature selection and classification of high-dimensional normal vectors with possibly large number of classes | http://arxiv.org/abs/1506.01567 | id:1506.01567 author:Felix Abramovich, Marianna Pensky category:math.ST stat.ME stat.ML stat.TH  published:2015-06-04 summary:We consider high-dimensional multi-class classification of normal vectors, where unlike standard assumptions, the number of classes may be also large. We derive the (non-asymptotic) conditions on effects of significant features, and the low and the upper bounds for distances between classes required for successful feature selection and classification with a given accuracy. In particular, we present an interesting and, at first glance, somewhat counter-intuitive phenomenon that the precision of classification can improve as a number of classes grows. This is due to more accurate feature selection since even weak significant features, which are not sufficiently strong to be manifested in a coarse classification, can nevertheless have a strong impact when the number of classes is large. The presented simulation study illustrates the performance of the procedure. version:1
arxiv-1504-02824 | A Deep Embedding Model for Co-occurrence Learning | http://arxiv.org/abs/1504.02824 | id:1504.02824 author:Yelong Shen, Ruoming Jin, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li Deng category:cs.LG  published:2015-04-11 summary:Co-occurrence Data is a common and important information source in many areas, such as the word co-occurrence in the sentences, friends co-occurrence in social networks and products co-occurrence in commercial transaction data, etc, which contains rich correlation and clustering information about the items. In this paper, we study co-occurrence data using a general energy-based probabilistic model, and we analyze three different categories of energy-based model, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capture different levels of dependency in the co-occurrence data. We also discuss how several typical existing models are related to these three types of energy models, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), Matrix Factorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the Restricted Boltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model (DEM) (an $L_k$ model) from the energy model in a \emph{principled} manner. Furthermore, motivated by the observation that the partition function in the energy model is intractable and the fact that the major objective of modeling the co-occurrence data is to predict using the conditional probability, we apply the \emph{maximum pseudo-likelihood} method to learn DEM. In consequence, the developed model and its learning method naturally avoid the above difficulties and can be easily used to compute the conditional probability in prediction. Interestingly, our method is equivalent to learning a special structured deep neural network using back-propagation and a special sampling strategy, which makes it scalable on large-scale datasets. Finally, in the experiments, we show that the DEM can achieve comparable or better results than state-of-the-art methods on datasets across several application domains. version:2
arxiv-1505-01861 | Jointly Modeling Embedding and Translation to Bridge Video and Language | http://arxiv.org/abs/1505.01861 | id:1505.01861 author:Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui category:cs.CV cs.MM  published:2015-05-07 summary:Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true. This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. Our proposed LSTM-E consists of three components: a 2-D and/or 3-D deep convolutional neural networks for learning powerful video representation, a deep RNN for generating sentences, and a joint embedding model for exploring the relationships between visual content and sentence semantics. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO) triplets to several state-of-the-art techniques. version:3
arxiv-1506-01710 | A Novel Approach Towards Clustering Based Image Segmentation | http://arxiv.org/abs/1506.01710 | id:1506.01710 author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV  published:2015-06-04 summary:In computer vision, image segmentation is always selected as a major research topic by researchers. Due to its vital rule in image processing, there always arises the need of a better image segmentation method. Clustering is an unsupervised study with its application in almost every field of science and engineering. Many researchers used clustering in image segmentation process. But still there requires improvement of such approaches. In this paper, a novel approach for clustering based image segmentation is proposed. Here, we give importance on color space and choose lab for this task. The famous hard clustering algorithm K-means is used, but as its performance is dependent on choosing a proper distance measure, so, we go for cosine distance measure. Then the segmented image is filtered with sobel filter. The filtered image is analyzed with marker watershed algorithm to have the final segmented result of our original image. The MSE and PSNR values are evaluated to observe the performance. version:1
arxiv-1506-01472 | Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to Color Image Segmentation | http://arxiv.org/abs/1506.01472 | id:1506.01472 author:Dibya Jyoti Bora, Anil Kumar Gupta, Fayaz Ahmad Khan category:cs.CV  published:2015-06-04 summary:Color image segmentation is a very emerging topic for image processing research. Since it has the ability to present the result in a way that is much more close to the human yes perceive, so todays more research is going on this area. Choosing a proper color space is a very important issue for color image segmentation process. Generally LAB and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces with respect to color image segmentation. For measuring their performance, we consider the parameters: mse and psnr . It is found that HSV color space is performing better than LAB. version:1
arxiv-1412-0165 | Robust Camera Location Estimation by Convex Programming | http://arxiv.org/abs/1412.0165 | id:1412.0165 author:Onur Ozyesil, Amit Singer category:cs.CV  published:2014-11-29 summary:$3$D structure recovery from a collection of $2$D images requires the estimation of the camera locations and orientations, i.e. the camera motion. For large, irregular collections of images, existing methods for the location estimation part, which can be formulated as the inverse problem of estimating $n$ locations $\mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_n$ in $\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions $\frac{\mathbf{t}_i - \mathbf{t}_j}{\ \mathbf{t}_i - \mathbf{t}_j\ }$, are sensitive to outliers in direction measurements. In this paper, we firstly provide a complete characterization of well-posed instances of the location estimation problem, by presenting its relation to the existing theory of parallel rigidity. For robust estimation of camera locations, we introduce a two-step approach, comprised of a pairwise direction estimation method robust to outliers in point correspondences between image pairs, and a convex program to maintain robustness to outlier directions. In the presence of partially corrupted measurements, we empirically demonstrate that our convex formulation can even recover the locations exactly. Lastly, we demonstrate the utility of our formulations through experiments on Internet photo collections. version:2
arxiv-1412-7504 | Higher-order Spatial Accuracy in Diffeomorphic Image Registration | http://arxiv.org/abs/1412.7504 | id:1412.7504 author:Henry O. Jacobs, Stefan Sommer category:cs.CV math.DG math.OC  published:2014-12-23 summary:We discretize a cost functional for image registration problems by deriving Taylor expansions for the matching term. Minima of the discretized cost functionals can be computed with no spatial discretization error, and the optimal solutions are equivalent to minimal energy curves in the space of $k$-jets. We show that the solutions convergence to optimal solutions of the original cost functional as the number of particles increases with a convergence rate of $O(h^{d+k})$ where $h$ is a resolution parameter. The effect of this approach over traditional particle methods is illustrated on synthetic examples and real images. version:2
arxiv-1506-01195 | Implementation of Training Convolutional Neural Networks | http://arxiv.org/abs/1506.01195 | id:1506.01195 author:Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang category:cs.CV cs.LG cs.NE  published:2015-06-03 summary:Deep learning refers to the shining branch of machine learning that is based on learning levels of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we gave a detailed analysis of the process of CNN algorithm both the forward process and back propagation. Then we applied the particular convolutional neural network to implement the typical face recognition problem by java. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically. version:2
arxiv-1506-01398 | Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM | http://arxiv.org/abs/1506.01398 | id:1506.01398 author:Jismy Alphonse, Biju V. G. category:cs.CV  published:2015-06-03 summary:A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR (Synthetic aperture Radar) image change detection method is proposed in this paper. It involves three steps: Difference Image (DI) generation by using Gauss-log ratio operator, speckle noise reduction by SRAD (Speckle Reducing Anisotropic Diffusion), and the detection of changed regions by using MRFFCM. The proposed method is compared with existing methods such as FCM and MRFFCM using simulated and real SAR images. The measures used for evaluation includes Overall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient (KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). The results show that the proposed method is better compared to FCM and MRFFCM based change detection method. version:1
arxiv-1506-01367 | A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture of k Gaussians, for any Constant k | http://arxiv.org/abs/1506.01367 | id:1506.01367 author:Jerry Li, Ludwig Schmidt category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH  published:2015-06-03 summary:Learning a Gaussian mixture model (GMM) is a fundamental problem in machine learning, learning theory, and statistics. One notion of learning a GMM is proper learning: here, the goal is to find a mixture of $k$ Gaussians $\mathcal{M}$ that is close to the density $f$ of the unknown distribution from which we draw samples. The distance between $\mathcal{M}$ and $f$ is typically measured in the total variation or $L_1$-norm. We give an algorithm for learning a mixture of $k$ univariate Gaussians that is nearly optimal for any fixed $k$. The sample complexity of our algorithm is $\tilde{O}(\frac{k}{\epsilon^2})$ and the running time is $(k \cdot \log\frac{1}{\epsilon})^{O(k^4)} + \tilde{O}(\frac{k}{\epsilon^2})$. It is well-known that this sample complexity is optimal (up to logarithmic factors), and it was already achieved by prior work. However, the best known time complexity for proper learning a $k$-GMM was $\tilde{O}(\frac{1}{\epsilon^{3k-1}})$. In particular, the dependence between $\frac{1}{\epsilon}$ and $k$ was exponential. We significantly improve this dependence by replacing the $\frac{1}{\epsilon}$ term with a $\log \frac{1}{\epsilon}$ while only increasing the exponent moderately. Hence, for any fixed $k$, the $\tilde{O} (\frac{k}{\epsilon^2})$ term dominates our running time, and thus our algorithm runs in time which is nearly-linear in the number of samples drawn. Achieving a running time of $\textrm{poly}(k, \frac{1}{\epsilon})$ for proper learning of $k$-GMMs has recently been stated as an open problem by multiple researchers, and we make progress on this question. Moreover, our approach offers an agnostic learning guarantee: our algorithm returns a good GMM even if the distribution we are sampling from is not a mixture of Gaussians. To the best of our knowledge, our algorithm is the first agnostic proper learning algorithm for GMMs. version:1
arxiv-1407-7502 | Understanding Random Forests: From Theory to Practice | http://arxiv.org/abs/1407.7502 | id:1407.7502 author:Gilles Louppe category:stat.ML  published:2014-07-28 summary:Data analysis and machine learning have become an integrative part of the modern scientific methodology, offering automated procedures for the prediction of a phenomenon based on past observations, unraveling underlying patterns in data and providing insights about the problem. Yet, caution should avoid using machine learning as a black-box tool, but rather consider it as a methodology, with a rational thought process that is entirely dependent on the problem under study. In particular, the use of algorithms should ideally require a reasonable understanding of their mechanisms, properties and limitations, in order to better apprehend and interpret their results. Accordingly, the goal of this thesis is to provide an in-depth analysis of random forests, consistently calling into question each and every part of the algorithm, in order to shed new light on its learning capabilities, inner workings and interpretability. The first part of this work studies the induction of decision trees and the construction of ensembles of randomized trees, motivating their design and purpose whenever possible. Our contributions follow with an original complexity analysis of random forests, showing their good computational performance and scalability, along with an in-depth discussion of their implementation details, as contributed within Scikit-Learn. In the second part of this work, we analyse and discuss the interpretability of random forests in the eyes of variable importance measures. The core of our contributions rests in the theoretical characterization of the Mean Decrease of Impurity variable importance measure, from which we prove and derive some of its properties in the case of multiway totally randomized trees and in asymptotic conditions. In consequence of this work, our analysis demonstrates that variable importances [...]. version:3
arxiv-1506-01351 | Celeste: Variational inference for a generative model of astronomical images | http://arxiv.org/abs/1506.01351 | id:1506.01351 author:Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, Prabhat category:astro-ph.IM stat.ML 62P35  85A35  68T01 G.3  published:2015-06-03 summary:We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors. version:1
arxiv-1506-01349 | Bayesian optimization for materials design | http://arxiv.org/abs/1506.01349 | id:1506.01349 author:Peter I. Frazier, Jialei Wang category:stat.ML math.OC  published:2015-06-03 summary:We introduce Bayesian optimization, a technique developed for optimizing time-consuming engineering simulations and for fitting machine learning models on large datasets. Bayesian optimization guides the choice of experiments during materials design and discovery to find good material designs in as few experiments as possible. We focus on the case when materials designs are parameterized by a low-dimensional vector. Bayesian optimization is built on a statistical technique called Gaussian process regression, which allows predicting the performance of a new design based on previously tested designs. After providing a detailed introduction to Gaussian process regression, we introduce two Bayesian optimization methods: expected improvement, for design problems with noise-free evaluations; and the knowledge-gradient method, which generalizes expected improvement and may be used in design problems with noisy evaluations. Both methods are derived using a value-of-information analysis, and enjoy one-step Bayes-optimality. version:1
arxiv-1506-01338 | Optimal change point detection in Gaussian processes | http://arxiv.org/abs/1506.01338 | id:1506.01338 author:Hossein Keshavarz, Clayton Scott, XuanLong Nguyen category:math.ST cs.IT cs.LG math.IT stat.ML stat.TH  published:2015-06-03 summary:We study the problem of detecting a change in the mean of one-dimensional Gaussian process data. This problem is investigated in the setting of increasing domain (customarily employed in time series analysis) and in the setting of fixed domain (typically arising in spatial data analysis). We propose a detection method based on generalized likelihood ratio test (GLRT), and show that our method achieves asymptotically optimal rate in the minimax sense, in both settings. The salient feature of the proposed method is that it exploits in an efficient way the data dependence captured by the Gaussian process covariance structure. When the covariance is not known, we propose plug-in GLRT method and derive conditions under which the method remains asymptotically optimal. By contrast, the standard CUSUM method, which does not account for the covariance structure, is shown to be asymptotically optimal only in the increasing domain. Our algorithms and accompanying theory are applicable to a wide variety of covariance structures, including the Matern class, the powered exponential class, and others. The plug-in GLRT method is shown to perform well for a number of covariance estimators, including maximum likelihood estimators with a dense or a tapered covariance matrix. version:1
arxiv-1506-01330 | Unsupervised Feature Analysis with Class Margin Optimization | http://arxiv.org/abs/1506.01330 | id:1506.01330 author:Sen Wang, Feiping Nie, Xiaojun Chang, Lina Yao, Xue Li, Quan Z. Sheng category:cs.LG  published:2015-06-03 summary:Unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades. In this paper, we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features. Specifically, our proposed algorithm integrates the Maximum Margin Criterion with a sparsity-based model into a joint framework, where the class margin and feature correlation are taken into account at the same time. To maximize the total data separability while preserving minimized within-class scatter simultaneously, we propose to embed Kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection. Meanwhile, a sparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix. In this way, noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros. To alleviate the local optimum problem that is caused by random initializations of K-means, a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix, is proposed to iteractively chase the optimal solution. Performance evaluation is extensively conducted over six benchmark data sets. From plenty of experimental results, it is demonstrated that our method has superior performance against all other compared approaches. version:1
arxiv-1506-01326 | Probabilistic Numerics and Uncertainty in Computations | http://arxiv.org/abs/1506.01326 | id:1506.01326 author:Philipp Hennig, Michael A Osborne, Mark Girolami category:math.NA cs.AI cs.LG stat.CO stat.ML  published:2015-06-03 summary:We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data has led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimisers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations. version:1
arxiv-1505-00322 | Using PCA to Efficiently Represent State Spaces | http://arxiv.org/abs/1505.00322 | id:1505.00322 author:William Curran, Tim Brys, Matthew Taylor, William Smart category:cs.LG cs.RO  published:2015-05-02 summary:Reinforcement learning algorithms need to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces. This is known as the curse of dimensionality. By projecting the agent's state onto a low-dimensional manifold, we can represent the state space in a smaller and more efficient representation. By using this representation during learning, the agent can converge to a good policy much faster. We test this approach in the Mario Benchmarking Domain. When using dimensionality reduction in Mario, learning converges much faster to a good policy. But, there is a critical convergence-performance trade-off. By projecting onto a low-dimensional manifold, we are ignoring important data. In this paper, we explore this trade-off of convergence and performance. We find that learning in as few as 4 dimensions (instead of 9), we can improve performance past learning in the full dimensional space at a faster convergence rate. version:2
arxiv-1504-00976 | Convex Denoising using Non-Convex Tight Frame Regularization | http://arxiv.org/abs/1504.00976 | id:1504.00976 author:Ankit Parekh, Ivan W. Selesnick category:cs.CV math.OC  published:2015-04-04 summary:This paper considers the problem of signal denoising using a sparse tight-frame analysis prior. The L1 norm has been extensively used as a regularizer to promote sparsity; however, it tends to under-estimate non-zero values of the underlying signal. To more accurately estimate non-zero values, we propose the use of a non-convex regularizer, chosen so as to ensure convexity of the objective function. The convexity of the objective function is ensured by constraining the parameter of the non-convex penalty. We use ADMM to obtain a solution and show how to guarantee that ADMM converges to the global optimum of the objective function. We illustrate the proposed method for 1D and 2D signal denoising. version:2
arxiv-1506-01286 | PeakSegJoint: fast supervised peak detection via joint segmentation of multiple count data samples | http://arxiv.org/abs/1506.01286 | id:1506.01286 author:Toby Dylan Hocking, Guillaume Bourque category:stat.ML q-bio.GN  published:2015-06-03 summary:Joint peak detection is a central problem when comparing samples in genomic data analysis, but current algorithms for this task are unsupervised and limited to at most 2 sample types. We propose PeakSegJoint, a new constrained maximum likelihood segmentation model for any number of sample types. To select the number of peaks in the segmentation, we propose a supervised penalty learning model. To infer the parameters of these two models, we propose to use a discrete optimization heuristic for the segmentation, and convex optimization for the penalty learning. In comparisons with state-of-the-art peak detection algorithms, PeakSegJoint achieves similar accuracy, faster speeds, and a more interpretable model with overlapping peaks that occur in exactly the same positions across all samples. version:1
arxiv-1412-5896 | On the Stability of Deep Networks | http://arxiv.org/abs/1412.5896 | id:1412.5896 author:Raja Giryes, Guillermo Sapiro, Alex M. Bronstein category:stat.ML cs.IT cs.LG cs.NE math.IT math.MG  published:2014-12-18 summary:In this work we study the properties of deep neural networks (DNN) with random weights. We formally prove that these networks perform a distance-preserving embedding of the data. Based on this we then draw conclusions on the size of the training data and the networks' structure. A longer version of this paper with more results and details can be found in (Giryes et al., 2015). In particular, we formally prove in the longer version that DNN with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. version:3
arxiv-1411-0326 | High Dynamic Range Imaging by Perceptual Logarithmic Exposure Merging | http://arxiv.org/abs/1411.0326 | id:1411.0326 author:Corneliu Florea, Constantin Vertan, Laura Florea category:cs.CV  published:2014-11-02 summary:In this paper we emphasize a similarity between the Logarithmic-Type Image Processing (LTIP) model and the Naka-Rushton model of the Human Visual System (HVS). LTIP is a derivation of the Logarithmic Image Processing (LIP), which further replaces the logarithmic function with a ratio of polynomial functions. Based on this similarity, we show that it is possible to present an unifying framework for the High Dynamic Range (HDR) imaging problem, namely that performing exposure merging under the LTIP model is equivalent to standard irradiance map fusion. The resulting HDR algorithm is shown to provide high quality in both subjective and objective evaluations. version:2
arxiv-1506-01192 | Personalizing a Universal Recurrent Neural Network Language Model with User Characteristic Features by Crowdsouring over Social Networks | http://arxiv.org/abs/1506.01192 | id:1506.01192 author:Bo-Hsiang Tseng, Hung-Yi Lee, Lin-Shan Lee category:cs.CL cs.LG  published:2015-06-03 summary:With the popularity of mobile devices, personalized speech recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it's possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models. version:1
arxiv-1502-00756 | Design of a Mobile Face Recognition System for Visually Impaired Persons | http://arxiv.org/abs/1502.00756 | id:1502.00756 author:Shonal Chaudhry, Rohitash Chandra category:cs.CY cs.CV cs.HC  published:2015-02-03 summary:It is estimated that 285 million people globally are visually impaired. A majority of these people live in developing countries and are among the elderly population. One of the most difficult tasks faced by the visually impaired is identification of people. While naturally, voice recognition is a common method of identification, it is an intuitive and difficult process. The rise of computation capability of mobile devices gives motivation to develop applications that can assist visually impaired persons. With the availability of mobile devices, these people can be assisted by an additional method of identification through intelligent software based on computer vision techniques. In this paper, we present the design and implementation of a face detection and recognition system for the visually impaired through the use of mobile computing. This mobile system is assisted by a server-based support system. The system was tested on a custom video database. Experiment results show high face detection accuracy and promising face recognition accuracy in suitable conditions. The challenges of the system lie in better recognition techniques for difficult situations in terms of lighting and weather. version:2
arxiv-1406-4112 | Semantic Graph for Zero-Shot Learning | http://arxiv.org/abs/1406.4112 | id:1406.4112 author:Zhen-Yong Fu, Tao Xiang, Shaogang Gong category:cs.CV cs.LG  published:2014-06-16 summary:Zero-shot learning aims to classify visual objects without any training data via knowledge transfer between seen and unseen classes. This is typically achieved by exploring a semantic embedding space where the seen and unseen classes can be related. Previous works differ in what embedding space is used and how different classes and a test image can be related. In this paper, we utilize the annotation-free semantic word space for the former and focus on solving the latter issue of modeling relatedness. Specifically, in contrast to previous work which ignores the semantic relationships between seen classes and focus merely on those between seen and unseen classes, in this paper a novel approach based on a semantic graph is proposed to represent the relationships between all the seen and unseen class in a semantic word space. Based on this semantic graph, we design a special absorbing Markov chain process, in which each unseen class is viewed as an absorbing state. After incorporating one test image into the semantic graph, the absorbing probabilities from the test data to each unseen class can be effectively computed; and zero-shot classification can be achieved by finding the class label with the highest absorbing probability. The proposed model has a closed-form solution which is linear with respect to the number of test images. We demonstrate the effectiveness and computational efficiency of the proposed method over the state-of-the-arts on the AwA (animals with attributes) dataset. version:2
arxiv-1506-01171 | A Hybrid Model for Enhancing Lexical Statistical Machine Translation (SMT) | http://arxiv.org/abs/1506.01171 | id:1506.01171 author:Ahmed G. M. ElSayed, Ahmed S. Salama, Alaa El-Din M. El-Ghazali category:cs.CL  published:2015-06-03 summary:The interest in statistical machine translation systems increases currently due to political and social events in the world. A proposed Statistical Machine Translation (SMT) based model that can be used to translate a sentence from the source Language (English) to the target language (Arabic) automatically through efficiently incorporating different statistical and Natural Language Processing (NLP) models such as language model, alignment model, phrase based model, reordering model, and translation model. These models are combined to enhance the performance of statistical machine translation (SMT). Many implementation tools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and BLEU. Based on the implementation, evaluation of this model, and comparing the generated translation with other implemented machine translation systems like Google Translate, it was proved that this proposed model has enhanced the results of the statistical machine translation, and forms a reliable and efficient model in this field of research. version:1
arxiv-1506-01166 | Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree | http://arxiv.org/abs/1506.01166 | id:1506.01166 author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3  published:2015-06-03 summary:This chapter approaches the image retrieval system on the base of the colors of image. It creates fuzzy signature to describe the color of image on color space HSV and builds fuzzy Hamming distance (FHD) to evaluate the similarity between the images. In order to reduce the storage space and speed up the search of similar images, it aims to create S-tree to store fuzzy signature relies on FHD and builds image retrieval algorithm on S-tree. Then, it provides the content-based image retrieval (CBIR) and an image retrieval method on FHD and S-tree. Last but not least, based on this theory, it also presents an application and experimental assessment of the process of querying similar image on the database system over 10,000 images. version:1
arxiv-1506-01165 | Image Retrieval System Base on EMD Similarity Measure and S-Tree | http://arxiv.org/abs/1506.01165 | id:1506.01165 author:Thanh Manh Le, Thanh The Van category:cs.CV cs.IR H.2.8; H.3.3  published:2015-06-03 summary:The paper approaches the binary signature for each image based on the percentage of the pixels in each color images, at the same time the paper builds a similar measure between images based on EMD (Earth Mover's Distance). Besides, the paper proceeded to create the S-tree based on the similar measure EMD to store the image's binary signatures to quickly query image signature data. From there, the paper build an image retrieval algorithm and CBIR (Content-Based Image Retrieval) based on a similar measure EMD and S-tree. Based on this theory, the paper proceeded to build application and experimental assessment of the process of querying image on the database system which have over 10,000 images. version:1
arxiv-1506-01163 | Towards Structured Deep Neural Network for Automatic Speech Recognition | http://arxiv.org/abs/1506.01163 | id:1506.01163 author:Yi-Hsiu Liao, Hung-Yi Lee, Lin-shan Lee category:cs.LG  published:2015-06-03 summary:In this paper we propose the Structured Deep Neural Network (Structured DNN) as a structured and deep learning algorithm, learning to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structure rather than item by item. When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learned utterance by utterance as a whole, rather than frame by frame. Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning algorithm. It was shown to beat structured SVM in preliminary experiments on TIMIT. version:1
arxiv-1306-3690 | Do semidefinite relaxations solve sparse PCA up to the information limit? | http://arxiv.org/abs/1306.3690 | id:1306.3690 author:Robert Krauthgamer, Boaz Nadler, Dan Vilenchik category:math.ST stat.ML stat.TH  published:2013-06-16 summary:Estimating the leading principal components of data, assuming they are sparse, is a central task in modern high-dimensional statistics. Many algorithms were developed for this sparse PCA problem, from simple diagonal thresholding to sophisticated semidefinite programming (SDP) methods. A key theoretical question is under what conditions can such algorithms recover the sparse principal components? We study this question for a single-spike model with an $\ell_0$-sparse eigenvector, in the asymptotic regime as dimension $p$ and sample size $n$ both tend to infinity. Amini and Wainwright [Ann. Statist. 37 (2009) 2877-2921] proved that for sparsity levels $k\geq\Omega(n/\log p)$, no algorithm, efficient or not, can reliably recover the sparse eigenvector. In contrast, for $k\leq O(\sqrt{n/\log p})$, diagonal thresholding is consistent. It was further conjectured that an SDP approach may close this gap between computational and information limits. We prove that when $k\geq\Omega(\sqrt{n})$, the proposed SDP approach, at least in its standard usage, cannot recover the sparse spike. In fact, we conjecture that in the single-spike model, no computationally-efficient algorithm can recover a spike of $\ell_0$-sparsity $k\geq\Omega(\sqrt{n})$. Finally, we present empirical results suggesting that up to sparsity levels $k=O(\sqrt{n})$, recovery is possible by a simple covariance thresholding algorithm. version:4
arxiv-1501-01029 | Innovated interaction screening for high-dimensional nonlinear classification | http://arxiv.org/abs/1501.01029 | id:1501.01029 author:Yingying Fan, Yinfei Kong, Daoji Li, Zemin Zheng category:stat.ML  published:2015-01-05 summary:This paper is concerned with the problems of interaction screening and nonlinear classification in a high-dimensional setting. We propose a two-step procedure, IIS-SQDA, where in the first step an innovated interaction screening (IIS) approach based on transforming the original $p$-dimensional feature vector is proposed, and in the second step a sparse quadratic discriminant analysis (SQDA) is proposed for further selecting important interactions and main effects and simultaneously conducting classification. Our IIS approach screens important interactions by examining only $p$ features instead of all two-way interactions of order $O(p^2)$. Our theory shows that the proposed method enjoys sure screening property in interaction selection in the high-dimensional setting of $p$ growing exponentially with the sample size. In the selection and classification step, we establish a sparse inequality on the estimated coefficient vector for QDA and prove that the classification error of our procedure can be upper-bounded by the oracle classification error plus some smaller order term. Extensive simulation studies and real data analysis show that our proposal compares favorably with existing methods in interaction selection and high-dimensional classification. version:2
arxiv-1506-01151 | Understanding deep features with computer-generated imagery | http://arxiv.org/abs/1506.01151 | id:1506.01151 author:Mathieu Aubry, Bryan Russell category:cs.CV  published:2015-06-03 summary:We introduce an approach for analyzing the variation of features generated by convolutional neural networks (CNNs) with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses corresponding to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors. We perform a decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three CNNs trained on large image datasets: AlexNet, Places, and Oxford VGG. We observe important differences across the networks and CNN layers for different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images. version:1
arxiv-1404-6000 | Robust and computationally feasible community detection in the presence of arbitrary outlier nodes | http://arxiv.org/abs/1404.6000 | id:1404.6000 author:T. Tony Cai, Xiaodong Li category:math.ST cs.IT math.IT math.OC stat.ML stat.TH  published:2014-04-23 summary:Community detection, which aims to cluster $N$ nodes in a given graph into $r$ distinct groups based on the observed undirected edges, is an important problem in network data analysis. In this paper, the popular stochastic block model (SBM) is extended to the generalized stochastic block model (GSBM) that allows for adversarial outlier nodes, which are connected with the other nodes in the graph in an arbitrary way. Under this model, we introduce a procedure using convex optimization followed by $k$-means algorithm with $k=r$. Both theoretical and numerical properties of the method are analyzed. A theoretical guarantee is given for the procedure to accurately detect the communities with small misclassification rate under the setting where the number of clusters can grow with $N$. This theoretical result admits to the best-known result in the literature of computationally feasible community detection in SBM without outliers. Numerical results show that our method is both computationally fast and robust to different kinds of outliers, while some popular computationally fast community detection algorithms, such as spectral clustering applied to adjacency matrices or graph Laplacians, may fail to retrieve the major clusters due to a small portion of outliers. We apply a slight modification of our method to a political blogs data set, showing that our method is competent in practice and comparable to existing computationally feasible methods in the literature. To the best of the authors' knowledge, our result is the first in the literature in terms of clustering communities with fast growing numbers under the GSBM where a portion of arbitrary outlier nodes exist. version:4
arxiv-1506-01125 | Unsupervised domain adaption dictionary learning for visual recognition | http://arxiv.org/abs/1506.01125 | id:1506.01125 author:Zhun Zhong, Zongmin Li, Runlin Li, Xiaoxia Sun category:cs.CV  published:2015-06-03 summary:Over the last years, dictionary learning method has been extensively applied to deal with various computer vision recognition applications, and produced state-of-the-art results. However, when the data instances of a target domain have a different distribution than that of a source domain, the dictionary learning method may fail to perform well. In this paper, we address the cross-domain visual recognition problem and propose a simple but effective unsupervised domain adaption approach, where labeled data are only from source domain. In order to bring the original data in source and target domain into the same distribution, the proposed method forcing nearest coupled data between source and target domain to have identical sparse representations while jointly learning dictionaries for each domain, where the learned dictionaries can reconstruct original data in source and target domain respectively. So that sparse representations of original data can be used to perform visual recognition tasks. We demonstrate the effectiveness of our approach on standard datasets. Our method performs on par or better than competitive state-of-the-art methods. version:1
arxiv-1309-6024 | Asymptotic normality and optimalities in estimation of large Gaussian graphical models | http://arxiv.org/abs/1309.6024 | id:1309.6024 author:Zhao Ren, Tingni Sun, Cun-Hui Zhang, Harrison H. Zhou category:math.ST stat.ME stat.ML stat.TH  published:2013-09-24 summary:The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement. The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix $\ell_q$ operator norm and to make inference in latent variables in the graphical model. All of this is achieved under a sparsity condition on the precision matrix and a side condition on the range of its spectrum. This significantly relaxes the commonly imposed uniform signal strength condition on the precision matrix, irrepresentability condition on the Hessian tensor operator of the covariance matrix or the $\ell_1$ constraint on the precision matrix. Numerical results confirm our theoretical findings. The ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT), for support recovery significantly outperforms that of the popular GLasso algorithm. version:3
arxiv-1411-6231 | Compound Rank-k Projections for Bilinear Analysis | http://arxiv.org/abs/1411.6231 | id:1411.6231 author:Xiaojun Chang, Feiping Nie, Sen Wang, Yi Yang, Xiaofang Zhou, Chengqi Zhang category:cs.LG  published:2014-11-23 summary:In many real-world applications, data are represented by matrices or high-order tensors. Despite the promising performance, the existing two-dimensional discriminant analysis algorithms employ a single projection model to exploit the discriminant information for projection, making the model less flexible. In this paper, we propose a novel Compound Rank-k Projection (CRP) algorithm for bilinear analysis. CRP deals with matrices directly without transforming them into vectors, and it therefore preserves the correlations within the matrix and decreases the computation complexity. Different from the existing two dimensional discriminant analysis algorithms, objective function values of CRP increase monotonically.In addition, CRP utilizes multiple rank-k projection models to enable a larger search space in which the optimal solution can be found. In this way, the discriminant ability is enhanced. version:3
arxiv-1506-01115 | Hyperspectral Image Classification and Clutter Detection via Multiple Structural Embeddings and Dimension Reductions | http://arxiv.org/abs/1506.01115 | id:1506.01115 author:Alexandros-Stavros Iliopoulos, Tiancheng Liu, Xiaobai Sun category:cs.CV I.4.6; I.4.8; I.4.9  published:2015-06-03 summary:We present a new and effective approach for Hyperspectral Image (HSI) classification and clutter detection, overcoming a few long-standing challenges presented by HSI data characteristics. Residing in a high-dimensional spectral attribute space, HSI data samples are known to be strongly correlated in their spectral signatures, exhibit nonlinear structure due to several physical laws, and contain uncertainty and noise from multiple sources. In the presented approach, we generate an adaptive, structurally enriched representation environment, and employ the locally linear embedding (LLE) in it. There are two structure layers external to LLE. One is feature space embedding: the HSI data attributes are embedded into a discriminatory feature space where spatio-spectral coherence and distinctive structures are distilled and exploited to mitigate various difficulties encountered in the native hyperspectral attribute space. The other structure layer encloses the ranges of algorithmic parameters for LLE and feature embedding, and supports a multiplexing and integrating scheme for contending with multi-source uncertainty. Experiments on two commonly used HSI datasets with a small number of learning samples have rendered remarkably high-accuracy classification results, as well as distinctive maps of detected clutter regions. version:1
arxiv-1504-05823 | Normal Bandits of Unknown Means and Variances: Asymptotic Optimality, Finite Horizon Regret Bounds, and a Solution to an Open Problem | http://arxiv.org/abs/1504.05823 | id:1504.05823 author:Wesley Cowan, Junya Honda, Michael N. Katehakis category:stat.ML cs.LG  published:2015-04-22 summary:Consider the problem of sampling sequentially from a finite number of $N \geq 2$ populations, specified by random variables $X^i_k$, $ i = 1,\ldots , N,$ and $k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from population $i$ the $k^{th}$ time it is sampled. It is assumed that for each fixed $i$, $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. normal random variables, with unknown mean $\mu_i$ and unknown variance $\sigma_i^2$. The objective is to have a policy $\pi$ for deciding from which of the $N$ populations to sample form at any time $n=1,2,\ldots$ so as to maximize the expected sum of outcomes of $n$ samples or equivalently to minimize the regret due to lack on information of the parameters $\mu_i$ and $\sigma_i^2$. In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from Burnetas and Katehakis (1996). Additionally, finite horizon regret bounds are given. version:2
arxiv-1506-01110 | Multi-view Machines | http://arxiv.org/abs/1506.01110 | id:1506.01110 author:Bokai Cao, Hucheng Zhou, Philip S. Yu category:cs.LG stat.ML H.2.8  published:2015-06-03 summary:For a learning task, data can usually be collected from different sources or be represented from multiple views. For example, laboratory results from different medical examinations are available for disease diagnosis, and each of them can only reflect the health state of a person from a particular aspect/view. Therefore, different views provide complementary information for learning tasks. An effective integration of the multi-view information is expected to facilitate the learning performance. In this paper, we propose a general predictor, named multi-view machines (MVMs), that can effectively include all the possible interactions between features from multiple views. A joint factorization is embedded for the full-order interaction parameters which allows parameter estimation under sparsity. Moreover, MVMs can work in conjunction with different loss functions for a variety of machine learning tasks. A stochastic gradient descent method is presented to learn the MVM model. We further illustrate the advantages of MVMs through comparison with other methods for multi-view classification, including support vector machines (SVMs), support tensor machines (STMs) and factorization machines (FMs). version:1
arxiv-1411-5726 | CIDEr: Consensus-based Image Description Evaluation | http://arxiv.org/abs/1411.5726 | id:1411.5726 author:Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh category:cs.CV cs.CL cs.IR  published:2014-11-20 summary:Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking. version:2
arxiv-1501-07430 | Bayesian Hierarchical Clustering with Exponential Family: Small-Variance Asymptotics and Reducibility | http://arxiv.org/abs/1501.07430 | id:1501.07430 author:Juho Lee, Seungjin Choi category:stat.ML cs.LG  published:2015-01-29 summary:Bayesian hierarchical clustering (BHC) is an agglomerative clustering method, where a probabilistic model is defined and its marginal likelihoods are evaluated to decide which clusters to merge. While BHC provides a few advantages over traditional distance-based agglomerative clustering algorithms, successive evaluation of marginal likelihoods and careful hyperparameter tuning are cumbersome and limit the scalability. In this paper we relax BHC into a non-probabilistic formulation, exploring small-variance asymptotics in conjugate-exponential models. We develop a novel clustering algorithm, referred to as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that exhibits the scalability of distance-based agglomerative clustering algorithms as well as the flexibility of Bayesian nonparametric models. We also investigate the reducibility of the dissimilarity measure emerged from the asymptotic limit of the BHC model, allowing us to use scalable algorithms such as the nearest neighbor chain algorithm. Numerical experiments on both synthetic and real-world datasets demonstrate the validity and high performance of our method. version:2
arxiv-1506-01092 | Bilinear Random Projections for Locality-Sensitive Binary Codes | http://arxiv.org/abs/1506.01092 | id:1506.01092 author:Saehoon Kim, Seungjin Choi category:cs.CV  published:2015-06-03 summary:Locality-sensitive hashing (LSH) is a popular data-independent indexing method for approximate similarity search, where random projections followed by quantization hash the points from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. Most of high-dimensional visual descriptors for images exhibit a natural matrix structure. When visual descriptors are represented by high-dimensional feature vectors and long binary codes are assigned, a random projection matrix requires expensive complexities in both space and time. In this paper we analyze a bilinear random projection method where feature matrices are transformed to binary codes by two smaller random projection matrices. We base our theoretical analysis on extending Raginsky and Lazebnik's result where random Fourier features are composed with random binary quantizers to form locality sensitive binary codes. To this end, we answer the following two questions: (1) whether a bilinear random projection also yields similarity-preserving binary codes; (2) whether a bilinear random projection yields performance gain or loss, compared to a large linear projection. Regarding the first question, we present upper and lower bounds on the expected Hamming distance between binary codes produced by bilinear random projections. In regards to the second question, we analyze the upper and lower bounds on covariance between two bits of binary codes, showing that the correlation between two bits is small. Numerical experiments on MNIST and Flickr45K datasets confirm the validity of our method. version:1
arxiv-1409-4141 | Probabilistic Network Metrics: Variational Bayesian Network Centrality | http://arxiv.org/abs/1409.4141 | id:1409.4141 author:Harold Soh category:stat.ML  published:2014-09-15 summary:Network metrics form a fundamental part of the network analysis toolbox. Used to quantitatively measure different aspects of the network, these metrics can give insights into the underlying network structure and function. In this work, we connect network metrics to modern probabilistic machine learning. We focus on the centrality metric, which is used a wide variety of applications from web search to gene-analysis. First, we formulate an eigenvector-based Bayesian centrality model for determining node importance. Compared to existing methods, our probabilistic model allows for the assimilation of multiple edge weight observations, the inclusion of priors and the extraction of uncertainties. To enable tractable inference, we develop a variational lower bound (VBC) that is demonstrated to be effective on a variety of networks (two synthetic and five real-world graphs). We then bridge this model to sparse Gaussian processes. The sparse variational Bayesian centrality Gaussian process (VBC-GP) learns a mapping between node attributes to latent centrality and hence, is capable of predicting centralities from node features and can potentially represent a large number of nodes using only a limited number of inducing inputs. Experiments show that the VBC-GP learns high-quality mappings and compares favorably to a two-step baseline, i.e., a full GP trained on the node attributes and pre-computed centralities. Finally, we present two case-studies using the VBC-GP: first, to ascertain relevant features in a taxi transport network and second, to distribute a limited number of vaccines to mitigate the severity of a viral outbreak. version:2
arxiv-1506-01077 | On bicluster aggregation and its benefits for enumerative solutions | http://arxiv.org/abs/1506.01077 | id:1506.01077 author:Saullo Haniell Galvão de Oliveira, Rosana Veroneze, Fernando José Von Zuben category:cs.LG  published:2015-06-02 summary:Biclustering involves the simultaneous clustering of objects and their attributes, thus defining local two-way clustering models. Recently, efficient algorithms were conceived to enumerate all biclusters in real-valued datasets. In this case, the solution composes a complete set of maximal and non-redundant biclusters. However, the ability to enumerate biclusters revealed a challenging scenario: in noisy datasets, each true bicluster may become highly fragmented and with a high degree of overlapping. It prevents a direct analysis of the obtained results. To revert the fragmentation, we propose here two approaches for properly aggregating the whole set of enumerated biclusters: one based on single linkage and the other directly exploring the rate of overlapping. Both proposals were compared with each other and with the actual state-of-the-art in several experiments, and they not only significantly reduced the number of biclusters but also consistently increased the quality of the solution. version:1
arxiv-1406-4465 | Multi-stage Multi-task feature learning via adaptive threshold | http://arxiv.org/abs/1406.4465 | id:1406.4465 author:Yaru Fan, Yilun Wang category:cs.LG cs.CV stat.ML 68T10 F.2.2  published:2014-06-16 summary:Multi-task feature learning aims to identity the shared features among tasks to improve generalization. It has been shown that by minimizing non-convex learning models, a better solution than the convex alternatives can be obtained. Therefore, a non-convex model based on the capped-$\ell_{1},\ell_{1}$ regularization was proposed in \cite{Gong2013}, and a corresponding efficient multi-stage multi-task feature learning algorithm (MSMTFL) was presented. However, this algorithm harnesses a prescribed fixed threshold in the definition of the capped-$\ell_{1},\ell_{1}$ regularization and the lack of adaptivity might result in suboptimal performance. In this paper we propose to employ an adaptive threshold in the capped-$\ell_{1},\ell_{1}$ regularized formulation, where the corresponding variant of MSMTFL will incorporate an additional component to adaptively determine the threshold value. This variant is expected to achieve a better feature selection performance over the original MSMTFL algorithm. In particular, the embedded adaptive threshold component comes from our previously proposed iterative support detection (ISD) method \cite{Wang2010}. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of this new variant over the original MSMTFL. version:2
arxiv-1506-00999 | Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases | http://arxiv.org/abs/1506.00999 | id:1506.00999 author:Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier, Yves Grandvalet category:cs.AI cs.CL cs.LG  published:2015-06-02 summary:This paper tackles the problem of endogenous link prediction for Knowledge Base completion. Knowledge Bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature. version:1
arxiv-1412-2669 | Two step recovery of jointly sparse and low-rank matrices: theoretical guarantees | http://arxiv.org/abs/1412.2669 | id:1412.2669 author:Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob category:stat.ML cs.IT math.IT  published:2014-12-05 summary:We introduce a two step algorithm with theoretical guarantees to recover a jointly sparse and low-rank matrix from undersampled measurements of its columns. The algorithm first estimates the row subspace of the matrix using a set of common measurements of the columns. In the second step, the subspace aware recovery of the matrix is solved using a simple least square algorithm. The results are verified in the context of recovering CINE data from undersampled measurements; we obtain good recovery when the sampling conditions are satisfied. version:2
arxiv-1412-2700 | Subspace based low rank and joint sparse matrix recovery | http://arxiv.org/abs/1412.2700 | id:1412.2700 author:Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob category:cs.NA cs.CV  published:2014-12-05 summary:We consider the recovery of a low rank and jointly sparse matrix from under sampled measurements of its columns. This problem is highly relevant in the recovery of dynamic MRI data with high spatio-temporal resolution, where each column of the matrix corresponds to a frame in the image time series; the matrix is highly low-rank since the frames are highly correlated. Similarly the non-zero locations of the matrix in appropriate transform/frame domains (e.g. wavelet, gradient) are roughly the same in different frame. The superset of the support can be safely assumed to be jointly sparse. Unlike the classical multiple measurement vector (MMV) setup that measures all the snapshots using the same matrix, we consider each snapshot to be measured using a different measurement matrix. We show that this approach reduces the total number of measurements, especially when the rank of the matrix is much smaller than than its sparsity. Our experiments in the context of dynamic imaging shows that this approach is very useful in realizing free breathing cardiac MRI. version:2
arxiv-1506-00935 | Discovering Valuable Items from Massive Data | http://arxiv.org/abs/1506.00935 | id:1506.00935 author:Hastagiri P. Vanchinathan, Andreas Marfurt, Charles-Antoine Robelin, Donald Kossmann, Andreas Krause category:cs.LG cs.AI cs.IT math.IT H.2.8  published:2015-06-02 summary:Suppose there is a large collection of items, each with an associated cost and an inherent utility that is revealed only once we commit to selecting it. Given a budget on the cumulative cost of the selected items, how can we pick a subset of maximal value? This task generalizes several important problems such as multi-arm bandits, active search and the knapsack problem. We present an algorithm, GP-Select, which utilizes prior knowledge about similarity be- tween items, expressed as a kernel function. GP-Select uses Gaussian process prediction to balance exploration (estimating the unknown value of items) and exploitation (selecting items of high value). We extend GP-Select to be able to discover sets that simultaneously have high utility and are diverse. Our preference for diversity can be specified as an arbitrary monotone submodular function that quantifies the diminishing returns obtained when selecting similar items. Furthermore, we exploit the structure of the model updates to achieve an order of magnitude (up to 40X) speedup in our experiments without resorting to approximations. We provide strong guarantees on the performance of GP-Select and apply it to three real-world case studies of industrial relevance: (1) Refreshing a repository of prices in a Global Distribution System for the travel industry, (2) Identifying diverse, binding-affine peptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale recommender system by recommending items to users. version:1
arxiv-1506-00925 | Facial Expressions Tracking and Recognition: Database Protocols for Systems Validation and Evaluation | http://arxiv.org/abs/1506.00925 | id:1506.00925 author:Catarina Runa Miranda, Pedro Mendes, Pedro Coelho, Xenxo Alvarez, João Freitas, Miguel Sales Dias, Verónica Costa Orvalho category:cs.CV  published:2015-06-02 summary:Each human face is unique. It has its own shape, topology, and distinguishing features. As such, developing and testing facial tracking systems are challenging tasks. The existing face recognition and tracking algorithms in Computer Vision mainly specify concrete situations according to particular goals and applications, requiring validation methodologies with data that fits their purposes. However, a database that covers all possible variations of external and factors does not exist, increasing researchers' work in acquiring their own data or compiling groups of databases. To address this shortcoming, we propose a methodology for facial data acquisition through definition of fundamental variables, such as subject characteristics, acquisition hardware, and performance parameters. Following this methodology, we also propose two protocols that allow the capturing of facial behaviors under uncontrolled and real-life situations. As validation, we executed both protocols which lead to creation of two sample databases: FdMiee (Facial database with Multi input, expressions, and environments) and FACIA (Facial Multimodal database driven by emotional induced acting). Using different types of hardware, FdMiee captures facial information under environmental and facial behaviors variations. FACIA is an extension of FdMiee introducing a pipeline to acquire additional facial behaviors and speech using an emotion-acting method. Therefore, this work eases the creation of adaptable database according to algorithm's requirements and applications, leading to simplified validation and testing processes. version:1
arxiv-1506-00839 | The Influence of Context on Dialogue Act Recognition | http://arxiv.org/abs/1506.00839 | id:1506.00839 author:Eugénio Ribeiro, Ricardo Ribeiro, David Martins de Matos category:cs.CL H.1.2; H.3.1; I.2.7  published:2015-06-02 summary:This article presents a deep analysis of the influence of context information on dialogue act recognition. We performed experiments on the annotated subsets of three different corpora: the widely explored Switchboard Dialog Act Corpus, as well as the unexplored LEGO and Cambridge Restaurant corpora. In contrast with previous work, especially in what concerns the Switchboard corpus, we used an event-based classification approach, using SVMs, instead of the more common sequential approaches, such as HMMs. We opted for such an approach so that we could control the amount of provided context information and, thus, explore its range of influence. Our base features consist of n-grams, punctuation, and wh-words. Context information is obtained from previous utterances and provided in three ways -- n-grams, n-grams tagged with relative position, and dialogue act classifications. A comparative study was conducted to evaluate the performance of the three approaches. From it, we were able to assess the importance of context information on dialogue act recognition, as well as its range of influence for each of the three selected representations. In addition to the conclusions originated by the analysis, this work also produced results that advance the state-of-the-art, especially considering previous work on the Switchboard corpus. Furthermore, since, to our knowledge, the remaining datasets had not been previously explored for this task, our experiments can be used as baselines for future work on those corpora. version:1
arxiv-1504-08117 | Average Convergence Rate of Evolutionary Algorithms | http://arxiv.org/abs/1504.08117 | id:1504.08117 author:Jun He, Guangming Lin category:cs.NE  published:2015-04-30 summary:In evolutionary optimization, it is important to understand how fast evolutionary algorithms converge to the optimum per generation, or their convergence rate. This paper proposes a new measure of the convergence rate, called average convergence rate. It is a normalised geometric mean of the reduction ratio of the fitness difference per generation. The calculation of the average convergence rate is very simple and it is applicable for most evolutionary algorithms on both continuous and discrete optimization. A theoretical study of the average convergence rate is conducted for discrete optimization. Lower bounds on the average convergence rate are derived. The limit of the average convergence rate is analysed and then the asymptotic average convergence rate is proposed. version:3
arxiv-1506-00799 | Learning Speech Rate in Speech Recognition | http://arxiv.org/abs/1506.00799 | id:1506.00799 author:Xiangyu Zeng, Shi Yin, Dong Wang category:cs.CL cs.LG  published:2015-06-02 summary:A significant performance reduction is often observed in speech recognition when the rate of speech (ROS) is too low or too high. Most of present approaches to addressing the ROS variation focus on the change of speech signals in dynamic properties caused by ROS, and accordingly modify the dynamic model, e.g., the transition probabilities of the hidden Markov model (HMM). However, an abnormal ROS changes not only the dynamic but also the static property of speech signals, and thus can not be compensated for purely by modifying the dynamic model. This paper proposes an ROS learning approach based on deep neural networks (DNN), which involves an ROS feature as the input of the DNN model and so the spectrum distortion caused by ROS can be learned and compensated for. The experimental results show that this approach can deliver better performance for too slow and too fast utterances, demonstrating our conjecture that ROS impacts both the dynamic and the static property of speech. In addition, the proposed approach can be combined with the conventional HMM transition adaptation method, offering additional performance gains. version:1
arxiv-1506-00165 | Labeled compression schemes for extremal classes | http://arxiv.org/abs/1506.00165 | id:1506.00165 author:Shay Moran, Manfred K. Warmuth category:cs.LG cs.DM math.CO  published:2015-05-30 summary:It is a long-standing open problem whether there always exists a compression scheme whose size is of the order of the Vapnik-Chervonienkis (VC) dimension $d$. Recently compression schemes of size exponential in $d$ have been found for any concept class of VC dimension $d$. Previously size $d$ unlabeled compression scheme have been given for maximum classes, which are special concept classes whose size equals an upper bound due to Sauer-Shelah. We consider a natural generalization of the maximum classes called extremal classes. Their definition is based on a generalization of the Sauer-Shelah bound called the Sandwich Theorem which has applications in many areas of combinatorics. The key result of the paper is the construction of a labeled compression scheme for extremal classes of size equal to their VC dimension. We also give a number of open problems concerning the combinatorial structure of extremal classes and the existence of unlabeled compression schemes for them. version:2
arxiv-1506-00768 | Soft Computing Techniques for Change Detection in remotely sensed images : A Review | http://arxiv.org/abs/1506.00768 | id:1506.00768 author:Madhu Khurana, Vikas Saxena category:cs.NE cs.CV  published:2015-06-02 summary:With the advent of remote sensing satellites, a huge repository of remotely sensed images is available. Change detection in remotely sensed images has been an active research area as it helps us understand the transitions that are taking place on the Earths surface. This paper discusses the methods and their classifications proposed by various researchers for change detection. Since use of soft computing based techniques are now very popular among research community, this paper also presents a classification based on learning techniques used in soft-computing methods for change detection. version:1
arxiv-1506-00765 | Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology | http://arxiv.org/abs/1506.00765 | id:1506.00765 author:Zheng Cai, Donglin Cao, Rongrong Ji category:cs.MM cs.CL cs.IR  published:2015-06-02 summary:With faster connection speed, Internet users are now making social network a huge reservoir of texts, images and video clips (GIF). Sentiment analysis for such online platform can be used to predict political elections, evaluates economic indicators and so on. However, GIF sentiment analysis is quite challenging, not only because it hinges on spatio-temporal visual contentabstraction, but also for the relationship between such abstraction and final sentiment remains unknown.In this paper, we dedicated to find out such relationship.We proposed a SentiPairSequence basedspatiotemporal visual sentiment ontology, which forms the midlevel representations for GIFsentiment. The establishment process of SentiPair contains two steps. First, we construct the Synset Forest to define the semantic tree structure of visual sentiment label elements. Then, through theSynset Forest, we organically select and combine sentiment label elements to form a mid-level visual sentiment representation. Our experiments indicate that SentiPair outperforms other competing mid-level attributes. Using SentiPair, our analysis frameworkcan achieve satisfying prediction accuracy (72.6%). We also opened ourdataset (GSO-2015) to the research community. GSO-2015 contains more than 6,000 manually annotated GIFs out of more than 40,000 candidates. Each is labeled with both sentiment and SentiPair Sequence. version:1
arxiv-1506-00761 | Image Retrieval Based on Binary Signature ang S-kGraph | http://arxiv.org/abs/1506.00761 | id:1506.00761 author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3  published:2015-06-02 summary:In this paper, we introduce an optimum approach for querying similar images on large digital-image databases. Our work is based on RBIR (region-based image retrieval) method which uses multiple regions as the key to retrieval images. This method significantly improves the accuracy of queries. However, this also increases the cost of computing. To reduce this expensive computational cost, we implement binary signature encoder which maps an image to its identification in binary. In order to fasten the lookup, binary signatures of images are classified by the help of S-kGraph. Finally, our work is evaluated on COREL's images. version:1
arxiv-1504-01106 | Discriminative Neural Sentence Modeling by Tree-Based Convolution | http://arxiv.org/abs/1504.01106 | id:1504.01106 author:Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE  published:2015-04-05 summary:This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our models leverage either constituency trees or dependency trees of sentences. The tree-based convolution process extracts sentences' structural features, and these features are aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, which enables effective structural feature learning and extraction. We evaluate our models on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. version:5
arxiv-1506-00752 | What Makes Kevin Spacey Look Like Kevin Spacey | http://arxiv.org/abs/1506.00752 | id:1506.00752 author:Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, Steve Seitz category:cs.CV  published:2015-06-02 summary:We reconstruct a controllable model of a person from a large photo collection that captures his or her {\em persona}, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or {\em puppeteer} the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video. version:1
arxiv-1411-2942 | 3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach | http://arxiv.org/abs/1411.2942 | id:1411.2942 author:Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, Kostas Daniilidis category:cs.CV  published:2014-11-11 summary:We investigate the problem of estimating the 3D shape of an object, given a set of 2D landmarks in a single image. To alleviate the reconstruction ambiguity, a widely-used approach is to confine the unknown 3D shape within a shape space built upon existing shapes. While this approach has proven to be successful in various applications, a challenging issue remains, i.e., the joint estimation of shape parameters and camera-pose parameters requires to solve a nonconvex optimization problem. The existing methods often adopt an alternating minimization scheme to locally update the parameters, and consequently the solution is sensitive to initialization. In this paper, we propose a convex formulation to address this problem and develop an efficient algorithm to solve the proposed convex program. We demonstrate the exact recovery property of the proposed method, its merits compared to alternative methods, and the applicability in human pose and car shape estimation. version:4
arxiv-1412-5661 | DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection | http://arxiv.org/abs/1412.5661 | id:1412.5661 author:Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, Xiaoou Tang category:cs.CV cs.NE  published:2014-12-17 summary:In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN \cite{girshick2014rich}, which was the state-of-the-art, from 31\% to 50.3\% on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1\%. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline. version:2
arxiv-1506-00711 | Quantifying Creativity in Art Networks | http://arxiv.org/abs/1506.00711 | id:1506.00711 author:Ahmed Elgammal, Babak Saleh category:cs.AI cs.CV cs.CY cs.MM cs.SI  published:2015-06-02 summary:Can we develop a computer algorithm that assesses the creativity of a painting given its context within art history? This paper proposes a novel computational framework for assessing the creativity of creative products, such as paintings, sculptures, poetry, etc. We use the most common definition of creativity, which emphasizes the originality of the product and its influential value. The proposed computational framework is based on constructing a network between creative products and using this network to infer about the originality and influence of its nodes. Through a series of transformations, we construct a Creativity Implication Network. We show that inference about creativity in this network reduces to a variant of network centrality problems which can be solved efficiently. We apply the proposed framework to the task of quantifying creativity of paintings (and sculptures). We experimented on two datasets with over 62K paintings to illustrate the behavior of the proposed framework. We also propose a methodology for quantitatively validating the results of the proposed algorithm, which we call the "time machine experiment". version:1
arxiv-1506-00698 | Statistical Machine Translation Features with Multitask Tensor Networks | http://arxiv.org/abs/1506.00698 | id:1506.00698 author:Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar, Rabih Zbib, Richard Schwartz, John Makhoul category:cs.CL  published:2015-06-01 summary:We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non-local translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features. version:1
arxiv-1506-00673 | Mutual Dependence: A Novel Method for Computing Dependencies Between Random Variables | http://arxiv.org/abs/1506.00673 | id:1506.00673 author:Rahul Agarwal, Pierre Sacre, Sridevi V. Sarma category:math.ST stat.ML stat.TH  published:2015-06-01 summary:In data science, it is often required to estimate dependencies between different data sources. These dependencies are typically calculated using Pearson's correlation, distance correlation, and/or mutual information. However, none of these measures satisfy all the Granger's axioms for an "ideal measure". One such ideal measure, proposed by Granger himself, calculates the Bhattacharyya distance between the joint probability density function (pdf) and the product of marginal pdfs. We call this measure the mutual dependence. However, to date this measure has not been directly computable from data. In this paper, we use our recently introduced maximum likelihood non-parametric estimator for band-limited pdfs, to compute the mutual dependence directly from the data. We construct the estimator of mutual dependence and compare its performance to standard measures (Pearson's and distance correlation) for different known pdfs by computing convergence rates, computational complexity, and the ability to capture nonlinear dependencies. Our mutual dependence estimator requires fewer samples to converge to theoretical values, is faster to compute, and captures more complex dependencies than standard measures. version:1
arxiv-1506-00671 | Sample-Optimal Density Estimation in Nearly-Linear Time | http://arxiv.org/abs/1506.00671 | id:1506.00671 author:Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH  published:2015-06-01 summary:We design a new, fast algorithm for agnostically learning univariate probability distributions whose densities are well approximated by piecewise polynomial functions. Let $f$ be the density function of an arbitrary univariate distribution, and suppose that $f$ is $\mathrm{OPT}$-close in $L_1$-distance to an unknown piecewise polynomial function with $t$ interval pieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\epsilon^2)$ samples from $f$, runs in time $\tilde{O}(n \cdot \mathrm{poly}(d))$, and with probability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis $h$ that is $4 \cdot \mathrm{OPT} +\epsilon$ close to $f$. Our general algorithm yields (nearly) sample-optimal and nearly-linear time estimators for a wide range of structured distribution families over both continuous and discrete domains in a unified way. For most of our applications, these are the first sample-optimal and nearly-linear time estimators in the literature. As a consequence, our work resolves the sample and computational complexities of a broad class of inference tasks via a single "meta-algorithm". Moreover, we experimentally demonstrate that our algorithm performs very well in practice. Our algorithm consists of three "levels": (i) At the top level, we employ an iterative greedy algorithm for finding a good partition of the real line into the pieces of a piecewise polynomial. (ii) For each piece, we show that the sub-problem of finding a good polynomial fit on the current interval can be solved efficiently with a separation oracle method. (iii) We reduce the task of finding a separating hyperplane to a combinatorial problem and give an efficient algorithm for this problem. Combining these three procedures gives a density estimation algorithm with the claimed guarantees. version:1
arxiv-1503-01007 | Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets | http://arxiv.org/abs/1503.01007 | id:1503.01007 author:Armand Joulin, Tomas Mikolov category:cs.NE cs.LG  published:2015-03-03 summary:Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory. version:4
arxiv-1412-2197 | Practice in Synonym Extraction at Large Scale | http://arxiv.org/abs/1412.2197 | id:1412.2197 author:Liangliang Cao, Chang Wang category:cs.CL  published:2014-12-06 summary:Synonym extraction is an important task in natural language processing and often used as a submodule in query expansion, question answering and other applications. Automatic synonym extractor is highly preferred for large scale applications. Previous studies in synonym extraction are most limited to small scale datasets. In this paper, we build a large dataset with 3.4 million synonym/non-synonym pairs to capture the challenges in real world scenarios. We proposed (1) a new cost function to accommodate the unbalanced learning problem, and (2) a feature learning based deep neural network to model the complicated relationships in synonym pairs. We compare several different approaches based on SVMs and neural networks, and find out a novel feature learning based neural network outperforms the methods with hand-assigned features. Specifically, the best performance of our model surpasses the SVM baseline with a significant 97\% relative improvement. version:3
arxiv-1506-00619 | Blocks and Fuel: Frameworks for deep learning | http://arxiv.org/abs/1506.00619 | id:1506.00619 author:Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, Yoshua Bengio category:cs.LG cs.NE stat.ML  published:2015-06-01 summary:We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano's symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly. version:1
arxiv-1506-00578 | On Quantum Generalizations of Information-Theoretic Measures and their Contribution to Distributional Semantics | http://arxiv.org/abs/1506.00578 | id:1506.00578 author:William Blacoe category:cs.IT cs.CL math.IT  published:2015-06-01 summary:Information-theoretic measures such as relative entropy and correlation are extremely useful when modeling or analyzing the interaction of probabilistic systems. We survey the quantum generalization of 5 such measures and point out some of their commonalities and interpretations. In particular we find the application of information theory to distributional semantics useful. By modeling the distributional meaning of words as density operators rather than vectors, more of their semantic structure may be exploited. Furthermore, properties of and interactions between words such as ambiguity, similarity and entailment can be simulated more richly and intuitively when using methods from quantum information theory. version:1
arxiv-1506-00553 | Bootstrap Bias Corrections for Ensemble Methods | http://arxiv.org/abs/1506.00553 | id:1506.00553 author:Giles Hooker, Lucas Mentch category:stat.ML  published:2015-06-01 summary:This paper examines the use of a residual bootstrap for bias correction in machine learning regression methods. Accounting for bias is an important obstacle in recent efforts to develop statistical inference for machine learning methods. We demonstrate empirically that the proposed bootstrap bias correction can lead to substantial improvements in both bias and predictive accuracy. In the context of ensembles of trees, we show that this correction can be approximated at only double the cost of training the original ensemble without introducing additional variance. Our method is shown to improve test-set accuracy over random forests by up to 70\% on example problems from the UCI repository. version:1
arxiv-1506-00552 | Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection | http://arxiv.org/abs/1506.00552 | id:1506.00552 author:Julie Nutini, Mark Schmidt, Issam H. Laradji, Michael Friedlander, Hoyt Koepke category:math.OC cs.LG stat.CO stat.ML  published:2015-06-01 summary:There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that---except in extreme cases---it's convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. version:1
arxiv-1506-00528 | Medical Synonym Extraction with Concept Space Models | http://arxiv.org/abs/1506.00528 | id:1506.00528 author:Chang Wang, Liangliang Cao, Bowen Zhou category:cs.CL  published:2015-06-01 summary:In this paper, we present a novel approach for medical synonym extraction. We aim to integrate the term embedding with the medical domain knowledge for healthcare applications. One advantage of our method is that it is very scalable. Experiments on a dataset with more than 1M term pairs show that the proposed approach outperforms the baseline approaches by a large margin. version:1
arxiv-1506-00527 | User Preferences Modeling and Learning for Pleasing Photo Collage Generation | http://arxiv.org/abs/1506.00527 | id:1506.00527 author:Simone Bianco, Gianluigi Ciocca category:cs.MM cs.CV cs.HC  published:2015-06-01 summary:In this paper we consider how to automatically create pleasing photo collages created by placing a set of images on a limited canvas area. The task is formulated as an optimization problem. Differently from existing state-of-the-art approaches, we here exploit subjective experiments to model and learn pleasantness from user preferences. To this end, we design an experimental framework for the identification of the criteria that need to be taken into account to generate a pleasing photo collage. Five different thematic photo datasets are used to create collages using state-of-the-art criteria. A first subjective experiment where several subjects evaluated the collages, emphasizes that different criteria are involved in the subjective definition of pleasantness. We then identify new global and local criteria and design algorithms to quantify them. The relative importance of these criteria are automatically learned by exploiting the user preferences, and new collages are generated. To validate our framework, we performed several psycho-visual experiments involving different users. The results shows that the proposed framework allows to learn a novel computational model which effectively encodes an inter-user definition of pleasantness. The learned definition of pleasantness generalizes well to new photo datasets of different themes and sizes not used in the learning. Moreover, compared with two state of the art approaches, the collages created using our framework are preferred by the majority of the users. version:1
arxiv-1506-00481 | Robust Face Recognition with Structural Binary Gradient Patterns | http://arxiv.org/abs/1506.00481 | id:1506.00481 author:Weilin Huang, Hujun Yin category:cs.CV  published:2015-06-01 summary:This paper presents a computationally efficient yet powerful binary framework for robust facial representation based on image gradients. It is termed as structural binary gradient patterns (SBGP). To discover underlying local structures in the gradient domain, we compute image gradients from multiple directions and simplify them into a set of binary strings. The SBGP is derived from certain types of these binary strings that have meaningful local structures and are capable of resembling fundamental textural information. They detect micro orientational edges and possess strong orientation and locality capabilities, thus enabling great discrimination. The SBGP also benefits from the advantages of the gradient domain and exhibits profound robustness against illumination variations. The binary strategy realized by pixel correlations in a small neighborhood substantially simplifies the computational complexity and achieves extremely efficient processing with only 0.0032s in Matlab for a typical face image. Furthermore, the discrimination power of the SBGP can be enhanced on a set of defined orientational image gradient magnitudes, further enforcing locality and orientation. Results of extensive experiments on various benchmark databases illustrate significant improvements of the SBGP based representations over the existing state-of-the-art local descriptors in the terms of discrimination, robustness and complexity. Codes for the SBGP methods will be available at http://www.eee.manchester.ac.uk/research/groups/sisp/software/. version:1
arxiv-1501-02655 | Texture Retrieval via the Scattering Transform | http://arxiv.org/abs/1501.02655 | id:1501.02655 author:Alexander Sagel, Dominik Meyer, Hao Shen category:cs.IR cs.CV  published:2015-01-12 summary:This work studies the problem of content-based image retrieval, specifically, texture retrieval. It focuses on feature extraction and similarity measure for texture images. Our approach employs a recently developed method, the so-called Scattering transform, for the process of feature extraction in texture retrieval. It shares a distinctive property of providing a robust representation, which is stable with respect to spatial deformations. Recent work has demonstrated its capability for texture classification, and hence as a promising candidate for the problem of texture retrieval. Moreover, we adopt a common approach of measuring the similarity of textures by comparing the subband histograms of a filterbank transform. To this end we derive a similarity measure based on the popular Bhattacharyya Kernel. Despite the popularity of describing histograms using parametrized probability density functions, such as the Generalized Gaussian Distribution, it is unfortunately not applicable for describing most of the Scattering transform subbands, due to the complex modulus performed on each one of them. In this work, we propose to use the Weibull distribution to model the Scattering subbands of descendant layers. Our numerical experiments demonstrated the effectiveness of the proposed approach, in comparison with several state of the arts. version:4
arxiv-1506-00395 | Hierarchical structure-and-motion recovery from uncalibrated images | http://arxiv.org/abs/1506.00395 | id:1506.00395 author:Roberto Toldo, Riccardo Gherardi, Michela Farenzena, Andrea Fusiello category:cs.CV  published:2015-06-01 summary:This paper addresses the structure-and-motion problem, that requires to find camera motion and 3D struc- ture from point matches. A new pipeline, dubbed Samantha, is presented, that departs from the prevailing sequential paradigm and embraces instead a hierarchical approach. This method has several advantages, like a provably lower computational complexity, which is necessary to achieve true scalability, and better error containment, leading to more stability and less drift. Moreover, a practical autocalibration procedure allows to process images without ancillary information. Experiments with real data assess the accuracy and the computational efficiency of the method. version:1
arxiv-1506-00368 | RBIR using Interest Regions and Binary Signatures | http://arxiv.org/abs/1506.00368 | id:1506.00368 author:Thanh The Van, Thanh Manh Le category:cs.CV H.2.8; H.3.3  published:2015-06-01 summary:In this paper, we introduce an approach to overcome the low accuracy of the Content-Based Image Retrieval (CBIR) (when using the global features). To increase the accuracy, we use Harris-Laplace detector to identify the interest regions of image. Then, we build the Region-Based Image Retrieval (RBIR). For the efficient image storage and retrieval, we encode images into binary signatures. The binary signature of a image is created from its interest regions. Furthermore, this paper also provides an algorithm for image retrieval on S-tree by comparing the images' signatures on a metric similarly to EMD (earth mover's distance). Finally, we evaluate the created models on COREL's images. version:1
arxiv-1409-7480 | Generalized Twin Gaussian Processes using Sharma-Mittal Divergence | http://arxiv.org/abs/1409.7480 | id:1409.7480 author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.LG cs.CV stat.ML  published:2014-09-26 summary:There has been a growing interest in mutual information measures due to their wide range of applications in Machine Learning and Computer Vision. In this paper, we present a generalized structured regression framework based on Shama-Mittal divergence, a relative entropy measure, which is introduced to the Machine Learning community in this work. Sharma-Mittal (SM) divergence is a generalized mutual information measure for the widely used R\'enyi, Tsallis, Bhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we study Sharma-Mittal divergence as a cost function in the context of the Twin Gaussian Processes (TGP)~\citep{Bo:2010}, which generalizes over the KL-divergence without computational penalty. We show interesting properties of Sharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing insights in the traditional TGP formulation. However, we generalize this theory based on SM-divergence instead of KL-divergence which is a special case. Experimentally, we evaluated the proposed SMTGP framework on several datasets. The results show that SMTGP reaches better predictions than KL-based TGP, since it offers a bigger class of models through its parameters that we learn from the data. version:5
arxiv-1505-06475 | A Fast and Flexible Algorithm for the Graph-Fused Lasso | http://arxiv.org/abs/1505.06475 | id:1505.06475 author:Wesley Tansey, James G. Scott category:stat.ML stat.CO  published:2015-05-24 summary:We propose a new algorithm for solving the graph-fused lasso (GFL), a method for parameter estimation that operates under the assumption that the signal tends to be locally constant over a predefined graph structure. Our key insight is to decompose the graph into a set of trails which can then each be solved efficiently using techniques for the ordinary (1D) fused lasso. We leverage these trails in a proximal algorithm that alternates between closed form primal updates and fast dual trail updates. The resulting techinque is both faster than previous GFL methods and more flexible in the choice of loss function and graph structure. Furthermore, we present two algorithms for constructing trail sets and show empirically that they offer a tradeoff between preprocessing time and convergence rate. version:3
arxiv-1506-00327 | Imaging Time-Series to Improve Classification and Imputation | http://arxiv.org/abs/1506.00327 | id:1506.00327 author:Zhiguang Wang, Tim Oates category:cs.LG cs.NE stat.ML  published:2015-06-01 summary:Inspired by recent successes of deep learning in computer vision, we propose a novel framework for encoding time series as different types of images, namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov Transition Fields (MTF). This enables the use of techniques from computer vision for time series classification and imputation. We used Tiled Convolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn high-level features from the individual and compound GASF-GADF-MTF images. Our approaches achieve highly competitive results when compared to nine of the current best time series classification approaches. Inspired by the bijection property of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on the GASF images of four standard and one synthesized compound dataset. The imputation MSE on test data is reduced by 12.18%-48.02% when compared to using the raw data. An analysis of the features and weights learned via tiled CNNs and DAs explains why the approaches work. version:1
arxiv-1506-00323 | Robust PCA: Optimization of the Robust Reconstruction Error over the Stiefel Manifold | http://arxiv.org/abs/1506.00323 | id:1506.00323 author:Anastasia Podosinnikova, Simon Setzer, Matthias Hein category:stat.ML cs.LG  published:2015-06-01 summary:It is well known that Principal Component Analysis (PCA) is strongly affected by outliers and a lot of effort has been put into robustification of PCA. In this paper we present a new algorithm for robust PCA minimizing the trimmed reconstruction error. By directly minimizing over the Stiefel manifold, we avoid deflation as often used by projection pursuit methods. In distinction to other methods for robust PCA, our method has no free parameter and is computationally very efficient. We illustrate the performance on various datasets including an application to background modeling and subtraction. Our method performs better or similar to current state-of-the-art methods while being faster. version:1
arxiv-1506-00312 | Copeland Dueling Bandits | http://arxiv.org/abs/1506.00312 | id:1506.00312 author:Masrour Zoghi, Zohar Karnin, Shimon Whiteson, Maarten de Rijke category:cs.LG  published:2015-06-01 summary:A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form $O(K \log T)$ but require restrictive assumptions, or offer bounds of the form $O(K^2 \log T)$ without requiring such assumptions. Our results offer the best of both worlds: $O(K \log T)$ bounds without restrictive assumptions. version:1
arxiv-1506-00308 | Automatic Inference for Inverting Software Simulators via Probabilistic Programming | http://arxiv.org/abs/1506.00308 | id:1506.00308 author:Ardavan Saeedi, Vlad Firoiu, Vikash Mansinghka category:stat.ML  published:2015-05-31 summary:Models of complex systems are often formalized as sequential software simulators: computationally intensive programs that iteratively build up probable system configurations given parameters and initial conditions. These simulators enable modelers to capture effects that are difficult to characterize analytically or summarize statistically. However, in many real-world applications, these simulations need to be inverted to match the observed data. This typically requires the custom design, derivation and implementation of sophisticated inversion algorithms. Here we give a framework for inverting a broad class of complex software simulators via probabilistic programming and automatic inference, using under 20 lines of probabilistic code. Our approach is based on a formulation of inversion as approximate inference in a simple sequential probabilistic model. We implement four inference strategies, including Metropolis-Hastings, a sequentialized Metropolis-Hastings scheme, and a particle Markov chain Monte Carlo scheme, requiring 4 or fewer lines of probabilistic code each. We demonstrate our framework by applying it to invert a real geological software simulator from the oil and gas industry. version:1
arxiv-1506-00301 | Interactive Knowledge Base Population | http://arxiv.org/abs/1506.00301 | id:1506.00301 author:Travis Wolfe, Mark Dredze, James Mayfield, Paul McNamee, Craig Harman, Tim Finin, Benjamin Van Durme category:cs.AI cs.CL  published:2015-05-31 summary:Most work on building knowledge bases has focused on collecting entities and facts from as large a collection of documents as possible. We argue for and describe a new paradigm where the focus is on a high-recall extraction over a small collection of documents under the supervision of a human expert, that we call Interactive Knowledge Base Population (IKBP). version:1
arxiv-1502-04081 | A Linear Dynamical System Model for Text | http://arxiv.org/abs/1502.04081 | id:1502.04081 author:David Belanger, Sham Kakade category:stat.ML cs.CL cs.LG  published:2015-02-13 summary:Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words' representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity. version:2
arxiv-1506-00278 | Visual Madlibs: Fill in the blank Image Generation and Question Answering | http://arxiv.org/abs/1506.00278 | id:1506.00278 author:Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg category:cs.CV cs.CL  published:2015-05-31 summary:In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks. version:1
arxiv-1306-1587 | Spectral Convergence of the connection Laplacian from random samples | http://arxiv.org/abs/1306.1587 | id:1306.1587 author:Amit Singer, Hau-tieng Wu category:math.NA math.ST stat.ME stat.ML stat.TH  published:2013-06-07 summary:Spectral methods that are based on eigenvectors and eigenvalues of discrete graph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps are often used for manifold learning and non-linear dimensionality reduction. It was previously shown by Belkin and Niyogi \cite{belkin_niyogi:2007} that the eigenvectors and eigenvalues of the graph Laplacian converge to the eigenfunctions and eigenvalues of the Laplace-Beltrami operator of the manifold in the limit of infinitely many data points sampled independently from the uniform distribution over the manifold. Recently, we introduced Vector Diffusion Maps and showed that the connection Laplacian of the tangent bundle of the manifold can be approximated from random samples. In this paper, we present a unified framework for approximating other connection Laplacians over the manifold by considering its principle bundle structure. We prove that the eigenvectors and eigenvalues of these Laplacians converge in the limit of infinitely many independent random samples. We generalize the spectral convergence results to the case where the data points are sampled from a non-uniform distribution, and for manifolds with and without boundary. version:3
arxiv-1506-00227 | Parallel Spectral Clustering Algorithm Based on Hadoop | http://arxiv.org/abs/1506.00227 | id:1506.00227 author:Yajun Cui, Yang Zhao, Kafei Xiao, Chenglong Zhang, Lei Wang category:cs.DC cs.DS cs.LG  published:2015-05-31 summary:Spectral clustering and cloud computing is emerging branch of computer science or related discipline. It overcome the shortcomings of some traditional clustering algorithm and guarantee the convergence to the optimal solution, thus have to the widespread attention. This article first introduced the parallel spectral clustering algorithm research background and significance, and then to Hadoop the cloud computing Framework has carried on the detailed introduction, then has carried on the related to spectral clustering is introduced, then introduces the spectral clustering arithmetic Method of parallel and relevant steps, finally made the related experiments, and the experiment are summarized. version:1
arxiv-1506-00195 | Recurrent Neural Networks with External Memory for Language Understanding | http://arxiv.org/abs/1506.00195 | id:1506.00195 author:Baolin Peng, Kaisheng Yao category:cs.CL cs.AI cs.LG cs.NE  published:2015-05-31 summary:Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research. version:1
arxiv-1506-00176 | An Open Source Testing Tool for Evaluating Handwriting Input Methods | http://arxiv.org/abs/1506.00176 | id:1506.00176 author:Liquan Qiu, Lianwen Jin, Ruifen Dai, Yuxiang Zhang, Lei Li category:cs.HC cs.CV  published:2015-05-30 summary:This paper presents an open source tool for testing the recognition accuracy of Chinese handwriting input methods. The tool consists of two modules, namely the PC and Android mobile client. The PC client reads handwritten samples in the computer, and transfers them individually to the Android client in accordance with the socket communication protocol. After the Android client receives the data, it simulates the handwriting on screen of client device, and triggers the corresponding handwriting recognition method. The recognition accuracy is recorded by the Android client. We present the design principles and describe the implementation of the test platform. We construct several test datasets for evaluating different handwriting recognition systems, and conduct an objective and comprehensive test using six Chinese handwriting input methods with five datasets. The test results for the recognition accuracy are then compared and analyzed. version:1
arxiv-1502-03175 | Proximal Algorithms in Statistics and Machine Learning | http://arxiv.org/abs/1502.03175 | id:1502.03175 author:Nicholas G. Polson, James G. Scott, Brandon T. Willard category:stat.ML cs.LG stat.ME  published:2015-02-11 summary:In this paper we develop proximal methods for statistical learning. Proximal point algorithms are useful in statistics and machine learning for obtaining optimization solutions for composite functions. Our approach exploits closed-form solutions of proximal operators and envelope representations based on the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes. Envelope representations lead to novel proximal algorithms for statistical optimisation of composite objective functions which include both non-smooth and non-convex objectives. We illustrate our methodology with regularized Logistic and Poisson regression and non-convex bridge penalties with a fused lasso norm. We provide a discussion of convergence of non-descent algorithms with acceleration and for non-convex functions. Finally, we provide directions for future research. version:3
arxiv-1506-00168 | Using curvature to distinguish between surface reflections and vessel contents in computer vision based recognition of materials in transparent vessels | http://arxiv.org/abs/1506.00168 | id:1506.00168 author:Sagi Eppel category:cs.CV  published:2015-05-30 summary:The recognition of materials and objects inside transparent containers using computer vision has a wide range of applications, ranging from industrial bottles filling to the automation of chemistry laboratory. One of the main challenges in such recognition is the ability to distinguish between image features resulting from the vessels surface and image features resulting from the material inside the vessel. Reflections and the functional parts of a vessels surface can create strong edges that can be mistakenly identified as corresponding to the vessel contents, and cause recognition errors. The ability to evaluate whether a specific edge in an image stems from the vessels surface or from its contents can considerably improve the ability to identify materials inside transparent vessels. This work will suggest a method for such evaluation, based on the following two assumptions: 1) Areas of high curvature on the vessel surface are likely to cause strong edges due to changes in reflectivity, as is the appearance of functional parts (e.g. corks or valves). 2) Most transparent vessels (bottles, glasses) have high symmetry (cylindrical). As a result the curvature angle of the vessels surface at each point of the image is similar to the curvature angle of the contour line of the vessel in the same row in the image. These assumptions, allow the identification of image regions with strong edges corresponding to the vessel surface reflections. Combining this method with existing image analysis methods for detecting materials inside transparent containers allows considerable improvement in accuracy. version:1
arxiv-1410-8206 | Addressing the Rare Word Problem in Neural Machine Translation | http://arxiv.org/abs/1410.8206 | id:1410.8206 author:Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech Zaremba category:cs.CL cs.LG cs.NE  published:2014-10-30 summary:Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT14 contest task. version:4
arxiv-1412-8659 | Deep Roto-Translation Scattering for Object Classification | http://arxiv.org/abs/1412.8659 | id:1412.8659 author:Edouard Oyallon, Stéphane Mallat category:cs.CV  published:2014-12-30 summary:Dictionary learning algorithms or supervised deep convolution networks have considerably improved the efficiency of predefined feature representations such as SIFT. We introduce a deep scattering convolution network, with predefined wavelet filters over spatial and angular variables. This representation brings an important improvement to results previously obtained with predefined features over object image databases such as Caltech and CIFAR. The resulting accuracy is comparable to results obtained with unsupervised deep learning and dictionary based representations. This shows that refining image representations by using geometric priors is a promising direction to improve image classification and its understanding. version:2
arxiv-1504-00905 | Robust Anomaly Detection Using Semidefinite Programming | http://arxiv.org/abs/1504.00905 | id:1504.00905 author:Jose A. Lopez, Octavia Camps, Mario Sznaier category:math.OC cs.CV cs.LG cs.SY  published:2015-04-03 summary:This paper presents a new approach, based on polynomial optimization and the method of moments, to the problem of anomaly detection. The proposed technique only requires information about the statistical moments of the normal-state distribution of the features of interest and compares favorably with existing approaches (such as Parzen windows and 1-class SVM). In addition, it provides a succinct description of the normal state. Thus, it leads to a substantial simplification of the the anomaly detection problem when working with higher dimensional datasets. version:2
arxiv-1506-00102 | Efficient combination of pairswise feature networks | http://arxiv.org/abs/1506.00102 | id:1506.00102 author:Pau Bellot, Patrick E. Meyer category:stat.ML cs.LG  published:2015-05-30 summary:This paper presents a novel method for the reconstruction of a neural network connectivity using calcium fluorescence data. We introduce a fast unsupervised method to integrate different networks that reconstructs structural connectivity from neuron activity. Our method improves the state-of-the-art reconstruction method General Transfer Entropy (GTE). We are able to better eliminate indirect links, improving therefore the quality of the network via a normalization and ensemble process of GTE and three new informative features. The approach is based on a simple combination of networks, which is remarkably fast. The performance of our approach is benchmarked on simulated time series provided at the connectomics challenge and also submitted at the public competition. version:1
arxiv-1506-00097 | A Review of Feature and Data Fusion with Medical Images | http://arxiv.org/abs/1506.00097 | id:1506.00097 author:Alex Pappachen James, Belur Dasarathy category:cs.CV  published:2015-05-30 summary:The fusion techniques that utilize multiple feature sets to form new features that are often more robust and contain useful information for future processing are referred to as feature fusion. The term data fusion is applied to the class of techniques used for combining decisions obtained from multiple feature sets to form global decisions. Feature and data fusion interchangeably represent two important classes of techniques that have proved to be of practical importance in a wide range of medical imaging problems version:1
arxiv-1503-00075 | Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks | http://arxiv.org/abs/1503.00075 | id:1503.00075 author:Kai Sheng Tai, Richard Socher, Christopher D. Manning category:cs.CL cs.AI cs.LG  published:2015-02-28 summary:Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank). version:3
arxiv-1506-00074 | Recognition of convolutional neural network based on CUDA Technology | http://arxiv.org/abs/1506.00074 | id:1506.00074 author:Yi-bin Huang, Kang Li, Ge Wang, Min Cao, Pin Li, Yu-jia Zhang category:cs.DC cs.NE  published:2015-05-30 summary:For the problem whether Graphic Processing Unit(GPU),the stream processor with high performance of floating-point computing is applicable to neural networks, this paper proposes the parallel recognition algorithm of Convolutional Neural Networks(CNNs).It adopts Compute Unified Device Architecture(CUDA)technology, definite the parallel data structures, and describes the mapping mechanism for computing tasks on CUDA. It compares the parallel recognition algorithm achieved on GPU of GTX200 hardware architecture with the serial algorithm on CPU. It improves speed by nearly 60 times. Result shows that GPU based the stream processor architecture ate more applicable to some related applications about neural networks than CPU. version:1
arxiv-1506-00060 | A Three-stage Approach for Segmenting Degraded Color Images: Smoothing, Lifting and Thresholding (SLaT) | http://arxiv.org/abs/1506.00060 | id:1506.00060 author:Xiaohao Cai, Raymond Chan, Mila Nikolova, Tieyong Zeng category:cs.CV math.NA 65F22 I.4.6  published:2015-05-30 summary:In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) method with three stages for multiphase segmentation of color images corrupted by different degradations: noise, information loss, and blur. At the first stage, a convex variant of the Mumford-Shah model is applied to each channel to obtain a smooth image. We show that the model has unique solution under the different degradations. In order to properly handle the color information, the second stage is dimension lifting where we consider a new vector-valued image composed of the restored image and its transform in the secondary color space with additional information. This ensures that even if the first color space has highly correlated channels, we can still have enough information to give good segmentation results. In the last stage, we apply multichannel thresholding to the combined vector-valued image to find the segmentation. The number of phases is only required in the last stage, so users can choose or change it all without the need of solving the previous stages again. Experiments demonstrate that our SLaT method gives excellent results in terms of segmentation quality and CPU time in comparison with other state-of-the-art segmentation methods. version:1
arxiv-1506-00051 | Bag-of-Genres for Video Genre Retrieval | http://arxiv.org/abs/1506.00051 | id:1506.00051 author:Leonardo A. Duarte, Otávio A. B. Penatti, Jurandy Almeida category:cs.CV  published:2015-05-30 summary:This paper presents a higher level representation for videos aiming at video genre retrieval. In video genre retrieval, there is a challenge that videos may comprise multiple categories, for instance, news videos may be composed of sports, documentary, and action. Therefore, it is interesting to encode the distribution of such genres in a compact and effective manner. We propose to create a visual dictionary using a genre classifier. Each visual word in the proposed model corresponds to a region in the classification space determined by the classifier's model learned on the training frames. Therefore, the video feature vector contains a summary of the activations of each genre in its contents. We evaluate the bag-of-genres model for video genre retrieval, using the dataset of MediaEval Tagging Task of 2012. Results show that the proposed model increases the quality of the representation being more compact than existing features. version:1
arxiv-1411-8003 | Guaranteed Matrix Completion via Non-convex Factorization | http://arxiv.org/abs/1411.8003 | id:1411.8003 author:Ruoyu Sun, Zhi-Quan Luo category:cs.LG  published:2014-11-28 summary:Matrix factorization is a popular approach for large-scale matrix completion. In this approach, the unknown low-rank matrix is expressed as the product of two much smaller matrices so that the low-rank property is automatically fulfilled. The resulting optimization problem, even with huge size, can be solved (to stationary points) very efficiently through standard optimization algorithms such as alternating minimization and stochastic gradient descent (SGD). However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of whether these algorithms will generate a good solution. In this paper, we establish a theoretical guarantee for the factorization based formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of a factorization based formulation, and recover the true low-rank matrix. A major difference of our work from the existing results is that we do not need resampling (i.e., using independent samples at each iteration) in either the algorithm or its analysis. To the best of our knowledge, our result is the first one that provides exact recovery guarantee for many standard algorithms such as gradient descent, SGD and block coordinate gradient descent. version:2
arxiv-1506-00037 | Using Syntactic Features for Phishing Detection | http://arxiv.org/abs/1506.00037 | id:1506.00037 author:Gilchan Park, Julia M. Taylor category:cs.CL  published:2015-05-29 summary:This paper reports on the comparison of the subject and object of verbs in their usage between phishing emails and legitimate emails. The purpose of this research is to explore whether the syntactic structures and subjects and objects of verbs can be distinguishable features for phishing detection. To achieve the objective, we have conducted two series of experiments: the syntactic similarity for sentences, and the subject and object of verb comparison. The results of the experiments indicated that both features can be used for some verbs, but more work has to be done for others. version:1
arxiv-1505-08153 | Feature Representation for Online Signature Verification | http://arxiv.org/abs/1505.08153 | id:1505.08153 author:Mohsen Fayyaz, Mohammad Hajizadeh_Saffar, Mohammad Sabokrou, Mahmood Fathy category:cs.CV cs.AI  published:2015-05-29 summary:Biometrics systems have been used in a wide range of applications and have improved people authentication. Signature verification is one of the most common biometric methods with techniques that employ various specifications of a signature. Recently, deep learning has achieved great success in many fields, such as image, sounds and text processing. In this paper, deep learning method has been used for feature extraction and feature selection. version:1
arxiv-1505-08149 | Modeling of the meaning: computational interpreting and understanding of natural language fragments | http://arxiv.org/abs/1505.08149 | id:1505.08149 author:Michael Kapustin, Pavlo Kapustin category:cs.CL  published:2015-05-29 summary:In this introductory article we present the basics of an approach to implementing computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties). version:1
arxiv-1505-08082 | Learning to count with deep object features | http://arxiv.org/abs/1505.08082 | id:1505.08082 author:Santi Seguí, Oriol Pujol, Jordi Vitrià category:cs.CV  published:2015-05-29 summary:Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene. version:1
arxiv-1505-08075 | Transition-Based Dependency Parsing with Stack Long Short-Term Memory | http://arxiv.org/abs/1505.08075 | id:1505.08075 author:Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A. Smith category:cs.CL cs.LG cs.NE  published:2015-05-29 summary:We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance. version:1
arxiv-1411-7414 | Signal Recovery on Graphs: Variation Minimization | http://arxiv.org/abs/1411.7414 | id:1411.7414 author:Siheng Chen, Aliaksei Sandryhaila, José M. F. Moura, Jelena Kovačević category:cs.SI cs.LG stat.ML  published:2014-11-26 summary:We consider the problem of signal recovery on graphs as graphs model data with complex structure as signals on a graph. Graph signal recovery implies recovery of one or multiple smooth graph signals from noisy, corrupted, or incomplete measurements. We propose a graph signal model and formulate signal recovery as a corresponding optimization problem. We provide a general solution by using the alternating direction methods of multipliers. We next show how signal inpainting, matrix completion, robust principal component analysis, and anomaly detection all relate to graph signal recovery, and provide corresponding specific solutions and theoretical analysis. Finally, we validate the proposed methods on real-world recovery problems, including online blog classification, bridge condition identification, temperature estimation, recommender system, and expert opinion combination of online blog classification. version:3
arxiv-1505-08071 | Geometry of Graph Edit Distance Spaces | http://arxiv.org/abs/1505.08071 | id:1505.08071 author:Brijnesh J. Jain category:cs.CV math.MG  published:2015-05-29 summary:In this paper we study the geometry of graph spaces endowed with a special class of graph edit distances. The focus is on geometrical results useful for statistical pattern recognition. The main result is the Graph Representation Theorem. It states that a graph is a point in some geometrical space, called orbit space. Orbit spaces are well investigated and easier to explore than the original graph space. We derive a number of geometrical results from the orbit space representation, translate them to the graph space, and indicate their significance and usefulness in statistical pattern recognition. version:1
arxiv-1505-08070 | General Deformations of Point Configurations Viewed By a Pinhole Model Camera | http://arxiv.org/abs/1505.08070 | id:1505.08070 author:Yirmeyahu Kaminski, Mike Werman category:cs.CV math.AG 68W30  14N99  68T45  published:2015-05-29 summary:This paper is a theoretical study of the following Non-Rigid Structure from Motion problem. What can be computed from a monocular view of a parametrically deforming set of points? We treat various variations of this problem for affine and polynomial deformations with calibrated and uncalibrated cameras. We show that in general at least three images with quasi-identical two deformations are needed in order to have a finite set of solutions of the points' structure and calculate some simple examples. version:1
arxiv-1504-05427 | Signal Recovery on Graphs: Random versus Experimentally Designed Sampling | http://arxiv.org/abs/1504.05427 | id:1504.05427 author:Siheng Chen, Rohan Varma, Aarti Singh, Jelena Kovačević category:cs.IT math.IT stat.ML  published:2015-04-21 summary:We study signal recovery on graphs based on two sampling strategies: random sampling and experimentally designed sampling. We propose a new class of smooth graph signals, called approximately bandlimited, which generalizes the bandlimited class and is similar to the globally smooth class. We then propose two recovery strategies based on random sampling and experimentally designed sampling. The proposed recovery strategy based on experimentally designed sampling is similar to the leverage scores used in the matrix approximation. We show that while both strategies are unbiased estimators for the low-frequency components, the convergence rate of experimentally designed sampling is much faster than that of random sampling when a graph is irregular. We validate the proposed recovery strategies on three specific graphs: a ring graph, an Erd\H{o}s-R\'enyi graph, and a star graph. The simulation results support the theoretical analysis. version:2
arxiv-1505-08019 | Research on the fast Fourier transform of image based on GPU | http://arxiv.org/abs/1505.08019 | id:1505.08019 author:Feifei Shen, Zhenjian Song, Congrui Wu, Jiaqi Geng, Qingyun Wang category:cs.MS cs.CV  published:2015-05-29 summary:Study of general purpose computation by GPU (Graphics Processing Unit) can improve the image processing capability of micro-computer system. This paper studies the parallelism of the different stages of decimation in time radix 2 FFT algorithm, designs the butterfly and scramble kernels and implements 2D FFT on GPU. The experiment result demonstrates the validity and advantage over general CPU, especially in the condition of large input size. The approach can also be generalized to other transforms alike. version:1
arxiv-1412-7259 | Unsupervised Feature Learning with C-SVDDNet | http://arxiv.org/abs/1412.7259 | id:1412.7259 author:Dong Wang, Xiaoyang Tan category:cs.CV cs.LG cs.NE  published:2014-12-23 summary:In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers' great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from K-means with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called C-SVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of C-SVDD can be solved very efficiently as a linear programming problem. Additionally, traditional unsupervised feature learning methods usually take an average or sum of local representations to obtain global representation which ignore spatial relationship among them. To use spatial information we propose a global representation with a variant of SIFT descriptor. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as STL-10, MINST, Holiday and Copydays shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods. version:3
arxiv-1505-07934 | Symbolic Segmentation Using Algorithm Selection | http://arxiv.org/abs/1505.07934 | id:1505.07934 author:Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama category:cs.CV  published:2015-05-29 summary:In this paper we present an alternative approach to symbolic segmentation; instead of implementing a new method we approach symbolic segmentation as an algorithm selection problem. That is, let there be $n$ available algorithms for symbolic segmentation, a selection mechanism forms a set of input features and image attributes and selects on a case by case basis the best algorithm. The selection mechanism is demonstrated from within an algorithm framework where the selection is done in a set of various algorithm networks. Two sets of experiments are performed and in both cases we demonstrate that the algorithm selection allows to increase the result of the symbolic segmentation by a considerable amount. version:1
arxiv-1505-07931 | Supervised Fine Tuning for Word Embedding with Integrated Knowledge | http://arxiv.org/abs/1505.07931 | id:1505.07931 author:Xuefeng Yang, Kezhi Mao category:cs.CL  published:2015-05-29 summary:Learning vector representation for words is an important research field which may benefit many natural language processing tasks. Two limitations exist in nearly all available models, which are the bias caused by the context definition and the lack of knowledge utilization. They are difficult to tackle because these algorithms are essentially unsupervised learning approaches. Inspired by deep learning, the authors propose a supervised framework for learning vector representation of words to provide additional supervised fine tuning after unsupervised learning. The framework is knowledge rich approacher and compatible with any numerical vectors word representation. The authors perform both intrinsic evaluation like attributional and relational similarity prediction and extrinsic evaluations like the sentence completion and sentiment analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show that the proposed fine tuning framework may significantly improve the quality of the vector representation of words. version:1
arxiv-1505-07930 | Salient Object Detection via Augmented Hypotheses | http://arxiv.org/abs/1505.07930 | id:1505.07930 author:Tam V. Nguyen, Jose Sepulveda category:cs.CV  published:2015-05-29 summary:In this paper, we propose using \textit{augmented hypotheses} which consider objectness, foreground and compactness for salient object detection. Our algorithm consists of four basic steps. First, our method generates the objectness map via objectness hypotheses. Based on the objectness map, we estimate the foreground margin and compute the corresponding foreground map which prefers the foreground objects. From the objectness map and the foreground map, the compactness map is formed to favor the compact objects. We then derive a saliency measure that produces a pixel-accurate saliency map which uniformly covers the objects of interest and consistently separates fore- and background. We finally evaluate the proposed framework on two challenging datasets, MSRA-1000 and iCoSeg. Our extensive experimental results show that our method outperforms state-of-the-art approaches. version:1
arxiv-1505-07925 | On the Computational Complexity of High-Dimensional Bayesian Variable Selection | http://arxiv.org/abs/1505.07925 | id:1505.07925 author:Yun Yang, Martin J. Wainwright, Michael I. Jordan category:math.ST cs.LG stat.CO stat.ME stat.ML stat.TH  published:2015-05-29 summary:We study the computational complexity of Markov chain Monte Carlo (MCMC) methods for high-dimensional Bayesian linear regression under sparsity constraints. We first show that a Bayesian approach can achieve variable-selection consistency under relatively mild conditions on the design matrix. We then demonstrate that the statistical criterion of posterior concentration need not imply the computational desideratum of rapid mixing of the MCMC algorithm. By introducing a truncated sparsity prior for variable selection, we provide a set of conditions that guarantee both variable-selection consistency and rapid mixing of a particular Metropolis-Hastings algorithm. The mixing time is linear in the number of covariates up to a logarithmic factor. Our proof controls the spectral gap of the Markov chain by constructing a canonical path ensemble that is inspired by the steps taken by greedy algorithms for variable selection. version:1
arxiv-1505-07923 | Fast Computation of PERCLOS and Saccadic Ratio | http://arxiv.org/abs/1505.07923 | id:1505.07923 author:Anirban Dasgupta, Aurobinda Routray category:cs.CV  published:2015-05-29 summary:This thesis describes the development of fast algorithms for the computation of PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR). PERCLOS and SR are two ocular parameters reported to be measures of alertness levels in human beings. PERCLOS is the percentage of time in which at least 80% of the eyelid remains closed over the pupil. Saccades are fast and simultaneous movement of both the eyes in the same direction. SR is the ratio of peak saccadic velocity to the saccadic duration. This thesis addresses the issues of image based estimation of PERCLOS and SR, prevailing in the literature such as illumination variation, poor illumination conditions, head rotations etc. In this work, algorithms for real-time PERCLOS computation has been developed and implemented on an embedded platform. The platform has been used as a case study for assessment of loss of attention in automotive drivers. The SR estimation has been carried out offline as real-time implementation requires high frame rates of processing which is difficult to achieve due to hardware limitations. The accuracy in estimation of the loss of attention using PERCLOS and SR has been validated using brain signals, which are reported to be an authentic cue for estimating the state of alertness in human beings. The major contributions of this thesis include database creation, design and implementation of fast algorithms for estimating PERCLOS and SR on embedded computing platforms. version:1
arxiv-1505-07922 | Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network | http://arxiv.org/abs/1505.07922 | id:1505.07922 author:Junshi Huang, Rogerio S. Feris, Qiang Chen, Shuicheng Yan category:cs.CV  published:2015-05-29 summary:We address the problem of cross-domain image retrieval, considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. This is a challenging problem due to the large discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. To address this problem, we propose a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning. More specifically, DARN consists of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. We show that this attribute-guided learning is a key factor for retrieval accuracy improvement. In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two sub-networks. Another contribution of our work is a large-scale dataset which makes the network learning feasible. We exploit customer review websites to crawl a large set of online shopping images and corresponding offline user photos with fine-grained clothing attributes, i.e., around 450,000 online shopping images and about 90,000 exact offline counterpart images of those online ones. All these images are collected from real-world consumer websites reflecting the diversity of the data modality, which makes this dataset unique and rare in the academic community. We extensively evaluate the retrieval performance of networks in different configurations. The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268). version:1
arxiv-1412-8063 | Coordinate Descent with Arbitrary Sampling II: Expected Separable Overapproximation | http://arxiv.org/abs/1412.8063 | id:1412.8063 author:Zheng Qu, Peter Richtárik category:math.OC cs.LG cs.NA math.NA math.PR  published:2014-12-27 summary:The design and complexity analysis of randomized coordinate descent methods, and in particular of variants which update a random subset (sampling) of coordinates in each iteration, depends on the notion of expected separable overapproximation (ESO). This refers to an inequality involving the objective function and the sampling, capturing in a compact way certain smoothness properties of the function in a random subspace spanned by the sampled coordinates. ESO inequalities were previously established for special classes of samplings only, almost invariably for uniform samplings. In this paper we develop a systematic technique for deriving these inequalities for a large class of functions and for arbitrary samplings. We demonstrate that one can recover existing ESO results using our general approach, which is based on the study of eigenvalues associated with samplings and the data describing the function. version:2
arxiv-1505-06750 | Reply to Garcia et al.: Common mistakes in measuring frequency dependent word characteristics | http://arxiv.org/abs/1505.06750 | id:1505.06750 author:P. S. Dodds, E. M. Clark, S. Desu, M. R. Frank, A. J. Reagan, J. R. Williams, L. Mitchell, K. D. Harris, I. M. Kloumann, J. P. Bagrow, K. Megerdoomian, M. T. McMahon, B. F. Tivnan, C. M. Danforth category:physics.soc-ph cs.CL  published:2015-05-25 summary:We demonstrate that the concerns expressed by Garcia et al. are misplaced, due to (1) a misreading of our findings in [1]; (2) a widespread failure to examine and present words in support of asserted summary quantities based on word usage frequencies; and (3) a range of misconceptions about word usage frequency, word rank, and expert-constructed word lists. In particular, we show that the English component of our study compares well statistically with two related surveys, that no survey design influence is apparent, and that estimates of measurement error do not explain the positivity biases reported in our work and that of others. We further demonstrate that for the frequency dependence of positivity---of which we explored the nuances in great detail in [1]---Garcia et al. did not perform a reanalysis of our data---they instead carried out an analysis of a different, statistically improper data set and introduced a nonlinearity before performing linear regression. version:2
arxiv-1505-07778 | Query by String word spotting based on character bi-gram indexing | http://arxiv.org/abs/1505.07778 | id:1505.07778 author:Suman K. Ghosh, Ernest Valveny category:cs.CV  published:2015-05-28 summary:In this paper we propose a segmentation-free query by string word spotting method. Both the documents and query strings are encoded using a recently proposed word representa- tion that projects images and strings into a common atribute space based on a pyramidal histogram of characters(PHOC). These attribute models are learned using linear SVMs over the Fisher Vector representation of the images along with the PHOC labels of the corresponding strings. In order to search through the whole page, document regions are indexed per character bi- gram using a similar attribute representation. On top of that, we propose an integral image representation of the document using a simplified version of the attribute model for efficient computation. Finally we introduce a re-ranking step in order to boost retrieval performance. We show state-of-the-art results for segmentation-free query by string word spotting in single-writer and multi-writer standard datasets version:1
arxiv-1407-0623 | A Data-Driven Approach for Tag Refinement and Localization in Web Videos | http://arxiv.org/abs/1407.0623 | id:1407.0623 author:Lamberto Ballan, Marco Bertini, Giuseppe Serra, Alberto Del Bimbo category:cs.CV cs.IR cs.MM  published:2014-07-02 summary:Tagging of visual content is becoming more and more widespread as web-based services and social networks have popularized tagging functionalities among their users. These user-generated tags are used to ease browsing and exploration of media collections, e.g. using tag clouds, or to retrieve multimedia content. However, not all media are equally tagged by users. Using the current systems is easy to tag a single photo, and even tagging a part of a photo, like a face, has become common in sites like Flickr and Facebook. On the other hand, tagging a video sequence is more complicated and time consuming, so that users just tag the overall content of a video. In this paper we present a method for automatic video annotation that increases the number of tags originally provided by users, and localizes them temporally, associating tags to keyframes. Our approach exploits collective knowledge embedded in user-generated tags and web sources, and visual similarity of keyframes and images uploaded to social sites like YouTube and Flickr, as well as web sources like Google and Bing. Given a keyframe, our method is able to select on the fly from these visual sources the training exemplars that should be the most relevant for this test sample, and proceeds to transfer labels across similar images. Compared to existing video tagging approaches that require training classifiers for each tag, our system has few parameters, is easy to implement and can deal with an open vocabulary scenario. We demonstrate the approach on tag refinement and localization on DUT-WEBV, a large dataset of web videos, and show state-of-the-art results. version:3
arxiv-1505-02137 | Human Social Interaction Modeling Using Temporal Deep Networks | http://arxiv.org/abs/1505.02137 | id:1505.02137 author:Mohamed R. Amer, Behjat Siddiquie, Amir Tamrakar, David A. Salter, Brian Lande, Darius Mehri, Ajay Divakaran category:cs.CY cs.LG  published:2015-05-06 summary:We present a novel approach to computational modeling of social interactions based on modeling of essential social interaction predicates (ESIPs) such as joint attention and entrainment. Based on sound social psychological theory and methodology, we collect a new "Tower Game" dataset consisting of audio-visual capture of dyadic interactions labeled with the ESIPs. We expect this dataset to provide a new avenue for research in computational social interaction modeling. We propose a novel joint Discriminative Conditional Restricted Boltzmann Machine (DCRBM) model that combines a discriminative component with the generative power of CRBMs. Such a combination enables us to uncover actionable constituents of the ESIPs in two steps. First, we train the DCRBM model on the labeled data and get accurate (76\%-49\% across various ESIPs) detection of the predicates. Second, we exploit the generative capability of DCRBMs to activate the trained model so as to generate the lower-level data corresponding to the specific ESIP that closely matches the actual training data (with mean square error 0.01-0.1 for generating 100 frames). We are thus able to decompose the ESIPs into their constituent actionable behaviors. Such a purely computational determination of how to establish an ESIP such as engagement is unprecedented. version:2
arxiv-1505-07712 | A Category Theory of Communication Theory | http://arxiv.org/abs/1505.07712 | id:1505.07712 author:Eric Werner category:cs.IT cs.CL cs.LO math.IT  published:2015-05-28 summary:A theory of how agents can come to understand a language is presented. If understanding a sentence $\alpha$ is to associate an operator with $\alpha$ that transforms the representational state of the agent as intended by the sender, then coming to know a language involves coming to know the operators that correspond to the meaning of any sentence. This involves a higher order operator that operates on the possible transformations that operate on the representational capacity of the agent. We formalize these constructs using concepts and diagrams analogous to category theory. version:1
arxiv-1505-07690 | Invertible Orientation Scores of 3D Images | http://arxiv.org/abs/1505.07690 | id:1505.07690 author:Michiel Janssen, Remco Duits, Marcel Breeuwer category:math.NA cs.CV  published:2015-05-28 summary:The enhancement and detection of elongated structures in noisy image data is relevant for many biomedical applications. To handle complex crossing structures in 2D images, 2D orientation scores were introduced, which already showed their use in a variety of applications. Here we extend this work to 3D orientation scores. First, we construct the orientation score from a given dataset, which is achieved by an invertible coherent state type of transform. For this transformation we introduce 3D versions of the 2D cake-wavelets, which are complex wavelets that can simultaneously detect oriented structures and oriented edges. For efficient implementation of the different steps in the wavelet creation we use a spherical harmonic transform. Finally, we show some first results of practical applications of 3D orientation scores. version:1
arxiv-1505-07675 | Improved Deep Convolutional Neural Network For Online Handwritten Chinese Character Recognition using Domain-Specific Knowledge | http://arxiv.org/abs/1505.07675 | id:1505.07675 author:Weixin Yang, Lianwen Jin, Zecheng Xie, Ziyong Feng category:cs.CV  published:2015-05-28 summary:Deep convolutional neural networks (DCNNs) have achieved great success in various computer vision and pattern recognition applications, including those for handwritten Chinese character recognition (HCCR). However, most current DCNN-based HCCR approaches treat the handwritten sample simply as an image bitmap, ignoring some vital domain-specific information that may be useful but that cannot be learnt by traditional networks. In this paper, we propose an enhancement of the DCNN approach to online HCCR by incorporating a variety of domain-specific knowledge, including deformation, non-linear normalization, imaginary strokes, path signature, and 8-directional features. Our contribution is twofold. First, these domain-specific technologies are investigated and integrated with a DCNN to form a composite network to achieve improved performance. Second, the resulting DCNNs with diversity in their domain knowledge are combined using a hybrid serial-parallel (HSP) strategy. Consequently, we achieve a promising accuracy of 97.20% and 96.87% on CASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the best results previously reported in the literature. version:1
arxiv-1505-07672 | A Generative Model of Natural Texture Surrogates | http://arxiv.org/abs/1505.07672 | id:1505.07672 author:Niklas Ludtke, Debapriya Das, Lucas Theis, Matthias Bethge category:cs.CV  published:2015-05-28 summary:Natural images can be viewed as patchworks of different textures, where the local image statistics is roughly stationary within a small neighborhood but otherwise varies from region to region. In order to model this variability, we first applied the parametric texture algorithm of Portilla and Simoncelli to image patches of 64X64 pixels in a large database of natural images such that each image patch is then described by 655 texture parameters which specify certain statistics, such as variances and covariances of wavelet coefficients or coefficient magnitudes within that patch. To model the statistics of these texture parameters, we then developed suitable nonlinear transformations of the parameters that allowed us to fit their joint statistics with a multivariate Gaussian distribution. We find that the first 200 principal components contain more than 99% of the variance and are sufficient to generate textures that are perceptually extremely close to those generated with all 655 components. We demonstrate the usefulness of the model in several ways: (1) We sample ensembles of texture patches that can be directly compared to samples of patches from the natural image database and can to a high degree reproduce their perceptual appearance. (2) We further developed an image compression algorithm which generates surprisingly accurate images at bit rates as low as 0.14 bits/pixel. Finally, (3) We demonstrate how our approach can be used for an efficient and objective evaluation of samples generated with probabilistic models of natural images. version:1
arxiv-1505-07649 | A trust-region method for stochastic variational inference with applications to streaming data | http://arxiv.org/abs/1505.07649 | id:1505.07649 author:Lucas Theis, Matthew D. Hoffman category:stat.ML stat.AP  published:2015-05-28 summary:Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance. version:1
arxiv-1505-07634 | Learning with Symmetric Label Noise: The Importance of Being Unhinged | http://arxiv.org/abs/1505.07634 | id:1505.07634 author:Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson category:cs.LG  published:2015-05-28 summary:Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss. version:1
arxiv-1504-07218 | Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons | http://arxiv.org/abs/1504.07218 | id:1504.07218 author:Yuxin Chen, Changho Suh category:cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH  published:2015-04-27 summary:This paper explores the preference-based top-$K$ rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and one wishes to recover a consistent ordering that emphasizes the top-$K$ ranked items, based on partially revealed preferences. We focus on the Bradley-Terry-Luce (BTL) model that postulates a set of latent preference scores underlying all items, where the odds of paired comparisons depend only on the relative scores of the items involved. We characterize the minimax limits on identifiability of top-$K$ ranked items, in the presence of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimum sample complexity required for reliable top-$K$ ranking scales inversely with the separation measure irrespective of other preference distribution metrics. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called \emph{Spectral MLE}, that returns the indices of the top-$K$ items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an initial score estimate with minimal squared loss (obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item identification under minimal sample complexity. The practical applicability of Spectral MLE is further corroborated by numerical experiments. version:2
arxiv-1505-07522 | Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures | http://arxiv.org/abs/1505.07522 | id:1505.07522 author:Miriam Redi, Daniele Quercia, Lindsay T. Graham, Samuel D. Gosling category:cs.HC cs.CV cs.CY  published:2015-05-28 summary:To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a place's ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state-of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm. version:1
arxiv-1412-6586 | A deep-structured fully-connected random field model for structured inference | http://arxiv.org/abs/1412.6586 | id:1412.6586 author:Alexander Wong, Mohammad Javad Shafiee, Parthipan Siva, Xiao Yu Wang category:stat.ML cs.IT cs.LG math.IT stat.ME  published:2014-12-20 summary:There has been significant interest in the use of fully-connected graphical models and deep-structured graphical models for the purpose of structured inference. However, fully-connected and deep-structured graphical models have been largely explored independently, leaving the unification of these two concepts ripe for exploration. A fundamental challenge with unifying these two types of models is in dealing with computational complexity. In this study, we investigate the feasibility of unifying fully-connected and deep-structured models in a computationally tractable manner for the purpose of structured inference. To accomplish this, we introduce a deep-structured fully-connected random field (DFRF) model that integrates a series of intermediate sparse auto-encoding layers placed between state layers to significantly reduce computational complexity. The problem of image segmentation was used to illustrate the feasibility of using the DFRF for structured inference in a computationally tractable manner. Results in this study show that it is feasible to unify fully-connected and deep-structured models in a computationally tractable manner for solving structured inference problems such as image segmentation. version:3
arxiv-1407-3501 | Robots that can adapt like animals | http://arxiv.org/abs/1407.3501 | id:1407.3501 author:Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret category:cs.RO cs.AI cs.LG cs.NE q-bio.NC  published:2014-07-13 summary:As robots leave the controlled environments of factories to autonomously function in more complex, natural environments, they will have to respond to the inevitable fact that they will become damaged. However, while animals can quickly adapt to a wide variety of injuries, current robots cannot "think outside the box" to find a compensatory behavior when damaged: they are limited to their pre-specified self-sensing abilities, can diagnose only anticipated failure modes, and require a pre-programmed contingency plan for every type of potential damage, an impracticality for complex robots. Here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans. Before deployment, a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors: This map represents the robot's intuitions about what behaviors it can perform and their value. If the robot is damaged, it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage. Experiments reveal successful adaptations for a legged robot injured in five different ways, including damaged, broken, and missing legs, and for a robotic arm with joints broken in 14 different ways. This new technique will enable more robust, effective, autonomous robots, and suggests principles that animals may use to adapt to injury. version:4
arxiv-1404-4888 | Supervised detection of anomalous light-curves in massive astronomical catalogs | http://arxiv.org/abs/1404.4888 | id:1404.4888 author:Isadora Nun, Karim Pichara, Pavlos Protopapas, Dae-Won Kim category:cs.CE astro-ph.IM cs.LG  published:2014-04-18 summary:The development of synoptic sky surveys has led to a massive amount of data for which resources needed for analysis are beyond human capabilities. To process this information and to extract all possible knowledge, machine learning techniques become necessary. Here we present a new method to automatically discover unknown variable objects in large astronomical catalogs. With the aim of taking full advantage of all the information we have about known objects, our method is based on a supervised algorithm. In particular, we train a random forest classifier using known variability classes of objects and obtain votes for each of the objects in the training set. We then model this voting distribution with a Bayesian network and obtain the joint voting distribution among the training objects. Consequently, an unknown object is considered as an outlier insofar it has a low joint probability. Our method is suitable for exploring massive datasets given that the training process is performed offline. We tested our algorithm on 20 millions light-curves from the MACHO catalog and generated a list of anomalous candidates. We divided the candidates into two main classes of outliers: artifacts and intrinsic outliers. Artifacts were principally due to air mass variation, seasonal variation, bad calibration or instrumental errors and were consequently removed from our outlier list and added to the training set. After retraining, we selected about 4000 objects, which we passed to a post analysis stage by perfoming a cross-match with all publicly available catalogs. Within these candidates we identified certain known but rare objects such as eclipsing Cepheids, blue variables, cataclysmic variables and X-ray sources. For some outliers there were no additional information. Among them we identified three unknown variability types and few individual outliers that will be followed up for a deeper analysis. version:3
arxiv-1504-06662 | Compositional Vector Space Models for Knowledge Base Completion | http://arxiv.org/abs/1504.06662 | id:1504.06662 author:Arvind Neelakantan, Benjamin Roth, Andrew McCallum category:cs.CL stat.ML  published:2015-04-24 summary:Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recursive neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%. version:2
arxiv-1411-6970 | Post-acquisition image based compensation for thickness variation in microscopy section series | http://arxiv.org/abs/1411.6970 | id:1411.6970 author:Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld category:cs.CV q-bio.QM stat.AP  published:2014-11-25 summary:Serial section Microscopy is an established method for volumetric anatomy reconstruction. Section series imaged with Electron Microscopy are currently vital for the reconstruction of the synaptic connectivity of entire animal brains such as that of Drosophila melanogaster. The process of removing ultrathin layers from a solid block containing the specimen, however, is a fragile procedure and has limited precision with respect to section thickness. We have developed a method to estimate the relative z-position of each individual section as a function of signal change across the section series. First experiments show promising results on both serial section Transmission Electron Microscopy (ssTEM) data and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) series. We made our solution available as Open Source plugins for the TrakEM2 software and the ImageJ distribution Fiji. version:2
arxiv-1505-07428 | Training a Convolutional Neural Network for Appearance-Invariant Place Recognition | http://arxiv.org/abs/1505.07428 | id:1505.07428 author:Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier Gonzalez-Jimenez category:cs.CV cs.LG cs.RO  published:2015-05-27 summary:Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets. version:1
arxiv-1505-07409 | Improving Spatial Codification in Semantic Segmentation | http://arxiv.org/abs/1505.07409 | id:1505.07409 author:Carles Ventura, Xavier Giró-i-Nieto, Verónica Vilaplana, Kevin McGuinness, Ferran Marqués, Noel E. O'Connor category:cs.CV  published:2015-05-27 summary:This paper explores novel approaches for improving the spatial codification for the pooling of local descriptors to solve the semantic segmentation problem. We propose to partition the image into three regions for each object to be described: Figure, Border and Ground. This partition aims at minimizing the influence of the image context on the object description and vice versa by introducing an intermediate zone around the object contour. Furthermore, we also propose a richer visual descriptor of the object by applying a Spatial Pyramid over the Figure region. Two novel Spatial Pyramid configurations are explored: Cartesian-based and crown-based Spatial Pyramids. We test these approaches with state-of-the-art techniques and show that they improve the Figure-Ground based pooling in the Pascal VOC 2011 and 2012 semantic segmentation challenges. version:1
arxiv-1505-07293 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling | http://arxiv.org/abs/1505.07293 | id:1505.07293 author:Vijay Badrinarayanan, Ankur Handa, Roberto Cipolla category:cs.CV  published:2015-05-27 summary:We propose a novel deep architecture, SegNet, for semantic pixel wise image labelling. SegNet has several attractive properties; (i) it only requires forward evaluation of a fully learnt function to obtain smooth label predictions, (ii) with increasing depth, a larger context is considered for pixel labelling which improves accuracy, and (iii) it is easy to visualise the effect of feature activation(s) in the pixel label space at any depth. SegNet is composed of a stack of encoders followed by a corresponding decoder stack which feeds into a soft-max classification layer. The decoders help map low resolution feature maps at the output of the encoder stack to full input image size feature maps. This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling. These methods lack a mechanism to map deep layer feature maps to input dimensions. They resort to ad hoc methods to upsample features, e.g. by replication. This results in noisy predictions and also restricts the number of pooling layers in order to avoid too much upsampling and thus reduces spatial context. SegNet overcomes these problems by learning to map encoder outputs to image pixel labels. We test the performance of SegNet on outdoor RGB scenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results show that SegNet achieves state-of-the-art performance even without use of additional cues such as depth, video frames or post-processing with CRF models. version:1
arxiv-1406-3852 | A low variance consistent test of relative dependency | http://arxiv.org/abs/1406.3852 | id:1406.3852 author:Wacha Bounliphone, Arthur Gretton, Arthur Tenenhaus, Matthew Blaschko category:stat.ML cs.LG stat.CO  published:2014-06-15 summary:We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep. version:3
arxiv-1407-4610 | Understanding Zipf's law of word frequencies through sample-space collapse in sentence formation | http://arxiv.org/abs/1407.4610 | id:1407.4610 author:Stefan Thurner, Rudolf Hanel, Bo Liu, Bernat Corominas-Murtra category:physics.soc-ph cs.CL  published:2014-07-17 summary:The formation of sentences is a highly structured and history-dependent process. The probability of using a specific word in a sentence strongly depends on the 'history' of word-usage earlier in that sentence. We study a simple history-dependent model of text generation assuming that the sample-space of word usage reduces along sentence formation, on average. We first show that the model explains the approximate Zipf law found in word frequencies as a direct consequence of sample-space reduction. We then empirically quantify the amount of sample-space reduction in the sentences of ten famous English books, by analysis of corresponding word-transition tables that capture which words can follow any given word in a text. We find a highly nested structure in these transition tables and show that this `nestedness' is tightly related to the power law exponents of the observed word frequency distributions. With the proposed model it is possible to understand that the nestedness of a text can be the origin of the actual scaling exponent, and that deviations from the exact Zipf law can be understood by variations of the degree of nestedness on a book-by-book basis. On a theoretical level we are able to show that in case of weak nesting, Zipf's law breaks down in a fast transition. Unlike previous attempts to understand Zipf's law in language the sample-space reducing model is not based on assumptions of multiplicative, preferential, or self-organised critical mechanisms behind language formation, but simply used the empirically quantifiable parameter 'nestedness' to understand the statistics of word frequencies. version:2
arxiv-1505-04252 | Global Convergence of Unmodified 3-Block ADMM for a Class of Convex Minimization Problems | http://arxiv.org/abs/1505.04252 | id:1505.04252 author:Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC cs.LG stat.ML  published:2015-05-16 summary:The alternating direction method of multipliers (ADMM) has been successfully applied to solve structured convex optimization problems due to its superior practical performance. The convergence properties of the 2-block ADMM have been studied extensively in the literature. Specifically, it has been proven that the 2-block ADMM globally converges for any penalty parameter $\gamma>0$. In this sense, the 2-block ADMM allows the parameter to be free, i.e., there is no need to restrict the value for the parameter when implementing this algorithm in order to ensure convergence. However, for the 3-block ADMM, Chen et al. recently constructed a counter-example showing that it can diverge if no further condition is imposed. The existing results on studying further sufficient conditions on guaranteeing the convergence of the 3-block ADMM usually require $\gamma$ to be smaller than a certain bound, which is usually either difficult to compute or too small to make it a practical algorithm. In this paper, we show that the 3-block ADMM still globally converges with any penalty parameter $\gamma>0$ when applied to solve a class of commonly encountered problems to be called regularized least squares decomposition (RLSD) in this paper, which covers many important applications in practice. version:3
arxiv-1412-5335 | Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews | http://arxiv.org/abs/1412.5335 | id:1412.5335 author:Grégoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio category:cs.CL cs.IR cs.LG cs.NE  published:2014-12-17 summary:Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative lan- guage models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort. version:7
arxiv-1505-07203 | New characterizations of minimum spanning trees and of saliency maps based on quasi-flat zones | http://arxiv.org/abs/1505.07203 | id:1505.07203 author:Jean Cousty, Laurent Najman, Yukiko Kenmochi, Silvio GuimarÃ£es category:cs.CV cs.DS  published:2015-05-27 summary:We study three representations of hierarchies of partitions: dendrograms (direct representations), saliency maps, and minimum spanning trees. We provide a new bijection between saliency maps and hierarchies based on quasi-flat zones as used in image processing and characterize saliency maps and minimum spanning trees as solutions to constrained minimization problems where the constraint is quasi-flat zones preservation. In practice, these results form a toolkit for new hierarchical methods where one can choose the most convenient representation. They also invite us to process non-image data with morphological hierarchies. version:1
arxiv-1412-2032 | On using the Microsoft Kinect$^{\rm TM}$ sensors in the analysis of human motion | http://arxiv.org/abs/1412.2032 | id:1412.2032 author:M. J. Malinowski, E. Matsinos, S. Roth category:physics.med-ph cs.CV cs.RO  published:2014-12-04 summary:The present paper aims at providing the theoretical background required for investigating the use of the Microsoft Kinect$^{\rm TM}$ (`Kinect', for short) sensors (original and upgraded) in the analysis of human motion. Our methodology is developed in such a way that its application be easily adaptable to comparative studies of other systems used in capturing human-motion data. Our future plans include the application of this methodology to two situations: first, in a comparative study of the performance of the two Kinect sensors; second, in pursuing their validation on the basis of comparisons with a marker-based system (MBS). One important feature in our approach is the transformation of the MBS output into Kinect-output format, thus enabling the analysis of the measurements, obtained from different systems, with the same software application, i.e., the one we use in the analysis of Kinect-captured data; one example of such a transformation, for one popular marker-placement scheme (`Plug-in Gait'), is detailed. We propose that the similarity of the output, obtained from the different systems, be assessed on the basis of the comparison of a number of waveforms, representing the variation within the gait cycle of quantities which are commonly used in the modelling of the human motion. The data acquisition may involve commercially-available treadmills and a number of velocity settings: for instance, walking-motion data may be acquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommend that particular attention be called to systematic effects associated with the subject's knee and lower leg, as well as to the ability of the Kinect sensors in reliably capturing the details in the asymmetry of the motion for the left and right parts of the human body. The previous versions of the study have been withdrawn due to the use of a non-representative database. version:4
arxiv-1502-02791 | Learning Transferable Features with Deep Adaptation Networks | http://arxiv.org/abs/1502.02791 | id:1502.02791 author:Mingsheng Long, Yue Cao, Jianmin Wang, Michael I. Jordan category:cs.LG  published:2015-02-10 summary:Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks. version:2
arxiv-1505-07192 | Inner and Inter Label Propagation: Salient Object Detection in the Wild | http://arxiv.org/abs/1505.07192 | id:1505.07192 author:Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price category:cs.CV  published:2015-05-27 summary:In this paper, we propose a novel label propagation based method for saliency detection. A key observation is that saliency in an image can be estimated by propagating the labels extracted from the most certain background and object regions. For most natural images, some boundary superpixels serve as the background labels and the saliency of other superpixels are determined by ranking their similarities to the boundary labels based on an inner propagation scheme. For images of complex scenes, we further deploy a 3-cue-center-biased objectness measure to pick out and propagate foreground labels. A co-transduction algorithm is devised to fuse both boundary and objectness labels based on an inter propagation scheme. The compactness criterion decides whether the incorporation of objectness labels is necessary, thus greatly enhancing computational efficiency. Results on five benchmark datasets with pixel-wise accurate annotations show that the proposed method achieves superior performance compared with the newest state-of-the-arts in terms of different evaluation metrics. version:1
arxiv-1505-07184 | Unsupervised Cross-Domain Word Representation Learning | http://arxiv.org/abs/1505.07184 | id:1505.07184 author:Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL  published:2015-05-27 summary:Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of \emph{source}-\emph{target} domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domain-specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as \emph{pivots}. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-the-art domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset. version:1
arxiv-1504-04343 | Caffe con Troll: Shallow Ideas to Speed Up Deep Learning | http://arxiv.org/abs/1504.04343 | id:1504.04343 author:Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher Ré category:cs.LG cs.CV stat.ML  published:2015-04-16 summary:We present Caffe con Troll (CcT), a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals. We built CcT to examine the performance characteristics of training and deploying general-purpose convolutional neural networks across different hardware architectures. We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet. Moreover, with these improvements, the end-to-end training time for CNNs is directly proportional to the FLOPS delivered by the CPU, which enables us to efficiently train hybrid CPU-GPU systems for CNNs. version:2
arxiv-1505-07067 | Belief Flows of Robust Online Learning | http://arxiv.org/abs/1505.07067 | id:1505.07067 author:Pedro A. Ortega, Koby Crammer, Daniel D. Lee category:stat.ML cs.LG  published:2015-05-26 summary:This paper introduces a new probabilistic model for online learning which dynamically incorporates information from stochastic gradients of an arbitrary loss function. Similar to probabilistic filtering, the model maintains a Gaussian belief over the optimal weight parameters. Unlike traditional Bayesian updates, the model incorporates a small number of gradient evaluations at locations chosen using Thompson sampling, making it computationally tractable. The belief is then transformed via a linear flow field which optimally updates the belief distribution using rules derived from information theoretic principles. Several versions of the algorithm are shown using different constraints on the flow field and compared with conventional online learning algorithms. Results are given for several classification tasks including logistic regression and multilayer neural networks. version:1
arxiv-1503-02318 | Understanding Image Virality | http://arxiv.org/abs/1503.02318 | id:1503.02318 author:Arturo Deza, Devi Parikh category:cs.SI cs.CV  published:2015-03-08 summary:Virality of online content on social networking websites is an important but esoteric phenomenon often studied in fields like marketing, psychology and data mining. In this paper we study viral images from a computer vision perspective. We introduce three new image datasets from Reddit, and define a virality score using Reddit metadata. We train classifiers with state-of-the-art image features to predict virality of individual images, relative virality in pairs of images, and the dominant topic of a viral image. We also compare machine performance to human performance on these tasks. We find that computers perform poorly with low level features, and high level information is critical for predicting virality. We encode semantic information through relative attributes. We identify the 5 key visual attributes that correlate with virality. We create an attribute-based characterization of images that can predict relative virality with 68.10% accuracy (SVM+Deep Relative Attributes) -- better than humans at 60.12%. Finally, we study how human prediction of image virality varies with different `contexts' in which the images are viewed, such as the influence of neighbouring images, images recently viewed, as well as the image title or caption. This work is a first step in understanding the complex but important phenomenon of image virality. Our datasets and annotations will be made publicly available. version:3
arxiv-1505-07008 | An Overview of the Asymptotic Performance of the Family of the FastICA Algorithms | http://arxiv.org/abs/1505.07008 | id:1505.07008 author:Tianwen Wei category:stat.ML cs.LG  published:2015-05-26 summary:This contribution summarizes the results on the asymptotic performance of several variants of the FastICA algorithm. A number of new closed-form expressions are presented. version:1
arxiv-1505-06999 | Some Open Problems in Optimal AdaBoost and Decision Stumps | http://arxiv.org/abs/1505.06999 | id:1505.06999 author:Joshua Belanich, Luis E. Ortiz category:cs.LG stat.ML  published:2015-05-26 summary:The significance of the study of the theoretical and practical properties of AdaBoost is unquestionable, given its simplicity, wide practical use, and effectiveness on real-world datasets. Here we present a few open problems regarding the behavior of "Optimal AdaBoost," a term coined by Rudin, Daubechies, and Schapire in 2004 to label the simple version of the standard AdaBoost algorithm in which the weak learner that AdaBoost uses always outputs the weak classifier with lowest weighted error among the respective hypothesis class of weak classifiers implicit in the weak learner. We concentrate on the standard, "vanilla" version of Optimal AdaBoost for binary classification that results from using an exponential-loss upper bound on the misclassification training error. We present two types of open problems. One deals with general weak hypotheses. The other deals with the particular case of decision stumps, as often and commonly used in practice. Answers to the open problems can have immediate significant impact to (1) cementing previously established results on asymptotic convergence properties of Optimal AdaBoost, for finite datasets, which in turn can be the start to any convergence-rate analysis; (2) understanding the weak-hypotheses class of effective decision stumps generated from data, which we have empirically observed to be significantly smaller than the typically obtained class, as well as the effect on the weak learner's running time and previously established improved bounds on the generalization performance of Optimal AdaBoost classifiers; and (3) shedding some light on the "self control" that AdaBoost tends to exhibit in practice. version:1
arxiv-1505-06957 | Sequential Dimensionality Reduction for Extracting Localized Features | http://arxiv.org/abs/1505.06957 | id:1505.06957 author:Gabriella Casalino, Nicolas Gillis category:cs.CV cs.LG cs.NA math.NA stat.ML  published:2015-05-26 summary:Linear dimensionality reduction techniques are powerful tools for image analysis as they allow the identification of important features in a data set. In particular, nonnegative matrix factorization (NMF) has become very popular as it is able to extract sparse, localized and easily interpretable features by imposing an additive combination of nonnegative basis elements. Nonnegative matrix underapproximation (NMU) is a closely related technique that has the advantage to identify features sequentially. In this paper, we propose a variant of NMU that is particularly well suited for image analysis as it incorporates the spatial information, that is, it takes into account the fact that neighboring pixels are more likely to be contained in the same features, and favors the extraction of localized features by looking for sparse basis elements. We show that our new approach competes favorably with comparable state-of-the-art techniques on several facial and hyperspectral image data sets. version:1
arxiv-1505-07343 | Approximate Joint Diagonalization and Geometric Mean of Symmetric Positive Definite Matrices | http://arxiv.org/abs/1505.07343 | id:1505.07343 author:Marco Congedo, Bijan Afsari, Alexandre Barachant, Maher Moakher category:math.DG stat.ML  published:2015-05-26 summary:We explore the connection between two problems that have arisen independently in the signal processing and related fields: the estimation of the geometric mean of a set of symmetric positive definite (SPD) matrices and their approximate joint diagonalization (AJD). Today there is a considerable interest in estimating the geometric mean of a SPD matrix set in the manifold of SPD matrices endowed with the Fisher information metric. The resulting mean has several important invariance properties and has proven very useful in diverse engineering applications such as biomedical and image data processing. While for two SPD matrices the mean has an algebraic closed form solution, for a set of more than two SPD matrices it can only be estimated by iterative algorithms. However, none of the existing iterative algorithms feature at the same time fast convergence, low computational complexity per iteration and guarantee of convergence. For this reason, recently other definitions of geometric mean based on symmetric divergence measures, such as the Bhattacharyya divergence, have been considered. The resulting means, although possibly useful in practice, do not satisfy all desirable invariance properties. In this paper we consider geometric means of co-variance matrices estimated on high-dimensional time-series, assuming that the data is generated according to an instantaneous mixing model, which is very common in signal processing. We show that in these circumstances we can approximate the Fisher information geometric mean by employing an efficient AJD algorithm. Our approximation is in general much closer to the Fisher information geometric mean as compared to its competitors and verifies many invariance properties. Furthermore, convergence is guaranteed, the computational complexity is low and the convergence rate is quadratic. The accuracy of this new geometric mean approximation is demonstrated by means of simulations. version:1
arxiv-1505-06918 | Fantasy Football Prediction | http://arxiv.org/abs/1505.06918 | id:1505.06918 author:Roman Lutz category:cs.LG I.2.6  published:2015-05-26 summary:The ubiquity of professional sports and specifically the NFL have lead to an increase in popularity for Fantasy Football. Users have many tools at their disposal: statistics, predictions, rankings of experts and even recommendations of peers. There are issues with all of these, though. Especially since many people pay money to play, the prediction tools should be enhanced as they provide unbiased and easy-to-use assistance for users. This paper provides and discusses approaches to predict Fantasy Football scores of Quarterbacks with relatively limited data. In addition to that, it includes several suggestions on how the data could be enhanced to achieve better results. The dataset consists only of game data from the last six NFL seasons. I used two different methods to predict the Fantasy Football scores of NFL players: Support Vector Regression (SVR) and Neural Networks. The results of both are promising given the limited data that was used. version:1
arxiv-1505-06915 | Large-scale Machine Learning for Metagenomics Sequence Classification | http://arxiv.org/abs/1505.06915 | id:1505.06915 author:Kévin Vervier, Pierre Mahé, Maud Tournoud, Jean-Baptiste Veyrieras, Jean-Philippe Vert category:q-bio.QM cs.CE cs.LG q-bio.GN stat.ML  published:2015-05-26 summary:Metagenomics characterizes the taxonomic diversity of microbial communities by sequencing DNA directly from an environmental sample. One of the main challenges in metagenomics data analysis is the binning step, where each sequenced read is assigned to a taxonomic clade. Due to the large volume of metagenomics datasets, binning methods need fast and accurate algorithms that can operate with reasonable computing requirements. While standard alignment-based methods provide state-of-the-art performance, compositional approaches that assign a taxonomic class to a DNA read based on the k-mers it contains have the potential to provide faster solutions. In this work, we investigate the potential of modern, large-scale machine learning implementations for taxonomic affectation of next-generation sequencing reads based on their k-mers profile. We show that machine learning-based compositional approaches benefit from increasing the number of fragments sampled from reference genome to tune their parameters, up to a coverage of about 10, and from increasing the k-mer size to about 12. Tuning these models involves training a machine learning model on about 10 8 samples in 10 7 dimensions, which is out of reach of standard soft-wares but can be done efficiently with modern implementations for large-scale machine learning. The resulting models are competitive in terms of accuracy with well-established alignment tools for problems involving a small to moderate number of candidate species, and for reasonable amounts of sequencing errors. We show, however, that compositional approaches are still limited in their ability to deal with problems involving a greater number of species, and more sensitive to sequencing errors. We finally confirm that compositional approach achieve faster prediction times, with a gain of 3 to 15 times with respect to the BWA-MEM short read mapper, depending on the number of candidate species and the level of sequencing noise. version:1
arxiv-1505-06907 | Using Dimension Reduction to Improve the Classification of High-dimensional Data | http://arxiv.org/abs/1505.06907 | id:1505.06907 author:Andreas Grünauer, Markus Vincze category:cs.LG cs.CV  published:2015-05-26 summary:In this work we show that the classification performance of high-dimensional structural MRI data with only a small set of training examples is improved by the usage of dimension reduction methods. We assessed two different dimension reduction variants: feature selection by ANOVA F-test and feature transformation by PCA. On the reduced datasets, we applied common learning algorithms using 5-fold cross-validation. Training, tuning of the hyperparameters, as well as the performance evaluation of the classifiers was conducted using two different performance measures: Accuracy, and Receiver Operating Characteristic curve (AUC). Our hypothesis is supported by experimental results. version:1
arxiv-1502-03696 | Monte Carlo Planning method estimates planning horizons during interactive social exchange | http://arxiv.org/abs/1502.03696 | id:1502.03696 author:Andreas Hula, P. Read Montague, Peter Dayan category:stat.ML  published:2015-02-12 summary:Reciprocating interactions represent a central feature of all human exchanges. They have been the target of various recent experiments, with healthy participants and psychiatric populations engaging as dyads in multi-round exchanges such as a repeated trust task. Behaviour in such exchanges involves complexities related to each agent's preference for equity with their partner, beliefs about the partner's appetite for equity, beliefs about the partner's model of their partner, and so on. Agents may also plan different numbers of steps into the future. Providing a computationally precise account of the behaviour is an essential step towards understanding what underlies choices. A natural framework for this is that of an interactive partially observable Markov decision process (IPOMDP). However, the various complexities make IPOMDPs inordinately computationally challenging. Here, we show how to approximate the solution for the multi-round trust task using a variant of the Monte-Carlo tree search algorithm. We demonstrate that the algorithm is efficient and effective, and therefore can be used to invert observations of behavioural choices. We use generated behaviour to elucidate the richness and sophistication of interactive inference. version:7
arxiv-1505-06814 | Discrete Independent Component Analysis (DICA) with Belief Propagation | http://arxiv.org/abs/1505.06814 | id:1505.06814 author:Francesco A. N. Palmieri, Amedeo Buonanno category:cs.CV cs.LG stat.ML  published:2015-05-26 summary:We apply belief propagation to a Bayesian bipartite graph composed of discrete independent hidden variables and discrete visible variables. The network is the Discrete counterpart of Independent Component Analysis (DICA) and it is manipulated in a factor graph form for inference and learning. A full set of simulations is reported for character images from the MNIST dataset. The results show that the factorial code implemented by the sources contributes to build a good generative model for the data that can be used in various inference modes. version:1
arxiv-1505-06813 | Surrogate Functions for Maximizing Precision at the Top | http://arxiv.org/abs/1505.06813 | id:1505.06813 author:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain category:stat.ML cs.LG  published:2015-05-26 summary:The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k. version:1
arxiv-1505-06812 | Optimizing Non-decomposable Performance Measures: A Tale of Two Classes | http://arxiv.org/abs/1505.06812 | id:1505.06812 author:Harikrishna Narasimhan, Purushottam Kar, Prateek Jain category:stat.ML cs.LG  published:2015-05-26 summary:Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data. version:1
arxiv-1505-06807 | MLlib: Machine Learning in Apache Spark | http://arxiv.org/abs/1505.06807 | id:1505.06807 author:Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, Ameet Talwalkar category:cs.LG cs.DC cs.MS stat.ML  published:2015-05-26 summary:Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed. version:1
arxiv-1505-06800 | Boosting-like Deep Learning For Pedestrian Detection | http://arxiv.org/abs/1505.06800 | id:1505.06800 author:Lei Wang, Baochang Zhang category:cs.CV cs.LG cs.NE  published:2015-05-26 summary:This paper proposes boosting-like deep learning (BDL) framework for pedestrian detection. Due to overtraining on the limited training samples, overfitting is a major problem of deep learning. We incorporate a boosting-like technique into deep learning to weigh the training samples, and thus prevent overtraining in the iterative process. We theoretically give the details of derivation of our algorithm, and report the experimental results on open data sets showing that BDL achieves a better stable performance than the state-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the average miss rate compared with ACF and JointDeep on the largest Caltech benchmark dataset, respectively. version:1
arxiv-1409-7556 | Location Recognition Over Large Time Lags | http://arxiv.org/abs/1409.7556 | id:1409.7556 author:Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars category:cs.CV  published:2014-09-26 summary:Would it be possible to automatically associate ancient pictures to modern ones and create fancy cultural heritage city maps? We introduce here the task of recognizing the location depicted in an old photo given modern annotated images collected from the Internet. We present an extensive analysis on different features, looking for the most discriminative and most robust to the image variability induced by large time lags. Moreover, we show that the described task benefits from domain adaptation. version:3
arxiv-1505-06769 | VeinPLUS: A Transillumination and Reflection-based Hand Vein Database | http://arxiv.org/abs/1505.06769 | id:1505.06769 author:Alexander Gruschina category:cs.CV  published:2015-05-25 summary:This paper gives a short summary of work related to the creation of a department-hosted hand vein database. After the introducing section, special properties of the hand vein acquisition are explained, followed by a comparison table, which shows key differences to existing well-known hand vein databases. At the end, the ROI extraction process is described and sample images and ROIs are presented. version:1
arxiv-1405-6791 | Agnostic Learning of Disjunctions on Symmetric Distributions | http://arxiv.org/abs/1405.6791 | id:1405.6791 author:Vitaly Feldman, Pravesh Kothari category:cs.LG cs.CC cs.DS  published:2014-05-27 summary:We consider the problem of approximating and learning disjunctions (or equivalently, conjunctions) on symmetric distributions over $\{0,1\}^n$. Symmetric distributions are distributions whose PDF is invariant under any permutation of the variables. We give a simple proof that for every symmetric distribution $\mathcal{D}$, there exists a set of $n^{O(\log{(1/\epsilon)})}$ functions $\mathcal{S}$, such that for every disjunction $c$, there is function $p$, expressible as a linear combination of functions in $\mathcal{S}$, such that $p$ $\epsilon$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$ or $\mathbf{E}_{x \sim \mathcal{D}}[ c(x)-p(x) ] \leq \epsilon$. This directly gives an agnostic learning algorithm for disjunctions on symmetric distributions that runs in time $n^{O( \log{(1/\epsilon)})}$. The best known previous bound is $n^{O(1/\epsilon^4)}$ and follows from approximation of the more general class of halfspaces (Wimmer, 2010). We also show that there exists a symmetric distribution $\mathcal{D}$, such that the minimum degree of a polynomial that $1/3$-approximates the disjunction of all $n$ variables is $\ell_1$ distance on $\mathcal{D}$ is $\Omega( \sqrt{n})$. Therefore the learning result above cannot be achieved via $\ell_1$-regression with a polynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution $\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of degree $O(\log{(1/\epsilon)})$ such that $p$ $\epsilon$-approximates $c$ in $\ell_1$ distance on $\mathcal{D}$. This was first proved by Blais et al. (2008) via a more involved argument. version:2
arxiv-1505-06723 | Stochastic Annealing for Variational Inference | http://arxiv.org/abs/1505.06723 | id:1505.06723 author:San Gultekin, Aonan Zhang, John Paisley category:stat.ML  published:2015-05-25 summary:We empirically evaluate a stochastic annealing strategy for Bayesian posterior optimization with variational inference. Variational inference is a deterministic approach to approximate posterior inference in Bayesian models in which a typically non-convex objective function is locally optimized over the parameters of the approximating distribution. We investigate an annealing method for optimizing this objective with the aim of finding a better local optimal solution and compare with deterministic annealing methods and no annealing. We show that stochastic annealing can provide clear improvement on the GMM and HMM, while performance on LDA tends to favor deterministic annealing methods. version:1
arxiv-1505-06702 | Smooth and iteratively Restore: A simple and fast edge-preserving smoothing model | http://arxiv.org/abs/1505.06702 | id:1505.06702 author:Philipp Kniefacz, Walter Kropatsch category:cs.CV  published:2015-05-25 summary:In image processing, it can be a useful pre-processing step to smooth away small structures, such as noise or unimportant details, while retaining the overall structure of the image by keeping edges, which separate objects, sharp. Typically this edge-preserving smoothing process is achieved using edge-aware filters. However such filters may preserve unwanted small structures as well if they contain edges. In this work we present a novel framework for edge-preserving smoothing which separates the process into two different steps: First the image is smoothed using a blurring filter and in the second step the important edges are restored using a guided edge-aware filter. The presented method proves to deliver very good results, compared to state-of-the-art edge-preserving smoothing filters, especially at removing unwanted small structures. Furthermore it is very versatile and can easily be adapted to different fields of applications while at the same time being very fast to compute and therefore well-suited for real time applications. version:1
arxiv-1505-06659 | Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares -- ICML | http://arxiv.org/abs/1505.06659 | id:1505.06659 author:Garvesh Raskutti, Michael Mahoney category:stat.ML  published:2015-05-25 summary:We consider statistical and algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. Prior results show that, from an \emph{algorithmic perspective}, when using sketching matrices constructed from random projections and leverage-score sampling, if the number of samples $r$ much smaller than the original sample size $n$, then the worst-case (WC) error is the same as solving the original problem, up to a very small relative error. From a \emph{statistical perspective}, one typically considers the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling algorithms. Among other results, we show that the RE can be upper bounded when $r$ is much smaller than $n$, while the PE typically requires the number of samples $r$ to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE can not be improved. version:1
arxiv-1505-06623 | Recognition Confidence Analysis of Handwritten Chinese Character with CNN | http://arxiv.org/abs/1505.06623 | id:1505.06623 author:Meijun He, Shuye Zhang, Huiyun Mao, Lianwen Jin category:cs.CV  published:2015-05-25 summary:In this paper, we present an effective method to analyze the recognition confidence of handwritten Chinese character, based on the softmax regression score of a high performance convolutional neural networks (CNN). Through careful and thorough statistics of 827,685 testing samples that randomly selected from total 8836 different classes of Chinese characters, we find that the confidence measurement based on CNN is an useful metric to know how reliable the recognition results are. Furthermore, we find by experiments that the recognition confidence can be used to find out similar and confusable character-pairs, to check wrongly or cursively written samples, and even to discover and correct mis-labelled samples. Many interesting observations and statistics are given and analyzed in this study. version:1
arxiv-1505-06621 | Machine learning based data mining for Milky Way filamentary structures reconstruction | http://arxiv.org/abs/1505.06621 | id:1505.06621 author:Giuseppe Riccio, Stefano Cavuoti, Eugenio Schisano, Massimo Brescia, Amata Mercurio, Davide Elia, Milena Benedettini, Stefano Pezzuto, Sergio Molinari, Anna Maria Di Giorgio category:astro-ph.IM cs.CV  published:2015-05-25 summary:We present an innovative method called FilExSeC (Filaments Extraction, Selection and Classification), a data mining tool developed to investigate the possibility to refine and optimize the shape reconstruction of filamentary structures detected with a consolidated method based on the flux derivative analysis, through the column-density maps computed from Herschel infrared Galactic Plane Survey (Hi-GAL) observations of the Galactic plane. The present methodology is based on a feature extraction module followed by a machine learning model (Random Forest) dedicated to select features and to classify the pixels of the input images. From tests on both simulations and real observations the method appears reliable and robust with respect to the variability of shape and distribution of filaments. In the cases of highly defined filament structures, the presented method is able to bridge the gaps among the detected fragments, thus improving their shape reconstruction. From a preliminary "a posteriori" analysis of derived filament physical parameters, the method appears potentially able to add a sufficient contribution to complete and refine the filament reconstruction. version:1
arxiv-1505-06614 | Electre Tri-Machine Learning Approach to the Record Linkage Problem | http://arxiv.org/abs/1505.06614 | id:1505.06614 author:Renato De Leone, Valentina Minnetti category:stat.ML cs.LG  published:2015-05-25 summary:In this short paper, the Electre Tri-Machine Learning Method, generally used to solve ordinal classification problems, is proposed for solving the Record Linkage problem. Preliminary experimental results show that, using the Electre Tri method, high accuracy can be achieved and more than 99% of the matches and nonmatches were correctly identified by the procedure. version:1
arxiv-1505-06600 | Fast Detection of Curved Edges at Low SNR | http://arxiv.org/abs/1505.06600 | id:1505.06600 author:Nati Ofir, Meirav Galun, Boaz Nadler, Ronen Basri category:cs.CV  published:2015-05-25 summary:Detecting edges is a fundamental problem in computer vision with many applications, some involving very noisy images. While most edge detection methods are fast, they perform well only on relatively clean images. Indeed, edges in such images can be reliably detected using only local filters. Detecting faint edges under high levels of noise cannot be done locally at the individual pixel level, and requires more sophisticated global processing. Unfortunately, existing methods that achieve this goal are quite slow. In this paper we develop a novel multiscale method to detect curved edges in noisy images. While our algorithm searches for edges over a huge set of candidate curves, it does so in a practical runtime, nearly linear in the total number of image pixels. As we demonstrate experimentally, our algorithm is orders of magnitude faster than previous methods designed to deal with high noise levels. Nevertheless, it obtains comparable, if not better, edge detection quality on a variety of challenging noisy images. version:1
arxiv-1505-05008 | Boosting Named Entity Recognition with Neural Character Embeddings | http://arxiv.org/abs/1505.05008 | id:1505.05008 author:Cicero Nogueira dos Santos, Victor Guimarães category:cs.CL  published:2015-05-19 summary:Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification. We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in Spanish. Our experimental results shade light on the contribution of neural character embeddings for NER. Moreover, we demonstrate that the same neural network which has been successfully applied to POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyperparameters, and without any handcrafted features. For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes), and by 7.2 points in the F1 for the selective scenario (five NE classes). version:2
arxiv-1505-00077 | Fast and Accurate Bilateral Filtering using Gauss-Polynomial Decomposition | http://arxiv.org/abs/1505.00077 | id:1505.00077 author:Kunal N. Chaudhury category:cs.CV  published:2015-05-01 summary:The bilateral filter is a versatile non-linear filter that has found diverse applications in image processing, computer vision, computer graphics, and computational photography. A widely-used form of the filter is the Gaussian bilateral filter in which both the spatial and range kernels are Gaussian. A direct implementation of this filter requires $O(\sigma^2)$ operations per pixel, where $\sigma$ is the standard deviation of the spatial Gaussian. In this paper, we propose an accurate approximation algorithm that can cut down the computational complexity to $O(1)$ per pixel for any arbitrary $\sigma$ (constant-time implementation). This is based on the observation that the range kernel operates via the translations of a fixed Gaussian over the range space, and that these translated Gaussians can be accurately approximated using the so-called Gauss-polynomials. The overall algorithm emerging from this approximation involves a series of spatial Gaussian filtering, which can be implemented in constant-time using separability and recursion. We present some preliminary results to demonstrate that the proposed algorithm compares favorably with some of the existing fast algorithms in terms of speed and accuracy. version:2
arxiv-1505-00074 | Image Denoising using Optimally Weighted Bilateral Filters: A Sure and Fast Approach | http://arxiv.org/abs/1505.00074 | id:1505.00074 author:Kunal N. Chaudhury, Kollipara Rithwik category:cs.CV  published:2015-05-01 summary:The bilateral filter is known to be quite effective in denoising images corrupted with small dosages of additive Gaussian noise. The denoising performance of the filter, however, is known to degrade quickly with the increase in noise level. Several adaptations of the filter have been proposed in the literature to address this shortcoming, but often at a substantial computational overhead. In this paper, we report a simple pre-processing step that can substantially improve the denoising performance of the bilateral filter, at almost no additional cost. The modified filter is designed to be robust at large noise levels, and often tends to perform poorly below a certain noise threshold. To get the best of the original and the modified filter, we propose to combine them in a weighted fashion, where the weights are chosen to minimize (a surrogate of) the oracle mean-squared-error (MSE). The optimally-weighted filter is thus guaranteed to perform better than either of the component filters in terms of the MSE, at all noise levels. We also provide a fast algorithm for the weighted filtering. Visual and quantitative denoising results on standard test images are reported which demonstrate that the improvement over the original filter is significant both visually and in terms of PSNR. Moreover, the denoising performance of the optimally-weighted bilateral filter is competitive with the computation-intensive non-local means filter. version:2
arxiv-1505-06578 | A Simple Yet Effective Improvement to the Bilateral Filter for Image Denoising | http://arxiv.org/abs/1505.06578 | id:1505.06578 author:Kollipara Rithwik, Kunal Narayan Chaudhury category:cs.CV  published:2015-05-25 summary:The bilateral filter has diverse applications in image processing, computer vision, and computational photography. In particular, this non-linear filter is quite effective in denoising images corrupted with additive Gaussian noise. The filter, however, is known to perform poorly at large noise levels. Several adaptations of the filter have been proposed in the literature to address this shortcoming, but often at an added computational cost. In this paper, we report a simple yet effective modification that improves the denoising performance of the bilateral filter at almost no additional cost. We provide visual and quantitative results on standard test images which show that this improvement is significant both visually and in terms of PSNR and SSIM (often as large as 5 dB). We also demonstrate how the proposed filtering can be implemented at reduced complexity by adapting a recent idea for fast bilateral filtering. version:1
arxiv-1505-06538 | Clustering via Content-Augmented Stochastic Blockmodels | http://arxiv.org/abs/1505.06538 | id:1505.06538 author:J. Massey Cashore, Xiaoting Zhao, Alexander A. Alemi, Yujia Liu, Peter I. Frazier category:stat.ML cs.LG cs.SI  published:2015-05-25 summary:Much of the data being created on the web contains interactions between users and items. Stochastic blockmodels, and other methods for community detection and clustering of bipartite graphs, can infer latent user communities and latent item clusters from this interaction data. These methods, however, typically ignore the items' contents and the information they provide about item clusters, despite the tendency of items in the same latent cluster to share commonalities in content. We introduce content-augmented stochastic blockmodels (CASB), which use item content together with user-item interaction data to enhance the user communities and item clusters learned. Comparisons to several state-of-the-art benchmark methods, on datasets arising from scientists interacting with scientific articles, show that content-augmented stochastic blockmodels provide highly accurate clusters with respect to metrics representative of the underlying community structure. version:1
arxiv-1505-06531 | Affine and Regional Dynamic Time Warpng | http://arxiv.org/abs/1505.06531 | id:1505.06531 author:Tsu-Wei Chen, Meena Abdelmaseeh, Daniel Stashuk category:cs.CV cs.CE cs.LG  published:2015-05-25 summary:Pointwise matches between two time series are of great importance in time series analysis, and dynamic time warping (DTW) is known to provide generally reasonable matches. There are situations where time series alignment should be invariant to scaling and offset in amplitude or where local regions of the considered time series should be strongly reflected in pointwise matches. Two different variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are proposed to handle scaling and offset in amplitude and provide regional emphasis respectively. Furthermore, ADTW and RDTW can be combined in two different ways to generate alignments that incorporate advantages from both methods, where the affine model can be applied either globally to the entire time series or locally to each region. The proposed alignment methods outperform DTW on specific simulated datasets, and one-nearest-neighbor classifiers using their associated difference measures are competitive with the difference measures associated with state-of-the-art alignment methods on real datasets. version:1
arxiv-1503-06680 | Structural Similarity Index SSIMplified: Is there really a simpler concept at the heart of image quality measurement? | http://arxiv.org/abs/1503.06680 | id:1503.06680 author:Kieran Gerard Larkin category:cs.CV  published:2015-01-29 summary:The Structural Similarity Index (SSIM) is generally considered to be a milestone in the recent history of Image Quality Assessment (IQA). Alas, SSIM's accepted development from the product of three heuristic factors continues to obscure it's real underlying simplicity. Starting instead from a symmetric-antisymmetric reformulation we first show SSIM to be a contrast or visibility function in the classic sense. Furthermore, the previously enigmatic structural covariance is revealed to be the difference of variances. The second step, eliminating the intrinsic quadratic nature of SSIM, allows a near linear correlation with human observer scores, and without invoking the usual, but arbitrary, sigmoid model fitting. We conclude that SSIM can be re-interpreted in terms of perceptual masking: it is essentially equivalent to a normalised error or noise visibility function (NVF), and, furthermore, the NVF alone explains it success in modelling perceptual image quality. We use the term Dissimilarity Quotient (DQ) for the specifically anti/symmetric SSIM derived NVF. It seems that IQA researchers may now have two choices: 1) Continue to use the complex SSIM formula, but noting that SSIM only works coincidentally since the covariance term is actually the mean square error (MSE) in disguise. 2) Use the simplest of all perceptually-masked image quality metrics, namely NVF or DQ. On this choice Occam is clear: in the absence of differences in predictive ability, the fewer assumptions that are made, the better. version:2
arxiv-1505-06485 | Constrained 1-Spectral Clustering | http://arxiv.org/abs/1505.06485 | id:1505.06485 author:Syama Sundar Rangapuram, Matthias Hein category:stat.ML cs.LG  published:2015-05-24 summary:An important form of prior information in clustering comes in form of cannot-link and must-link constraints. We present a generalization of the popular spectral clustering technique which integrates such constraints. Motivated by the recently proposed $1$-spectral clustering for the unconstrained problem, our method is based on a tight relaxation of the constrained normalized cut into a continuous optimization problem. Opposite to all other methods which have been suggested for constrained spectral clustering, we can always guarantee to satisfy all constraints. Moreover, our soft formulation allows to optimize a trade-off between normalized cut and the number of violated constraints. An efficient implementation is provided which scales to large datasets. We outperform consistently all other proposed methods in the experiments. version:1
arxiv-1505-06478 | Tight Continuous Relaxation of the Balanced $k$-Cut Problem | http://arxiv.org/abs/1505.06478 | id:1505.06478 author:Syama Sundar Rangapuram, Pramod Kaushik Mudrakarta, Matthias Hein category:stat.ML cs.LG  published:2015-05-24 summary:Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters, corresponding to a balanced $k$-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced $k$-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the difficult sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method outperforms all existing approaches for ratio cut and other balanced $k$-cut criteria. version:1
arxiv-1410-0633 | Deterministic Conditions for Subspace Identifiability from Incomplete Sampling | http://arxiv.org/abs/1410.0633 | id:1410.0633 author:Daniel L. Pimentel-Alarcón, Robert D. Nowak, Nigel Boston category:stat.ML cs.LG math.CO  published:2014-10-02 summary:Consider a generic $r$-dimensional subspace of $\mathbb{R}^d$, $r<d$, and suppose that we are only given projections of this subspace onto small subsets of the canonical coordinates. The paper establishes necessary and sufficient deterministic conditions on the subsets for subspace identifiability. version:3
arxiv-1503-02596 | A Characterization of Deterministic Sampling Patterns for Low-Rank Matrix Completion | http://arxiv.org/abs/1503.02596 | id:1503.02596 author:Daniel L. Pimentel-Alarcón, Nigel Boston, Robert D. Nowak category:stat.ML cs.LG math.AG  published:2015-03-09 summary:Low-rank matrix completion (LRMC) problems arise in a wide variety of applications. Previous theory mainly provides conditions for completion under missing-at-random samplings. An incomplete $d \times N$ matrix is $\textit{finitely completable}$ if there are at most finitely many rank-$r$ matrices that agree with all its observed entries. Finite completability is the tipping point in LRMC, as a few additional samples of a finitely completable matrix guarantee its $\textit{unique}$ completability. The main contribution of this paper is a full characterization of finitely completable observation sets. We use this characterization to derive sufficient deterministic sampling conditions for unique completability. We also show that under uniform random sampling schemes, these conditions are satisfied with high probability if at least $\mathscr{O}(\max\{r,\log d \})$ entries per column are observed. version:2
arxiv-1505-06443 | Detecting bird sound in unknown acoustic background using crowdsourced training data | http://arxiv.org/abs/1505.06443 | id:1505.06443 author:Timos Papadopoulos, Stephen Roberts, Kathy Willis category:stat.ML cs.LG cs.SD  published:2015-05-24 summary:Biodiversity monitoring using audio recordings is achievable at a truly global scale via large-scale deployment of inexpensive, unattended recording stations or by large-scale crowdsourcing using recording and species recognition on mobile devices. The ability, however, to reliably identify vocalising animal species is limited by the fact that acoustic signatures of interest in such recordings are typically embedded in a diverse and complex acoustic background. To avoid the problems associated with modelling such backgrounds, we build generative models of bird sounds and use the concept of novelty detection to screen recordings to detect sections of data which are likely bird vocalisations. We present detection results against various acoustic environments and different signal-to-noise ratios. We discuss the issues related to selecting the cost function and setting detection thresholds in such algorithms. Our methods are designed to be scalable and automatically applicable to arbitrary selections of species depending on the specific geographic region and time period of deployment. version:1
arxiv-1504-06580 | Classifying Relations by Ranking with Convolutional Neural Networks | http://arxiv.org/abs/1504.06580 | id:1504.06580 author:Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou category:cs.CL cs.LG cs.NE  published:2015-04-24 summary:Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. version:2
arxiv-1505-06427 | Deep Speaker Vectors for Semi Text-independent Speaker Verification | http://arxiv.org/abs/1505.06427 | id:1505.06427 author:Lantian Li, Dong Wang, Zhiyong Zhang, Thomas Fang Zheng category:cs.CL cs.LG cs.NE  published:2015-05-24 summary:Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method. This paper extends the d-vector approach to semi text-independent speaker verification tasks, i.e., the text of the speech is in a limited set of short phrases. We explore various settings of the DNN structure used for d-vector extraction, and present a phone-dependent training which employs the posterior features obtained from an ASR system. The experimental results show that it is possible to apply d-vectors on semi text-independent speaker recognition, and the phone-dependent training improves system performance. version:1
arxiv-1409-4689 | Compute Less to Get More: Using ORC to Improve Sparse Filtering | http://arxiv.org/abs/1409.4689 | id:1409.4689 author:Johannes Lederer, Sergio Guadarrama category:cs.CV cs.LG  published:2014-09-16 summary:Sparse Filtering is a popular feature learning algorithm for image classification pipelines. In this paper, we connect the performance of Sparse Filtering with spectral properties of the corresponding feature matrices. This connection provides new insights into Sparse Filtering; in particular, it suggests early stopping of Sparse Filtering. We therefore introduce the Optimal Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We show that this stopping criterion is related with pre-processing procedures such as Statistical Whitening and demonstrate that it can make image classification with Sparse Filtering considerably faster and more accurate. version:2
arxiv-1404-0541 | Don't Fall for Tuning Parameters: Tuning-Free Variable Selection in High Dimensions With the TREX | http://arxiv.org/abs/1404.0541 | id:1404.0541 author:Johannes Lederer, Christian Müller category:stat.ME stat.ML  published:2014-04-02 summary:Lasso is a seminal contribution to high-dimensional statistics, but it hinges on a tuning parameter that is difficult to calibrate in practice. A partial remedy for this problem is Square-Root Lasso, because it inherently calibrates to the noise variance. However, Square-Root Lasso still requires the calibration of a tuning parameter to all other aspects of the model. In this study, we introduce TREX, an alternative to Lasso with an inherent calibration to all aspects of the model. This adaptation to the entire model renders TREX an estimator that does not require any calibration of tuning parameters. We show that TREX can outperform cross-validated Lasso in terms of variable selection and computational efficiency. We also introduce a bootstrapped version of TREX that can further improve variable selection. We illustrate the promising performance of TREX both on synthetic data and on a recent high-dimensional biological data set that considers riboflavin production in B. subtilis. version:3
arxiv-1505-06405 | Domain Adaptation Extreme Learning Machines for Drift Compensation in E-nose Systems | http://arxiv.org/abs/1505.06405 | id:1505.06405 author:Lei Zhang, David Zhang category:cs.LG  published:2015-05-24 summary:This paper addresses an important issue, known as sensor drift that behaves a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly due to the frequent acquisition and labeling process for gases samples recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. However, ELMs primarily focus on the supervised, semi-supervised and unsupervised learning problems in single domain (i.e. source domain). To our best knowledge, ELM with cross-domain learning capability has never been studied. This paper proposes a unified framework, referred to as Domain Adaptation Extreme Learning Machine (DAELM), which learns a robust classifier by leveraging a limited number of labeled data from target domain for drift compensation as well as gases recognition in E-nose systems, without loss of the computational efficiency and learning ability of traditional ELM. In the unified framework, two algorithms called DAELM-S and DAELM-T are proposed for the purpose of this paper, respectively. In order to percept the differences among ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the popular sensor drift data with multiple batches collected by E-nose system clearly demonstrate that the proposed DAELM significantly outperforms existing drift compensation methods without cumbersome measures, and also bring new perspectives for ELM. version:1
arxiv-1505-06353 | The evolutionary origins of hierarchy | http://arxiv.org/abs/1505.06353 | id:1505.06353 author:Henok Mengistu, Joost Huizinga, Jean-Baptiste Mouret, Jeff Clune category:cs.NE  published:2015-05-23 summary:Hierarchical organization -- the recursive composition of sub-modules -- is ubiquitous in biological networks, including neural, metabolic, ecological, and genetic regulatory networks, and in human-made systems, such as large organizations and the Internet. To date, most research on hierarchy in networks has been limited to quantifying this property. However, an open, important question in evolutionary biology is why hierarchical organization evolves in the first place. It has recently been shown that modularity evolves because of the presence of a cost for network connections. Here we investigate whether such connection costs also tend to cause a hierarchical organization of such modules. In computational simulations, we find that networks without a connection cost do not evolve to be hierarchical, even when the task has a hierarchical structure. However, with a connection cost, networks evolve to be both modular and hierarchical, and these networks exhibit higher overall performance and evolvability (i.e. faster adaptation to new environments). Additional analyses confirm that hierarchy independently improves adaptability after controlling for modularity. Overall, our results suggest that the same force--the cost of connections--promotes the evolution of both hierarchy and modularity, and that these properties are important drivers of network performance and adaptability. In addition to shedding light on the emergence of hierarchy across the many domains in which it appears, these findings will also accelerate future research into evolving more complex, intelligent computational brains in the fields of artificial intelligence and robotics. version:1
arxiv-1412-1740 | Image Data Compression for Covariance and Histogram Descriptors | http://arxiv.org/abs/1412.1740 | id:1412.1740 author:Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q. Weinberger category:stat.ML cs.CV cs.LG  published:2014-12-04 summary:Covariance and histogram image descriptors provide an effective way to capture information about images. Both excel when used in combination with special purpose distance metrics. For covariance descriptors these metrics measure the distance along the non-Euclidean Riemannian manifold of symmetric positive definite matrices. For histogram descriptors the Earth Mover's distance measures the optimal transport between two histograms. Although more precise, these distance metrics are very expensive to compute, making them impractical in many applications, even for data sets of only a few thousand examples. In this paper we present two methods to compress the size of covariance and histogram datasets with only marginal increases in test error for k-nearest neighbor classification. Specifically, we show that we can reduce data sets to 16% and in some cases as little as 2% of their original size, while approximately matching the test error of kNN classification on the full training set. In fact, because the compressed set is learned in a supervised fashion, it sometimes even outperforms the full data set, while requiring only a fraction of the space and drastically reducing test-time computation. version:2
arxiv-1505-06319 | The Minimum Spanning Tree of Maximum Entropy | http://arxiv.org/abs/1505.06319 | id:1505.06319 author:Samuel de Sousa, Walter G. Kropatsch category:cs.CV  published:2015-05-23 summary:In computer vision, we have the problem of creating graphs out of unstructured point-sets, i.e. the data graph. A common approach for this problem consists of building a triangulation which might not always lead to the best solution. Small changes in the location of the points might generate graphs with unstable configurations and the topology of the graph could change significantly. After building the data-graph, one could apply Graph Matching techniques to register the original point-sets. In this paper, we propose a data graph technique based on the Minimum Spanning Tree of Maximum Entropty (MSTME). We aim at a data graph construction which could be more stable than the Delaunay triangulation with respect to small variations in the neighborhood of points. Our technique aims at creating data graphs which could help the point-set registration process. We propose an algorithm with a single free parameter that weighs the importance between the total weight cost and the entropy of the current spanning tree. We compare our algorithm on a number of different databases with the Delaunay triangulation. version:1
arxiv-1505-06294 | A Frobenius Model of Information Structure in Categorical Compositional Distributional Semantics | http://arxiv.org/abs/1505.06294 | id:1505.06294 author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT math.RA  published:2015-05-23 summary:The categorical compositional distributional model of Coecke, Sadrzadeh and Clark provides a linguistically motivated procedure for computing the meaning of a sentence as a function of the distributional meaning of the words therein. The theoretical framework allows for reasoning about compositional aspects of language and offers structural ways of studying the underlying relationships. While the model so far has been applied on the level of syntactic structures, a sentence can bring extra information conveyed in utterances via intonational means. In the current paper we extend the framework in order to accommodate this additional information, using Frobenius algebraic structures canonically induced over the basis of finite-dimensional vector spaces. We detail the theory, provide truth-theoretic and distributional semantics for meanings of intonationally-marked utterances, and present justifications and extensive examples. version:1
arxiv-1505-06292 | Low-Rank Matrix Recovery from Row-and-Column Affine Measurements | http://arxiv.org/abs/1505.06292 | id:1505.06292 author:Avishai Wagner, Or Zuk category:cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH 15A83  published:2015-05-23 summary:We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix $X$. This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition ($SVD$) and least-squares ($LS$), which we term \alg. We prove that (a simplified version of) our algorithm can recover $X$ exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and \alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction. version:1
arxiv-1505-06256 | Exposing ambiguities in a relation-extraction gold standard with crowdsourcing | http://arxiv.org/abs/1505.06256 | id:1505.06256 author:Tong Shu Li, Benjamin M. Good, Andrew I. Su category:cs.CL q-bio.QM  published:2015-05-23 summary:Semantic relation extraction is one of the frontiers of biomedical natural language processing research. Gold standards are key tools for advancing this research. It is challenging to generate these standards because of the high cost of expert time and the difficulty in establishing agreement between annotators. We implemented and evaluated a microtask crowdsourcing approach that can produce a gold standard for extracting drug-disease relations. The aggregated crowd judgment agreed with expert annotations from a pre-existing corpus on 43 of 60 sentences tested. The levels of crowd agreement varied in a similar manner to the levels of agreement among the original expert annotators. This work rein-forces the power of crowdsourcing in the process of assembling gold standards for relation extraction. Further, it high-lights the importance of exposing the levels of agreement between human annotators, expert or crowd, in gold standard corpora as these are reproducible signals indicating ambiguities in the data or in the annotation guidelines. version:1
arxiv-1505-06250 | Efficient Large Scale Video Classification | http://arxiv.org/abs/1505.06250 | id:1505.06250 author:Balakrishnan Varadarajan, George Toderici, Sudheendra Vijayanarasimhan, Apostol Natsev category:cs.CV cs.MM cs.NE  published:2015-05-22 summary:Video classification has advanced tremendously over the recent years. A large part of the improvements in video classification had to do with the work done by the image classification community and the use of deep convolutional networks (CNNs) which produce competitive results with hand- crafted motion features. These networks were adapted to use video frames in various ways and have yielded state of the art classification results. We present two methods that build on this work, and scale it up to work with millions of videos and hundreds of thousands of classes while maintaining a low computational cost. In the context of large scale video processing, training CNNs on video frames is extremely time consuming, due to the large number of frames involved. We propose to avoid this problem by training CNNs on either YouTube thumbnails or Flickr images, and then using these networks' outputs as features for other higher level classifiers. We discuss the challenges of achieving this and propose two models for frame-level and video-level classification. The first is a highly efficient mixture of experts while the latter is based on long short term memory neural networks. We present results on the Sports-1M video dataset (1 million videos, 487 classes) and on a new dataset which has 12 million videos and 150,000 labels. version:1
arxiv-1505-06249 | Greedy Biomarker Discovery in the Genome with Applications to Antimicrobial Resistance | http://arxiv.org/abs/1505.06249 | id:1505.06249 author:Alexandre Drouin, Sébastien Giguère, Maxime Déraspe, François Laviolette, Mario Marchand, Jacques Corbeil category:q-bio.GN cs.LG stat.ML  published:2015-05-22 summary:The Set Covering Machine (SCM) is a greedy learning algorithm that produces sparse classifiers. We extend the SCM for datasets that contain a huge number of features. The whole genetic material of living organisms is an example of such a case, where the number of feature exceeds 10^7. Three human pathogens were used to evaluate the performance of the SCM at predicting antimicrobial resistance. Our results show that the SCM compares favorably in terms of sparsity and accuracy against L1 and L2 regularized Support Vector Machines and CART decision trees. Moreover, the SCM was the only algorithm that could consider the full feature space. For all other algorithms, the latter had to be filtered as a preprocessing step. version:1
arxiv-1505-06237 | Tunnel Surface 3D Reconstruction from Unoriented Image Sequences | http://arxiv.org/abs/1505.06237 | id:1505.06237 author:Arnold Bauer, Karlheinz Gutjahr, Gerhard Paar, Heiner Kontrus, Robert Glatzl category:cs.CV  published:2015-05-22 summary:The 3D documentation of the tunnel surface during construction requires fast and robust measurement systems. In the solution proposed in this paper, during tunnel advance a single camera is taking pictures of the tunnel surface from several positions. The recorded images are automatically processed to gain a 3D tunnel surface model. Image acquisition is realized by the tunneling/advance/driving personnel close to the tunnel face (= the front end of the advance). Based on the following fully automatic analysis/evaluation, a decision on the quality of the outbreak can be made within a few minutes. This paper describes the image recording system and conditions as well as the stereo-photogrammetry based workflow for the continuously merged dense 3D reconstruction of the entire advance region. Geo-reference is realized by means of signalized targets that are automatically detected in the images. We report on the results of recent testing under real construction conditions, and conclude with prospects for further development in terms of on-site performance. version:1
arxiv-1505-06228 | Keyphrase Based Evaluation of Automatic Text Summarization | http://arxiv.org/abs/1505.06228 | id:1505.06228 author:Fatma Elghannam, Tarek El-Shishtawy category:cs.CL 94AXX  published:2015-05-22 summary:The development of methods to deal with the informative contents of the text units in the matching process is a major challenge in automatic summary evaluation systems that use fixed n-gram matching. The limitation causes inaccurate matching between units in a peer and reference summaries. The present study introduces a new Keyphrase based Summary Evaluator KpEval for evaluating automatic summaries. The KpEval relies on the keyphrases since they convey the most important concepts of a text. In the evaluation process, the keyphrases are used in their lemma form as the matching text unit. The system was applied to evaluate different summaries of Arabic multi-document data set presented at TAC2011. The results showed that the new evaluation technique correlates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4, and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667 respectively. version:1
arxiv-1505-06169 | Learning Dynamic Feature Selection for Fast Sequential Prediction | http://arxiv.org/abs/1505.06169 | id:1505.06169 author:Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum category:cs.CL cs.LG  published:2015-05-22 summary:We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed. version:1
arxiv-1505-06162 | Design and Implementation of Real-time Algorithms for Eye Tracking and PERCLOS Measurement for on board Estimation of Alertness of Drivers | http://arxiv.org/abs/1505.06162 | id:1505.06162 author:Anjith George, Aurobinda Routray category:cs.CV  published:2015-05-22 summary:The alertness level of drivers can be estimated with the use of computer vision based methods. The level of fatigue can be found from the value of PERCLOS. It is the ratio of closed eye frames to the total frames processed. The main objective of the thesis is the design and implementation of real-time algorithms for measurement of PERCLOS. In this work we have developed a real-time system which is able to process the video onboard and to alarm the driver in case the driver is in alert. For accurate estimation of PERCLOS the frame rate should be greater than 4 and accuracy should be greater than 90%. For eye detection we have used mainly two approaches Haar classifier based method and Principal Component Analysis (PCA) based method for day time. During night time active Near Infra Red (NIR) illumination is used. Local Binary Pattern (LBP) histogram based method is used for the detection of eyes at night time. The accuracy rate of the algorithms was found to be more than 90% at frame rates more than 5 fps which was suitable for the application. version:1
arxiv-1411-0547 | Correlation Clustering with Constrained Cluster Sizes and Extended Weights Bounds | http://arxiv.org/abs/1411.0547 | id:1411.0547 author:Gregory J. Puleo, Olgica Milenkovic category:cs.LG cs.DS  published:2014-11-03 summary:We consider the problem of correlation clustering on graphs with constraints on both the cluster sizes and the positive and negative weights of edges. Our contributions are twofold: First, we introduce the problem of correlation clustering with bounded cluster sizes. Second, we extend the regime of weight values for which the clustering may be performed with constant approximation guarantees in polynomial time and apply the results to the bounded cluster size problem. version:3
arxiv-1503-04474 | Statistical Estimation and Clustering of Group-invariant Orientation Parameters | http://arxiv.org/abs/1503.04474 | id:1503.04474 author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey Simmons, Alfred Hero category:stat.ML physics.data-an  published:2015-03-15 summary:We treat the problem of estimation of orientation parameters whose values are invariant to transformations from a spherical symmetry group. Previous work has shown that any such group-invariant distribution must satisfy a restricted finite mixture representation, which allows the orientation parameter to be estimated using an Expectation Maximization (EM) maximum likelihood (ML) estimation algorithm. In this paper, we introduce two parametric models for this spherical symmetry group estimation problem: 1) the hyperbolic Von Mises Fisher (VMF) mixture distribution and 2) the Watson mixture distribution. We also introduce a new EM-ML algorithm for clustering samples that come from mixtures of group-invariant distributions with different parameters. We apply the models to the problem of mean crystal orientation estimation under the spherically symmetric group associated with the crystal form, e.g., cubic or octahedral or hexahedral. Simulations and experiments establish the advantages of the extended EM-VMF and EM-Watson estimators for data acquired by Electron Backscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloy sample. version:2
arxiv-1505-06125 | Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors | http://arxiv.org/abs/1505.06125 | id:1505.06125 author:David Mascharka, Eric Manley category:cs.LG cs.NI  published:2015-05-22 summary:In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a real-world environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment. version:1
arxiv-1311-6530 | A Mixture of Generalized Hyperbolic Factor Analyzers | http://arxiv.org/abs/1311.6530 | id:1311.6530 author:Cristina Tortora, Paul D. McNicholas, Ryan P. Browne category:stat.ME stat.ML  published:2013-11-26 summary:Model-based clustering imposes a finite mixture modelling structure on data for clustering. Finite mixture models assume that the population is a convex combination of a finite number of densities, the distribution within each population is a basic assumption of each particular model. Among all distributions that have been tried, the generalized hyperbolic distribution has the advantage that is a generalization of several other methods, such as the Gaussian distribution, the skew t-distribution, etc. With specific parameters, it can represent either a symmetric or a skewed distribution. While its inherent flexibility is an advantage in many ways, it means the estimation of more parameters than its special and limiting cases. The aim of this work is to propose a mixture of generalized hyperbolic factor analyzers to introduce parsimony and extend the method to high dimensional data. This work can be seen as an extension of the mixture of factor analyzers model to generalized hyperbolic mixtures. The performance of our generalized hyperbolic factor analyzers is illustrated on real data, where it performs favourably compared to its Gaussian analogue. version:3
arxiv-1505-06079 | Robust Rotation Synchronization via Low-rank and Sparse Matrix Decomposition | http://arxiv.org/abs/1505.06079 | id:1505.06079 author:Federica Arrigoni, Andrea Fusiello, Beatrice Rossi, Pasqualina Fragneto category:cs.CV  published:2015-05-22 summary:This paper deals with the rotation synchronization problem, which arises in global registration of 3D point-sets and in structure from motion. The problem is formulated in an unprecedented way as a "low-rank and sparse" matrix decomposition that handles both outliers and missing data. A minimization strategy, dubbed R-GoDec, is also proposed and evaluated experimentally against state-of-the-art algorithms on simulated and real data. The results show that R-GoDec is the fastest among the robust algorithms. version:1
arxiv-1412-4690 | GPTIPS 2: an open-source software platform for symbolic data mining | http://arxiv.org/abs/1412.4690 | id:1412.4690 author:Dominic P. Searson category:cs.MS cs.NE  published:2014-12-15 summary:GPTIPS is a free, open source MATLAB based software platform for symbolic data mining (SDM). It uses a multigene variant of the biologically inspired machine learning method of genetic programming (MGGP) as the engine that drives the automatic model discovery process. Symbolic data mining is the process of extracting hidden, meaningful relationships from data in the form of symbolic equations. In contrast to other data-mining methods, the structural transparency of the generated predictive equations can give new insights into the physical systems or processes that generated the data. Furthermore, this transparency makes the models very easy to deploy outside of MATLAB. The rationale behind GPTIPS is to reduce the technical barriers to using, understanding, visualising and deploying GP based symbolic models of data, whilst at the same time remaining highly customisable and delivering robust numerical performance for power users. In this chapter, notable new features of the latest version of the software are discussed with these aims in mind. Additionally, a simplified variant of the MGGP high level gene crossover mechanism is proposed. It is demonstrated that the new functionality of GPTIPS 2 (a) facilitates the discovery of compact symbolic relationships from data using multiple approaches, e.g. using novel gene-centric visualisation analysis to mitigate horizontal bloat and reduce complexity in multigene symbolic regression models (b) provides numerous methods for visualising the properties of symbolic models (c) emphasises the generation of graphically navigable libraries of models that are optimal in terms of the Pareto trade off surface of model performance and complexity and (d) expedites real world applications by the simple, rapid and robust deployment of symbolic models outside the software environment they were developed in. version:2
arxiv-1505-05969 | Learning Program Embeddings to Propagate Feedback on Student Code | http://arxiv.org/abs/1505.05969 | id:1505.05969 author:Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas category:cs.LG cs.NE cs.SE  published:2015-05-22 summary:Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University's CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions. version:1
arxiv-1502-02843 | Distributed Gaussian Processes | http://arxiv.org/abs/1502.02843 | id:1502.02843 author:Marc Peter Deisenroth, Jun Wei Ng category:stat.ML  published:2015-02-10 summary:To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets. version:3
arxiv-1505-05957 | Joint Inference of Groups, Events and Human Roles in Aerial Videos | http://arxiv.org/abs/1505.05957 | id:1505.05957 author:Tianmin Shu, Dan Xie, Brandon Rothrock, Sinisa Todorovic, Song-Chun Zhu category:cs.CV  published:2015-05-22 summary:With the advent of drones, aerial video analysis becomes increasingly important; yet, it has received scant attention in the literature. This paper addresses a new problem of parsing low-resolution aerial videos of large spatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigning roles to people engaged in events. We propose a novel framework aimed at conducting joint inference of the above tasks, as reasoning about each in isolation typically fails in our setting. Given noisy tracklets of people and detections of large objects and scene surfaces (e.g., building, grass), we use a spatiotemporal AND-OR graph to drive our joint inference, using Markov Chain Monte Carlo and dynamic programming. We also introduce a new formalism of spatiotemporal templates characterizing latent sub-events. For evaluation, we have collected and released a new aerial videos dataset using a hex-rotor flying over picnic areas rich with group events. Our results demonstrate that we successfully address above inference tasks under challenging conditions. version:1
arxiv-1412-3489 | Quantum Deep Learning | http://arxiv.org/abs/1412.3489 | id:1412.3489 author:Nathan Wiebe, Ashish Kapoor, Krysta M. Svore category:quant-ph cs.LG cs.NE  published:2014-12-10 summary:In recent years, deep learning has had a profound impact on machine learning and artificial intelligence. At the same time, algorithms for quantum computers have been shown to efficiently solve some problems that are intractable on conventional, classical computers. We show that quantum computing not only reduces the time required to train a deep restricted Boltzmann machine, but also provides a richer and more comprehensive framework for deep learning than classical computing and leads to significant improvements in the optimization of the underlying objective function. Our quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well known classical counterparts. version:2
arxiv-1505-05916 | Rendering of Eyes for Eye-Shape Registration and Gaze Estimation | http://arxiv.org/abs/1505.05916 | id:1505.05916 author:Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, Andreas Bulling category:cs.CV  published:2015-05-21 summary:Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild. version:1
arxiv-1412-6544 | Qualitatively characterizing neural network optimization problems | http://arxiv.org/abs/1412.6544 | id:1412.6544 author:Ian J. Goodfellow, Oriol Vinyals, Andrew M. Saxe category:cs.NE cs.LG stat.ML  published:2014-12-19 summary:Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles. version:6
arxiv-1505-05899 | The IBM 2015 English Conversational Telephone Speech Recognition System | http://arxiv.org/abs/1505.05899 | id:1505.05899 author:George Saon, Hong-Kwang J. Kuo, Steven Rennie, Michael Picheny category:cs.CL  published:2015-05-21 summary:We describe the latest improvements to the IBM English conversational telephone speech recognition system. Some of the techniques that were found beneficial are: maxout networks with annealed dropout rates; networks with a very large number of outputs trained on 2000 hours of data; joint modeling of partially unfolded recurrent neural networks and convolutional nets by combining the bottleneck and output layers and retraining the resulting model; and lastly, sophisticated language model rescoring with exponential and neural network LMs. These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result. version:1
arxiv-1505-05841 | Translation Memory Retrieval Methods | http://arxiv.org/abs/1505.05841 | id:1505.05841 author:Michael Bloodgood, Benjamin Strauss category:cs.CL I.2.7  published:2015-05-21 summary:Translation Memory (TM) systems are one of the most widely used translation technologies. An important part of TM systems is the matching algorithm that determines what translations get retrieved from the bank of available translations to assist the human translator. Although detailed accounts of the matching algorithms used in commercial systems can't be found in the literature, it is widely believed that edit distance algorithms are used. This paper investigates and evaluates the use of several matching algorithms, including the edit distance algorithm that is believed to be at the heart of most modern commercial TM systems. This paper presents results showing how well various matching algorithms correlate with human judgments of helpfulness (collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm based on weighted n-gram precision that can be adjusted for translator length preferences consistently returns translations judged to be most helpful by translators for multiple domains and language pairs. version:1
arxiv-1505-05798 | Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret | http://arxiv.org/abs/1505.05798 | id:1505.05798 author:Haitham Bou Ammar, Rasul Tutunov, Eric Eaton category:cs.LG  published:2015-05-21 summary:Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial set- ting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control. version:1
arxiv-1412-6515 | On distinguishability criteria for estimating generative models | http://arxiv.org/abs/1412.6515 | id:1412.6515 author:Ian J. Goodfellow category:stat.ML  published:2014-12-19 summary:Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification. Noise-contrastive estimation (NCE) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks (GANs) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation (MLE). NCE corresponds to training an internal data model belonging to the {\em discriminator} network but using a fixed generator network. We show that a variant of NCE, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers MLE, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. However, we show that recovering MLE for a learned generator requires departing from the distinguishability game. Specifically: (i) The expected gradient of the NCE discriminator can be made to match the expected gradient of MLE, if one is allowed to use a non-stationary noise distribution for NCE, (ii) No choice of discriminator network can make the expected gradient for the GAN generator match that of MLE, and (iii) The existing theory does not guarantee that GANs will converge in the non-convex case. This suggests that the key next step in GAN research is to determine whether GANs converge, and if not, to modify their training algorithm to force convergence. version:4
arxiv-1505-05769 | Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos | http://arxiv.org/abs/1505.05769 | id:1505.05769 author:Ishan Misra, Abhinav Shrivastava, Martial Hebert category:cs.CV  published:2015-05-21 summary:We present a semi-supervised approach that localizes multiple unknown object instances in long videos. We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances. We propose criteria for reliable object detection and tracking for constraining the semi-supervised learning process and minimizing semantic drift. Our approach does not assume exhaustive labeling of each object instance in any single frame, or any explicit annotation of negative data. Working in such a generic setting allow us to tackle multiple object instances in video, many of which are static. In contrast, existing approaches either do not consider multiple object instances per video, or rely heavily on the motion of the objects present. The experiments demonstrate the effectiveness of our approach by evaluating the automatically labeled data on a variety of metrics like quality, coverage (recall), diversity, and relevance to training an object detector. version:1
arxiv-1505-05753 | GazeDPM: Early Integration of Gaze Information in Deformable Part Models | http://arxiv.org/abs/1505.05753 | id:1505.05753 author:Iaroslav Shcherbatyi, Andreas Bulling, Mario Fritz category:cs.CV cs.HC  published:2015-05-21 summary:An increasing number of works explore collaborative human-computer systems in which human gaze is used to enhance computer vision systems. For object detection these efforts were so far restricted to late integration approaches that have inherent limitations, such as increased precision without increase in recall. We propose an early integration approach in a deformable part model, which constitutes a joint formulation over gaze and visual data. We show that our GazeDPM method improves over the state-of-the-art DPM baseline by 4% and a recent method for gaze-supported object detection by 3% on the public POET dataset. Our approach additionally provides introspection of the learnt models, can reveal salient image structures, and allows us to investigate the interplay between gaze attracting and repelling areas, the importance of view-specific models, as well as viewers' personal biases in gaze patterns. We finally study important practical aspects of our approach, such as the impact of using saliency maps instead of real fixations, the impact of the number of fixations, as well as robustness to gaze estimation error. version:1
arxiv-1505-05424 | Weight Uncertainty in Neural Networks | http://arxiv.org/abs/1505.05424 | id:1505.05424 author:Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra category:stat.ML cs.LG  published:2015-05-20 summary:We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning. version:2
arxiv-1410-8618 | Symmetric low-rank representation for subspace clustering | http://arxiv.org/abs/1410.8618 | id:1410.8618 author:Jie Chen, Haixian Zhang, Hua Mao, Yongsheng Sang, Zhang Yi category:cs.CV  published:2014-10-31 summary:We propose a symmetric low-rank representation (SLRR) method for subspace clustering, which assumes that a data set is approximately drawn from the union of multiple subspaces. The proposed technique can reveal the membership of multiple subspaces through the self-expressiveness property of the data. In particular, the SLRR method considers a collaborative representation combined with low-rank matrix recovery techniques as a low-rank representation to learn a symmetric low-rank representation, which preserves the subspace structures of high-dimensional data. In contrast to performing iterative singular value decomposition in some existing low-rank representation based algorithms, the symmetric low-rank representation in the SLRR method can be calculated as a closed form solution by solving the symmetric low-rank optimization problem. By making use of the angular information of the principal directions of the symmetric low-rank representation, an affinity graph matrix is constructed for spectral clustering. Extensive experimental results show that it outperforms state-of-the-art subspace clustering algorithms. version:2
arxiv-1505-05740 | Graph edit distance : a new binary linear programming formulation | http://arxiv.org/abs/1505.05740 | id:1505.05740 author:Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre Héroux, Sébastien Adam category:cs.DS cs.CV  published:2015-05-21 summary:Graph edit distance (GED) is a powerful and flexible graph matching paradigm that can be used to address different tasks in structural pattern recognition, machine learning, and data mining. In this paper, some new binary linear programming formulations for computing the exact GED between two graphs are proposed. A major strength of the formulations lies in their genericity since the GED can be computed between directed or undirected fully attributed graphs (i.e. with attributes on both vertices and edges). Moreover, a relaxation of the domain constraints in the formulations provides efficient lower bound approximations of the GED. A complete experimental study comparing the proposed formulations with 4 state-of-the-art algorithms for exact and approximate graph edit distances is provided. By considering both the quality of the proposed solution and the efficiency of the algorithms as performance criteria, the results show that none of the compared methods dominates the others in the Pareto sense. As a consequence, faced to a given real-world problem, a trade-off between quality and efficiency has to be chosen w.r.t. the application constraints. In this context, this paper provides a guide that can be used to choose the appropriate method. version:1
arxiv-1505-05723 | On the relation between accuracy and fairness in binary classification | http://arxiv.org/abs/1505.05723 | id:1505.05723 author:Indre Zliobaite category:cs.LG cs.AI  published:2015-05-21 summary:Our study revisits the problem of accuracy-fairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of non-discriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination. version:1
arxiv-1505-05667 | A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network | http://arxiv.org/abs/1505.05667 | id:1505.05667 author:Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang category:cs.CL cs.LG cs.NE  published:2015-05-21 summary:In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. version:1
arxiv-1505-05663 | Inferring Graphs from Cascades: A Sparse Recovery Framework | http://arxiv.org/abs/1505.05663 | id:1505.05663 author:Jean Pouget-Abadie, Thibaut Horel category:cs.SI cs.LG stat.ML  published:2015-05-21 summary:In the Network Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. In this paper, we approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graph's edges with high probability and $O(s\log m)$ measurements where $s$ is the maximum degree of the graph and $m$ is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound of $\Omega(s\log\frac{m}{s})$ and validate our approach empirically on synthetic graphs. version:1
arxiv-1505-05643 | Object Modelling with a Handheld RGB-D Camera | http://arxiv.org/abs/1505.05643 | id:1505.05643 author:Aitor Aldoma, Johann Prankl, Alexander Svejda, Markus Vincze category:cs.CV  published:2015-05-21 summary:This work presents a flexible system to reconstruct 3D models of objects captured with an RGB-D sensor. A major advantage of the method is that our reconstruction pipeline allows the user to acquire a full 3D model of the object. This is achieved by acquiring several partial 3D models in different sessions that are automatically merged together to reconstruct a full model. In addition, the 3D models acquired by our system can be directly used by state-of-the-art object instance recognition and object tracking modules, providing object-perception capabilities for different applications, such as human-object interaction analysis or robot grasping. The system does not impose constraints in the appearance of objects (textured, untextured) nor in the modelling setup (moving camera with static object or a turn-table setup). The proposed reconstruction system has been used to model a large number of objects resulting in metrically accurate and visually appealing 3D models. version:1
arxiv-1505-05641 | Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views | http://arxiv.org/abs/1505.05641 | id:1505.05641 author:Hao Su, Charles R. Qi, Yangyan Li, Leonidas Guibas category:cs.CV  published:2015-05-21 summary:Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs. We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark. version:1
arxiv-1505-05601 | Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm | http://arxiv.org/abs/1505.05601 | id:1505.05601 author:S L Happy, Swarnadip Chatterjee, Debdoot Sheet category:cs.CV  published:2015-05-21 summary:Overlapping of cervical cells and poor contrast of cell cytoplasm are the major issues in accurate detection and segmentation of cervical cells. An unsupervised cell segmentation approach is presented here. Cell clump segmentation was carried out using the extended depth of field (EDF) image created from the images of different focal planes. A modified Otsu method with prior class weights is proposed for accurate segmentation of nuclei from the cell clumps. The cell cytoplasm was further segmented from cell clump depending upon the number of nucleus detected in that cell clump. Level set model was used for cytoplasm segmentation. version:1
arxiv-1505-05572 | The development of an information criterion for Change-Point Analysis | http://arxiv.org/abs/1505.05572 | id:1505.05572 author:Paul A. Wiggins, Colin H. LaMont category:physics.data-an cs.LG stat.ML  published:2015-05-21 summary:Change-point analysis is a flexible and computationally tractable tool for the analysis of times series data from systems that transition between discrete states and whose observables are corrupted by noise. The change-point algorithm is used to identify the time indices (change points) at which the system transitions between these discrete states. We present a unified information-based approach to testing for the existence of change points. This new approach reconciles two previously disparate approaches to Change-Point Analysis (frequentist and information-based) for testing transitions between states. The resulting method is statistically principled, parameter and prior free and widely applicable to a wide range of change-point problems. version:1
arxiv-1502-02362 | Counterfactual Risk Minimization: Learning from Logged Bandit Feedback | http://arxiv.org/abs/1502.02362 | id:1502.02362 author:Adith Swaminathan, Thorsten Joachims category:cs.LG stat.ML  published:2015-02-09 summary:We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method -- called Policy Optimizer for Exponential Models (POEM) -- for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art. version:2
arxiv-1505-04413 | Harmonic Exponential Families on Manifolds | http://arxiv.org/abs/1505.04413 | id:1505.04413 author:Taco S. Cohen, Max Welling category:stat.ML  published:2015-05-17 summary:In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train. version:2
arxiv-1504-06755 | TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking | http://arxiv.org/abs/1504.06755 | id:1504.06755 author:Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, Jianxiong Xiao category:cs.CV  published:2015-04-25 summary:Traditional eye tracking requires specialized hardware, which means collecting gaze data from many observers is expensive, tedious and slow. Therefore, existing saliency prediction datasets are order-of-magnitudes smaller than typical datasets for other vision recognition tasks. The small size of these datasets limits the potential for training data intensive algorithms, and causes overfitting in benchmark evaluation. To address this deficiency, this paper introduces a webcam-based gaze tracking system that supports large-scale, crowdsourced eye tracking deployed on Amazon Mechanical Turk (AMTurk). By a combination of careful algorithm and gaming protocol design, our system obtains eye tracking data for saliency prediction comparable to data gathered in a traditional lab setting, with relatively lower cost and less effort on the part of the researchers. Using this tool, we build a saliency dataset for a large number of natural images. We will open-source our tool and provide a web server where researchers can upload their images to get eye tracking results from AMTurk. version:2
arxiv-1505-05459 | Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect | http://arxiv.org/abs/1505.05459 | id:1505.05459 author:Hamed Sarbolandi, Damien Lefloch, Andreas Kolb category:cs.CV  published:2015-05-20 summary:Recently, the new Kinect One has been issued by Microsoft, providing the next generation of real-time range sensing devices based on the Time-of-Flight (ToF) principle. As the first Kinect version was using a structured light approach, one would expect various differences in the characteristics of the range data delivered by both devices. This paper presents a detailed and in-depth comparison between both devices. In order to conduct the comparison, we propose a framework of seven different experimental setups, which is a generic basis for evaluating range cameras such as Kinect. The experiments have been designed with the goal to capture individual effects of the Kinect devices as isolatedly as possible and in a way, that they can also be adopted, in order to apply them to any other range sensing device. The overall goal of this paper is to provide a solid insight into the pros and cons of either device. Thus, scientists that are interested in using Kinect range sensing cameras in their specific application scenario can directly assess the expected, specific benefits and potential problem of either device. version:1
arxiv-1505-05451 | Fuzzy Least Squares Twin Support Vector Machines | http://arxiv.org/abs/1505.05451 | id:1505.05451 author:Javad Salimi Sartakhti, Nasser Ghadiri, Homayun Afrabandpey category:cs.AI cs.LG  published:2015-05-20 summary:Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficient and fast version of SVM algorithm for binary classification. LSTSVM combines the idea of Least Squares SVM and Twin SVM in which two non-parallel hyperplanes are found by solving two systems of linear equations. Although, the algorithm is very fast and efficient in many classification tasks, it is unable to cope with two features of real-world problems. First, in many real-world classification problems, it is almost impossible to assign data points to a single class. Second, data points in real-world problems may have different importance. In this study, we propose a novel version of LSTSVM based on fuzzy concepts to deal with these two characteristics of real-world data. The algorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility than binary classification of LSTSVM. Two models are proposed for the algorithm. In the first model, a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples. In the second model we construct fuzzy hyperplanes to classify data. Finally, we apply our proposed FLSTSVM to an artificial as well as three real-world datasets. Results demonstrate that FLSTSVM obtains better performance than SVM and LSTSVM. version:1
arxiv-1502-04623 | DRAW: A Recurrent Neural Network For Image Generation | http://arxiv.org/abs/1502.04623 | id:1502.04623 author:Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra category:cs.CV cs.LG cs.NE  published:2015-02-16 summary:This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye. version:2
