arxiv-1406-1546 | Consistent procedures for cluster tree estimation and pruning | http://arxiv.org/abs/1406.1546 | id:1406.1546 author:Kamalika Chaudhuri, Sanjoy Dasgupta, Samory Kpotufe, Ulrike von Luxburg category:stat.ML  published:2014-06-05 summary:For a density $f$ on ${\mathbb R}^d$, a {\it high-density cluster} is any connected component of $\{x: f(x) \geq \lambda\}$, for some $\lambda > 0$. The set of all high-density clusters forms a hierarchy called the {\it cluster tree} of $f$. We present two procedures for estimating the cluster tree given samples from $f$. The first is a robust variant of the single linkage algorithm for hierarchical clustering. The second is based on the $k$-nearest neighbor graph of the samples. We give finite-sample convergence rates for these algorithms which also imply consistency, and we derive lower bounds on the sample complexity of cluster tree estimation. Finally, we study a tree pruning procedure that guarantees, under milder conditions than usual, to remove clusters that are spurious while recovering those that are salient. version:1
arxiv-1406-1528 | Towards building a Crowd-Sourced Sky Map | http://arxiv.org/abs/1406.1528 | id:1406.1528 author:Dustin Lang, David W. Hogg, Bernhard Scholkopf category:cs.CV astro-ph.IM  published:2014-06-05 summary:We describe a system that builds a high dynamic-range and wide-angle image of the night sky by combining a large set of input images. The method makes use of pixel-rank information in the individual input images to improve a "consensus" pixel rank in the combined image. Because it only makes use of ranks and the complexity of the algorithm is linear in the number of images, the method is useful for large sets of uncalibrated images that might have undergone unknown non-linear tone mapping transformations for visualization or aesthetic reasons. We apply the method to images of the night sky (of unknown provenance) discovered on the Web. The method permits discovery of astronomical objects or features that are not visible in any of the input images taken individually. More importantly, however, it permits scientific exploitation of a huge source of astronomical images that would not be available to astronomical research without our automatic system. version:1
arxiv-1405-0892 | A Continuous Max-Flow Approach to Multi-Labeling Problems under Arbitrary Region Regularization | http://arxiv.org/abs/1405.0892 | id:1405.0892 author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV  published:2014-05-05 summary:The incorporation of region regularization into max-flow segmentation has traditionally focused on ordering and part-whole relationships. A side effect of the development of such models is that it constrained regularization only to those cases, rather than allowing for arbitrary region regularization. Directed Acyclic Graphical Max-Flow (DAGMF) segmentation overcomes these limitations by allowing for the algorithm designer to specify an arbitrary directed acyclic graph to structure a max-flow segmentation. This allows for individual 'parts' to be a member of multiple distinct 'wholes.' version:2
arxiv-1404-0336 | A Continuous Max-Flow Approach to General Hierarchical Multi-Labeling Problems | http://arxiv.org/abs/1404.0336 | id:1404.0336 author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV  published:2014-04-01 summary:Multi-region segmentation algorithms often have the onus of incorporating complex anatomical knowledge representing spatial or geometric relationships between objects, and general-purpose methods of addressing this knowledge in an optimization-based manner have thus been lacking. This paper presents Generalized Hierarchical Max-Flow (GHMF) segmentation, which captures simple anatomical part-whole relationships in the form of an unconstrained hierarchy. Regularization can then be applied to both parts and wholes independently, allowing for spatial grouping and clustering of labels in a globally optimal convex optimization framework. For the purposes of ready integration into a variety of segmentation tasks, the hierarchies can be presented in run-time, allowing for the segmentation problem to be readily specified and alternatives explored without undue programming effort or recompilation. version:2
arxiv-1308-0850 | Generating Sequences With Recurrent Neural Networks | http://arxiv.org/abs/1308.0850 | id:1308.0850 author:Alex Graves category:cs.NE cs.CL  published:2013-08-04 summary:This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles. version:5
arxiv-1405-7292 | An Easy to Use Repository for Comparing and Improving Machine Learning Algorithm Usage | http://arxiv.org/abs/1405.7292 | id:1405.7292 author:Michael R. Smith, Andrew White, Christophe Giraud-Carrier, Tony Martinez category:stat.ML cs.LG  published:2014-05-28 summary:The results from most machine learning experiments are used for a specific purpose and then discarded. This results in a significant loss of information and requires rerunning experiments to compare learning algorithms. This also requires implementation of another algorithm for comparison, that may not always be correctly implemented. By storing the results from previous experiments, machine learning algorithms can be compared easily and the knowledge gained from them can be used to improve their performance. The purpose of this work is to provide easy access to previous experimental results for learning and comparison. These stored results are comprehensive -- storing the prediction for each test instance as well as the learning algorithm, hyperparameters, and training set that were used. Previous results are particularly important for meta-learning, which, in a broad sense, is the process of learning from previous machine learning results such that the learning process is improved. While other experiment databases do exist, one of our focuses is on easy access to the data. We provide meta-learning data sets that are ready to be downloaded for meta-learning experiments. In addition, queries to the underlying database can be made if specific information is desired. We also differ from previous experiment databases in that our databases is designed at the instance level, where an instance is an example in a data set. We store the predictions of a learning algorithm trained on a specific training set for each instance in the test set. Data set level information can then be obtained by aggregating the results from the instances. The instance level information can be used for many tasks such as determining the diversity of a classifier or algorithmically determining the optimal subset of training instances for a learning algorithm. version:2
arxiv-1406-1385 | Learning the Information Divergence | http://arxiv.org/abs/1406.1385 | id:1406.1385 author:Onur Dikmen, Zhirong Yang, Erkki Oja category:cs.LG  published:2014-06-05 summary:Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the beta-divergence family. Selecting the best beta then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate alpha-divergence in terms of beta-divergence, which enables automatic selection of alpha by maximum likelihood with reuse of the learning principle for beta-divergence. Furthermore, we show the connections between gamma and beta-divergences as well as R\'enyi and alpha-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families. version:1
arxiv-1406-1280 | Basis Identification for Automatic Creation of Pronunciation Lexicon for Proper Names | http://arxiv.org/abs/1406.1280 | id:1406.1280 author:Sunil Kumar Kopparapu, M Laxminarayana category:cs.CL  published:2014-06-05 summary:Development of a proper names pronunciation lexicon is usually a manual effort which can not be avoided. Grapheme to phoneme (G2P) conversion modules, in literature, are usually rule based and work best for non-proper names in a particular language. Proper names are foreign to a G2P module. We follow an optimization approach to enable automatic construction of proper names pronunciation lexicon. The idea is to construct a small orthogonal set of words (basis) which can span the set of names in a given database. We propose two algorithms for the construction of this basis. The transcription lexicon of all the proper names in a database can be produced by the manual transcription of only the small set of basis words. We first construct a cost function and show that the minimization of the cost function results in a basis. We derive conditions for convergence of this cost function and validate them experimentally on a very large proper name database. Experiments show the transcription can be achieved by transcribing a set of small number of basis words. The algorithms proposed are generic and independent of language; however performance is better if the proper names have same origin, namely, same language or geographical region. version:1
arxiv-1406-1265 | Illusory Shapes via Phase Transition | http://arxiv.org/abs/1406.1265 | id:1406.1265 author:Yoon Mo Jung, Jianhong Jackie Shen category:math.OC cs.CV q-bio.NC  published:2014-06-05 summary:We propose a new variational illusory shape (VIS) model via phase fields and phase transitions. It is inspired by the first-order variational illusory contour (VIC) model proposed by Jung and Shen [{\em J. Visual Comm. Image Repres.}, {\bf 19}:42-55, 2008]. Under the new VIS model, illusory shapes are represented by phase values close to 1 while the rest by values close to 0. The 0-1 transition is achieved by an elliptic energy with a double-well potential, as in the theory of $\Gamma$-convergence. The VIS model is non-convex, with the zero field as its trivial global optimum. To seek visually meaningful local optima that can induce illusory shapes, an iterative algorithm is designed and its convergence behavior is closely studied. Several generic numerical examples confirm the versatility of the model and the algorithm. version:1
arxiv-1406-0588 | Image retrieval with hierarchical matching pursuit | http://arxiv.org/abs/1406.0588 | id:1406.0588 author:Shasha Bu, Yu-Jin Zhang category:cs.CV  published:2014-06-03 summary:A novel representation of images for image retrieval is introduced in this paper, by using a new type of feature with remarkable discriminative power. Despite the multi-scale nature of objects, most existing models perform feature extraction on a fixed scale, which will inevitably degrade the performance of the whole system. Motivated by this, we introduce a hierarchical sparse coding architecture for image retrieval to explore multi-scale cues. Sparse codes extracted on lower layers are transmitted to higher layers recursively. With this mechanism, cues from different scales are fused. Experiments on the Holidays dataset show that the proposed method achieves an excellent retrieval performance with a small code length. version:2
arxiv-1406-1247 | Shared Representation Learning for Heterogeneous Face Recognition | http://arxiv.org/abs/1406.1247 | id:1406.1247 author:Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV  published:2014-06-05 summary:After intensive research, heterogenous face recognition is still a challenging problem. The main difficulties are owing to the complex relationship between heterogenous face image spaces. The heterogeneity is always tightly coupled with other variations, which makes the relationship of heterogenous face images highly nonlinear. Many excellent methods have been proposed to model the nonlinear relationship, but they apt to overfit to the training set, due to limited samples. Inspired by the unsupervised algorithms in deep learning, this paper proposes an novel framework for heterogeneous face recognition. We first extract Gabor features at some localized facial points, and then use Restricted Boltzmann Machines (RBMs) to learn a shared representation locally to remove the heterogeneity around each facial point. Finally, the shared representations of local RBMs are connected together and processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases are selected to evaluate the proposed method. For Sketch-Photo problem, we obtain perfect results on the CUFS database. For NIR-VIS problem, we produce new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases. version:1
arxiv-1404-2229 | Towards the Safety of Human-in-the-Loop Robotics: Challenges and Opportunities for Safety Assurance of Robotic Co-Workers | http://arxiv.org/abs/1404.2229 | id:1404.2229 author:Kerstin Eder, Chris Harper, Ute Leonards category:cs.RO cs.LG I.2.9  published:2014-04-08 summary:The success of the human-robot co-worker team in a flexible manufacturing environment where robots learn from demonstration heavily relies on the correct and safe operation of the robot. How this can be achieved is a challenge that requires addressing both technical as well as human-centric research questions. In this paper we discuss the state of the art in safety assurance, existing as well as emerging standards in this area, and the need for new approaches to safety assurance in the context of learning machines. We then focus on robotic learning from demonstration, the challenges these techniques pose to safety assurance and indicate opportunities to integrate safety considerations into algorithms "by design". Finally, from a human-centric perspective, we stipulate that, to achieve high levels of safety and ultimately trust, the robotic co-worker must meet the innate expectations of the humans it works with. It is our aim to stimulate a discussion focused on the safety aspects of human-in-the-loop robotics, and to foster multidisciplinary collaboration to address the research challenges identified. version:3
arxiv-1406-1241 | The Best Templates Match Technique For Example Based Machine Translation | http://arxiv.org/abs/1406.1241 | id:1406.1241 author:T. El-Shishtawy, A. El-Sammak category:cs.CL  published:2014-06-04 summary:It has been proved that large scale realistic Knowledge Based Machine Translation applications require acquisition of huge knowledge about language and about the world. This knowledge is encoded in computational grammars, lexicons and domain models. Another approach which avoids the need for collecting and analyzing massive knowledge, is the Example Based approach, which is the topic of this paper. We show through the paper that using Example Based in its native form is not suitable for translating into Arabic. Therefore a modification to the basic approach is presented to improve the accuracy of the translation process. The basic idea of the new approach is to improve the technique by which template-based approaches select the appropriate templates. version:1
arxiv-1406-1234 | A Geometric Method to Obtain the Generation Probability of a Sentence | http://arxiv.org/abs/1406.1234 | id:1406.1234 author:Chen Lijiang category:cs.CL cs.AI math.ST stat.CO stat.ME stat.TH  published:2014-06-04 summary:"How to generate a sentence" is the most critical and difficult problem in all the natural language processing technologies. In this paper, we present a new approach to explain the generation process of a sentence from the perspective of mathematics. Our method is based on the premise that in our brain a sentence is a part of a word network which is formed by many word nodes. Experiments show that the probability of the entire sentence can be obtained by the probabilities of single words and the probabilities of the co-occurrence of word pairs, which indicate that human use the synthesis method to generate a sentence. version:1
arxiv-1406-1231 | Multi-task Neural Networks for QSAR Predictions | http://arxiv.org/abs/1406.1231 | id:1406.1231 author:George E. Dahl, Navdeep Jaitly, Ruslan Salakhutdinov category:stat.ML cs.LG cs.NE  published:2014-06-04 summary:Although artificial neural networks have occasionally been used for Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in the past, the literature has of late been dominated by other machine learning techniques such as random forests. However, a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches. In this work, inspired by the winning team's use of neural networks in a recent QSAR competition, we used an artificial neural network to learn a function that predicts activities of compounds for multiple assays at the same time. We conducted experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networks literature. We compared our methods to alternative methods reported to perform well on these tasks and found that our neural net methods provided superior performance. version:1
arxiv-1406-1203 | A Semantic Approach to Summarization | http://arxiv.org/abs/1406.1203 | id:1406.1203 author:Divyanshu Bhartiya, Ashudeep Singh category:cs.CL  published:2014-06-04 summary:Sentence extraction based summarization methods has some limitations as it doesn't go into the semantics of the document. Also, it lacks the capability of sentence generation which is intuitive to humans. Here we present a novel method to summarize text documents taking the process to semantic levels with the use of WordNet and other resources, and using a technique for sentence generation. We involve semantic role labeling to get the semantic representation of text and use of segmentation to form clusters of the related pieces of text. Picking out the centroids and sentence generation completes the task. We evaluate our system against human composed summaries and also present an evaluation done by humans to measure the quality attributes of our summaries. version:1
arxiv-1406-1143 | Identifying Duplicate and Contradictory Information in Wikipedia | http://arxiv.org/abs/1406.1143 | id:1406.1143 author:Sarah Weissman, Samet Ayhan, Joshua Bradley, Jimmy Lin category:cs.IR cs.CL cs.DL cs.SI  published:2014-06-04 summary:Our study identifies sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify clusters of sentences with high Jaccard similarity. We show that these clusters can be categorized into six different types, two of which are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia. version:1
arxiv-1402-0030 | Neural Variational Inference and Learning in Belief Networks | http://arxiv.org/abs/1402.0030 | id:1402.0030 author:Andriy Mnih, Karol Gregor category:cs.LG stat.ML  published:2014-01-31 summary:Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset. version:2
arxiv-1406-1111 | PAC Learning, VC Dimension, and the Arithmetic Hierarchy | http://arxiv.org/abs/1406.1111 | id:1406.1111 author:Wesley Calvert category:math.LO cs.LG cs.LO 03D80  03D45 I.2.6  published:2014-06-04 summary:We compute that the index set of PAC-learnable concept classes is $m$-complete $\Sigma^0_3$ within the set of indices for all concept classes of a reasonable form. All concept classes considered are computable enumerations of computable $\Pi^0_1$ classes, in a sense made precise here. This family of concept classes is sufficient to cover all standard examples, and also has the property that PAC learnability is equivalent to finite VC dimension. version:1
arxiv-1406-1089 | A variational approach to stable principal component pursuit | http://arxiv.org/abs/1406.1089 | id:1406.1089 author:Aleksandr Aravkin, Stephen Becker, Volkan Cevher, Peder Olsen category:math.OC stat.ML  published:2014-06-04 summary:We introduce a new convex formulation for stable principal component pursuit (SPCP) to decompose noisy signals into low-rank and sparse representations. For numerical solutions of our SPCP formulation, we first develop a convex variational framework and then accelerate it with quasi-Newton methods. We show, via synthetic and real data experiments, that our approach offers advantages over the classical SPCP formulations in scalability and practical parameter selection. version:1
arxiv-1406-1167 | Learning to Diversify via Weighted Kernels for Classifier Ensemble | http://arxiv.org/abs/1406.1167 | id:1406.1167 author:Xu-Cheng Yin, Chun Yang, Hong-Wei Hao category:cs.LG cs.CV I.5  published:2014-06-04 summary:Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers' outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming. version:1
arxiv-1207-6430 | Optimal Data Collection For Informative Rankings Expose Well-Connected Graphs | http://arxiv.org/abs/1207.6430 | id:1207.6430 author:Braxton Osting, Christoph Brune, Stanley J. Osher category:stat.ML cs.LG stat.AP  published:2012-07-26 summary:Given a graph where vertices represent alternatives and arcs represent pairwise comparison data, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. Our goal in this paper is to develop a method for collecting data for which the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximizes the informativeness of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding multigraphs with large algebraic connectivity. This reduction of the data collection problem to graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. As another application, we study the 2011-12 NCAA football schedule and propose schedules with the same number of games which are significantly more informative. Using spectral clustering methods to identify highly-connected communities within the division, we argue that the NCAA could improve its notoriously poor rankings by simply scheduling more out-of-conference games. version:2
arxiv-1406-0968 | Integration of a Predictive, Continuous Time Neural Network into Securities Market Trading Operations | http://arxiv.org/abs/1406.0968 | id:1406.0968 author:Christopher S Kirk category:q-fin.CP cs.CE cs.NE  published:2014-06-04 summary:This paper describes recent development and test implementation of a continuous time recurrent neural network that has been configured to predict rates of change in securities. It presents outcomes in the context of popular technical analysis indicators and highlights the potential impact of continuous predictive capability on securities market trading operations. version:1
arxiv-1406-0728 | A Game-theoretic Machine Learning Approach for Revenue Maximization in Sponsored Search | http://arxiv.org/abs/1406.0728 | id:1406.0728 author:Di He, Wei Chen, Liwei Wang, Tie-Yan Liu category:cs.GT cs.LG  published:2014-06-03 summary:Sponsored search is an important monetization channel for search engines, in which an auction mechanism is used to select the ads shown to users and determine the prices charged from advertisers. There have been several pieces of work in the literature that investigate how to design an auction mechanism in order to optimize the revenue of the search engine. However, due to some unrealistic assumptions used, the practical values of these studies are not very clear. In this paper, we propose a novel \emph{game-theoretic machine learning} approach, which naturally combines machine learning and game theory, and learns the auction mechanism using a bilevel optimization framework. In particular, we first learn a Markov model from historical data to describe how advertisers change their bids in response to an auction mechanism, and then for any given auction mechanism, we use the learnt model to predict its corresponding future bid sequences. Next we learn the auction mechanism through empirical revenue maximization on the predicted bid sequences. We show that the empirical revenue will converge when the prediction period approaches infinity, and a Genetic Programming algorithm can effectively optimize this empirical revenue. Our experiments indicate that the proposed approach is able to produce a much more effective auction mechanism than several baselines. version:2
arxiv-1406-0946 | Beyond $χ^2$ Difference: Learning Optimal Metric for Boundary Detection | http://arxiv.org/abs/1406.0946 | id:1406.0946 author:Fei He, Shengjin Wang category:cs.CV  published:2014-06-04 summary:This letter focuses on solving the challenging problem of detecting natural image boundaries. A boundary usually refers to the border between two regions with different semantic meanings. Therefore, a measurement of dissimilarity between image regions plays a pivotal role in boundary detection of natural images. To improve the performance of boundary detection, a Learning-based Boundary Metric (LBM) is proposed to replace $\chi^2$ difference adopted by the classical algorithm mPb. Compared with $\chi^2$ difference, LBM is composed of a single layer neural network and an RBF kernel, and is fine-tuned by supervised learning rather than human-crafted. It is more effective in describing the dissimilarity between natural image regions while tolerating large variance of image data. After substituting $\chi^2$ difference with LBM, the F-measure metric of mPb on the BSDS500 benchmark is increased from 0.69 to 0.71. Moreover, when image features are computed on a single scale, the proposed LBM algorithm still achieves competitive results compared with \emph{mPb}, which makes use of multi-scale image features. version:1
arxiv-1406-0930 | ACO Implementation for Sequence Alignment with Genetic Algorithms | http://arxiv.org/abs/1406.0930 | id:1406.0930 author:Aaron Lee, Livia King category:cs.CE cs.NE  published:2014-06-04 summary:In this paper, we implement Ant Colony Optimization (ACO) for sequence alignment. ACO is a meta-heuristic recently developed for nearest neighbor approximations in large, NP-hard search spaces. Here we use a genetic algorithm approach to evolve the best parameters for an ACO designed to align two sequences. We then used the best parameters found to interpolate approximate optimal parameters for a given string length within a range. The basis of our comparison is the alignment given by the Needleman-Wunsch algorithm. We found that ACO can indeed be applied to sequence alignment. While it is computationally expensive compared to other equivalent algorithms, it is a promising algorithm that can be readily applied to a variety of other biological problems. version:1
arxiv-1406-0909 | Improvement Tracking Dynamic Programming using Replication Function for Continuous Sign Language Recognition | http://arxiv.org/abs/1406.0909 | id:1406.0909 author:S. Ildarabadi, M. Ebrahimi, H. R. Pourreza category:cs.CV  published:2014-06-04 summary:In this paper we used a Replication Function (R. F.)for improvement tracking with dynamic programming. The R. F. transforms values of gray level [0 255] to [0 1]. The resulting images of R. F. are more striking and visible in skin regions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and face. Results show that Tracking Error Rate 11% and Average Tracked Distance 7% reduced version:1
arxiv-1406-0824 | Supervised classification-based stock prediction and portfolio optimization | http://arxiv.org/abs/1406.0824 | id:1406.0824 author:Sercan Arik, Sukru Burc Eryilmaz, Adam Goldberg category:q-fin.ST cs.CE cs.LG q-fin.PM stat.ML  published:2014-06-03 summary:As the number of publicly traded companies as well as the amount of their financial data grows rapidly, it is highly desired to have tracking, analysis, and eventually stock selections automated. There have been few works focusing on estimating the stock prices of individual companies. However, many of those have worked with very small number of financial parameters. In this work, we apply machine learning techniques to address automated stock picking, while using a larger number of financial parameters for individual companies than the previous studies. Our approaches are based on the supervision of prediction parameters using company fundamentals, time-series properties, and correlation information between different stocks. We examine a variety of supervised learning techniques and found that using stock fundamentals is a useful approach for the classification problem, when combined with the high dimensional data handling capabilities of support vector machine. The portfolio our system suggests by predicting the behavior of stocks results in a 3% larger growth on average than the overall market within a 3-month time period, as the out-of-sample test suggests. version:1
arxiv-1404-0431 | Learning Latent Block Structure in Weighted Networks | http://arxiv.org/abs/1404.0431 | id:1404.0431 author:Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.SI physics.data-an physics.soc-ph  published:2014-04-02 summary:Community detection is an important task in network analysis, in which we aim to learn a network partition that groups together vertices with similar community-level connectivity patterns. By finding such groups of vertices with similar structural roles, we extract a compact representation of the network's large-scale structure, which can facilitate its scientific interpretation and the prediction of unknown or future interactions. Popular approaches, including the stochastic block model, assume edges are unweighted, which limits their utility by throwing away potentially useful information. We introduce the `weighted stochastic block model' (WSBM), which generalizes the stochastic block model to networks with edge weights drawn from any exponential family distribution. This model learns from both the presence and weight of edges, allowing it to discover structure that would otherwise be hidden when weights are discarded or thresholded. We describe a Bayesian variational algorithm for efficiently approximating this model's posterior distribution over latent block structures. We then evaluate the WSBM's performance on both edge-existence and edge-weight prediction tasks for a set of real-world weighted networks. In all cases, the WSBM performs as well or better than the best alternatives on these tasks. version:2
arxiv-1406-0680 | Visual Reranking with Improved Image Graph | http://arxiv.org/abs/1406.0680 | id:1406.0680 author:Ziqiong Liu, Shengjin Wang, Liang Zheng, Qi Tian category:cs.CV  published:2014-06-03 summary:This paper introduces an improved reranking method for the Bag-of-Words (BoW) based image search. Built on [1], a directed image graph robust to outlier distraction is proposed. In our approach, the relevance among images is encoded in the image graph, based on which the initial rank list is refined. Moreover, we show that the rank-level feature fusion can be adopted in this reranking method as well. Taking advantage of the complementary nature of various features, the reranking performance is further enhanced. Particularly, we exploit the reranking method combining the BoW and color information. Experiments on two benchmark datasets demonstrate that ourmethod yields significant improvements and the reranking results are competitive to the state-of-the-art methods. version:1
arxiv-1405-6447 | An Ordered Lasso and Sparse Time-Lagged Regression | http://arxiv.org/abs/1405.6447 | id:1405.6447 author:Xiaotong Suo, Robert Tibshirani category:stat.AP stat.ML  published:2014-05-26 summary:We consider regression scenarios where it is natural to impose an order constraint on the coefficients. We propose an order-constrained version of L1-regularized regression for this problem, and show how to solve it efficiently using the well-known Pool Adjacent Violators Algorithm as its proximal operator. The main application of this idea is time-lagged regression, where we predict an outcome at time t from features at the previous K time points. In this setting it is natural to assume that the coefficients decay as we move farther away from t, and hence the order constraint is reasonable. Potential applications include financial time series and prediction of dynamic patient out- comes based on clinical measurements. We illustrate this idea on real and simulated data. version:2
arxiv-1405-7716 | Experimental Demonstration of Array-level Learning with Phase Change Synaptic Devices | http://arxiv.org/abs/1405.7716 | id:1405.7716 author:S. Burc Eryilmaz, Duygu Kuzum, Rakesh G. D. Jeyasingh, SangBum Kim, Matthew BrightSky, Chung Lam, H. -S. Philip Wong category:cs.NE cs.AI  published:2014-05-29 summary:The computational performance of the biological brain has long attracted significant interest and has led to inspirations in operating principles, algorithms, and architectures for computing and signal processing. In this work, we focus on hardware implementation of brain-like learning in a brain-inspired architecture. We demonstrate, in hardware, that 2-D crossbar arrays of phase change synaptic devices can achieve associative learning and perform pattern recognition. Device and array-level studies using an experimental 10x10 array of phase change synaptic devices have shown that pattern recognition is robust against synaptic resistance variations and large variations can be tolerated by increasing the number of training iterations. Our measurements show that increase in initial variation from 9 % to 60 % causes required training iterations to increase from 1 to 11. version:2
arxiv-1404-1530 | Provable Deterministic Leverage Score Sampling | http://arxiv.org/abs/1404.1530 | id:1404.1530 author:Dimitris Papailiopoulos, Anastasios Kyrillidis, Christos Boutsidis category:cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH  published:2014-04-06 summary:We explain theoretically a curious empirical phenomenon: "Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate". To obtain provable guarantees, previous work requires randomized sampling of the columns with probabilities proportional to their leverage scores. In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such deterministic sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques. version:3
arxiv-1406-0554 | Universal Convexification via Risk-Aversion | http://arxiv.org/abs/1406.0554 | id:1406.0554 author:Krishnamurthy Dvijotham, Maryam Fazel, Emanuel Todorov category:cs.SY cs.LG math.OC  published:2014-06-03 summary:We develop a framework for convexifying a fairly general class of optimization problems. Under additional assumptions, we analyze the suboptimality of the solution to the convexified problem relative to the original nonconvex problem and prove additive approximation guarantees. We then develop algorithms based on stochastic gradient methods to solve the resulting optimization problems and show bounds on convergence rates. %We show a simple application of this framework to supervised learning, where one can perform integration explicitly and can use standard (non-stochastic) optimization algorithms with better convergence guarantees. We then extend this framework to apply to a general class of discrete-time dynamical systems. In this context, our convexification approach falls under the well-studied paradigm of risk-sensitive Markov Decision Processes. We derive the first known model-based and model-free policy gradient optimization algorithms with guaranteed convergence to the optimal solution. Finally, we present numerical results validating our formulation in different applications. version:1
arxiv-1406-0416 | More Bang For Your Buck: Quorum-Sensing Capabilities Improve the Efficacy of Suicidal Altruism | http://arxiv.org/abs/1406.0416 | id:1406.0416 author:Anya Elaine Johnson, Eli Strauss, Rodney Pickett, Christoph Adami, Ian Dworkin, Heather J. Goldsby category:cs.NE cs.CE q-bio.PE  published:2014-06-02 summary:Within the context of evolution, an altruistic act that benefits the receiving individual at the expense of the acting individual is a puzzling phenomenon. An extreme form of altruism can be found in colicinogenic E. coli. These suicidal altruists explode, releasing colicins that kill unrelated individuals, which are not colicin resistant. By committing suicide, the altruist makes it more likely that its kin will have less competition. The benefits of this strategy rely on the number of competitors and kin nearby. If the organism explodes at an inopportune time, the suicidal act may not harm any competitors. Communication could enable organisms to act altruistically when environmental conditions suggest that that strategy would be most beneficial. Quorum sensing is a form of communication in which bacteria produce a protein and gauge the amount of that protein around them. Quorum sensing is one means by which bacteria sense the biotic factors around them and determine when to produce products, such as antibiotics, that influence competition. Suicidal altruists could use quorum sensing to determine when exploding is most beneficial, but it is challenging to study the selective forces at work in microbes. To address these challenges, we use digital evolution (a form of experimental evolution that uses self-replicating computer programs as organisms) to investigate the effects of enabling altruistic organisms to communicate via quorum sensing. We found that quorum-sensing altruists killed a greater number of competitors per explosion, winning competitions against non-communicative altruists. These findings indicate that quorum sensing could increase the beneficial effect of altruism and the suite of conditions under which it will evolve. version:1
arxiv-1310-2418 | Linear Algorithm for Digital Euclidean Connected Skeleton | http://arxiv.org/abs/1310.2418 | id:1310.2418 author:Aurélie Leborgne, Julien Mille, Laure Tougne category:cs.CV  published:2013-10-09 summary:The skeleton is an essential shape characteristic providing a compact representation of the studied shape. Its computation on the image grid raises many issues. Due to the effects of discretization, the required properties of the skeleton - thinness, homotopy to the shape, reversibility, connectivity - may become incompatible. However, as regards practical use, the choice of a specific skeletonization algorithm depends on the application. This allows to classify the desired properties by order of importance, and tend towards the most critical ones. Our goal is to make a skeleton dedicated to shape matching for recognition. So, the discrete skeleton has to be thin - so that it can be represented by a graph -, robust to noise, reversible - so that the initial shape can be fully reconstructed - and homotopic to the shape. We propose a linear-time skeletonization algorithm based on the squared Euclidean distance map from which we extract the maximal balls and ridges. After a thinning and pruning process, we obtain the skeleton. The proposed method is finally compared to fairly recent methods. version:3
arxiv-1406-0312 | Generalized Max Pooling | http://arxiv.org/abs/1406.0312 | id:1406.0312 author:Naila Murray, Florent Perronnin category:cs.CV  published:2014-06-02 summary:State-of-the-art patch-based image representations involve a pooling operation that aggregates statistics computed from local descriptors. Standard pooling operations include sum- and max-pooling. Sum-pooling lacks discriminability because the resulting representation is strongly influenced by frequent yet often uninformative descriptors, but only weakly influenced by rare yet potentially highly-informative ones. Max-pooling equalizes the influence of frequent and rare descriptors but is only applicable to representations that rely on count statistics, such as the bag-of-visual-words (BOV) and its soft- and sparse-coding extensions. We propose a novel pooling mechanism that achieves the same effect as max-pooling but is applicable beyond the BOV and especially to the state-of-the-art Fisher Vector -- hence the name Generalized Max Pooling (GMP). It involves equalizing the similarity between each patch and the pooled representation, which is shown to be equivalent to re-weighting the per-patch statistics. We show on five public image classification benchmarks that the proposed GMP can lead to significant performance gains with respect to heuristic alternatives. version:1
arxiv-1406-0304 | Transductive Learning for Multi-Task Copula Processes | http://arxiv.org/abs/1406.0304 | id:1406.0304 author:Markus Schneider, Fabio Ramos category:cs.LG stat.ML  published:2014-06-02 summary:We tackle the problem of multi-task learning with copula process. Multivariable prediction in spatial and spatial-temporal processes such as natural resource estimation and pollution monitoring have been typically addressed using techniques based on Gaussian processes and co-Kriging. While the Gaussian prior assumption is convenient from analytical and computational perspectives, nature is dominated by non-Gaussian likelihoods. Copula processes are an elegant and flexible solution to handle various non-Gaussian likelihoods by capturing the dependence structure of random variables with cumulative distribution functions rather than their marginals. We show how multi-task learning for copula processes can be used to improve multivariable prediction for problems where the simple Gaussianity prior assumption does not hold. Then, we present a transductive approximation for multi-task learning and derive analytical expressions for the copula process model. The approach is evaluated and compared to other techniques in one artificial dataset and two publicly available datasets for natural resource estimation and concrete slump prediction. version:1
arxiv-1406-0289 | The constitution of visual perceptual units in the functional architecture of V1 | http://arxiv.org/abs/1406.0289 | id:1406.0289 author:Alessandro Sarti, Giovanna Citti category:cs.CV  published:2014-06-02 summary:Scope of this paper is to consider a mean field neural model which takes into account the functional neurogeometry of the visual cortex modelled as a group of rotations and translations. The model generalizes well known results of Bressloff and Cowan which, in absence of input, accounts for hallucination patterns. The main result of our study consists in showing that in presence of a visual input, the eigenmodes of the linearized operator which become stable represent perceptual units present in the image. The result is strictly related to dimensionality reduction and clustering problems. version:1
arxiv-1406-0288 | Continuous Action Recognition Based on Sequence Alignment | http://arxiv.org/abs/1406.0288 | id:1406.0288 author:Kaustubh Kulkarni, Georgios Evangelidis, Jan Cech, Radu Horaud category:cs.CV  published:2014-06-02 summary:Continuous action recognition is more challenging than isolated recognition because classification and segmentation must be simultaneously carried out. We build on the well known dynamic time warping (DTW) framework and devise a novel visual alignment technique, namely dynamic frame warping (DFW), which performs isolated recognition based on per-frame representation of videos, and on aligning a test sequence with a model sequence. Moreover, we propose two extensions which enable to perform recognition concomitant with segmentation, namely one-pass DFW and two-pass DFW. These two methods have their roots in the domain of continuous recognition of speech and, to the best of our knowledge, their extension to continuous visual action recognition has been overlooked. We test and illustrate the proposed techniques with a recently released dataset (RAVEL) and with two public-domain datasets widely used in action recognition (Hollywood-1 and Hollywood-2). We also compare the performances of the proposed isolated and continuous recognition algorithms with several recently published methods. version:1
arxiv-1406-0231 | Ambiguous Proximity Distribution | http://arxiv.org/abs/1406.0231 | id:1406.0231 author:Quanquan Wang, Yongping Li category:cs.CV  published:2014-06-02 summary:Proximity Distribution Kernel is an effective method for bag-of-featues based image representation. In this paper, we investigate the soft assignment of visual words to image features for proximity distribution. Visual word contribution function is proposed to model ambiguous proximity distributions. Three ambiguous proximity distributions is developed by three ambiguous contribution functions. The experiments are conducted on both classification and retrieval of medical image data sets. The results show that the performance of the proposed methods, Proximity Distribution Kernel (PDK), is better or comparable to the state-of-the-art bag-of-features based image representation methods. version:1
arxiv-1406-0223 | Holistic Measures for Evaluating Prediction Models in Smart Grids | http://arxiv.org/abs/1406.0223 | id:1406.0223 author:Saima Aman, Yogesh Simmhan, Viktor K. Prasanna category:cs.LG  published:2014-06-02 summary:The performance of prediction models is often based on "abstract metrics" that estimate the model's ability to limit residual errors between the observed and predicted values. However, meaningful evaluation and selection of prediction models for end-user domains requires holistic and application-sensitive performance measures. Inspired by energy consumption prediction models used in the emerging "big data" domain of Smart Power Grids, we propose a suite of performance measures to rationally compare models along the dimensions of scale independence, reliability, volatility and cost. We include both application independent and dependent measures, the latter parameterized to allow customization by domain experts to fit their scenario. While our measures are generalizable to other domains, we offer an empirical analysis using real energy use data for three Smart Grid applications: planning, customer education and demand response, which are relevant for energy sustainability. Our results underscore the value of the proposed measures to offer a deeper insight into models' behavior and their impact on real applications, which benefit both data mining researchers and practitioners. version:1
arxiv-1406-0214 | Topological and Statistical Behavior Classifiers for Tracking Applications | http://arxiv.org/abs/1406.0214 | id:1406.0214 author:Paul Bendich, Sang Chin, Jesse Clarke, Jonathan deSena, John Harer, Elizabeth Munch, Andrew Newman, David Porter, David Rouse, Nate Strawn, Adam Watkins category:cs.SY math.AT stat.ML  published:2014-06-01 summary:We introduce the first unified theory for target tracking using Multiple Hypothesis Tracking, Topological Data Analysis, and machine learning. Our string of innovations are 1) robust topological features are used to encode behavioral information, 2) statistical models are fitted to distributions over these topological features, and 3) the target type classification methods of Wigren and Bar Shalom et al. are employed to exploit the resulting likelihoods for topological features inside of the tracking procedure. To demonstrate the efficacy of our approach, we test our procedure on synthetic vehicular data generated by the Simulation of Urban Mobility package. version:1
arxiv-1406-0193 | Inference of Sparse Networks with Unobserved Variables. Application to Gene Regulatory Networks | http://arxiv.org/abs/1406.0193 | id:1406.0193 author:Nikolai Slavov category:stat.ML cs.LG q-bio.MN q-bio.QM stat.AP  published:2014-06-01 summary:Networks are a unifying framework for modeling complex systems and network inference problems are frequently encountered in many fields. Here, I develop and apply a generative approach to network inference (RCweb) for the case when the network is sparse and the latent (not observed) variables affect the observed ones. From all possible factor analysis (FA) decompositions explaining the variance in the data, RCweb selects the FA decomposition that is consistent with a sparse underlying network. The sparsity constraint is imposed by a novel method that significantly outperforms (in terms of accuracy, robustness to noise, complexity scaling, and computational efficiency) Bayesian methods and MLE methods using l1 norm relaxation such as K-SVD and l1--based sparse principle component analysis (PCA). Results from simulated models demonstrate that RCweb recovers exactly the model structures for sparsity as low (as non-sparse) as 50% and with ratio of unobserved to observed variables as high as 2. RCweb is robust to noise, with gradual decrease in the parameter ranges as the noise level increases. version:1
arxiv-1406-0189 | Convex Total Least Squares | http://arxiv.org/abs/1406.0189 | id:1406.0189 author:Dmitry Malioutov, Nikolai Slavov category:stat.ML cs.LG q-bio.GN q-bio.QM stat.AP  published:2014-06-01 summary:We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identification and econometrics. The special case when all dependent and independent variables have the same level of uncorrelated Gaussian noise, known as ordinary TLS, can be solved by singular value decomposition (SVD). However, SVD cannot solve many important practical TLS problems with realistic noise structure, such as having varying measurement noise, known structure on the errors, or large outliers requiring robust error-norms. To solve such problems, we develop convex relaxation approaches for a general class of structured TLS (STLS). We show both theoretically and experimentally, that while the plain nuclear norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear norm approach is very effective, and achieves better accuracy on challenging STLS problems than popular non-convex solvers. We describe a fast solution based on augmented Lagrangian formulation, and apply our approach to an important class of biological problems that use population average measurements to infer cell-type and physiological-state specific expression levels that are very hard to measure directly. version:1
arxiv-1406-0175 | Evolutionary Search in the Space of Rules for Creation of New Two-Player Board Games | http://arxiv.org/abs/1406.0175 | id:1406.0175 author:Zahid Halim category:cs.NE cs.AI  published:2014-06-01 summary:Games have always been a popular test bed for artificial intelligence techniques. Game developers are always in constant search for techniques that can automatically create computer games minimizing the developer's task. In this work we present an evolutionary strategy based solution towards the automatic generation of two player board games. To guide the evolutionary process towards games, which are entertaining, we propose a set of metrics. These metrics are based upon different theories of entertainment in computer games. This work also compares the entertainment value of the evolved games with the existing popular board based games. Further to verify the entertainment value of the evolved games with the entertainment value of the human user a human user survey is conducted. In addition to the user survey we check the learnability of the evolved games using an artificial neural network based controller. The proposed metrics and the evolutionary process can be employed for generating new and entertaining board games, provided an initial search space is given to the evolutionary algorithm. version:1
arxiv-1406-0132 | Seeing the Big Picture: Deep Embedding with Contextual Evidences | http://arxiv.org/abs/1406.0132 | id:1406.0132 author:Liang Zheng, Shengjin Wang, Fei He, Qi Tian category:cs.CV  published:2014-06-01 summary:In the Bag-of-Words (BoW) model based image retrieval task, the precision of visual matching plays a critical role in improving retrieval performance. Conventionally, local cues of a keypoint are employed. However, such strategy does not consider the contextual evidences of a keypoint, a problem which would lead to the prevalence of false matches. To address this problem, this paper defines "true match" as a pair of keypoints which are similar on three levels, i.e., local, regional, and global. Then, a principled probabilistic framework is established, which is capable of implicitly integrating discriminative cues from all these feature levels. Specifically, the Convolutional Neural Network (CNN) is employed to extract features from regional and global patches, leading to the so-called "Deep Embedding" framework. CNN has been shown to produce excellent performance on a dozen computer vision tasks such as image classification and detection, but few works have been done on BoW based image retrieval. In this paper, firstly we show that proper pre-processing techniques are necessary for effective usage of CNN feature. Then, in the attempt to fit it into our model, a novel indexing structure called "Deep Indexing" is introduced, which dramatically reduces memory usage. Extensive experiments on three benchmark datasets demonstrate that, the proposed Deep Embedding method greatly promotes the retrieval accuracy when CNN feature is integrated. We show that our method is efficient in terms of both memory and time cost, and compares favorably with the state-of-the-art methods. version:1
arxiv-1405-1439 | A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication | http://arxiv.org/abs/1405.1439 | id:1405.1439 author:Chenhao Tan, Lillian Lee category:cs.CL  published:2014-05-06 summary:The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task. version:2
arxiv-1406-0118 | Improved graph Laplacian via geometric self-consistency | http://arxiv.org/abs/1406.0118 | id:1406.0118 author:Dominique Perrault-Joncas, Marina Meila category:stat.ML cs.LG  published:2014-05-31 summary:We address the problem of setting the kernel bandwidth used by Manifold Learning algorithms to construct the graph Laplacian. Exploiting the connection between manifold geometry, represented by the Riemannian metric, and the Laplace-Beltrami operator, we set the bandwidth by optimizing the Laplacian's ability to preserve the geometry of the data. Experiments show that this principled approach is effective and robust. version:1
arxiv-1406-0079 | Bridging the gap between Legal Practitioners and Knowledge Engineers using semi-formal KR | http://arxiv.org/abs/1406.0079 | id:1406.0079 author:Shashishekar Ramakrishna, Adrian Paschke category:cs.CL cs.AI K.6.3; D.2.5; F.4.1  published:2014-05-31 summary:The use of Structured English as a computation independent knowledge representation format for non-technical users in business rules representation has been proposed in OMGs Semantics and Business Vocabulary Representation (SBVR). In the legal domain we face a similar problem. Formal representation languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2 ontologies etc.) support the technical knowledge engineer and the automated reasoning. But, they can be hardly used directly by the legal domain experts who do not have a computer science background. In this paper we adapt the SBVR Structured English approach for the legal domain and implement a proof-of-concept, called KR4IPLaw, which enables legal domain experts to represent their knowledge in Structured English in a computational independent and hence, for them, more usable way. The benefit of this approach is that the underlying pre-defined semantics of the Structured English approach makes transformations into formal languages such as OASIS LegalRuleML and OWL2 ontologies possible. We exemplify our approach in the domain of patent law. version:1
arxiv-1406-0074 | Combined Approach for Image Segmentation | http://arxiv.org/abs/1406.0074 | id:1406.0074 author:Shradha Dakhare, Harshal Chowhan, Manoj B. Chandak category:cs.CV  published:2014-05-31 summary:Many image segmentation techniques have been developed over the past two decades for segmenting the images, which help for object recognition, occlusion boundary estimation within motion or stereo systems, image compression, image editing. In this, there is a combined approach for segmenting the image. By using histogram equalization to the input image, from which it gives contrast enhancement output image .After that by applying median filtering,which will remove noise from contrast output image . At last I applied fuzzy c-mean clustering algorithm to denoising output image, which give segmented output image. In this way it produce better segmented image with less computation time. version:1
arxiv-1406-0071 | Adaptive Reconfiguration Moves for Dirichlet Mixtures | http://arxiv.org/abs/1406.0071 | id:1406.0071 author:Tue Herlau, Morten Mørup, Yee Whye Teh, Mikkel N. Schmidt category:stat.ML  published:2014-05-31 summary:Bayesian mixture models are widely applied for unsupervised learning and exploratory data analysis. Markov chain Monte Carlo based on Gibbs sampling and split-merge moves are widely used for inference in these models. However, both methods are restricted to limited types of transitions and suffer from torpid mixing and low accept rates even for problems of modest size. We propose a method that considers a broader range of transitions that are close to equilibrium by exploiting multiple chains in parallel and using the past states adaptively to inform the proposal distribution. The method significantly improves on Gibbs and split-merge sampling as quantified using convergence diagnostics and acceptance rates. Adaptive MCMC methods which use past states to inform the proposal distribution has given rise to many ingenious sampling schemes for continuous problems and the present work can be seen as an important first step in bringing these benefits to partition-based problems version:1
arxiv-1406-0032 | Comparing and Combining Sentiment Analysis Methods | http://arxiv.org/abs/1406.0032 | id:1406.0032 author:Pollyanna Gonçalves, Matheus Araújo, Fabrício Benevenuto, Meeyoung Cha category:cs.CL  published:2014-05-30 summary:Several messages express opinions about events, products, and services, political views or even their author's emotional state and mood. Sentiment analysis has been used in several applications including analysis of the repercussions of events in social networks, analysis of opinions about products and services, and simply to better understand aspects of social communication in Online Social Networks (OSNs). There are multiple methods for measuring sentiments, including lexical-based approaches and supervised machine learning methods. Despite the wide use and popularity of some methods, it is unclear which method is better for identifying the polarity (i.e., positive or negative) of a message as the current literature does not provide a method of comparison among existing methods. Such a comparison is crucial for understanding the potential limitations, advantages, and disadvantages of popular methods in analyzing the content of OSNs messages. Our study aims at filling this gap by presenting comparisons of eight popular sentiment analysis methods in terms of coverage (i.e., the fraction of messages whose sentiment is identified) and agreement (i.e., the fraction of identified sentiments that are in tune with ground truth). We develop a new method that combines existing approaches, providing the best coverage results and competitive agreement. We also present a free Web service called iFeel, which provides an open API for accessing and comparing results across different sentiment methods for a given text. version:1
arxiv-1406-0023 | Circle detection using electro-magnetism optimization | http://arxiv.org/abs/1406.0023 | id:1406.0023 author:Erik Cuevas, Diego Oliva, Daniel Zaldivar, Marco Perez-Cisneros, Humberto Sossa category:cs.CV  published:2014-05-30 summary:This paper describes a circle detection method based on Electromagnetism-Like Optimization (EMO). Circle detection has received considerable attention over the last years thanks to its relevance for many computer vision tasks. EMO is a heuristic method for solving complex optimization problems inspired in electromagnetism principles. This algorithm searches a solution based in the attraction and repulsion among prototype candidates. In this paper the detection process is considered to be similar to an optimization problem, the algorithm uses the combination of three edge points (x, y, r) as parameters to determine circles candidates in the scene. An objective function determines if such circle candidates are actually present in the image. The EMO algorithm is used to find the circle candidate that is better related with the real circle present in the image according to the objective function. The final algorithm is a fast circle detector that locates circles with sub-pixel accuracy even considering complicated conditions and noisy images. version:1
arxiv-1406-0013 | Estimating Vector Fields on Manifolds and the Embedding of Directed Graphs | http://arxiv.org/abs/1406.0013 | id:1406.0013 author:Dominique Perrault-Joncas, Marina Meila category:stat.ML cs.LG  published:2014-05-30 summary:This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model a directed graph as a finite set of observations from a diffusion on a manifold endowed with a vector field. This is the first generative model of its kind for directed graphs. We introduce a graph embedding algorithm that estimates all three features of this model: the low-dimensional embedding of the manifold, the data density and the vector field. In the process, we also obtain new theoretical results on the limits of "Laplacian type" matrices derived from directed graphs. The application of our method to both artificially constructed and real data highlights its strengths. version:1
arxiv-1311-2520 | The Infinite Degree Corrected Stochastic Block Model | http://arxiv.org/abs/1311.2520 | id:1311.2520 author:Tue Herlau, Mikkel N. Schmidt, Morten Mørup category:stat.ML  published:2013-11-11 summary:In Stochastic blockmodels, which are among the most prominent statistical models for cluster analysis of complex networks, clusters are defined as groups of nodes with statistically similar link probabilities within and between groups. A recent extension by Karrer and Newman incorporates a node degree correction to model degree heterogeneity within each group. Although this demonstrably leads to better performance on several networks it is not obvious whether modelling node degree is always appropriate or necessary. We formulate the degree corrected stochastic blockmodel as a non-parametric Bayesian model, incorporating a parameter to control the amount of degree correction which can then be inferred from data. Additionally, our formulation yields principled ways of inferring the number of groups as well as predicting missing links in the network which can be used to quantify the model's predictive performance. On synthetic data we demonstrate that including the degree correction yields better performance both on recovering the true group structure and predicting missing links when degree heterogeneity is present, whereas performance is on par for data with no degree heterogeneity within clusters. On seven real networks (with no ground truth group structure available) we show that predictive performance is about equal whether or not degree correction is included; however, for some networks significantly fewer clusters are discovered when correcting for degree indicating that the data can be more compactly explained by clusters of heterogenous degree nodes. version:3
arxiv-1405-7908 | Semantic Composition and Decomposition: From Recognition to Generation | http://arxiv.org/abs/1405.7908 | id:1405.7908 author:Peter D. Turney category:cs.CL cs.AI cs.LG H.3.1; I.2.6; I.2.7  published:2014-05-30 summary:Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram ("red salmon"), generate a noun unigram that is synonymous with the given bigram ("sockeye"). A test for semantic decomposition is, given a context vector for a noun unigram ("snifter"), generate a noun-modifier bigram that is synonymous with the given unigram ("brandy glass"). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time. version:1
arxiv-1405-7903 | The Shortlist Method for Fast Computation of the Earth Mover's Distance and Finding Optimal Solutions to Transportation Problems | http://arxiv.org/abs/1405.7903 | id:1405.7903 author:Carsten Gottschlich, Dominic Schuhmacher category:cs.CV  published:2014-05-30 summary:Finding solutions to the classical transportation problem is of great importance, since this optimization problem arises in many engineering and computer science applications. Especially the Earth Mover's Distance is used in a plethora of applications ranging from content-based image retrieval, shape matching, fingerprint recognition, object tracking and phishing web page detection to computing color differences in linguistics and biology. Our starting point is the well-known revised simplex algorithm, which iteratively improves a feasible solution to optimality. The Shortlist Method that we propose substantially reduces the number of candidates inspected for improving the solution, while at the same time balancing the number of pivots required. Tests on simulated benchmarks demonstrate a considerable reduction in computation time for the new method as compared to the usual revised simplex algorithm implemented with state-of-the-art initialization and pivot strategies. As a consequence, the Shortlist Method facilitates the computation of large scale transportation problems in viable time. In addition we describe a novel method for finding an initial feasible solution which we coin Modified Russell's Method. version:1
arxiv-1405-7897 | Flip-Flop Sublinear Models for Graphs: Proof of Theorem 1 | http://arxiv.org/abs/1405.7897 | id:1405.7897 author:Brijnesh Jain category:cs.LG  published:2014-05-30 summary:We prove that there is no class-dual for almost all sublinear models on graphs. version:1
arxiv-1401-4082 | Stochastic Backpropagation and Approximate Inference in Deep Generative Models | http://arxiv.org/abs/1401.4082 | id:1401.4082 author:Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra category:stat.ML cs.AI cs.LG stat.CO stat.ME  published:2014-01-16 summary:We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation. version:3
arxiv-1405-7780 | ELM Solutions for Event-Based Systems | http://arxiv.org/abs/1405.7780 | id:1405.7780 author:Jonathan Tapson, André van Schaik category:cs.NE  published:2014-05-30 summary:Whilst most engineered systems use signals that are continuous in time, there is a domain of systems in which signals consist of events. Events, like Dirac delta functions, have no meaningful time duration. Many important real-world systems are intrinsically event-based, including the mammalian brain, in which the primary packets of data are spike events, or action potentials. In this domain, signal processing requires responses to spatio-temporal patterns of events. We show that some straightforward modifications to the standard ELM topology produce networks that are able to perform spatio-temporal event processing online with a high degree of accuracy. The modifications involve the re-definition of hidden layer units as synaptic kernels, in which the input delta functions are transformed into continuous-valued signals using a variety of impulse-response functions. This permits the use of linear solution methods in the output layer, which can produce events as output, if modeled as a classifier; the output classes are 'event' or 'no event'. We illustrate the method in application to a spike-processing problem. version:1
arxiv-1405-7777 | Online and Adaptive Pseudoinverse Solutions for ELM Weights | http://arxiv.org/abs/1405.7777 | id:1405.7777 author:André van Schaik, Jonathan Tapson category:cs.NE  published:2014-05-30 summary:The ELM method has become widely used for classification and regressions problems as a result of its accuracy, simplicity and ease of use. The solution of the hidden layer weights by means of a matrix pseudoinverse operation is a significant contributor to the utility of the method; however, the conventional calculation of the pseudoinverse by means of a singular value decomposition (SVD) is not always practical for large data sets or for online updates to the solution. In this paper we discuss incremental methods for solving the pseudoinverse which are suitable for ELM. We show that careful choice of methods allows us to optimize for accuracy, ease of computation, or adaptability of the solution. version:1
arxiv-1405-7771 | DEM Registration and Error Analysis using ASCII values | http://arxiv.org/abs/1405.7771 | id:1405.7771 author:Suma Dawn, Vikas Saxena, Bhu Dev Sharma category:cs.CV  published:2014-05-30 summary:Digital Elevation Model (DEM), while providing a bare earth look, is heavily used in many applications including construction modeling, visualization, and GIS. Their registration techniques have not been explored much. Methods like Coarse-to-fine or pyramid making are common in DEM-to-image or DEM-to-map registration. Self-consistency measure is used to detect any change in terrain elevation and hence was used for DEM-to-DEM registration. But these methods apart from being time and complexity intensive, lack in error matrix evaluation. This paper gives a method of registration of DEMs using specified height values as control points by initially converting these DEMs to ASCII files. These control points may be found by two mannerisms - either by direct detection of appropriate height data in ASCII files or by edge matching along congruous quadrangle of the control point, followed by sub-graph matching. Error analysis for the same has also been done. version:1
arxiv-1310-6319 | Efficient State-Space Inference of Periodic Latent Force Models | http://arxiv.org/abs/1310.6319 | id:1310.6319 author:Steven Reece, Stephen Roberts, Siddhartha Ghosh, Alex Rogers, Nicholas Jennings category:stat.ML  published:2013-10-23 summary:Latent force models (LFM) are principled approaches to incorporating solutions to differential equations within non-parametric inference methods. Unfortunately, the development and application of LFMs can be inhibited by their computational cost, especially when closed-form solutions for the LFM are unavailable, as is the case in many real world problems where these latent forces exhibit periodic behaviour. Given this, we develop a new sparse representation of LFMs which considerably improves their computational efficiency, as well as broadening their applicability, in a principled way, to domains with periodic or near periodic latent forces. Our approach uses a linear basis model to approximate one generative model for each periodic force. We assume that the latent forces are generated from Gaussian process priors and develop a linear basis model which fully expresses these priors. We apply our approach to model the thermal dynamics of domestic buildings and show that it is effective at predicting day-ahead temperatures within the homes. We also apply our approach within queueing theory in which quasi-periodic arrival rates are modelled as latent forces. In both cases, we demonstrate that our approach can be implemented efficiently using state-space methods which encode the linear dynamic systems via LFMs. Further, we show that state estimates obtained using periodic latent force models can reduce the root mean squared error to 17% of that from non-periodic models and 27% of the nearest rival approach which is the resonator model. version:2
arxiv-1405-7626 | Classification of Basmati Rice Grain Variety using Image Processing and Principal Component Analysis | http://arxiv.org/abs/1405.7626 | id:1405.7626 author:Rubi Kambo, Amit Yerpude category:cs.CV  published:2014-05-29 summary:All important decisions about the variety of rice grain end product are based on the different features of rice grain.There are various methods available for classification of basmati rice. This paper proposed a new principal component analysis based approach for classification of different variety of basmati rice. The experimental result shows the effectiveness of the proposed methodology for various samples of different variety of basmati rice. version:1
arxiv-1405-7624 | Simultaneous Feature and Expert Selection within Mixture of Experts | http://arxiv.org/abs/1405.7624 | id:1405.7624 author:Billy Peralta category:cs.LG  published:2014-05-29 summary:A useful strategy to deal with complex classification scenarios is the "divide and conquer" approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using $L1$ regularization, with a simultaneous expert selection. The experiments are still pending. version:1
arxiv-1405-7569 | Functional Gaussian processes for regression with linear PDE models | http://arxiv.org/abs/1405.7569 | id:1405.7569 author:Ngoc-Cuong Nguyen, Jaime Peraire category:math.AP math.PR stat.CO stat.ML  published:2014-05-29 summary:In this paper, we present a new statistical approach to the problem of incorporating experimental observations into a mathematical model described by linear partial differential equations (PDEs) to improve the prediction of the state of a physical system. We augment the linear PDE with a functional that accounts for the uncertainty in the mathematical model and is modeled as a {\em Gaussian process}. This gives rise to a stochastic PDE which is characterized by the Gaussian functional. We develop a {\em functional Gaussian process regression} method to determine the posterior mean and covariance of the Gaussian functional, thereby solving the stochastic PDE to obtain the posterior distribution for our prediction of the physical state. Our method has the following features which distinguish itself from other regression methods. First, it incorporates both the mathematical model and the observations into the regression procedure. Second, it can handle the observations given in the form of linear functionals of the field variable. Third, the method is non-parametric in the sense that it provides a systematic way to optimally determine the prior covariance operator of the Gaussian functional based on the observations. Fourth, it provides the posterior distribution quantifying the magnitude of uncertainty in our prediction of the physical state. We present numerical results to illustrate these features of the method and compare its performance to that of the standard Gaussian process regression. version:1
arxiv-1405-7545 | Feature sampling and partitioning for visual vocabulary generation on large action classification datasets | http://arxiv.org/abs/1405.7545 | id:1405.7545 author:Michael Sapienza, Fabio Cuzzolin, Philip H. S. Torr category:cs.CV  published:2014-05-29 summary:The recent trend in action recognition is towards larger datasets, an increasing number of action classes and larger visual vocabularies. State-of-the-art human action classification in challenging video data is currently based on a bag-of-visual-words pipeline in which space-time features are aggregated globally to form a histogram. The strategies chosen to sample features and construct a visual vocabulary are critical to performance, in fact often dominating performance. In this work we provide a critical evaluation of various approaches to building a vocabulary and show that good practises do have a significant impact. By subsampling and partitioning features strategically, we are able to achieve state-of-the-art results on 5 major action recognition datasets using relatively small visual vocabularies. version:1
arxiv-1405-7519 | Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value | http://arxiv.org/abs/1405.7519 | id:1405.7519 author:Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi category:cs.IR cs.CL  published:2014-05-29 summary:Opinion Mining and Sentiment Analysis is a process of identifying opinions in large unstructured/structured data and then analysing polarity of those opinions. Opinion mining and sentiment analysis have found vast application in analysing online ratings, analysing product based reviews, e-governance, and managing hostile content over the internet. This paper proposes an algorithm to implement aspect level sentiment analysis. The algorithm takes input from the remarks submitted by various teachers of a student. An aspect tree is formed which has various levels and weights are assigned to each branch to identify level of aspect. Aspect value is calculated by the algorithm by means of the proposed aspect tree. Dictionary based method is implemented to evaluate the polarity of the remark. The algorithm returns the aspect value clubbed with opinion value and sentiment value which helps in concluding the summarized value of remark. version:1
arxiv-1405-7471 | Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab | http://arxiv.org/abs/1405.7471 | id:1405.7471 author:Mr. Dibya Jyoti Bora, Dr. Anil Kumar Gupta category:cs.LG  published:2014-05-29 summary:K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown. version:1
arxiv-1405-7460 | Universal Compression of Envelope Classes: Tight Characterization via Poisson Sampling | http://arxiv.org/abs/1405.7460 | id:1405.7460 author:Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha Suresh category:cs.IT cs.LG math.IT  published:2014-05-29 summary:The Poisson-sampling technique eliminates dependencies among symbol appearances in a random sequence. It has been used to simplify the analysis and strengthen the performance guarantees of randomized algorithms. Applying this method to universal compression, we relate the redundancies of fixed-length and Poisson-sampled sequences, use the relation to derive a simple single-letter formula that approximates the redundancy of any envelope class to within an additive logarithmic term. As a first application, we consider i.i.d. distributions over a small alphabet as a step-envelope class, and provide a short proof that determines the redundancy of discrete distributions over a small al- phabet up to the first order terms. We then show the strength of our method by applying the formula to tighten the existing bounds on the redundancy of exponential and power-law classes, in particular answering a question posed by Boucheron, Garivier and Gassiat. version:1
arxiv-1310-5035 | Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning | http://arxiv.org/abs/1310.5035 | id:1310.5035 author:Zhouchen Lin, Risheng Liu, Huan Li category:cs.NA cs.LG math.OC stat.ML  published:2013-10-18 summary:Many problems in machine learning and other fields can be (re)for-mulated as linearly constrained separable convex programs. In most of the cases, there are multiple blocks of variables. However, the traditional alternating direction method (ADM) and its linearized version (LADM, obtained by linearizing the quadratic penalty term) are for the two-block case and cannot be naively generalized to solve the multi-block case. So there is great demand on extending the ADM based methods for the multi-block case. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-block separable convex programs efficiently. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions} for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, with refined parameter estimation we devise a practical version of LADMPSAP for faster convergence. Finally, we generalize LADMPSAP to handle programs with more difficult objective functions by linearizing part of the objective function as well. LADMPSAP is particularly suitable for sparse representation and low-rank recovery problems because its subproblems have closed form solutions and the sparsity and low-rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. Numerical experiments testify to the advantages of LADMPSAP in speed and numerical accuracy. version:2
arxiv-1405-7430 | BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits | http://arxiv.org/abs/1405.7430 | id:1405.7430 author:Ruben Martinez-Cantin category:cs.LG  published:2014-05-29 summary:BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave. version:1
arxiv-1405-7406 | A Comparison of Nature Inspired Algorithms for Multi-threshold Image Segmentation | http://arxiv.org/abs/1405.7406 | id:1405.7406 author:Valentín Osuna-Enciso, Erik Cuevas, Humberto Sossa category:cs.CV cs.NE  published:2014-05-28 summary:In the field of image analysis, segmentation is one of the most important preprocessing steps. One way to achieve segmentation is by mean of threshold selection, where each pixel that belongs to a determined class islabeled according to the selected threshold, giving as a result pixel groups that share visual characteristics in the image. Several methods have been proposed in order to solve threshold selectionproblems; in this work, it is used the method based on the mixture of Gaussian functions to approximate the 1D histogram of a gray level image and whose parameters are calculated using three nature inspired algorithms (Particle Swarm Optimization, Artificial Bee Colony Optimization and Differential Evolution). Each Gaussian function approximates thehistogram, representing a pixel class and therefore a threshold point. Experimental results are shown, comparing in quantitative and qualitative fashion as well as the main advantages and drawbacks of each algorithm, applied to multi-threshold problem. version:1
arxiv-1405-7397 | An HMM Based Named Entity Recognition System for Indian Languages: The JU System at ICON 2013 | http://arxiv.org/abs/1405.7397 | id:1405.7397 author:Vivekananda Gayen, Kamal Sarkar category:cs.CL  published:2014-05-28 summary:This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named Entity Recognition. We submitted runs for Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of 0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively. version:1
arxiv-1308-4189 | Seeing What You're Told: Sentence-Guided Activity Recognition In Video | http://arxiv.org/abs/1308.4189 | id:1308.4189 author:N. Siddharth, Andrei Barbu, Jeffrey Mark Siskind category:cs.CV cs.AI cs.CL  published:2013-08-19 summary:We present a system that demonstrates how the compositional structure of events, in concert with the compositional structure of language, can interplay with the underlying focusing mechanisms in video action recognition, thereby providing a medium, not only for top-down and bottom-up integration, but also for multi-modal integration between vision and language. We show how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) in the form of whole sentential descriptions mediated by a grammar, guides the activity-recognition process. Further, the utility and expressiveness of our framework is demonstrated by performing three separate tasks in the domain of multi-activity videos: sentence-guided focus of attention, generation of sentential descriptions of video, and query-based video search, simply by leveraging the framework in different manners. version:2
arxiv-1405-7362 | Circle detection using Discrete Differential Evolution Optimization | http://arxiv.org/abs/1405.7362 | id:1405.7362 author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Marte Ramirez category:cs.CV  published:2014-05-28 summary:This paper introduces a circle detection method based on Differential Evolution (DE) optimization. Just as circle detection has been lately considered as a fundamental component for many computer vision algorithms, DE has evolved as a successful heuristic method for solving complex optimization problems, still keeping a simple structure and an easy implementation. It has also shown advantageous convergence properties and remarkable robustness. The detection process is considered similar to a combinational optimization problem. The algorithm uses the combination of three edge points as parameters to determine circles candidates in the scene yielding a reduction of the search space. The objective function determines if some circle candidates are actually present in the image. This paper focuses particularly on one DE-based algorithm known as the Discrete Differential Evolution (DDE), which eventually has shown better results than the original DE in particular for solving combinatorial problems. In the DDE, suitable conversion routines are incorporated into the DE, aiming to operate from integer values to real values and then getting integer values back, following the crossover operation. The final algorithm is a fast circle detector that locates circles with sub-pixel accuracy even considering complicated conditions and noisy images. Experimental results on several synthetic and natural images with varying range of complexity validate the efficiency of the proposed technique considering accuracy, speed, and robustness. version:1
arxiv-1312-6190 | Adaptive Feature Ranking for Unsupervised Transfer Learning | http://arxiv.org/abs/1312.6190 | id:1312.6190 author:Son N. Tran, Artur d'Avila Garcez category:cs.LG  published:2013-12-21 summary:Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain, and it works with unsupervised learning and knowledge-based transfer. version:2
arxiv-1404-6312 | Reconstructing Native Language Typology from Foreign Language Usage | http://arxiv.org/abs/1404.6312 | id:1404.6312 author:Yevgeni Berzak, Roi Reichart, Boris Katz category:cs.CL  published:2014-04-25 summary:Linguists and psychologists have long been studying cross-linguistic transfer, the influence of native language properties on linguistic performance in a foreign language. In this work we provide empirical evidence for this process in the form of a strong correlation between language similarities derived from structural features in English as Second Language (ESL) texts and equivalent similarities obtained from the typological features of the native languages. We leverage this finding to recover native language typological similarity structure directly from ESL text, and perform prediction of typological features in an unsupervised fashion with respect to the target languages. Our method achieves 72.2% accuracy on the typology prediction task, a result that is highly competitive with equivalent methods that rely on typological resources. version:2
arxiv-1405-7361 | Seeking multi-thresholds for image segmentation with Learning Automata | http://arxiv.org/abs/1405.7361 | id:1405.7361 author:Erik Cuevas, Daniel Zaldivar, Marco Perez category:cs.CV  published:2014-05-28 summary:This paper explores the use of the Learning Automata (LA) algorithm to compute threshold selection for image segmentation as it is a critical preprocessing step for image analysis, pattern recognition and computer vision. LA is a heuristic method which is able to solve complex optimization problems with interesting results in parameter estimation. Despite other techniques commonly seek through the parameter map, LA explores in the probability space providing appropriate convergence properties and robustness. The segmentation task is therefore considered as an optimization problem and the LA is used to generate the image multi-threshold separation. In this approach, one 1D histogram of a given image is approximated through a Gaussian mixture model whose parameters are calculated using the LA algorithm. Each Gaussian function approximating the histogram represents a pixel class and therefore a threshold point. The method shows fast convergence avoiding the typical sensitivity to initial conditions such as the Expectation Maximization (EM) algorithm or the complex time-consuming computations commonly found in gradient methods. Experimental results demonstrate the algorithm ability to perform automatic multi-threshold selection and show interesting advantages as it is compared to other algorithms solving the same task. version:1
arxiv-1405-7242 | Circle detection by Harmony Search Optimization | http://arxiv.org/abs/1405.7242 | id:1405.7242 author:Erik Cuevas, Noe Ortega, Daniel Zaldivar, Marco Perez category:cs.CV  published:2014-05-28 summary:Automatic circle detection in digital images has received considerable attention over the last years in computer vision as several efforts have aimed for an optimal circle detector. This paper presents an algorithm for automatic detection of circular shapes that considers the overall process as an optimization problem. The approach is based on the Harmony Search Algorithm (HSA), a derivative free meta-heuristic optimization algorithm inspired by musicians while improvising new harmonies. The algorithm uses the encoding of three points as candidate circles (harmonies) over the edge-only image. An objective function evaluates (harmony quality) if such candidate circles are actually present in the edge image. Guided by the values of this objective function, the set of encoded candidate circles are evolved using the HSA so that they can fit to the actual circles on the edge map of the image (optimal harmony). Experimental results from several tests on synthetic and natural images with a varying complexity range have been included to validate the efficiency of the proposed technique regarding accuracy, speed and robustness. version:1
arxiv-1402-2359 | Machine Learner for Automated Reasoning 0.4 and 0.5 | http://arxiv.org/abs/1402.2359 | id:1402.2359 author:Cezary Kaliszyk, Josef Urban, Jiří Vyskočil category:cs.LG cs.AI cs.LO  published:2014-02-11 summary:Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoning system for proving in large formal libraries where thousands of theorems are available when attacking a new conjecture, and a large number of related problems and proofs can be used to learn specific theorem-proving knowledge. The last version of the system has by a large margin won the 2013 CASC LTB competition. This paper describes the motivation behind the methods used in MaLARea, discusses the general approach and the issues arising in evaluation of such system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea. version:2
arxiv-1405-7229 | A Multi-threshold Segmentation Approach Based on Artificial Bee Colony Optimization | http://arxiv.org/abs/1405.7229 | id:1405.7229 author:Erik Cuevas, Felipe Sencion, Daniel Zaldivar, Marco Perez, Humberto Sossa category:cs.CV cs.NE  published:2014-05-28 summary:This paper explores the use of the Artificial Bee Colony (ABC) algorithm to compute threshold selection for image segmentation. ABC is a heuristic algorithm motivated by the intelligent behavior of honey-bees which has been successfully employed to solve complex optimization problems. In this approach, an image 1D histogram is approximated through a Gaussian mixture model whose parameters are calculated by the ABC algorithm. For the approximation scheme, each Gaussian function represents a pixel class and therefore a threshold. Unlike the Expectation Maximization (EM) algorithm, the ABC based method shows fast convergence and low sensitivity to initial conditions. Remarkably, it also improves complex time consuming computations commonly required by gradient-based methods. Experimental results demonstrate the algorithms ability to perform automatic multi threshold selection yet showing interesting advantages by comparison to other well known algorithms. version:1
arxiv-1301-2683 | BliStr: The Blind Strategymaker | http://arxiv.org/abs/1301.2683 | id:1301.2683 author:Josef Urban category:cs.AI cs.LG cs.LO  published:2013-01-12 summary:BliStr is a system that automatically develops strategies for E prover on a large set of problems. The main idea is to interleave (i) iterated low-timelimit local search for new strategies on small sets of similar easy problems with (ii) higher-timelimit evaluation of the new strategies on all problems. The accumulated results of the global higher-timelimit runs are used to define and evolve the notion of "similar easy problems", and to control the selection of the next strategy to be improved. The technique was used to significantly strengthen the set of E strategies used by the MaLARea, PS-E, E-MaLeS, and E systems in the CASC@Turing 2012 competition, particularly in the Mizar division. Similar improvement was obtained on the problems created from the Flyspeck corpus. version:2
arxiv-1405-4604 | On the saddle point problem for non-convex optimization | http://arxiv.org/abs/1405.4604 | id:1405.4604 author:Razvan Pascanu, Yann N. Dauphin, Surya Ganguli, Yoshua Bengio category:cs.LG cs.NE  published:2014-05-19 summary:A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for the ability of these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, and neural network theory, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new algorithm, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep neural network training, and provide preliminary numerical evidence for its superior performance. version:2
arxiv-1405-6804 | Layered Logic Classifiers: Exploring the `And' and `Or' Relations | http://arxiv.org/abs/1405.6804 | id:1405.6804 author:Zhuowen Tu, Piotr Dollar, Yingnian Wu category:stat.ML cs.LG  published:2014-05-27 summary:Designing effective and efficient classifier for pattern analysis is a key problem in machine learning and computer vision. Many the solutions to the problem require to perform logic operations such as `and', `or', and `not'. Classification and regression tree (CART) include these operations explicitly. Other methods such as neural networks, SVM, and boosting learn/compute a weighted sum on features (weak classifiers), which weakly perform the 'and' and 'or' operations. However, it is hard for these classifiers to deal with the 'xor' pattern directly. In this paper, we propose layered logic classifiers for patterns of complicated distributions by combining the `and', `or', and `not' operations. The proposed algorithm is very general and easy to implement. We test the classifiers on several typical datasets from the Irvine repository and two challenging vision applications, object segmentation and pedestrian detection. We observe significant improvements on all the datasets over the widely used decision stump based AdaBoost algorithm. The resulting classifiers have much less training complexity than decision tree based AdaBoost, and can be applied in a wide range of domains. version:2
arxiv-1304-2079 | Learning Coverage Functions and Private Release of Marginals | http://arxiv.org/abs/1304.2079 | id:1304.2079 author:Vitaly Feldman, Pravesh Kothari category:cs.LG cs.CC cs.DS  published:2013-04-08 summary:We study the problem of approximating and learning coverage functions. A function $c: 2^{[n]} \rightarrow \mathbf{R}^{+}$ is a coverage function, if there exists a universe $U$ with non-negative weights $w(u)$ for each $u \in U$ and subsets $A_1, A_2, \ldots, A_n$ of $U$ such that $c(S) = \sum_{u \in \cup_{i \in S} A_i} w(u)$. Alternatively, coverage functions can be described as non-negative linear combinations of monotone disjunctions. They are a natural subclass of submodular functions and arise in a number of applications. We give an algorithm that for any $\gamma,\delta>0$, given random and uniform examples of an unknown coverage function $c$, finds a function $h$ that approximates $c$ within factor $1+\gamma$ on all but $\delta$-fraction of the points in time $poly(n,1/\gamma,1/\delta)$. This is the first fully-polynomial algorithm for learning an interesting class of functions in the demanding PMAC model of Balcan and Harvey (2011). Our algorithms are based on several new structural properties of coverage functions. Using the results in (Feldman and Kothari, 2014), we also show that coverage functions are learnable agnostically with excess $\ell_1$-error $\epsilon$ over all product and symmetric distributions in time $n^{\log(1/\epsilon)}$. In contrast, we show that, without assumptions on the distribution, learning coverage functions is at least as hard as learning polynomial-size disjoint DNF formulas, a class of functions for which the best known algorithm runs in time $2^{\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004). As an application of our learning results, we give simple differentially-private algorithms for releasing monotone conjunction counting queries with low average error. In particular, for any $k \leq n$, we obtain private release of $k$-way marginals with average error $\bar{\alpha}$ in time $n^{O(\log(1/\bar{\alpha}))}$. version:3
arxiv-1404-1561 | Fast Supervised Hashing with Decision Trees for High-Dimensional Data | http://arxiv.org/abs/1404.1561 | id:1404.1561 author:Guosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David Suter category:cs.CV cs.LG  published:2014-04-06 summary:Supervised hashing aims to map the original features to compact binary codes that are able to preserve label based similarity in the Hamming space. Non-linear hash functions have demonstrated the advantage over linear ones due to their powerful generalization capability. In the literature, kernel functions are typically used to achieve non-linearity in hashing, which achieve encouraging retrieval performance at the price of slow evaluation and training time. Here we propose to use boosted decision trees for achieving non-linearity in hashing, which are fast to train and evaluate, hence more suitable for hashing with high dimensional data. In our approach, we first propose sub-modular formulations for the hashing binary code inference problem and an efficient GraphCut based block search method for solving large-scale inference. Then we learn hash functions by training boosted decision trees to fit the binary codes. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods in retrieval precision and training time. Especially for high-dimensional data, our method is orders of magnitude faster than many methods in terms of training time. version:2
arxiv-1405-7032 | An FPGA-based Parallel Architecture for Face Detection using Mixed Color Models | http://arxiv.org/abs/1405.7032 | id:1405.7032 author:Luo Tao, Shi zaifeng category:cs.CV 68U10  published:2014-05-27 summary:In this paper, a reliable method for detecting human faces in color images is proposed. This system firstly detects skin color in YCgCr and YIQ color space, then filters binary texture and the result is morphological processed, finally converts skin tone to the preferred skin color configured by users in YIQ color space. The real-time adjusting circuit is implemented and some of simulation results are given out. Experimental results demonstrate that the method has achieved high rates and low false positives, another advantage is its simplicity and minor computational costs. version:1
arxiv-1404-5236 | Sum-of-squares proofs and the quest toward optimal algorithms | http://arxiv.org/abs/1404.5236 | id:1404.5236 author:Boaz Barak, David Steurer category:cs.DS cs.CC cs.LG math.OC  published:2014-04-21 summary:In order to obtain the best-known guarantees, algorithms are traditionally tailored to the particular problem we want to solve. Two recent developments, the Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method, surprisingly suggest that this tailoring is not necessary and that a single efficient algorithm could achieve best possible guarantees for a wide range of different problems. The Unique Games Conjecture (UGC) is a tantalizing conjecture in computational complexity, which, if true, will shed light on the complexity of a great many problems. In particular this conjecture predicts that a single concrete algorithm provides optimal guarantees among all efficient algorithms for a large class of computational problems. The Sum-of-Squares (SOS) method is a general approach for solving systems of polynomial constraints. This approach is studied in several scientific disciplines, including real algebraic geometry, proof complexity, control theory, and mathematical programming, and has found applications in fields as diverse as quantum information theory, formal verification, game theory and many others. We survey some connections that were recently uncovered between the Unique Games Conjecture and the Sum-of-Squares method. In particular, we discuss new tools to rigorously bound the running time of the SOS method for obtaining approximate solutions to hard optimization problems, and how these tools give the potential for the sum-of-squares method to provide new guarantees for many problems of interest, and possibly to even refute the UGC. version:2
arxiv-1405-6974 | Futility Analysis in the Cross-Validation of Machine Learning Models | http://arxiv.org/abs/1405.6974 | id:1405.6974 author:Max Kuhn category:stat.ML cs.LG  published:2014-05-27 summary:Many machine learning models have important structural tuning parameters that cannot be directly estimated from the data. The common tactic for setting these parameters is to use resampling methods, such as cross--validation or the bootstrap, to evaluate a candidate set of values and choose the best based on some pre--defined criterion. Unfortunately, this process can be time consuming. However, the model tuning process can be streamlined by adaptively resampling candidate values so that settings that are clearly sub-optimal can be discarded. The notion of futility analysis is introduced in this context. An example is shown that illustrates how adaptive resampling can be used to reduce training time. Simulation studies are used to understand how the potential speed--up is affected by parallel processing techniques. version:1
arxiv-1405-6922 | Large Scale, Large Margin Classification using Indefinite Similarity Measures | http://arxiv.org/abs/1405.6922 | id:1405.6922 author:Omid Aghazadeh, Stefan Carlsson category:cs.LG cs.CV stat.ML  published:2014-05-27 summary:Despite the success of the popular kernelized support vector machines, they have two major limitations: they are restricted to Positive Semi-Definite (PSD) kernels, and their training complexity scales at least quadratically with the size of the data. Many natural measures of similarity between pairs of samples are not PSD e.g. invariant kernels, and those that are implicitly or explicitly defined by latent variable models. In this paper, we investigate scalable approaches for using indefinite similarity measures in large margin frameworks. In particular we show that a normalization of similarity to a subset of the data points constitutes a representation suitable for linear classifiers. The result is a classifier which is competitive to kernelized SVM in terms of accuracy, despite having better training and test time complexities. Experimental results demonstrate that on CIFAR-10 dataset, the model equipped with similarity measures invariant to rigid and non-rigid deformations, can be made more than 5 times sparser while being more accurate than kernelized SVM using RBF kernels. version:1
arxiv-1405-6914 | Supervised Dictionary Learning by a Variational Bayesian Group Sparse Nonnegative Matrix Factorization | http://arxiv.org/abs/1405.6914 | id:1405.6914 author:Ivan Ivek category:cs.CV cs.LG stat.ML  published:2014-05-27 summary:Nonnegative matrix factorization (NMF) with group sparsity constraints is formulated as a probabilistic graphical model and, assuming some observed data have been generated by the model, a feasible variational Bayesian algorithm is derived for learning model parameters. When used in a supervised learning scenario, NMF is most often utilized as an unsupervised feature extractor followed by classification in the obtained feature subspace. Having mapped the class labels to a more general concept of groups which underlie sparsity of the coefficients, what the proposed group sparse NMF model allows is incorporating class label information to find low dimensional label-driven dictionaries which not only aim to represent the data faithfully, but are also suitable for class discrimination. Experiments performed in face recognition and facial expression recognition domains point to advantages of classification in such label-driven feature subspaces over classification in feature subspaces obtained in an unsupervised manner. version:1
arxiv-1405-6886 | A Topic Model Approach to Multi-Modal Similarity | http://arxiv.org/abs/1405.6886 | id:1405.6886 author:Rasmus Troelsgård, Bjørn Sand Jensen, Lars Kai Hansen category:cs.IR stat.ML  published:2014-05-27 summary:Calculating similarities between objects defined by many heterogeneous data modalities is an important challenge in many multimedia applications. We use a multi-modal topic model as a basis for defining such a similarity between objects. We propose to compare the resulting similarities from different model realizations using the non-parametric Mantel test. The approach is evaluated on a music dataset. version:1
arxiv-1405-1213 | Human Pose Estimation from RGB Input Using Synthetic Training Data | http://arxiv.org/abs/1405.1213 | id:1405.1213 author:Oscar Danielsson, Omid Aghazadeh category:cs.CV  published:2014-05-06 summary:We address the problem of estimating the pose of humans using RGB image input. More specifically, we are using a random forest classifier to classify pixels into joint-based body part categories, much similar to the famous Kinect pose estimator [11], [12]. However, we are using pure RGB input, i.e. no depth. Since the random forest requires a large number of training examples, we are using computer graphics generated, synthetic training data. In addition, we assume that we have access to a large number of real images with bounding box labels, extracted for example by a pedestrian detector or a tracking system. We propose a new objective function for random forest training that uses the weakly labeled data from the target domain to encourage the learner to select features that generalize from the synthetic source domain to the real target domain. We demonstrate on a publicly available dataset [6] that the proposed objective function yields a classifier that significantly outperforms a baseline classifier trained using the standard entropy objective [10]. version:2
arxiv-1404-1333 | Understanding Machine-learned Density Functionals | http://arxiv.org/abs/1404.1333 | id:1404.1333 author:Li Li, John C. Snyder, Isabelle M. Pelaschier, Jessica Huang, Uma-Naresh Niranjan, Paul Duncan, Matthias Rupp, Klaus-Robert Müller, Kieron Burke category:physics.chem-ph cs.LG physics.comp-ph stat.ML  published:2014-04-04 summary:Kernel ridge regression is used to approximate the kinetic energy of non-interacting fermions in a one-dimensional box as a functional of their density. The properties of different kernels and methods of cross-validation are explored, and highly accurate energies are achieved. Accurate {\em constrained optimal densities} are found via a modified Euler-Lagrange constrained minimization of the total energy. A projected gradient descent algorithm is derived using local principal component analysis. Additionally, a sparse grid representation of the density can be used without degrading the performance of the methods. The implications for machine-learned density functional approximations are discussed. version:2
arxiv-1405-6757 | Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces | http://arxiv.org/abs/1405.6757 | id:1405.6757 author:Sridhar Mahadevan, Bo Liu, Philip Thomas, Will Dabney, Steve Giguere, Nicholas Jacek, Ian Gemp, Ji Liu category:cs.LG  published:2014-05-26 summary:In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified "safety" guarantees, and remains in a stable region of the parameter space (iii) how to design "off-policy" temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators. The key idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design "true" stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators. version:1
arxiv-1405-6684 | Visualizing Random Forest with Self-Organising Map | http://arxiv.org/abs/1405.6684 | id:1405.6684 author:Piotr Płoński, Krzysztof Zaremba category:cs.LG  published:2014-05-26 summary:Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM. version:1
arxiv-1405-6682 | Optimality Theory as a Framework for Lexical Acquisition | http://arxiv.org/abs/1405.6682 | id:1405.6682 author:Thierry Poibeau category:cs.CL  published:2014-05-26 summary:This paper re-investigates a lexical acquisition system initially developed for French.We show that, interestingly, the architecture of the system reproduces and implements the main components of Optimality Theory. However, we formulate the hypothesis that some of its limitations are mainly due to a poor representation of the constraints used. Finally, we show how a better representation of the constraints used would yield better results. version:1
arxiv-1405-6678 | Hybrid Type-Logical Grammars, First-Order Linear Logic and the Descriptive Inadequacy of Lambda Grammars | http://arxiv.org/abs/1405.6678 | id:1405.6678 author:Richard Moot category:cs.LO cs.CL  published:2014-05-26 summary:In this article we show that hybrid type-logical grammars are a fragment of first-order linear logic. This embedding result has several important consequences: it not only provides a simple new proof theory for the calculus, thereby clarifying the proof-theoretic foundations of hybrid type-logical grammars, but, since the translation is simple and direct, it also provides several new parsing strategies for hybrid type-logical grammars. Second, NP-completeness of hybrid type-logical grammars follows immediately. The main embedding result also sheds new light on problems with lambda grammars/abstract categorial grammars and shows lambda grammars/abstract categorial grammars suffer from problems of over-generation and from problems at the syntax-semantics interface unlike any other categorial grammar. version:1
arxiv-1405-5732 | Self-tuned Visual Subclass Learning with Shared Samples An Incremental Approach | http://arxiv.org/abs/1405.5732 | id:1405.5732 author:Hossein Azizpour, Stefan Carlsson category:cs.CV  published:2014-05-22 summary:Computer vision tasks are traditionally defined and evaluated using semantic categories. However, it is known to the field that semantic classes do not necessarily correspond to a unique visual class (e.g. inside and outside of a car). Furthermore, many of the feasible learning techniques at hand cannot model a visual class which appears consistent to the human eye. These problems have motivated the use of 1) Unsupervised or supervised clustering as a preprocessing step to identify the visual subclasses to be used in a mixture-of-experts learning regime. 2) Felzenszwalb et al. part model and other works model mixture assignment with latent variables which is optimized during learning 3) Highly non-linear classifiers which are inherently capable of modelling multi-modal input space but are inefficient at the test time. In this work, we promote an incremental view over the recognition of semantic classes with varied appearances. We propose an optimization technique which incrementally finds maximal visual subclasses in a regularized risk minimization framework. Our proposed approach unifies the clustering and classification steps in a single algorithm. The importance of this approach is its compliance with the classification via the fact that it does not need to know about the number of clusters, the representation and similarity measures used in pre-processing clustering methods a priori. Following this approach we show both qualitatively and quantitatively significant results. We show that the visual subclasses demonstrate a long tail distribution. Finally, we show that state of the art object detection methods (e.g. DPM) are unable to use the tails of this distribution comprising 50\% of the training samples. In fact we show that DPM performance slightly increases on average by the removal of this half of the data. version:2
arxiv-1405-6667 | Inferring gender of a Twitter user using celebrities it follows | http://arxiv.org/abs/1405.6667 | id:1405.6667 author:Puneet Singh Ludu category:cs.IR cs.CL  published:2014-05-26 summary:This paper addresses the task of user gender classification in social media, with an application to Twitter. The approach automatically predicts gender by leveraging observable information such as the tweet behavior, linguistic content of the user's Twitter feed and the celebrities followed by the user. This paper first evaluates linguistic content based features using LIWC dictionary and popular neighborhood features using Wikipedia and Freebase. Then augments both features which yielded a significant increase in the accuracy for gender prediction. Results show that rich linguistic features combined with popular neighborhood prove valuables and promising for additional user classification needs. version:1
arxiv-1308-6273 | New Algorithms for Learning Incoherent and Overcomplete Dictionaries | http://arxiv.org/abs/1308.6273 | id:1308.6273 author:Sanjeev Arora, Rong Ge, Ankur Moitra category:cs.DS cs.LG stat.ML  published:2013-08-28 summary:In sparse recovery we are given a matrix $A$ (the dictionary) and a vector of the form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is a central notion in signal processing, statistics and machine learning. But in applications such as sparse coding, edge detection, compression and super resolution, the dictionary $A$ is unknown and has to be learned from random examples of the form $Y = AX$ where $X$ is drawn from an appropriate distribution --- this is the dictionary learning problem. In most settings, $A$ is overcomplete: it has more columns than rows. This paper presents a polynomial-time algorithm for learning overcomplete dictionaries; the only previously known algorithm with provable guarantees is the recent work of Spielman, Wang and Wright who gave an algorithm for the full-rank case, which is rarely the case in applications. Our algorithm applies to incoherent dictionaries which have been a central object of study since they were introduced in seminal work of Donoho and Huo. In particular, a dictionary is $\mu$-incoherent if each pair of columns has inner product at most $\mu / \sqrt{n}$. The algorithm makes natural stochastic assumptions about the unknown sparse vector $X$, which can contain $k \leq c \min(\sqrt{n}/\mu \log n, m^{1/2 -\eta})$ non-zero entries (for any $\eta > 0$). This is close to the best $k$ allowable by the best sparse recovery algorithms even if one knows the dictionary $A$ exactly. Moreover, both the running time and sample complexity depend on $\log 1/\epsilon$, where $\epsilon$ is the target accuracy, and so our algorithms converge very quickly to the true dictionary. Our algorithm can also tolerate substantial amounts of noise provided it is incoherent with respect to the dictionary (e.g., Gaussian). In the noisy setting, our running time and sample complexity depend polynomially on $1/\epsilon$, and this is necessary. version:5
arxiv-1311-7320 | Bayesian Inference for Gaussian Process Classifiers with Annealing and Pseudo-Marginal MCMC | http://arxiv.org/abs/1311.7320 | id:1311.7320 author:Maurizio Filippone category:stat.ME stat.ML  published:2013-11-28 summary:Kernel methods have revolutionized the fields of pattern recognition and machine learning. Their success, however, critically depends on the choice of kernel parameters. Using Gaussian process (GP) classification as a working example, this paper focuses on Bayesian inference of covariance (kernel) parameters using Markov chain Monte Carlo (MCMC) methods. The motivation is that, compared to standard optimization of kernel parameters, they have been systematically demonstrated to be superior in quantifying uncertainty in predictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a practical inference tool for GP models. In particular, it amounts in replacing the analytically intractable marginal likelihood by an unbiased estimate obtainable by approximate methods and importance sampling. After discussing the potential drawbacks in employing importance sampling, this paper proposes the application of annealed importance sampling. The results empirically demonstrate that compared to importance sampling, annealed importance sampling can reduce the variance of the estimate of the marginal likelihood exponentially in the number of data at a computational cost that scales only polynomially. The results on real data demonstrate that employing annealed importance sampling in the Pseudo-Marginal MCMC approach represents a step forward in the development of fully automated exact inference engines for GP models. version:2
arxiv-1405-6563 | Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D Articulated Bodies | http://arxiv.org/abs/1405.6563 | id:1405.6563 author:Fabio Cuzzolin, Diana Mateus, Radu Horaud category:cs.CV cs.GR cs.LG  published:2014-05-26 summary:In motion analysis and understanding it is important to be able to fit a suitable model or structure to the temporal series of observed data, in order to describe motion patterns in a compact way, and to discriminate between them. In an unsupervised context, i.e., no prior model of the moving object(s) is available, such a structure has to be learned from the data in a bottom-up fashion. In recent times, volumetric approaches in which the motion is captured from a number of cameras and a voxel-set representation of the body is built from the camera views, have gained ground due to attractive features such as inherent view-invariance and robustness to occlusions. Automatic, unsupervised segmentation of moving bodies along entire sequences, in a temporally-coherent and robust way, has the potential to provide a means of constructing a bottom-up model of the moving body, and track motion cues that may be later exploited for motion classification. Spectral methods such as locally linear embedding (LLE) can be useful in this context, as they preserve "protrusions", i.e., high-curvature regions of the 3D volume, of articulated shapes, while improving their separation in a lower dimensional space, making them in this way easier to cluster. In this paper we therefore propose a spectral approach to unsupervised and temporally-coherent body-protrusion segmentation along time sequences. Volumetric shapes are clustered in an embedding space, clusters are propagated in time to ensure coherence, and merged or split to accommodate changes in the body's topology. Experiments on both synthetic and real sequences of dense voxel-set data are shown. This supports the ability of the proposed method to cluster body-parts consistently over time in a totally unsupervised fashion, its robustness to sampling density and shape quality, and its potential for bottom-up model construction version:1
arxiv-1405-6524 | Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning | http://arxiv.org/abs/1405.6524 | id:1405.6524 author:Dan Stowell, Mark D. Plumbley category:cs.SD cs.LG  published:2014-05-26 summary:Automatic species classification of birds from their sound is a computational tool of increasing importance in ecology, conservation monitoring and vocal communication studies. To make classification useful in practice, it is crucial to improve its accuracy while ensuring that it can run at big data scales. Many approaches use acoustic measures based on spectrogram-type data, such as the Mel-frequency cepstral coefficient (MFCC) features which represent a manually-designed summary of spectral information. However, recent work in machine learning has demonstrated that features learnt automatically from data can often outperform manually-designed feature transforms. Feature learning can be performed at large scale and "unsupervised", meaning it requires no manual data labelling, yet it can improve performance on "supervised" tasks such as classification. In this work we introduce a technique for feature learning from large volumes of bird sound recordings, inspired by techniques that have proven useful in other domains. We experimentally compare twelve different feature representations derived from the Mel spectrum (of which six use this technique), using four large and diverse databases of bird vocalisations, with a random forest classifier. We demonstrate that MFCCs are of limited power in this context, leading to worse performance than the raw Mel spectral data. Conversely, we demonstrate that unsupervised feature learning provides a substantial boost over MFCCs and Mel spectra without adding computational complexity after the model has been trained. The boost is particularly notable for single-label classification tasks at large scale. The spectro-temporal activations learned through our procedure resemble spectro-temporal receptive fields calculated from avian primary auditory forebrain. version:1
arxiv-1405-6472 | Fast and Robust Archetypal Analysis for Representation Learning | http://arxiv.org/abs/1405.6472 | id:1405.6472 author:Yuansi Chen, Julien Mairal, Zaid Harchaoui category:cs.CV cs.LG stat.ML  published:2014-05-26 summary:We revisit a pioneer unsupervised learning technique called archetypal analysis, which is related to successful data analysis methods such as sparse coding and non-negative matrix factorization. Since it was proposed, archetypal analysis did not gain a lot of popularity even though it produces more interpretable models than other alternatives. Because no efficient implementation has ever been made publicly available, its application to important scientific problems may have been severely limited. Our goal is to bring back into favour archetypal analysis. We propose a fast optimization scheme using an active-set strategy, and provide an efficient open-source implementation interfaced with Matlab, R, and Python. Then, we demonstrate the usefulness of archetypal analysis for computer vision tasks, such as codebook learning, signal classification, and large image collection visualization. version:1
arxiv-1403-2485 | Optimal interval clustering: Application to Bregman clustering and statistical mixture learning | http://arxiv.org/abs/1403.2485 | id:1403.2485 author:Frank Nielsen, Richard Nock category:cs.IT cs.LG math.IT  published:2014-03-11 summary:We present a generic dynamic programming method to compute the optimal clustering of $n$ scalar elements into $k$ pairwise disjoint intervals. This case includes 1D Euclidean $k$-means, $k$-medoids, $k$-medians, $k$-centers, etc. We extend the method to incorporate cluster size constraints and show how to choose the appropriate $k$ by model selection. Finally, we illustrate and refine the method on two case studies: Bregman clustering and statistical mixture learning maximizing the complete likelihood. version:2
arxiv-1405-6444 | The role of dimensionality reduction in linear classification | http://arxiv.org/abs/1405.6444 | id:1405.6444 author:Weiran Wang, Miguel Á. Carreira-Perpiñán category:cs.LG math.OC stat.ML  published:2014-05-26 summary:Dimensionality reduction (DR) is often used as a preprocessing step in classification, but usually one first fixes the DR mapping, possibly using label information, and then learns a classifier (a filter approach). Best performance would be obtained by optimizing the classification error jointly over DR mapping and classifier (a wrapper approach), but this is a difficult nonconvex problem, particularly with nonlinear DR. Using the method of auxiliary coordinates, we give a simple, efficient algorithm to train a combination of nonlinear DR and a classifier, and apply it to a RBF mapping with a linear SVM. This alternates steps where we train the RBF mapping and a linear SVM as usual regression and classification, respectively, with a closed-form step that coordinates both. The resulting nonlinear low-dimensional classifier achieves classification errors competitive with the state-of-the-art but is fast at training and testing, and allows the user to trade off runtime for classification accuracy easily. We then study the role of nonlinear DR in linear classification, and the interplay between the DR mapping, the number of latent dimensions and the number of classes. When trained jointly, the DR mapping takes an extreme role in eliminating variation: it tends to collapse classes in latent space, erasing all manifold structure, and lay out class centroids so they are linearly separable with maximum margin. version:1
arxiv-1206-5333 | TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations | http://arxiv.org/abs/1206.5333 | id:1206.5333 author:Naushad UzZaman, Hector Llorens, James Allen, Leon Derczynski, Marc Verhagen, James Pustejovsky category:cs.CL  published:2012-06-22 summary:We describe the TempEval-3 task which is currently in preparation for the SemEval-2013 evaluation exercise. The aim of TempEval is to advance research on temporal information processing. TempEval-3 follows on from previous TempEval events, incorporating: a three-part task structure covering event, temporal expression and temporal relation extraction; a larger dataset; and single overall task quality scores. version:2
arxiv-1312-4605 | Parallelizing MCMC via Weierstrass Sampler | http://arxiv.org/abs/1312.4605 | id:1312.4605 author:Xiangyu Wang, David B. Dunson category:stat.CO cs.DC stat.ML  published:2013-12-17 summary:With the rapidly growing scales of statistical problems, subset based communication-free parallel MCMC methods are a promising future for large scale Bayesian analysis. In this article, we propose a new Weierstrass sampler for parallel MCMC based on independent subsets. The new sampler approximates the full data posterior samples via combining the posterior draws from independent subset MCMC chains, and thus enjoys a higher computational efficiency. We show that the approximation error for the Weierstrass sampler is bounded by some tuning parameters and provide suggestions for choice of the values. Simulation study shows the Weierstrass sampler is very competitive compared to other methods for combining MCMC chains generated for subsets, including averaging and kernel smoothing. version:2
arxiv-1402-2864 | Sparse Estimation From Noisy Observations of an Overdetermined Linear System | http://arxiv.org/abs/1402.2864 | id:1402.2864 author:Liang Dai, Kristiaan Pelckmans category:cs.SY stat.ML  published:2014-02-12 summary:This note studies a method for the efficient estimation of a finite number of unknown parameters from linear equations, which are perturbed by Gaussian noise. In case the unknown parameters have only few nonzero entries, the proposed estimator performs more efficiently than a traditional approach. The method consists of three steps: (1) a classical Least Squares Estimate (LSE), (2) the support is recovered through a Linear Programming (LP) optimization problem which can be computed using a soft-thresholding step, (3) a de-biasing step using a LSE on the estimated support set. The main contribution of this note is a formal derivation of an associated ORACLE property of the final estimate. That is, when the number of samples is large enough, the estimate is shown to equal the LSE based on the support of the {\em true} parameters. version:2
arxiv-1402-4437 | Learning the Irreducible Representations of Commutative Lie Groups | http://arxiv.org/abs/1402.4437 | id:1402.4437 author:Taco Cohen, Max Welling category:cs.LG  published:2014-02-18 summary:We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups -- a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification. version:2
arxiv-1312-6214 | Volumetric Spanners: an Efficient Exploration Basis for Learning | http://arxiv.org/abs/1312.6214 | id:1312.6214 author:Elad Hazan, Zohar Karnin, Raghu Mehka category:cs.LG cs.AI cs.DS  published:2013-12-21 summary:Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance, called volumetric spanners, and give efficient algorithms to construct such a basis. We show how efficient volumetric spanners give rise to the first efficient and optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set. version:3
arxiv-1310-2063 | Active causation and the origin of meaning | http://arxiv.org/abs/1310.2063 | id:1310.2063 author:J. H. van Hateren category:q-bio.PE cs.NE nlin.AO q-bio.NC  published:2013-10-08 summary:Purpose and meaning are necessary concepts for understanding mind and culture, but appear to be absent from the physical world and are not part of the explanatory framework of the natural sciences. Understanding how meaning (in the broad sense of the term) could arise from a physical world has proven to be a tough problem. The basic scheme of Darwinian evolution produces adaptations that only represent apparent ("as if") goals and meaning. Here I use evolutionary models to show that a slight, evolvable extension of the basic scheme is sufficient to produce genuine goals. The extension, targeted modulation of mutation rate, is known to be generally present in biological cells, and gives rise to two phenomena that are absent from the non-living world: intrinsic meaning and the ability to initiate goal-directed chains of causation (active causation). The extended scheme accomplishes this by utilizing randomness modulated by a feedback loop that is itself regulated by evolutionary pressure. The mechanism can be extended to behavioural variability as well, and thus shows how freedom of behaviour is possible. A further extension to communication suggests that the active exchange of intrinsic meaning between organisms may be the origin of consciousness, which in combination with active causation can provide a physical basis for the phenomenon of free will. version:4
arxiv-1405-6353 | A Novel Stochastic Decoding of LDPC Codes with Quantitative Guarantees | http://arxiv.org/abs/1405.6353 | id:1405.6353 author:Nima Noorshams, Aravind Iyengar category:cs.IT math.IT stat.ML  published:2014-05-25 summary:Low-density parity-check codes, a class of capacity-approaching linear codes, are particularly recognized for their efficient decoding scheme. The decoding scheme, known as the sum-product, is an iterative algorithm consisting of passing messages between variable and check nodes of the factor graph. The sum-product algorithm is fully parallelizable, owing to the fact that all messages can be update concurrently. However, since it requires extensive number of highly interconnected wires, the fully-parallel implementation of the sum-product on chips is exceedingly challenging. Stochastic decoding algorithms, which exchange binary messages, are of great interest for mitigating this challenge and have been the focus of extensive research over the past decade. They significantly reduce the required wiring and computational complexity of the message-passing algorithm. Even though stochastic decoders have been shown extremely effective in practice, the theoretical aspect and understanding of such algorithms remains limited at large. Our main objective in this paper is to address this issue. We first propose a novel algorithm referred to as the Markov based stochastic decoding. Then, we provide concrete quantitative guarantees on its performance for tree-structured as well as general factor graphs. More specifically, we provide upper-bounds on the first and second moments of the error, illustrating that the proposed algorithm is an asymptotically consistent estimate of the sum-product algorithm. We also validate our theoretical predictions with experimental results, showing we achieve comparable performance to other practical stochastic decoders. version:1
arxiv-1405-6341 | Efficient Model Learning for Human-Robot Collaborative Tasks | http://arxiv.org/abs/1405.6341 | id:1405.6341 author:Stefanos Nikolaidis, Keren Gu, Ramya Ramakrishnan, Julie Shah category:cs.RO cs.AI cs.LG cs.SY I.2.6; I.2.8; I.2.9  published:2014-05-24 summary:We present a framework for learning human user models from joint-action demonstrations that enables the robot to compute a robust policy for a collaborative task with a human. The learning takes place completely automatically, without any human intervention. First, we describe the clustering of demonstrated action sequences into different human types using an unsupervised learning algorithm. These demonstrated sequences are also used by the robot to learn a reward function that is representative for each type, through the employment of an inverse reinforcement learning algorithm. The learned model is then used as part of a Mixed Observability Markov Decision Process formulation, wherein the human type is a partially observable variable. With this framework, we can infer, either offline or online, the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this new user and will be robust to deviations of the human actions from prior demonstrations. Finally we validate the approach using data collected in human subject experiments, and conduct proof-of-concept demonstrations in which a person performs a collaborative task with a small industrial robot. version:1
arxiv-1405-6296 | Four Classes of Morphogenetic Collective Systems | http://arxiv.org/abs/1405.6296 | id:1405.6296 author:Hiroki Sayama category:nlin.AO cs.NE  published:2014-05-24 summary:We studied the roles of morphogenetic principles---heterogeneity of components, dynamic differentiation/re-differentiation of components, and local information sharing among components---in the self-organization of morphogenetic collective systems. By incrementally introducing these principles to collectives, we defined four distinct classes of morphogenetic collective systems. Monte Carlo simulations were conducted using an extended version of the Swarm Chemistry model that was equipped with dynamic differentiation/re-differentiation and local information sharing capabilities. Self-organization of swarms was characterized by several kinetic and topological measurements, the latter of which were facilitated by a newly developed network-based method. Results of simulations revealed that, while heterogeneity of components had a strong impact on the structure and behavior of the swarms, dynamic differentiation/re-differentiation of components and local information sharing helped the swarms maintain spatially adjacent, coherent organization. version:1
arxiv-1405-6293 | Cross-Language Personal Name Mapping | http://arxiv.org/abs/1405.6293 | id:1405.6293 author:Ahmed H. Yousef category:cs.CL  published:2014-05-24 summary:Name matching between multiple natural languages is an important step in cross-enterprise integration applications and data mining. It is difficult to decide whether or not two syntactic values (names) from two heterogeneous data sources are alternative designation of the same semantic entity (person), this process becomes more difficult with Arabic language due to several factors including spelling and pronunciation variation, dialects and special vowel and consonant distinction and other linguistic characteristics. This paper proposes a new framework for name matching between the Arabic language and other languages. The framework uses a dictionary based on a new proposed version of the Soundex algorithm to encapsulate the recognition of special features of Arabic names. The framework proposes a new proximity matching algorithm to suit the high importance of order sensitivity in Arabic name matching. New performance evaluation metrics are proposed as well. The framework is implemented and verified empirically in several case studies demonstrating substantial improvements compared to other well-known techniques found in literature. version:1
arxiv-1208-3728 | Online Learning with Predictable Sequences | http://arxiv.org/abs/1208.3728 | id:1208.3728 author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG  published:2012-08-18 summary:We present methods for online linear optimization that take advantage of benign (as opposed to worst-case) sequences. Specifically if the sequence encountered by the learner is described well by a known "predictable process", the algorithms presented enjoy tighter bounds as compared to the typical worst case bounds. Additionally, the methods achieve the usual worst-case regret bounds if the sequence is not benign. Our approach can be seen as a way of adding prior knowledge about the sequence within the paradigm of online learning. The setting is shown to encompass partial and side information. Variance and path-length bounds can be seen as particular examples of online learning with simple predictable sequences. We further extend our methods and results to include competing with a set of possible predictable processes (models), that is "learning" the predictable process itself concurrently with using it to obtain better regret guarantees. We show that such model selection is possible under various assumptions on the available feedback. Our results suggest a promising direction of further research with potential applications to stock market and time series prediction. version:2
arxiv-1212-1073 | Kernel Estimation from Salient Structure for Robust Motion Deblurring | http://arxiv.org/abs/1212.1073 | id:1212.1073 author:Jinshan Pan, Risheng Liu, Zhixun Su, Xianfeng Gu category:cs.CV  published:2012-12-05 summary:Blind image deblurring algorithms have been improving steadily in the past years. Most state-of-the-art algorithms, however, still cannot perform perfectly in challenging cases, especially in large blur setting. In this paper, we focus on how to estimate a good kernel estimate from a single blurred image based on the image structure. We found that image details caused by blurring could adversely affect the kernel estimation, especially when the blur kernel is large. One effective way to eliminate these details is to apply image denoising model based on the Total Variation (TV). First, we developed a novel method for computing image structures based on TV model, such that the structures undermining the kernel estimation will be removed. Second, to mitigate the possible adverse effect of salient edges and improve the robustness of kernel estimation, we applied a gradient selection method. Third, we proposed a novel kernel estimation method, which is capable of preserving the continuity and sparsity of the kernel and reducing the noises. Finally, we developed an adaptive weighted spatial prior, for the purpose of preserving sharp edges in latent image restoration. The effectiveness of our method is demonstrated by experiments on various kinds of challenging examples. version:2
arxiv-1405-6285 | Traversing News with Ant Colony Optimisation and Negative Pheromones | http://arxiv.org/abs/1405.6285 | id:1405.6285 author:David M. S. Rodrigues, Vitorino Ramos category:cs.IR cs.NE  published:2014-05-24 summary:The past decade has seen the rapid development of the online newsroom. News published online are the main outlet of news surpassing traditional printed newspapers. This poses challenges to the production and to the consumption of those news. With those many sources of information available it is important to find ways to cluster and organise the documents if one wants to understand this new system. A novel bio inspired approach to the problem of traversing the news is presented. It finds Hamiltonian cycles over documents published by the newspaper The Guardian. A Second Order Swarm Intelligence algorithm based on Ant Colony Optimisation was developed that uses a negative pheromone to mark unrewarding paths with a "no-entry" signal. This approach follows recent findings of negative pheromone usage in real ants. version:1
arxiv-1405-6275 | Improvements and Experiments of a Compact Statistical Background Model | http://arxiv.org/abs/1405.6275 | id:1405.6275 author:Dong Liang, Shun'ichi Kaneko category:cs.CV  published:2014-05-24 summary:Change detection plays an important role in most video-based applications. The first stage is to build appropriate background model, which is now becoming increasingly complex as more sophisticated statistical approaches are introduced to cover challenging situations and provide reliable detection. This paper reports a simple and intuitive statistical model based on deeper learning spatial correlation among pixels: For each observed pixel, we select a group of supporting pixels with high correlation, and then use a single Gaussian to model the intensity deviations between the observed pixel and the supporting ones. In addition, a multi-channel model updating is integrated on-line and a temporal intensity constraint for each pixel is defined. Although this method is mainly designed for coping with sudden illumination changes, experimental results using all the video sequences provided on changedetection.net validate it is comparable with other recent methods under various situations. version:1
arxiv-1405-6261 | Geometric Polynomial Constraints in Higher-Order Graph Matching | http://arxiv.org/abs/1405.6261 | id:1405.6261 author:Mayank Bansal, Kostas Daniilidis category:cs.CV  published:2014-05-24 summary:Correspondence is a ubiquitous problem in computer vision and graph matching has been a natural way to formalize correspondence as an optimization problem. Recently, graph matching solvers have included higher-order terms representing affinities beyond the unary and pairwise level. Such higher-order terms have a particular appeal for geometric constraints that include three or more correspondences like the PnP 2D-3D pose problems. In this paper, we address the problem of finding correspondences in the absence of unary or pairwise constraints as it emerges in problems where unary appearance similarity like SIFT matches is not available. Current higher order matching approaches have targeted problems where higher order affinity can simply be formulated as a difference of invariances such as lengths, angles, or cross-ratios. In this paper, we present a method of how to apply geometric constraints modeled as polynomial equation systems. As opposed to RANSAC where such systems have to be solved and then tested for inlier hypotheses, our constraints are derived as a single affinity weight based on $n>2$ hypothesized correspondences without solving the polynomial system. Since the result is directly a correspondence without a transformation model, our approach supports correspondence matching in the presence of multiple geometric transforms like articulated motions. version:1
arxiv-1306-1091 | Deep Generative Stochastic Networks Trainable by Backprop | http://arxiv.org/abs/1306.1091 | id:1306.1091 author:Yoshua Bengio, Éric Thibodeau-Laufer, Guillaume Alain, Jason Yosinski category:cs.LG  published:2013-06-05 summary:We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining. version:5
arxiv-1405-6231 | Connection graph Laplacian methods can be made robust to noise | http://arxiv.org/abs/1405.6231 | id:1405.6231 author:Noureddine El Karoui, Hau-tieng Wu category:math.ST math.SP stat.ME stat.ML stat.TH 60F99  53A99  published:2014-05-23 summary:Recently, several data analytic techniques based on connection graph laplacian (CGL) ideas have appeared in the literature. At this point, the properties of these methods are starting to be understood in the setting where the data is observed without noise. We study the impact of additive noise on these methods, and show that they are remarkably robust. As a by-product of our analysis, we propose modifications of the standard algorithms that increase their robustness to noise. We illustrate our results in numerical simulations. version:1
arxiv-1405-6210 | Convex Banding of the Covariance Matrix | http://arxiv.org/abs/1405.6210 | id:1405.6210 author:Jacob Bien, Florentina Bunea, Luo Xiao category:math.ST stat.CO stat.ME stat.ML stat.TH  published:2014-05-23 summary:We introduce a new sparse estimator of the covariance matrix for high-dimensional models in which the variables have a known ordering. Our estimator, which is the solution to a convex optimization problem, is equivalently expressed as an estimator which tapers the sample covariance matrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result of this adaptivity, the convex banding estimator enjoys theoretical optimality properties not attained by previous banding or tapered estimators. In particular, our convex banding estimator is minimax rate adaptive in Frobenius and operator norms, up to log factors, over commonly-studied classes of covariance matrices, and over more general classes. Furthermore, it correctly recovers the bandwidth when the true covariance is exactly banded. Our convex formulation admits a simple and efficient algorithm. Empirical studies demonstrate its practical effectiveness and illustrate that our exactly-banded estimator works well even when the true covariance matrix is only close to a banded matrix, confirming our theoretical results. Our method compares favorably with all existing methods, in terms of accuracy and speed. We illustrate the practical merits of the convex banding estimator by showing that it can be used to improve the performance of discriminant analysis for classifying sound recordings. version:1
arxiv-1405-6103 | Evaluating the fully automatic multi-language translation of the Swiss avalanche bulletin | http://arxiv.org/abs/1405.6103 | id:1405.6103 author:Kurt Winkler, Tobias Kuhn, Martin Volk category:cs.CL  published:2014-05-23 summary:The Swiss avalanche bulletin is produced twice a day in four languages. Due to the lack of time available for manual translation, a fully automated translation system is employed, based on a catalogue of predefined phrases and predetermined rules of how these phrases can be combined to produce sentences. The system is able to automatically translate such sentences from German into the target languages French, Italian and English without subsequent proofreading or correction. Our catalogue of phrases is limited to a small sublanguage. The reduction of daily translation costs is expected to offset the initial development costs within a few years. After being operational for two winter seasons, we assess here the quality of the produced texts based on an evaluation where participants rate real danger descriptions from both origins, the catalogue of phrases versus the manually written and translated texts. With a mean recognition rate of 55%, users can hardly distinguish between the two types of texts, and give similar ratings with respect to their language quality. Overall, the output from the catalogue system can be considered virtually equivalent to a text written by avalanche forecasters and then manually translated by professional translators. Furthermore, forecasters declared that all relevant situations were captured by the system with sufficient accuracy and within the limited time available. version:1
arxiv-1405-6076 | Online Linear Optimization via Smoothing | http://arxiv.org/abs/1405.6076 | id:1405.6076 author:Jacob Abernethy, Chansoo Lee, Abhinav Sinha, Ambuj Tewari category:cs.LG  published:2014-05-23 summary:We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between "Follow the Regularized Leader" and "Follow the Perturbed Leader" up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader. version:1
arxiv-1405-6068 | Building of Networks of Natural Hierarchies of Terms Based on Analysis of Texts Corpora | http://arxiv.org/abs/1405.6068 | id:1405.6068 author:Dmitry Lande category:cs.CL  published:2014-05-23 summary:The technique of building of networks of hierarchies of terms based on the analysis of chosen text corpora is offered. The technique is based on the methodology of horizontal visibility graphs. Constructed and investigated language network, formed on the basis of electronic preprints arXiv on topics of information retrieval. version:1
arxiv-1301-2603 | Robust subspace clustering | http://arxiv.org/abs/1301.2603 | id:1301.2603 author:Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J. Candès category:cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH  published:2013-01-11 summary:Subspace clustering refers to the task of finding a multi-subspace representation that best fits a collection of points taken from a high-dimensional space. This paper introduces an algorithm inspired by sparse subspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern Recognition, CVPR (2009) 2790-2797] to cluster noisy data, and develops some novel theory demonstrating its correctness. In particular, the theory uses ideas from geometric functional analysis to show that the algorithm can accurately recover the underlying subspaces under minimal requirements on their orientation, and on the number of samples per subspace. Synthetic as well as real data experiments complement our theoretical study, illustrating our approach and demonstrating its effectiveness. version:3
arxiv-1311-0989 | Large Margin Distribution Machine | http://arxiv.org/abs/1311.0989 | id:1311.0989 author:Teng Zhang, Zhi-Hua Zhou category:cs.LG  published:2013-11-05 summary:Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The LDM is a general learning approach which can be used in any place where SVM can be applied, and its superiority is verified both theoretically and empirically in this paper. version:2
arxiv-1209-5075 | Gemini: Graph estimation with matrix variate normal instances | http://arxiv.org/abs/1209.5075 | id:1209.5075 author:Shuheng Zhou category:stat.ML math.ST stat.TH  published:2012-09-23 summary:Undirected graphs can be used to describe matrix variate distributions. In this paper, we develop new methods for estimating the graphical structures and underlying parameters, namely, the row and column covariance and inverse covariance matrices from the matrix variate data. Under sparsity conditions, we show that one is able to recover the graphs and covariance matrices with a single random matrix from the matrix variate normal distribution. Our method extends, with suitable adaptation, to the general setting where replicates are available. We establish consistency and obtain the rates of convergence in the operator and the Frobenius norm. We show that having replicates will allow one to estimate more complicated graphical structures and achieve faster rates of convergence. We provide simulation evidence showing that we can recover graphical structures as well as estimating the precision matrices, as predicted by theory. version:2
arxiv-1405-6012 | On the Optimal Solution of Weighted Nuclear Norm Minimization | http://arxiv.org/abs/1405.6012 | id:1405.6012 author:Qi Xie, Deyu Meng, Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, Zongben Xu category:cs.CV cs.LG stat.ML  published:2014-05-23 summary:In recent years, the nuclear norm minimization (NNM) problem has been attracting much attention in computer vision and machine learning. The NNM problem is capitalized on its convexity and it can be solved efficiently. The standard nuclear norm regularizes all singular values equally, which is however not flexible enough to fit real scenarios. Weighted nuclear norm minimization (WNNM) is a natural extension and generalization of NNM. By assigning properly different weights to different singular values, WNNM can lead to state-of-the-art results in applications such as image denoising. Nevertheless, so far the global optimal solution of WNNM problem is not completely solved yet due to its non-convexity in general cases. In this article, we study the theoretical properties of WNNM and prove that WNNM can be equivalently transformed into a quadratic programming problem with linear constraints. This implies that WNNM is equivalent to a convex problem and its global optimum can be readily achieved by off-the-shelf convex optimization solvers. We further show that when the weights are non-descending, the globally optimal solution of WNNM can be obtained in closed-form. version:1
arxiv-1405-5960 | LASS: a simple assignment model with Laplacian smoothing | http://arxiv.org/abs/1405.5960 | id:1405.5960 author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG math.OC stat.ML  published:2014-05-23 summary:We consider the problem of learning soft assignments of $N$ items to $K$ categories given two sources of information: an item-category similarity matrix, which encourages items to be assigned to categories they are similar to (and to not be assigned to categories they are dissimilar to), and an item-item similarity matrix, which encourages similar items to have similar assignments. We propose a simple quadratic programming model that captures this intuition. We give necessary conditions for its solution to be unique, define an out-of-sample mapping, and derive a simple, effective training algorithm based on the alternating direction method of multipliers. The model predicts reasonable assignments from even a few similarity values, and can be seen as a generalization of semisupervised learning. It is particularly useful when items naturally belong to multiple categories, as for example when annotating documents with keywords or pictures with tags, with partially tagged items, or when the categories have complex interrelations (e.g. hierarchical) that are unknown. version:1
arxiv-1405-4053 | Distributed Representations of Sentences and Documents | http://arxiv.org/abs/1405.4053 | id:1405.4053 author:Quoc V. Le, Tomas Mikolov category:cs.CL cs.AI cs.LG  published:2014-05-16 summary:Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. version:2
arxiv-1405-5893 | Computerization of African languages-French dictionaries | http://arxiv.org/abs/1405.5893 | id:1405.5893 author:Chantal Enguehard, Mathieu Mangeot category:cs.CL  published:2014-05-22 summary:This paper relates work done during the DiLAF project. It consists in converting 5 bilingual African language-French dictionaries originally in Word format into XML following the LMF model. The languages processed are Bambara, Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced languages concerning Natural Language Processing tools. Once converted, the dictionaries are available online on the Jibiki platform for lookup and modification. The DiLAF project is first presented. A description of each dictionary follows. Then, the conversion methodology from .doc format to XML files is presented. A specific point on the usage of Unicode follows. Then, each step of the conversion into XML and LMF is detailed. The last part presents the Jibiki lexical resources management platform used for the project. version:1
arxiv-1405-5869 | Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS) | http://arxiv.org/abs/1405.5869 | id:1405.5869 author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DS cs.IR cs.LG  published:2014-05-22 summary:We present the first provably sublinear time algorithm for approximate \emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets. version:1
arxiv-1405-5829 | Node Classification in Uncertain Graphs | http://arxiv.org/abs/1405.5829 | id:1405.5829 author:Michele Dallachiesa, Charu Aggarwal, Themis Palpanas category:cs.DB cs.LG  published:2014-05-22 summary:In many real applications that use and analyze networked data, the links in the network graph may be erroneous, or derived from probabilistic techniques. In such cases, the node classification problem can be challenging, since the unreliability of the links may affect the final results of the classification process. If the information about link reliability is not used explicitly, the classification accuracy in the underlying network may be affected adversely. In this paper, we focus on situations that require the analysis of the uncertainty that is present in the graph structure. We study the novel problem of node classification in uncertain graphs, by treating uncertainty as a first-class citizen. We propose two techniques based on a Bayes model and automatic parameter selection, and show that the incorporation of uncertainty in the classification process as a first-class citizen is beneficial. We experimentally evaluate the proposed approach using different real data sets, and study the behavior of the algorithms under different conditions. The results demonstrate the effectiveness and efficiency of our approach. version:1
arxiv-1405-5873 | Compressive Mining: Fast and Optimal Data Mining in the Compressed Domain | http://arxiv.org/abs/1405.5873 | id:1405.5873 author:Michail Vlachos, Nikolaos Freris, Anastasios Kyrillidis category:stat.ML cs.DS cs.IT math.IT  published:2014-05-22 summary:Real-world data typically contain repeated and periodic patterns. This suggests that they can be effectively represented and compressed using only a few coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.). However, distance estimation when the data are represented using different sets of coefficients is still a largely unexplored area. This work studies the optimization problems related to obtaining the \emph{tightest} lower/upper bound on Euclidean distances when each data object is potentially compressed using a different set of orthonormal coefficients. Our technique leads to tighter distance estimates, which translates into more accurate search, learning and mining operations \textit{directly} in the compressed domain. We formulate the problem of estimating lower/upper distance bounds as an optimization problem. We establish the properties of optimal solutions, and leverage the theoretical analysis to develop a fast algorithm to obtain an \emph{exact} solution to the problem. The suggested solution provides the tightest estimation of the $L_2$-norm or the correlation. We show that typical data-analysis operations, such as k-NN search or k-Means clustering, can operate more accurately using the proposed compression and distance reconstruction technique. We compare it with many other prevalent compression and reconstruction techniques, including random projections and PCA-based techniques. We highlight a surprising result, namely that when the data are highly sparse in some basis, our technique may even outperform PCA-based compression. The contributions of this work are generic as our methodology is applicable to any sequential or high-dimensional data as well as to any orthogonal data transformation used for the underlying data compression scheme. version:1
arxiv-1405-5674 | MotàMot project: conversion of a French-Khmer published dictionary for building a multilingual lexical system | http://arxiv.org/abs/1405.5674 | id:1405.5674 author:Mathieu Mangeot category:cs.CL  published:2014-05-22 summary:Economic issues related to the information processing techniques are very important. The development of such technologies is a major asset for developing countries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and Thailand. The MotAMot project aims to computerize an under-resourced language: Khmer, spoken mainly in Cambodia. The main goal of the project is the development of a multilingual lexical system targeted for Khmer. The macrostructure is a pivot one with each word sense of each language linked to a pivot axi. The microstructure comes from a simplification of the explanatory and combinatory dictionary. The lexical system has been initialized with data coming mainly from the conversion of the French-Khmer bilingual dictionary of Denis Richer from Word to XML format. The French part was completed with pronunciation and parts-of-speech coming from the FeM French-english-Malay dictionary. The Khmer headwords noted in IPA in the Richer dictionary were converted to Khmer writing with OpenFST, a finite state transducer tool. The resulting resource is available online for lookup, editing, download and remote programming via a REST API on a Jibiki platform. version:1
arxiv-1405-5654 | Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning | http://arxiv.org/abs/1405.5654 | id:1405.5654 author:Lijiang Chen category:cs.CL  published:2014-05-22 summary:Although the parallel corpus has an irreplaceable role in machine translation, its scale and coverage is still beyond the actual needs. Non-parallel corpus resources on the web have an inestimable potential value in machine translation and other natural language processing tasks. This article proposes a semi-supervised transductive learning method for expanding the training corpus in statistical machine translation system by extracting parallel sentences from the non-parallel corpus. This method only requires a small amount of labeled corpus and a large unlabeled corpus to build a high-performance classifier, especially for when there is short of labeled corpus. The experimental results show that by combining the non-parallel corpus alignment and the semi-supervised transductive learning method, we can more effectively use their respective strengths to improve the performance of machine translation system. version:1
arxiv-1310-4389 | ImageSpirit: Verbal Guided Image Parsing | http://arxiv.org/abs/1310.4389 | id:1310.4389 author:Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Jonathan Warrell, Vibhav Vineet, Paul Sturgess, Nigel Crook, Niloy Mitra, Philip Torr category:cs.GR cs.CV I.3.6; I.4.8  published:2013-10-16 summary:Humans describe images in terms of nouns and adjectives while algorithms operate on images represented as sets of pixels. Bridging this gap between how humans would like to access images versus their typical representation is the goal of image parsing, which involves assigning object and attribute labels to pixel. In this paper we propose treating nouns as object labels and adjectives as visual attribute labels. This allows us to formulate the image parsing problem as one of jointly estimating per-pixel object and attribute labels from a set of training images. We propose an efficient (interactive time) solution. Using the extracted labels as handles, our system empowers a user to verbally refine the results. This enables hands-free parsing of an image into pixel-wise object/attribute labels that correspond to human semantics. Verbally selecting objects of interests enables a novel and natural interaction modality that can possibly be used to interact with new generation devices (e.g. smart phones, Google Glass, living room devices). We demonstrate our system on a large number of real-world images with varying complexity. To help understand the tradeoffs compared to traditional mouse based interactions, results are reported for both a large scale quantitative evaluation and a user study. version:2
arxiv-1405-5474 | New Perspectives in Sinographic Language Processing Through the Use of Character Structure | http://arxiv.org/abs/1405.5474 | id:1405.5474 author:Yannis Haralambous category:cs.CL  published:2014-05-21 summary:Chinese characters have a complex and hierarchical graphical structure carrying both semantic and phonetic information. We use this structure to enhance the text model and obtain better results in standard NLP operations. First of all, to tackle the problem of graphical variation we define allographic classes of characters. Next, the relation of inclusion of a subcharacter in a characters, provides us with a directed graph of allographic classes. We provide this graph with two weights: semanticity (semantic relation between subcharacter and character) and phoneticity (phonetic relation) and calculate "most semantic subcharacter paths" for each character. Finally, adding the information contained in these paths to unigrams we claim to increase the efficiency of text mining methods. We evaluate our method on a text classification task on two corpora (Chinese and Japanese) of a total of 18 million characters and get an improvement of 3% on an already high baseline of 89.6% precision, obtained by a linear SVM classifier. Other possible applications and perspectives of the system are discussed. version:1
arxiv-1312-6203 | Spectral Networks and Locally Connected Networks on Graphs | http://arxiv.org/abs/1312.6203 | id:1312.6203 author:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun category:cs.LG cs.CV cs.NE  published:2013-12-21 summary:Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures. version:3
arxiv-1403-0628 | Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations | http://arxiv.org/abs/1403.0628 | id:1403.0628 author:H. Brendan McMahan, Francesco Orabona category:cs.LG  published:2014-03-03 summary:We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries. Moreover, using our tools, we develop an algorithm that provides a regret bound of $\mathcal{O}\Big(U \sqrt{T \log(U \sqrt{T} \log^2 T +1)}\Big)$, where $U$ is the $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to the player. This bound is optimal up to $\sqrt{\log \log T}$ terms. When $T$ is known, we derive an algorithm with an optimal regret bound (up to constant factors). For both the known and unknown $T$ case, a Normal approximation to the conditional value of the game proves to be the key analysis tool. version:2
arxiv-1405-5422 | Robust Fuzzy corner detector | http://arxiv.org/abs/1405.5422 | id:1405.5422 author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Edgar Sanchez, Marte Ramirez category:cs.CV  published:2014-05-21 summary:Reliable corner detection is an important task in determining the shape of different regions within an image. Real-life image data are always imprecise due to inherent uncertainties that may arise from the imaging process such as defocusing, illumination changes, noise, etc. Therefore, the localization and detection of corners has become a difficult task to accomplish under such imperfect situations. On the other hand, Fuzzy systems are well known for their efficient handling of impreciseness and incompleteness, which make them inherently suitable for modelling corner properties by means of a rule-based fuzzy system. The paper presents a corner detection algorithm which employs such fuzzy reasoning. The robustness of the proposed algorithm is compared to well-known conventional corner detectors and its performance is also tested over a number of benchmark images to illustrate the efficiency of the algorithm under uncertainty. version:1
arxiv-1405-5531 | Fast algorithm for Multiple-Circle detection on images using Learning Automata | http://arxiv.org/abs/1405.5531 | id:1405.5531 author:Erik Cuevas, Fernando Wario, Valentin Osuna, Daniel Zaldivar, Marco Perez category:cs.CV  published:2014-05-21 summary:Hough transform (HT) has been the most common method for circle detection exhibiting robustness but adversely demanding a considerable computational load and large storage. Alternative approaches include heuristic methods that employ iterative optimization procedures for detecting multiple circles under the inconvenience that only one circle can be marked at each optimization cycle demanding a longer execution time. On the other hand, Learning Automata (LA) is a heuristic method to solve complex multi-modal optimization problems. Although LA converges to just one global minimum, the final probability distribution holds valuable information regarding other local minima which have emerged during the optimization process. The detection process is considered as a multi-modal optimization problem, allowing the detection of multiple circular shapes through only one optimization procedure. The algorithm uses a combination of three edge points as parameters to determine circles candidates. A reinforcement signal determines if such circle candidates are actually present at the image. Guided by the values of such reinforcement signal, the set of encoded candidate circles are evolved using the LA so that they can fit into actual circular shapes over the edge-only map of the image. The overall approach is a fast multiple-circle detector despite facing complicated conditions. version:1
arxiv-1405-5406 | Circle detection on images using Learning Automata | http://arxiv.org/abs/1405.5406 | id:1405.5406 author:Erik Cuevas, Fernando Wario, Daniel Zaldivar, Marco Perez category:cs.CV  published:2014-05-21 summary:Circle detection over digital images has received considerable attention from the computer vision community over the last few years devoting a tremendous amount of research seeking for an optimal detector. This article presents an algorithm for the automatic detection of circular shapes from complicated and noisy images with no consideration of conventional Hough transform principles. The proposed algorithm is based on Learning Automata (LA) which is a probabilistic optimization method that explores an unknown random environment by progressively improving the performance via a reinforcement signal (objective function). The approach uses the encoding of three non-collinear points as a candidate circle over the edge image. A reinforcement signal (matching function) indicates if such candidate circles are actually present in the edge map. Guided by the values of such reinforcement signal, the probability set of the encoded candidate circles is modified through the LA algorithm so that they can fit to the actual circles on the edge map. Experimental results over several complex synthetic and natural images have validated the efficiency of the proposed technique regarding accuracy, speed and robustness. version:1
arxiv-1405-2641 | Multi Modal Face Recognition Using Block Based Curvelet Features | http://arxiv.org/abs/1405.2641 | id:1405.2641 author:Jyothi K, Prabhakar C. J category:cs.CV  published:2014-05-12 summary:In this paper, we present multimodal 2D +3D face recognition method using block based curvelet features. The 3D surface of face (Depth Map) is computed from the stereo face images using stereo vision technique. The statistical measures such as mean, standard deviation, variance and entropy are extracted from each block of curvelet subband for both depth and intensity images independently.In order to compute the decision score, the KNN classifier is employed independently for both intensity and depth map. Further, computed decision scoresof intensity and depth map are combined at decision level to improve the face recognition rate. The combination of intensity and depth map is verified experimentally using benchmark face database. The experimental results show that the proposed multimodal method is better than individual modality. version:2
arxiv-1405-5358 | Off-Policy Shaping Ensembles in Reinforcement Learning | http://arxiv.org/abs/1405.5358 | id:1405.5358 author:Anna Harutyunyan, Tim Brys, Peter Vrancx, Ann Nowe category:cs.AI cs.LG  published:2014-05-21 summary:Recent advances of gradient temporal-difference methods allow to learn off-policy multiple value functions in parallel with- out sacrificing convergence guarantees or computational efficiency. This opens up new possibilities for sound ensemble techniques in reinforcement learning. In this work we propose learning an ensemble of policies related through potential-based shaping rewards. The ensemble induces a combination policy by using a voting mechanism on its components. Learning happens in real time, and we empirically show the combination policy to outperform the individual policies of the ensemble. version:1
arxiv-1405-5311 | Compressive Sampling Using EM Algorithm | http://arxiv.org/abs/1405.5311 | id:1405.5311 author:Atanu Kumar Ghosh, Arnab Chakraborty category:stat.ME cs.LG stat.ML  published:2014-05-21 summary:Conventional approaches of sampling signals follow the celebrated theorem of Nyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg and Tao, is a new paradigm that goes against the conventional methods in data acquisition and provides a way of recovering signals using fewer samples than the traditional methods use. Here we suggest an alternative way of reconstructing the original signals in compressive sampling using EM algorithm. We first propose a naive approach which has certain computational difficulties and subsequently modify it to a new approach which performs better than the conventional methods of compressive sampling. The comparison of the different approaches and the performance of the new approach has been studied using simulated data. version:1
arxiv-1405-3272 | Fast and Fuzzy Private Set Intersection | http://arxiv.org/abs/1405.3272 | id:1405.3272 author:Nicholas Kersting category:cs.CR cs.CL  published:2014-05-13 summary:Private Set Intersection (PSI) is usually implemented as a sequence of encryption rounds between pairs of users, whereas the present work implements PSI in a simpler fashion: each set only needs to be encrypted once, after which each pair of users need only one ordinary set comparison. This is typically orders of magnitude faster than ordinary PSI at the cost of some ``fuzziness" in the matching, which may nonetheless be tolerable or even desirable. This is demonstrated in the case where the sets consist of English words processed with WordNet. version:2
arxiv-1405-5248 | Dynamic Hierarchical Bayesian Network for Arabic Handwritten Word Recognition | http://arxiv.org/abs/1405.5248 | id:1405.5248 author:Khaoula jayech, Nesrine Trimech, Mohamed Ali Mahjoub, Najoua Essoukri Ben Amara category:cs.CV  published:2014-05-20 summary:This paper presents a new probabilistic graphical model used to model and recognize words representing the names of Tunisian cities. In fact, this work is based on a dynamic hierarchical Bayesian network. The aim is to find the best model of Arabic handwriting to reduce the complexity of the recognition process by permitting the partial recognition. Actually, we propose a segmentation of the word based on smoothing the vertical histogram projection using different width values to reduce the error of segmentation. Then, we extract the characteristics of each cell using the Zernike and HU moments, which are invariant to rotation, translation and scaling. Our approach is tested using the IFN / ENIT database, and the experiment results are very promising. version:1
arxiv-1405-5239 | Sequential Advantage Selection for Optimal Treatment Regimes | http://arxiv.org/abs/1405.5239 | id:1405.5239 author:Ailin Fan, Wenbin Lu, Rui Song category:stat.ME stat.ML  published:2014-05-20 summary:Variable selection for optimal treatment regime in a clinical trial or an observational study is getting more attention. Most existing variable selection techniques focused on selecting variables that are important for prediction, therefore some variables that are poor in prediction but are critical for decision-making may be ignored. A qualitative interaction of a variable with treatment arises when treatment effect changes direction as the value of this variable varies. The qualitative interaction indicates the importance of this variable for decision-making. Gunter et al. (2011) proposed S-score which characterizes the magnitude of qualitative interaction of each variable with treatment individually. In this article, we developed a sequential advantage selection method based on the modified S-score. Our method selects qualitatively interacted variables sequentially, and hence excludes marginally important but jointly unimportant variables {or vice versa}. The optimal treatment regime based on variables selected via joint model is more comprehensive and reliable. With the proposed stopping criteria, our method can handle a large amount of covariates even if sample size is small. Simulation results show our method performs well in practical settings. We further applied our method to data from a clinical trial for depression. version:1
arxiv-1311-1704 | Scalable Recommendation with Poisson Factorization | http://arxiv.org/abs/1311.1704 | id:1311.1704 author:Prem Gopalan, Jake M. Hofman, David M. Blei category:cs.IR cs.AI cs.LG stat.ML  published:2013-11-07 summary:We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods. version:3
arxiv-1311-1958 | Constructing Time Series Shape Association Measures: Minkowski Distance and Data Standardization | http://arxiv.org/abs/1311.1958 | id:1311.1958 author:Ildar Batyrshin category:cs.LG  published:2013-11-07 summary:It is surprising that last two decades many works in time series data mining and clustering were concerned with measures of similarity of time series but not with measures of association that can be used for measuring possible direct and inverse relationships between time series. Inverse relationships can exist between dynamics of prices and sell volumes, between growth patterns of competitive companies, between well production data in oilfields, between wind velocity and air pollution concentration etc. The paper develops a theoretical basis for analysis and construction of time series shape association measures. Starting from the axioms of time series shape association measures it studies the methods of construction of measures satisfying these axioms. Several general methods of construction of such measures suitable for measuring time series shape similarity and shape association are proposed. Time series shape association measures based on Minkowski distance and data standardization methods are considered. The cosine similarity and the Pearsons correlation coefficient are obtained as particular cases of the proposed general methods that can be used also for construction of new association measures in data analysis. version:3
arxiv-1405-5164 | Multi-ellipses detection on images inspired by collective animal behavior | http://arxiv.org/abs/1405.5164 | id:1405.5164 author:Erik Cuevas, Maurici Gonzalez, Daniel Zaldivar, Marco Perez category:cs.CV  published:2014-05-20 summary:This paper presents a novel and effective technique for extracting multiple ellipses from an image. The approach employs an evolutionary algorithm to mimic the way animals behave collectively assuming the overall detection process as a multi-modal optimization problem. In the algorithm, searcher agents emulate a group of animals that interact to each other using simple biological rules which are modeled as evolutionary operators. In turn, such operators are applied to each agent considering that the complete group has a memory to store optimal solutions (ellipses) seen so-far by applying a competition principle. The detector uses a combination of five edge points as parameters to determine ellipse candidates (possible solutions) while a matching function determines if such ellipse candidates are actually present in the image. Guided by the values of such matching functions, the set of encoded candidate ellipses are evolved through the evolutionary algorithm so that the best candidates can be fitted into the actual ellipses within the image. Just after the optimization process ends, an analysis over the embedded memory is executed in order to find the best obtained solution (the best ellipse) and significant local minima (remaining ellipses). Experimental results over several complex synthetic and natural images have validated the efficiency of the proposed technique regarding accuracy, speed and robustness. version:1
arxiv-1405-5156 | Gaussian Approximation of Collective Graphical Models | http://arxiv.org/abs/1405.5156 | id:1405.5156 author:Li-Ping Liu, Daniel Sheldon, Thomas G. Dietterich category:cs.LG cs.AI stat.ML  published:2014-05-20 summary:The Collective Graphical Model (CGM) models a population of independent and identically distributed individuals when only collective statistics (i.e., counts of individuals) are observed. Exact inference in CGMs is intractable, and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP approximations for learning and inference. This paper studies Gaussian approximations to the CGM. As the population grows large, we show that the CGM distribution converges to a multivariate Gaussian distribution (GCGM) that maintains the conditional independence properties of the original CGM. If the observations are exact marginals of the CGM or marginals that are corrupted by Gaussian noise, inference in the GCGM approximation can be computed efficiently in closed form. If the observations follow a different noise model (e.g., Poisson), then expectation propagation provides efficient and accurate approximate inference. The accuracy and speed of GCGM inference is compared to the MCMC and MAP methods on a simulated bird migration problem. The GCGM matches or exceeds the accuracy of the MAP method while being significantly faster. version:1
arxiv-1405-5147 | Predicting Online Video Engagement Using Clickstreams | http://arxiv.org/abs/1405.5147 | id:1405.5147 author:Everaldo Aguiar, Saurabh Nagrecha, Nitesh V. Chawla category:cs.LG cs.IR I.5.2  published:2014-05-20 summary:In the nascent days of e-content delivery, having a superior product was enough to give companies an edge against the competition. With today's fiercely competitive market, one needs to be multiple steps ahead, especially when it comes to understanding consumers. Focusing on a large set of web portals owned and managed by a private communications company, we propose methods by which these sites' clickstream data can be used to provide a deep understanding of their visitors, as well as their interests and preferences. We further expand the use of this data to show that it can be effectively used to predict user engagement to video streams. version:1
arxiv-1310-8499 | Deep AutoRegressive Networks | http://arxiv.org/abs/1310.8499 | id:1310.8499 author:Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan Wierstra category:cs.LG stat.ML  published:2013-10-31 summary:We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games. version:2
arxiv-1405-5096 | Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms | http://arxiv.org/abs/1405.5096 | id:1405.5096 author:Richard Combes, Alexandre Proutiere category:cs.LG stat.ML  published:2014-05-20 summary:We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope 2009, Yu 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in non-stationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure. version:1
arxiv-1405-5050 | A Genetic Algorithm for solving Quadratic Assignment Problem(QAP) | http://arxiv.org/abs/1405.5050 | id:1405.5050 author:Hosein Azarbonyad, Reza Babazadeh category:cs.NE  published:2014-05-20 summary:The Quadratic Assignment Problem (QAP) is one of the models used for the multi-row layout problem with facilities of equal area. There are a set of n facilities and a set of n locations. For each pair of locations, a distance is specified and for each pair of facilities a weight or flow is specified (e.g., the amount of supplies transported between the two facilities). The problem is to assign all facilities to different locations with the aim of minimizing the sum of the distances multiplied by the corresponding flows. The QAP is among the most difficult NP-hard combinatorial optimization problems. Because of this, this paper presents an efficient Genetic algorithm (GA) to solve this problem in reasonable time. For validation the proposed GA some examples are selected from QAP library. The obtained results in reasonable time show the efficiency of proposed GA. version:1
arxiv-1405-4589 | A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm | http://arxiv.org/abs/1405.4589 | id:1405.4589 author:Chao Zhang, Hong-cen Mei, Hao Yang category:cs.NE cs.LG 13-XX 13Pxx 13P25  published:2014-05-19 summary:A large number of experimental data shows that Support Vector Machine (SVM) algorithm has obvious advantages in text classification, handwriting recognition, image classification, bioinformatics, and some other fields. To some degree, the optimization of SVM depends on its kernel function and Slack variable, the determinant of which is its parameters $\delta$ and c in the classification function. That is to say,to optimize the SVM algorithm, the optimization of the two parameters play a huge role. Ant Colony Optimization (ACO) is optimization algorithm which simulate ants to find the optimal path.In the available literature, we mix the ACO algorithm and Parallel algorithm together to find a well parameters. version:2
arxiv-1405-5447 | Learning to Exploit Different Translation Resources for Cross Language Information Retrieval | http://arxiv.org/abs/1405.5447 | id:1405.5447 author:Hosein Azarbonyad, Azadeh Shakery, Heshaam Faili category:cs.IR cs.CL  published:2014-05-20 summary:One of the important factors that affects the performance of Cross Language Information Retrieval(CLIR)is the quality of translations being employed in CLIR. In order to improve the quality of translations, it is important to exploit available resources efficiently. Employing different translation resources with different characteristics has many challenges. In this paper, we propose a method for exploiting available translation resources simultaneously. This method employs Learning to Rank(LTR) for exploiting different translation resources. To apply LTR methods for query translation, we define different translation relation based features in addition to context based features. We use the contextual information contained in translation resources for extracting context based features.The proposed method uses LTR to construct a translation ranking model based on defined features. The constructed model is used for ranking translation candidates of query words. To evaluate the proposed method we do English-Persian CLIR, in which we employ the translation ranking model to find translations of English queries and employ the translations to retrieve Persian documents. Experimental results show that our approach significantly outperforms single resource based CLIR methods. version:1
arxiv-1311-6594 | Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis | http://arxiv.org/abs/1311.6594 | id:1311.6594 author:Ángela Fernández, Neta Rabin, Dalia Fishelov, José R. Dorronsoro category:cs.AI cs.LG stat.ML  published:2013-11-26 summary:Non-linear dimensionality reduction techniques such as manifold learning algorithms have become a common way for processing and analyzing high-dimensional patterns that often have attached a target that corresponds to the value of an unknown function. Their application to new points consists in two steps: first, embedding the new data point into the low dimensional space and then, estimating the function value on the test point from its neighbors in the embedded space. However, finding the low dimension representation of a test point, while easy for simple but often not powerful enough procedures such as PCA, can be much more complicated for methods that rely on some kind of eigenanalysis, such as Spectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a target function is to be evaluated, averaging methods like nearest neighbors may give unstable results if the function is noisy. Thus, the smoothing of the target function with respect to the intrinsic, low-dimensional representation that describes the geometric structure of the examined data is a challenging task. In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extension of the standard Laplacian Pyramids model that incorporates a modified LOOCV procedure that avoids the large cost of the standard one and offers the following advantages: (i) it selects automatically the optimal function resolution (stopping time) adapted to the data and its noise, (ii) it is easy to apply as it does not require parameterization, (iii) it does not overfit the training set and (iv) it adds no extra cost compared to other classical interpolation methods. We illustrate numerically ALP's behavior on a synthetic problem and apply it to the computation of the DM projection of new patterns and to the extension to them of target function values on a radiation forecasting problem over very high dimensional patterns. version:2
arxiv-1405-4951 | Secure Friend Discovery via Privacy-Preserving and Decentralized Community Detection | http://arxiv.org/abs/1405.4951 | id:1405.4951 author:Pili Hu, Sherman S. M. Chow, Wing Cheong Lau category:cs.CR cs.SI stat.ML  published:2014-05-20 summary:The problem of secure friend discovery on a social network has long been proposed and studied. The requirement is that a pair of nodes can make befriending decisions with minimum information exposed to the other party. In this paper, we propose to use community detection to tackle the problem of secure friend discovery. We formulate the first privacy-preserving and decentralized community detection problem as a multi-objective optimization. We design the first protocol to solve this problem, which transforms community detection to a series of Private Set Intersection (PSI) instances using Truncated Random Walk (TRW). Preliminary theoretical results show that our protocol can uncover communities with overwhelming probability and preserve privacy. We also discuss future works, potential extensions and variations. version:1
arxiv-1308-4915 | Minimal Dirichlet energy partitions for graphs | http://arxiv.org/abs/1308.4915 | id:1308.4915 author:Braxton Osting, Chris D. White, Edouard Oudet category:math.OC cs.LG stat.ML  published:2013-08-22 summary:Motivated by a geometric problem, we introduce a new non-convex graph partitioning objective where the optimality criterion is given by the sum of the Dirichlet eigenvalues of the partition components. A relaxed formulation is identified and a novel rearrangement algorithm is proposed, which we show is strictly decreasing and converges in a finite number of iterations to a local minimum of the relaxed objective function. Our method is applied to several clustering problems on graphs constructed from synthetic data, MNIST handwritten digits, and manifold discretizations. The model has a semi-supervised extension and provides a natural representative for the clusters as well. version:2
arxiv-0907-3986 | Contextual Bandits with Similarity Information | http://arxiv.org/abs/0907.3986 | id:0907.3986 author:Aleksandrs Slivkins category:cs.DS cs.LG F.2.2; F.1.2  published:2009-07-23 summary:In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms. We consider similarity information in the setting of "contextual bandits", a natural extension of the basic MAB problem where before each round an algorithm is given the "context" -- a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on webpages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a "similarity distance" between the context-arm pairs which gives an upper bound on the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses "uniform" partitions of the similarity space, which is potentially wasteful. We design more efficient algorithms that are based on adaptive partitions adjusted to "popular" context and "high-payoff" arms. version:5
arxiv-1310-6778 | Bayesian estimation of possible causal direction in the presence of latent confounders using a linear non-Gaussian acyclic structural equation model with individual-specific effects | http://arxiv.org/abs/1310.6778 | id:1310.6778 author:Shohei Shimizu, Kenneth Bollen category:stat.ML  published:2013-10-24 summary:We consider learning the possible causal direction of two observed variables in the presence of latent confounding variables. Several existing methods have been shown to consistently estimate causal direction assuming linear or some type of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is actually violated. In this paper, we first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that allows latent confounders to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data. version:2
arxiv-1405-4918 | Fighting Authorship Linkability with Crowdsourcing | http://arxiv.org/abs/1405.4918 | id:1405.4918 author:Mishari Almishari, Ekin Oguz, Gene Tsudik category:cs.DL cs.CL  published:2014-05-19 summary:Massive amounts of contributed content -- including traditional literature, blogs, music, videos, reviews and tweets -- are available on the Internet today, with authors numbering in many millions. Textual information, such as product or service reviews, is an important and increasingly popular type of content that is being used as a foundation of many trendy community-based reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown that, due partly to their specialized/topical nature, sets of reviews authored by the same person are readily linkable based on simple stylometric features. In practice, this means that individuals who author more than a few reviews under different accounts (whether within one site or across multiple sites) can be linked, which represents a significant loss of privacy. In this paper, we start by showing that the problem is actually worse than previously believed. We then explore ways to mitigate authorship linkability in community-based reviewing. We first attempt to harness the global power of crowdsourcing by engaging random strangers into the process of re-writing reviews. As our empirical results (obtained from Amazon Mechanical Turk) clearly demonstrate, crowdsourcing yields impressively sensible reviews that reflect sufficiently different stylometric characteristics such that prior stylometric linkability techniques become largely ineffective. We also consider using machine translation to automatically re-write reviews. Contrary to what was previously believed, our results show that translation decreases authorship linkability as the number of intermediate languages grows. Finally, we explore the combination of crowdsourcing and machine translation and report on the results. version:1
arxiv-1401-2678 | Inference in High Dimensions with the Penalized Score Test | http://arxiv.org/abs/1401.2678 | id:1401.2678 author:Arend Voorman, Ali Shojaie, Daniela Witten category:stat.ME stat.ML  published:2014-01-12 summary:In recent years, there has been considerable theoretical development regarding variable selection consistency of penalized regression techniques, such as the lasso. However, there has been relatively little work on quantifying the uncertainty in these selection procedures. In this paper, we propose a new method for inference in high dimensions using a score test based on penalized regression. In this test, we perform penalized regression of an outcome on all but a single feature, and test for correlation of the residuals with the held-out feature. This procedure is applied to each feature in turn. Interestingly, when an $\ell_1$ penalty is used, the sparsity pattern of the lasso corresponds exactly to a decision based on the proposed test. Further, when an $\ell_2$ penalty is used, the test corresponds precisely to a score test in a mixed effects model, in which the effects of all but one feature are assumed to be random. We formulate the hypothesis being tested as a compromise between the null hypotheses tested in simple linear regression on each feature and in multiple linear regression on all features, and develop reference distributions for some well-known penalties. We also examine the behavior of the test on real and simulated data. version:3
arxiv-1405-4897 | Screening Tests for Lasso Problems | http://arxiv.org/abs/1405.4897 | id:1405.4897 author:Zhen James Xiang, Yun Wang, Peter J. Ramadge category:cs.LG stat.ML  published:2014-05-19 summary:This paper is a survey of dictionary screening for the lasso problem. The lasso problem seeks a sparse linear combination of the columns of a dictionary to best match a given target vector. This sparse representation has proven useful in a variety of subsequent processing and decision tasks. For a given target vector, dictionary screening quickly identifies a subset of dictionary columns that will receive zero weight in a solution of the corresponding lasso problem. These columns can be removed from the dictionary, prior to solving the lasso problem, without impacting the optimality of the solution obtained. This has two potential advantages: it reduces the size of the dictionary, allowing the lasso problem to be solved with less resources, and it may speed up obtaining a solution. Using a geometrically intuitive framework, we provide basic insights for understanding useful lasso screening tests and their limitations. We also provide illustrative numerical studies on several datasets. version:1
arxiv-1402-0562 | Online Stochastic Optimization under Correlated Bandit Feedback | http://arxiv.org/abs/1402.0562 | id:1402.0562 author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG cs.SY  published:2014-02-04 summary:In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel any-time $\mathcal{X}$-armed bandit algorithm, and derive regret bounds matching the performance of existing state-of-the-art in terms of dependency on number of steps and smoothness factor. The main advantage of HCT is that it handles the challenging case of correlated rewards, whereas existing methods require that the reward-generating process of each arm is an identically and independent distributed (iid) random process. HCT also improves on the state-of-the-art in terms of its memory requirement as well as requiring a weaker smoothness assumption on the mean-reward function in compare to the previous anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results. version:3
arxiv-1405-4807 | Scalable Semidefinite Relaxation for Maximum A Posterior Estimation | http://arxiv.org/abs/1405.4807 | id:1405.4807 author:Qixing Huang, Yuxin Chen, Leonidas Guibas category:cs.LG cs.CV cs.IT math.IT math.OC stat.ML  published:2014-05-19 summary:Maximum a posteriori (MAP) inference over discrete Markov random fields is a fundamental task spanning a wide spectrum of real-world applications, which is known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of the new relaxation. Encouragingly, the proposed procedure allows solving SDR for large-scale problems, e.g., problems on a grid graph comprising hundreds of thousands of variables with multiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability, in contrast to the commonly held belief that semidefinite relaxation can only been applied on small-scale MRF problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC in terms of both the quality of the solutions and computation time. Experimental results demonstrate that for a broad class of problems, SDPAD-LR outperforms state-of-the-art algorithms in producing better MAP assignment in an efficient manner. version:1
arxiv-1405-4758 | Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms | http://arxiv.org/abs/1405.4758 | id:1405.4758 author:Stefan Magureanu, Richard Combes, Alexandre Proutiere category:cs.LG  published:2014-05-19 summary:We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities. version:1
arxiv-1312-1054 | Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians | http://arxiv.org/abs/1312.1054 | id:1312.1054 author:Constantinos Daskalakis, Gautam Kamath category:cs.DS cs.LG math.PR math.ST stat.TH  published:2013-12-04 summary:We provide an algorithm for properly learning mixtures of two single-dimensional Gaussians without any separability assumptions. Given $\tilde{O}(1/\varepsilon^2)$ samples from an unknown mixture, our algorithm outputs a mixture that is $\varepsilon$-close in total variation distance, in time $\tilde{O}(1/\varepsilon^5)$. Our sample complexity is optimal up to logarithmic factors, and significantly improves upon both Kalai et al., whose algorithm has a prohibitive dependence on $1/\varepsilon$, and Feldman et al., whose algorithm requires bounds on the mixture parameters and depends pseudo-polynomially in these parameters. One of our main contributions is an improved and generalized algorithm for selecting a good candidate distribution from among competing hypotheses. Namely, given a collection of $N$ hypotheses containing at least one candidate that is $\varepsilon$-close to an unknown distribution, our algorithm outputs a candidate which is $O(\varepsilon)$-close to the distribution. The algorithm requires ${O}(\log{N}/\varepsilon^2)$ samples from the unknown distribution and ${O}(N \log N/\varepsilon^2)$ time, which improves previous such results (such as the Scheff\'e estimator) from a quadratic dependence of the running time on $N$ to quasilinear. Given the wide use of such results for the purpose of hypothesis selection, our improved algorithm implies immediate improvements to any such use. version:3
arxiv-1402-4963 | Vesselness via Multiple Scale Orientation Scores | http://arxiv.org/abs/1402.4963 | id:1402.4963 author:Julius Hannink, Remco Duits, Erik Bekkers category:cs.CV  published:2014-02-20 summary:The multi-scale Frangi vesselness filter is an established tool in (retinal) vascular imaging. However, it cannot cope with crossings or bifurcations, since it only looks for elongated structures. Therefore, we disentangle crossing structures in the image via (multiple scale) invertible orientation scores. The described vesselness filter via scale-orientation scores performs considerably better at enhancing vessels throughout crossings and bifurcations than the Frangi version. Both methods are evaluated on a public dataset. Performance is measured by comparing ground truth data to the segmentation results obtained by basic thresholding and morphological component analysis of the filtered images. version:4
arxiv-1405-4599 | Modelling Data Dispersion Degree in Automatic Robust Estimation for Multivariate Gaussian Mixture Models with an Application to Noisy Speech Processing | http://arxiv.org/abs/1405.4599 | id:1405.4599 author:Dalei Wu, Haiqing Wu category:cs.CL cs.LG stat.ML  published:2014-05-19 summary:The trimming scheme with a prefixed cutoff portion is known as a method of improving the robustness of statistical models such as multivariate Gaussian mixture models (MG- MMs) in small scale tests by alleviating the impacts of outliers. However, when this method is applied to real- world data, such as noisy speech processing, it is hard to know the optimal cut-off portion to remove the outliers and sometimes removes useful data samples as well. In this paper, we propose a new method based on measuring the dispersion degree (DD) of the training data to avoid this problem, so as to realise automatic robust estimation for MGMMs. The DD model is studied by using two different measures. For each one, we theoretically prove that the DD of the data samples in a context of MGMMs approximately obeys a specific (chi or chi-square) distribution. The proposed method is evaluated on a real-world application with a moderately-sized speaker recognition task. Experiments show that the proposed method can significantly improve the robustness of the conventional training method of GMMs for speaker recognition. version:1
arxiv-1404-4095 | Multi-borders classification | http://arxiv.org/abs/1404.4095 | id:1404.4095 author:Peter Mills category:stat.ML cs.LG  published:2014-04-15 summary:The number of possible methods of generalizing binary classification to multi-class classification increases exponentially with the number of class labels. Often, the best method of doing so will be highly problem dependent. Here we present classification software in which the partitioning of multi-class classification problems into binary classification problems is specified using a recursive control language. version:3
arxiv-1405-4583 | ESSP: An Efficient Approach to Minimizing Dense and Nonsubmodular Energy Functions | http://arxiv.org/abs/1405.4583 | id:1405.4583 author:Wei Feng, Jiaya Jia, Zhi-Qiang Liu category:cs.CV cs.LG  published:2014-05-19 summary:Many recent advances in computer vision have demonstrated the impressive power of dense and nonsubmodular energy functions in solving visual labeling problems. However, minimizing such energies is challenging. None of existing techniques (such as s-t graph cut, QPBO, BP and TRW-S) can individually do this well. In this paper, we present an efficient method, namely ESSP, to optimize binary MRFs with arbitrary pairwise potentials, which could be nonsubmodular and with dense connectivity. We also provide a comparative study of our approach and several recent promising methods. From our study, we make some reasonable recommendations of combining existing methods that perform the best in different situations for this challenging problem. Experimental results validate that for dense and nonsubmodular energy functions, the proposed approach can usually obtain lower energies than the best combination of other techniques using comparably reasonable time. version:1
arxiv-1405-4574 | Kronecker PCA Based Spatio-Temporal Modeling of Video for Dismount Classification | http://arxiv.org/abs/1405.4574 | id:1405.4574 author:Kristjan H. Greenewald, Alfred O. Hero III category:cs.CV stat.ME  published:2014-05-19 summary:We consider the application of KronPCA spatio-temporal modeling techniques [Greenewald et al 2013, Tsiligkaridis et al 2013] to the extraction of spatiotemporal features for video dismount classification. KronPCA performs a low-rank type of dimensionality reduction that is adapted to spatio-temporal data and is characterized by the T frame multiframe mean and covariance of p spatial features. For further regularization and improved inverse estimation, we also use the diagonally corrected KronPCA shrinkage methods we presented in [Greenewald et al 2013]. We apply this very general method to the modeling of the multivariate temporal behavior of HOG features extracted from pedestrian bounding boxes in video, with gender classification in a challenging dataset chosen as a specific application. The learned covariances for each class are used to extract spatiotemporal features which are then classified, achieving competitive classification performance. version:1
arxiv-1405-4543 | A Distributed Algorithm for Training Nonlinear Kernel Machines | http://arxiv.org/abs/1405.4543 | id:1405.4543 author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan category:cs.LG  published:2014-05-18 summary:This paper concerns the distributed training of nonlinear kernel machines on Map-Reduce. We show that a re-formulation of Nystr\"om approximation based solution which is solved using gradient based techniques is well suited for this, especially when it is necessary to work with a large number of basis points. The main advantages of this approach are: avoidance of computing the pseudo-inverse of the kernel sub-matrix corresponding to the basis points; simplicity and efficiency of the distributed part of the computations; and, friendliness to stage-wise addition of basis points. We implement the method using an AllReduce tree on Hadoop and demonstrate its value on a few large benchmark datasets. version:1
arxiv-1405-4510 | A Memetic Algorithm for the Linear Ordering Problem with Cumulative Costs | http://arxiv.org/abs/1405.4510 | id:1405.4510 author:Tao Ye, Kan Zhou, Zhipeng Lu, Jin-Kao Hao category:cs.NE  published:2014-05-18 summary:This paper introduces an effective memetic algorithm for the linear ordering problem with cumulative costs. The proposed algorithm combines an order-based recombination operator with an improved forward-backward local search procedure and employs a solution quality based replacement criterion for pool updating. Extensive experiments on 118 well-known benchmark instances show that the proposed algorithm achieves competitive results by identifying 46 new upper bounds. Furthermore, some critical ingredients of our algorithm are analyzed to understand the source of its performance. version:1
arxiv-1405-4507 | A Multi-parent Memetic Algorithm for the Linear Ordering Problem | http://arxiv.org/abs/1405.4507 | id:1405.4507 author:Tao Ye, Tao Wang, Zhipeng Lu, Jin-Kao Hao category:cs.NE math.OC  published:2014-05-18 summary:In this paper, we present a multi-parent memetic algorithm (denoted by MPM) for solving the classic Linear Ordering Problem (LOP). The MPM algorithm integrates in particular a multi-parent recombination operator for generating offspring solutions and a distance-and-quality based criterion for pool updating. Our MPM algorithm is assessed on 8 sets of 484 widely used LOP instances and compared with several state-of-the-art algorithms in the literature, showing the efficacy of the MPM algorithm. Specifically, for the 255 instances whose optimal solutions are unknown, the MPM is able to detect better solutions than the previous best-known ones for 66 instances, while matching the previous best-known results for 163 instances. Furthermore, some additional experiments are carried out to analyze the key elements and important parameters of MPM. version:1
arxiv-1405-4506 | Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice | http://arxiv.org/abs/1405.4506 | id:1405.4506 author:Xiaojiang Peng, Limin Wang, Xingxing Wang, Yu Qiao category:cs.CV  published:2014-05-18 summary:Video based action recognition is one of the important and challenging problems in computer vision research. Bag of Visual Words model (BoVW) with local features has become the most popular method and obtained the state-of-the-art performance on several realistic datasets, such as the HMDB51, UCF50, and UCF101. BoVW is a general pipeline to construct a global representation from a set of local features, which is mainly composed of five steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization. Many efforts have been made in each step independently in different scenarios and their effect on action recognition is still unknown. Meanwhile, video data exhibits different views of visual pattern, such as static appearance and motion dynamics. Multiple descriptors are usually extracted to represent these different views. Many feature fusion methods have been developed in other areas and their influence on action recognition has never been investigated before. This paper aims to provide a comprehensive study of all steps in BoVW and different fusion methods, and uncover some good practice to produce a state-of-the-art action recognition system. Specifically, we explore two kinds of local features, ten kinds of encoding methods, eight kinds of pooling and normalization strategies, and three kinds of fusion methods. We conclude that every step is crucial for contributing to the final recognition rate. Furthermore, based on our comprehensive study, we propose a simple yet effective representation, called hybrid representation, by exploring the complementarity of different BoVW frameworks and local descriptors. Using this representation, we obtain the state-of-the-art on the three challenging datasets: HMDB51 (61.1%), UCF50 (92.3%), and UCF101 (87.9%). version:1
arxiv-1405-4471 | Online Learning with Composite Loss Functions | http://arxiv.org/abs/1405.4471 | id:1405.4471 author:Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres category:cs.LG  published:2014-05-18 summary:We study a new class of online learning problems where each of the online algorithm's actions is assigned an adversarial value, and the loss of the algorithm at each step is a known and deterministic function of the values assigned to its recent actions. This class includes problems where the algorithm's loss is the minimum over the recent adversarial values, the maximum over the recent values, or a linear combination of the recent values. We analyze the minimax regret of this class of problems when the algorithm receives bandit feedback, and prove that when the minimum or maximum functions are used, the minimax regret is $\tilde \Omega(T^{2/3})$ (so called hard online learning problems), and when a linear function is used, the minimax regret is $\tilde O(\sqrt{T})$ (so called easy learning problems). Previously, the only online learning problem that was known to be provably hard was the multi-armed bandit with switching costs. version:1
arxiv-1405-7975 | Multi-layered graph-based multi-document summarization model | http://arxiv.org/abs/1405.7975 | id:1405.7975 author:Ercan Canhasi category:cs.IR cs.CL  published:2014-05-17 summary:Multi-document summarization is a process of automatic generation of a compressed version of the given collection of documents. Recently, the graph-based models and ranking algorithms have been actively investigated by the extractive document summarization community. While most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences (e.g. sentence similarity weighted by document importance), in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations (e.g. a part of sentence similarity). version:1
arxiv-1405-4433 | Preliminary Report on the Structure of Croatian Linguistic Co-occurrence Networks | http://arxiv.org/abs/1405.4433 | id:1405.4433 author:Domagoj Margan, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL cs.SI physics.soc-ph  published:2014-05-17 summary:In this article, we investigate the structure of Croatian linguistic co-occurrence networks. We examine the change of network structure properties by systematically varying the co-occurrence window sizes, the corpus sizes and removing stopwords. In a co-occurrence window of size $n$ we establish a link between the current word and $n-1$ subsequent words. The results point out that the increase of the co-occurrence window size is followed by a decrease in diameter, average path shortening and expectedly condensing the average clustering coefficient. The same can be noticed for the removal of the stopwords. Finally, since the size of texts is reflected in the network properties, our results suggest that the corpus influence can be reduced by increasing the co-occurrence window size. version:1
arxiv-1311-2972 | Learning Mixtures of Discrete Product Distributions using Spectral Decompositions | http://arxiv.org/abs/1311.2972 | id:1311.2972 author:Prateek Jain, Sewoong Oh category:stat.ML cs.CC cs.IT cs.LG math.IT  published:2013-11-12 summary:We study the problem of learning a distribution from samples, when the underlying distribution is a mixture of product distributions over discrete domains. This problem is motivated by several practical applications such as crowd-sourcing, recommendation systems, and learning Boolean functions. The existing solutions either heavily rely on the fact that the number of components in the mixtures is finite or have sample/time complexity that is exponential in the number of components. In this paper, we introduce a polynomial time/sample complexity method for learning a mixture of $r$ discrete product distributions over $\{1, 2, \dots, \ell\}^n$, for general $\ell$ and $r$. We show that our approach is statistically consistent and further provide finite sample guarantees. We use techniques from the recent work on tensor decompositions for higher-order moment matching. A crucial step in these moment matching methods is to construct a certain matrix and a certain tensor with low-rank spectral decompositions. These tensors are typically estimated directly from the samples. The main challenge in learning mixtures of discrete product distributions is that these low-rank tensors cannot be obtained directly from the sample moments. Instead, we reduce the tensor estimation problem to: $a$) estimating a low-rank matrix using only off-diagonal block elements; and $b$) estimating a tensor using a small number of linear measurements. Leveraging on recent developments in matrix completion, we give an alternating minimization based method to estimate the low-rank matrix, and formulate the tensor completion problem as a least-squares problem. version:2
arxiv-1405-4423 | A two-step learning approach for solving full and almost full cold start problems in dyadic prediction | http://arxiv.org/abs/1405.4423 | id:1405.4423 author:Tapio Pahikkala, Michiel Stock, Antti Airola, Tero Aittokallio, Bernard De Baets, Willem Waegeman category:cs.LG  published:2014-05-17 summary:Dyadic prediction methods operate on pairs of objects (dyads), aiming to infer labels for out-of-sample dyads. We consider the full and almost full cold start problem in dyadic prediction, a setting that occurs when both objects in an out-of-sample dyad have not been observed during training, or if one of them has been observed, but very few times. A popular approach for addressing this problem is to train a model that makes predictions based on a pairwise feature representation of the dyads, or, in case of kernel methods, based on a tensor product pairwise kernel. As an alternative to such a kernel approach, we introduce a novel two-step learning algorithm that borrows ideas from the fields of pairwise learning and spectral filtering. We show theoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement. version:1
arxiv-1405-4394 | Identification of functionally related enzymes by learning-to-rank methods | http://arxiv.org/abs/1405.4394 | id:1405.4394 author:Michiel Stock, Thomas Fober, Eyke Hüllermeier, Serghei Glinca, Gerhard Klebe, Tapio Pahikkala, Antti Airola, Bernard De Baets, Willem Waegeman category:cs.LG cs.CE q-bio.QM stat.ML  published:2014-05-17 summary:Enzyme sequences and structures are routinely used in the biological sciences as queries to search for functionally related enzymes in online databases. To this end, one usually departs from some notion of similarity, comparing two enzymes by looking for correspondences in their sequences, structures or surfaces. For a given query, the search operation results in a ranking of the enzymes in the database, from very similar to dissimilar enzymes, while information about the biological function of annotated database enzymes is ignored. In this work we show that rankings of that kind can be substantially improved by applying kernel-based learning algorithms. This approach enables the detection of statistical dependencies between similarities of the active cleft and the biological function of annotated enzymes. This is in contrast to search-based approaches, which do not take annotated training data into account. Similarity measures based on the active cleft are known to outperform sequence-based or structure-based measures under certain conditions. We consider the Enzyme Commission (EC) classification hierarchy for obtaining annotated enzymes during the training phase. The results of a set of sizeable experiments indicate a consistent and significant improvement for a set of similarity measures that exploit information about small cavities in the surface of enzymes. version:1
arxiv-1405-4392 | That's sick dude!: Automatic identification of word sense change across different timescales | http://arxiv.org/abs/1405.4392 | id:1405.4392 author:Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh Mukherjee, Pawan Goyal category:cs.CL cs.AI 68T50  published:2014-05-17 summary:In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points. Subsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died. We conduct a thorough evaluation of the proposed methodology both manually as well as through comparison with WordNet. Manual evaluation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 randomly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respectively confirmed by WordNet. Our approach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search. version:1
arxiv-1405-4390 | Real Time Object Tracking Based on Inter-frame Coding: A Review | http://arxiv.org/abs/1405.4390 | id:1405.4390 author:Shraddha Mehta, Vaishali Kalariya category:cs.CV  published:2014-05-17 summary:Inter-frame Coding plays significant role for video Compression and Computer Vision. Computer vision systems have been incorporated in many real life applications (e.g. surveillance systems, medical imaging, robot navigation and identity verification systems). Object tracking is a key computer vision topic, which aims at detecting the position of a moving object from a video sequence. The application of Inter-frame Coding for low frame rate video, as well as for low resolution video. Various methods based on Top-down approach just like kernel based or mean shift technique are used to track the object for video, So, Inter-frame Coding algorithms are widely adopted by video coding standards, mainly due to their simplicity and good distortion performance for object tracking. version:1
arxiv-1405-4389 | Efficient Tracking of a Moving Object using Inter-Frame Coding | http://arxiv.org/abs/1405.4389 | id:1405.4389 author:Shraddha Mehta, Vaishali Kalariya category:cs.CV  published:2014-05-17 summary:Video surveillance has long been in use to monitor security sensitive areas such as banks, department stores, highways, crowded public places and borders.The advance in computing power, availability of large-capacity storage devices and high speed network infrastructure paved the way for cheaper, multi-sensor video surveillance systems.Traditionally, the video outputs are processed online by human operators and are usually saved to tapes for later use only after a forensic event.The increase in the number of cameras in ordinary surveillance systems overloaded both the human operators and the storage devices with high volumes of data and made it in-feasible to ensure proper monitoring of sensitive areas for long times. version:1
arxiv-1405-4364 | Thematically Reinforced Explicit Semantic Analysis | http://arxiv.org/abs/1405.4364 | id:1405.4364 author:Yannis Haralambous, Vitaly Klyuev category:cs.CL 68T50  published:2014-05-17 summary:We present an extended, thematically reinforced version of Gabrilovich and Markovitch's Explicit Semantic Analysis (ESA), where we obtain thematic information through the category structure of Wikipedia. For this we first define a notion of categorical tfidf which measures the relevance of terms in categories. Using this measure as a weight we calculate a maximal spanning tree of the Wikipedia corpus considered as a directed graph of pages and categories. This tree provides us with a unique path of "most related categories" between each page and the top of the hierarchy. We reinforce tfidf of words in a page by aggregating it with categorical tfidfs of the nodes of these paths, and define a thematically reinforced ESA semantic relatedness measure which is more robust than standard ESA and less sensitive to noise caused by out-of-context words. We apply our method to the French Wikipedia corpus, evaluate it through a text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a precision increase of 9-10% compared with standard ESA. version:1
arxiv-1405-4324 | Active Semi-Supervised Learning Using Sampling Theory for Graph Signals | http://arxiv.org/abs/1405.4324 | id:1405.4324 author:Akshay Gadde, Aamir Anis, Antonio Ortega category:cs.LG stat.ML  published:2014-05-16 summary:We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method. version:1
arxiv-1405-4322 | Leveraging Evolutionary Search to Discover Self-Adaptive and Self-Organizing Cellular Automata | http://arxiv.org/abs/1405.4322 | id:1405.4322 author:David B. Knoester, Heather J. Goldsby, Christoph Adami category:cs.NE nlin.CG  published:2014-05-16 summary:Building self-adaptive and self-organizing (SASO) systems is a challenging problem, in part because SASO principles are not yet well understood and few platforms exist for exploring them. Cellular automata (CA) are a well-studied approach to exploring the principles underlying self-organization. A CA comprises a lattice of cells whose states change over time based on a discrete update function. One challenge to developing CA is that the relationship of an update function, which describes the local behavior of each cell, to the global behavior of the entire CA is often unclear. As a result, many researchers have used stochastic search techniques, such as evolutionary algorithms, to automatically discover update functions that produce a desired global behavior. However, these update functions are typically defined in a way that does not provide for self-adaptation. Here we describe an approach to discovering CA update functions that are both self-adaptive and self-organizing. Specifically, we use a novel evolutionary algorithm-based approach to discover finite state machines (FSMs) that implement update functions for CA. We show how this approach is able to evolve FSM-based update functions that perform well on the density classification task for 1-, 2-, and 3-dimensional CA. Moreover, we show that these FSMs are self-adaptive, self-organizing, and highly scalable, often performing well on CA that are orders of magnitude larger than those used to evaluate performance during the evolutionary search. These results demonstrate that CA are a viable platform for studying the integration of self-adaptation and self-organization, and strengthen the case for using evolutionary algorithms as a component of SASO systems. version:1
arxiv-1405-4308 | Coarse-to-Fine Classification via Parametric and Nonparametric Models for Computer-Aided Diagnosis | http://arxiv.org/abs/1405.4308 | id:1405.4308 author:Meizhu Liu, Le Lu, Xiaojing Ye, Shipeng Yu category:cs.CV  published:2014-05-16 summary:Classification is one of the core problems in Computer-Aided Diagnosis (CAD), targeting for early cancer detection using 3D medical imaging interpretation. High detection sensitivity with desirably low false positive (FP) rate is critical for a CAD system to be accepted as a valuable or even indispensable tool in radiologists' workflow. Given various spurious imagery noises which cause observation uncertainties, this remains a very challenging task. In this paper, we propose a novel, two-tiered coarse-to-fine (CTF) classification cascade framework to tackle this problem. We first obtain classification-critical data samples (e.g., samples on the decision boundary) extracted from the holistic data distributions using a robust parametric model (e.g., \cite{Raykar08}); then we build a graph-embedding based nonparametric classifier on sampled data, which can more accurately preserve or formulate the complex classification boundary. These two steps can also be considered as effective "sample pruning" and "feature pursuing + $k$NN/template matching", respectively. Our approach is validated comprehensively in colorectal polyp detection and lung nodule detection CAD systems, as the top two deadly cancers, using hospital scale, multi-site clinical datasets. The results show that our method achieves overall better classification/detection performance than existing state-of-the-art algorithms using single-layer classifiers, such as the support vector machine variants \cite{Wang08}, boosting \cite{Slabaugh10}, logistic regression \cite{Ravesteijn10}, relevance vector machine \cite{Raykar08}, $k$-nearest neighbor \cite{Murphy09} or spectral projections on graph \cite{Cai08}. version:1
arxiv-1405-4273 | Compositional Morphology for Word Representations and Language Modelling | http://arxiv.org/abs/1405.4273 | id:1405.4273 author:Jan A. Botha, Phil Blunsom category:cs.CL 68T50 I.2.7; I.2.6  published:2014-05-16 summary:This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models. version:1
arxiv-1405-4248 | Les mathématiques de la langue : l'approche formelle de Montague | http://arxiv.org/abs/1405.4248 | id:1405.4248 author:Yannis Haralambous category:cs.CL 68T50  published:2014-05-16 summary:We present a natural language modelization method which is strongely relying on mathematics. This method, called "Formal Semantics," has been initiated by the American linguist Richard M. Montague in the 1970's. It uses mathematical tools such as formal languages and grammars, first-order logic, type theory and $\lambda$-calculus. Our goal is to have the reader discover both Montagovian formal semantics and the mathematical tools that he used in his method. ----- Nous pr\'esentons une m\'ethode de mod\'elisation de la langue naturelle qui est fortement bas\'ee sur les math\'ematiques. Cette m\'ethode, appel\'ee {\guillemotleft}s\'emantique formelle{\guillemotright}, a \'et\'e initi\'ee par le linguiste am\'ericain Richard M. Montague dans les ann\'ees 1970. Elle utilise des outils math\'ematiques tels que les langages et grammaires formels, la logique du 1er ordre, la th\'eorie de types et le $\lambda$-calcul. Nous nous proposons de faire d\'ecouvrir au lecteur tant la s\'emantique formelle de Montague que les outils math\'ematiques dont il s'est servi. version:1
arxiv-1303-0341 | Matrix Completion via Max-Norm Constrained Optimization | http://arxiv.org/abs/1303.0341 | id:1303.0341 author:T. Tony Cai, Wen-Xin Zhou category:cs.LG cs.IT math.IT stat.ML 62H12  15A83  published:2013-03-02 summary:Matrix completion has been well studied under the uniform sampling model and the trace-norm regularized methods perform well both theoretically and numerically in such a setting. However, the uniform sampling model is unrealistic for a range of applications and the standard trace-norm relaxation can behave very poorly when the sampling distribution is non-uniform. In this paper we propose and analyze a max-norm constrained empirical risk minimization method for noisy matrix completion under a general sampling model. The optimal rate of convergence is established under the Frobenius norm loss in the context of approximately low-rank matrix reconstruction. It is shown that the max-norm constrained method is minimax rate-optimal and it yields a uni?ed and robust approximate recovery guarantee, with respect to the sampling distributions. The computational effectiveness of this method is also studied, based on a first-order algorithm for solving convex programs involving a max-norm constraint. version:2
arxiv-1402-3869 | FTVd is beyond Fast Total Variation regularized Deconvolution | http://arxiv.org/abs/1402.3869 | id:1402.3869 author:Yilun Wang category:cs.CV G.1.6; G.4; I.4.4  published:2014-02-17 summary:In this paper, we revisit the "FTVd" algorithm for Fast Total Variation Regularized Deconvolution, which has been widely used in the past few years. Both its original version implemented in the MATLAB software FTVd 3.0 and its related variant implemented in the latter version FTVd 4.0 are considered \cite{Wang08FTVdsoftware}. We propose that the intermediate results during the iterations are the solutions of a series of combined Tikhonov and total variation regularized image deconvolution models and therefore some of them often have even better image quality than the final solution, which is corresponding to the pure total variation regularized model. version:2
arxiv-1405-4054 | Optimized Cartesian $K$-Means | http://arxiv.org/abs/1405.4054 | id:1405.4054 author:Jianfeng Wang, Jingdong Wang, Jingkuan Song, Xin-Shun Xu, Heng Tao Shen, Shipeng Li category:cs.CV  published:2014-05-16 summary:Product quantization-based approaches are effective to encode high-dimensional data points for approximate nearest neighbor search. The space is decomposed into a Cartesian product of low-dimensional subspaces, each of which generates a sub codebook. Data points are encoded as compact binary codes using these sub codebooks, and the distance between two data points can be approximated efficiently from their codes by the precomputed lookup tables. Traditionally, to encode a subvector of a data point in a subspace, only one sub codeword in the corresponding sub codebook is selected, which may impose strict restrictions on the search accuracy. In this paper, we propose a novel approach, named Optimized Cartesian $K$-Means (OCKM), to better encode the data points for more accurate approximate nearest neighbor search. In OCKM, multiple sub codewords are used to encode the subvector of a data point in a subspace. Each sub codeword stems from different sub codebooks in each subspace, which are optimally generated with regards to the minimization of the distortion errors. The high-dimensional data point is then encoded as the concatenation of the indices of multiple sub codewords from all the subspaces. This can provide more flexibility and lower distortion errors than traditional methods. Experimental results on the standard real-life datasets demonstrate the superiority over state-of-the-art approaches for approximate nearest neighbor search. version:1
arxiv-1405-5494 | Iterative Non-Local Shrinkage Algorithm for MR Image Reconstruction | http://arxiv.org/abs/1405.5494 | id:1405.5494 author:Yasir Q. Moshin, Greg Ongie, Mathews Jacob category:cs.CV  published:2014-05-15 summary:We introduce a fast iterative non-local shrinkage algorithm to recover MRI data from undersampled Fourier measurements. This approach is enabled by the reformulation of current non-local schemes as an alternating algorithm to minimize a global criterion. The proposed algorithm alternates between a non-local shrinkage step and a quadratic subproblem. We derive analytical shrinkage rules for several penalties that are relevant in non-local regularization. The redundancy in the searches used to evaluate the shrinkage steps are exploited using filtering operations. The resulting algorithm is observed to be considerably faster than current alternating non-local algorithms. The comparisons of the proposed scheme with state-of-the-art regularization schemes show a considerable reduction in alias artifacts and preservation of edges. version:1
arxiv-1403-1024 | On learning to localize objects with minimal supervision | http://arxiv.org/abs/1403.1024 | id:1403.1024 author:Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell category:cs.CV cs.LG  published:2014-03-05 summary:Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection. version:4
arxiv-1405-3952 | Fast Ridge Regression with Randomized Principal Component Analysis and Gradient Descent | http://arxiv.org/abs/1405.3952 | id:1405.3952 author:Yichao Lu, Dean P. Foster category:stat.ML  published:2014-05-15 summary:We propose a new two stage algorithm LING for large scale regression problems. LING has the same risk as the well known Ridge Regression under the fixed design setting and can be computed much faster. Our experiments have shown that LING performs well in terms of both prediction accuracy and computational efficiency compared with other large scale regression algorithms like Gradient Descent, Stochastic Gradient Descent and Principal Component Regression on both simulated and real datasets. version:1
arxiv-1405-3925 | Méthodes pour la représentation informatisée de données lexicales / Methoden der Speicherung lexikalischer Daten | http://arxiv.org/abs/1405.3925 | id:1405.3925 author:Laurent Romary, Andreas Witt category:cs.CL  published:2014-05-15 summary:In recent years, new developments in the area of lexicography have altered not only the management, processing and publishing of lexicographical data, but also created new types of products such as electronic dictionaries and thesauri. These expand the range of possible uses of lexical data and support users with more flexibility, for instance in assisting human translation. In this article, we give a short and easy-to-understand introduction to the problematic nature of the storage, display and interpretation of lexical data. We then describe the main methods and specifications used to build and represent lexical data. This paper is targeted for the following groups of people: linguists, lexicographers, IT specialists, computer linguists and all others who wish to learn more about the modelling, representation and visualization of lexical knowledge. This paper is written in two languages: French and German. version:1
arxiv-1310-1502 | Randomized Approximation of the Gram Matrix: Exact Computation and Probabilistic Bounds | http://arxiv.org/abs/1310.1502 | id:1310.1502 author:John T. Holodnak, Ilse C. F. Ipsen category:math.NA cs.LG stat.ML  published:2013-10-05 summary:Given a real matrix A with n columns, the problem is to approximate the Gram product AA^T by c << n weighted outer products of columns of A. Necessary and sufficient conditions for the exact computation of AA^T (in exact arithmetic) from c >= rank(A) columns depend on the right singular vector matrix of A. For a Monte-Carlo matrix multiplication algorithm by Drineas et al. that samples outer products, we present probabilistic bounds for the 2-norm relative error due to randomization. The bounds depend on the stable rank or the rank of A, but not on the matrix dimensions. Numerical experiments illustrate that the bounds are informative, even for stringent success probabilities and matrices of small dimension. We also derive bounds for the smallest singular value and the condition number of matrices obtained by sampling rows from orthonormal matrices. version:3
arxiv-1405-3866 | Speeding up Convolutional Neural Networks with Low Rank Expansions | http://arxiv.org/abs/1405.3866 | id:1405.3866 author:Max Jaderberg, Andrea Vedaldi, Andrew Zisserman category:cs.CV  published:2014-05-15 summary:The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5x speedup with no loss in accuracy, and 4.5x speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks. version:1
arxiv-1405-3843 | Logistic Regression: Tight Bounds for Stochastic and Online Optimization | http://arxiv.org/abs/1405.3843 | id:1405.3843 author:Elad Hazan, Tomer Koren, Kfir Y. Levy category:cs.LG  published:2014-05-15 summary:The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012). version:1
arxiv-1312-5419 | Large-scale Multi-label Text Classification - Revisiting Neural Networks | http://arxiv.org/abs/1312.5419 | id:1312.5419 author:Jinseok Nam, Jungi Kim, Eneldo Loza Mencía, Iryna Gurevych, Johannes Fürnkranz category:cs.LG  published:2013-12-19 summary:Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics. version:3
arxiv-1405-3786 | Complex Networks Measures for Differentiation between Normal and Shuffled Croatian Texts | http://arxiv.org/abs/1405.3786 | id:1405.3786 author:Domagoj Margan, Ana Meštrović, Sanda Martinčić-Ipšić category:cs.CL physics.soc-ph  published:2014-05-15 summary:This paper studies the properties of the Croatian texts via complex networks. We present network properties of normal and shuffled Croatian texts for different shuffling principles: on the sentence level and on the text level. In both experiments we preserved the vocabulary size, word and sentence frequency distributions. Additionally, in the first shuffling approach we preserved the sentence structure of the text and the number of words per sentence. Obtained results showed that degree rank distributions exhibit no substantial deviation in shuffled networks, and strength rank distributions are preserved due to the same word frequencies. Therefore, standard approach to study the structure of linguistic co-occurrence networks showed no clear difference among the topologies of normal and shuffled texts. Finally, we showed that the in- and out- selectivity values from shuffled texts are constantly below selectivity values calculated from normal texts. Our results corroborate that the node selectivity measure can capture structural differences between original and shuffled Croatian texts. version:1
arxiv-1311-0811 | Oracle Inequalities for High Dimensional Vector Autoregressions | http://arxiv.org/abs/1311.0811 | id:1311.0811 author:Anders Bredahl Kock, Laurent A. F. Callot category:math.ST stat.ML stat.TH  published:2013-11-04 summary:This paper establishes non-asymptotic oracle inequalities for the prediction error and estimation accuracy of the LASSO in stationary vector autoregressive models. These inequalities are used to establish consistency of the LASSO even when the number of parameters is of a much larger order of magnitude than the sample size. We also give conditions under which no relevant variables are excluded. Next, non-asymptotic probabilities are given for the Adaptive LASSO to select the correct sparsity pattern. We then give conditions under which the Adaptive LASSO reveals the correct sparsity pattern asymptotically. We establish that the estimates of the non-zero coefficients are asymptotically equivalent to the oracle assisted least squares estimator. This is used to show that the rate of convergence of the estimates of the non-zero coefficients is identical to the one of least squares only including the relevant covariates. version:2
arxiv-1405-3772 | INAUT, a Controlled Language for the French Coast Pilot Books Instructions nautiques | http://arxiv.org/abs/1405.3772 | id:1405.3772 author:Yannis Haralambous, Julie Sauvage-Vincent, John Puentes category:cs.CL 68T30  68T50  97G40 I.3.5  published:2014-05-15 summary:We describe INAUT, a controlled natural language dedicated to collaborative update of a knowledge base on maritime navigation and to automatic generation of coast pilot books (Instructions nautiques) of the French National Hydrographic and Oceanographic Service SHOM. INAUT is based on French language and abundantly uses georeferenced entities. After describing the structure of the overall system, giving details on the language and on its generation, and discussing the three major applications of INAUT (document production, interaction with ENCs and collaborative updates of the knowledge base), we conclude with future extensions and open problems. version:1
arxiv-1405-3738 | Effective Bayesian Modeling of Groups of Related Count Time Series | http://arxiv.org/abs/1405.3738 | id:1405.3738 author:Nicolas Chapados category:stat.ML stat.AP  published:2014-05-15 summary:Time series of counts arise in a variety of forecasting applications, for which traditional models are generally inappropriate. This paper introduces a hierarchical Bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series. We derive an efficient approximate inference technique, and illustrate its performance on a number of datasets from supply chain planning. version:1
arxiv-1405-3726 | Topic words analysis based on LDA model | http://arxiv.org/abs/1405.3726 | id:1405.3726 author:Xi Qiu, Christopher Stewart category:cs.SI cs.DC cs.IR cs.LG stat.ML  published:2014-05-15 summary:Social network analysis (SNA), which is a research field describing and modeling the social connection of a certain group of people, is popular among network services. Our topic words analysis project is a SNA method to visualize the topic words among emails from Obama.com to accounts registered in Columbus, Ohio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model of SNA, our project characterizes the preference of senders for target group of receptors. Gibbs sampling is used to estimate topic and word distribution. Our training and testing data are emails from the carbon-free server Datagreening.com. We use parallel computing tool BashReduce for word processing and generate related words under each latent topic to discovers typical information of political news sending specially to local Columbus receptors. Running on two instances using paralleling tool BashReduce, our project contributes almost 30% speedup processing the raw contents, comparing with processing contents on one instance locally. Also, the experimental result shows that the LDA model applied in our project provides precision rate 53.96% higher than TF-IDF model finding target words, on the condition that appropriate size of topic words list is selected. version:1
arxiv-1212-0935 | Computing Consensus Curves | http://arxiv.org/abs/1212.0935 | id:1212.0935 author:Livio De La Cruz, Stephen Kobourov, Sergey Pupyrev, Paul Shen, Sankar Veeramoni category:cs.CG cs.CV cs.GT cs.MA  published:2012-12-05 summary:We consider the problem of extracting accurate average ant trajectories from many (possibly inaccurate) input trajectories contributed by citizen scientists. Although there are many generic software tools for motion tracking and specific ones for insect tracking, even untrained humans are much better at this task, provided a robust method to computing the average trajectories. We implemented and tested several local (one ant at a time) and global (all ants together) method. Our best performing algorithm uses a novel global method, based on finding edge-disjoint paths in an ant-interaction graph constructed from the input trajectories. The underlying optimization problem is a new and interesting variant of network flow. Even though the problem is NP-hard, we implemented two heuristics, which work very well in practice, outperforming all other approaches, including the best automated system. version:5
arxiv-1312-0925 | Understanding Alternating Minimization for Matrix Completion | http://arxiv.org/abs/1312.0925 | id:1312.0925 author:Moritz Hardt category:cs.LG cs.DS stat.ML  published:2013-12-03 summary:Alternating Minimization is a widely used and empirically successful heuristic for matrix completion and related low-rank optimization problems. Theoretical guarantees for Alternating Minimization have been hard to come by and are still poorly understood. This is in part because the heuristic is iterative and non-convex in nature. We give a new algorithm based on Alternating Minimization that provably recovers an unknown low-rank matrix from a random subsample of its entries under a standard incoherence assumption. Our results reduce the sample size requirements of the Alternating Minimization approach by at least a quartic factor in the rank and the condition number of the unknown matrix. These improvements apply even if the matrix is only close to low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in the dimension of the matrix and, in a broad range of parameters, gives the strongest sample bounds among all subquadratic time algorithms that we are aware of. Underlying our work is a new robust convergence analysis of the well-known Power Method for computing the dominant singular vectors of a matrix. This viewpoint leads to a conceptually simple understanding of Alternating Minimization. In addition, we contribute a new technique for controlling the coherence of intermediate solutions arising in iterative algorithms based on a smoothed analysis of the QR factorization. These techniques may be of interest beyond their application here. version:3
arxiv-1312-7366 | Monte Carlo non local means: Random sampling for large-scale image filtering | http://arxiv.org/abs/1312.7366 | id:1312.7366 author:Stanley H. Chan, Todd Zickler, Yue M. Lu category:cs.CV stat.CO  published:2013-12-27 summary:We propose a randomized version of the non-local means (NLM) algorithm for large-scale image filtering. The new algorithm, called Monte Carlo non-local means (MCNLM), speeds up the classical NLM by computing a small subset of image patch distances, which are randomly selected according to a designed sampling pattern. We make two contributions. First, we analyze the performance of the MCNLM algorithm and show that, for large images or large external image databases, the random outcomes of MCNLM are tightly concentrated around the deterministic full NLM result. In particular, our error probability bounds show that, at any given sampling ratio, the probability for MCNLM to have a large deviation from the original NLM solution decays exponentially as the size of the image or database grows. Second, we derive explicit formulas for optimal sampling patterns that minimize the error probability bound by exploiting partial knowledge of the pairwise similarity weights. Numerical experiments show that MCNLM is competitive with other state-of-the-art fast NLM algorithms for single-image denoising. When applied to denoising images using an external database containing ten billion patches, MCNLM returns a randomized solution that is within 0.2 dB of the full NLM solution while reducing the runtime by three orders of magnitude. version:3
arxiv-1405-3559 | Credal Model Averaging for classification: representing prior ignorance and expert opinions | http://arxiv.org/abs/1405.3559 | id:1405.3559 author:Giorgio Corani, Andrea Mignatti category:stat.ME cs.AI q-bio.PE stat.ML  published:2014-05-14 summary:Bayesian model averaging (BMA) is the state of the art approach for overcoming model uncertainty. Yet, especially on small data sets, the results yielded by BMA might be sensitive to the prior over the models. Credal Model Averaging (CMA) addresses this problem by substituting the single prior over the models by a set of priors (credal set). Such approach solves the problem of how to choose the prior over the models and automates sensitivity analysis. We discuss various CMA algorithms for building an ensemble of logistic regressors characterized by different sets of covariates. We show how CMA can be appropriately tuned to the case in which one is prior-ignorant and to the case in which instead domain knowledge is available. CMA detects prior-dependent instances, namely instances in which a different class is more probable depending on the prior over the models. On such instances CMA suspends the judgment, returning multiple classes. We thoroughly compare different BMA and CMA variants on a real case study, predicting presence of Alpine marmot burrows in an Alpine valley. We find that BMA is almost a random guesser on the instances recognized as prior-dependent by CMA. version:1
arxiv-1405-3536 | Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques | http://arxiv.org/abs/1405.3536 | id:1405.3536 author:Olivier Nicol, Jérémie Mary, Philippe Preux category:stat.ML cs.LG  published:2014-05-14 summary:In many recommendation applications such as news recommendation, the items that can be rec- ommended come and go at a very fast pace. This is a challenge for recommender systems (RS) to face this setting. Online learning algorithms seem to be the most straight forward solution. The contextual bandit framework was introduced for that very purpose. In general the evaluation of a RS is a critical issue. Live evaluation is of- ten avoided due to the potential loss of revenue, hence the need for offline evaluation methods. Two options are available. Model based meth- ods are biased by nature and are thus difficult to trust when used alone. Data driven methods are therefore what we consider here. Evaluat- ing online learning algorithms with past data is not simple but some methods exist in the litera- ture. Nonetheless their accuracy is not satisfac- tory mainly due to their mechanism of data re- jection that only allow the exploitation of a small fraction of the data. We precisely address this issue in this paper. After highlighting the limita- tions of the previous methods, we present a new method, based on bootstrapping techniques. This new method comes with two important improve- ments: it is much more accurate and it provides a measure of quality of its estimation. The latter is a highly desirable property in order to minimize the risks entailed by putting online a RS for the first time. We provide both theoretical and ex- perimental proofs of its superiority compared to state-of-the-art methods, as well as an analysis of the convergence of the measure of quality. version:1
arxiv-1405-3515 | Temporal Analysis of Language through Neural Language Models | http://arxiv.org/abs/1405.3515 | id:1405.3515 author:Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, Slav Petrov category:cs.CL  published:2014-05-14 summary:We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as "cell" and "gay" as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change. version:1
arxiv-1403-4024 | Measuring Global Similarity between Texts | http://arxiv.org/abs/1403.4024 | id:1403.4024 author:Uli Fahrenberg, Fabrizio Biondi, Kevin Corre, Cyrille Jegourel, Simon Kongshøj, Axel Legay category:cs.CL  published:2014-03-17 summary:We propose a new similarity measure between texts which, contrary to the current state-of-the-art approaches, takes a global view of the texts to be compared. We have implemented a tool to compute our textual distance and conducted experiments on several corpuses of texts. The experiments show that our methods can reliably identify different global types of texts. version:3
arxiv-1405-3410 | Efficient classification using parallel and scalable compressed model and Its application on intrusion detection | http://arxiv.org/abs/1405.3410 | id:1405.3410 author:Tieming Chen, Xu Zhang, Shichao Jin, Okhee Kim category:cs.LG cs.CR  published:2014-05-14 summary:In order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression. OneR is utilized as horizontal com-pression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data. As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used. Experimental application study on two publicly available datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average. version:1
arxiv-1405-3396 | Reducing Dueling Bandits to Cardinal Bandits | http://arxiv.org/abs/1405.3396 | id:1405.3396 author:Nir Ailon, Thorsten Joachims, Zohar Karnin category:cs.LG  published:2014-05-14 summary:We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form "A is preferred to B" (as opposed to cardinal feedback like "A has value 2.5"), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions -- named $\Doubler$, $\MultiSbm$ and $\DoubleSbm$ -- provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For $\Doubler$ and $\MultiSbm$ we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of $\DoubleSbm$ which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms. version:1
arxiv-1405-3382 | Active Mining of Parallel Video Streams | http://arxiv.org/abs/1405.3382 | id:1405.3382 author:Samaneh Khoshrou, Jaime S. Cardoso, Luis F. Teixeira category:cs.CV cs.LG  published:2014-05-14 summary:The practicality of a video surveillance system is adversely limited by the amount of queries that can be placed on human resources and their vigilance in response. To transcend this limitation, a major effort under way is to include software that (fully or at least semi) automatically mines video footage, reducing the burden imposed to the system. Herein, we propose a semi-supervised incremental learning framework for evolving visual streams in order to develop a robust and flexible track classification system. Our proposed method learns from consecutive batches by updating an ensemble in each time. It tries to strike a balance between performance of the system and amount of data which needs to be labelled. As no restriction is considered, the system can address many practical problems in an evolving multi-camera scenario, such as concept drift, class evolution and various length of video streams which have not been addressed before. Experiments were performed on synthetic as well as real-world visual data in non-stationary environments, showing high accuracy with fairly little human collaboration. version:1
arxiv-1405-3379 | Learning rates for the risk of kernel based quantile regression estimators in additive models | http://arxiv.org/abs/1405.3379 | id:1405.3379 author:Andreas Christmann, Ding-Xuan Zhou category:stat.ML  published:2014-05-14 summary:Additive models play an important role in semiparametric statistics. This paper gives learning rates for regularized kernel based methods for additive models. These learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the Gaussian radial basis function kernel, provided the assumption of an additive model is valid. Additionally, a concrete example is presented to show that a Gaussian function depending only on one variable lies in a reproducing kernel Hilbert space generated by an additive Gaussian kernel, but does not belong to the reproducing kernel Hilbert space generated by the multivariate Gaussian kernel of the same variance. version:1
arxiv-1405-3034 | G-AMA: Sparse Gaussian graphical model estimation via alternating minimization | http://arxiv.org/abs/1405.3034 | id:1405.3034 author:Onkar Dalal, Bala Rajaratnam category:stat.CO stat.ML  published:2014-05-13 summary:Several methods have been recently proposed for estimating sparse Gaussian graphical models using $\ell_{1}$ regularization on the inverse covariance matrix. Despite recent advances, contemporary applications require methods that are even faster in order to handle ill-conditioned high dimensional modern day datasets. In this paper, we propose a new method, G-AMA, to solve the sparse inverse covariance estimation problem using Alternating Minimization Algorithm (AMA), that effectively works as a proximal gradient algorithm on the dual problem. Our approach has several novel advantages over existing methods. First, we demonstrate that G-AMA is faster than the previous best algorithms by many orders of magnitude and is thus an ideal approach for modern high throughput applications. Second, global linear convergence of G-AMA is demonstrated rigorously, underscoring its good theoretical properties. Third, the dual algorithm operates on the covariance matrix, and thus easily facilitates incorporating additional constraints on pairwise/marginal relationships between feature pairs based on domain specific knowledge. Over and above estimating a sparse inverse covariance matrix, we also illustrate how to (1) incorporate constraints on the (bivariate) correlations and, (2) incorporate equality (equisparsity) or linear constraints between individual inverse covariance elements. Fourth, we also show that G-AMA is better adept at handling extremely ill-conditioned problems, as is often the case with real data. The methodology is demonstrated on both simulated and real datasets to illustrate its superior performance over recently proposed methods. version:2
arxiv-1404-6055 | A General Homogeneous Matrix Formulation to 3D Rotation Geometric Transformations | http://arxiv.org/abs/1404.6055 | id:1404.6055 author:Feng Lu, Ziqiang Chen category:cs.CV  published:2014-04-24 summary:We present algebraic projective geometry definitions of 3D rotations so as to bridge a small gap between the applications and the definitions of 3D rotations in homogeneous matrix form. A general homogeneous matrix formulation to 3D rotation geometric transformations is proposed which suits for the cases when the rotation axis is unnecessarily through the coordinate system origin given their rotation axes and rotation angles. version:2
arxiv-1405-3351 | Group-based Sparse Representation for Image Restoration | http://arxiv.org/abs/1405.3351 | id:1405.3351 author:Jian Zhang, Debin Zhao, Wen Gao category:cs.CV  published:2014-05-14 summary:Traditional patch-based sparse representation modeling of natural images usually suffer from two problems. First, it has to solve a large-scale optimization problem with high computational complexity in dictionary learning. Second, each patch is considered independently in dictionary learning and sparse coding, which ignores the relationship among patches, resulting in inaccurate sparse coding coefficients. In this paper, instead of using patch as the basic unit of sparse representation, we exploit the concept of group as the basic unit of sparse representation, which is composed of nonlocal patches with similar structures, and establish a novel sparse representation modeling of natural images, called group-based sparse representation (GSR). The proposed GSR is able to sparsely represent natural images in the domain of group, which enforces the intrinsic local sparsity and nonlocal self-similarity of images simultaneously in a unified framework. Moreover, an effective self-adaptive dictionary learning method for each group with low complexity is designed, rather than dictionary learning from natural images. To make GSR tractable and robust, a split Bregman based technique is developed to solve the proposed GSR-driven minimization problem for image restoration efficiently. Extensive experiments on image inpainting, image deblurring and image compressive sensing recovery manifest that the proposed GSR modeling outperforms many current state-of-the-art schemes in both PSNR and visual perception. version:1
arxiv-1402-1892 | Thresholding Classifiers to Maximize F1 Score | http://arxiv.org/abs/1402.1892 | id:1402.1892 author:Zachary Chase Lipton, Charles Elkan, Balakrishnan Narayanaswamy category:stat.ML cs.IR cs.LG  published:2014-02-08 summary:This paper provides new insight into maximizing F1 scores in the context of binary classification and also in the context of multilabel classification. The harmonic mean of precision and recall, F1 score is widely used to measure the success of a binary classifier when one class is rare. Micro average, macro average, and per instance average F1 scores are used in multilabel classification. For any classifier that produces a real-valued output, we derive the relationship between the best achievable F1 score and the decision-making threshold that achieves this optimum. As a special case, if the classifier outputs are well-calibrated conditional probabilities, then the optimal threshold is half the optimal F1 score. As another special case, if the classifier is completely uninformative, then the optimal behavior is to classify all examples as positive. Since the actual prevalence of positive examples typically is low, this behavior can be considered undesirable. As a case study, we discuss the results, which can be surprising, of applying this procedure when predicting 26,853 labels for Medline documents. version:2
arxiv-1405-3318 | Adaptive Monte Carlo via Bandit Allocation | http://arxiv.org/abs/1405.3318 | id:1405.3318 author:James Neufeld, András György, Dale Schuurmans, Csaba Szepesvári category:cs.AI cs.LG  published:2014-05-13 summary:We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages. version:1
arxiv-1405-3316 | Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with Non-stationary Rewards | http://arxiv.org/abs/1405.3316 | id:1405.3316 author:Omar Besbes, Yonatan Gur, Assaf Zeevi category:cs.LG math.OC stat.ML  published:2014-05-13 summary:In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward "variation" and the minimal achievable regret. Our analysis draws some connections between two rather disparate strands of literature: the adversarial and the stochastic MAB frameworks. version:1
arxiv-1312-7853 | Communication Efficient Distributed Optimization using an Approximate Newton-type Method | http://arxiv.org/abs/1312.7853 | id:1312.7853 author:Ohad Shamir, Nathan Srebro, Tong Zhang category:cs.LG math.OC stat.ML  published:2013-12-30 summary:We present a novel Newton-type method for distributed optimization, which is particularly well suited for stochastic optimization and learning problems. For quadratic objectives, the method enjoys a linear rate of convergence which provably \emph{improves} with the data size, requiring an essentially constant number of iterations under reasonable assumptions. We provide theoretical and empirical evidence of the advantages of our method compared to other approaches, such as one-shot parameter averaging and ADMM. version:4
arxiv-1405-3295 | Effects of Sampling Methods on Prediction Quality. The Case of Classifying Land Cover Using Decision Trees | http://arxiv.org/abs/1405.3295 | id:1405.3295 author:Ronald Hochreiter, Christoph Waldhauser category:stat.ML cs.LG stat.AP  published:2014-05-13 summary:Clever sampling methods can be used to improve the handling of big data and increase its usefulness. The subject of this study is remote sensing, specifically airborne laser scanning point clouds representing different classes of ground cover. The aim is to derive a supervised learning model for the classification using CARTs. In order to measure the effect of different sampling methods on the classification accuracy, various experiments with varying types of sampling methods, sample sizes, and accuracy metrics have been designed. Numerical results for a subset of a large surveying project covering the lower Rhine area in Germany are shown. General conclusions regarding sampling design are drawn and presented. version:1
arxiv-1405-3292 | Learning with many experts: model selection and sparsity | http://arxiv.org/abs/1405.3292 | id:1405.3292 author:Rafael Izbicki, Rafael Bassi Stern category:stat.ME cs.LG  published:2014-05-13 summary:Experts classifying data are often imprecise. Recently, several models have been proposed to train classifiers using the noisy labels generated by these experts. How to choose between these models? In such situations, the true labels are unavailable. Thus, one cannot perform model selection using the standard versions of methods such as empirical risk minimization and cross validation. In order to allow model selection, we present a surrogate loss and provide theoretical guarantees that assure its consistency. Next, we discuss how this loss can be used to tune a penalization which introduces sparsity in the parameters of a traditional class of models. Sparsity provides more parsimonious models and can avoid overfitting. Nevertheless, it has seldom been discussed in the context of noisy labels due to the difficulty in model selection and, therefore, in choosing tuning parameters. We apply these techniques to several sets of simulated and real data. version:1
arxiv-1405-3282 | How to Ask for a Favor: A Case Study on the Success of Altruistic Requests | http://arxiv.org/abs/1405.3282 | id:1405.3282 author:Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky category:cs.CL cs.SI physics.soc-ph I.2.7; J.4  published:2014-05-13 summary:Requests are at the core of many social media systems such as question & answer sites and online philanthropy communities. While the success of such requests is critical to the success of the community, the factors that lead community members to satisfy a request are largely unknown. Success of a request depends on factors like who is asking, how they are asking, when are they asking, and most critically what is being requested, ranging from small favors to substantial monetary donations. We present a case study of altruistic requests in an online community where all requests ask for the very same contribution and do not offer anything tangible in return, allowing us to disentangle what is requested from textual and social factors. Drawing from social psychology literature, we extract high-level social features from text that operationalize social relations between recipient and donor and demonstrate that these extracted relations are predictive of success. More specifically, we find that clearly communicating need through the narrative is essential and that that linguistic indications of gratitude, evidentiality, and generalized reciprocity, as well as high status of the asker further increase the likelihood of success. Building on this understanding, we develop a model that can predict the success of unseen requests, significantly improving over several baselines. We link these findings to research in psychology on helping behavior, providing a basis for further analysis of success in social media systems. version:1
arxiv-1405-3263 | Scalable sparse covariance estimation via self-concordance | http://arxiv.org/abs/1405.3263 | id:1405.3263 author:Anastasios Kyrillidis, Rabeeh Karimi Mahabadi, Quoc Tran-Dinh, Volkan Cevher category:stat.ML cs.IT math.IT math.OC  published:2014-05-13 summary:We consider the class of convex minimization problems, composed of a self-concordant function, such as the $\log\det$ metric, a convex data fidelity term $h(\cdot)$ and, a regularizing -- possibly non-smooth -- function $g(\cdot)$. This type of problems have recently attracted a great deal of interest, mainly due to their omnipresence in top-notch applications. Under this \emph{locally} Lipschitz continuous gradient setting, we analyze the convergence behavior of proximal Newton schemes with the added twist of a probable presence of inexact evaluations. We prove attractive convergence rate guarantees and enhance state-of-the-art optimization schemes to accommodate such developments. Experimental results on sparse covariance estimation show the merits of our algorithm, both in terms of recovery efficiency and complexity. version:1
arxiv-1405-3229 | Rate of Convergence and Error Bounds for LSTD($λ$) | http://arxiv.org/abs/1405.3229 | id:1405.3229 author:Manel Tagorti, Bruno Scherrer category:cs.LG cs.AI math.OC math.ST stat.TH  published:2014-05-13 summary:We consider LSTD($\lambda$), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a $\beta$-mixing assumption, we derive, for any value of $\lambda \in (0,1)$, a high-probability estimate of the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where $\lambda=0$. In particular, our analysis sheds some light on the choice of $\lambda$ with respect to the quality of the chosen linear space and the number of samples, that complies with simulations. version:1
arxiv-1402-0119 | Randomized Nonlinear Component Analysis | http://arxiv.org/abs/1402.0119 | id:1402.0119 author:David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, Bernhard Schölkopf category:stat.ML cs.LG  published:2014-02-01 summary:Classical methods such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques are only able to reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, these are computationally prohibitive in the large scale. In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving drastic savings in computational requirements. In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. A simple R implementation of the presented algorithms is provided. version:2
arxiv-1405-3210 | Locally Boosted Graph Aggregation for Community Detection | http://arxiv.org/abs/1405.3210 | id:1405.3210 author:Jeremy Kun, Rajmonda Caceres, Kevin Carter category:cs.LG cs.SI physics.soc-ph  published:2014-05-13 summary:Learning the right graph representation from noisy, multi-source data has garnered significant interest in recent years. A central tenet of this problem is relational learning. Here the objective is to incorporate the partial information each data source gives us in a way that captures the true underlying relationships. To address this challenge, we present a general, boosting-inspired framework for combining weak evidence of entity associations into a robust similarity metric. Building on previous work, we explore the extent to which different local quality measurements yield graph representations that are suitable for community detection. We present empirical results on a variety of datasets demonstrating the utility of this framework, especially with respect to real datasets where noise and scale present serious challenges. Finally, we prove a convergence theorem in an ideal setting and outline future research into other application domains. version:1
arxiv-1405-3195 | An Intelligent Pixel Replication Technique by Binary Decomposition for Digital Image Zooming | http://arxiv.org/abs/1405.3195 | id:1405.3195 author:Kaeser M Sabrin, M Haider Ali category:cs.CV  published:2014-05-13 summary:Image zooming is the process of enlarging the spatial resolution of a given digital image. We present a novel technique that intelligently modifies the classical pixel replication method for zooming. Our method decomposes a given image into layer of binary images, interpolates them by magnifying the binary patterns preserving their geometric shape and finally aggregates them all to obtain the zoomed image. Although the quality of our zoomed images is much higher than that of nearest neighbor and bilinear interpolation and comparable with bicubic interpolation, the running time of our technique is extremely fast like nearest neighbor interpolation and much faster than bilinear and bicubic interpolation. version:1
arxiv-1405-3167 | Clustering, Hamming Embedding, Generalized LSH and the Max Norm | http://arxiv.org/abs/1405.3167 | id:1405.3167 author:Behnam Neyshabur, Yury Makarychev, Nathan Srebro category:cs.LG  published:2014-05-13 summary:We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by (Charikar 2002) and to the max-norm ball, and the differences between their symmetric and asymmetric versions. version:1
arxiv-1311-3315 | Sparse Matrix Factorization | http://arxiv.org/abs/1311.3315 | id:1311.3315 author:Behnam Neyshabur, Rina Panigrahy category:cs.LG stat.ML  published:2013-11-13 summary:We investigate the problem of factorizing a matrix into several sparse matrices and propose an algorithm for this under randomness and sparsity assumptions. This problem can be viewed as a simplification of the deep learning problem where finding a factorization corresponds to finding edges in different layers and values of hidden units. We prove that under certain assumptions for a sparse linear deep network with $n$ nodes in each layer, our algorithm is able to recover the structure of the network and values of top layer hidden units for depths up to $\tilde O(n^{1/6})$. We further discuss the relation among sparse matrix factorization, deep learning, sparse recovery and dictionary learning. version:3
arxiv-1405-3162 | Circulant Binary Embedding | http://arxiv.org/abs/1405.3162 | id:1405.3162 author:Felix X. Yu, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang category:stat.ML cs.LG  published:2014-05-13 summary:Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from $\mathcal{O}(d^2)$ to $\mathcal{O}(d\log{d})$, and the space complexity from $\mathcal{O}(d^2)$ to $\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits. version:1
arxiv-1405-3080 | Accelerating Minibatch Stochastic Gradient Descent using Stratified Sampling | http://arxiv.org/abs/1405.3080 | id:1405.3080 author:Peilin Zhao, Tong Zhang category:stat.ML cs.LG math.OC  published:2014-05-13 summary:Stochastic Gradient Descent (SGD) is a popular optimization method which has been applied to many important machine learning tasks such as Support Vector Machines and Deep Neural Networks. In order to parallelize SGD, minibatch training is often employed. The standard approach is to uniformly sample a minibatch at each step, which often leads to high variance. In this paper we propose a stratified sampling strategy, which divides the whole dataset into clusters with low within-cluster variance; we then take examples from these clusters using a stratified sampling technique. It is shown that the convergence rate can be significantly improved by the algorithm. Encouraging experimental results confirm the effectiveness of the proposed method. version:1
arxiv-1311-6976 | Dimensionality reduction for click-through rate prediction: Dense versus sparse representation | http://arxiv.org/abs/1311.6976 | id:1311.6976 author:Bjarne Ørum Fruergaard, Toke Jansen Hansen, Lars Kai Hansen category:stat.ML cs.LG stat.AP stat.ME  published:2013-11-27 summary:In online advertising, display ads are increasingly being placed based on real-time auctions where the advertiser who wins gets to serve the ad. This is called real-time bidding (RTB). In RTB, auctions have very tight time constraints on the order of 100ms. Therefore mechanisms for bidding intelligently such as clickthrough rate prediction need to be sufficiently fast. In this work, we propose to use dimensionality reduction of the user-website interaction graph in order to produce simplified features of users and websites that can be used as predictors of clickthrough rate. We demonstrate that the Infinite Relational Model (IRM) as a dimensionality reduction offers comparable predictive performance to conventional dimensionality reduction schemes, while achieving the most economical usage of features and fastest computations at run-time. For applications such as real-time bidding, where fast database I/O and few computations are key to success, we thus recommend using IRM based features as predictors to exploit the recommender effects from bipartite graphs. version:2
arxiv-1405-3033 | Phonetic based SoundEx & ShapeEx algorithm for Sindhi Spell Checker System | http://arxiv.org/abs/1405.3033 | id:1405.3033 author:Zeeshan Bhatti, Ahmad Waqas, Imdad Ali Ismaili, Dil Nawaz Hakro, Waseem Javaid Soomro category:cs.CL  published:2014-05-13 summary:This paper presents a novel combinational phonetic algorithm for Sindhi Language, to be used in developing Sindhi Spell Checker which has yet not been developed prior to this work. The compound textual forms and glyphs of Sindhi language presents a substantial challenge for developing Sindhi spell checker system and generating similar suggestion list for misspelled words. In order to implement such a system, phonetic based Sindhi language rules and patterns must be considered into account for increasing the accuracy and efficiency. The proposed system is developed with a blend between Phonetic based SoundEx algorithm and ShapeEx algorithm for pattern or glyph matching, generating accurate and efficient suggestion list for incorrect or misspelled Sindhi words. A table of phonetically similar sounding Sindhi characters for SoundEx algorithm is also generated along with another table containing similar glyph or shape based character groups for ShapeEx algorithm. Both these are first ever attempt of any such type of categorization and representation for Sindhi Language. version:1
arxiv-1405-2951 | A Neuron as a Signal Processing Device | http://arxiv.org/abs/1405.2951 | id:1405.2951 author:Tao Hu, Zaid J. Towfic, Cengiz Pehlevan, Alex Genkin, Dmitri B. Chklovskii category:q-bio.NC stat.ML  published:2014-05-12 summary:A neuron is a basic physiological and computational unit of the brain. While much is known about the physiological properties of a neuron, its computational role is poorly understood. Here we propose to view a neuron as a signal processing device that represents the incoming streaming data matrix as a sparse vector of synaptic weights scaled by an outgoing sparse activity vector. Formally, a neuron minimizes a cost function comprising a cumulative squared representation error and regularization terms. We derive an online algorithm that minimizes such cost function by alternating between the minimization with respect to activity and with respect to synaptic weights. The steps of this algorithm reproduce well-known physiological properties of a neuron, such as weighted summation and leaky integration of synaptic inputs, as well as an Oja-like, but parameter-free, synaptic learning rule. Our theoretical framework makes several predictions, some of which can be verified by the existing data, others require further experiments. Such framework should allow modeling the function of neuronal circuits without necessarily measuring all the microscopic biophysical parameters, as well as facilitate the design of neuromorphic electronics. version:1
arxiv-1405-2941 | Cross-view Action Modeling, Learning and Recognition | http://arxiv.org/abs/1405.2941 | id:1405.2941 author:Jiang wang, Xiaohan Nie, Yin Xia, Ying Wu, Song-Chun Zhu category:cs.CV  published:2014-05-12 summary:Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos. version:1
arxiv-1405-2936 | Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm | http://arxiv.org/abs/1405.2936 | id:1405.2936 author:Hadi Daneshmand, Manuel Gomez-Rodriguez, Le Song, Bernhard Schoelkopf category:cs.SI physics.soc-ph stat.ML  published:2014-05-12 summary:Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees? Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an $l_1$-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe $O(d^3 \log N)$ cascades, where $d$ is the maximum number of parents of a node and $N$ is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice. version:1
arxiv-1307-7577 | Safe Screening With Variational Inequalities and Its Application to LASSO | http://arxiv.org/abs/1307.7577 | id:1307.7577 author:Jun Liu, Zheng Zhao, Jie Wang, Jieping Ye category:cs.LG stat.ML  published:2013-07-29 summary:Sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non-zero entries. Safe screening, which eliminates the features that are guaranteed to have zero coefficients for a certain value of the regularization parameter, is a technique for improving the computational efficiency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2) one needs to try several regularization parameters to select a suitable model. In this paper, we propose an approach called "Sasvi" (Safe screening with variational inequalities). Sasvi makes use of the variational inequality that provides the sufficient and necessary optimality condition for the dual problem. Several existing approaches for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe screening rule. We further study the monotone properties of Sasvi for Lasso, based on which a sure removal regularization parameter can be identified for each feature. Experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed Sasvi for Lasso screening. version:3
arxiv-1106-6258 | A Note on Improved Loss Bounds for Multiple Kernel Learning | http://arxiv.org/abs/1106.6258 | id:1106.6258 author:Zakria Hussain, John Shawe-Taylor, Mario Marchand category:cs.LG  published:2011-06-30 summary:In this paper, we correct an upper bound, presented in~\cite{hs-11}, on the generalisation error of classifiers learned through multiple kernel learning. The bound in~\cite{hs-11} uses Rademacher complexity and has an\emph{additive} dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a \emph{multiplicative} dependence on the logarithm of the number of kernels and the margin achieved by the classifier. version:2
arxiv-1405-2878 | Approximate Policy Iteration Schemes: A Comparison | http://arxiv.org/abs/1405.2878 | id:1405.2878 author:Bruno Scherrer category:cs.AI cs.LG stat.ML  published:2014-05-12 summary:We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$), and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all algorithms, we describe performance bounds, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API/API($\alpha$), but this comes at the cost of a relative---exponential in $\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP$_\infty$ is proportional to their number of iterations, which may be problematic when the discount factor $\gamma$ is close to 1 or the approximation error $\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis. version:1
arxiv-1402-5666 | Dynamic Rate and Channel Selection in Cognitive Radio Systems | http://arxiv.org/abs/1402.5666 | id:1402.5666 author:Richard Combes, Alexandre Proutiere category:cs.IT cs.LG math.IT  published:2014-02-23 summary:In this paper, we investigate dynamic channel and rate selection in cognitive radio systems which exploit a large number of channels free from primary users. In such systems, transmitters may rapidly change the selected (channel, rate) pair to opportunistically learn and track the pair offering the highest throughput. We formulate the problem of sequential channel and rate selection as an online optimization problem, and show its equivalence to a {\it structured} Multi-Armed Bandit problem. The structure stems from inherent properties of the achieved throughput as a function of the selected channel and rate. We derive fundamental performance limits satisfied by {\it any} channel and rate adaptation algorithm, and propose algorithms that achieve (or approach) these limits. In turn, the proposed algorithms optimally exploit the inherent structure of the throughput. We illustrate the efficiency of our algorithms using both test-bed and simulation experiments, in both stationary and non-stationary radio environments. In stationary environments, the packet successful transmission probabilities at the various channel and rate pairs do not evolve over time, whereas in non-stationary environments, they may evolve. In practical scenarios, the proposed algorithms are able to track the best channel and rate quite accurately without the need of any explicit measurement and feedback of the quality of the various channels. version:2
arxiv-1405-2908 | Resource-Aware Programming for Robotic Vision | http://arxiv.org/abs/1405.2908 | id:1405.2908 author:Johny Paul, Walter Stechele, Manfred Kröhnert, Tamim Asfour category:cs.CV cs.DC cs.RO  published:2014-05-12 summary:Humanoid robots are designed to operate in human centered environments. They face changing, dynamic environments in which they need to fulfill a multitude of challenging tasks. Such tasks differ in complexity, resource requirements, and execution time. Latest computer architectures of humanoid robots consist of several industrial PCs containing single- or dual-core processors. According to the SIA roadmap for semiconductors, many-core chips with hundreds to thousands of cores are expected to be available in the next decade. Utilizing the full power of a chip with huge amounts of resources requires new computing paradigms and methodologies. In this paper, we analyze a resource-aware computing methodology named Invasive Computing, to address these challenges. The benefits and limitations of the new programming model is analyzed using two widely used computer vision algorithms, the Harris Corner detector and SIFT (Scale Invariant Feature Transform) feature matching. The result indicate that the new programming model together with the extensions within the application layer, makes them highly adaptable; leading to better quality in the results obtained. version:1
arxiv-1405-2798 | Two-Stage Metric Learning | http://arxiv.org/abs/1405.2798 | id:1405.2798 author:Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros Kalousis category:cs.LG cs.AI stat.ML  published:2014-05-12 summary:In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM. version:1
arxiv-1210-4567 | Gender identity and lexical variation in social media | http://arxiv.org/abs/1210.4567 | id:1210.4567 author:David Bamman, Jacob Eisenstein, Tyler Schnoebelen category:cs.CL  published:2012-10-16 summary:We present a study of the relationship between gender, linguistic style, and social networks, using a novel corpus of 14,000 Twitter users. Prior quantitative work on gender often treats this social variable as a female/male binary; we argue for a more nuanced approach. By clustering Twitter users, we find a natural decomposition of the dataset into various styles and topical interests. Many clusters have strong gender orientations, but their use of linguistic resources sometimes directly conflicts with the population-level language statistics. We view these clusters as a more accurate reflection of the multifaceted nature of gendered language styles. Previous corpus-based work has also had little to say about individuals whose linguistic styles defy population-level gender patterns. To identify such individuals, we train a statistical classifier, and measure the classifier confidence for each individual in the dataset. Examining individuals whose language does not match the classifier's model for their gender, we find that they have social networks that include significantly fewer same-gender social connections and that, in general, social network homophily is correlated with the use of same-gender language markers. Pairing computational methods and social theory thus offers a new perspective on how gender emerges as individuals position themselves relative to audiences, topics, and mainstream gender norms. version:2
arxiv-1402-2966 | Nonparametric Estimation of Renyi Divergence and Friends | http://arxiv.org/abs/1402.2966 | id:1402.2966 author:Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman category:stat.ML math.ST stat.TH  published:2014-02-12 summary:We consider nonparametric estimation of $L_2$, Renyi-$\alpha$ and Tsallis-$\alpha$ divergences between continuous distributions. Our approach is to construct estimators for particular integral functionals of two densities and translate them into divergence estimators. For the integral functionals, our estimators are based on corrections of a preliminary plug-in estimator. We show that these estimators achieve the parametric convergence rate of $n^{-1/2}$ when the densities' smoothness, $s$, are both at least $d/4$ where $d$ is the dimension. We also derive minimax lower bounds for this problem which confirm that $s > d/4$ is necessary to achieve the $n^{-1/2}$ rate of convergence. We validate our theoretical guarantees with a number of simulations. version:2
arxiv-1312-0976 | Multilinguals and Wikipedia Editing | http://arxiv.org/abs/1312.0976 | id:1312.0976 author:Scott A. Hale category:cs.CY cs.CL cs.DL cs.SI physics.soc-ph H.5.4; H.5.3  published:2013-12-03 summary:This article analyzes one month of edits to Wikipedia in order to examine the role of users editing multiple language editions (referred to as multilingual users). Such multilingual users may serve an important function in diffusing information across different language editions of the encyclopedia, and prior work has suggested this could reduce the level of self-focus bias in each edition. This study finds multilingual users are much more active than their single-edition (monolingual) counterparts. They are found in all language editions, but smaller-sized editions with fewer users have a higher percentage of multilingual users than larger-sized editions. About a quarter of multilingual users always edit the same articles in multiple languages, while just over 40% of multilingual users edit different articles in different languages. When non-English users do edit a second language edition, that edition is most frequently English. Nonetheless, several regional and linguistic cross-editing patterns are also present. version:2
arxiv-1405-2690 | Policy Gradients for CVaR-Constrained MDPs | http://arxiv.org/abs/1405.2690 | id:1405.2690 author:Prashanth L. A category:stat.ML cs.LG math.OC  published:2014-05-12 summary:We study a risk-constrained version of the stochastic shortest path (SSP) problem, where the risk measure considered is Conditional Value-at-Risk (CVaR). We propose two algorithms that obtain a locally risk-optimal policy by employing four tools: stochastic approximation, mini batches, policy gradients and importance sampling. Both the algorithms incorporate a CVaR estimation procedure, along the lines of Bardou et al. [2009], which in turn is based on Rockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio principle for estimating the gradient of the sum of one cost function (objective of the SSP) and the gradient of the CVaR of the sum of another cost function (in the constraint of SSP). The algorithms differ in the manner in which they approximate the CVaR estimates/necessary gradients - the first algorithm uses stochastic approximation, while the second employ mini-batches in the spirit of Monte Carlo methods. We establish asymptotic convergence of both the algorithms. Further, since estimating CVaR is related to rare-event simulation, we incorporate an importance sampling based variance reduction scheme into our proposed algorithms. version:1
arxiv-1403-6382 | CNN Features off-the-shelf: an Astounding Baseline for Recognition | http://arxiv.org/abs/1403.6382 | id:1403.6382 author:Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson category:cs.CV  published:2014-03-23 summary:Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks. version:3
arxiv-1402-4102 | Stochastic Gradient Hamiltonian Monte Carlo | http://arxiv.org/abs/1402.4102 | id:1402.4102 author:Tianqi Chen, Emily B. Fox, Carlos Guestrin category:stat.ME cs.LG stat.ML  published:2014-02-17 summary:Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization. version:2
arxiv-1405-2606 | Structural Return Maximization for Reinforcement Learning | http://arxiv.org/abs/1405.2606 | id:1405.2606 author:Joshua Joseph, Javier Velez, Nicholas Roy category:stat.ML cs.LG  published:2014-05-12 summary:Batch Reinforcement Learning (RL) algorithms attempt to choose a policy from a designer-provided class of policies given a fixed set of training data. Choosing the policy which maximizes an estimate of return often leads to over-fitting when only limited data is available, due to the size of the policy class in relation to the amount of data available. In this work, we focus on learning policy classes that are appropriately sized to the amount of data available. We accomplish this by using the principle of Structural Risk Minimization, from Statistical Learning Theory, which uses Rademacher complexity to identify a policy class that maximizes a bound on the return of the best policy in the chosen policy class, given the available data. Unlike similar batch RL approaches, our bound on return requires only extremely weak assumptions on the true system. version:1
arxiv-1405-2600 | Learning from networked examples | http://arxiv.org/abs/1405.2600 | id:1405.2600 author:Yuyi Wang, Jan Ramon, Zheng-Chu Guo category:cs.AI cs.LG stat.ML  published:2014-05-11 summary:Many machine learning algorithms are based on the assumption that training examples are drawn identically and independently. However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects, and hence share the features of these shared objects. We first show that the classic approach of ignoring this problem potentially can have a disastrous effect on the accuracy of statistics, and then consider alternatives. One of these is to only use independent examples, discarding other information. However, this is clearly suboptimal. We analyze sample error bounds in a networked setting, providing both improved and new results. Next, we propose an efficient weighting method which achieves a better sample error bound than those of previous methods. Our approach is based on novel concentration inequalities for networked variables. version:1
arxiv-1405-2584 | Sentiment Analysis: A Survey | http://arxiv.org/abs/1405.2584 | id:1405.2584 author:Rahul Tejwani category:cs.IR cs.CL  published:2014-05-11 summary:Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. Mining opinions expressed in the user generated content is a challenging yet practically very useful problem. This survey would cover various approaches and methodology used in Sentiment Analysis and Opinion Mining in general. The focus would be on Internet text like, Product review, tweets and other social media. version:1
arxiv-1405-2566 | Learning modular structures from network data and node variables | http://arxiv.org/abs/1405.2566 | id:1405.2566 author:Elham Azizi, James E. Galagan, Edoardo M. Airoldi category:stat.ML cs.SI physics.soc-ph q-bio.QM stat.AP  published:2014-05-11 summary:A standard technique for understanding underlying dependency structures among a set of variables posits a shared conditional probability distribution for the variables measured on individuals within a group. This approach is often referred to as module networks, where individuals are represented by nodes in a network, groups are termed modules, and the focus is on estimating the network structure among modules. However, estimation solely from node-specific variables can lead to spurious dependencies, and unverifiable structural assumptions are often used for regularization. Here, we propose an extended model that leverages direct observations about the network in addition to node-specific variables. By integrating complementary data types, we avoid the need for structural assumptions. We illustrate theoretical and practical significance of the model and develop a reversible-jump MCMC learning procedure for learning modules and model parameters. We demonstrate the method accuracy in predicting modular structures from synthetic data and capability to learn influence structures in twitter data and regulatory modules in the Mycobacterium tuberculosis gene regulatory network. version:1
arxiv-1405-2539 | A Review of Image Mosaicing Techniques | http://arxiv.org/abs/1405.2539 | id:1405.2539 author:Dushyant Vaghela, Prof. Kapildev Naina category:cs.CV  published:2014-05-11 summary:Image Mosaicing is a method of constructing multiple images of the same scene into a larger image. The output of the image mosaic will be the union of two input images. Image-mosaicing algorithms are used to get mosaiced image. Image Mosaicing processed is basically divided in to 5 phases. Which includes; Feature point extraction, Image registration, Homography computation, Warping and Blending if Image. Various corner detection algorithm is being used for Feature extraction. This corner produces an efficient and informative output mosaiced image. Image mosaicing is widely used in creating 3D images, medical imaging, computer vision, data from satellites, and military automatic target recognition. version:1
arxiv-1210-4460 | Fast SVM-based Feature Elimination Utilizing Data Radius, Hard-Margin, Soft-Margin | http://arxiv.org/abs/1210.4460 | id:1210.4460 author:Yaman Aksu category:stat.ML cs.LG  published:2012-10-16 summary:Margin maximization in the hard-margin sense, proposed as feature elimination criterion by the MFE-LO method, is combined here with data radius utilization to further aim to lower generalization error, as several published bounds and bound-related formulations pertaining to lowering misclassification risk (or error) pertain to radius e.g. product of squared radius and weight vector squared norm. Additionally, we propose additional novel feature elimination criteria that, while instead being in the soft-margin sense, too can utilize data radius, utilizing previously published bound-related formulations for approaching radius for the soft-margin sense, whereby e.g. a focus was on the principle stated therein as "finding a bound whose minima are in a region with small leave-one-out values may be more important than its tightness". These additional criteria we propose combine radius utilization with a novel and computationally low-cost soft-margin light classifier retraining approach we devise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. We correct an error in the MFE-LO description, find MFE-LO achieves the highest generalization accuracy among the previously published margin-based feature elimination (MFE) methods, discuss some limitations of MFE-LO, and find our novel methods herein outperform MFE-LO, attain lower test set classification error rate. On several datasets that each both have a large number of features and fall into the `large features few samples' dataset category, and on datasets with lower (low-to-intermediate) number of features, our novel methods give promising results. Especially, among our methods the tunable ones, that do not employ (the non-tunable) LO approach, can be tuned more aggressively in the future than herein, to aim to demonstrate for them even higher performance than herein. version:4
arxiv-1405-3173 | Image Restoration Using Joint Statistical Modeling in Space-Transform Domain | http://arxiv.org/abs/1405.3173 | id:1405.3173 author:Jian Zhang, Debin Zhao, Ruiqin Xiong, Siwei Ma, Wen Gao category:cs.MM cs.CV  published:2014-05-11 summary:This paper presents a novel strategy for high-fidelity image restoration by characterizing both local smoothness and nonlocal self-similarity of natural images in a unified statistical manner. The main contributions are three-folds. First, from the perspective of image statistics, a joint statistical modeling (JSM) in an adaptive hybrid space-transform domain is established, which offers a powerful mechanism of combining local smoothness and nonlocal self-similarity simultaneously to ensure a more reliable and robust estimation. Second, a new form of minimization functional for solving image inverse problem is formulated using JSM under regularization-based framework. Finally, in order to make JSM tractable and robust, a new Split-Bregman based algorithm is developed to efficiently solve the above severely underdetermined inverse problem associated with theoretical proof of convergence. Extensive experiments on image inpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise removal applications verify the effectiveness of the proposed algorithm. version:1
arxiv-1311-6107 | Off-policy reinforcement learning for $ H_\infty $ control design | http://arxiv.org/abs/1311.6107 | id:1311.6107 author:Biao Luo, Huai-Ning Wu, Tingwen Huang category:cs.SY cs.LG math.OC stat.ML  published:2013-11-24 summary:The $H_\infty$ control design problem is considered for nonlinear systems with unknown internal system model. It is known that the nonlinear $ H_\infty $ control problem can be transformed into solving the so-called Hamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial differential equation that is generally impossible to be solved analytically. Even worse, model-based approaches cannot be used for approximately solving HJI equation, when the accurate system model is unavailable or costly to obtain in practice. To overcome these difficulties, an off-policy reinforcement leaning (RL) method is introduced to learn the solution of HJI equation from real system data instead of mathematical system model, and its convergence is proved. In the off-policy RL method, the system data can be generated with arbitrary policies rather than the evaluating policy, which is extremely important and promising for practical systems. For implementation purpose, a neural network (NN) based actor-critic structure is employed and a least-square NN weight update algorithm is derived based on the method of weighted residuals. Finally, the developed NN-based off-policy RL method is tested on a linear F16 aircraft plant, and further applied to a rotational/translational actuator system. version:3
arxiv-1405-2496 | Anomaly-Sensitive Dictionary Learning for Unsupervised Diagnostics of Solid Media | http://arxiv.org/abs/1405.2496 | id:1405.2496 author:Jeffrey M. Druce, Jarvis D. Haupt, Stefano Gonella category:cs.CV  published:2014-05-11 summary:This paper proposes a strategy for the detection and triangulation of structural anomalies in solid media. The method revolves around the construction of sparse representations of the medium's dynamic response, obtained by learning instructive dictionaries which form a suitable basis for the response data. The resulting sparse coding problem is recast as a modified dictionary learning task with additional spatial sparsity constraints enforced on the atoms of the learned dictionaries, which provides them with a prescribed spatial topology that is designed to unveil anomalous regions in the physical domain. The proposed methodology is model agnostic, i.e., it forsakes the need for a physical model and requires virtually no a priori knowledge of the structure's material properties, as all the inferences are exclusively informed by the data through the layers of information that are available in the intrinsic salient structure of the material's dynamic response. This characteristic makes the approach powerful for anomaly identification in systems with unknown or heterogeneous property distribution, for which a model is unsuitable or unreliable. The method is validated using both synthetically version:1
arxiv-1405-2434 | Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation | http://arxiv.org/abs/1405.2434 | id:1405.2434 author:Chen Lijiang category:cs.CL  published:2014-05-10 summary:Minimum error rate training (MERT) is a widely used training procedure for statistical machine translation. A general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions. This paper introduces coordinate system selection (RSS) into the search algorithm for MERT. Contrary to previous approaches in which every dimension only corresponds to one independent feature function, we create several coordinate systems by moving one of the dimensions to a new direction. The basic idea is quite simple but critical that the training procedure of MERT should be based on a coordinate system formed by search directions but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be obtained without any other language knowledge. version:1
arxiv-1405-2432 | Functional Bandits | http://arxiv.org/abs/1405.2432 | id:1405.2432 author:Long Tran-Thanh, Jia Yuan Yu category:stat.ML cs.LG  published:2014-05-10 summary:We introduce the functional bandit problem, where the objective is to find an arm that optimises a known functional of the unknown arm-reward distributions. These problems arise in many settings such as maximum entropy methods in natural language processing, and risk-averse decision-making, but current best-arm identification techniques fail in these domains. We propose a new approach, that combines functional estimation and arm elimination, to tackle this problem. This method achieves provably efficient performance guarantees. In addition, we illustrate this method on a number of important functionals in risk management and information theory, and refine our generic theoretical results in those cases. version:1
arxiv-1401-1124 | A binary differential evolution algorithm learning from explored solutions | http://arxiv.org/abs/1401.1124 | id:1401.1124 author:Yu Chen, Weicheng Xie, Xiufen Zou category:cs.NE  published:2014-01-06 summary:Although real-coded differential evolution (DE) algorithms can perform well on continuous optimization problems (CoOPs), it is still a challenging task to design an efficient binary-coded DE algorithm. Inspired by the learning mechanism of particle swarm optimization (PSO) algorithms, we propose a binary learning differential evolution (BLDE) algorithm that can efficiently locate the global optimal solutions by learning from the last population. Then, we theoretically prove the global convergence of BLDE, and compare it with some existing binary-coded evolutionary algorithms (EAs) via numerical experiments. Numerical results show that BLDE is competitive to the compared EAs, and meanwhile, further study is performed via the change curves of a renewal metric and a refinement metric to investigate why BLDE cannot outperform some compared EAs for several selected benchmark problems. Finally, we employ BLDE solving the unit commitment problem (UCP) in power systems to show its applicability in practical problems. version:2
arxiv-1405-2420 | Optimal Learners for Multiclass Problems | http://arxiv.org/abs/1405.2420 | id:1405.2420 author:Amit Daniely, Shai Shalev-Shwartz category:cs.LG  published:2014-05-10 summary:The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundmamental question of "how to learn"? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005). version:1
arxiv-1211-0616 | The complexity of learning halfspaces using generalized linear methods | http://arxiv.org/abs/1211.0616 | id:1211.0616 author:Amit Daniely, Nati Linial, Shai Shalev-Shwartz category:cs.LG cs.DS  published:2012-11-03 summary:Many popular learning algorithms (E.g. Regression, Fourier-Transform based algorithms, Kernel SVM and Kernel ridge regression) operate by reducing the problem to a convex optimization problem over a vector space of functions. These methods offer the currently best approach to several central problems such as learning half spaces and learning DNF's. In addition they are widely used in numerous application domains. Despite their importance, there are still very few proof techniques to show limits on the power of these algorithms. We study the performance of this approach in the problem of (agnostically and improperly) learning halfspaces with margin $\gamma$. Let $\mathcal{D}$ be a distribution over labeled examples. The $\gamma$-margin error of a hyperplane $h$ is the probability of an example to fall on the wrong side of $h$ or at a distance $\le\gamma$ from it. The $\gamma$-margin error of the best $h$ is denoted $\mathrm{Err}_\gamma(\mathcal{D})$. An $\alpha(\gamma)$-approximation algorithm receives $\gamma,\epsilon$ as input and, using i.i.d. samples of $\mathcal{D}$, outputs a classifier with error rate $\le \alpha(\gamma)\mathrm{Err}_\gamma(\mathcal{D}) + \epsilon$. Such an algorithm is efficient if it uses $\mathrm{poly}(\frac{1}{\gamma},\frac{1}{\epsilon})$ samples and runs in time polynomial in the sample size. The best approximation ratio achievable by an efficient algorithm is $O\left(\frac{1/\gamma}{\sqrt{\log(1/\gamma)}}\right)$ and is achieved using an algorithm from the above class. Our main result shows that the approximation ratio of every efficient algorithm from this family must be $\ge \Omega\left(\frac{1/\gamma}{\mathrm{poly}\left(\log\left(1/\gamma\right)\right)}\right)$, essentially matching the best known upper bound. version:4
arxiv-1311-2838 | A PAC-Bayesian bound for Lifelong Learning | http://arxiv.org/abs/1311.2838 | id:1311.2838 author:Anastasia Pentina, Christoph H. Lampert category:stat.ML cs.LG 68T05  published:2013-11-12 summary:Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods. version:2
arxiv-1405-2403 | Hyperspectral pan-sharpening: a variational convex constrained formulation to impose parallel level lines, solved with ADMM | http://arxiv.org/abs/1405.2403 | id:1405.2403 author:Alexis Huck, François de Vieilleville, Pierre Weiss, Manuel Grizonnet category:cs.CV  published:2014-05-10 summary:In this paper, we address the issue of hyperspectral pan-sharpening, which consists in fusing a (low spatial resolution) hyperspectral image HX and a (high spatial resolution) panchromatic image P to obtain a high spatial resolution hyperspectral image. The problem is addressed under a variational convex constrained formulation. The objective favors high resolution spectral bands with level lines parallel to those of the panchromatic image. This term is balanced with a total variation term as regularizer. Fit-to-P data and fit-to-HX data constraints are effectively considered as mathematical constraints, which depend on the statistics of the data noise measurements. The developed Alternating Direction Method of Multipliers (ADMM) optimization scheme enables us to solve this problem efficiently despite the non differentiabilities and the huge number of unknowns. version:1
arxiv-1405-2386 | Predicting Central Topics in a Blog Corpus from a Networks Perspective | http://arxiv.org/abs/1405.2386 | id:1405.2386 author:Srayan Datta category:cs.IR cs.CL cs.SI physics.soc-ph  published:2014-05-10 summary:In today's content-centric Internet, blogs are becoming increasingly popular and important from a data analysis perspective. According to Wikipedia, there were over 156 million public blogs on the Internet as of February 2011. Blogs are a reflection of our contemporary society. The contents of different blog posts are important from social, psychological, economical and political perspectives. Discovery of important topics in the blogosphere is an area which still needs much exploring. We try to come up with a procedure using probabilistic topic modeling and network centrality measures which identifies the central topics in a blog corpus. version:1
arxiv-1405-2377 | A Hybrid Monte Carlo Architecture for Parameter Optimization | http://arxiv.org/abs/1405.2377 | id:1405.2377 author:James Brofos category:stat.ML cs.LG stat.ME  published:2014-05-10 summary:Much recent research has been conducted in the area of Bayesian learning, particularly with regard to the optimization of hyper-parameters via Gaussian process regression. The methodologies rely chiefly on the method of maximizing the expected improvement of a score function with respect to adjustments in the hyper-parameters. In this work, we present a novel algorithm that exploits notions of confidence intervals and uncertainties to enable the discovery of the best optimal within a targeted region of the parameter space. We demonstrate the efficacy of our algorithm with respect to machine learning problems and show cases where our algorithm is competitive with the method of maximizing expected improvement. version:1
arxiv-1311-6184 | Bounding the Test Log-Likelihood of Generative Models | http://arxiv.org/abs/1311.6184 | id:1311.6184 author:Yoshua Bengio, Li Yao, Kyunghyun Cho category:cs.LG  published:2013-11-24 summary:Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable normalization. Some of them may even not have an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood, and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing. We further propose a biased variant of the estimator that can be used reliably with a finite number of samples for the purpose of model comparison. version:4
arxiv-1405-2362 | Image Segmentation Using Frequency Locking of Coupled Oscillators | http://arxiv.org/abs/1405.2362 | id:1405.2362 author:Yan Fang, Matthew J. Cotter, Donald M. Chiarulli, Steven P. Levitan category:cs.CV q-bio.NC C.1.3  published:2014-05-09 summary:Synchronization of coupled oscillators is observed at multiple levels of neural systems, and has been shown to play an important function in visual perception. We propose a computing system based on locally coupled oscillator networks for image segmentation. The system can serve as the preprocessing front-end of an image processing pipeline where the common frequencies of clusters of oscillators reflect the segmentation results. To demonstrate the feasibility of our design, the system is simulated and tested on a human face image dataset and its performance is compared with traditional intensity threshold based algorithms. Our system shows both better performance and higher noise tolerance than traditional methods. version:1
arxiv-1405-2316 | Better Feature Tracking Through Subspace Constraints | http://arxiv.org/abs/1405.2316 | id:1405.2316 author:Bryan Poling, Gilad Lerman, Arthur Szlam category:cs.CV  published:2014-05-09 summary:Feature tracking in video is a crucial task in computer vision. Usually, the tracking problem is handled one feature at a time, using a single-feature tracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives. While this approach works quite well when dealing with high-quality video and "strong" features, it often falters when faced with dark and noisy video containing low-quality features. We present a framework for jointly tracking a set of features, which enables sharing information between the different features in the scene. We show that our method can be employed to track features for both rigid and nonrigid motions (possibly of few moving bodies) even when some features are occluded. Furthermore, it can be used to significantly improve tracking results in poorly-lit scenes (where there is a mix of good and bad features). Our approach does not require direct modeling of the structure or the motion of the scene, and runs in real time on a single CPU core. version:1
arxiv-1401-4112 | A bi-level view of inpainting - based image compression | http://arxiv.org/abs/1401.4112 | id:1401.4112 author:Yunjin Chen, René Ranftl, Thomas Pock category:cs.CV  published:2014-01-16 summary:Inpainting based image compression approaches, especially linear and non-linear diffusion models, are an active research topic for lossy image compression. The major challenge in these compression models is to find a small set of descriptive supporting points, which allow for an accurate reconstruction of the original image. It turns out in practice that this is a challenging problem even for the simplest Laplacian interpolation model. In this paper, we revisit the Laplacian interpolation compression model and introduce two fast algorithms, namely successive preconditioning primal dual algorithm and the recently proposed iPiano algorithm, to solve this problem efficiently. Furthermore, we extend the Laplacian interpolation based compression model to a more general form, which is based on principles from bi-level optimization. We investigate two different variants of the Laplacian model, namely biharmonic interpolation and smoothed Total Variation regularization. Our numerical results show that significant improvements can be obtained from the biharmonic interpolation model, and it can recover an image with very high quality from only 5% pixels. version:2
arxiv-1405-2278 | Hellinger Distance Trees for Imbalanced Streams | http://arxiv.org/abs/1405.2278 | id:1405.2278 author:R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers category:cs.LG astro-ph.IM stat.ML  published:2014-05-09 summary:Classifiers trained on data sets possessing an imbalanced class distribution are known to exhibit poor generalisation performance. This is known as the imbalanced learning problem. The problem becomes particularly acute when we consider incremental classifiers operating on imbalanced data streams, especially when the learning objective is rare class identification. As accuracy may provide a misleading impression of performance on imbalanced data, existing stream classifiers based on accuracy can suffer poor minority class performance on imbalanced streams, with the result being low minority class recall rates. In this paper we address this deficiency by proposing the use of the Hellinger distance measure, as a very fast decision tree split criterion. We demonstrate that by using Hellinger a statistically significant improvement in recall rates on imbalanced data streams can be achieved, with an acceptable increase in the false positive rate. version:1
arxiv-1405-2262 | Training Deep Fourier Neural Networks To Fit Time-Series Data | http://arxiv.org/abs/1405.2262 | id:1405.2262 author:Michael S. Gashler, Stephen C. Ashmore category:cs.NE cs.LG  published:2014-05-09 summary:We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends. version:1
arxiv-1405-2246 | Graph Regularized Non-negative Matrix Factorization By Maximizing Correntropy | http://arxiv.org/abs/1405.2246 | id:1405.2246 author:Le Li, Jianjun Yang, Kaili Zhao, Yang Xu, Honggang Zhang, Zhuoyi Fan category:cs.CV  published:2014-05-09 summary:Non-negative matrix factorization (NMF) has proved effective in many clustering and classification tasks. The classic ways to measure the errors between the original and the reconstructed matrix are $l_2$ distance or Kullback-Leibler (KL) divergence. However, nonlinear cases are not properly handled when we use these error measures. As a consequence, alternative measures based on nonlinear kernels, such as correntropy, are proposed. However, the current correntropy-based NMF only targets on the low-level features without considering the intrinsic geometrical distribution of data. In this paper, we propose a new NMF algorithm that preserves local invariance by adding graph regularization into the process of max-correntropy-based matrix factorization. Meanwhile, each feature can learn corresponding kernel from the data. The experiment results of Caltech101 and Caltech256 show the benefits of such combination against other NMF algorithms for the unsupervised image clustering. version:1
arxiv-1405-2227 | An Overview of Face Liveness Detection | http://arxiv.org/abs/1405.2227 | id:1405.2227 author:Saptarshi Chakraborty, Dhrubajyoti Das category:cs.CV  published:2014-05-09 summary:Face recognition is a widely used biometric approach. Face recognition technology has developed rapidly in recent years and it is more direct, user friendly and convenient compared to other methods. But face recognition systems are vulnerable to spoof attacks made by non-real faces. It is an easy way to spoof face recognition systems by facial pictures such as portrait photographs. A secure system needs Liveness detection in order to guard against such spoofing. In this work, face liveness detection approaches are categorized based on the various types techniques used for liveness detection. This categorization helps understanding different spoof attacks scenarios and their relation to the developed solutions. A review of the latest works regarding face liveness detection works is presented. The main aim is to provide a simple path for the future development of novel and more secured face liveness detection approach. version:1
arxiv-1405-2220 | Gaussian-Chain Filters for Heavy-Tailed Noise with Application to Detecting Big Buyers and Big Sellers in Stock Market | http://arxiv.org/abs/1405.2220 | id:1405.2220 author:Li-Xin Wang category:q-fin.TR cs.CE cs.CV cs.SY q-fin.ST  published:2014-05-09 summary:We propose a new heavy-tailed distribution --- Gaussian-Chain (GC) distribution, which is inspirited by the hierarchical structures prevailing in social organizations. We determine the mean, variance and kurtosis of the Gaussian-Chain distribution to show its heavy-tailed property, and compute the tail distribution table to give specific numbers showing how heavy is the heavy-tails. To filter out the heavy-tailed noise, we construct two filters --- 2nd and 3rd-order GC filters --- based on the maximum likelihood principle. Simulation results show that the GC filters perform much better than the benchmark least-squares algorithm when the noise is heavy-tail distributed. Using the GC filters, we propose a trading strategy, named Ride-the-Mood, to follow the mood of the market by detecting the actions of the big buyers and the big sellers in the market based on the noisy, heavy-tailed price data. Application of the Ride-the-Mood strategy to five blue-chip Hong Kong stocks over the recent two-year period from April 2, 2012 to March 31, 2014 shows that their returns are higher than the returns of the benchmark Buy-and-Hold strategy and the Hang Seng Index Fund. version:1
arxiv-1405-2168 | Evaluation The Efficiency Of Cuckoo Optimization Algorithm | http://arxiv.org/abs/1405.2168 | id:1405.2168 author:Elham Shadkam, Mehdi Bijari category:cs.NE cs.NA math.NA  published:2014-05-09 summary:In this paper a new evolutionary algorithm, for continuous nonlinear optimization problems, is surveyed. This method is inspired by the life of a bird, called Cuckoo. The Cuckoo Optimization Algorithm (COA) is evaluated by using the Rastrigin function. The problem is a non-linear continuous function which is used for evaluating optimization algorithms. The efficiency of the COA has been studied by obtaining optimal solution of various dimensions Rastrigin function in this paper. The mentioned function also was solved by FA and ABC algorithms. Comparing the results shows the COA has better performance than other algorithms. Application of algorithm to test function has proven its capability to deal with difficult optimization problems. version:1
arxiv-1405-6662 | Cognitive-mapping and contextual pyramid based Digital Elevation Model Registration and its effective storage using fractal based compression | http://arxiv.org/abs/1405.6662 | id:1405.6662 author:Suma Dawn, Vikas Saxena, Bhudev Sharma category:cs.AI cs.CV  published:2014-05-09 summary:Digital Elevation models (DEM) are images having terrain information embedded into them. Using cognitive mapping concepts for DEM registration, has evolved from this basic idea of using the mapping between the space to objects and defining their relationships to form the basic landmarks that need to be marked, stored and manipulated in and about the environment or other candidate environments, namely, in our case, the DEMs. The progressive two-level encapsulation of methods of geo-spatial cognition includes landmark knowledge and layout knowledge and can be useful for DEM registration. Space-based approach, that emphasizes on explicit extent of the environment under consideration, and object-based approach, that emphasizes on the relationships between objects in the local environment being the two paradigms of cognitive mapping can be methodically integrated in this three-architecture for DEM registration. Initially, P-model based segmentation is performed followed by landmark formation for contextual mapping that uses contextual pyramid formation. Apart from landmarks being used for registration key-point finding, Euclidean distance based deformation calculation has been used for transformation and change detection. Landmarks have been categorized to belong to either being flat-plain areas without much variation in the land heights; peaks that can be found when there is gradual increase in height as compared to the flat areas; valleys, marked with gradual decrease in the height seen in DEM; and finally, ripple areas with very shallow crests and nadirs. Fractal based compression was used for storage of co-registered DEMs. This method may further be extended for DEM-topographic map and DEM-to-remote sensed image registration. Experimental results further cement the fact that DEM registration may be effectively done using the proposed method. version:1
arxiv-1312-6813 | New explicit thresholding/shrinkage formulas for one class of regularization problems with overlapping group sparsity and their applications | http://arxiv.org/abs/1312.6813 | id:1312.6813 author:Gang Liu, Ting-Zhu Huang, Xiao-Guang Lv, Jun Liu category:math.NA cs.CV  published:2013-12-24 summary:The least-square regression problems or inverse problems have been widely studied in many fields such as compressive sensing, signal processing, and image processing. To solve this kind of ill-posed problems, a regularization term (i.e., regularizer) should be introduced, under the assumption that the solutions have some specific properties, such as sparsity and group sparsity. Widely used regularizers include the $\ell_1$ norm, total variation (TV) semi-norm, and so on. Recently, a new regularization term with overlapping group sparsity has been considered. Majorization minimization iteration method or variable duplication methods are often applied to solve them. However, there have been no direct methods for solve the relevant problems because of the difficulty of overlapping. In this paper, we proposed new explicit shrinkage formulas for one class of these relevant problems, whose regularization terms have translation invariant overlapping groups. Moreover, we apply our results in TV deblurring and denoising with overlapping group sparsity. We use alternating direction method of multipliers to iterate solve it. Numerical results also verify the validity and effectiveness of our new explicit shrinkage formulas. version:3
arxiv-1306-3895 | On-line PCA with Optimal Regrets | http://arxiv.org/abs/1306.3895 | id:1306.3895 author:Jiazhong Nie, Wojciech Kotlowski, Manfred K. Warmuth category:cs.LG  published:2013-06-17 summary:We carefully investigate the on-line version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we analyze two popular on-line algorithms, Gradient Descent (GD) and Exponentiated Gradient (EG). We show that both algorithms are essentially optimal in the worst-case. This comes as a surprise, since EG is known to perform sub-optimally when the instances are sparse. This different behavior of EG for PCA is mainly related to the non-negativity of the loss in this case, which makes the PCA setting qualitatively different from other settings studied in the literature. Furthermore, we show that when considering regret bounds as function of a loss budget, EG remains optimal and strictly outperforms GD. Next, we study the extension of the PCA setting, in which the Nature is allowed to play with dense instances, which are positive matrices with bounded largest eigenvalue. Again we can show that EG is optimal and strictly better than GD in this setting. version:2
arxiv-1405-0546 | Kaggle LSHTC4 Winning Solution | http://arxiv.org/abs/1405.0546 | id:1405.0546 author:Antti Puurula, Jesse Read, Albert Bifet category:cs.AI cs.CL cs.IR  published:2014-05-03 summary:Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies. version:2
arxiv-1405-2128 | Variational Image Segmentation Model Coupled with Image Restoration Achievements | http://arxiv.org/abs/1405.2128 | id:1405.2128 author:Xiaohao Cai category:cs.CV math.NA 65Kxx  65Yxx G.1.0; G.1.6  published:2014-05-09 summary:Image segmentation and image restoration are two important topics in image processing with great achievements. In this paper, we propose a new multiphase segmentation model by combining image restoration and image segmentation models. Utilizing image restoration aspects, the proposed segmentation model can effectively and robustly tackle high noisy images, blurry images, images with missing pixels, and vector-valued images. In particular, one of the most important segmentation models, the piecewise constant Mumford-Shah model, can be extended easily in this way to segment gray and vector-valued images corrupted for example by noise, blur or missing pixels after coupling a new data fidelity term which comes from image restoration topics. It can be solved efficiently using the alternating minimization algorithm, and we prove the convergence of this algorithm with three variables under mild condition. Experiments on many synthetic and real-world images demonstrate that our method gives better segmentation results in comparison to others state-of-the-art segmentation models especially for blurry images and images with missing pixels values. version:1
arxiv-1401-3531 | Highly comparative feature-based time-series classification | http://arxiv.org/abs/1401.3531 | id:1401.3531 author:Ben D. Fulcher, Nick S. Jones category:cs.LG cs.AI cs.DB physics.data-an q-bio.QM  published:2014-01-15 summary:A highly comparative, feature-based approach to time series classification is introduced that uses an extensive database of algorithms to extract thousands of interpretable features from time series. These features are derived from across the scientific time-series analysis literature, and include summaries of time series in terms of their correlation structure, distribution, entropy, stationarity, scaling properties, and fits to a range of time-series models. After computing thousands of features for each time series in a training set, those that are most informative of the class structure are selected using greedy forward feature selection with a linear classifier. The resulting feature-based classifiers automatically learn the differences between classes using a reduced number of time-series properties, and circumvent the need to calculate distances between time series. Representing time series in this way results in orders of magnitude of dimensionality reduction, allowing the method to perform well on very large datasets containing long time series or time series of different lengths. For many of the datasets studied, classification performance exceeded that of conventional instance-based classifiers, including one nearest neighbor classifiers using Euclidean distances and dynamic time warping and, most importantly, the features selected provide an understanding of the properties of the dataset, insight that can guide further scientific investigation. version:2
arxiv-1405-2102 | Improving Image Clustering using Sparse Text and the Wisdom of the Crowds | http://arxiv.org/abs/1405.2102 | id:1405.2102 author:Anna Ma, Arjuna Flenner, Deanna Needell, Allon G. Percus category:cs.LG cs.CV  published:2014-05-08 summary:We propose a method to improve image clustering using sparse text and the wisdom of the crowds. In particular, we present a method to fuse two different kinds of document features, image and text features, and use a common dictionary or "wisdom of the crowds" as the connection between the two different kinds of documents. With the proposed fusion matrix, we use topic modeling via non-negative matrix factorization to cluster documents. version:1
arxiv-1405-1533 | A consistent deterministic regression tree for non-parametric prediction of time series | http://arxiv.org/abs/1405.1533 | id:1405.1533 author:Pierre Gaillard, Paul Baudin category:math.ST cs.LG stat.ML stat.TH  published:2014-05-07 summary:We study online prediction of bounded stationary ergodic processes. To do so, we consider the setting of prediction of individual sequences and build a deterministic regression tree that performs asymptotically as well as the best L-Lipschitz constant predictors. Then, we show why the obtained regret bound entails the asymptotical optimality with respect to the class of bounded stationary ergodic processes. version:2
