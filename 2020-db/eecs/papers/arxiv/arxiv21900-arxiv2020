arxiv-1610-04265 | Fast, Scalable Phrase-Based SMT Decoding | http://arxiv.org/abs/1610.04265 | id:1610.04265 author:Hieu Hoang, Nikolay Bogoychev, Lane Schwartz, Marcin Junczys-Dowmunt category:cs.CL  published:2016-10-13 summary:The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers. In this paper we re-examine the major components of phrase-based decoding and decoder implementation with particular emphasis on speed and scalability on multicore machines. The result is a drop-in replacement for the Moses decoder which is up to fifteen times faster and scales monotonically with the number of cores. version:2
arxiv-1610-05815 | Statistical Learning Theory Approach for Data Classification with l-diversity | http://arxiv.org/abs/1610.05815 | id:1610.05815 author:Koray Mancuhan, Chris Clifton category:cs.LG cs.CR cs.DB  published:2016-10-18 summary:Corporations are retaining ever-larger corpuses of personal data; the frequency or breaches and corresponding privacy impact have been rising accordingly. One way to mitigate this risk is through use of anonymized data, limiting the exposure of individual data to only where it is absolutely needed. This would seem particularly appropriate for data mining, where the goal is generalizable knowledge rather than data on specific individuals. In practice, corporate data miners often insist on original data, for fear that they might "miss something" with anonymized or differentially private approaches. This paper provides a theoretical justification for the use of anonymized data. Specifically, we show that a support vector classifier trained on anatomized data satisfying l-diversity should be expected to do as well as on the original data. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values, thus satisfying l-diversity. The theoretical effectiveness of the proposed approach is validated using several publicly available datasets, showing that we outperform the state of the art for support vector classification using training data protected by k-anonymity, and are comparable to learning on the original data. version:1
arxiv-1610-05796 | Decision Tree Classification on Outsourced Data | http://arxiv.org/abs/1610.05796 | id:1610.05796 author:Koray Mancuhan, Chris Clifton category:cs.LG cs.CR cs.DB H.2.8; H.2.7  published:2016-10-18 summary:This paper proposes a client-server decision tree learning method for outsourced private data. The privacy model is anatomization/fragmentation: the server sees data values, but the link between sensitive and identifying information is encrypted with a key known only to clients. Clients have limited processing and storage capability. Both sensitive and identifying information thus are stored on the server. The approach presented also retains most processing at the server, and client-side processing is amortized over predictions made by the clients. Experiments on various datasets show that the method produces decision trees approaching the accuracy of a non-private decision tree, while substantially reducing the client's computing resource requirements. version:1
arxiv-1610-05214 | A polynomial-time relaxation of the Gromov-Hausdorff distance | http://arxiv.org/abs/1610.05214 | id:1610.05214 author:Soledad Villar, Afonso S. Bandeira, Andrew J. Blumberg, Rachel Ward category:math.GT cs.CG math.OC stat.ML  published:2016-10-17 summary:The Gromov-Hausdorff distance provides a metric on the set of isometry classes of compact metric spaces. Unfortunately, computing this metric directly is believed to be computationally intractable. Motivated by applications in shape matching and point-cloud comparison, we study a semidefinite programming relaxation of the Gromov-Hausdorff metric. This relaxation can be computed in polynomial time, and somewhat surprisingly is itself a pseudometric. We describe the induced topology on the set of compact metric spaces. Finally, we demonstrate the numerical performance of various algorithms for computing the relaxed distance and apply these algorithms to several relevant data sets. In particular we propose a greedy algorithm for finding the best correspondence between finite metric spaces that can handle hundreds of points. version:2
arxiv-1610-05792 | Big Batch SGD: Automated Inference using Adaptive Batch Sizes | http://arxiv.org/abs/1610.05792 | id:1610.05792 author:Soham De, Abhay Yadav, David Jacobs, Tom Goldstein category:cs.LG math.NA math.OC stat.ML  published:2016-10-18 summary:Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative "big batch" SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD methods without requiring convexity of the objective function. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. For this reason, big batch methods are easily automated and can run with little or no user oversight. version:1
arxiv-1610-05775 | Modeling the Dynamics of Online Learning Activity | http://arxiv.org/abs/1610.05775 | id:1610.05775 author:Charalampos Mavroforakis, Isabel Valera, Manuel Gomez Rodriguez category:stat.ML cs.LG cs.SI  published:2016-10-18 summary:People are increasingly relying on the Web and social media to find solutions to their problems in a wide range of domains. In this online setting, closely related problems often lead to the same characteristic learning pattern, in which people sharing these problems visit related pieces of information, perform almost identical queries or, more generally, take a series of similar actions. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions taken by thousands of users. Experiments on real data gathered from Stack Overflow reveal that our framework can recover meaningful learning patterns in terms of both content and temporal dynamics, as well as accurately track users' interests and goals over time. version:1
arxiv-1610-05773 | RedQueen: An Online Algorithm for Smart Broadcasting in Social Networks | http://arxiv.org/abs/1610.05773 | id:1610.05773 author:Ali Zarezade, Utkarsh Upadhyay, Hamid Rabiee, Manuel Gomez Rodriguez category:stat.ML cs.DS cs.LG cs.SI  published:2016-10-18 summary:Users in social networks whose posts stay at the top of their followers'{} feeds the longest time are more likely to be noticed. Can we design an online algorithm to help them decide when to post to stay at the top? In this paper, we address this question as a novel optimal control problem for jump stochastic differential equations. For a wide variety of feed dynamics, we show that the optimal broadcasting intensity for any user is surprisingly simple -- it is given by the position of her most recent post on each of her follower's feeds. As a consequence, we are able to develop a simple and highly efficient online algorithm, RedQueen, to sample the optimal times for the user to post. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently make a user's posts more visible over time, is robust to volume changes on her followers' feeds, and significantly outperforms the state of the art. version:1
arxiv-1610-01934 | Using Non-invertible Data Transformations to Build Adversary-Resistant Deep Neural Networks | http://arxiv.org/abs/1610.01934 | id:1610.01934 author:Qinglong Wang, Wenbo Guo, Alexander G. Ororbia II, Xinyu Xing, Lin Lin, C. Lee Giles, Xue Liu, Peng Liu, Gang Xiong category:cs.LG  published:2016-10-06 summary:Deep neural networks have proven to be quite effective in a wide variety of machine learning tasks, ranging from improved speech recognition systems to advancing the development of autonomous vehicles. However, despite their superior performance in many applications, these models have been recently shown to be susceptible to a particular type of attack possible through the generation of particular synthetic examples referred to as adversarial samples. These samples are constructed by manipulating real examples from the training data distribution in order to "fool" the original neural model, resulting in misclassification (with high confidence) of previously correctly classified samples. Addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. In this paper, we present an analysis of this fundamental flaw lurking in all neural architectures to uncover limitations of previously proposed defense mechanisms. More importantly, we present a unifying framework for protecting deep neural models using a non-invertible data transformation--developing two adversary-resilient architectures utilizing both linear and nonlinear dimensionality reduction. Empirical results indicate that our framework provides better robustness compared to state-of-art solutions while having negligible degradation in accuracy. version:4
arxiv-1610-05756 | Modeling community structure and topics in dynamic text networks | http://arxiv.org/abs/1610.05756 | id:1610.05756 author:Teague Henry, David Banks, Christine Chai, Derek Owens-Oas category:cs.SI physics.soc-ph stat.ML  published:2016-10-18 summary:The last decade has seen great progress in both dynamic network modeling and topic modeling. This paper draws upon both areas to create a Bayesian method that allows topic discovery to inform the latent network model and the network structure to facilitate topic identification. We apply this method to the 467 top political blogs of 2012. Our results find complex community structure within this set of blogs, where community membership depends strongly upon the set of topics in which the blogger is interested. version:1
arxiv-1610-05735 | Deep Amortized Inference for Probabilistic Programs | http://arxiv.org/abs/1610.05735 | id:1610.05735 author:Daniel Ritchie, Paul Horsfall, Noah D. Goodman category:cs.AI cs.LG stat.ML  published:2016-10-18 summary:Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks and deep generative models. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning. version:1
arxiv-1610-05729 | Scaling Up MAP-Elites Using Centroidal Voronoi Tessellations | http://arxiv.org/abs/1610.05729 | id:1610.05729 author:Vassilis Vassiliades, Konstantinos Chatzilygeroudis, Jean-Baptiste Mouret category:cs.NE  published:2016-10-18 summary:The recently introduced Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) is an evolutionary algorithm capable of producing a large archive of diverse, high-performing solutions in a single run. It works by discretizing a continuous feature space into unique regions according to the desired discretization per dimension. While simple, this algorithm has a main drawback: it cannot scale to high-dimensional feature spaces since the number of regions increase exponentially with the number of dimensions. In this paper, we address this limitation by introducing a simple extension of MAP-Elites that has a constant, pre-defined number of regions irrespective of the dimensionality of the feature space. Our main insight is that methods from computational geometry could partition a high-dimensional space into well-spread geometric regions. In particular, our algorithm uses a centroidal Voronoi tessellation (CVT) to divide the feature space into a desired number of regions; it then places every generated individual in its closest region, replacing a less fit one if the region is already occupied. We demonstrate the effectiveness of the new "CVT-MAP-Elites" algorithm in high-dimensional feature spaces through comparisons against MAP-Elites in a hexapod locomotion task. version:1
arxiv-1610-05716 | Design Mining Microbial Fuel Cell Cascades | http://arxiv.org/abs/1610.05716 | id:1610.05716 author:Richard J. Preen, Jiseon You, Larry Bull, Ioannis A. Ieropoulos category:cs.NE cs.AI  published:2016-10-18 summary:Microbial fuel cells (MFCs) perform wastewater treatment and electricity production through the conversion of organic matter using microorganisms. For practical applications, it has been suggested that greater efficiency can be achieved by arranging multiple MFC units into physical stacks in a cascade with feedstock flowing sequentially between units. In this paper, we investigate the use of computational intelligence to physically explore and optimise (potentially) heterogeneous MFC designs in a cascade, i.e. without simulation. Conductive structures are 3-D printed and inserted into the anodic chamber of each MFC unit, augmenting a carbon fibre veil anode and affecting the hydrodynamics, including the feedstock volume and hydraulic retention time, as well as providing unique habitats for microbial colonisation. We show that it is possible to use design mining to identify new conductive inserts that increase both the cascade power output and power density. version:1
arxiv-1610-05712 | Fast L1-NMF for Multiple Parametric Model Estimation | http://arxiv.org/abs/1610.05712 | id:1610.05712 author:Mariano Tepper, Guillermo Sapiro category:cs.CV cs.LG  published:2016-10-18 summary:In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a \textit{parameterless} biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples. version:1
arxiv-1610-05710 | Feasibility Based-Large Margin Nearest Neighbor Metric Learning | http://arxiv.org/abs/1610.05710 | id:1610.05710 author:Babak Hosseini, Barbara Hammer category:cs.DS cs.LG  published:2016-10-18 summary:In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier. In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem. However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen. In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points. We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates. In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space. Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets. version:1
arxiv-1610-05693 | Semantic Decomposition and Recognition of Long and Complex Manipulation Action Sequences | http://arxiv.org/abs/1610.05693 | id:1610.05693 author:Eren Erdal Aksoy, Adil Orhan, Florentin Woergoetter category:cs.CV  published:2016-10-18 summary:Understanding continuous human actions is a non-trivial but important problem in computer vision. Although there exists a large corpus of work in the recognition of action sequences, most approaches suffer from problems relating to vast variations in motions, action combinations, and scene contexts. In this paper, we introduce a novel method for semantic segmentation and recognition of long and complex manipulation action tasks, such as "preparing a breakfast" or "making a sandwich". We represent manipulations with our recently introduced "Semantic Event Chain" (SEC) concept, which captures the underlying spatiotemporal structure of an action invariant to motion, velocity, and scene context. Solely based on the spatiotemporal interactions between manipulated objects and hands in the extracted SEC, the framework automatically parses individual manipulation streams performed either sequentially or concurrently. Using event chains, our method further extracts basic primitive elements of each parsed manipulation. Without requiring any prior object knowledge, the proposed framework can also extract object-like scene entities that exhibit the same role in semantically similar manipulations. We conduct extensive experiments on various recent datasets to validate the robustness of the framework. version:1
arxiv-1610-05688 | Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models | http://arxiv.org/abs/1610.05688 | id:1610.05688 author:Pranay Dighe, Afsaneh Asaei, Herve Bourlard category:cs.CL cs.AI cs.HC cs.LG  published:2016-10-18 summary:Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate. version:1
arxiv-1610-05683 | Rejection Sampling Variational Inference | http://arxiv.org/abs/1610.05683 | id:1610.05683 author:Christian A. Naesseth, Francisco J. R. Ruiz, Scott W. Linderman, David M. Blei category:stat.ML stat.ME  published:2016-10-18 summary:Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a (differentiable) deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on rejection sampling. The discontinuity introduced by the accept--reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic optimization variational inference. version:1
arxiv-1610-05670 | Stylometric Analysis of Early Modern Period English Plays | http://arxiv.org/abs/1610.05670 | id:1610.05670 author:Mark Eisen, Santiago Segarra, Gabriel Egan, Alejandro Ribeiro category:cs.CL cs.LG  published:2016-10-18 summary:Function word adjacency networks (WANs) are used to study the authorship of plays from the Early Modern English period. In these networks, nodes are function words and directed edges between two nodes represent the likelihood of ordered co-appearance of the two words. For every analyzed play a WAN is constructed and these are aggregated to generate author profile networks. We first study the similarity of writing styles between Early English playwrights by comparing the profile WANs. The accuracy of using WANs for authorship attribution is then demonstrated by attributing known plays among six popular playwrights. The WAN method is shown to additionally outperform other frequency-based methods on attributing Early English plays. This high classification power is then used to investigate the authorship of anonymous plays. Moreover, WANs are shown to be reliable classifiers even when attributing collaborative plays. For several plays of disputed co- authorship, a deeper analysis is performed by attributing every act and scene separately, in which we both corroborate existing breakdowns and provide evidence of new assignments. Finally, the impact of genre on attribution accuracy is examined revealing that the genre of a play partially conditions the choice of the function words used in it. version:1
arxiv-1610-05613 | From Traditional to Modern : Domain Adaptation for Action Classification in Short Social Video Clips | http://arxiv.org/abs/1610.05613 | id:1610.05613 author:Aditya Singh, Saurabh Saini, Rajvi Shah, P J Narayanan category:cs.CV I.5.4; I.2.10  published:2016-10-18 summary:Short internet video clips like vines present a significantly wild distribution compared to traditional video datasets. In this paper, we focus on the problem of unsupervised action classification in wild vines using traditional labeled datasets. To this end, we use a data augmentation based simple domain adaptation strategy. We utilise semantic word2vec space as a common subspace to embed video features from both, labeled source domain and unlablled target domain. Our method incrementally augments the labeled source with target samples and iteratively modifies the embedding function to bring the source and target distributions together. Additionally, we utilise a multi-modal representation that incorporates noisy semantic information available in form of hash-tags. We show the effectiveness of this simple adaptation technique on a test set of vines and achieve notable improvements in performance. version:1
arxiv-1610-05604 | Dynamic Assortment Personalization in High Dimensions | http://arxiv.org/abs/1610.05604 | id:1610.05604 author:Nathan Kallus, Madeleine Udell category:stat.ML math.OC stat.ME  published:2016-10-18 summary:We demonstrate the importance of structural priors for effective, efficient large-scale dynamic assortment personalization. Assortment personalization is the problem of choosing, for each individual or consumer segment (type), a best assortment of products, ads, or other offerings (items) so as to maximize revenue. This problem is central to revenue management in e-commerce, online advertising, and multi-location brick-and-mortar retail, where both items and types can number in the thousands-to-millions. Data efficiency is paramount in this large-scale setting. A good personalization strategy must dynamically balance the need to learn consumer preferences and to maximize revenue. We formulate the dynamic assortment personalization problem as a discrete-contextual bandit with $m$ contexts (customer types) and many arms (assortments of the $n$ items). We assume that each type's preferences follow a simple parametric model with $n$ parameters. In all, there are $mn$ parameters, and existing literature suggests that order optimal regret scales as $mn$. However, this figure is orders of magnitude larger than the data available in large-scale applications, and imposes unacceptably high regret. In this paper, we impose natural structure on the problem -- a small latent dimension, or low rank. In the static setting, we show that this model can be efficiently learned from surprisingly few interactions, using a time- and memory-efficient optimization algorithm that converges globally whenever the model is learnable. In the dynamic setting, we show that structure-aware dynamic assortment personalization can have regret that is an order of magnitude smaller than structure-ignorant approaches. We validate our theoretical results empirically. version:1
arxiv-1610-05586 | Deep Identity-aware Transfer of Facial Attributes | http://arxiv.org/abs/1610.05586 | id:1610.05586 author:Mu Li, Wangmeng Zuo, David Zhang category:cs.CV  published:2016-10-18 summary:This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image (i.e., target image) that not only owns the reference attribute but also keep the same or similar identity to the input image. We develop a two-stage scheme to transfer the input image to each reference attribute label. A feed-forward transform network is first trained by combining perceptual identity-aware loss and GAN-based attribute loss, and a face enhancement network is then introduced to improve the visual quality. We further define perceptual identity loss on the convolutional feature maps of the attribute discriminator, resulting in a DIAT-A model. Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as expression transfer, accessory removal, age progression, and gender transfer. The experimental results validate their effectiveness. Even for some identity-related attribute (e.g., gender), our DIAT-A can obtain visually impressive results by changing the attribute while retaining most identity features of the source image. version:1
arxiv-1610-05567 | Master's Thesis : Deep Learning for Visual Recognition | http://arxiv.org/abs/1610.05567 | id:1610.05567 author:Rémi Cadène, Nicolas Thome, Matthieu Cord category:cs.CV  published:2016-10-18 summary:The goal of our research is to develop methods advancing automatic visual recognition. In order to predict the unique or multiple labels associated to an image, we study different kind of Deep Neural Networks architectures and methods for supervised features learning. We first draw up a state-of-the-art review of the Convolutional Neural Networks aiming to understand the history behind this family of statistical models, the limit of modern architectures and the novel techniques currently used to train deep CNNs. The originality of our work lies in our approach focusing on tasks with a low amount of data. We introduce different models and techniques to achieve the best accuracy on several kind of datasets, such as a medium dataset of food recipes (100k images) for building a web API, or a small dataset of satellite images (6,000) for the DSG online challenge that we've won. We also draw up the state-of-the-art in Weakly Supervised Learning, introducing different kind of CNNs able to localize regions of interest. Our last contribution is a framework, build on top of Torch7, for training and testing deep models on any visual recognition tasks and on datasets of any scale. version:1
arxiv-1610-05555 | Online Contrastive Divergence with Generative Replay: Experience Replay without Storing Data | http://arxiv.org/abs/1610.05555 | id:1610.05555 author:Decebal Constantin Mocanu, Maria Torres Vega, Eric Eaton, Peter Stone, Antonio Liotta category:cs.LG cs.NE  published:2016-10-18 summary:Conceived in the early 1990s, Experience Replay (ER) has been shown to be a successful mechanism to allow online learning algorithms to reuse past experiences. Traditionally, ER can be applied to all machine learning paradigms (i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has contributed to improving the performance of deep reinforcement learning. Yet, its application to many practical settings is still limited by the memory requirements of ER, necessary to explicitly store previous observations. To remedy this issue, we explore a novel approach, Online Contrastive Divergence with Generative Replay (OCD_GR), which uses the generative capability of Restricted Boltzmann Machines (RBMs) instead of recorded past experiences. The RBM is trained online, and does not require the system to store any of the observed data points. We compare OCD_GR to ER on 9 real-world datasets, considering a worst-case scenario (data points arriving in sorted order) as well as a more realistic one (sequential random-order data points). Our results show that in 64.28% of the cases OCD_GR outperforms ER and in the remaining 35.72% it has an almost equal performance, while having a considerably reduced space complexity (i.e., memory usage) at a comparable time complexity. version:1
arxiv-1610-05541 | M2CAI Workflow Challenge: Convolutional Neural Networks with Time Smoothing and Hidden Markov Model for Video Frames Classification | http://arxiv.org/abs/1610.05541 | id:1610.05541 author:Rémi Cadène, Thomas Robert, Nicolas Thome, Matthieu Cord category:cs.CV  published:2016-10-18 summary:Our approach is among the three best to tackle the M2CAI Workflow challenge. The latter consists in recognizing the operation phase for each frames of endoscopic videos. In this technical report, we compare several classification models and temporal smoothing methods. Our submitted solution is a fine tuned Residual Network-200 on 80% of the training set with temporal smoothing using simple temporal averaging of the predictions and a Hidden Markov Model modeling the sequence. version:1
arxiv-1610-05540 | SYSTRAN's Pure Neural Machine Translation Systems | http://arxiv.org/abs/1610.05540 | id:1610.05540 author:Josep Crego, Jungi Kim, Guillaume Klein, Anabel Rebollo, Kathy Yang, Jean Senellart, Egor Akhanov, Patrice Brunelle, Aurelien Coquard, Yongchao Deng, Satoshi Enoue, Chiyo Geiss, Joshua Johanson, Ardas Khalsa, Raoum Khiari, Byeongil Ko, Catherine Kobus, Jean Lorieux, Leidiana Martins, Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia Segal, Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou, Peter Zoldan category:cs.CL  published:2016-10-18 summary:Since the first online demonstration of Neural Machine Translation (NMT) by LISA, NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work. Our ultimate goal is to share our expertise to build competitive production systems for "generic" translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems. version:1
arxiv-1610-05522 | Addressing Community Question Answering in English and Arabic | http://arxiv.org/abs/1610.05522 | id:1610.05522 author:Giovanni Da San Martino, Alberto Barrón-Cedeño, Salvatore Romeo, Alessandro Moschitti, Shafiq Joty, Fahad A. Al Obaidli, Kateryna Tymoshenko, Antonio Uva category:cs.CL I.2.7; H.3.4  published:2016-10-18 summary:This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval-2016 Task 3 on "Community Question Answering". Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that i) the shallow structures used in our TKs are robust enough to noisy data and ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic. version:1
arxiv-1610-05518 | Shape-based defect classification for Non Destructive Testing | http://arxiv.org/abs/1610.05518 | id:1610.05518 author:Gianni D'Angelo, Salvatore Rampone category:cs.CV cs.CE  published:2016-10-18 summary:The aim of this work is to classify the aerospace structure defects detected by eddy current non-destructive testing. The proposed method is based on the assumption that the defect is bound to the reaction of the probe coil impedance during the test. Impedance plane analysis is used to extract a feature vector from the shape of the coil impedance in the complex plane, through the use of some geometric parameters. Shape recognition is tested with three different machine-learning based classifiers: decision trees, neural networks and Naive Bayes. The performance of the proposed detection system are measured in terms of accuracy, sensitivity, specificity, precision and Matthews correlation coefficient. Several experiments are performed on dataset of eddy current signal samples for aircraft structures. The obtained results demonstrate the usefulness of our approach and the competiveness against existing descriptors. version:1
arxiv-1610-05507 | Analysis and Implementation of an Asynchronous Optimization Algorithm for the Parameter Server | http://arxiv.org/abs/1610.05507 | id:1610.05507 author:Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson category:math.OC cs.DC cs.LG stat.ML  published:2016-10-18 summary:This paper presents an asynchronous incremental aggregated gradient algorithm and its implementation in a parameter server framework for solving regularized optimization problems. The algorithm can handle both general convex (possibly non-smooth) regularizers and general convex constraints. When the empirical data loss is strongly convex, we establish linear convergence rate, give explicit expressions for step-size choices that guarantee convergence to the optimum, and bound the associated convergence factors. The expressions have an explicit dependence on the degree of asynchrony and recover classical results under synchronous operation. Simulations and implementations on commercial compute clouds validate our findings. version:1
arxiv-1610-03708 | Generating captions without looking beyond objects | http://arxiv.org/abs/1610.03708 | id:1610.03708 author:Hendrik Heuer, Christof Monz, Arnold W. M. Smeulders category:cs.CV cs.CL  published:2016-10-12 summary:This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on n-gram precision. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions. version:2
arxiv-1610-05083 | Efficient Metric Learning for the Analysis of Motion Data | http://arxiv.org/abs/1610.05083 | id:1610.05083 author:Babak Hosseini, Barbara Hammer category:cs.LG stat.ML  published:2016-10-17 summary:We investigate metric learning in the context of dynamic time warping (DTW), the by far most popular dissimilarity measure used for the comparison and analysis of motion capture data. While metric learning enables a problem-adapted representation of data, the majority of meth- ods has been proposed for vectorial data only. In this contribution, we extend the popular principle offered by the large margin nearest neighbours learner (LMNN) to DTW by treating the resulting component-wise dissimilarity values as features. We demonstrate, that this principle greatly enhances the classification accuracy in several benchmarks. Further, we show that recent auxiliary concepts such as metric regularisation can be transferred from the vectorial case to component-wise DTW in a similar way. We illustrate, that metric regularisation constitutes a crucial prerequisite for the interpretation of the resulting relevance profiles. version:2
arxiv-1610-05492 | Federated Learning: Strategies for Improving Communication Efficiency | http://arxiv.org/abs/1610.05492 | id:1610.05492 author:Jakub Konečný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh, Dave Bacon category:cs.LG  published:2016-10-18 summary:Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude. version:1
arxiv-1610-04997 | Spatio-Temporal Attention Models for Grounded Video Captioning | http://arxiv.org/abs/1610.04997 | id:1610.04997 author:Mihai Zanfir, Elisabeta Marinoiu, Cristian Sminchisescu category:cs.CV  published:2016-10-17 summary:Automatic video captioning is challenging due to the complex interactions in dynamic real scenes. A comprehensive system would ultimately localize and track the objects, actions and interactions present in a video and generate a description that relies on temporal localization in order to ground the visual concepts. However, most existing automatic video captioning systems map from raw video data to high level textual description, bypassing localization and recognition, thus discarding potentially valuable information for content localization and generalization. In this work we present an automatic video captioning model that combines spatio-temporal attention and image classification by means of deep neural network structures based on long short-term memory. The resulting system is demonstrated to produce state-of-the-art results in the standard YouTube captioning benchmark while also offering the advantage of localizing the visual concepts (subjects, verbs, objects), with no grounding supervision, over space and time. version:2
arxiv-1610-05465 | Real-time analysis of cataract surgery videos using statistical models | http://arxiv.org/abs/1610.05465 | id:1610.05465 author:Katia Charrière, Gwenolé Quellec, Mathieu Lamard, David Martiano, Guy Cazuguel, Gouenou Coatrieux, Béatrice Cochener category:cs.CV  published:2016-10-18 summary:The automatic analysis of the surgical process, from videos recorded during surgeries, could be very useful to surgeons, both for training and for acquiring new techniques. The training process could be optimized by automatically providing some targeted recommendations or warnings, similar to the expert surgeon's guidance. In this paper, we propose to reuse videos recorded and stored during cataract surgeries to perform the analysis. The proposed system allows to automatically recognize, in real time, what the surgeon is doing: what surgical phase or, more precisely, what surgical step he or she is performing. This recognition relies on the inference of a multilevel statistical model which uses 1) the conditional relations between levels of description (steps and phases) and 2) the temporal relations among steps and among phases. The model accepts two types of inputs: 1) the presence of surgical tools, manually provided by the surgeons, or 2) motion in videos, automatically analyzed through the Content Based Video retrieval (CBVR) paradigm. Different data-driven statistical models are evaluated in this paper. For this project, a dataset of 30 cataract surgery videos was collected at Brest University hospital. The system was evaluated in terms of area under the ROC curve. Promising results were obtained using either the presence of surgical tools ($A_z$ = 0.983) or motion analysis ($A_z$ = 0.759). The generality of the method allows to adapt it to any kinds of surgeries. The proposed solution could be used in a computer assisted surgery tool to support surgeons during the surgery. version:1
arxiv-1610-05463 | An Interactive Machine Learning Framework | http://arxiv.org/abs/1610.05463 | id:1610.05463 author:Teng Lee, James Johnson, Steve Cheng category:cs.HC cs.LG  published:2016-10-18 summary:Machine learning (ML) is believed to be an effective and efficient tool to build reliable prediction model or extract useful structure from an avalanche of data. However, ML is also criticized by its difficulty in interpretation and complicated parameter tuning. In contrast, visualization is able to well organize and visually encode the entangled information in data and guild audiences to simpler perceptual inferences and analytic thinking. But large scale and high dimensional data will usually lead to the failure of many visualization methods. In this paper, we close a loop between ML and visualization via interaction between ML algorithm and users, so machine intelligence and human intelligence can cooperate and improve each other in a mutually rewarding way. In particular, we propose "transparent boosting tree (TBT)", which visualizes both the model structure and prediction statistics of each step in the learning process of gradient boosting tree to user, and involves user's feedback operations to trees into the learning process. In TBT, ML is in charge of updating weights in learning model and filtering information shown to user from the big data, while visualization is in charge of providing a visual understanding of ML model to facilitate user exploration. It combines the advantages of both ML in big data statistics and human in decision making based on domain knowledge. We develop a user friendly interface for this novel learning method, and apply it to two datasets collected from real applications. Our study shows that making ML transparent by using interactive visualization can significantly improve the exploration of ML algorithms, give rise to novel insights of ML models, and integrates both machine and human intelligence. version:1
arxiv-1610-05461 | Personalized Machine Translation: Preserving Original Author Traits | http://arxiv.org/abs/1610.05461 | id:1610.05461 author:Ella Rabinovich, Shachar Mirkin, Raj Nath Patel, Lucia Specia, Shuly Wintner category:cs.CL  published:2016-10-18 summary:The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait, gender, and study how it is manifested in original texts and in translations. We show that gender has a powerful, clear signal in originals, but this signal is obfuscated in human and machine translation. We then propose simple domain-adaptation techniques that help retain the original gender traits in the translation outcome, without harming the quality of the translation, thereby creating more personalized machine translation systems. version:1
arxiv-1610-05448 | Generalization error minimization: a new approach to model evaluation and selection with an application to penalized regression | http://arxiv.org/abs/1610.05448 | id:1610.05448 author:Ning Xu, Jian Hong, Timothy C. G. Fisher category:stat.ML math.ST q-fin.EC stat.TH  published:2016-10-18 summary:We study model evaluation and model selection from the perspective of generalization ability (GA): the ability of a model to predict outcomes in new samples from the same population. We believe that GA is one way formally to address concerns about the external validity of a model. The GA of a model estimated on a sample can be measured by its empirical out-of-sample errors, called the generalization errors (GE). We derive upper bounds for the GE, which depend on sample sizes, model complexity and the distribution of the loss function. The upper bounds can be used to evaluate the GA of a model, ex ante. We propose using generalization error minimization (GEM) as a framework for model selection. Using GEM, we are able to unify a big class of penalized regression estimators, including lasso, ridge and bridge, under the same set of assumptions. We establish finite-sample and asymptotic properties (including $\mathcal{L}_2$-consistency) of the GEM estimator for both the $n \geqslant p$ and the $n < p$ cases. We also derive the $\mathcal{L}_2$-distance between the penalized and corresponding unpenalized regression estimates. In practice, GEM can be implemented by validation or cross-validation. We show that the GE bounds can be used for selecting the optimal number of folds in $K$-fold cross-validation. We propose a variant of $R^2$, the $GR^2$, as a measure of GA, which considers both both in-sample and out-of-sample goodness of fit. Simulations are used to demonstrate our key results. version:1
arxiv-1610-05432 | ARTiS: Appearance-based Action Recognition in Task Space | http://arxiv.org/abs/1610.05432 | id:1610.05432 author:Markus Eich, Sareh Shirazi, Gordon Wyeth category:cs.RO cs.CV  published:2016-10-18 summary:To have a robot actively supporting a human during a collaborative task, it is crucial that robots are able to identify the current action in order to predict the next one. Common approaches make use of high-level knowledge, such as object affordances, semantics or understanding of actions in terms of pre- and post-conditions. These approaches often require hand-coded a priori knowledge, time- and resource- intensive or supervised learning techniques. We propose to reframe this problem as an appearance- based place recognition problem. In our framework, we regard sequences of visual images of human actions as a map in analogy to the visual place recognition problem. Observing the task for the second time, our approach is able to recognize pre-observed actions in a one-shot learning approach and is thereby able to recognize the current observation in the task space. We propose two new methods for creating and aligning action observations within a task map. We compare and verify our approaches with real data of humans assembling an IKEA flat pack drawer. version:1
arxiv-1610-05424 | Modern WLAN Fingerprinting Indoor Positioning Methods and Deployment Challenges | http://arxiv.org/abs/1610.05424 | id:1610.05424 author:Ali Khalajmehrabadi, Nikolaos Gatsis, David Akopian category:cs.NI cs.LG  published:2016-10-18 summary:Wireless Local Area Network (WLAN) has become a promising choice for indoor positioning as the only existing and established infrastructure, to localize the mobile and stationary users indoors. However, since WLAN has been initially designed for wireless networking and not positioning, the localization task based on WLAN signals has several challenges. Amongst the WLAN positioning methods, WLAN fingerprinting localization has recently achieved great attention due to its promising results. WLAN fingerprinting faces several challenges and hence, in this paper, our goal is to overview these challenges and the state-of-the-art solutions. This paper consists of three main parts: 1) Conventional localization schemes; 2) State-of-the-art approaches; 3) Practical deployment challenges. Since all the proposed methods in WLAN literature have been conducted and tested in different settings, the reported results are not equally comparable. So, we compare some of the main localization schemes in a single real environment and assess their localization accuracy, positioning error statistics, and complexity. Our results depict illustrative evaluation of WLAN localization systems and guide to future improvement opportunities. version:1
arxiv-1610-05421 | Structured Group Sparsity: A Novel Indoor WLAN Localization, Outlier Detection, and Radio Map Interpolation Scheme | http://arxiv.org/abs/1610.05421 | id:1610.05421 author:Ali Khalajmehrabadi, Nikolaos Gatsis, David Akopian category:cs.NI cs.LG  published:2016-10-18 summary:This paper introduces novel schemes for indoor localization, outlier detection, and radio map interpolation using Wireless Local Area Networks (WLANs). The localization method consists of a novel multicomponent optimization technique that minimizes the squared $\ell_{2}$-norm of the residuals between the radio map and the online Received Signal Strength (RSS) measurements, the $\ell_{1}$-norm of the user's location vector, and weighted $\ell_{2}$-norms of layered groups of Reference Points (RPs). RPs are grouped using a new criterion based on the similarity between the so-called Access Point (AP) coverage vectors. In addition, since AP readings are prone to containing inordinate readings, called outliers, an augmented optimization problem is proposed to detect the outliers and localize the user with cleaned online measurements. Moreover, a novel scheme to record fingerprints from a smaller number of RPs and estimate the radio map at RPs without recorded fingerprints is developed using sparse recovery techniques. All localization schemes are tested on RSS fingerprints collected from a real environment. The overall scheme has comparable complexity with competing approaches, while it performs with high accuracy under a small number of APs and finer granularity of RPs. version:1
arxiv-1610-05419 | A Joint Indoor WLAN Localization and Outlier Detection Scheme Using LASSO and Elastic-Net Optimization Techniques | http://arxiv.org/abs/1610.05419 | id:1610.05419 author:Ali Khalajmehrabadi, Nikolaos Gatsis, Daniel Pack, David Akopian category:cs.NI cs.LG  published:2016-10-18 summary:In this paper, we introduce two indoor Wireless Local Area Network (WLAN) positioning methods using augmented sparse recovery algorithms. These schemes render a sparse user's position vector, and in parallel, minimize the distance between the online measurement and radio map. The overall localization scheme for both methods consists of three steps: 1) coarse localization, obtained from comparing the online measurements with clustered radio map. A novel graph-based method is proposed to cluster the offline fingerprints. In the online phase, a Region Of Interest (ROI) is selected within which we search for the user's location; 2) Access Point (AP) selection; and 3) fine localization through the novel sparse recovery algorithms. Since the online measurements are subject to inordinate measurement readings, called outliers, the sparse recovery methods are modified in order to jointly estimate the outliers and user's position vector. The outlier detection procedure identifies the APs whose readings are either not available or erroneous. The proposed localization methods have been tested with Received Signal Strength (RSS) measurements in a typical office environment and the results show that they can localize the user with significantly high accuracy and resolution which is superior to the results from competing WLAN fingerprinting localization methods. version:1
arxiv-1610-05394 | Sequential Learning without Feedback | http://arxiv.org/abs/1610.05394 | id:1610.05394 author:Manjesh Hanawal, Csaba Szepesvari, Venkatesh Saligrama category:cs.LG  published:2016-10-18 summary:In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to {\it learn} strategies for selecting tests to optimize accuracy \& costs. Unfortunately it is often impossible to acquire in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an {\it unusual} reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well. We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret. version:1
arxiv-1610-04662 | Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images | http://arxiv.org/abs/1610.04662 | id:1610.04662 author:Noel Codella, Quoc-Bao Nguyen, Sharath Pankanti, David Gutman, Brian Helba, Allan Halpern, John R. Smith category:cs.CV  published:2016-10-14 summary:Melanoma is the deadliest form of skin cancer. While curable with early detection, only highly trained specialists are capable of accurately recognizing the disease. As expertise is in limited supply, automated systems capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as well as analyzing the detected area and surrounding tissue for melanoma detection. The system is evaluated using the largest publicly available benchmark dataset of dermoscopic images, containing 900 training and 379 testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic curve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624), and in specificity measured at the clinically relevant 95% sensitivity operating point 2.9 times higher than the previous state-of-the-art (36.8% specificity compared to 12.5%). Compared to the average of 8 expert dermatologists on a subset of 100 test images, the proposed system produces a higher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an equivalent sensitivity (82%). version:2
arxiv-1610-05361 | End-to-end attention-based distant speech recognition with Highway LSTM | http://arxiv.org/abs/1610.05361 | id:1610.05361 author:Hassan Taherian category:cs.CL  published:2016-10-17 summary:End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attention-based models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level. To gain a better performance, we also incorporate Highway long short-term memory (HLSTM) which outperforms previous models on AMI distant speech recognition task. version:1
arxiv-1610-05350 | How Well Do Local Algorithms Solve Semidefinite Programs? | http://arxiv.org/abs/1610.05350 | id:1610.05350 author:Zhou Fan, Andrea Montanari category:cs.DM math.OC stat.ML  published:2016-10-17 summary:Several probabilistic models from high-dimensional statistics and machine learning reveal an intriguing --and yet poorly understood-- dichotomy. Either simple local algorithms succeed in estimating the object of interest, or even sophisticated semi-definite programming (SDP) relaxations fail. In order to explore this phenomenon, we study a classical SDP relaxation of the minimum graph bisection problem, when applied to Erd\H{o}s-Renyi random graphs with bounded average degree $d>1$, and obtain several types of results. First, we use a dual witness construction (using the so-called non-backtracking matrix of the graph) to upper bound the SDP value. Second, we prove that a simple local algorithm approximately solves the SDP to within a factor $2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at most $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree. We then analyze a more sophisticated local algorithm, which aggregates information according to the harmonic measure on the limiting Galton-Watson (GW) tree. The resulting lower bound is expressed in terms of the conductance of the GW tree and matches surprisingly well the empirically determined SDP values on large-scale Erd\H{o}s-Renyi graphs. We finally consider the planted partition model. In this case, purely local algorithms are known to fail, but they do succeed if a small amount of side information is available. Our results imply quantitative bounds on the threshold for partial recovery using SDP in this model. version:1
arxiv-1610-03035 | Latent Sequence Decompositions | http://arxiv.org/abs/1610.03035 | id:1610.03035 author:William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly category:stat.ML cs.CL cs.LG  published:2016-10-10 summary:We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.2% WER. version:2
arxiv-1610-05275 | A Unified Computational and Statistical Framework for Nonconvex Low-Rank Matrix Estimation | http://arxiv.org/abs/1610.05275 | id:1610.05275 author:Lingxiao Wang, Xiao Zhang, Quanquan Gu category:stat.ML  published:2016-10-17 summary:We propose a unified framework for estimating low-rank matrices through nonconvex optimization based on gradient descent algorithm. Our framework is quite general and can be applied to both noisy and noiseless observations. In the general case with noisy observations, we show that our algorithm is guaranteed to linearly converge to the unknown low-rank matrix up to minimax optimal statistical error, provided an appropriate initial estimator. While in the generic noiseless setting, our algorithm converges to the unknown low-rank matrix at a linear rate and enables exact recovery with optimal sample complexity. In addition, we develop a new initialization algorithm to provide a desired initial estimator, which outperforms existing initialization algorithms for nonconvex low-rank matrix estimation. We illustrate the superiority of our framework through three examples: matrix regression, matrix completion, and one-bit matrix completion. We also corroborate our theory through extensive experiments on synthetic data. version:1
arxiv-1610-05261 | A probabilistic model for the numerical solution of initial value problems | http://arxiv.org/abs/1610.05261 | id:1610.05261 author:Michael Schober, Simo Särkkä, Philipp Hennig category:math.NA cs.LG stat.ML  published:2016-10-17 summary:Like many numerical methods, solvers for initial value problems (IVPs) on ordinary differential equations estimate an analytically intractable quantity, using the results of tractable computations as inputs. This structure is closely connected to the notion of inference on latent variables in statistics. We describe a class of algorithms that formulate the solution to an IVP as inference on a latent path that is a draw from a Gaussian process probability measure (or equivalently, the solution of a linear stochastic differential equation). We then show that certain members of this class are identified exactly with existing generalized linear methods for ODEs, in particular a number of Runge--Kutta methods and Nordsieck methods. This probabilistic formulation of classic methods is valuable in two ways: analytically, it highlights implicit prior assumptions favoring certain approximate solutions to the IVP over others, and gives a precise meaning to the old observation that these methods act like filters. Practically, it endows the classic solvers with `docking points' for notions of uncertainty and prior information about the initial value, the value of the ODE itself, and the solution of the problem. version:1
arxiv-1610-05256 | Achieving Human Parity in Conversational Speech Recognition | http://arxiv.org/abs/1610.05256 | id:1610.05256 author:W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig category:cs.CL  published:2016-10-17 summary:Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training. version:1
arxiv-1610-05247 | Black-box Importance Sampling | http://arxiv.org/abs/1610.05247 | id:1610.05247 author:Qiang Liu, Jason D. Lee category:stat.ML  published:2016-10-17 summary:Importance sampling is widely used in machine learning and statistics, but its power is limited by the restriction of using simple proposals for which the importance weights can be tractably calculated. We address this problem by studying black-box importance sampling methods that calculate importance weights for samples generated from any unknown proposal or black-box mechanism. Our method allows us to use better and richer proposals to solve difficult problems, and (somewhat counter-intuitively) also has the additional benefit of improving the estimation accuracy beyond typical importance sampling. Both theoretical and empirical analyses are provided. version:1
arxiv-1610-05246 | BET on Independence | http://arxiv.org/abs/1610.05246 | id:1610.05246 author:Kai Zhang category:math.ST cs.LG stat.CO stat.ME stat.ML stat.TH  published:2016-10-17 summary:We study the problem of model-free dependence detection. This problem can be difficult even when the marginal distributions are known. We explain this difficulty by showing the impossibility to uniformly consistently distinguish degeneracy from independence with any single test. To make model-free dependence detection a tractable problem, we introduce the concept of binary expansion statistics (BEStat) and propose the binary expansion testing (BET) framework. Through simple mathematics, we convert the dependence detection problem to a multiple testing problem. Besides being model-free, the BET also enjoys many other advantages which include (1) invariance to monotone marginal transformations, (2) clear interpretability of local relationships upon rejection, and (3) close connections to computing for efficient algorithms. We illustrate the BET by studying the distribution of the brightest stars in the night sky. version:1
arxiv-1610-05243 | Pre-Translation for Neural Machine Translation | http://arxiv.org/abs/1610.05243 | id:1610.05243 author:Jan Niehues, Eunah Cho, Thanh-Le Ha, Alex Waibel category:cs.CL  published:2016-10-17 summary:Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result. version:1
arxiv-1610-03761 | Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble of Autoencoders | http://arxiv.org/abs/1610.03761 | id:1610.03761 author:Shehroz S. Khan, Babak Taati category:cs.CV cs.LG stat.ML  published:2016-10-12 summary:A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatically detect falls is the choice of features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. We show that choosing a threshold as maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. We propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls. We present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods. version:2
arxiv-1610-05231 | Evolving the Structure of Evolution Strategies | http://arxiv.org/abs/1610.05231 | id:1610.05231 author:Sander van Rijn, Hao Wang, Matthijs van Leeuwen, Thomas Bäck category:cs.NE  published:2016-10-17 summary:Various variants of the well known Covariance Matrix Adaptation Evolution Strategy (CMA-ES) have been proposed recently, which improve the empirical performance of the original algorithm by structural modifications. However, in practice it is often unclear which variation is best suited to the specific optimization problem at hand. As one approach to tackle this issue, algorithmic mechanisms attached to CMA-ES variants are considered and extracted as functional \emph{modules}, allowing for combinations of them. This leads to a configuration space over ES structures, which enables the exploration of algorithm structures and paves the way toward novel algorithm generation. Specifically, eleven modules are incorporated in this framework with two or three alternative configurations for each module, resulting in $4\,608$ algorithms. A self-adaptive Genetic Algorithm (GA) is used to efficiently evolve effective ES-structures for given classes of optimization problems, outperforming any classical CMA-ES variants from literature. The proposed approach is evaluated on noiseless functions from BBOB suite. Furthermore, such an observation is again confirmed on different function groups and dimensionality, indicating the feasibility of ES configuration on real-world problem classes. version:1
arxiv-1610-05211 | Structured Sparse Subspace Clustering: A Joint Affinity Learning and Subspace Clustering Framework | http://arxiv.org/abs/1610.05211 | id:1610.05211 author:Chun-Guang Li, Chong You, René Vidal category:cs.CV  published:2016-10-17 summary:Subspace clustering refers to the problem of segmenting data drawn from a union of subspaces. State of the art approaches for solving this problem follow a two-stage approach. In the first step, an affinity matrix is learned from the data using sparse or low-rank minimization techniques. In the second step, the segmentation is found by applying spectral clustering to this affinity. While this approach has led to state-of-the-art results in many applications, it is sub-optimal because it does not exploit the fact that the affinity and the segmentation depend on each other. In this paper, we propose a joint optimization framework --- Structured Sparse Subspace Clustering (S$^3$C) --- for learning both the affinity and the segmentation. The proposed S$^3$C framework is based on expressing each data point as a structured sparse linear combination of all other data points, where the structure is induced by a norm that depends on the unknown segmentation. Moreover, we extend the proposed S$^3$C framework into Constrained Structured Sparse Subspace Clustering (CS$^3$C) in which available partial side-information is incorporated into the stage of learning the affinity. We show that both the structured sparse representation and the segmentation can be found via a combination of an alternating direction method of multipliers with spectral clustering. Experiments on a synthetic data set, the Extended Yale B data set, the Hopkins 155 motion segmentation database, and three cancer data sets demonstrate the effectiveness of our approach. version:1
arxiv-1610-05202 | Decentralized Collaborative Learning of Personalized Models over Networks | http://arxiv.org/abs/1610.05202 | id:1610.05202 author:Paul Vanhaesebrouck, Aurélien Bellet, Marc Tommasi category:cs.LG cs.DC cs.SY stat.ML  published:2016-10-17 summary:We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. Our algorithm to optimize this challenging objective in a decentralized way is based on ADMM. version:1
arxiv-1610-05163 | Spatio-temporal Gaussian processes modeling of dynamical systems in systems biology | http://arxiv.org/abs/1610.05163 | id:1610.05163 author:Mu Niu, Zhenwen Dai, Neil Lawrence, Kolja Becker category:stat.ML  published:2016-10-17 summary:Quantitative modeling of post-transcriptional regulation process is a challenging problem in systems biology. A mechanical model of the regulatory process needs to be able to describe the available spatio-temporal protein concentration and mRNA expression data and recover the continuous spatio-temporal fields. Rigorous methods are required to identify model parameters. A promising approach to deal with these difficulties is proposed using Gaussian process as a prior distribution over the latent function of protein concentration and mRNA expression. In this study, we consider a partial differential equation mechanical model with differential operators and latent function. Since the operators at stake are linear, the information from the physical model can be encoded into the kernel function. Hybrid Monte Carlo methods are employed to carry out Bayesian inference of the partial differential equation parameters and Gaussian process kernel parameters. The spatio-temporal field of protein concentration and mRNA expression are reconstructed without explicitly solving the partial differential equation. version:1
arxiv-1610-05160 | The Peaking Phenomenon in Semi-supervised Learning | http://arxiv.org/abs/1610.05160 | id:1610.05160 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2016-10-17 summary:For the supervised least squares classifier, when the number of training objects is smaller than the dimensionality of the data, adding more data to the training set may first increase the error rate before decreasing it. This, possibly counterintuitive, phenomenon is known as peaking. In this work, we observe that a similar but more pronounced version of this phenomenon also occurs in the semi-supervised setting, where instead of labeled objects, unlabeled objects are added to the training set. We explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by Raudys & Duin. version:1
arxiv-1610-05150 | Neural Machine Translation Advised by Statistical Machine Translation | http://arxiv.org/abs/1610.05150 | id:1610.05150 author:Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang Li, Deyi Xiong, Min Zhang category:cs.CL  published:2016-10-17 summary:Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016; He et al. 2016). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of the-art NMT and SMT systems on multiple NIST test sets. version:1
arxiv-1610-05129 | Risk-Aware Algorithms for Adversarial Contextual Bandits | http://arxiv.org/abs/1610.05129 | id:1610.05129 author:Wen Sun, Debadeepta Dey, Ashish Kapoor category:cs.LG stat.ML  published:2016-10-17 summary:In this work we consider adversarial contextual bandits with risk constraints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and then receives the corresponding cost and risk associated with the pulled arm. In addition to minimizing the cumulative cost, the learner also needs to satisfy long-term risk constraints -- the average of the cumulative risk from all pulled arms should not be larger than a pre-defined threshold. To address this problem, we first study the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develop a meta algorithm leveraging online mirror descent for the full information setting and extend it to contextual bandit with risk constraints setting using expert advice. Our algorithms can achieve near-optimal regret in terms of minimizing the total cost, while successfully maintaining a sublinear growth of cumulative risk constraint violation. version:1
arxiv-1610-02891 | Personalizing a Dialogue System with Transfer Learning | http://arxiv.org/abs/1610.02891 | id:1610.02891 author:Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, Qiang Yang category:cs.AI cs.CL cs.LG  published:2016-10-10 summary:It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose "PETAL"(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting. version:2
arxiv-1610-05120 | Lazifying Conditional Gradient Algorithms | http://arxiv.org/abs/1610.05120 | id:1610.05120 author:Gábor Braun, Sebastian Pokutta, Daniel Zink category:cs.DS cs.LG 68Q32 68W27 90C52  published:2016-10-17 summary:Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls. version:1
arxiv-1610-02906 | A General Framework for Content-enhanced Network Representation Learning | http://arxiv.org/abs/1610.02906 | id:1610.02906 author:Xiaofei Sun, Jiang Guo, Xiao Ding, Ting Liu category:cs.SI cs.CL cs.LG  published:2016-10-10 summary:This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning. version:3
arxiv-1610-05047 | Deep Learning Prototype Domains for Person Re-Identification | http://arxiv.org/abs/1610.05047 | id:1610.05047 author:Arne Schumann, Shaogang Gong, Tobias Schuchert category:cs.CV  published:2016-10-17 summary:Person re-identification (re-id) is the task of matching multiple occurrences of the same person from different cameras, poses, lighting conditions, and a multitude of other factors which alter the visual appearance. Typically, this is achieved by learning either optimal features or matching metrics which are adapted to specific pairs of camera views dictated by the pairwise labelled training datasets. In this work, we formulate a deep learning based novel approach to automatic prototype-domain discovery for domain perceptive (adaptive) person re-id (rather than camera pair specific learning) for any camera views scalable to new unseen scenes without training data. We learn a separate re-id model for each of the discovered prototype-domains and during model deployment, use the person probe image to select automatically the model of the closest prototype domain. Our approach requires neither supervised nor unsupervised domain adaptation learning, i.e. no data available from the target domains. We evaluate extensively our model under realistic re-id conditions using automatically detected bounding boxes with low-resolution and partial occlusion. We show that our approach outperforms most of the state-of-the-art supervised and unsupervised methods on the latest CUHK-SYSU and PRW benchmarks. version:1
arxiv-1610-05036 | Encoding the Local Connectivity Patterns of fMRI for Cognitive State Classification | http://arxiv.org/abs/1610.05036 | id:1610.05036 author:Itir Onal Ertugrul, Mete Ozay, Fatos T. Yarman Vural category:cs.CV cs.LG  published:2016-10-17 summary:In this work, we propose a novel framework to encode the local connectivity patterns of brain, using Fisher Vectors (FV), Vector of Locally Aggregated Descriptors (VLAD) and Bag-of-Words (BoW) methods. We first obtain local descriptors, called Mesh Arc Descriptors (MADs) from fMRI data, by forming local meshes around anatomical regions, and estimating their relationship within a neighborhood. Then, we extract a dictionary of relationships, called \textit{brain connectivity dictionary} by fitting a generative Gaussian mixture model (GMM) to a set of MADs, and selecting the codewords at the mean of each component of the mixture. Codewords represent the connectivity patterns among anatomical regions. We also encode MADs by VLAD and BoW methods using the k-Means clustering. We classify the cognitive states of Human Connectome Project (HCP) task fMRI dataset, where we train support vector machines (SVM) by the encoded MADs. Results demonstrate that, FV encoding of MADs can be successfully employed for classification of cognitive tasks, and outperform the VLAD and BoW representations. Moreover, we identify the significant Gaussians in mixture models by computing energy of their corresponding FV parts, and analyze their effect on classification accuracy. Finally, we suggest a new method to visualize the codewords of brain connectivity dictionary. version:1
arxiv-1610-05016 | Weekly maintenance scheduling using exact and genetic methods | http://arxiv.org/abs/1610.05016 | id:1610.05016 author:Andrew W. Palmer, Robin Vujanic, Andrew J. Hill, Steven J. Scheding category:cs.NE cs.AI  published:2016-10-17 summary:The weekly maintenance schedule specifies when maintenance activities should be performed on the equipment, taking into account the availability of workers and maintenance bays, and other operational constraints. The current approach to generating this schedule is labour intensive and requires coordination between the maintenance schedulers and operations staff to minimise its impact on the operation of the mine. This paper presents methods for automatically generating this schedule from the list of maintenance tasks to be performed, the availability roster of the maintenance staff, and time windows in which each piece of equipment is available for maintenance. Both Mixed-Integer Linear Programming (MILP) and genetic algorithms are evaluated, with the genetic algorithm shown to significantly outperform the MILP. Two fitness functions for the genetic algorithm are also examined, with a linear fitness function outperforming an inverse fitness function by up to 5% for the same calculation time. The genetic algorithm approach is computationally fast, allowing the schedule to be rapidly recalculated in response to unexpected delays and breakdowns. version:1
arxiv-1610-05011 | Interactive Attention for Neural Machine Translation | http://arxiv.org/abs/1610.05011 | id:1610.05011 author:Fandong Meng, Zhengdong Lu, Hang Li, Qun Liu category:cs.CL  published:2016-10-17 summary:Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. version:1
arxiv-1610-05009 | Wind ramp event prediction with parallelized Gradient Boosted Regression Trees | http://arxiv.org/abs/1610.05009 | id:1610.05009 author:Saurav Gupta, Nitin Anand Shrivastava, Abbas Khosravi, Bijaya Ketan Panigrahi category:cs.LG cs.AI  published:2016-10-17 summary:Accurate prediction of wind ramp events is critical for ensuring the reliability and stability of the power systems with high penetration of wind energy. This paper proposes a classification based approach for estimating the future class of wind ramp event based on certain thresholds. A parallelized gradient boosted regression tree based technique has been proposed to accurately classify the normal as well as rare extreme wind power ramp events. The model has been validated using wind power data obtained from the National Renewable Energy Laboratory database. Performance comparison with several benchmark techniques indicates the superiority of the proposed technique in terms of superior classification accuracy. version:1
arxiv-1610-04989 | Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification | http://arxiv.org/abs/1610.04989 | id:1610.04989 author:Jiacheng Xu, Danlu Chen, Xipeng Qiu, Xuangjing Huang category:cs.CL cs.NE  published:2016-10-17 summary:Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets. version:1
arxiv-1610-04973 | Multiple Instance Fuzzy Inference Neural Networks | http://arxiv.org/abs/1610.04973 | id:1610.04973 author:Amine Ben Khalifa, Hichem Frigui category:cs.NE cs.CV cs.SY  published:2016-10-17 summary:Fuzzy logic is a powerful tool to model knowledge uncertainty, measurements imprecision, and vagueness. However, there is another type of vagueness that arises when data have multiple forms of expression that fuzzy logic does not address quite well. This is the case for multiple instance learning problems (MIL). In MIL, an object is represented by a collection of instances, called a bag. A bag is labeled negative if all of its instances are negative, and positive if at least one of its instances is positive. Positive bags encode ambiguity since the instances themselves are not labeled. In this paper, we introduce fuzzy inference systems and neural networks designed to handle bags of instances as input and capable of learning from ambiguously labeled data. First, we introduce the Multiple Instance Sugeno style fuzzy inference (MI-Sugeno) that extends the standard Sugeno style inference to handle reasoning with multiple instances. Second, we use MI-Sugeno to define and develop Multiple Instance Adaptive Neuro Fuzzy Inference System (MI-ANFIS). We expand the architecture of the standard ANFIS to allow reasoning with bags and derive a learning algorithm using backpropagation to identify the premise and consequent parameters of the network. The proposed inference system is tested and validated using synthetic and benchmark datasets suitable for MIL problems. We also apply the proposed MI-ANFIS to fuse the output of multiple discrimination algorithms for the purpose of landmine detection using Ground Penetrating Radar. version:1
arxiv-1610-04957 | What is the Best Way for Extracting Meaningful Attributes from Pictures? | http://arxiv.org/abs/1610.04957 | id:1610.04957 author:Liangchen Liu, Arnold Wiliem, Shaokang Chen, Brian C. Lovell category:cs.CV  published:2016-10-17 summary:Automatic attribute discovery methods have gained in popularity to extract sets of visual attributes from images or videos for various tasks. Despite their good performance in some classification tasks, it is difficult to evaluate whether the attributes discovered by these methods are meaningful and which methods are the most appropriate to discover attributes for visual descriptions. In its simplest form, such an evaluation can be performed by manually verifying whether there is any consistent identifiable visual concept distinguishing between positive and negative exemplars labelled by an attribute. This manual checking is tedious, expensive and labour intensive. In addition, comparisons between different methods could also be problematic as it is not clear how one could quantitatively decide which attribute is more meaningful than the others. In this paper, we propose a novel attribute meaningfulness metric to address this challenging problem. With this metric, automatic quantitative evaluation can be performed on the attribute sets; thus, reducing the enormous effort to perform manual evaluation. The proposed metric is applied to some recent automatic attribute discovery and hashing methods on four attribute-labelled datasets. To further validate the efficacy of the proposed method, we conducted a user study. In addition, we also compared our metric with a semi-supervised attribute discover method using the mixture of probabilistic PCA. In our evaluation, we gleaned several insights that could be beneficial in developing new automatic attribute discovery methods. version:1
arxiv-1610-04936 | Partial Procedural Geometric Model Fitting for Point Clouds | http://arxiv.org/abs/1610.04936 | id:1610.04936 author:Zongliang Zhang, Jonathan Li, Yulan Guo, Yangbin Lin, Ming Cheng, Cheng Wang category:cs.GR cs.CV  published:2016-10-17 summary:Geometric model fitting is a fundamental task in computer graphics and computer vision. However, most geometric model fitting methods are unable to fit an arbitrary geometric model (e.g. a surface with holes) to incomplete data, due to that the similarity metrics used in these methods are unable to measure the rigid partial similarity between arbitrary models. This paper hence proposes a novel rigid geometric similarity metric, which is able to measure both the full similarity and the partial similarity between arbitrary geometric models. The proposed metric enables us to perform partial procedural geometric model fitting (PPGMF). The task of PPGMF is to search a procedural geometric model space for the model rigidly similar to a query of non-complete point set. Models in the procedural model space are generated according to a set of parametric modeling rules. A typical query is a point cloud. PPGMF is very useful as it can be used to fit arbitrary geometric models to non-complete (incomplete, over-complete or hybrid-complete) point cloud data. For example, most laser scanning data is non-complete due to occlusion. Our PPGMF method uses Markov chain Monte Carlo technique to optimize the proposed similarity metric over the model space. To accelerate the optimization process, the method also employs a novel coarse-to-fine model dividing strategy to reject dissimilar models in advance. Our method has been demonstrated on a variety of geometric models and non-complete data. Experimental results show that the PPGMF method based on the proposed metric is able to fit non-complete data, while the method based on other metrics is unable. It is also shown that our method can be accelerated by several times via early rejection. version:1
arxiv-1610-04929 | Probabilistic Dimensionality Reduction via Structure Learning | http://arxiv.org/abs/1610.04929 | id:1610.04929 author:Li Wang category:stat.ML cs.LG  published:2016-10-16 summary:We propose a novel probabilistic dimensionality reduction framework that can naturally integrate the generative model and the locality information of data. Based on this framework, we present a new model, which is able to learn a smooth skeleton of embedding points in a low-dimensional space from high-dimensional noisy data. The formulation of the new model can be equivalently interpreted as two coupled learning problem, i.e., structure learning and the learning of projection matrix. This interpretation motivates the learning of the embedding points that can directly form an explicit graph structure. We develop a new method to learn the embedding points that form a spanning tree, which is further extended to obtain a discriminative and compact feature representation for clustering problems. Unlike traditional clustering methods, we assume that centers of clusters should be close to each other if they are connected in a learned graph, and other cluster centers should be distant. This can greatly facilitate data visualization and scientific discovery in downstream analysis. Extensive experiments are performed that demonstrate that the proposed framework is able to obtain discriminative feature representations, and correctly recover the intrinsic structures of various real-world datasets. version:1
arxiv-1610-04181 | Removal of Batch Effects using Distribution-Matching Residual Networks | http://arxiv.org/abs/1610.04181 | id:1610.04181 author:Uri Shaham, Kelly P. Stanton, jun Zhao, Huamin Li, Ruth Montgomery, Yuval Kluger category:stat.ML  published:2016-10-13 summary:Sources of variability in experimentally derived data include measurement error in addition to the physical phenomena of interest. This measurement error is a combination of systematic components, originating from the measuring instrument, and random measurement errors. Several novel biological technologies, such as mass cytometry and single-cell RNA-seq, are plagued with systematic errors that may severely affect statistical analysis if the data is not properly calibrated. Here, we propose a novel deep learning approach for removing systematic batch effects. Our method is based on a residual network, trained to minimize the Maximum Mean Discrepancy (MMD) between the multivariate distributions of two replicates, measured in different batches. We apply our method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that it effectively attenuates batch effects, and outperforms several popular methods. version:2
arxiv-1610-04900 | Convergence rate of stochastic k-means | http://arxiv.org/abs/1610.04900 | id:1610.04900 author:Cheng Tang, Claire Monteleoni category:cs.LG  published:2016-10-16 summary:We analyze online and mini-batch k-means variants. Both scale up the widely used Lloyd 's algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that they have global convergence towards local optima at $O(\frac{1}{t})$ rate under general conditions. In addition, we show if the dataset is clusterable, with suitable initialization, mini-batch k-means converges to an optimal k-means solution with $O(\frac{1}{t})$ convergence rate with high probability. The k-means objective is non-convex and non-differentiable: we exploit ideas from non-convex gradient-based optimization by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent its non-differentiability via geometric insights about k-means update. version:1
arxiv-1610-04889 | Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D Input | http://arxiv.org/abs/1610.04889 | id:1610.04889 author:Srinath Sridhar, Franziska Mueller, Michael Zollhöfer, Dan Casas, Antti Oulasvirta, Christian Theobalt category:cs.CV  published:2016-10-16 summary:Real-time simultaneous tracking of hands manipulating and interacting with external objects has many potential applications in augmented reality, tangible computing, and wearable computing. However, due to difficult occlusions, fast motions, and uniform hand appearance, jointly tracking hand and object pose is more challenging than tracking either of the two separately. Many previous approaches resort to complex multi-camera setups to remedy the occlusion problem and often employ expensive segmentation and optimization steps which makes real-time tracking impossible. In this paper, we propose a real-time solution that uses a single commodity RGB-D camera. The core of our approach is a 3D articulated Gaussian mixture alignment strategy tailored to hand-object tracking that allows fast pose optimization. The alignment energy uses novel regularizers to address occlusions and hand-object contacts. For added robustness, we guide the optimization with discriminative part classification of the hand and segmentation of the object. We conducted extensive experiments on several existing datasets and introduce a new annotated hand-object dataset. Quantitative and qualitative results show the key advantages of our method: speed, accuracy, and robustness. version:1
arxiv-1610-04861 | Digital Makeup from Internet Images | http://arxiv.org/abs/1610.04861 | id:1610.04861 author:Asad Khan, Yudong Guo, Ligang Liu category:cs.CV cs.GR  published:2016-10-16 summary:We present a novel approach of color transfer between images by exploring their high-level semantic information. First, we set up a database which consists of the collection of downloaded images from the internet, which are segmented automatically by using matting techniques. We then, extract image foregrounds from both source and multiple target images. Then by using image matting algorithms, the system extracts the semantic information such as faces, lips, teeth, eyes, eyebrows, etc., from the extracted foregrounds of the source image. And, then the color is transferred between corresponding parts with the same semantic information. Next we get the color transferred result by seamlessly compositing different parts together using alpha blending. In the final step, we present an efficient method of color consistency to optimize the color of a collection of images showing the common scene. The main advantage of our method over existing techniques is that it does not need face matching, as one could use more than one target images. It is not restricted to head shot images as we can also change the color style in the wild. Moreover, our algorithm does not require to choose the same color style, same pose and image size between source and target images. Our algorithm is not restricted to one-to-one image color transfer and can make use of more than one target images to transfer the color in different parts in the source image. Comparing with other approaches, our algorithm is much better in color blending in the input data. version:1
arxiv-1610-04823 | To Frontalize or Not To Frontalize: Do We Really Need Elaborate Pre-Processing to Improve Face Recognition Performance? | http://arxiv.org/abs/1610.04823 | id:1610.04823 author:Sandipan Banerjee, Joel Brogan, Janez Krizaj, Aparna Bharati, Brandon RichardWebster, Vitomir Struc, Patrick Flynn, Walter Scheirer category:cs.CV  published:2016-10-16 summary:Automatic face recognition performance has improved remarkably in the last decade. Much of this success can be attributed to the development of deep learning techniques like convolutional neural networks (CNNs). But the training process for CNNs requires a large amount of clean and well-labelled training data. If a CNN is intended to work with non-frontal face images, should this training data be diverse in terms of facial poses, or should face images be frontalized as a pre-processing step? We address this question in this paper. We evaluate a set of popular facial landmarking and pose frontalization algorithms to understand their effect on facial recognition performance. We also introduce a new automatic frontalization scheme that operates over a single image without the need for a subject-specific 3D model, and perform a comparative analysis between the new scheme and other methods in the literature. A CNN trained on face images frontalized using different pre-processing methods is used to extract features from the Point and Shoot Challenge (PaSC) video dataset. The verification and identification performance of the CNN serves to quantify the effectiveness of each landmarking and frontalization scheme. We find that frontalization, although an intuitive pre-processing strategy, does not significantly improve face recognition performance when compared with a simple 2D face alignment. version:1
arxiv-1610-04814 | Term-Class-Max-Support (TCMS): A Simple Text Document Categorization Approach Using Term-Class Relevance Measure | http://arxiv.org/abs/1610.04814 | id:1610.04814 author:D S Guru, Mahamad Suhil category:cs.IR cs.CL  published:2016-10-16 summary:In this paper, a simple text categorization method using term-class relevance measures is proposed. Initially, text documents are processed to extract significant terms present in them. For every term extracted from a document, we compute its importance in preserving the content of a class through a novel term-weighting scheme known as Term_Class Relevance (TCR) measure proposed by Guru and Suhil (2015) [1]. In this way, for every term, its relevance for all the classes present in the corpus is computed and stored in the knowledgebase. During testing, the terms present in the test document are extracted and the term-class relevance of each term is obtained from the stored knowledgebase. To achieve quick search of term weights, Btree indexing data structure has been adapted. Finally, the class which receives maximum support in terms of term-class relevance is decided to be the class of the given test document. The proposed method works in logarithmic complexity in testing time and simple to implement when compared to any other text categorization techniques available in literature. The experiments conducted on various benchmarking datasets have revealed that the performance of the proposed method is satisfactory and encouraging. version:1
arxiv-1610-03147 | Context-Aware Online Learning for Course Recommendation of MOOC Big Data | http://arxiv.org/abs/1610.03147 | id:1610.03147 author:Yifan Hou, Pan Zhou, Ting Wang, Li Yu, Yuchong Hu, Dapeng Wu category:cs.LG cs.CY cs.IR  published:2016-10-11 summary:The Massive Open Online Course (MOOC) has expanded significantly in recent years. With the widespread of MOOC, the opportunity to study the fascinating courses for free has attracted numerous people of diverse educational backgrounds all over the world. In the big data era, a key research topic for MOOC is how to mine the needed courses in the massive course databases in cloud for each individual student accurately and rapidly as the number of courses is increasing fleetly. In this respect, the key challenge is how to realize personalized course recommendation as well as to reduce the computing and storage costs for the tremendous course data. In this paper, we propose a big data-supported, context-aware online learning-based course recommender system that could handle the dynamic and infinitely massive datasets, which recommends courses by using personalized context information and historical statistics. The context-awareness takes the personal preferences into consideration, making the recommendation suitable for people with different backgrounds. Besides, the algorithm achieves the sublinear regret performance, which means it can gradually recommend the mostly preferred and matched courses to students. In addition, our storage module is expanded to the distributed-connected storage nodes, where the devised algorithm can handle massive course storage problems from heterogeneous sources of course datasets. Comparing to existing algorithms, our proposed algorithms achieve the linear time complexity and space complexity. Experiment results verify the superiority of our algorithms when comparing with existing ones in the MOOC big data setting. version:2
arxiv-1610-04811 | Estimation of low rank density matrices by Pauli measurements | http://arxiv.org/abs/1610.04811 | id:1610.04811 author:Dong Xia category:stat.ML  published:2016-10-16 summary:Density matrices are positively semi-definite Hermitian matrices with unit trace that describe the states of quantum systems. Many quantum systems of physical interest can be represented as high-dimensional low rank density matrices. A popular problem in {\it quantum state tomography} (QST) is to estimate the unknown low rank density matrix of a quantum system by conducting Pauli measurements. Our main contribution is twofold. First, we establish the minimax lower bounds in Schatten $p$-norms with $1\leq p\leq +\infty$ for low rank density matrices estimation by Pauli measurements. In our previous paper, these minimax lower bounds are proved under the trace regression model with Gaussian noise and the noise is assumed to have common variance. In this paper, we prove these bounds under the Binomial observation model which meets the actual model in QST. Second, we study the Dantzig estimator (DE) for estimating the unknown low rank density matrix under the Binomial observation model by using Pauli measurements. In our previous papers, we studied the least squares estimator and the projection estimator, where we proved the optimal convergence rates for the least squares estimator in Schatten $p$-norms with $1\leq p\leq 2$ and, under a stronger condition, the optimal convergence rates for the projection estimator in Schatten $p$-norms with $1\leq p\leq +\infty$. In this paper, we show that the results of these two distinct estimators can be simultaneously obtained by the Dantzig estimator. Moreover, better convergence rates in Schatten norm distances can be proved for Dantzig estimator under conditions weaker than those needed in previous papers. When the objective function of DE is replaced by the negative von Neumann entropy, we obtain sharp convergence rate in Kullback-Leibler divergence. version:1
arxiv-1610-04805 | Beyond Spatial Auto-Regressive Models: Predicting Housing Prices with Satellite Imagery | http://arxiv.org/abs/1610.04805 | id:1610.04805 author:Archith J. Bency, Swati Rallapalli, Raghu K. Ganti, Mudhakar Srivatsa, B. S. Manjunath category:cs.CV  published:2016-10-16 summary:When modeling geo-spatial data, it is critical to capture spatial correlations for achieving high accuracy. Spatial Auto-Regression (SAR) is a common tool used to model such data, where the spatial contiguity matrix (W) encodes the spatial correlations. However, the efficacy of SAR is limited by two factors. First, it depends on the choice of contiguity matrix, which is typically not learnt from data, but instead, is assumed to be known apriori. Second, it assumes that the observations can be explained by linear models. In this paper, we propose a Convolutional Neural Network (CNN) framework to model geo-spatial data (specifi- cally housing prices), to learn the spatial correlations automatically. We show that neighborhood information embedded in satellite imagery can be leveraged to achieve the desired spatial smoothing. An additional upside of our framework is the relaxation of linear assumption on the data. Specific challenges we tackle while implementing our framework include, (i) how much of the neighborhood is relevant while estimating housing prices? (ii) what is the right approach to capture multiple resolutions of satellite imagery? and (iii) what other data-sources can help improve the estimation of spatial correlations? We demonstrate a marked improvement of 57% on top of the SAR baseline through the use of features from deep neural networks for the cities of London, Birmingham and Liverpool. version:1
arxiv-1610-04804 | Dynamic Stacked Generalization for Node Classification on Networks | http://arxiv.org/abs/1610.04804 | id:1610.04804 author:Zhen Han, Alyson Wilson category:stat.ML cs.LG cs.SI stat.AP  published:2016-10-16 summary:We propose a novel stacked generalization (stacking) method as a dynamic ensemble technique using a pool of heterogeneous classifiers for node label classification on networks. The proposed method assigns component models a set of functional coefficients, which can vary smoothly with certain topological features of a node. Compared to the traditional stacking model, the proposed method can dynamically adjust the weights of individual models as we move across the graph and provide a more versatile and significantly more accurate stacking model for label prediction on a network. We demonstrate the benefits of the proposed model using both a simulation study and real data analysis. version:1
arxiv-1610-04798 | Communication-efficient Distributed Sparse Linear Discriminant Analysis | http://arxiv.org/abs/1610.04798 | id:1610.04798 author:Lu Tian, Quanquan Gu category:stat.ML  published:2016-10-15 summary:We propose a communication-efficient distributed estimation method for sparse linear discriminant analysis (LDA) in the high dimensional regime. Our method distributes the data of size $N$ into $m$ machines, and estimates a local sparse LDA estimator on each machine using the data subset of size $N/m$. After the distributed estimation, our method aggregates the debiased local estimators from $m$ machines, and sparsifies the aggregated estimator. We show that the aggregated estimator attains the same statistical rate as the centralized estimation method, as long as the number of machines $m$ is chosen appropriately. Moreover, we prove that our method can attain the model selection consistency under a milder condition than the centralized method. Experiments on both synthetic and real datasets corroborate our theory. version:1
arxiv-1610-04795 | Sample Efficient Optimization for Learning Controllers for Bipedal Locomotion | http://arxiv.org/abs/1610.04795 | id:1610.04795 author:Rika Antonova, Akshara Rai, Christopher G. Atkeson category:cs.RO cs.LG  published:2016-10-15 summary:Learning policies for bipedal locomotion can be difficult, as experiments are expensive and simulation does not usually transfer well to hardware. To counter this, we need al- gorithms that are sample efficient and inherently safe. Bayesian Optimization is a powerful sample-efficient tool for optimizing non-convex black-box functions. However, its performance can degrade in higher dimensions. We develop a distance metric for bipedal locomotion that enhances the sample-efficiency of Bayesian Optimization and use it to train a 16 dimensional neuromuscular model for planar walking. This distance metric reflects some basic gait features of healthy walking and helps us quickly eliminate a majority of unstable controllers. With our approach we can learn policies for walking in less than 100 trials for a range of challenging settings. In simulation, we show results on two different costs and on various terrains including rough ground and ramps, sloping upwards and downwards. We also perturb our models with unknown inertial disturbances analogous with differences between simulation and hardware. These results are promising, as they indicate that this method can potentially be used to learn control policies on hardware. version:1
arxiv-1610-04794 | Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering | http://arxiv.org/abs/1610.04794 | id:1610.04794 author:Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos, Mingyi Hong category:cs.LG  published:2016-10-15 summary:Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the `clustering-friendly' latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network's ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using five different real datasets are employed to showcase the effectiveness of the proposed approach. version:1
arxiv-1610-04787 | Recovering the Missing Link: Predicting Class-Attribute Associations for Unsupervised Zero-Shot Learning | http://arxiv.org/abs/1610.04787 | id:1610.04787 author:Ziad Al-Halah, Makarand Tapaswi, Rainer Stiefelhagen category:cs.CV  published:2016-10-15 summary:Collecting training images for all visual categories is not only expensive but also impractical. Zero-shot learning (ZSL), especially using attributes, offers a pragmatic solution to this problem. However, at test time most attribute-based methods require a full description of attribute associations for each unseen class. Providing these associations is time consuming and often requires domain specific knowledge. In this work, we aim to carry out attribute-based zero-shot classification in an unsupervised manner. We propose an approach to learn relations that couples class embeddings with their corresponding attributes. Given only the name of an unseen class, the learned relationship model is used to automatically predict the class-attribute associations. Furthermore, our model facilitates transferring attributes across data sets without additional effort. Integrating knowledge from multiple sources results in a significant additional improvement in performance. We evaluate on two public data sets: Animals with Attributes and aPascal/aYahoo. Our approach outperforms state-of-the-art methods in both predicting class-attribute associations and unsupervised ZSL by a large margin. version:1
arxiv-1610-04783 | Similarity Learning for Time Series Classification | http://arxiv.org/abs/1610.04783 | id:1610.04783 author:Maria-Irina Nicolae, Éric Gaussier, Amaury Habrard, Marc Sebban category:cs.LG  published:2016-10-15 summary:Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers. version:1
arxiv-1610-04782 | An Adaptive Test of Independence with Analytic Kernel Embeddings | http://arxiv.org/abs/1610.04782 | id:1610.04782 author:Wittawat Jitkrittum, Zoltan Szabo, Arthur Gretton category:stat.ML cs.LG 46E22  62G10 G.3; I.2.6  published:2016-10-15 summary:A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests. version:1
arxiv-1610-05672 | Markov Chain Truncation for Doubly-Intractable Inference | http://arxiv.org/abs/1610.05672 | id:1610.05672 author:Colin Wei, Iain Murray category:stat.ML cs.LG  published:2016-10-15 summary:Computing partition functions, the normalizing constants of probability distributions, is often hard. Variants of importance sampling give unbiased estimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z are harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo sampling of "doubly-intractable" distributions, such as the parameter posterior for Markov Random Fields or Exponential Random Graphs. We demonstrate how to construct unbiased estimates for 1/Z given access to black-box importance sampling estimators for Z. We adapt recent work on random series truncation and Markov chain coupling, producing estimators with lower variance and a higher percentage of positive estimates than before. Our debiasing algorithms are simple to implement, and have some theoretical and empirical advantages over existing methods. version:1
arxiv-1610-06421 | Mixed Neural Network Approach for Temporal Sleep Stage Classification | http://arxiv.org/abs/1610.06421 | id:1610.06421 author:Hao Dong, Akara Supratak, Wei Pan, Chao Wu, Paul M. Matthews, Yike Guo category:q-bio.NC cs.CV cs.LG cs.NE  published:2016-10-15 summary:This paper proposes a practical approach to addressing limitations posed by use of single active electrodes in applications for sleep stage classification. Electroencephalography (EEG)-based characterizations of sleep stage progression contribute the diagnosis and monitoring of the many pathologies of sleep. Several prior reports have explored ways of automating the analysis of sleep EEG and of reducing the complexity of the data needed for reliable discrimination of sleep stages in order to make it possible to perform sleep studies at lower cost in the home (rather than only in specialized clinical facilities). However, these reports have involved recordings from electrodes placed on the cranial vertex or occiput, which can be uncomfortable or difficult for subjects to position. Those that have utilized single EEG channels which contain less sleep information, have showed poor classification performance. We have taken advantage of Rectifier Neural Network for feature detection and Long Short-Term Memory (LSTM) network for sequential data learning to optimize classification performance with single electrode recordings. After exploring alternative electrode placements, we found a comfortable configuration of a single-channel EEG on the forehead and have shown that it can be integrated with additional electrodes for simultaneous recording of the electroocuolgram (EOG). Evaluation of data from 62 people (with 494 hours sleep) demonstrated better performance of our analytical algorithm for automated sleep classification than existing approaches using vertex or occipital electrode placements. Use of this recording configuration with neural network deconvolution promises to make clinically indicated home sleep studies practical. version:1
arxiv-1610-04751 | Unsupervised clustering under the Union of Polyhedral Cones (UOPC) model | http://arxiv.org/abs/1610.04751 | id:1610.04751 author:Wenqi Wang, Vaneet Aggarwal, Shuchin Aeron category:stat.ML  published:2016-10-15 summary:In this paper, we consider clustering data that is assumed to come from one of finitely many pointed convex polyhedral cones. This model is referred to as the Union of Polyhedral Cones (UOPC) model. Similar to the Union of Subspaces (UOS) model where each data from each subspace is generated from a (unknown) basis, in the UOPC model each data from each cone is assumed to be generated from a finite number of (unknown) \emph{extreme rays}.To cluster data under this model, we consider several algorithms - (a) Sparse Subspace Clustering by Non-negative constraints Lasso (NCL), (b) Least squares approximation (LSA), and (c) K-nearest neighbor (KNN) algorithm to arrive at affinity between data points. Spectral Clustering (SC) is then applied on the resulting affinity matrix to cluster data into different polyhedral cones. We show that on an average KNN outperforms both NCL and LSA and for this algorithm we provide the deterministic conditions for correct clustering. For an affinity measure between the cones it is shown that as long as the cones are not very coherent and as long as the density of data within each cone exceeds a threshold, KNN leads to accurate clustering. Finally, simulation results on real datasets (MNIST and YaleFace datasets) depict that the proposed algorithm works well on real data indicating the utility of the UOPC model and the proposed algorithm. version:1
arxiv-1610-04725 | Incremental One-Class Models for Data Classification | http://arxiv.org/abs/1610.04725 | id:1610.04725 author:Takoua Kefi, Riadh Ksantini, M. Becha Kaaniche, Adel Bouhoula category:cs.CV  published:2016-10-15 summary:In this paper we outline a PhD research plan. This research contributes to the field of one-class incremental learning and classification in case of non-stationary environments. The goal of this PhD is to define a new classification framework able to deal with very small learning dataset at the beginning of the process and with abilities to adjust itself according to the variability of the incoming data which create large scale datasets. As a preliminary work, incremental Covariance-guided One-Class Support Vector Machine is proposed to deal with sequentially obtained data. It is inspired from COSVM which put more emphasis on the low variance directions while keeping the basic formulation of incremental One-Class Support Vector Machine untouched. The incremental procedure is introduced by controlling the possible changes of support vectors after the addition of new data points, thanks to the Karush-Kuhn-Tucker conditions, that have to be maintained on all previously acquired data. Comparative experimental results with contemporary incremental and non-incremental one-class classifiers on numerous artificial and real data sets show that our method results in significantly better classification performance. version:1
arxiv-1610-04718 | Generalization of metric classification algorithms for sequences classification and labelling | http://arxiv.org/abs/1610.04718 | id:1610.04718 author:Roman Samarev, Andrey Vasnetsov, Elizaveta Smelkova category:cs.LG cs.CL  published:2016-10-15 summary:The article deals with the issue of modification of metric classification algorithms. In particular, it studies the algorithm k-Nearest Neighbours for its application to sequential data. A method of generalization of metric classification algorithms is proposed. As a part of it, there has been developed an algorithm for solving the problem of classification and labelling of sequential data. The advantages of the developed algorithm of classification in comparison with the existing one are also discussed in the article. There is a comparison of the effectiveness of the proposed algorithm with the algorithm of CRF in the task of chunking in the open data set CoNLL2000. version:1
arxiv-1610-04668 | A Closed Form Solution to Multi-View Low-Rank Regression | http://arxiv.org/abs/1610.04668 | id:1610.04668 author:Shuai Zheng, Xiao Cai, Chris Ding, Feiping Nie, Heng Huang category:cs.LG  published:2016-10-14 summary:Real life data often includes information from different channels. For example, in computer vision, we can describe an image using different image features, such as pixel intensity, color, HOG, GIST feature, SIFT features, etc.. These different aspects of the same objects are often called multi-view (or multi-modal) data. Low-rank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data. But previous low-rank regression model only works on single view data. In this paper, we propose a multi-view low-rank regression model by imposing low-rank constraints on multi-view regression model. Most importantly, we provide a closed-form solution to the multi-view low-rank regression model. Extensive experiments on 4 multi-view datasets show that the multi-view low-rank regression model outperforms single-view regression model and reveals that multi-view low-rank structure is very helpful. version:1
arxiv-1610-04658 | Simultaneous Learning of Trees and Representations for Extreme Classification, with Application to Language Modeling | http://arxiv.org/abs/1610.04658 | id:1610.04658 author:Yacine Jernite, Anna Choromanska, David Sontag, Yann LeCun category:stat.ML cs.CL cs.LG  published:2016-10-14 summary:This paper addresses the problem of multi-class classification with an extremely large number of classes, where the class predictor is learned jointly with the data representation, as is the case in language modeling problems. The predictor admits a hierarchical structure, which allows for efficient handling of settings that deal with a very large number of labels. The predictive power of the model however can heavily depend on the structure of the tree. We address this problem with an algorithm for tree construction and training that is based on a new objective function which favors balanced and easily-separable node partitions. We describe theoretical properties of this objective function and show that it gives rise to a boosting algorithm for which we provide a bound on classification error, i.e. we show that if the objective is weakly optimized in the internal nodes of the tree, then our algorithm will amplify this weak advantage to build a tree achieving any desired level of accuracy. We apply the algorithm to the task of language modeling by re-framing conditional density estimation as a variant of the hierarchical classification problem. We empirically demonstrate on text data that the proposed approach leads to high-quality trees in terms of perplexity and computational running time compared to its non-hierarchical counterpart. version:1
arxiv-1610-04599 | Data-Driven Threshold Machine: Scan Statistics, Change-Point Detection, and Extreme Bandits | http://arxiv.org/abs/1610.04599 | id:1610.04599 author:Shuang Li, Yao Xie, Le Song category:cs.LG math.ST stat.ML stat.TH  published:2016-10-14 summary:We present a novel distribution-free approach, the data-driven threshold machine (DTM), for a fundamental problem at the core of many learning tasks: choose a threshold for a given pre-specified level that bounds the tail probability of the maximum of a (possibly dependent but stationary) random sequence. We do not assume data distribution, but rather relying on the asymptotic distribution of extremal values, and reduce the problem to estimate three parameters of the extreme value distributions and the extremal index. We specially take care of data dependence via estimating extremal index since in many settings, such as scan statistics, change-point detection, and extreme bandits, where dependence in the sequence of statistics can be significant. Key features of our DTM also include robustness and the computational efficiency, and it only requires one sample path to form a reliable estimate of the threshold, in contrast to the Monte Carlo sampling approach which requires drawing a large number of sample paths. We demonstrate the good performance of DTM via numerical examples in various dependent settings. version:1
arxiv-1610-04583 | Message-passing algorithms for synchronization problems over compact groups | http://arxiv.org/abs/1610.04583 | id:1610.04583 author:Amelia Perry, Alexander S. Wein, Afonso S. Bandeira, Ankur Moitra category:cs.IT cs.CV cs.DS math.IT math.OC stat.ML  published:2016-10-14 summary:Various alignment problems arising in cryo-electron microscopy, community detection, time synchronization, computer vision, and other fields fall into a common framework of synchronization problems over compact groups such as Z/L, U(1), or SO(3). The goal of such problems is to estimate an unknown vector of group elements given noisy relative observations. We present an efficient iterative algorithm to solve a large class of these problems, allowing for any compact group, with measurements on multiple 'frequency channels' (Fourier modes, or more generally, irreducible representations of the group). Our algorithm is a highly efficient iterative method following the blueprint of approximate message passing (AMP), which has recently arisen as a central technique for inference problems such as structured low-rank estimation and compressed sensing. We augment the standard ideas of AMP with ideas from representation theory so that the algorithm can work with distributions over compact groups. Using standard but non-rigorous methods from statistical physics we analyze the behavior of our algorithm on a Gaussian noise model, identifying phases where the problem is easy, (computationally) hard, and (statistically) impossible. In particular, such evidence predicts that our algorithm is information-theoretically optimal in many cases, and that the remaining cases show evidence of statistical-to-computational gaps. version:1
arxiv-1610-04579 | Automatically tracking neurons in a moving and deforming brain | http://arxiv.org/abs/1610.04579 | id:1610.04579 author:Jeffrey P. Nguyen, Ashley N. Linder, George S. Plummer, Joshua W. Shaevitz, Andrew M. Leifer category:q-bio.NC cs.CV physics.bio-ph  published:2016-10-14 summary:Advances in optical neuroimaging techniques now allow neural activity to be recorded with cellular resolution in awake and behaving animals. Brain motion in these recordings pose a unique challenge. The location of individual neurons must be tracked in 3D over time to accurately extract single neuron activity traces. Recordings from small invertebrates like C. elegans are especially challenging because they undergo very large brain motion and deformation during animal movement. Here we present an automated computer vision pipeline to reliably track populations of neurons with single neuron resolution in the brain of a freely moving C. elegans undergoing large motion and deformation. 3D volumetric fluorescent images of the animal's brain are straightened, aligned and registered, and the locations of neurons in the images are found via segmentation. Each neuron is then assigned an identity using a new time-independent machine-learning approach we call Neuron Registration Vector Encoding. In this approach, non-rigid point-set registration is used to match each segmented neuron in each volume with a set of reference volumes taken from throughout the recording. The way each neuron matches with the references defines a feature vector which is clustered to assign an identity to each neuron in each volume. Finally, thin-plate spline interpolation is used to correct errors in segmentation and check consistency of assigned identities. The Neuron Registration Vector Encoding approach proposed here is uniquely well suited for tracking neurons in brains undergoing large deformations. When applied to whole-brain calcium imaging recordings in freely moving C. elegans, this analysis pipeline located 150 neurons for the duration of an 8 minute recording and consistently found more neurons more quickly than manual or semi-automated approaches. version:1
arxiv-1610-04578 | Improved Strongly Adaptive Online Learning using Coin Betting | http://arxiv.org/abs/1610.04578 | id:1610.04578 author:Kwang-Sung Jun, Francesco Orabona, Rebecca Willett, Stephen Wright category:stat.ML cs.LG  published:2016-10-14 summary:This paper describes a new parameter-free online learning algorithm for changing environments. In comparing against algorithms with the same time complexity as ours, we obtain a strongly adaptive regret bound that is a factor of at least $\sqrt{\log(T)}$ better, where $T$ is the time horizon. Empirical results show that our algorithm outperforms state-of-the-art methods in learning with expert advice and metric learning scenarios. version:1
arxiv-1610-04576 | Kernel Alignment Inspired Linear Discriminant Analysis | http://arxiv.org/abs/1610.04576 | id:1610.04576 author:Shuai Zheng, Chris Ding category:cs.LG stat.ML  published:2016-10-14 summary:Kernel alignment measures the degree of similarity between two kernels. In this paper, inspired from kernel alignment, we propose a new Linear Discriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA). We first define two kernels, data kernel and class indicator kernel. The problem is to find a subspace to maximize the alignment between subspace-transformed data kernel and class indicator kernel. Surprisingly, the kernel alignment induced kaLDA objective function is very similar to classical LDA and can be expressed using between-class and total scatter matrices. This can be extended to multi-label data. We use a Stiefel-manifold gradient descent algorithm to solve this problem. We perform experiments on 8 single-label and 6 multi-label data sets. Results show that kaLDA has very good performance on many single-label and multi-label problems. version:1
arxiv-1610-04574 | Generalization Error of Invariant Classifiers | http://arxiv.org/abs/1610.04574 | id:1610.04574 author:Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues category:stat.ML cs.AI cs.CV cs.LG  published:2016-10-14 summary:This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets. version:1
arxiv-1610-04563 | Are Accuracy and Robustness Correlated? | http://arxiv.org/abs/1610.04563 | id:1610.04563 author:Andras Rozsa, Manuel Günther, Terrance E. Boult category:cs.CV  published:2016-10-14 summary:Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples. version:1
arxiv-1610-04542 | On Duality Of Multiple Target Tracking and Segmentation | http://arxiv.org/abs/1610.04542 | id:1610.04542 author:Yicong Tian, Mubarak Shah category:cs.CV  published:2016-10-14 summary:Traditionally, object tracking and segmentation are treated as two separate problems and solved independently. However, in this paper, we argue that tracking and segmentation are actually closely related and solving one should help the other. On one hand, the object track, which is a set of bounding boxes with one bounding box in every frame, would provide strong high-level guidance for the target/background segmentation task. On the other hand, the object segmentation would separate object from other objects and background, which will be useful for determining track locations in every frame. We propose a novel framework which combines online multiple target tracking and segmentation in a video. In our approach, the tracking and segmentation problems are coupled by Lagrange dual decomposition, which leads to more accurate segmentation results and also \emph{helps resolve typical difficulties in multiple target tracking, such as occlusion handling, ID-switch and track drifting}. To track targets, an individual appearance model is learned for each target via structured learning and network flow is employed to generate tracks from densely sampled candidates. For segmentation, multi-label Conditional Random Field (CRF) is applied to a superpixel based spatio-temporal graph in a segment of video to assign background or target labels to every superpixel. The experiments on diverse sequences show that our method outperforms state-of-the-art approaches for multiple target tracking as well as segmentation. version:1
arxiv-1610-04531 | Numerical Inversion of SRNF Maps for Elastic Shape Analysis of Genus-Zero Surfaces | http://arxiv.org/abs/1610.04531 | id:1610.04531 author:Hamid Laga, Qian Xie, Ian H. Jermyn, Anuj Srivastava category:cs.GR cs.CG cs.CV  published:2016-10-14 summary:Recent developments in elastic shape analysis (ESA) are motivated by the fact that it provides comprehensive frameworks for simultaneous registration, deformation, and comparison of shapes. These methods achieve computational efficiency using certain square-root representations that transform invariant elastic metrics into Euclidean metrics, allowing for applications of standard algorithms and statistical tools. For analyzing shapes of embeddings of $\mathbb{S}^2$ in $\mathbb{R}^3$, Jermyn et al. introduced square-root normal fields (SRNFs) that transformed an elastic metric, with desirable invariant properties, into the $\mathbb{L}^2$ metric. These SRNFs are essentially surface normals scaled by square-roots of infinitesimal area elements. A critical need in shape analysis is to invert solutions (deformations, averages, modes of variations, etc) computed in the SRNF space, back to the original surface space for visualizations and inferences. Due to the lack of theory for understanding SRNFs maps and their inverses, we take a numerical approach and derive an efficient multiresolution algorithm, based on solving an optimization problem in the surface space, that estimates surfaces corresponding to given SRNFs. This solution is found effective, even for complex shapes, e.g. human bodies and animals, that undergo significant deformations including bending and stretching. Specifically, we use this inversion for computing elastic shape deformations, transferring deformations, summarizing shapes, and for finding modes of variability in a given collection, while simultaneously registering the surfaces. We demonstrate the proposed algorithms using a statistical analysis of human body shapes, classification of generic surfaces and analysis of brain structures. version:1
arxiv-1610-04490 | Amortised MAP Inference for Image Super-resolution | http://arxiv.org/abs/1610.04490 | id:1610.04490 author:Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, Ferenc Huszár category:cs.CV cs.LG stat.ML  published:2016-10-14 summary:Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR. version:1
arxiv-1610-04491 | The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits | http://arxiv.org/abs/1610.04491 | id:1610.04491 author:Tor Lattimore, Csaba Szepesvari category:stat.ML cs.LG  published:2016-10-14 summary:Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the optimism principle and Thompson sampling. While prior work has mostly been in the worst-case setting, we analyse the asymptotic instance-dependent regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation. For example, for generalised linear bandits and reinforcement learning. version:1
arxiv-1610-04460 | A Reduction Theorem for the Sample Mean in Dynamic Time Warping Spaces | http://arxiv.org/abs/1610.04460 | id:1610.04460 author:Brijnesh J. Jain, David Schultz category:cs.CV math.OC stat.ML  published:2016-10-14 summary:Though the concept of sample mean in dynamic time warping (DTW) spaces is used in pattern recognition applications, its existence has neither been proved nor called into question. This article shows that a sample mean exists under general conditions that cover common variations of different DTW-spaces mentioned in the literature. The existence proofs are based on a Reduction Theorem that bounds the length of the candidate solutions we need to consider. The proposed results place the concept of sample mean in DTW-spaces on a sound mathematical foundation and serves as a first step towards a statistical theory of DTW-spaces. version:1
arxiv-1610-04420 | Theoretical Analysis of Domain Adaptation with Optimal Transport | http://arxiv.org/abs/1610.04420 | id:1610.04420 author:Ievgen Redko, Amaury Habrard, Marc Sebban category:stat.ML cs.LG  published:2016-10-14 summary:Domain adaptation (DA) is an important and emerging field of machine learning that tackles the problem occurring when the distributions of training (source domain) and test (target domain) data are similar but different. Current theoretical results show that the efficiency of DA algorithms depends on their capacity of minimizing the divergence between source and target probability distributions. In this paper, we provide a theoretical study on the advantages that concepts borrowed from optimal transportation theory can bring to DA. In particular, we show that the Wasserstein metric can be used as a divergence measure between distributions to obtain generalization guarantees for three different learning settings: (i) classic DA with unsupervised target data (ii) DA combining source and target labeled data, (iii) multiple source DA. Based on the obtained results, we provide some insights showing when this analysis can be tighter than other existing frameworks. We also show that in the context of multiple source DA, the problem of estimating of the best joint hypothesis between source and target labeling functions can be reformulated using a Wasserstein distance-based loss function. We think that these results open the door to novel ideas and directions for DA. version:1
arxiv-1610-04416 | Distributional Inclusion Hypothesis for Tensor-based Composition | http://arxiv.org/abs/1610.04416 | id:1610.04416 author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI  published:2016-10-14 summary:According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed's and Clarke's, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures. version:1
arxiv-1610-03670 | Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes | http://arxiv.org/abs/1610.03670 | id:1610.03670 author:Qi Dong, Shaogang Gong, Xiatian Zhu category:cs.CV  published:2016-10-12 summary:Recognising detailed clothing characteristics (fine-grained attributes) in unconstrained images of people in-the-wild is a challenging task for computer vision, especially when there is only limited training data from the wild whilst most data available for model learning are captured in well-controlled environments using fashion models (well lit, no background clutter, frontal view, high-resolution). In this work, we develop a deep learning framework capable of model transfer learning from well-controlled shop clothing images collected from web retailers to in-the-wild images from the street. Specifically, we formulate a novel Multi-Task Curriculum Transfer (MTCT) deep learning method to explore multiple sources of different types of web annotations with multi-labelled fine-grained attributes. Our multi-task loss function is designed to extract more discriminative representations in training by jointly learning all attributes, and our curriculum strategy exploits the staged easy-to-complex transfer learning motivated by cognitive studies. We demonstrate the advantages of the MTCT model over the state-of-the-art methods on the X-Domain benchmark, a large scale clothing attribute dataset. Moreover, we show that the MTCT model has a notable advantage over contemporary models when the training data size is small. version:3
arxiv-1610-04386 | Practical Learning of Deep Gaussian Processes via Random Fourier Features | http://arxiv.org/abs/1610.04386 | id:1610.04386 author:Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, Maurizio Filippone category:stat.ML stat.CO  published:2016-10-14 summary:The composition of multiple Gaussian Processes as a Deep Gaussian Process (DGP) enables a deep probabilistic approach to flexibly quantify uncertainty and carry out model selection in various learning scenarios. In this work, we introduce a novel formulation of DGPs based on random Fourier features that we train using stochastic variational inference. Our proposal yields an efficient way of training DGP architectures without compromising on predictive performance. Through a series of experiments, we illustrate how our model compares favorably to other state-of-the-art inference methods for DGPs for both regression and classification tasks. We also demonstrate how an asynchronous implementation of stochastic gradient optimization can exploit the computational power of distributed systems for large-scale DGP learning. version:1
arxiv-1610-03995 | Semi-Supervised Active Learning for Support Vector Machines: A Novel Approach that Exploits Structure Information in Data | http://arxiv.org/abs/1610.03995 | id:1610.03995 author:Tobias Reitmaier, Adrian Calma, Bernhard Sick category:stat.ML cs.LG  published:2016-10-13 summary:In our today's information society more and more data emerges, e.g.~in social networks, technical applications, or business applications. Companies try to commercialize these data using data mining or machine learning methods. For this purpose, the data are categorized or classified, but often at high (monetary or temporal) costs. An effective approach to reduce these costs is to apply any kind of active learning (AL) methods, as AL controls the training process of a classifier by specific querying individual data points (samples), which are then labeled (e.g., provided with class memberships) by a domain expert. However, an analysis of current AL research shows that AL still has some shortcomings. In particular, the structure information given by the spatial pattern of the (un)labeled data in the input space of a classification model (e.g.,~cluster information), is used in an insufficient way. In addition, many existing AL techniques pay too little attention to their practical applicability. To meet these challenges, this article presents several techniques that together build a new approach for combining AL and semi-supervised learning (SSL) for support vector machines (SVM) in classification tasks. Structure information is captured by means of probabilistic models that are iteratively improved at runtime when label information becomes available. The probabilistic models are considered in a selection strategy based on distance, density, diversity, and distribution (4DS strategy) information for AL and in a kernel function (Responsibility Weighted Mahalanobis kernel) for SVM. The approach fuses generative and discriminative modeling techniques. With 20 benchmark data sets and with the MNIST data set it is shown that our new solution yields significantly better results than state-of-the-art methods. version:2
arxiv-1610-04377 | Civique: Using Social Media to Detect Urban Emergencies | http://arxiv.org/abs/1610.04377 | id:1610.04377 author:Diptesh Kanojia, Vishwajeet Kumar, Krithi Ramamritham category:cs.CL cs.CY cs.SI  published:2016-10-14 summary:We present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like "fire", "accident", "earthquake", etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively. We demonstrate Civique using a web interface and on an Android application, in realtime, and show its use for both tweet detection and visualization. version:1
arxiv-1610-04371 | Aboveground biomass mapping in French Guiana by combining remote sensing, forest inventories and environmental data | http://arxiv.org/abs/1610.04371 | id:1610.04371 author:Ibrahim Fayad, Nicolas Baghdadi, Stéphane Guitet, Jean-Stéphane Bailly, Bruno Hérault, Valéry Gond, Mahmoud Hajj, Dinh Ho Tong Minh category:stat.ML stat.AP  published:2016-10-14 summary:Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\textgreater{}150 Mg/ha, and \textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypothesize that the problem of misrepresenting local variations in AGB and AGB estimation with good precision occurs because of both methodological limits (signal saturation or dilution bias) and a lack of adequate calibration data in this range of AGB values. We test this hypothesis by developing a calibrated regression model to predict variations in high AGB values (mean \textgreater{}300 Mg/ha) in French Guiana by a methodological approach for spatial extrapolation with data from the optical geoscience laser altimeter system (GLAS), forest inventories, radar, optics, and environmental variables for spatial inter-and extrapolation. Given their higher point count, GLAS data allow a wider coverage of AGB values. We find that the metrics from GLAS footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3 Mg/ha) with no bias for high values. First, predictive models, including remote-sensing, environmental variables and spatial correlation functions, allow us to obtain "wall-to-wall" AGB maps over French Guiana with an RMSE for the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We conclude that a calibrated regression model based on GLAS with dependent environmental data can produce good AGB predictions even for high AGB values if the calibration data fit the AGB range. We also demonstrate that small temporal and spatial mismatches between field data and GLAS footprints are not a problem for regional and global calibrated regression models because field data aim to predict large and deep tendencies in AGB variations from environmental gradients and do not aim to represent high but stochastic and temporally limited variations from forest dynamics. Thus, we advocate including a greater variety of data, even if less precise and shifted, to better represent high AGB values in global models and to improve the fitting of these models for high values. version:1
arxiv-1610-04351 | Semi-supervised Graph Embedding Approach to Dynamic Link Prediction | http://arxiv.org/abs/1610.04351 | id:1610.04351 author:Ryohei Hisano category:stat.ML cs.LG cs.SI physics.soc-ph  published:2016-10-14 summary:We propose a simple discrete time semi-supervised graph embedding approach to link prediction in dynamic networks. The learned embedding reflects information from both the temporal and cross-sectional network structures, which is performed by defining the loss function as a weighted sum of the supervised loss from past dynamics and the unsupervised loss of predicting the neighborhood context in the current network. Our model is also capable of learning different embeddings for both formation and dissolution dynamics. These key aspects contributes to the predictive performance of our model and we provide experiments with three real--world dynamic networks showing that our method is comparable to state of the art methods in link formation prediction and outperforms state of the art baseline methods in link dissolution prediction. version:1
arxiv-1610-04345 | A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts | http://arxiv.org/abs/1610.04345 | id:1610.04345 author:Fei Liu, Julien Perez, Scott Nowson category:cs.CL stat.ML  published:2016-10-14 summary:Many methods have been used to recognize author personality traits from text, typically combining linguistic feature engineering with shallow learning models, e.g. linear regression or Support Vector Machines. This work uses deep-learning-based models and atomic features of text, the characters, to build hierarchical, vectorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. version:1
arxiv-1610-02490 | A nonparametric sequential test for online randomized experiments | http://arxiv.org/abs/1610.02490 | id:1610.02490 author:Vineet Abhishek, Shie Mannor category:stat.ML  published:2016-10-08 summary:We propose a nonparametric sequential test that aims to address two practical problems pertinent to online randomized experiments: (i) how to do a hypothesis test for complex metrics; (ii) how to prevent type $1$ error inflation under continuous monitoring. The proposed test does not require knowledge of the underlying probability distribution generating the data. We use the bootstrap to estimate the likelihood for blocks of data followed by mixture sequential probability ratio test. We validate this procedure on data from a major online e-commerce website and show that the proposed test controls type $1$ error at any time, has good power, and allows quick inference in online randomized experiments. version:2
arxiv-1610-04337 | Spectral Inference Methods on Sparse Graphs: Theory and Applications | http://arxiv.org/abs/1610.04337 | id:1610.04337 author:Alaa Saade category:cond-mat.dis-nn cs.IT cs.LG math.IT  published:2016-10-14 summary:In an era of unprecedented deluge of (mostly unstructured) data, graphs are proving more and more useful, across the sciences, as a flexible abstraction to capture complex relationships between complex objects. One of the main challenges arising in the study of such networks is the inference of macroscopic, large-scale properties affecting a large number of objects, based solely on the microscopic interactions between their elementary constituents. Statistical physics, precisely created to recover the macroscopic laws of thermodynamics from an idealized model of interacting particles, provides significant insight to tackle such complex networks. In this dissertation, we use methods derived from the statistical physics of disordered systems to design and study new algorithms for inference on graphs. Our focus is on spectral methods, based on certain eigenvectors of carefully chosen matrices, and sparse graphs, containing only a small amount of information. We develop an original theory of spectral inference based on a relaxation of various mean-field free energy optimizations. Our approach is therefore fully probabilistic, and contrasts with more traditional motivations based on the optimization of a cost function. We illustrate the efficiency of our approach on various problems, including community detection, randomized similarity-based clustering, and matrix completion. version:1
arxiv-1610-04336 | MML is not consistent for Neyman-Scott | http://arxiv.org/abs/1610.04336 | id:1610.04336 author:Michael Brand category:stat.ML cs.LG math.ST stat.TH 62G20  62G05  62A01 I.2.6  published:2016-10-14 summary:Minimum Message Length (MML) is a popular method for statistical inference, belonging to the Minimum Description Length (MDL) family. It is a general name for any of several computationally-feasible approximations to the generally NP-Hard Strict Minimum Message Length (SMML) estimator. One often-cited showcase for the power of MML is the Neyman-Scott estimation problem, where most popular estimation algorithms fail to produce a consistent result. MML's performance on Neyman-Scott was analysed by Dowe and Wallace (1997) and by Wallace (2005) and MML was shown to be consistent for the problem. However, this analysis was not performed on SMML, but rather on two SMML approximations: Wallace-Freeman and Ideal Group. As for most estimation problems, the exact SMML solution is not known for Neyman-Scott. We analyse the Dowe-Wallace solution, and show that it hinges critically on the use of an unnatural prior for the problem. We argue that the Jeffreys prior is a more natural prior to assume in this case. Re-analysing the problem over its Jeffreys prior, we show that both the Ideal Group and the Wallace-Freeman approximations converge to the (inconsistent) Maximum Likelihood (ML) solution. We develop novel techniques that enable determining properties of the SMML estimator for some general families of estimation problems without requiring a full construction of the estimator, and use these to show that for many problems, including Neyman-Scott, the SMML estimator is not a point-estimator at all. Rather, it maps each observation to an entire continuum of estimates. Furthermore, using the tools developed we show that for Neyman-Scott the SMML estimate is inconsistent for all parameter sets as well as asymptotically. We discuss methodological problems in the arguments put forward by previous authors, who argued that MML is consistent for Neyman-Scott and in general. version:1
arxiv-1610-04322 | Learning and Fusing Multimodal Features from and for Multi-task Facial Computing | http://arxiv.org/abs/1610.04322 | id:1610.04322 author:Wei Li, Zhigang Zhu category:cs.CV  published:2016-10-14 summary:We propose a deep learning-based feature fusion approach for facial computing including face recognition as well as gender, race and age detection. Instead of training a single classifier on face images to classify them based on the features of the person whose face appears in the image, we first train four different classifiers for classifying face images based on race, age, gender and identification (ID). Multi-task features are then extracted from the trained models and cross-task-feature training is conducted which shows the value of fusing multimodal features extracted from multi-tasks. We have found that features trained for one task can be used for other related tasks. More interestingly, the features trained for a task with more classes (e.g. ID) and then used in another task with fewer classes (e.g. race) outperforms the features trained for the other task itself. The final feature fusion is performed by combining the four types of features extracted from the images by the four classifiers. The feature fusion approach improves the classifications accuracy by a 7.2%, 20.1%, 22.2%, 21.8% margin, respectively, for ID, age, race and gender recognition, over the results of single classifiers trained only on their individual features. The proposed method can be applied to applications in which different types of data or features can be extracted. version:1
arxiv-1610-04317 | Approximate Counting, the Lovasz Local Lemma and Inference in Graphical Models | http://arxiv.org/abs/1610.04317 | id:1610.04317 author:Ankur Moitra category:cs.DS cs.CC cs.LG  published:2016-10-14 summary:In this paper we introduce a new approach for approximately counting in bounded degree systems with higher-order constraints. Our main result is an algorithm to approximately count the number of solutions to a CNF formula $\Phi$ with at least $k$ variables per clause and degree at most $d$ when $k$ is logarithmic in $d$. This closes an exponential gap between the known upper and lower bounds. Moreover our algorithm extends straightforwardly to approximate sampling, which shows that under Lovasz Local Lemma-like conditions it is not only possible to find a satisfying assignment, it is also possible to generate one approximately uniformly at random from the set of all satisfying assignments. Our approach is a significant departure from earlier techniques in approximate counting, and is based on a framework to bootstrap an oracle for computing marginal probabilities on individual variables. Finally, we give an application of our results to show that it is algorithmically possible to sample from the posterior distribution in an interesting class of graphical models. version:1
arxiv-1610-03604 | Subspace clustering based on low rank representation and weighted nuclear norm minimization | http://arxiv.org/abs/1610.03604 | id:1610.03604 author:Yu Song, Yiquan Wu category:cs.CV  published:2016-10-12 summary:Subspace clustering refers to the problem of segmenting a set of data points approximately drawn from a union of multiple linear subspaces. Aiming at the subspace clustering problem, various subspace clustering algorithms have been proposed and low rank representation based subspace clustering is a very promising and efficient subspace clustering algorithm. Low rank representation method seeks the lowest rank representation among all the candidates that can represent the data points as linear combinations of the bases in a given dictionary. Nuclear norm minimization is adopted to minimize the rank of the representation matrix. However, nuclear norm is not a very good approximation of the rank of a matrix and the representation matrix thus obtained can be of high rank which will affect the final clustering accuracy. Weighted nuclear norm (WNN) is a better approximation of the rank of a matrix and WNN is adopted in this paper to describe the rank of the representation matrix. The convex program is solved via conventional alternation direction method of multipliers (ADMM) and linearized alternating direction method of multipliers (LADMM) and they are respectively refer to as WNNM-LRR and WNNM-LRR(L). Experimental results show that, compared with low rank representation method and several other state-of-the-art subspace clustering methods, WNNM-LRR and WNNM-LRR(L) can get higher clustering accuracy. version:3
arxiv-1610-04308 | Recurrent 3D Attentional Networks for End-to-End Active Object Recognition in Cluttered Scenes | http://arxiv.org/abs/1610.04308 | id:1610.04308 author:Min Liu, Yifei Shi, Lintao Zheng, Kai Xu category:cs.CV  published:2016-10-14 summary:Active vision is inherently attention-driven: The agent selects views of observation to best approach the vision task while improving its internal representation of the scene being observed. Inspired by the recent success of attention-based models in 2D vision tasks based on single RGB images, we propose to address the multi-view depth-based active object recognition using attention mechanism, through developing an end-to-end recurrent 3D attentional network. The architecture comprises of a recurrent neural network (RNN), storing and updating an internal representation, and two levels of spatial transformer units, guiding two-level attentions. Our model, trained with a 3D shape database, is able to iteratively attend to the best views targeting an object of interest for recognizing it, and focus on the object in each view for removing the background clutter. To realize 3D view selection, we derive a 3D spatial transformer network which is differentiable for training with back-propagation, achieving must faster convergence than the reinforcement learning employed by most existing attention-based models. Experiments show that our method outperforms state-of-the-art methods in cluttered scenes. version:1
arxiv-1610-03725 | Post Selection Inference with Kernels | http://arxiv.org/abs/1610.03725 | id:1610.03725 author:Makoto Yamada, Yuta Umezu, Kenji Fukumizu, Ichiro Takeuchi category:stat.ML stat.ME  published:2016-10-12 summary:We propose a novel kernel based post selection inference (PSI) algorithm, which can not only handle non-linearity in data but also structured output such as multi-dimensional and multi-label outputs. Specifically, we develop a PSI algorithm for independence measures, and propose the Hilbert-Schmidt Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the proposed algorithm is that it can handle non-linearity and/or structured data through kernels. Namely, the proposed algorithm can be used for wider range of applications including nonlinear multi-class classification and multi-variate regressions, while existing PSI algorithms cannot handle them. Through synthetic experiments, we show that the proposed approach can find a set of statistically significant features for both regression and classification problems. Moreover, we apply the hsicInf algorithm to a real-world data, and show that hsicInf can successfully identify important features. version:2
arxiv-1610-04286 | Sim-to-Real Robot Learning from Pixels with Progressive Nets | http://arxiv.org/abs/1610.04286 | id:1610.04286 author:Andrei A. Rusu, Matej Vecerik, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, Raia Hadsell category:cs.RO cs.LG  published:2016-10-13 summary:Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards. version:1
arxiv-1610-03640 | Analyzing the Affect of a Group of People Using Multi-modal Framework | http://arxiv.org/abs/1610.03640 | id:1610.03640 author:Xiaohua Huang, Abhinav Dhall, Xin Liu, Guoying Zhao, Jingang Shi, Roland Goecke, Matti Pietikainen category:cs.CV  published:2016-10-12 summary:Millions of images on the web enable us to explore images from social events such as a family party, thus it is of interest to understand and model the affect exhibited by a group of people in images. But analysis of the affect expressed by multiple people is challenging due to varied indoor and outdoor settings, and interactions taking place between various numbers of people. A few existing works on Group-level Emotion Recognition (GER) have investigated on face-level information. Due to the challenging environments, face may not provide enough information to GER. Relatively few studies have investigated multi-modal GER. Therefore, we propose a novel multi-modal approach based on a new feature description for understanding emotional state of a group of people in an image. In this paper, we firstly exploit three kinds of rich information containing face, upperbody and scene in a group-level image. Furthermore, in order to integrate multiple person's information in a group-level image, we propose an information aggregation method to generate three features for face, upperbody and scene, respectively. We fuse face, upperbody and scene information for robustness of GER against the challenging environments. Intensive experiments are performed on two challenging group-level emotion databases to investigate the role of face, upperbody and scene as well as multi-modal framework. Experimental results demonstrate that our framework achieves very promising performance for GER. version:2
arxiv-1610-04256 | Assessing Threat of Adversarial Examples on Deep Neural Networks | http://arxiv.org/abs/1610.04256 | id:1610.04256 author:Abigail Graese, Andras Rozsa, Terrance E. Boult category:cs.CV  published:2016-10-13 summary:Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat. version:1
arxiv-1610-04211 | Gated End-to-End Memory Networks | http://arxiv.org/abs/1610.04211 | id:1610.04211 author:Julien Perez, Fei Liu category:cs.CL stat.ML  published:2016-10-13 summary:Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art. version:1
arxiv-1610-04210 | Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation | http://arxiv.org/abs/1610.04210 | id:1610.04210 author:Sohail Bahmani, Justin Romberg category:cs.IT cs.LG math.FA math.IT math.OC stat.ML  published:2016-10-13 summary:We propose a flexible convex relaxation for the phase retrieval problem that operates in the natural domain of the signal. Therefore, we avoid the prohibitive computational cost associated with "lifting" and semidefinite programming (SDP) in methods such as PhaseLift and compete with recently developed non-convex techniques for phase retrieval. We relax the quadratic equations for phaseless measurements to inequality constraints each of which representing a symmetric "slab". Through a simple convex program, our proposed estimator finds an extreme point of the intersection of these slabs that is best aligned with a given anchor vector. We characterize geometric conditions that certify success of the proposed estimator. Furthermore, using classic results in statistical learning theory, we show that for random measurements the geometric certificates hold with high probability at an optimal sample complexity. Phase transition of our estimator is evaluated through simulations. Our numerical experiments also suggest that the proposed method can solve phase retrieval problems with coded diffraction measurements as well. version:1
arxiv-1610-08087 | Infinite-dimensional Log-Determinant divergences II: Alpha-Beta divergences | http://arxiv.org/abs/1610.08087 | id:1610.08087 author:Minh Ha Quang category:math.FA cs.AI cs.IT math.IT stat.ML  published:2016-10-13 summary:This work presents a parametrized family of divergences, namely Alpha-Beta Log- Determinant (Log-Det) divergences, between positive definite unitized trace class operators on a Hilbert space. This is a generalization of the Alpha-Beta Log-Determinant divergences between symmetric, positive definite matrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det divergences is highly general and contains many divergences as special cases, including the recently formulated infinite dimensional affine-invariant Riemannian distance and the infinite-dimensional Alpha Log-Det divergences between positive definite unitized trace class operators. In particular, it includes a parametrized family of metrics between positive definite trace class operators, with the affine-invariant Riemannian distance and the square root of the symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det divergences between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), we obtain closed form formulas via the corresponding Gram matrices. version:1
arxiv-1610-04167 | Tensorial Mixture Models | http://arxiv.org/abs/1610.04167 | id:1610.04167 author:Or Sharir, Ronen Tamari, Nadav Cohen, Amnon Shashua category:cs.LG cs.NE stat.ML  published:2016-10-13 summary:We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a "priors tensor" holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution. version:1
arxiv-1610-04161 | Why Deep Neural Networks? | http://arxiv.org/abs/1610.04161 | id:1610.04161 author:Shiyu Liang, R. Srikant category:cs.LG cs.NE  published:2016-10-13 summary:Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\varepsilon$) require $\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\varepsilon$) require $\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a "small" number of neurons. version:1
arxiv-1610-02072 | An efficient high-probability algorithm for Linear Bandits | http://arxiv.org/abs/1610.02072 | id:1610.02072 author:Gábor Braun, Sebastian Pokutta category:cs.DS cs.LG 68Q32  68Q25  published:2016-10-06 summary:For the linear bandit problem, we extend the analysis of algorithm CombEXP from [R. Combes, M. S. Talebi Mazraeh Shahi, A. Proutiere, and M. Lelarge. Combinatorial bandits revisited. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2116--2124. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5831-combinatorial-bandits-revisited.pdf] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope. We prove a high-probability regret of \(O(T^{2/3})\) for time horizon \(T\). While this bound is weaker than the optimal \(O(\sqrt{T})\) bound achieved by GeometricHedge in [P. L. Bartlett, V. Dani, T. Hayes, S. Kakade, A. Rakhlin, and A. Tewari. High-probability regret bounds for bandit online linear optimization. In 21th Annual Conference on Learning Theory (COLT 2008), July 2008. http://eprints.qut.edu.au/45706/1/30-Bartlett.pdf], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions. version:2
arxiv-1610-04124 | GPU-accelerated real-time stixel computation | http://arxiv.org/abs/1610.04124 | id:1610.04124 author:Daniel Hernandez-Juarez, Antonio Espinosa, David Vázquez, Antonio Manuel López, Juan Carlos Moure category:cs.CV  published:2016-10-13 summary:The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and evaluate a complete multi-stixel estimation pipeline on an embedded, energy-efficient, GPU-accelerated device. This work presents a full GPU-accelerated implementation of stixel estimation that produces reliable results at 26 frames per second (real-time) on the Tegra X1 for disparity images of 1024x440 pixels and stixel widths of 5 pixels, and achieves more than 400 frames per second on a high-end Titan X GPU card. version:1
arxiv-1610-04121 | Embedded real-time stereo estimation via Semi-Global Matching on the GPU | http://arxiv.org/abs/1610.04121 | id:1610.04121 author:Daniel Hernandez-Juarez, Alejandro Chacón, Antonio Espinosa, David Vázquez, Juan Carlos Moure, Antonio Manuel López category:cs.CV  published:2016-10-13 summary:Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energy-efficient GPU devices. Our design runs on a Tegra X1 at 42 frames per second (fps) for an image size of 640x480, 128 disparity levels, and using 4 path directions for the SGM method. version:1
arxiv-1610-04120 | Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding | http://arxiv.org/abs/1610.04120 | id:1610.04120 author:Lina M. Rojas Barahona, Milica Gasic, Nikola Mrkšić, Pei-Hao Su, Stefan Ultes, Tsung-Hsien Wen, Steve Young category:cs.AI cs.CL cs.NE  published:2016-10-13 summary:This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER). version:1
arxiv-1610-02746 | Accelerate Monte Carlo Simulations with Restricted Boltzmann Machines | http://arxiv.org/abs/1610.02746 | id:1610.02746 author:Li Huang, Lei Wang category:physics.comp-ph cond-mat.str-el stat.ML  published:2016-10-10 summary:Despite their exceptional flexibility and popularity, the Monte Carlo methods often suffer from slow mixing times for challenging statistical physics problems. We present a general strategy to overcome this difficulty by adopting ideas and techniques from the machine learning community. We fit the unnormalized probability of the physical model to a feedforward neural network and reinterpret the architecture as a restricted Boltzmann machine. Then, exploiting its feature detection ability, we utilize the restricted Boltzmann machine for efficient Monte Carlo updates and to speed up the simulation of the original physical system. We implement these ideas for the Falicov-Kimball model and demonstrate improved acceptance ratio and autocorrelation time near the phase transition point. version:2
arxiv-1610-04097 | Automatic View-Point Selection for Inter-Operative Endoscopic Surveillance | http://arxiv.org/abs/1610.04097 | id:1610.04097 author:Anant S. Vemuri, Stephane A. Nicolau, Jacques Marescaux, Luc Soler, Nicholas Ayache category:cs.CV  published:2016-10-13 summary:Esophageal adenocarcinoma arises from Barrett's esophagus, which is the most serious complication of gastroesophageal reflux disease. Strategies for screening involve periodic surveillance and tissue biopsies. A major challenge in such regular examinations is to record and track the disease evolution and re-localization of biopsied sites to provide targeted treatments. In this paper, we extend our original inter-operative relocalization framework to provide a constrained image based search for obtaining the best view-point match to the live view. Within this context we investigate the effect of: the choice of feature descriptors and color-space; filtering of uninformative frames and endoscopic modality, for view-point localization. Our experiments indicate an improvement in the best view-point retrieval rate to [92%,87%] from [73%,76%] (in our previous approach) for NBI and WL. version:1
arxiv-1610-04086 | Unorganized Malicious Attacks Detection | http://arxiv.org/abs/1610.04086 | id:1610.04086 author:Ming Pang, Wei Gao, Min Tao, Zhi-Hua Zhou category:cs.IR cs.LG  published:2016-10-13 summary:Recommender system has attracted much attention during the past decade, and many attack detection algorithms have been developed for better recommendation. Most previous approaches focus on the shilling attacks, where the attack organizer fakes a large number of user profiles by the same strategy to promote or demote an item. In this paper, we study a different attack style: unorganized malicious attacks, where attackers respectively use a small number of user profiles to attack their own target items without any organizer. This attack style occurs in many real applications, yet relevant study remains open. In this paper, we formulate the unorganized malicious attacks detection as a variant of matrix completion problem, and prove that attackers can be detected theoretically. We propose the Unorganized Malicious Attacks detection (UMA) algorithm, which can be viewed as a proximal alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of our proposed algorithm. version:1
arxiv-1610-04079 | Towards end-to-end optimisation of functional image analysis pipelines | http://arxiv.org/abs/1610.04079 | id:1610.04079 author:Albert Vilamala, Kristoffer Hougaard Madsen, Lars Kai Hansen category:cs.CV q-bio.NC stat.ML  published:2016-10-13 summary:The study of neurocognitive tasks requiring accurate localisation of activity often rely on functional Magnetic Resonance Imaging, a widely adopted technique that makes use of a pipeline of data processing modules, each involving a variety of parameters. These parameters are frequently set according to the local goal of each specific module, not accounting for the rest of the pipeline. Given recent success of neural network research in many different domains, we propose to convert the whole data pipeline into a deep neural network, where the parameters involved are jointly optimised by the network to best serve a common global goal. As a proof of concept, we develop a module able to adaptively apply the most suitable spatial smoothing to every brain volume for each specific neuroimaging task, and we validate its results in a standard brain decoding experiment. version:1
arxiv-1610-04062 | Video Fill in the Blank with Merging LSTMs | http://arxiv.org/abs/1610.04062 | id:1610.04062 author:Amir Mazaheri, Dong Zhang, Mubarak Shah category:cs.CV  published:2016-10-13 summary:Given a video and its incomplete textural description with missing words, the Video-Fill-in-the-Blank (ViFitB) task is to automatically find the missing word. The contextual information of the sentences are important to infer the missing words; the visual cues are even more crucial to get a more accurate inference. In this paper, we presents a new method which intuitively takes advantage of the structure of the sentences and employs merging LSTMs (to merge two LSTMs) to tackle the problem with embedded textural and visual cues. In the experiments, we have demonstrated the superior performance of the proposed method on the challenging "Movie Fill-in-the-Blank" dataset. version:1
arxiv-1610-04057 | Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition | http://arxiv.org/abs/1610.04057 | id:1610.04057 author:Baotian Hu, Xin Liu, Xiangping Wu, Qingcai Chen category:cs.CV  published:2016-10-13 summary:In this paper, we propose a novel model, named Stroke Sequence-dependent Deep Convolutional Neural Network (SSDCNN), using the stroke sequence information and eight-directional features for Online Handwritten Chinese Character Recognition (OLHCCR). On one hand, SSDCNN can learn the representation of Online Handwritten Chinese Character (OLHCC) by incorporating the natural sequence information of the strokes. On the other hand, SSDCNN can incorporate eight-directional features in a natural way. In order to train SSDCNN, we divide the process of training into two stages: 1) The training data is used to pre-train the whole architecture until the performance tends to converge. 2) Fully-connected neural network which is used to combine the stroke sequence-dependent representation with eight-directional features and softmax layer are further trained. Experiments were conducted on the OLHCCR competition tasks of ICDAR 2013. Results show that, SSDCNN can reduce the recognition error by 50\% (5.13\% vs 2.56\%) compared to the model which only use eight-directional features. The proposed SSDCNN achieves 97.44\% accuracy which reduces the recognition error by about 1.9\% compared with the best submitted system on ICDAR2013 competition. These results indicate that SSDCNN can exploit the stroke sequence information to learn high-quality representation of OLHCC. It also shows that the learnt representation and the classical eight-directional features complement each other within the SSDCNN architecture. version:1
arxiv-1610-04042 | Generalized Online Transfer Learning for Climate Control in Residential Buildings | http://arxiv.org/abs/1610.04042 | id:1610.04042 author:Thomas Grubinger, Georgios Chasparis, Thomas Natschlaeger category:cs.SY cs.LG  published:2016-10-13 summary:This paper presents an online transfer learning framework for improving temperature predictions in residential buildings. In transfer learning, prediction models trained under a set of available data from a target domain (e.g., house with limited data) can be improved through the use of data generated from similar source domains (e.g., houses with rich data). Given also the need for prediction models that can be trained online (e.g., as part of a model-predictive-control implementation), this paper introduces the generalized online transfer learning algorithm (GOTL). It employs a weighted combination of the available predictors (i.e., the target and source predictors) and guarantees convergence to the best weighted predictor. Furthermore, the use of Transfer Component Analysis (TCA) allows for using more than a single source domains, since it may facilitate the fit of a single model on more than one source domains (houses). This allows GOTL to transfer knowledge from more than one source domains. We further validate our results through experiments in climate control for residential buildings and show that GOTL may lead to non-negligible energy savings for given comfort levels. version:1
arxiv-1610-04032 | Predicting the dynamics of 2d objects with a deep residual network | http://arxiv.org/abs/1610.04032 | id:1610.04032 author:François Fleuret category:cs.CV cs.LG  published:2016-10-13 summary:We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression problem. With a simple 2d physics simulator, we generate short sequences composed of rectangles put in motion by applying a pulling force at a point picked at random. The network is trained with a quadratic loss to predict the image of the resulting configuration, given the image of the starting configuration and an image indicating the point of grasping. Experiments show that the network learns to predict accurately the resulting image, which implies in particular that (1) it segments rectangles as distinct components, (2) it infers which one contains the grasping point, (3) it models properly the dynamic of a single rectangle, including the torque, (4) it detects and handles collisions to some extent, and (5) it re-synthesizes properly the entire scene with displaced rectangles. version:1
arxiv-1610-04019 | Voice Conversion from Non-parallel Corpora Using Variational Auto-encoder | http://arxiv.org/abs/1610.04019 | id:1610.04019 author:Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, Hsin-Min Wang category:stat.ML cs.LG cs.SD  published:2016-10-13 summary:We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora. version:1
arxiv-1610-03996 | Bank Card Usage Prediction Exploiting Geolocation Information | http://arxiv.org/abs/1610.03996 | id:1610.03996 author:Martin Wistuba, Nghia Duong-Trung, Nicolas Schilling, Lars Schmidt-Thieme category:cs.LG cs.AI  published:2016-10-13 summary:We describe the solution of team ISMLL for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage for both tasks. Our solution is based on three pillars. Gradient boosted decision trees as a strong regression and classification model, an intensive search for good hyperparameter configurations and strong features that exploit geolocation information. This approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task. version:1
arxiv-1610-03988 | Dictionary Update for NMF-based Voice Conversion Using an Encoder-Decoder Network | http://arxiv.org/abs/1610.03988 | id:1610.03988 author:Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, Hsin-Min Wang category:stat.ML cs.LG cs.SD  published:2016-10-13 summary:In this paper, we propose a dictionary update method for Nonnegative Matrix Factorization (NMF) with high dimensional data in a spectral conversion (SC) task. Voice conversion has been widely studied due to its potential applications such as personalized speech synthesis and speech enhancement. Exemplar-based NMF (ENMF) emerges as an effective and probably the simplest choice among all techniques for SC, as long as a source-target parallel speech corpus is given. ENMF-based SC systems usually need a large amount of bases (exemplars) to ensure the quality of the converted speech. However, a small and effective dictionary is desirable but hard to obtain via dictionary update, in particular when high-dimensional features such as STRAIGHT spectra are used. Therefore, we propose a dictionary update framework for NMF by means of an encoder-decoder reformulation. Regarding NMF as an encoder-decoder network makes it possible to exploit the whole parallel corpus more effectively and efficiently when applied to SC. Our experiments demonstrate significant gains of the proposed system with small dictionaries over conventional ENMF-based systems with dictionaries of same or much larger size. version:1
arxiv-1610-03955 | Dialogue Session Segmentation by Embedding-Enhanced TextTiling | http://arxiv.org/abs/1610.03955 | id:1610.03955 author:Yiping Song, Lili Mou, Rui Yan, Li Yi, Zinan Zhu, Xiaohua Hu, Ming Zhang category:cs.CL cs.HC  published:2016-10-13 summary:In human-computer conversation systems, the context of a user-issued utterance is particularly important because it provides useful background information of the conversation. However, it is unwise to track all previous utterances in the current session as not all of them are equally important. In this paper, we address the problem of session segmentation. We propose an embedding-enhanced TextTiling approach, inspired by the observation that conversation utterances are highly noisy, and that word embeddings provide a robust way of capturing semantics. Experimental results show that our approach achieves better performance than the TextTiling, MMD approaches. version:1
arxiv-1610-03950 | Compressing Neural Language Models by Sparse Word Representations | http://arxiv.org/abs/1610.03950 | id:1610.03950 author:Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, Zhi Jin category:cs.CL cs.LG  published:2016-10-13 summary:Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure. version:1
arxiv-1610-03946 | A Neural Network for Coordination Boundary Prediction | http://arxiv.org/abs/1610.03946 | id:1610.03946 author:Jessica Ficler, Yoav Goldberg category:cs.CL  published:2016-10-13 summary:We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus. version:1
arxiv-1610-03934 | A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder | http://arxiv.org/abs/1610.03934 | id:1610.03934 author:Hans Krupakar, Keerthika Rajvel, Bharathi B, Angel Deborah S, Vallidevi Krishnamurthy category:cs.CL cs.NE cs.SD stat.ML  published:2016-10-13 summary:Speech Translation has always been about giving source text or audio input and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice ear-piece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Translation and Synthesis. We tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text, a translation unit for source language text to target language text, and a synthesis unit for target language text to target language speech. Speech from the surroundings will be recorded by the recognition unit present on the ear-piece and translation will start as soon as one sentence is successfully read. This way, we hope to give translated output as and when input is being read. The recognition unit will use Hidden Markov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory cells, and the synthesis unit, HMM based speech synthesis system HTS. This system will initially be built as an English to Tamil translation device. version:1
arxiv-1610-03927 | Statistical Inference Using Mean Shift Denoising | http://arxiv.org/abs/1610.03927 | id:1610.03927 author:Yunhua Xiang, Yen-Chi Chen category:stat.ME stat.ML  published:2016-10-13 summary:In this paper, we study how the mean shift algorithm can be used to denoise a dataset. We introduce a new framework to analyze the mean shift algorithm as a denoising approach by viewing the algorithm as an operator on a distribution function. We investigate how the mean shift algorithm changes the distribution and show that data points shifted by the mean shift concentrate around high density regions of the underlying density function. By using the mean shift as a denoising method, we enhance the performance of several clustering techniques, improve the power of two-sample tests, and obtain a new method for anomaly detection. version:1
arxiv-1610-03914 | Mapping Between Natural Movie fMRI Responses and Word-Sequence Representations | http://arxiv.org/abs/1610.03914 | id:1610.03914 author:Kiran Vodrahalli, Po-Hsuan Chen, Yingyu Liang, Janice Chen, Esther Yong, Christopher Honey, Peter Ramadge, Ken Norman, Sanjeev Arora category:q-bio.NC cs.CL cs.LG  published:2016-10-13 summary:This work provides support for the notion that distributional methods of representing word meaning from computational linguistics are useful for capturing neural correlates of real life multi-sensory stimuli, where the stimuli ---in this case, a movie being watched by the human subjects--- have been given text annotations. We present an approach to combining sequences of word vectors into a single vector. We also identify a semantically-relevant low-dimensional shared representation of fMRI response in an unsupervised fashion by using views of multiple subjects watching the same natural movie stimulus. Learned orthogonal linear maps between the fMRI and semantic representations allow us to successfully transform fMRI data generated by a natural movie stimulus into semantic vectors representing textual descriptions of the movie. We succeed at a scene classification task with 76% accuracy, over a 20% chance rate. When we selected five brain regions-of-interest (ROIs) and learned distinct maps from these ROIs to the text representations, the Default Mode Network (DMN) supported the highest level of decoding performance. version:1
arxiv-1610-03098 | Neural Paraphrase Generation with Stacked Residual LSTM Networks | http://arxiv.org/abs/1610.03098 | id:1610.03098 author:Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek Datla, Ashequl Qadir, Joey Liu, Oladimeji Farri category:cs.CL  published:2016-10-10 summary:In this paper, we propose a novel neural approach for paraphrase generation. Conventional para- phrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi- directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric. version:3
arxiv-1610-03898 | Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at Extremely Low Resolutions | http://arxiv.org/abs/1610.03898 | id:1610.03898 author:Jiawei Chen, Jonathan Wu, Janusz Konrad, Prakash Ishwar category:cs.CV  published:2016-10-12 summary:Deep convolutional neural networks (ConvNets) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos. However, less attention has been paid to recognition performance at extremely low resolutions (eLR) (e.g., 16 x 12 pixels). Reliable action recognition using eLR cameras would address privacy concerns in various application environments such as private homes, hospitals, nursing/rehabilitation facilities, etc. In this paper, we propose a semi-coupled filter-sharing network that leverages high resolution (HR) videos during training in order to assist an eLR ConvNet. We also study methods for fusing spatial and temporal ConvNets customized for eLR videos in order to take advantage of appearance and motion information. Our method outperforms state-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB (29.2%) datasets. version:1
arxiv-1506-00059 | Saddle-free Hessian-free Optimization | http://arxiv.org/abs/1506.00059 | id:1506.00059 author:Martin Arjovsky category:cs.NA stat.ML  published:2015-05-30 summary:Nonconvex optimization problems such as the ones in training deep neural networks suffer from a phenomenon called saddle point proliferation. This means that there are a vast number of high error saddle points present in the loss function. Second order methods have been tremendously successful and widely adopted in the convex optimization community, while their usefulness in deep learning remains limited. This is due to two problems: computational complexity and the methods being driven towards the high error saddle points. We introduce a novel algorithm specially designed to solve these two issues, providing a crucial first step to take the widely known advantages of Newton's method to the nonconvex optimization community, especially in high dimensional settings. version:2
arxiv-1610-03819 | Recursive Diffeomorphism-Based Regression for Shape Functions | http://arxiv.org/abs/1610.03819 | id:1610.03819 author:Jieren Xu, Haizhao Yang, Ingrid Daubechies category:math.NA cs.CV math.ST stat.TH  published:2016-10-12 summary:This paper proposes a recursive diffeomorphism based regression method for one-dimensional generalized mode decomposition problem that aims at extracting generalized modes $\alpha_k(t)s_k(2\pi N_k\phi_k(t))$ from their superposition $\sum_{k=1}^K \alpha_k(t)s_k(2\pi N_k\phi_k(t))$. First, a one-dimensional synchrosqueezed transform is applied to estimate instantaneous information, e.g., $\alpha_k(t)$ and $N_k\phi_k(t)$. Second, a novel approach based on diffeomorphisms and nonparametric regression is proposed to estimate wave shape functions $s_k(t)$. These two methods lead to a framework for the generalized mode decomposition problem under a weak well-separation condition. Numerical examples of synthetic and real data are provided to demonstrate the fruitful applications of these methods. version:1
arxiv-1610-03807 | Domain-specific Question Generation from a Knowledge Base | http://arxiv.org/abs/1610.03807 | id:1610.03807 author:Linfeng Song, Lin Zhao category:cs.CL  published:2016-10-12 summary:Question generation has been a research topic for a long time, where a big challenge is how to generate deep and natural questions. To tackle this challenge, we propose a system to generate natural language questions from a domain-specific knowledge base (KB) by utilizing rich web information. A small number of question templates are first created based on the KB and instantiated into questions, which are used as seed set and further expanded through the web to get more question candidates. A filtering model is then applied to select candidates with high grammaticality and domain relevance. The system is able to generate large amount of in-domain natural language questions with considerable semantic diversity and is easily applicable to other domains. We evaluate the quality of the generated questions by human judgments and the results show the effectiveness of our proposed system. version:1
arxiv-1610-03793 | Introduction to the "Industrial Benchmark" | http://arxiv.org/abs/1610.03793 | id:1610.03793 author:Daniel Hein, Alexander Hentschel, Volkmar Sterzing, Michel Tokic, Steffen Udluft category:cs.LG  published:2016-10-12 summary:A novel reinforcement learning benchmark, called Industrial Benchmark, is introduced. The Industrial Benchmark aims at being be realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity. version:1
arxiv-1610-03782 | Video Depth-From-Defocus | http://arxiv.org/abs/1610.03782 | id:1610.03782 author:Hyeongwoo Kim, Christian Richardt, Christian Theobalt category:cs.CV  published:2016-10-12 summary:Many compelling video post-processing effects, in particular aesthetic focus editing and refocusing effects, are feasible if per-frame depth information is available. Existing computational methods to capture RGB and depth either purposefully modify the optics (coded aperture, light-field imaging), or employ active RGB-D cameras. Since these methods are less practical for users with normal cameras, we present an algorithm to capture all-in-focus RGB-D video of dynamic scenes with an unmodified commodity video camera. Our algorithm turns the often unwanted defocus blur into a valuable signal. The input to our method is a video in which the focus plane is continuously moving back and forth during capture, and thus defocus blur is provoked and strongly visible. This can be achieved by manually turning the focus ring of the lens during recording. The core algorithmic ingredient is a new video-based depth-from-defocus algorithm that computes space-time-coherent depth maps, deblurred all-in-focus video, and the focus distance for each frame. We extensively evaluate our approach, and show that it enables compelling video post-processing effects, such as different types of refocusing. version:1
arxiv-1610-03777 | Deep disentangled representations for volumetric reconstruction | http://arxiv.org/abs/1610.03777 | id:1610.03777 author:Edward Grant, Pushmeet Kohli, Marcel van Gerven category:cs.CV  published:2016-10-12 summary:We introduce a convolutional neural network for inferring a compact disentangled graphical description of objects from 2D images that can be used for volumetric reconstruction. The network comprises an encoder and a twin-tailed decoder. The encoder generates a disentangled graphics code. The first decoder generates a volume, and the second decoder reconstructs the input image using a novel training regime that allows the graphics code to learn a separate representation of the 3D object and a description of its lighting and pose conditions. We demonstrate this method by generating volumes and disentangled graphical descriptions from images and videos of faces and chairs. version:1
arxiv-1610-03771 | SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods | http://arxiv.org/abs/1610.03771 | id:1610.03771 author:Marzieh Saeidi, Guillaume Bouchard, Maria Liakata, Sebastian Riedel category:cs.CL  published:2016-10-12 summary:In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks. version:1
arxiv-1610-03759 | Language Models with GloVe Word Embeddings | http://arxiv.org/abs/1610.03759 | id:1610.03759 author:Victor Makarenkov, Bracha Shapira, Lior Rokach category:cs.CL  published:2016-10-12 summary:In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4]. version:1
arxiv-1610-03750 | Semi-supervised Discovery of Informative Tweets During the Emerging Disasters | http://arxiv.org/abs/1610.03750 | id:1610.03750 author:Shanshan Zhang, Slobodan Vucetic category:cs.CL cs.SI  published:2016-10-12 summary:The first objective towards the effective use of microblogging services such as Twitter for situational awareness during the emerging disasters is discovery of the disaster-related postings. Given the wide range of possible disasters, using a pre-selected set of disaster-related keywords for the discovery is suboptimal. An alternative that we focus on in this work is to train a classifier using a small set of labeled postings that are becoming available as a disaster is emerging. Our hypothesis is that utilizing large quantities of historical microblogs could improve the quality of classification, as compared to training a classifier only on the labeled data. We propose to use unlabeled microblogs to cluster words into a limited number of clusters and use the word clusters as features for classification. To evaluate the proposed semi-supervised approach, we used Twitter data from 6 different disasters. Our results indicate that when the number of labeled tweets is 100 or less, the proposed approach is superior to the standard classification based on the bag or words feature representation. Our results also reveal that the choice of the unlabeled corpus, the choice of word clustering algorithm, and the choice of hyperparameters can have a significant impact on the classification accuracy. version:1
arxiv-1610-03738 | Exploring the Entire Regularization Path for the Asymmetric Cost Linear Support Vector Machine | http://arxiv.org/abs/1610.03738 | id:1610.03738 author:Daniel Wesierski category:cs.LG stat.ML  published:2016-10-12 summary:We propose an algorithm for exploring the entire regularization path of asymmetric-cost linear support vector machines. Empirical evidence suggests the predictive power of support vector machines depends on the regularization parameters of the training algorithms. The algorithms exploring the entire regularization paths have been proposed for single-cost support vector machines thereby providing the complete knowledge on the behavior of the trained model over the hyperparameter space. Considering the problem in two-dimensional hyperparameter space though enables our algorithm to maintain greater flexibility in dealing with special cases and sheds light on problems encountered by algorithms building the paths in one-dimensional spaces. We demonstrate two-dimensional regularization paths for linear support vector machines that we train on synthetic and real data. version:1
arxiv-1610-03724 | The Weak Efficient Market Hypothesis in Light of Statistical Learning | http://arxiv.org/abs/1610.03724 | id:1610.03724 author:Lucas Fievet, Didier Sornette category:stat.AP stat.ML  published:2016-10-12 summary:We make an unprecedented evaluation of statistical learning methods to forecast daily returns. Using a randomization test to adjust for data snooping, several models are found statistically significant on the tested equity indices: CSI 300, FTSE, and S&P 500. A best Sharpe ratio portfolio has abnormal returns on the S&P 500, breaking even with the market at 10 bps in round trip costs. The returns produce statistically significant intercept for factor regression models, qualifying as a new anomalous 3-day crisis persistency factor. These results open the path towards a standardized usage of statistical learning methods in finance. version:1
arxiv-1610-03713 | Optimistic Semi-supervised Least Squares Classification | http://arxiv.org/abs/1610.03713 | id:1610.03713 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2016-10-12 summary:The goal of semi-supervised learning is to improve supervised classifiers by using additional unlabeled training examples. In this work we study a simple self-learning approach to semi-supervised learning applied to the least squares classifier. We show that a soft-label and a hard-label variant of self-learning can be derived by applying block coordinate descent to two related but slightly different objective functions. The resulting soft-label approach is related to an idea about dealing with missing data that dates back to the 1930s. We show that the soft-label variant typically outperforms the hard-label variant on benchmark datasets and partially explain this behaviour by studying the relative difficulty of finding good local minima for the corresponding objective functions. version:1
arxiv-1610-02496 | SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs | http://arxiv.org/abs/1610.02496 | id:1610.02496 author:Kaiwei Li, Jianfei Chen, Wenguang Chen, Jun Zhu category:cs.DC cs.IR cs.LG stat.ML  published:2016-10-08 summary:Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics. In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before. version:2
arxiv-1610-03684 | Light Field Compression with Disparity Guided Sparse Coding based on Structural Key Views | http://arxiv.org/abs/1610.03684 | id:1610.03684 author:Jie Chen, Junhui Hou, Lap-Pui Chau category:cs.CV  published:2016-10-12 summary:Recent imaging technologies are rapidly evolving for sampling richer and more immersive representations of the 3D world. And one of the emerging technologies are light field (LF) cameras based on micro-lens arrays. To record the directional information of the light rays, a much larger storage space and transmission bandwidth are required by a LF image as compared to a conventional 2D image of similar spatial dimension, and the compression of LF data becomes a vital part of its application. In this paper, we propose a LF codec that fully exploit the intrinsic geometry between the LF sub-views by first approximating the LF with disparity guided sparse coding over a perspective shifted light field dictionary. The sparse coding is only based on several optimized Structural Key Views (SKV), however the entire LF can be recovered from the coding coefficients. By keeping the approximation identical between encoder and decoder, only the sparse coding residual and the SKVs needs to be transmitted. An optimized SKV selection method is proposed such that most LF spatial information could be preserved. And to achieve optimum dictionary efficiency, the LF is divided into several Coding Regions (CR), over which the reconstruction works individually. Experiments and comparisons have been carried out over benchmark LF dataset which show that the proposed SC-SKV codec produces state-of-the-art compression results in terms of rate-distortion performance and visual quality compared with High Efficiency Video Coding (HEVC): with 37.79% BD rate reduction and 0.92 dB BD-PSNR improvement achieved on average, especially with up to 4 dB improvement for low bit rate scenarios. version:1
arxiv-1610-03677 | Deep Fruit Detection in Orchards | http://arxiv.org/abs/1610.03677 | id:1610.03677 author:Suchet Bargoti, James Underwood category:cs.RO cs.AI cs.CV cs.LG  published:2016-10-12 summary:An accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Ablation studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduction in the number of training images required. In contrast, transferring knowledge between orchards contributed to negligible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of >0.9 achieved for apples and mangoes. version:1
arxiv-1610-03660 | Image Based Camera Localization: an Overview | http://arxiv.org/abs/1610.03660 | id:1610.03660 author:Yihong Wu category:cs.CV  published:2016-10-12 summary:Recently, virtual reality, augmented reality, robotics, self-driving cars et al attractive much attention of industrial community, in which image based camera localization is a key task. It is urgent to give an overview of image based camera localization. In this paper, an overview of image based camera localization is presented. It will be useful to not only researchers but also engineers. version:1
arxiv-1610-04156 | Theory and computer simulation of the moiré patterns in single-layer cylindrical particles | http://arxiv.org/abs/1610.04156 | id:1610.04156 author:Vladimir Saveljev, Irina Palchikova category:cond-mat.mes-hall cs.CV  published:2016-10-12 summary:Basing on the theory for arbitrary oriented surfaces, we developed the theory of the moir\'e effect for cylindrical single-layer objects in the paraxial approximation. With using the dual grids, the moir\'e effect in the plane gratings is simulated, as well as the near-axis moir\'e effect in cylinders including the chiral layouts. The results can be applied to the graphene layers, to single-walled nanotubes, and to cylinders in general. version:1
arxiv-1610-03628 | RetiNet: Automatic AMD identification in OCT volumetric data | http://arxiv.org/abs/1610.03628 | id:1610.03628 author:Stefanos Apostolopoulos, Carlos Ciller, Sandro I. De Zanet, Sebastian Wolf, Raphael Sznitman category:cs.CV cs.LG cs.NE  published:2016-10-12 summary:Optical Coherence Tomography (OCT) provides a unique ability to image the eye retina in 3D at micrometer resolution and gives ophthalmologist the ability to visualize retinal diseases such as Age-Related Macular Degeneration (AMD). While visual inspection of OCT volumes remains the main method for AMD identification, doing so is time consuming as each cross-section within the volume must be inspected individually by the clinician. In much the same way, acquiring ground truth information for each cross-section is expensive and time consuming. This fact heavily limits the ability to acquire large amounts of ground truth, which subsequently impacts the performance of learning-based methods geared at automatic pathology identification. To avoid this burden, we propose a novel strategy for automatic analysis of OCT volumes where only volume labels are needed. That is, we train a classifier in a semi-supervised manner to conduct this task. Our approach uses a novel Convolutional Neural Network (CNN) architecture, that only needs volume-level labels to be trained to automatically asses whether an OCT volume is healthy or contains AMD. Our architecture involves first learning a cross-section pathology classifier using pseudo-labels that could be corrupted and then leverage these towards a more accurate volume-level classification. We then show that our approach provides excellent performances on a publicly available dataset and outperforms a number of existing automatic techniques. version:1
arxiv-1610-03623 | Fast Training of Convolutional Neural Networks via Kernel Rescaling | http://arxiv.org/abs/1610.03623 | id:1610.03623 author:Pedro Porto Buarque de Gusmão, Gianluca Francini, Skjalg Lepsøy, Enrico Magli category:cs.CV  published:2016-10-12 summary:Training deep Convolutional Neural Networks (CNN) is a time consuming task that may take weeks to complete. In this article we propose a novel, theoretically founded method for reducing CNN training time without incurring any loss in accuracy. The basic idea is to begin training with a pre-train network using lower-resolution kernels and input images, and then refine the results at the full resolution by exploiting the spatial scaling property of convolutions. We apply our method to the ImageNet winner OverFeat and to the more recent ResNet architecture and show a reduction in training time of nearly 20% while test set accuracy is preserved in both cases. version:1
arxiv-1610-03618 | Optimizing Memory Efficiency for Deep Convolutional Neural Networks on GPUs | http://arxiv.org/abs/1610.03618 | id:1610.03618 author:Chao Li, Yi Yang, Min Feng, Srimat Chakradhar, Huiyang Zhou category:cs.DC cs.LG cs.NE cs.PF  published:2016-10-12 summary:Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve state-of-the-art recognition accuracy. Due to the substantial compute and memory operations, however, they require significant execution time. The massive parallel computing capability of GPUs make them as one of the ideal platforms to accelerate CNNs and a number of GPU-based CNN libraries have been developed. While existing works mainly focus on the computational efficiency of CNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have intricate data structures and their memory behavior can have significant impact on the performance. In this work, we study the memory efficiency of various CNN layers and reveal the performance implication from both data layouts and memory access patterns. Experiments show the universal effect of our proposed optimizations on both single layers and various networks, with up to 27.9x for a single layer and up to 5.6x on the whole networks. version:1
arxiv-1610-03615 | The Virtual Electromagnetic Interaction between Digital Images for Image Matching with Shifting Transformation | http://arxiv.org/abs/1610.03615 | id:1610.03615 author:Xiaodong Zhuang, N. E. Mastorakis category:cs.CV  published:2016-10-12 summary:A novel way of matching two images with shifting transformation is studied. The approach is based on the presentation of the virtual edge current in images, and also the study of virtual electromagnetic interaction between two related images inspired by electromagnetism. The edge current in images is proposed as a discrete simulation of the physical current, which is based on the significant edge line extracted by Canny-like edge detection. Then the virtual interaction of the edge currents between related images is studied by imitating the electro-magnetic interaction between current-carrying wires. Based on the virtual interaction force between two related images, a novel method is presented and applied in image matching for shifting transformation. The preliminary experimental results indicate the effectiveness of the proposed method. version:1
arxiv-1610-03614 | A Model of Virtual Carrier Immigration in Digital Images for Region Segmentation | http://arxiv.org/abs/1610.03614 | id:1610.03614 author:Xiaodong Zhuang, N. E. Mastorakis category:cs.CV  published:2016-10-12 summary:A novel model for image segmentation is proposed, which is inspired by the carrier immigration mechanism in physical P-N junction. The carrier diffusing and drifting are simulated in the proposed model, which imitates the physical self-balancing mechanism in P-N junction. The effect of virtual carrier immigration in digital images is analyzed and studied by experiments on test images and real world images. The sign distribution of net carrier at the model's balance state is exploited for region segmentation. The experimental results for both test images and real-world images demonstrate self-adaptive and meaningful gathering of pixels to suitable regions, which prove the effectiveness of the proposed method for image region segmentation. version:1
arxiv-1610-03612 | The Analysis of Local Motion and Deformation in Image Sequences Inspired by Physical Electromagnetic Interaction | http://arxiv.org/abs/1610.03612 | id:1610.03612 author:Xiaodong Zhuang, N. E. Mastorakis category:cs.CV  published:2016-10-12 summary:In order to analyze the moving and deforming of the objects in image sequence, a novel way is presented to analyze the local changes of object edges between two related images (such as two adjacent frames in a video sequence), which is inspired by the physical electromagnetic interaction. The changes of edge between adjacent frames in sequences are analyzed by simulation of virtual current interaction, which can reflect the change of the object's position or shape. The virtual current along the main edge line is proposed based on the significant edge extraction. Then the virtual interaction between the current elements in the two related images is studied by imitating the interaction between physical current-carrying wires. The experimental results prove that the distribution of magnetic forces on the current elements in one image applied by the other can reflect the local change of edge lines from one image to the other, which is important in further analysis. version:1
arxiv-1610-03595 | Towards a Theoretical Analysis of PCA for Heteroscedastic Data | http://arxiv.org/abs/1610.03595 | id:1610.03595 author:David Hong, Laura Balzano, Jeffrey A. Fessler category:math.ST stat.ML stat.TH  published:2016-10-12 summary:Principal Component Analysis (PCA) is a method for estimating a subspace given noisy samples. It is useful in a variety of problems ranging from dimensionality reduction to anomaly detection and the visualization of high dimensional data. PCA performs well in the presence of moderate noise and even with missing data, but is also sensitive to outliers. PCA is also known to have a phase transition when noise is independent and identically distributed; recovery of the subspace sharply declines at a threshold noise variance. Effective use of PCA requires a rigorous understanding of these behaviors. This paper provides a step towards an analysis of PCA for samples with heteroscedastic noise, that is, samples that have non-uniform noise variances and so are no longer identically distributed. In particular, we provide a simple asymptotic prediction of the recovery of a one-dimensional subspace from noisy heteroscedastic samples. The prediction enables: a) easy and efficient calculation of the asymptotic performance, and b) qualitative reasoning to understand how PCA is impacted by heteroscedasticity (such as outliers). version:1
arxiv-1610-03592 | On statistical learning via the lens of compression | http://arxiv.org/abs/1610.03592 | id:1610.03592 author:Ofir David, Shay Moran, Amir Yehudayoff category:cs.LG cs.DM cs.LO math.CO math.LO  published:2016-10-12 summary:This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. The central theme of this work is establishing equivalences between learnability and compressibility, and utilizing these equivalences in the study of statistical learning theory. We begin with the setting of multiclass categorization (zero/one loss). We prove that in this case learnability is equivalent to compression of logarithmic sample size, and that uniform convergence implies compression of constant size. We then consider Vapnik's general learning setting: we show that in order to extend the compressibility-learnability equivalence to this case, it is necessary to consider an approximate variant of compression. Finally, we provide some applications of the compressibility-learnability equivalences: (i) Agnostic-case learnability and realizable-case learnability are equivalent in multiclass categorization problems (in terms of sample complexity). (ii) This equivalence between agnostic-case learnability and realizable-case learnability does not hold for general learning problems: There exists a learning problem whose loss function takes just three values, under which agnostic-case and realizable-case learnability are not equivalent. (iii) Uniform convergence implies compression of constant size in multiclass categorization problems. Part of the argument includes an analysis of the uniform convergence rate in terms of the graph dimension, in which we improve upon previous bounds. (iv) A dichotomy for sample compression in multiclass categorization problems: If a non-trivial compression exists then a compression of logarithmic size exists. (v) A compactness theorem for multiclass categorization problems. version:1
arxiv-1610-03585 | A Paradigm for Situated and Goal-Driven Language Learning | http://arxiv.org/abs/1610.03585 | id:1610.03585 author:Jon Gauthier, Igor Mordatch category:cs.CL  published:2016-10-12 summary:A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans. version:1
arxiv-1610-03577 | Minimax Filter: Learning to Preserve Privacy from Inference Attacks | http://arxiv.org/abs/1610.03577 | id:1610.03577 author:Jihun Hamm category:cs.LG  published:2016-10-12 summary:Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differential privacy, which provides a more formal definition of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform the optimization. In addition, the paper proposes an extension that combines minimax filter and differentially-private noisy mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower inference accuracy, often significantly lower than previous methods. version:1
arxiv-1610-03548 | Visual Place Recognition with Probabilistic Vertex Voting | http://arxiv.org/abs/1610.03548 | id:1610.03548 author:Mathias Gehrig, Elena Stumm, Timo Hinzmann, Roland Siegwart category:cs.RO cs.CV  published:2016-10-11 summary:We propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation. Based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model, loop closures can be detected with high precision. By casting the problem into a probabilistic framework, we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places. We present methods for both a 2D-2D pose-graph vertex matching and a 2D-3D landmark matching based on the above scoring. The approach maintains accuracy while being efficient enough for online application through the use of compact (low dimensional) descriptors and fast nearest neighbor retrieval techniques. The proposed methods are evaluated on several challenging datasets in varied environments, showing state-of-the-art results with high precision and high recall. version:1
arxiv-1610-03518 | Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model | http://arxiv.org/abs/1610.03518 | id:1610.03518 author:Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, Wojciech Zaremba category:cs.RO cs.AI cs.LG cs.SY  published:2016-10-11 summary:Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation. version:1
arxiv-1610-03483 | Learning in Implicit Generative Models | http://arxiv.org/abs/1610.03483 | id:1610.03483 author:Shakir Mohamed, Balaji Lakshminarayanan category:stat.ML cs.LG stat.CO  published:2016-10-11 summary:Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. version:1
arxiv-1610-03467 | Deep Learning Assessment of Tumor Proliferation in Breast Cancer Histological Images | http://arxiv.org/abs/1610.03467 | id:1610.03467 author:Manan Shah, Christopher Rubadue, David Suster, Dayong Wang category:cs.CV  published:2016-10-11 summary:Current analysis of tumor proliferation, the most salient prognostic biomarker for invasive breast cancer, is limited to subjective mitosis counting by pathologists in localized regions of tissue images. This study presents the first data-driven integrative approach to characterize the severity of tumor growth and spread on a categorical and molecular level, utilizing multiple biologically salient deep learning classifiers to develop a comprehensive prognostic model. Our approach achieves pathologist-level performance on three-class categorical tumor severity prediction. It additionally pioneers prediction of molecular expression data from a tissue image, obtaining a Spearman's rank correlation coefficient of 0.60 with ex vivo mean calculated RNA expression. Furthermore, our framework is applied to identify over two hundred unprecedented biomarkers critical to the accurate assessment of tumor proliferation, validating our proposed integrative pipeline as the first to holistically and objectively analyze histopathological images. version:1
arxiv-1610-03466 | Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection | http://arxiv.org/abs/1610.03466 | id:1610.03466 author:Xianzhi Du, Mostafa El-Khamy, Jungwon Lee, Larry S. Davis category:cs.CV  published:2016-10-11 summary:We propose a deep neural network fusion architecture for fast and robust pedestrian detection. The proposed network fusion architecture allows for parallel processing of multiple networks for speed. A single shot deep convolutional network is trained as a object detector to generate all possible pedestrian candidates of different sizes and occlusions. This network outputs a large variety of pedestrian candidates to cover the majority of ground-truth pedestrians while also introducing a large number of false positives. Next, multiple deep neural networks are used in parallel for further refinement of these pedestrian candidates. We introduce a soft-rejection based network fusion method to fuse the soft metrics from all networks together to generate the final confidence scores. Our method performs better than existing state-of-the-arts, especially when detecting small-size and occluded pedestrians. Furthermore, we propose a method for integrating pixel-wise semantic segmentation network into the network fusion architecture as a reinforcement to the pedestrian detector. The approach outperforms state-of-the-art methods on most protocols on Caltech Pedestrian dataset, with significant boosts on several protocols. It is also faster than all other methods. version:1
arxiv-1610-03454 | Deep Variational Canonical Correlation Analysis | http://arxiv.org/abs/1610.03454 | id:1610.03454 author:Weiran Wang, Honglak Lee, Karen Livescu category:cs.LG  published:2016-10-11 summary:We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~\citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Marginal data likelihood as well as inference are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~\citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the "common variables" underlying both views, extract the "private variables" within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision. version:1
arxiv-1610-03437 | Restoring STM images via Sparse Coding: noise and artifact removal | http://arxiv.org/abs/1610.03437 | id:1610.03437 author:João P. Oliveira, Ana Bragança, José Bioucas-Dias, Mário Figueiredo, Luís Alcácer, Jorge Morgado, Quirina Ferreira category:cs.CV  published:2016-10-11 summary:In this article, we present a denoising algorithm to improve the interpretation and quality of scanning tunneling microscopy (STM) images. Given the high level of self-similarity of STM images, we propose a denoising algorithm by reformulating the true estimation problem as a sparse regression, often termed sparse coding. We introduce modifications to the algorithm to cope with the existence of artifacts, mainly dropouts, which appear in a structured way as consecutive line segments on the scanning direction. The resulting algorithm treats the artifacts as missing data, and the estimated values outperform those algorithms that substitute the outliers by a local filtering. We provide code implementations for both Matlab and Gwyddion. version:1
arxiv-1610-02357 | Xception: Deep Learning with Depthwise Separable Convolutions | http://arxiv.org/abs/1610.02357 | id:1610.02357 author:François Chollet category:cs.CV  published:2016-10-07 summary:We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the \textit{depthwise separable convolution} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. version:2
arxiv-1610-03425 | Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach | http://arxiv.org/abs/1610.03425 | id:1610.03425 author:John Duchi, Peter Glynn, Hongseok Namkoong category:stat.ML  published:2016-10-11 summary:We study statistical inference and robust solution methods for stochastic optimization problems. We first develop an generalized empirical likelihood framework for stochastic optimization. We show an empirical likelihood theory for Hadamard differentiable functionals with general $f$-divergences and give conditions under which $T(P) = \inf_{x\in\mathcal{X}} \mathbb{E}_{P}[l(x; \xi)]$ is Hadamard differentiable. Noting that the right endpoint of the generalized empirical likelihood confidence interval is a distributionally robust optimization problem with uncertainty regions given by $f$-divergences, we show various statistical properties of robust optimization. First, we give a statistically principled method of choosing the size of the uncertainty set to obtain a \texit{calibrated} one-sided confidence interval. Next, we prove an asymptotic expansion for the robust formulation, showing how robustification is a variance regularization. Finally, we show that robust solutions are consistent under (essentially) identical conditions as that required for sample average approximations. version:1
arxiv-1610-03414 | Maximum entropy models capture melodic styles | http://arxiv.org/abs/1610.03414 | id:1610.03414 author:Jason Sakellariou, Francesca Tria, Vittorio Loreto, François Pachet category:stat.ML cs.LG  published:2016-10-11 summary:We introduce a Maximum Entropy model able to capture the statistics of melodies in music. The model can be used to generate new melodies that emulate the style of the musical corpus which was used to train it. Instead of using the $n-$body interactions of $(n-1)-$order Markov models, traditionally used in automatic music generation, we use a $k-$nearest neighbour model with pairwise interactions only. In that way, we keep the number of parameters low and avoid over-fitting problems typical of Markov models. We show that long-range musical phrases don't need to be explicitly enforced using high-order Markov interactions, but can instead emerge from multiple, competing, pairwise interactions. We validate our Maximum Entropy model by contrasting how much the generated sequences capture the style of the original corpus without plagiarizing it. To this end we use a data-compression approach to discriminate the levels of borrowing and innovation featured by the artificial sequences. The results show that our modelling scheme outperforms both fixed-order and variable-order Markov models. This shows that, despite being based only on pairwise interactions, this Maximum Entropy scheme opens the possibility to generate musically sensible alterations of the original phrases, providing a way to generate innovation. version:1
arxiv-1610-03393 | Crossing the Road Without Traffic Lights: An Android-based Safety Device | http://arxiv.org/abs/1610.03393 | id:1610.03393 author:Adi Perry, Dor Verbin, Nahum Kiryati category:cs.CV  published:2016-10-11 summary:In the absence of pedestrian crossing lights, finding a safe moment to cross the road is often hazardous and challenging, especially for people with visual impairments. We present a reliable low-cost solution, an Android device attached to a traffic sign or lighting pole near the crossing, indicating whether it is safe to cross the road. The indication can be by sound, display, vibration, and various communication modalities provided by the Android device. The integral system camera is aimed at approaching traffic. Optical flow is computed from the incoming video stream, and projected onto an influx map, automatically acquired during a brief training period. The crossing safety is determined based on a 1-dimensional temporal signal derived from the projection. We implemented the complete system on a Samsung Galaxy K-Zoom Android smartphone, and obtained real-time operation. The system achieves promising experimental results, providing pedestrians with sufficiently early warning of approaching vehicles. The system can serve as a stand-alone safety device, that can be installed where pedestrian crossing lights are ruled out. Requiring no dedicated infrastructure, it can be powered by a solar panel and remotely maintained via the cellular network. version:1
arxiv-1610-03378 | Machine learning applied to single-shot x-ray diagnostics in an XFEL | http://arxiv.org/abs/1610.03378 | id:1610.03378 author:A. Sanchez-Gonzalez, P. Micaelli, C. Olivier, T. R. Barillot, M. Ilchen, A. A. Lutman, A. Marinelli, T. Maxwell, A. Achner, M. Agåker, N. Berrah, C. Bostedt, J. Buck, P. H. Bucksbaum, S. Carron Montero, B. Cooper, J. P. Cryan, M. Dong, R. Feifel, L. J. Frasinski, H. Fukuzawa, A. Galler, G. Hartmann, N. Hartmann, W. Helml, A. S. Johnson, A. Knie, A. O. Lindahl, J. Liu, K. Motomura, M. Mucke, C. O'Grady, J-E. Rubensson, E. R. Simpson, R. J. Squibb, C. Såthe, K. Ueda, M. Vacher, D. J. Walke, V. Zhaunerchyk, R. N. Coffee, J. P. Marangos category:physics.data-an physics.acc-ph stat.ML  published:2016-10-11 summary:X-ray free-electron lasers (XFELs) are the only sources currently able to produce bright few-fs pulses with tunable photon energies from 100 eV to more than 10 keV. Due to the stochastic SASE operating principles and other technical issues the output pulses are subject to large fluctuations, making it necessary to characterize the x-ray pulses on every shot for data sorting purposes. We present a technique that applies machine learning tools to predict x-ray pulse properties using simple electron beam and x-ray parameters as input. Using this technique at the Linac Coherent Light Source (LCLS), we report mean errors below 0.3 eV for the prediction of the photon energy at 530 eV and below 1.6 fs for the prediction of the delay between two x-ray pulses. We also demonstrate spectral shape prediction with a mean agreement of 97%. This approach could potentially be used at the next generation of high-repetition-rate XFELs to provide accurate knowledge of complex x-ray pulses at the full repetition rate. version:1
arxiv-1610-03368 | DOTmark - A Benchmark for Discrete Optimal Transport | http://arxiv.org/abs/1610.03368 | id:1610.03368 author:Jörn Schrieber, Dominic Schuhmacher, Carsten Gottschlich category:math.OC cs.CV  published:2016-10-11 summary:The Wasserstein metric or earth mover's distance (EMD) is a useful tool in statistics, machine learning and computer science with many applications to biological or medical imaging, among others. Especially in the light of increasingly complex data, the computation of these distances via optimal transport is often the limiting factor. Inspired by this challenge, a variety of new approaches to optimal transport has been proposed in recent years and along with these new methods comes the need for a meaningful comparison. In this paper, we introduce a benchmark for discrete optimal transport, called DOTmark, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy. Along with the DOTmark we present a survey and a performance test for a cross section of established methods ranging from more traditional algorithms, such as the transportation simplex, to recently developed approaches, such as the shielding neighborhood method, and including also a comparison with commercial solvers. version:1
arxiv-1610-03349 | Survey on the Use of Typological Information in Natural Language Processing | http://arxiv.org/abs/1610.03349 | id:1610.03349 author:Helen O'Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Anna Korhonen category:cs.CL  published:2016-10-11 summary:In recent years linguistic typology, which classifies the world's languages according to their functional and structural properties, has been widely used to support multilingual NLP. While the growing importance of typological information in supporting multilingual tasks has been recognised, no systematic survey of existing typological resources and their use in NLP has been published. This paper provides such a survey as well as discussion which we hope will both inform and inspire future work in the area. version:1
arxiv-1610-03342 | From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning | http://arxiv.org/abs/1610.03342 | id:1610.03342 author:Lieke Gelderloos, Grzegorz Chrupała category:cs.CL cs.LG  published:2016-10-11 summary:We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning. version:1
arxiv-1610-03321 | Keystroke dynamics as signal for shallow syntactic parsing | http://arxiv.org/abs/1610.03321 | id:1610.03321 author:Barbara Plank category:cs.CL  published:2016-10-11 summary:Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models? We postulate that keystroke dynamics contain information about syntactic structure that can inform shallow syntactic parsing. To test this hypothesis, we explore labels derived from keystroke logs as auxiliary task in a multi-task bidirectional Long Short-Term Memory (bi-LSTM). Our results show promising results on two shallow syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that data can come from distinct sources, and produces models that are significantly better than models trained on the text annotations alone. version:1
arxiv-1610-03317 | A Greedy Approach for Budgeted Maximum Inner Product Search | http://arxiv.org/abs/1610.03317 | id:1610.03317 author:Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, Inderjit S. Dhillon category:cs.DS cs.LG  published:2016-10-11 summary:Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75\%. version:1
arxiv-1610-03295 | Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving | http://arxiv.org/abs/1610.03295 | id:1610.03295 author:Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua category:cs.AI cs.LG stat.ML  published:2016-10-11 summary:Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further. version:1
arxiv-1610-03276 | Assisted Dictionary Learning for fMRI Data Analysis | http://arxiv.org/abs/1610.03276 | id:1610.03276 author:Manuel Morante Moreno, Yannis Kopsinis, Eleftherios Kofidis, Christos Chatzichristos, Sergios Theodoridis category:stat.ML  published:2016-10-11 summary:Extracting information from functional magnetic resonance (fMRI) images has been a major area of research for more than two decades. The goal of this work is to present a new method for the analysis of fMRI data sets, that is capable to incorporate a priori available information, via an efficient optimization framework. Tests on synthetic data sets demonstrate significant performance gains over existing methods of this kind. version:1
arxiv-1610-03263 | Error Asymmetry in Causal and Anticausal Regression | http://arxiv.org/abs/1610.03263 | id:1610.03263 author:Patrick Blöbaum, Takashi Washio, Shohei Shimizu category:cs.AI cs.LG stat.ML  published:2016-10-11 summary:It is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated. Recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings. Assuming an additive noise and an independence between data generating mechanism and its input, we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error. We formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and, on the contrary, greater when the cause is predicted from its effect. The theorem implies an asymmetry in the error depending on the prediction direction. This is further corroborated with empirical evaluations in artificial and real-world data sets. version:1
arxiv-1610-03256 | GMM-Free Flat Start Sequence-Discriminative DNN Training | http://arxiv.org/abs/1610.03256 | id:1610.03256 author:Gábor Gosztolya, Tamás Grósz, László Tóth category:cs.CL  published:2016-10-11 summary:Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequence-discriminative training criterion for flat start. While sequence-discriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well. version:1
arxiv-1610-03246 | Toward a new instances of NELL | http://arxiv.org/abs/1610.03246 | id:1610.03246 author:Maisa C. Duarte, Pierre Maret category:cs.CL  published:2016-10-11 summary:We are developing the method to start new instances of NELL in various languages and develop then NELL multilingualism. We base our method on our experience on NELL Portuguese and NELL French. This reports explain our method and develops some research perspectives. version:1
arxiv-1610-03167 | An Empirical Exploration of Skip Connections for Sequential Tagging | http://arxiv.org/abs/1610.03167 | id:1610.03167 author:Huijia Wu, Jiajun Zhang, Chengqing Zong category:cs.CL  published:2016-10-11 summary:In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging. version:1
arxiv-1610-03165 | Long Short-Term Memory based Convolutional Recurrent Neural Networks for Large Vocabulary Speech Recognition | http://arxiv.org/abs/1610.03165 | id:1610.03165 author:Xiangang Li, Xihong Wu category:cs.CL cs.NE  published:2016-10-11 summary:Long short-term memory (LSTM) recurrent neural networks (RNNs) have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks (CNNs) have brought significant improvements to deep feed-forward neural networks (FFNNs), as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed state-of-the-art speech recognition performance. version:1
arxiv-1610-03164 | Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation | http://arxiv.org/abs/1610.03164 | id:1610.03164 author:Andrea F. Daniele, Mohit Bansal, Matthew R. Walter category:cs.RO cs.AI cs.CL cs.LG  published:2016-10-11 summary:Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a navigational guide model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then "translate" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% when compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants that demonstrate that our method generates instructions that people follow as accurately and easily as those produced by humans. version:1
arxiv-1610-03155 | Multiple Instance Learning Convolutional Neural Networks for Object Recognition | http://arxiv.org/abs/1610.03155 | id:1610.03155 author:Miao Sun, Tony X. Han, Ming-Chang Liu, Ahmad Khodayari-Rostamabad category:cs.CV  published:2016-10-11 summary:Convolutional Neural Networks (CNN) have demon- strated its successful applications in computer vision, speech recognition, and natural language processing. For object recog- nition, CNNs might be limited by its strict label requirement and an implicit assumption that images are supposed to be target- object-dominated for optimal solutions. However, the labeling procedure, necessitating laying out the locations of target ob- jects, is very tedious, making high-quality large-scale dataset prohibitively expensive. Data augmentation schemes are widely used when deep networks suffer the insufficient training data problem. All the images produced through data augmentation share the same label, which may be problematic since not all data augmentation methods are label-preserving. In this paper, we propose a weakly supervised CNN framework named Multiple Instance Learning Convolutional Neural Networks (MILCNN) to solve this problem. We apply MILCNN framework to object recognition and report state-of-the-art performance on three benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2015 classification dataset. version:1
arxiv-1610-03151 | FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in Virtual Reality | http://arxiv.org/abs/1610.03151 | id:1610.03151 author:Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, Matthias Nießner category:cs.CV  published:2016-10-11 summary:We introduce FaceVR, a novel method for gaze-aware facial reenactment in the Virtual Reality (VR) context. The key component of FaceVR is a robust algorithm to perform real-time facial motion capture of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven approach for eye tracking from monocular videos. In addition to these face reconstruction components, FaceVR incorporates photo-realistic re-rendering in real time, thus allowing artificial modifications of face and eye appearances. For instance, we can alter facial expressions, change gaze directions, or remove the VR goggles in realistic re-renderings. In a live setup with a source and a target actor, we apply these newly-introduced algorithmic components. We assume that the source actor is wearing a VR device, and we capture his facial expressions and eye movement in real-time. For the target video, we mimic a similar tracking process; however, we use the source input to drive the animations of the target video, thus enabling gaze-aware facial reenactment. To render the modified target video on a stereo display, we augment our capture and reconstruction process with stereo data. In the end, FaceVR produces compelling results for a variety of applications, such as gaze-aware facial reenactment, reenactment in virtual reality, removal of VR goggles, and re-targeting of somebody's gaze direction in a video conferencing call. version:1
arxiv-1610-03129 | Tangled Splines | http://arxiv.org/abs/1610.03129 | id:1610.03129 author:Aditya Tatu category:cs.CV cs.CG  published:2016-10-10 summary:Extracting shape information from object bound- aries is a well studied problem in vision, and has found tremen- dous use in applications like object recognition. Conversely, studying the space of shapes represented by curves satisfying certain constraints is also intriguing. In this paper, we model and analyze the space of shapes represented by a 3D curve (space curve) formed by connecting n pieces of quarter of a unit circle. Such a space curve is what we call a Tangle, the name coming from a toy built on the same principle. We provide two models for the shape space of n-link open and closed tangles, and we show that tangles are a subset of trigonometric splines of a certain order. We give algorithms for curve approximation using open/closed tangles, computing geodesics on these shape spaces, and to find the deformation that takes one given tangle to another given tangle, i.e., the Log map. The algorithms provided yield tangles upto a small and acceptable tolerance, as shown by the results given in the paper. version:1
arxiv-1610-03120 | Correlation-Based Method for Sentiment Classification | http://arxiv.org/abs/1610.03120 | id:1610.03120 author:Hussam Hamdan category:cs.CL cs.IR  published:2016-10-10 summary:The classic supervised classification algorithms are efficient, but time-consuming, complicated and not interpretable, which makes it difficult to analyze their results that limits the possibility to improve them based on real observations. In this paper, we propose a new and a simple classifier to predict a sentiment label of a short text. This model keeps the capacity of human interpret-ability and can be extended to integrate NLP techniques in a more interpretable way. Our model is based on a correlation metric which measures the degree of association between a sentiment label and a word. Ten correlation metrics are proposed and evaluated intrinsically. And then a classifier based on each metric is proposed, evaluated and compared to the classic classification algorithms which have proved their performance in many studies. Our model outperforms these algorithms with several correlation metrics. version:1
arxiv-1610-03113 | Truncated Variational Expectation Maximization | http://arxiv.org/abs/1610.03113 | id:1610.03113 author:Jörg Lücke category:stat.ML  published:2016-10-10 summary:We derive a novel variational expectation maximization approach based on truncated variational distributions. The truncated distributions are proportional to exact posteriors in a subset of a discrete state space and equal zero otherwise. In contrast to factored variational approximations or Gaussian approximations, truncated approximations neither assume posterior independence nor mono-modal posteriors. The novel variational approach is closely related to Expectation Truncation (L\"ucke and Eggert, 2010) - a preselection based EM approximation. It shares with Expectation Truncation the central idea of truncated distributions and the application domain of discrete hidden variables. In contrast to Expectation Truncation we here show how truncated distributions can be included into the theoretical framework of variational EM approximations. A fully variational treatment of truncated distributions then allows for derivations of novel general and mathematically grounded results, which in turn can be used to formulate novel efficient algorithms for parameter optimization of probabilistic data models. Apart from showing that truncated distributions are fully consistent with the variational free-energy framework, we find the free-energy that corresponds to truncated distributions to be given by compact and efficiently computable expressions, while update equations for model parameters (M-steps) remain in their standard form. Furthermore, expectation values w.r.t. truncated distributions are given in a generic form. Based on these observations, we show how an efficient and easily applicable meta-algorithm can be formulated that guarantees a monotonic increase of the free-energy. More generally, the obtained variational framework developed here links variational E-steps to discrete optimization, and it provides a theoretical basis to tightly couple sampling and variational approaches. version:1
arxiv-1610-03112 | Leveraging Recurrent Neural Networks for Multimodal Recognition of Social Norm Violation in Dialog | http://arxiv.org/abs/1610.03112 | id:1610.03112 author:Tiancheng Zhao, Ran Zhao, Zhao Meng, Justine Cassell category:cs.CL  published:2016-10-10 summary:Social norms are shared rules that govern and facilitate social interaction. Violating such social norms via teasing and insults may serve to upend power imbalances or, on the contrary reinforce solidarity and rapport in conversation, rapport which is highly situated and context-dependent. In this work, we investigate the task of automatically identifying the phenomena of social norm violation in discourse. Towards this goal, we leverage the power of recurrent neural networks and multimodal information present in the interaction, and propose a predictive model to recognize social norm violation. Using long-term temporal and contextual information, our model achieves an F1 score of 0.705. Implications of our work regarding developing a social-aware agent are discussed. version:1
arxiv-1610-03106 | Supervised Term Weighting Metrics for Sentiment Analysis in Short Text | http://arxiv.org/abs/1610.03106 | id:1610.03106 author:Hussam Hamdan, Patrice Bellot, Frederic Bechet category:cs.CL cs.IR cs.LG  published:2016-10-10 summary:Term weighting metrics assign weights to terms in order to discriminate the important terms from the less crucial ones. Due to this characteristic, these metrics have attracted growing attention in text classification and recently in sentiment analysis. Using the weights given by such metrics could lead to more accurate document representation which may improve the performance of the classification. While previous studies have focused on proposing or comparing different weighting metrics at two-classes document level sentiment analysis, this study propose to analyse the results given by each metric in order to find out the characteristics of good and bad weighting metrics. Therefore we present an empirical study of fifteen global supervised weighting metrics with four local weighting metrics adopted from information retrieval, we also give an analysis to understand the behavior of each metric by observing and analysing how each metric distributes the terms and deduce some characteristics which may distinguish the good and bad metrics. The evaluation has been done using Support Vector Machine on three different datasets: Twitter, restaurant and laptop reviews. version:1
arxiv-1610-03090 | Dynamic Metric Learning from Pairwise Comparisons | http://arxiv.org/abs/1610.03090 | id:1610.03090 author:Kristjan Greenewald, Stephen Kelley, Alfred Hero III category:cs.LG  published:2016-10-10 summary:Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retro-initialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, demonstrate RICE-OCELAD on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms. version:1
arxiv-1610-03045 | Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data | http://arxiv.org/abs/1610.03045 | id:1610.03045 author:Jialei Wang, Jason D. Lee, Mehrdad Mahdavi, Mladen Kolar, Nathan Srebro category:cs.LG math.OC stat.ML  published:2016-10-10 summary:Sketching techniques have become popular for scaling up machine learning algorithms by reducing the sample size or dimensionality of massive data sets, while still maintaining the statistical power of big data. In this paper, we study sketching from an optimization point of view: we first show that the iterative Hessian sketch is an optimization process with preconditioning, and develop accelerated iterative Hessian sketch via the searching the conjugate direction; we then establish primal-dual connections between the Hessian sketch and dual random projection, and apply the preconditioned conjugate gradient approach on the dual problem, which leads to the accelerated iterative dual random projection methods. Finally to tackle the challenges from both large sample size and high-dimensionality, we propose the primal-dual sketch, which iteratively sketches the primal and dual formulations. We show that using a logarithmic number of calls to solvers of small scale problem, primal-dual sketch is able to recover the optimum of the original problem up to arbitrary precision. The proposed algorithms are validated via extensive experiments on synthetic and real data sets which complements our theoretical results. version:1
arxiv-1610-03023 | Deep feature representations for high-resolution remote-sensing imagery retrieval | http://arxiv.org/abs/1610.03023 | id:1610.03023 author:Weixun Zhou, Congmin Li category:cs.CV  published:2016-10-10 summary:In this letter, we investigate how to extract deep feature representations based on convolutional neural networks (CNN) for high-resolution remote-sensing imagery retrieval (HRRSIR). Two effective schemes are proposed to generate the final feature rep-resentations for similarity measure. In the first scheme, the deep features are extracted from the fully-connected and convolutional layers of the pre-trained CNN models, respectively; in the second scheme, we fine-tune the pre-trained CNN model using the target remote sensing dataset to learn dataset-specific features. The deep feature representations generated by the two schemes are evalu-ated on two public and challenging datasets. The experimental results indicate that the proposed schemes are able to achieve state-of-the-art performance due to the good transferability of the CNN models. version:1
arxiv-1610-03022 | Very Deep Convolutional Networks for End-to-End Speech Recognition | http://arxiv.org/abs/1610.03022 | id:1610.03022 author:Yu Zhang, William Chan, Navdeep Jaitly category:cs.CL  published:2016-10-10 summary:Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5\% word error rate without any dictionary or language using a 15 layer deep network. version:1
arxiv-1610-03009 | Investigation of Synthetic Speech Detection Using Frame- and Segment-Specific Importance Weighting | http://arxiv.org/abs/1610.03009 | id:1610.03009 author:Ali Khodabakhsh, Cenk Demiroglu category:cs.SD cs.CL  published:2016-10-10 summary:Speaker verification systems are vulnerable to spoofing attacks which presents a major problem in their real-life deployment. To date, most of the proposed synthetic speech detectors (SSDs) have weighted the importance of different segments of speech equally. However, different attack methods have different strengths and weaknesses and the traces that they leave may be short or long term acoustic artifacts. Moreover, those may occur for only particular phonemes or sounds. Here, we propose three algorithms that weigh likelihood-ratio scores of individual frames, phonemes, and sound-classes depending on their importance for the SSD. Significant improvement over the baseline system has been obtained for known attack methods that were used in training the SSDs. However, improvement with unknown attack types was not substantial. Thus, the type of distortions that were caused by the unknown systems were different and could not be captured better with the proposed SSD compared to the baseline SSD. version:1
arxiv-1610-02995 | Extrapolation and learning equations | http://arxiv.org/abs/1610.02995 | id:1610.02995 author:Georg Martius, Christoph H. Lampert category:cs.LG cs.AI I.2.6; I.2.8  published:2016-10-10 summary:In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified. version:1
arxiv-1610-02987 | Linear Hypothesis Testing in Dense High-Dimensional Linear Models | http://arxiv.org/abs/1610.02987 | id:1610.02987 author:Yinchu Zhu, Jelena Bradic category:stat.ME math.ST stat.ML stat.TH  published:2016-10-10 summary:Providing asymptotically valid methods for testing general linear functions of the regression parameters in high-dimensional models is extremely challenging -- especially without making restrictive or unverifiable assumptions on the number of non-zero elements, i.e., the model sparsity. In this article, we propose a new methodology that transforms the original hypothesis into a moment condition and demonstrate that valid tests can be created without making any assumptions on the model sparsity. We formulate a restructured regression problem with the new features synthesized according to the null hypothesis directly; further, with the help of such new features, we have designed a valid test for the transformed moment condition. This construction enables us to test the null hypothesis, even if the original model cannot be estimated well. Although the linear tests in high dimensions are by nature very difficult to analyze we establish theoretical guarantees for Type I error control, allowing both the model and the vector representing the hypothesis to be non-sparse. The assumptions that are necessary to establish Type I error guarantees are shown to be weaker than the weakest known assumptions that are necessary to construct confidence intervals in the high-dimensional regression. Our methods are also shown to achieve certain optimality in detecting deviations from the null hypothesis. We demonstrate favorable finite-sample performance of the proposed methods, via both a numerical and a real data example. version:1
arxiv-1610-02984 | Person Re-identification: Past, Present and Future | http://arxiv.org/abs/1610.02984 | id:1610.02984 author:Liang Zheng, Yi Yang, Alexander G. Hauptmann category:cs.CV  published:2016-10-10 summary:Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues. version:1
arxiv-1610-02962 | Low-rank Approximation and Dynamic Mode Decomposition | http://arxiv.org/abs/1610.02962 | id:1610.02962 author:Patrick Héas, Cédric Herzet category:stat.ML cs.NA  published:2016-10-10 summary:Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of non-linear systems from experimental datasets. Recently, several attempts have extended DMD to the context of low-rank approximations. This extension is of particular interest for reduced-order modeling in various applicative domains, e.g. for climate prediction, to study molecular dynamics or micro-electromechanical devices. This low-rank extension takes the form of a nonconvex optimization problem. To the best of our knowledge, only sub-optimal algorithms have been proposed in the literature to compute the solution of this problem. In this paper, we prove that there exists a closed-form optimal solution to this problem and design an effective algorithm to compute it based on Singular Value Decomposition (SVD). Based on this solution, we then propose efficient procedures for reduced-order modeling and for the identification of the the low-rank DMD modes and amplitudes. Experiments illustrates the gain in performance of the proposed algorithm compared to state-of-the-art techniques. version:1
arxiv-1610-02947 | Video Captioning and Retrieval Models with Semantic Attention | http://arxiv.org/abs/1610.02947 | id:1610.02947 author:Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim category:cs.CV  published:2016-10-10 summary:We present the approaches for the four video-to-language tasks of LSMDC 2016, including movie description, fill-in-the-blank, multiple-choice test, and movie retrieval. Our key idea is to adopt the semantic attention mechanism; we first build a set of attribute words that are consistently discovered on video frames, and then selectively fuse them with input words for more semantic representation and with output words for more accurate prediction. We show that our implementation of semantic attention indeed improves the performance of multiple video-to-language tasks. Specifically, the presented approaches participated in all the four tasks of the LSMDC 2016, and have won three of them, including fill-in-the-blank, multiple-choice test, and movie retrieval. version:1
arxiv-1610-02923 | EM-Based Mixture Models Applied to Video Event Detection | http://arxiv.org/abs/1610.02923 | id:1610.02923 author:Alessandra Martins Coelho, Vania V. Estrela category:cs.CV  published:2016-10-10 summary:Surveillance system (SS) development requires hi-tech support to prevail over the shortcomings related to the massive quantity of visual information from SSs. Anything but reduced human monitoring became impossible by means of its physical and economic implications, and an advance towards an automated surveillance becomes the only way out. When it comes to a computer vision system, automatic video event comprehension is a challenging task due to motion clutter, event understanding under complex scenes, multilevel semantic event inference, contextualization of events and views obtained from multiple cameras, unevenness of motion scales, shape changes, occlusions and object interactions among lots of other impairments. In recent years, state-of-the-art models for video event classification and recognition include modeling events to discern context, detecting incidents with only one camera, low-level feature extraction and description, high-level semantic event classification, and recognition. Even so, it is still very burdensome to recuperate or label a specific video part relying solely on its content. Principal component analysis (PCA) has been widely known and used, but when combined with other techniques such as the expectation-maximization (EM) algorithm its computation becomes more efficient. This chapter introduces advances associated with the concept of Probabilistic PCA (PPCA) analysis of video event and it also aims at looking closely to ways and metrics to evaluate these less intensive EM implementations of PCA and KPCA. version:1
arxiv-1610-02920 | Generative Adversarial Nets from a Density Ratio Estimation Perspective | http://arxiv.org/abs/1610.02920 | id:1610.02920 author:Masatoshi Uehara, Issei Sato, Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo category:stat.ML  published:2016-10-10 summary:Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. version:1
arxiv-1610-02918 | Phase transitions and optimal algorithms in high-dimensional Gaussian mixture clustering | http://arxiv.org/abs/1610.02918 | id:1610.02918 author:Thibault Lesieur, Caterina De Bacco, Jess Banks, Florent Krzakala, Cris Moore, Lenka Zdeborová category:stat.ML cond-mat.dis-nn cs.IT math.IT  published:2016-10-10 summary:We consider the problem of Gaussian mixture clustering in the high-dimensional limit where the data consists of $m$ points in $n$ dimensions, $n,m \rightarrow \infty$ and $\alpha = m/n$ stays finite. Using exact but non-rigorous methods from statistical physics, we determine the critical value of $\alpha$ and the distance between the clusters at which it becomes information-theoretically possible to reconstruct the membership into clusters better than chance. We also determine the accuracy achievable by the Bayes-optimal estimation algorithm. In particular, we find that when the number of clusters is sufficiently large, $r > 4 + 2 \sqrt{\alpha}$, there is a gap between the threshold for information-theoretically optimal performance and the threshold at which known algorithms succeed. version:1
arxiv-1610-02915 | Deep Pyramidal Residual Networks | http://arxiv.org/abs/1610.02915 | id:1610.02915 author:Dongyoon Han, Jiwhan Kim, Junmo Kim category:cs.CV  published:2016-10-10 summary:Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolution layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. At the same time, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the capability of high-level attributes. Moreover, this also applies to residual networks and is very closely related to their performance. In this research, instead of using downsampling to achieve a sharp increase at each residual unit, we gradually increase the feature map dimension at all the units to involve as many locations as possible. This is discussed in depth together with our new insights as it has proven to be an effective design to improve the generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR datasets have shown that our network architecture has a superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet version:1
arxiv-1610-02902 | Content Based Image Retrieval (CBIR) in Remote Clinical Diagnosis and Healthcare | http://arxiv.org/abs/1610.02902 | id:1610.02902 author:Albany E. Herrmann, Vania Vieira Estrela category:cs.CV  published:2016-10-10 summary:Content-Based Image Retrieval (CBIR) locates, retrieves and displays images alike to one given as a query, using a set of features. It demands accessible data in medical archives and from medical equipment, to infer meaning after some processing. A problem similar in some sense to the target image can aid clinicians. CBIR complements text-based retrieval and improves evidence-based diagnosis, administration, teaching, and research in healthcare. It facilitates visual/automatic diagnosis and decision-making in real-time remote consultation/screening, store-and-forward tests, home care assistance and overall patient surveillance. Metrics help comparing visual data and improve diagnostic. Specially designed architectures can benefit from the application scenario. CBIR use calls for file storage standardization, querying procedures, efficient image transmission, realistic databases, global availability, access simplicity, and Internet-based structures. This chapter recommends important and complex aspects required to handle visual content in healthcare. version:1
arxiv-1610-02876 | Heuristic Approaches for Generating Local Process Models through Log Projections | http://arxiv.org/abs/1610.02876 | id:1610.02876 author:Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst, Reinder Haakma category:cs.LG cs.AI cs.DB  published:2016-10-10 summary:Local Process Model (LPM) discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially, i.e. subsets of possible events are taken into account to create so-called local process models. Often such smaller models provide valuable insights into the behavior of the process, especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end. The practical application of LPM discovery is however hindered by computational issues in the case of logs with many activities (problems may already occur when there are more than 17 unique activities). In this paper, we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up LPM discovery considerably while still finding high-quality LPMs. We found that a Markov clustering approach to create projection sets results in the largest improvement of execution time, with discovered LPMs still being better than with the use of randomly generated activity sets of the same size. Another heuristic, based on log entropy, yields a more moderate speedup, but enables the discovery of higher quality LPMs. The third heuristic, based on the relative information gain, shows unstable performance: for some data sets the speedup and LPM quality are higher than with the log entropy based method, while for other data sets there is no speedup at all. version:1
arxiv-1610-02850 | Impatient DNNs - Deep Neural Networks with Dynamic Time Budgets | http://arxiv.org/abs/1610.02850 | id:1610.02850 author:Manuel Amthor, Erik Rodner, Joachim Denzler category:cs.CV  published:2016-10-10 summary:We propose Impatient Deep Neural Networks (DNNs) which deal with dynamic time budgets during application. They allow for individual budgets given a priori for each test example and for anytime prediction, i.e., a possible interruption at multiple stages during inference while still providing output estimates. Our approach can therefore tackle the computational costs and energy demands of DNNs in an adaptive manner, a property essential for real-time applications. Our Impatient DNNs are based on a new general framework of learning dynamic budget predictors using risk minimization, which can be applied to current DNN architectures by adding early prediction and additional loss layers. A key aspect of our method is that all of the intermediate predictors are learned jointly. In experiments, we evaluate our approach for different budget distributions, architectures, and datasets. Our results show a significant gain in expected accuracy compared to common baselines. version:1
arxiv-1610-02828 | Ranking academic institutions on potential paper acceptance in upcoming conferences | http://arxiv.org/abs/1610.02828 | id:1610.02828 author:Jobin Wilson, Ram Mohan, Muhammad Arif, Santanu Chaudhury, Brejesh Lall category:cs.AI cs.DL cs.LG  published:2016-10-10 summary:The crux of the problem in KDD Cup 2016 involves developing data mining techniques to rank research institutions based on publications. Rank importance of research institutions are derived from predictions on the number of full research papers that would potentially get accepted in upcoming top-tier conferences, utilizing public information on the web. This paper describes our solution to KDD Cup 2016. We used a two step approach in which we first identify full research papers corresponding to each conference of interest and then train two variants of exponential smoothing models to make predictions. Our solution achieves an overall score of 0.7508, while the winning submission scored 0.7656 in the overall results. version:1
arxiv-1610-02806 | Modelling Sentence Pairs with Tree-structured Attentive Encoder | http://arxiv.org/abs/1610.02806 | id:1610.02806 author:Yao Zhou, Cong Liu, Yan Pan category:cs.CL  published:2016-10-10 summary:We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks. version:1
arxiv-1610-02762 | Matching of Images with Rotation Transformation Based on the Virtual Electromagnetic Interaction | http://arxiv.org/abs/1610.02762 | id:1610.02762 author:Xiaodong Zhuang, N. E. Mastorakis category:cs.CV  published:2016-10-10 summary:A novel approach of image matching for rotating transformation is presented and studied. The approach is inspired by electromagnetic interaction force between physical currents. The virtual current in images is proposed based on the significant edge lines extracted as the fundamental structural feature of images. The virtual electromagnetic force and the corresponding moment is studied between two images after the extraction of the virtual currents in the images. Then image matching for rotating transformation is implemented by exploiting the interaction between the virtual currents in the two images to be matched. The experimental results prove the effectiveness of the novel idea, which indicates the promising application of the proposed method in image registration. version:1
arxiv-1610-02760 | Image Segmentation Based on the Self-Balancing Mechanism in Virtual 3D Elastic Mesh | http://arxiv.org/abs/1610.02760 | id:1610.02760 author:Xiaodong Zhuang, N. E. Mastorakis, Jieru Chi, Hanping Wang category:cs.CV  published:2016-10-10 summary:In this paper, a novel model of 3D elastic mesh is presented for image segmentation. The model is inspired by stress and strain in physical elastic objects, while the repulsive force and elastic force in the model are defined slightly different from the physical force to suit the segmentation problem well. The self-balancing mechanism in the model guarantees the stability of the method in segmentation. The shape of the elastic mesh at balance state is used for region segmentation, in which the sign distribution of the points'z coordinate values is taken as the basis for segmentation. The effectiveness of the proposed method is proved by analysis and experimental results for both test images and real world images. version:1
arxiv-1610-02758 | Stochastic Alternating Direction Method of Multipliers with Variance Reduction for Nonconvex Optimization | http://arxiv.org/abs/1610.02758 | id:1610.02758 author:Feihu Huang, Songcan Chen, Zhaosong Lu category:math.OC stat.ML  published:2016-10-10 summary:In this work, we study the stochastic alternating direction method of multipliers (ADMM) for optimizing nonconvex problems, and propose two classes of nonconvex stochastic ADMM with variance reduction. The first class is the nonconvex stochastic variance reduced gradient ADMM (SVRG-ADMM), which uses a multi-stage strategy to progressively reduce the variance of stochas- tic gradients. The second is the nonconvex stochastic average gradient ADMM (SAG-ADMM), which additionally uses the old gradients estimated in the previous iteration to reduce the vari- ance of stochastic gradients. Theoretically, we analyze the convergence of the SVRG-ADMM and SAG-ADMM, and prove that they enjoy the iteration complexity bound of O(1/e) to reach an e-stationary solution. In addition, we prove that the simple stochastic ADMM (S-ADMM), in which the variance of the stochastic gradients is free, is divergent under some conditions. Finally, the experimental results on some real datasets demonstrate our theoretical results. To the best of our knowledge, this is the first study of convergence and iteration complexity of the stochastic ADMM for the noncovex problems. version:1
arxiv-1610-02757 | Dataiku's Solution to SPHERE's Activity Recognition Challenge | http://arxiv.org/abs/1610.02757 | id:1610.02757 author:Maxime Voisin, Leo Dreyfus-Schmidt, Pierre Gutierrez, Samuel Ronsin, Marc Beillevaire category:stat.ML cs.LG  published:2016-10-10 summary:Our team won the second prize of the Safe Aging with SPHERE Challenge organized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of the competition was to recognize activities performed by humans, using sensor data. This paper presents our solution. It is based on a rich pre-processing and state of the art machine learning methods. From the raw train data, we generate a synthetic train set with the same statistical characteristics as the test set. We then perform feature engineering. The machine learning modeling part is based on stacking weak learners through a grid searched XGBoost algorithm. Finally, we use post-processing to smooth our predictions over time. version:1
arxiv-1610-02751 | A New Theoretical and Technological System of Imprecise-Information Processing | http://arxiv.org/abs/1610.02751 | id:1610.02751 author:Shiyou Lian category:cs.CL cs.AI  published:2016-10-10 summary:Imprecise-information processing will play an indispensable role in intelligent systems, especially in the anthropomorphic intelligent systems (as intelligent robots). A new theoretical and technological system of imprecise-information processing has been founded in Principles of Imprecise-Information Processing: A New Theoretical and Technological System[1] which is different from fuzzy technology. The system has clear hierarchy and rigorous structure, which results from the formation principle of imprecise information and has solid mathematical and logical bases, and which has many advantages beyond fuzzy technology. The system provides a technological platform for relevant applications and lays a theoretical foundation for further research. version:1
arxiv-1610-02749 | A Dynamic Window Neural Network for CCG Supertagging | http://arxiv.org/abs/1610.02749 | id:1610.02749 author:Huijia Wu, Jiajun Zhang, Chengqing Zong category:cs.CL  published:2016-10-10 summary:Combinatory Category Grammar (CCG) supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set. version:1
arxiv-1610-02736 | Emergence of linguistic laws in human voice | http://arxiv.org/abs/1610.02736 | id:1610.02736 author:Ivan Gonzalez Torre, Bartolo Luque, Lucas Lacasa, Jordi Luque, Antoni Hernandez-Fernandez category:physics.soc-ph cs.CL  published:2016-10-09 summary:Linguistic laws constitute one of the quantitative cornerstones of modern cognitive sciences and have been routinely investigated in written corpora, or in the equivalent transcription of oral corpora. This means that inferences of statistical patterns of language in acoustics are biased by the arbitrary, language-dependent segmentation of the signal, and virtually precludes the possibility of making comparative studies between human voice and other animal communication systems. Here we bridge this gap by proposing a method that allows to measure such patterns in acoustic signals of arbitrary origin, without needs to have access to the language corpus underneath. The method has been applied to six different human languages, recovering successfully some well-known laws of human communication at timescales even below the phoneme and finding yet another link between complexity and criticality in a biological system. These methods further pave the way for new comparative studies in animal communication or the analysis of signals of unknown code. version:1
arxiv-1610-02732 | Investigating the effects Diversity Mechanisms have on Evolutionary Algorithms in Dynamic Environments | http://arxiv.org/abs/1610.02732 | id:1610.02732 author:Matthew Hughes category:cs.NE  published:2016-10-09 summary:Evolutionary algorithms have been successfully applied to a variety of optimisation problems in stationary environments. However, many real world optimisation problems are set in dynamic environments where the success criteria shifts regularly. Population diversity affects algorithmic performance, particularly on multiobjective and dynamic problems. Diversity mechanisms are methods of altering evolutionary algorithms in a way that promotes the maintenance of population diversity. This project intends to measure and compare the performance effect a variety of diversity mechanisms have on an evolutionary algorithm when facing an assortment of dynamic problems. version:1
arxiv-1610-02714 | Egocentric Height Estimation | http://arxiv.org/abs/1610.02714 | id:1610.02714 author:Jessica Finocchiaro, Aisha Urooj Khan, Ali Borji category:cs.CV  published:2016-10-09 summary:Egocentric, or first-person vision which became popular in recent years with an emerge in wearable technology, is different than exocentric (third-person) vision in some distinguishable ways, one of which being that the camera wearer is generally not visible in the video frames. Recent work has been done on action and object recognition in egocentric videos, as well as work on biometric extraction from first-person videos. Height estimation can be a useful feature for both soft-biometrics and object tracking. Here, we propose a method of estimating the height of an egocentric camera without any calibration or reference points. We used both traditional computer vision approaches and deep learning in order to determine the visual cues that results in best height estimation. Here, we introduce a framework inspired by two stream networks comprising of two Convolutional Neural Networks, one based on spatial information, and one based on information given by optical flow in a frame. Given an egocentric video as an input to the framework, our model yields a height estimate as an output. We also incorporate late fusion to learn a combination of temporal and spatial cues. Comparing our model with other methods we used as baselines, we achieve height estimates for videos with a Mean Average Error of 14.04 cm over a range of 103 cm of data, and classification accuracy for relative height (tall, medium or short) up to 93.75% where chance level is 33%. version:1
arxiv-1610-02703 | Nonparametric Bayesian inference of the microcanonical stochastic block model | http://arxiv.org/abs/1610.02703 | id:1610.02703 author:Tiago P. Peixoto category:physics.data-an physics.soc-ph stat.ML  published:2016-10-09 summary:A principled approach to characterize the hidden modular structure of networks is to formulate generative models, and then infer their parameters from data. When the desired structure is composed of modules or "communities", a suitable choice for this task is the stochastic block model (SBM), where nodes are divided into groups, and the placement of edges is conditioned on the group memberships. Here, we present a nonparametric Bayesian method to infer the modular structure of empirical networks, including the number of modules and their hierarchical organization. We focus on a microcanonical variant of the SBM, where the structure is imposed via hard constraints. We show how this simple model variation allows simultaneously for two important improvements over more traditional inference approaches: 1. Deeper Bayesian hierarchies, with noninformative priors replaced by sequences of priors and hyperpriors, that not only remove limitations that seriously degrade the inference on large networks, but also reveal structures at multiple scales; 2. A very efficient inference algorithm that scales well not only for networks with a large number of nodes and edges, but also with an unlimited number of modules. We show also how this approach can be used to sample modular hierarchies from the posterior distribution, as well as to perform model selection. Furthermore, we expose a direct equivalence between our microcanonical approach and alternative derivations based on the canonical SBM. version:1
arxiv-1610-02692 | Open-Ended Visual Question-Answering | http://arxiv.org/abs/1610.02692 | id:1610.02692 author:Issey Masuda, Santiago Pascual de la Puente, Xavier Giro-i-Nieto category:cs.CL cs.CV cs.MM  published:2016-10-09 summary:This thesis report studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations. version:1
arxiv-1610-02683 | Interpreting Neural Networks to Improve Politeness Comprehension | http://arxiv.org/abs/1610.02683 | id:1610.02683 author:Malika Aubakirova, Mohit Bansal category:cs.CL cs.AI  published:2016-10-09 summary:We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks. version:1
arxiv-1610-03341 | Proposal for Automatic License and Number Plate Recognition System for Vehicle Identification | http://arxiv.org/abs/1610.03341 | id:1610.03341 author:Hamed Saghaei category:cs.CV  published:2016-10-09 summary:In this paper, we propose an automatic and mechanized license and number plate recognition (LNPR) system which can extract the license plate number of the vehicles passing through a given location using image processing algorithms. No additional devices such as GPS or radio frequency identification (RFID) need to be installed for implementing the proposed system. Using special cameras, the system takes pictures from each passing vehicle and forwards the image to the computer for being processed by the LPR software. Plate recognition software uses different algorithms such as localization, orientation, normalization, segmentation and finally optical character recognition (OCR). The resulting data is applied to compare with the records on a database. Experimental results reveal that the presented system successfully detects and recognizes the vehicle number plate on real images. This system can also be used for security and traffic control. version:1
arxiv-1610-02651 | Zero Shot Hashing | http://arxiv.org/abs/1610.02651 | id:1610.02651 author:Shubham Pachori, Shanmuganathan Raman category:cs.CV  published:2016-10-09 summary:This paper provides a framework to hash images containing instances of unknown object classes. In many object recognition problems, we might have access to huge amount of data. It may so happen that even this huge data doesn't cover the objects belonging to classes that we see in our day to day life. Zero shot learning exploits auxiliary information (also called as signatures) in order to predict the labels corresponding to unknown classes. In this work, we attempt to generate the hash codes for images belonging to unseen classes, information of which is available only through the textual corpus. We formulate this as an unsupervised hashing formulation as the exact labels are not available for the instances of unseen classes. We show that the proposed solution is able to generate hash codes which can predict labels corresponding to unseen classes with appreciably good precision. version:1
arxiv-1610-02649 | A new selection strategy for selective cluster ensemble based on Diversity and Independency | http://arxiv.org/abs/1610.02649 | id:1610.02649 author:Muhammad Yousefnezhad, Ali Reihanian, Daoqiang Zhang, Behrouz Minaei-Bidgoli category:stat.ML cs.AI cs.LG  published:2016-10-09 summary:This research introduces a new strategy in cluster ensemble selection by using Independency and Diversity metrics. In recent years, Diversity and Quality, which are two metrics in evaluation procedure, have been used for selecting basic clustering results in the cluster ensemble selection. Although quality can improve the final results in cluster ensemble, it cannot control the procedures of generating basic results, which causes a gap in prediction of the generated basic results' accuracy. Instead of quality, this paper introduces Independency as a supplementary method to be used in conjunction with Diversity. Therefore, this paper uses a heuristic metric, which is based on the procedure of converting code to graph in Software Testing, in order to calculate the Independency of two basic clustering algorithms. Moreover, a new modeling language, which we called as "Clustering Algorithms Independency Language" (CAIL), is introduced in order to generate graphs which depict Independency of algorithms. Also, Uniformity, which is a new similarity metric, has been introduced for evaluating the diversity of basic results. As a credential, our experimental results on varied different standard data sets show that the proposed framework improves the accuracy of final results dramatically in comparison with other cluster ensemble methods. version:1
arxiv-1610-07995 | Spatial Relationship Based Features for Indian Sign Language Recognition | http://arxiv.org/abs/1610.07995 | id:1610.07995 author:B. M. Chethana Kumara, H. S. Nagendraswamy, R. Lekha Chinmayi category:cs.CV  published:2016-10-09 summary:In this paper, the task of recognizing signs made by hearing impaired people at sentence level has been addressed. A novel method of extracting spatial features to capture hand movements of a signer has been proposed. Frames of a given video of a sign are preprocessed to extract face and hand components of a signer. The local centroids of the extracted components along with the global centroid are exploited to extract spatial features. The concept of interval valued type symbolic data has been explored to capture variations in the same sign made by the different signers at different instances of time. A suitable symbolic similarity measure is studied to establish matching between test and reference signs and a simple nearest neighbour classifier is used to recognize an unknown sign as one among the known signs by specifying a desired level of threshold. An extensive experimentation is conducted on a considerably large database of signs created by us during the course of research work in order to evaluate the performance of the proposed system version:1
arxiv-1610-02633 | Enabling Medical Translation for Low-Resource Languages | http://arxiv.org/abs/1610.02633 | id:1610.02633 author:Ahmad Musleh, Nadir Durrani, Irina Temnikova, Preslav Nakov, Stephan Vogel, Osama Alsaad category:cs.CL I.2.7  published:2016-10-09 summary:We present research towards bridging the language gap between migrant workers in Qatar and medical staff. In particular, we present the first steps towards the development of a real-world Hindi-English machine translation system for doctor-patient communication. As this is a low-resource language pair, especially for speech and for the medical domain, our initial focus has been on gathering suitable training data from various sources. We applied a variety of methods ranging from fully automatic extraction from the Web to manual annotation of test data. Moreover, we developed a method for automatically augmenting the training data with synthetically generated variants, which yielded a very sizable improvement of more than 3 BLEU points absolute. version:1
arxiv-1610-02616 | Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition | http://arxiv.org/abs/1610.02616 | id:1610.02616 author:Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, Terry Lyons category:cs.CV  published:2016-10-09 summary:Online handwritten Chinese text recognition (OHCTR) is a challenging problem as it involves a large-scale character set, ambiguous segmentation, and variable-length input sequences. In this paper, we exploit the outstanding capability of path signature to translate online pen-tip trajectories into informative signature feature maps using a sliding window-based method, successfully capturing the analytic and geometric properties of pen strokes with strong local invariance and robustness. A multi-spatial-context fully convolutional recurrent network (MCFCRN) is proposed to exploit the multiple spatial contexts from the signature feature maps and generate a prediction sequence while completely avoiding the difficult segmentation problem. Furthermore, an implicit language model is developed to make predictions based on semantic context within a predicting feature sequence, providing a new perspective for incorporating lexicon constraints and prior knowledge about a certain language in the recognition procedure. Experiments on two standard benchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with correct rates of 97.10% and 97.15%, respectively, which are significantly better than the best result reported thus far in the literature. version:1
arxiv-1610-02610 | Visual Closed-Loop Control for Pouring Liquids | http://arxiv.org/abs/1610.02610 | id:1610.02610 author:Connor Schenck, Dieter Fox category:cs.RO cs.CV  published:2016-10-09 summary:Pouring a specific amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour specific amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the first use of raw visual feedback to pour liquids in robotics. version:1
arxiv-1610-02588 | Iterative proportional scaling revisited: a modern optimization perspective on big count data | http://arxiv.org/abs/1610.02588 | id:1610.02588 author:Yiyuan She, Shao Tang category:stat.CO stat.ML  published:2016-10-08 summary:We revisit the classic iterative proportional scaling (IPS) for contingency table analysis, from a modern optimization perspective. In contrast to the criticisms made in the literature, we show that based on a coordinate descent characterization, IPS can be slightly modified to deliver coefficient estimates, and from a majorization-minimization standpoint, IPS can be extended to handle log-affine models with general designs. The optimization techniques help accelerate IPS to provide highly salable algorithms for big count data applications, and can adapt IPS to shrinkage estimation to deal with a large number of variables. version:1
arxiv-1610-02583 | A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation | http://arxiv.org/abs/1610.02583 | id:1610.02583 author:Gang Chen category:cs.LG  published:2016-10-08 summary:We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit. version:1
arxiv-1610-02579 | Crafting GBD-Net for Object Detection | http://arxiv.org/abs/1610.02579 | id:1610.02579 author:Xingyu Zeng, Wanli Ouyang, Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu, Yucong Zhou, Bin Yang, Zhe Wang, Hui Zhou, Xiaogang Wang category:cs.CV  published:2016-10-08 summary:The visual cues from multiple support regions of different sizes and resolutions are complementary in classifying a candidate box in object detection. Effective integration of local and contextual visual cues from these regions has become a fundamental problem in object detection. In this paper, we propose a gated bi-directional CNN (GBD-Net) to pass messages among features from different support regions during both feature learning and feature extraction. Such message passing can be implemented through convolution between neighboring support regions in two directions and can be conducted in various layers. Therefore, local and contextual visual patterns can validate the existence of each other by learning their nonlinear relationships and their close interactions are modeled in a more complex way. It is also shown that message passing is not always helpful but dependent on individual samples. Gated functions are therefore needed to control message transmission, whose on-or-offs are controlled by extra visual evidence from the input sample. The effectiveness of GBD-Net is shown through experiments on three object detection datasets, ImageNet, Pascal VOC2007 and Microsoft COCO. This paper also shows the details of our approach in wining the ImageNet object detection challenge of 2016, with source code provided on \url{https://github.com/craftGBD/craftGBD}. version:1
arxiv-1610-02567 | Mining the Web for Pharmacovigilance: the Case Study of Duloxetine and Venlafaxine | http://arxiv.org/abs/1610.02567 | id:1610.02567 author:Abbas Chokor, Abeed Sarker, Graciela Gonzalez category:cs.CL cs.CY  published:2016-10-08 summary:Adverse reactions caused by drugs following their release into the market are among the leading causes of death in many countries. The rapid growth of electronically available health related information, and the ability to process large volumes of them automatically, using natural language processing (NLP) and machine learning algorithms, have opened new opportunities for pharmacovigilance. Survey found that more than 70% of US Internet users consult the Internet when they require medical information. In recent years, research in this area has addressed for Adverse Drug Reaction (ADR) pharmacovigilance using social media, mainly Twitter and medical forums and websites. This paper will show the information which can be collected from a variety of Internet data sources and search engines, mainly Google Trends and Google Correlate. While considering the case study of two popular Major depressive Disorder (MDD) drugs, Duloxetine and Venlafaxine, we will provide a comparative analysis for their reactions using publicly-available alternative data sources. version:1
arxiv-1610-02544 | Computational linking theory | http://arxiv.org/abs/1610.02544 | id:1610.02544 author:Aaron Steven White, Drew Reisinger, Rachel Rudinger, Kyle Rawlins, Benjamin Van Durme category:cs.CL  published:2016-10-08 summary:A linking theory explains how verbs' semantic arguments are mapped to their syntactic arguments---the inverse of the Semantic Role Labeling task from the shallow semantic parsing literature. In this paper, we develop the Computational Linking Theory framework as a method for implementing and testing linking theories proposed in the theoretical literature. We deploy this framework to assess two cross-cutting types of linking theory: local v. global models and categorical v. featural models. To further investigate the behavior of these models, we develop a measurement model in the spirit of previous work in semantic role induction: the Semantic Proto-Role Linking Model. We use this model, which implements a generalization of Dowty's seminal Proto-Role Theory, to induce semantic proto-roles, which we compare to those Dowty proposes. version:1
arxiv-1610-02527 | Federated Optimization: Distributed Machine Learning for On-Device Intelligence | http://arxiv.org/abs/1610.02527 | id:1610.02527 author:Jakub Konečný, H. Brendan McMahan, Daniel Ramage, Peter Richtárik category:cs.LG  published:2016-10-08 summary:We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \federated optimization. version:1
arxiv-1610-02509 | Content-Based Image Retrieval Using Multiresolution Analysis Of Shape-Based Classified Images | http://arxiv.org/abs/1610.02509 | id:1610.02509 author:I. M. El-Henawy, Kareem Ahmed category:cs.CV  published:2016-10-08 summary:Content-Based Image Retrieval (CBIR) systems have been widely used for a wide range of applications such as Art collections, Crime prevention and Intellectual property. In this paper, a novel CBIR system, which utilizes visual contents (color, texture and shape) of an image to retrieve images, is proposed. The proposed system builds three feature vectors and stores them into MySQL database. The first feature vector uses descriptive statistics to describe the distribution of data in each channel of RGB channels of the image. The second feature vector describes the texture using eigenvalues of the 39 sub-bands that are generated after applying four levels 2D DWT in each channel (red, green and blue channels) of the image. These wavelets sub-bands perfectly describes the horizontal, vertical and diagonal edges that exist in the multi-resolution analysis of the image. The third feature vector describes the basic shapes that exist in the skeletonization version of the black and white representation of the image. Experimental results on a private MYSQL database that consists of 10000 images, using color, texture, shape and stored relevance feedbacks, showed 96.4% average correct retrieval rate in an efficient recovery time. version:1
arxiv-1610-02501 | Revisiting Multiple Instance Neural Networks | http://arxiv.org/abs/1610.02501 | id:1610.02501 author:Xinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, Wenyu Liu category:stat.ML cs.LG  published:2016-10-08 summary:Recently neural networks and multiple instance learning are both attractive topics in Artificial Intelligence related research fields. Deep neural networks have achieved great success in supervised learning problems, and multiple instance learning as a typical weakly-supervised learning method is effective for many applications in computer vision, biometrics, nature language processing, etc. In this paper, we revisit the problem of solving multiple instance learning problems using neural networks. Neural networks are appealing for solving multiple instance learning problem. The multiple instance neural networks perform multiple instance learning in an end-to-end way, which take a bag with various number of instances as input and directly output bag label. All of the parameters in a multiple instance network are able to be optimized via back-propagation. We propose a new multiple instance neural network to learn bag representations, which is different from the existing multiple instance neural networks that focus on estimating instance label. In addition, recent tricks developed in deep learning have been studied in multiple instance networks, we find deep supervision is effective for boosting bag classification accuracy. In the experiments, the proposed multiple instance networks achieve state-of-the-art or competitive performance on several MIL benchmarks. Moreover, it is extremely fast for both testing and training, e.g., it takes only 0.0003 second to predict a bag and a few seconds to train on a MIL datasets on a moderate CPU. version:1
arxiv-1610-02493 | A Semantic Analyzer for the Comprehension of the Spontaneous Arabic Speech | http://arxiv.org/abs/1610.02493 | id:1610.02493 author:Mourad Mars, Mounir Zrigui, Mohamed Belgacem, Anis Zouaghi category:cs.CL  published:2016-10-08 summary:This work is part of a large research project entitled "Or\'eodule" aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to improve the probabilistic model on which our semantic decoder is based. To achieve this goal, we have decided to test the influence of the pertinent context use, and of the contextual data integration of different types, on the effectiveness of the semantic decoder. The findings are quite satisfactory. version:1
arxiv-1610-02483 | Boost K-Means | http://arxiv.org/abs/1610.02483 | id:1610.02483 author:Wan-Lei Zhao, Cheng-Hao Deng, Chong-Wah Ngo category:cs.LG cs.CV cs.DB  published:2016-10-08 summary:Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. Since then, continuous efforts have been taken to enhance its performance. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is explicitly driven by an objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. K-means therefore becomes simpler, faster and better. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios. version:1
arxiv-1610-02482 | 4D Crop Monitoring: Spatio-Temporal Reconstruction for Agriculture | http://arxiv.org/abs/1610.02482 | id:1610.02482 author:Jing Dong, John Gary Burnham, Byron Boots, Glen C. Rains, Frank Dellaert category:cs.RO cs.CV  published:2016-10-08 summary:Autonomous crop monitoring at high spatial and temporal resolution is a critical problem in precision agriculture. While Structure from Motion and Multi-View Stereo algorithms can finely reconstruct the 3D structure of a field with low-cost image sensors, these algorithms fail to capture the dynamic nature of continuously growing crops. In this paper we propose a 4D reconstruction approach to crop monitoring, which employs a spatio-temporal model of dynamic scenes that is useful for precision agriculture applications. Additionally, we provide a robust data association algorithm to address the problem of large appearance changes due to scenes being viewed from different angles at different points in time, which is critical to achieving 4D reconstruction. Finally, we collected a high quality dataset with ground truth statistics to evaluate the performance of our method. We demonstrate that our 4D reconstruction approach provides models that are qualitatively correct with respect to visual appearance and quantitatively accurate when measured against the ground truth geometric properties of the monitored crops. version:1
arxiv-1610-02478 | Deep Convolutional Networks as Models of Generalization and Blending Within Visual Creativity | http://arxiv.org/abs/1610.02478 | id:1610.02478 author:Graeme McCaig, Steve DiPaola, Liane Gabora category:cs.NE cs.CV q-bio.NC  published:2016-10-08 summary:We examine two recent artificial intelligence (AI) based deep learning algorithms for visual blending in convolutional neural networks (Mordvintsev et al. 2015, Gatys et al. 2015). To investigate the potential value of these algorithms as tools for computational creativity research, we explain and schematize the essential aspects of the algorithms' operation and give visual examples of their output. We discuss the relationship of the two algorithms to human cognitive science theories of creativity such as conceptual blending theory and honing theory, and characterize the algorithms with respect to generation of novelty and aesthetic quality. version:1
arxiv-1610-02454 | Learning What and Where to Draw | http://arxiv.org/abs/1610.02454 | id:1610.02454 author:Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, Honglak Lee category:cs.CV cs.NE  published:2016-10-08 summary:Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text- and location-controllable synthesis of images of human actions on the MPII Human Pose dataset. version:1
arxiv-1610-02431 | ResearchDoom and CocoDoom: Learning Computer Vision with Games | http://arxiv.org/abs/1610.02431 | id:1610.02431 author:A. Mahendran, H. Bilen, J. F. Henriques, A. Vedaldi category:cs.CV  published:2016-10-07 summary:In this short note we introduce ResearchDoom, an implementation of the Doom first-person shooter that can extract detailed metadata from the game. We also introduce the CocoDoom dataset, a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format. ResearchDoom and CocoDoom can be used to train and evaluate a variety of computer vision methods such as object recognition, detection and segmentation at the level of instances and categories, tracking, ego-motion estimation, monocular depth estimation and scene segmentation. The code and data are available at http://www.robots.ox.ac.uk/~vgg/research/researchdoom. version:1
arxiv-1610-02424 | Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models | http://arxiv.org/abs/1610.02424 | id:1610.02424 author:Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, Dhruv Batra category:cs.AI cs.CL cs.CV  published:2016-10-07 summary:Neural sequence models are widely used to model time-series data in many fields. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-$B$ candidates -- resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose \emph{Diverse Beam Search} (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space -- implying that DBS is a \emph{better search algorithm}. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models. version:1
arxiv-1610-02415 | Automatic chemical design using a data-driven continuous representation of molecules | http://arxiv.org/abs/1610.02415 | id:1610.02415 author:Rafael Gómez-Bombarelli, David Duvenaud, José Miguel Hernández-Lobato, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, Alán Aspuru-Guzik category:cs.LG physics.chem-ph  published:2016-10-07 summary:We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This generative model allows efficient search and optimization through open-ended spaces of chemical compounds. We train deep neural networks on hundreds of thousands of existing chemical structures to construct two coupled functions: an encoder and a decoder. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to the discrete representation from this latent space. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the design of drug-like molecules as well as organic light-emitting diodes. version:1
arxiv-1610-02414 | Indoor Space Recognition using Deep Convolutional Neural Network: A Case Study at MIT Campus | http://arxiv.org/abs/1610.02414 | id:1610.02414 author:Fan Zhang, Fabio Duarte, Ruixian Ma, Dimitrios Milioris, Hui Lin, Carlo Ratti category:cs.CV  published:2016-10-07 summary:In this paper, we propose a robust and parsimonious approach using Deep Convolutional Neural Network (DCNN) to recognize and interpret interior space. DCNN has achieved incredible success in object and scene recognition. In this study we design and train a DCNN to classify a pre-zoning indoor space, and from a single phone photo to recognize the learned space features, with no need of additional assistive technology. We collect more than 600,000 images inside MIT campus buildings to train our DCNN model, and achieved 97.9% accuracy in validation dataset and 81.7% accuracy in test dataset based on spatial-scale fixed model. Furthermore, the recognition accuracy and spatial resolution can be potentially improved through multiscale classification model. We identify the discriminative image regions through Class Activating Mapping (CAM) technique, to observe the model's behavior in how to recognize space and interpret it in an abstract way. By evaluating the results with misclassification matrix, we investigate the visual spatial feature of interior space by looking into its visual similarity and visual distinctiveness, giving insights into interior design and human indoor perception and wayfinding research. The contribution of this paper is threefold. First, we propose a robust and parsimonious approach for indoor navigation using DCNN. Second, we demonstrate that DCNN also has a potential capability in space feature learning and recognition, even under severe appearance changes. Third, we introduce a DCNN based approach to look into the visual similarity and visual distinctiveness of interior space. version:1
arxiv-1610-02413 | Equality of Opportunity in Supervised Learning | http://arxiv.org/abs/1610.02413 | id:1610.02413 author:Moritz Hardt, Eric Price, Nathan Srebro category:cs.LG  published:2016-10-07 summary:We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores. version:1
arxiv-1610-02391 | Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization | http://arxiv.org/abs/1610.02391 | id:1610.02391 author:Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra category:cs.CV cs.AI cs.LG  published:2016-10-07 summary:We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are "important" for predictions from these models - or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one even when both networks make identical predictions. version:1
arxiv-1610-02373 | Distributed Averaging CNN-ELM for Big Data | http://arxiv.org/abs/1610.02373 | id:1610.02373 author:Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin category:cs.LG cs.CV cs.DC 68T05 I.2.6  published:2016-10-07 summary:Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required. version:1
arxiv-1610-02372 | Combining local and global smoothing in multivariate density estimation | http://arxiv.org/abs/1610.02372 | id:1610.02372 author:Adelchi Azzalini category:stat.ME stat.ML  published:2016-10-07 summary:Non-parametric estimation of a multivariate density estimation is tackled via a method which combines traditional local smoothing with a form of global smoothing but without imposing a rigid structure. Simulation work delivers encouraging indications on the effectiveness of the method. An application to density-based clustering illustrates a possible usage. version:1
arxiv-1610-01239 | Random Feature Nullification for Adversary Resistant Deep Architecture | http://arxiv.org/abs/1610.01239 | id:1610.01239 author:Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Xinyu Xing, C. Lee Giles, Xue Liu category:cs.LG  published:2016-10-05 summary:Deep neural networks (DNN) have been proven to be quite effective in many applications such as image recognition and using software to process security or traffic camera footage, for example to measure traffic flows or spot suspicious activities. Despite the superior performance of DNN in these applications, it has recently been shown that a DNN is susceptible to a particular type of attack that exploits a fundamental flaw in its design. Specifically, an attacker can craft a particular synthetic example, referred to as an adversarial sample, causing the DNN to produce an output behavior chosen by attackers, such as misclassification. Addressing this flaw is critical if a DNN is to be used in critical applications such as those in cybersecurity. Previous work provided various defence mechanisms by either increasing the model nonlinearity or enhancing model complexity. However, after a thorough analysis of the fundamental flaw in the DNN, we discover that the effectiveness of such methods is limited. As such, we propose a new adversary resistant technique that obstructs attackers from constructing impactful adversarial samples by randomly nullifying features within samples. Using the MNIST dataset, we evaluate our proposed technique and empirically show our technique significantly boosts DNN's robustness against adversarial samples while maintaining high accuracy in classification. version:3
arxiv-1610-00850 | Comparing Human-Centric and Robot-Centric Sampling for Robot Deep Learning from Demonstrations | http://arxiv.org/abs/1610.00850 | id:1610.00850 author:Michael Laskey, Caleb Chuck, Jonathan Lee, Jeffrey Mahler, Sanjay Krishnan, Kevin Jamieson, Anca Dragan, Ken Goldberg category:cs.RO cs.LG  published:2016-10-04 summary:Motivated by recent advances in Deep Learning for robot control, this paper considers two learning algorithms in terms of how they acquire demonstrations. "Human-Centric" (HC) sampling is the standard supervised learning algorithm, where a human supervisor demonstrates the task by teleoperating the robot to provide trajectories consisting of state-control pairs. "Robot-Centric" (RC) sampling is an increasingly popular alternative used in algorithms such as DAgger, where a human supervisor observes the robot executing a learned policy and provides corrective control labels for each state visited. RC sampling can be challenging for human supervisors and prone to mislabeling. RC sampling can also induce error in policy performance because it repeatedly visits areas of the state space that are harder to learn. Although policies learned with RC sampling can be superior to HC sampling for standard learning models such as linear SVMs, policies learned with HC sampling may be comparable with highly-expressive learning models such as deep learning and hyper-parametric decision trees, which have little model error. We compare HC and RC using a grid world and a physical robot singulation task, where in the latter the input is a binary image of a connected set of objects on a planar worksurface and the policy generates a motion of the gripper to separate one object from the rest. We observe in simulation that for linear SVMs, policies learned with RC outperformed those learned with HC but that with deep models this advantage disappears. We also find that with RC, the corrective control labels provided by humans can be highly inconsistent. We prove there exists a class of examples where in the limit, HC is guaranteed to converge to an optimal policy while RC may fail to converge. version:2
arxiv-1610-02348 | Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data | http://arxiv.org/abs/1610.02348 | id:1610.02348 author:Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin category:cs.AI cs.LG cs.NE 68T05 I.2.6  published:2016-10-07 summary:In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels. We verified our method in extended MNIST data set and not MNIST data set. We set the experiment to simulate virtual drift, real drift, and hybrid drift event and we demonstrated how our CNNELM adaptability works. Our proposed method works well and gives better accuracy, computation scalability, and concept drifts adaptability compared to the regular ELM and CNN. Further researches are still required to study the optimum parameters and to use more varied image data set. version:1
arxiv-1610-00732 | Sequential Low-Rank Change Detection | http://arxiv.org/abs/1610.00732 | id:1610.00732 author:Yao Xie, Lee Seversky category:stat.ML cs.LG math.ST stat.TH  published:2016-10-03 summary:Detecting emergence of a low-rank signal from high-dimensional data is an important problem arising from many applications such as camera surveillance and swarm monitoring using sensors. We consider a procedure based on the largest eigenvalue of the sample covariance matrix over a sliding window to detect the change. To achieve dimensionality reduction, we present a sketching-based approach for rank change detection using the low-dimensional linear sketches of the original high-dimensional observations. The premise is that when the sketching matrix is a random Gaussian matrix, and the dimension of the sketching vector is sufficiently large, the rank of sample covariance matrix for these sketches equals the rank of the original sample covariance matrix with high probability. Hence, we may be able to detect the low-rank change using sample covariance matrices of the sketches without having to recover the original covariance matrix. We character the performance of the largest eigenvalue statistic in terms of the false-alarm-rate and the expected detection delay, and present an efficient online implementation via subspace tracking. version:2
arxiv-1610-01945 | Connecting Generative Adversarial Networks and Actor-Critic Methods | http://arxiv.org/abs/1610.01945 | id:1610.01945 author:David Pfau, Oriol Vinyals category:cs.LG stat.ML  published:2016-10-06 summary:Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities. version:2
arxiv-1610-02306 | Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm | http://arxiv.org/abs/1610.02306 | id:1610.02306 author:Vina Ayumi, L. M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy category:cs.CV 68Txx I.2.10  published:2016-10-07 summary:Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\%). On the CIFAR10 dataset, currently, state of the art is 96.53\% using fractional pooling, while this proposed method achieves 99.14\%. version:1
arxiv-1610-02967 | Distributed Convex Optimization with Many Convex Constraints | http://arxiv.org/abs/1610.02967 | id:1610.02967 author:Joachim Giesen, Sören Laue category:math.OC cs.LG cs.NA stat.ML  published:2016-10-07 summary:We address the problem of solving convex optimization problems with many convex constraints in a distributed setting. Our approach is based on an extension of the alternating direction method of multipliers (ADMM) that recently gained a lot of attention in the Big Data context. Although it has been invented decades ago, ADMM so far can be applied only to unconstrained problems and problems with linear equality or inequality constraints. Our extension can handle arbitrary inequality constraints directly. It combines the ability of ADMM to solve convex optimization problems in a distributed setting with the ability of the Augmented Lagrangian method to solve constrained optimization problems, and as we show, it inherits the convergence guarantees of ADMM and the Augmented Lagrangian method. version:1
arxiv-1610-02256 | Deep Image Aesthetics Classification using Inception Modules and Fine-tuning Connected Layer | http://arxiv.org/abs/1610.02256 | id:1610.02256 author:Xin Jin, Jingying Chi, Siwei Peng, Yulu Tian, Chaochen Ye, Xiaodong Li category:cs.CV cs.MM I.4.9  published:2016-10-07 summary:In this paper we investigate the image aesthetics classification problem, aka, automatically classifying an image into low or high aesthetic quality, which is quite a challenging problem beyond image recognition. Deep convolutional neural network (DCNN) methods have recently shown promising results for image aesthetics assessment. Currently, a powerful inception module is proposed which shows very high performance in object classification. However, the inception module has not been taken into consideration for the image aesthetics assessment problem. In this paper, we propose a novel DCNN structure codenamed ILGNet for image aesthetics classification, which introduces the Inception module and connects intermediate Local layers to the Global layer for the output. Besides, we use a pre-trained image classification CNN called GoogLeNet on the ImageNet dataset and fine tune our connected local and global layer on the large scale aesthetics assessment AVA dataset. The experimental results show that the proposed ILGNet outperforms the state of the art results in image aesthetics assessment in the AVA benchmark. version:1
arxiv-1610-02255 | Learning Grimaces by Watching TV | http://arxiv.org/abs/1610.02255 | id:1610.02255 author:Samuel Albanie, Andrea Vedaldi category:cs.CV  published:2016-10-07 summary:Differently from computer vision systems which require explicit supervision, humans can learn facial expressions by observing people in their environment. In this paper, we look at how similar capabilities could be developed in machine vision. As a starting point, we consider the problem of relating facial expressions to objectively measurable events occurring in videos. In particular, we consider a gameshow in which contestants play to win significant sums of money. We extract events affecting the game and corresponding facial expressions objectively and automatically from the videos, obtaining large quantities of labelled data for our study. We also develop, using benchmarks such as FER and SFEW 2.0, state-of-the-art deep neural networks for facial expression recognition, showing that pre-training on face verification data can be highly beneficial for this task. Then, we extend these models to use facial expressions to predict events in videos and learn nameable expressions from them. The dataset and emotion recognition models are available at http://www.robots.ox.ac.uk/~vgg/data/facevalue version:1
arxiv-1610-02251 | Automated Detection of Individual Micro-calcifications from Mammograms using a Multi-stage Cascade Approach | http://arxiv.org/abs/1610.02251 | id:1610.02251 author:Zhi Lu, Gustavo Carneiro, Neeraj Dhungel, Andrew P. Bradley category:cs.CV  published:2016-10-07 summary:In mammography, the efficacy of computer-aided detection methods depends, in part, on the robust localisation of micro-calcifications ($\mu$C). Currently, the most effective methods are based on three steps: 1) detection of individual $\mu$C candidates, 2) clustering of individual $\mu$C candidates, and 3) classification of $\mu$C clusters. Where the second step is motivated both to reduce the number of false positive detections from the first step and on the evidence that malignancy depends on a relatively large number of $\mu$C detections within a certain area. In this paper, we propose a novel approach to $\mu$C detection, consisting of the detection \emph{and} classification of individual $\mu$C candidates, using shape and appearance features, using a cascade of boosting classifiers. The final step in our approach then clusters the remaining individual $\mu$C candidates. The main advantage of this approach lies in its ability to reject a significant number of false positive $\mu$C candidates compared to previously proposed methods. Specifically, on the INbreast dataset, we show that our approach has a true positive rate (TPR) for individual $\mu$Cs of 40\% at one false positive per image (FPI) and a TPR of 80\% at 10 FPI. These results are significantly more accurate than the current state of the art, which has a TPR of less than 1\% at one FPI and a TPR of 10\% at 10 FPI. Our results are competitive with the state of the art at the subsequent stage of detecting clusters of $\mu$Cs. version:1
arxiv-1610-02242 | Temporal Ensembling for Semi-Supervised Learning | http://arxiv.org/abs/1610.02242 | id:1610.02242 author:Samuli Laine, Timo Aila category:cs.NE cs.LG  published:2016-10-07 summary:In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63% to 12.89% in CIFAR-10 with 4000 labels and from 18.44% to 6.83% in SVHN with 500 labels. version:1
arxiv-1610-02237 | Weakly supervised learning of actions from transcripts | http://arxiv.org/abs/1610.02237 | id:1610.02237 author:Hilde Kuehne, Alexander Richard, Juergen Gall category:cs.CV  published:2016-10-07 summary:We present an approach for weakly supervised learning of human actions from video transcriptions. Our system is based on the idea that, given a sequence of input data and a transcript, i.e. a list of the order the actions occur in the video, it is possible to infer the actions within the video stream, and thus, learn the related action models without the need for any frame-based annotation. Starting from the transcript information at hand, we split the given data sequences uniformly based on the number of expected actions. We then learn action models for each class by maximizing the probability that the training video sequences are generated by the action models given the sequence order as defined by the transcripts. The learned model can be used to temporally segment an unseen video with or without transcript. We evaluate our approach on four distinct activity datasets, namely Hollywood Extended, MPII Cooking, Breakfast and CRIM13. We show that our system is able to align the scripted actions with the video data and that the learned models localize and classify actions competitively in comparison to models trained with full supervision, i.e. with frame level annotations, and that they outperform any current state-of-the-art approach for aligning transcripts with video data. version:1
arxiv-1610-02213 | Challenges of Computational Processing of Code-Switching | http://arxiv.org/abs/1610.02213 | id:1610.02213 author:Özlem Çetinoğlu, Sarah Schulz, Ngoc Thang Vu category:cs.CL  published:2016-10-07 summary:This paper addresses challenges of Natural Language Processing (NLP) on non-canonical multilingual data in which two or more languages are mixed. It refers to code-switching which has become more popular in our daily life and therefore obtains an increasing amount of attention from the research community. We report our experience that cov- ers not only core NLP tasks such as normalisation, language identification, language modelling, part-of-speech tagging and dependency parsing but also more downstream ones such as machine translation and automatic speech recognition. We highlight and discuss the key problems for each of the tasks with supporting examples from different language pairs and relevant previous work. version:1
arxiv-1610-02209 | Morphology Generation for Statistical Machine Translation using Deep Learning Techniques | http://arxiv.org/abs/1610.02209 | id:1610.02209 author:Marta R. Costa-jussà, Carlos Escolano category:cs.CL stat.ML  published:2016-10-07 summary:Morphology unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR. version:1
arxiv-1610-02177 | Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields | http://arxiv.org/abs/1610.02177 | id:1610.02177 author:Patrick Ferdinand Christ, Mohamed Ezzeldin A. Elshaer, Florian Ettlinger, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Marco Armbruster, Felix Hofmann, Melvin D'Anastasi, Wieland H. Sommer, Seyed-Ahmad Ahmadi, Bjoern H. Menze category:cs.CV  published:2016-10-07 summary:Automatic segmentation of the liver and its lesion is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT abdomen images using cascaded fully convolutional neural networks (CFCNs) and dense 3D conditional random fields (CRFs). We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions from the predicted liver ROIs of step 1. We refine the segmentations of the CFCN using a dense 3D CRF that accounts for both spatial coherence and appearance. CFCN models were trained in a 2-fold cross-validation on the abdominal CT dataset 3DIRCAD comprising 15 hepatic tumor volumes. Our results show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over 94% for liver with computation times below 100s per volume. We experimentally demonstrate the robustness of the proposed method as a decision support system with a high accuracy and speed for usage in daily clinical routine. version:1
arxiv-1610-02164 | Deep Reinforcement Learning From Raw Pixels in Doom | http://arxiv.org/abs/1610.02164 | id:1610.02164 author:Danijar Hafner category:cs.LG cs.AI  published:2016-10-07 summary:Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain. version:1
arxiv-1610-02143 | Stochastic Averaging for Constrained Optimization with Application to Online Resource Allocation | http://arxiv.org/abs/1610.02143 | id:1610.02143 author:Tianyi Chen, Aryan Mokhtari, Xin Wang, Alejandro Ribeiro, Georgios B. Giannakis category:math.OC cs.DC cs.LG stat.ML  published:2016-10-07 summary:Existing approaches to resource allocation for nowadays stochastic networks are challenged to meet fast convergence and tolerable delay requirements. In the era of data deluge and information explosion, the present paper leverages online learning advances to facilitate stochastic resource allocation tasks. By recognizing the central role of Lagrange multipliers, the underlying constrained optimization problem is formulated as a machine learning task involving both training and operational modes, with the goal of learning the sought multipliers in a fast and efficient manner. To this end, an order-optimal offline learning approach is developed first for batch training, and it is then generalized to the online setting with a procedure termed learn-and-adapt. The novel resource allocation protocol permeates benefits of stochastic approximation and statistical learning to obtain low-complexity online updates with learning errors close to the statistical accuracy limits, while still preserving adaptation performance, which in the stochastic network optimization context guarantees queue stability. Analysis and simulated tests demonstrate that the proposed data-driven approach improves the delay and convergence performance of existing resource allocation schemes. version:1
arxiv-1610-02136 | A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks | http://arxiv.org/abs/1610.02136 | id:1610.02136 author:Dan Hendrycks, Kevin Gimpel category:cs.NE cs.CV cs.LG  published:2016-10-07 summary:We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks. version:1
arxiv-1610-02132 | QSGD: Randomized Quantization for Communication-Optimal Stochastic Gradient Descent | http://arxiv.org/abs/1610.02132 | id:1610.02132 author:Dan Alistarh, Jerry Li, Ryota Tomioka, Milan Vojnovic category:cs.LG cs.DS  published:2016-10-07 summary:Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. version:1
arxiv-1610-02124 | There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction | http://arxiv.org/abs/1610.02124 | id:1610.02124 author:Courtney Napoles, Keisuke Sakaguchi, Joel Tetreault category:cs.CL  published:2016-10-07 summary:Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics. version:1
arxiv-1610-02122 | High-dimensional inference in linear models: robustness and adaptivity to model sparsity | http://arxiv.org/abs/1610.02122 | id:1610.02122 author:Yinchu Zhu, Jelena Bradic category:stat.ME math.SP math.ST stat.ML stat.TH  published:2016-10-07 summary:In high-dimensional linear models, the sparsity assumption is typically made, stating that most of the model parameters have value equal to zero. Under the sparsity assumption, estimation and, recently, inference as well as the fundamental limits of detection have been well studied. However, in certain cases, sparsity assumption may be violated, and a large number of covariates can be expected to be associated with the response, indicating that possibly all, rather just a few, model parameters are different from zero. A natural example is a genome-wide gene expression profiling, where all genes are believed to affect a common disease marker. We show that the current inferential methods are sensitive to the sparsity assumption, and may in turn result in severe bias: lack of control of Type-I error is apparent once the model is not sparse. In this article, we propose a new inferential method, named CorrT, which is robust and adaptive to the sparsity assumption. CorrT is shown to have Type I error approaching the nominal level, regardless of how sparse or dense the model is. Specifically, the developed test is based on a moment condition induced by the hypothesis and the covariate structure of the model design. Such a construction circumvents the fundamental difficulty of accurately estimating non-sparse high-dimensional models. As a result, the proposed test guards against large estimation errors caused by potential absence of sparsity, and at the same time, adapts to the model sparsity. In fact, CorrT is also shown to be optimal whenever sparsity holds. Numerical experiments show favorable performance of CorrT compared to existing methods. We also apply CorrT to a real dataset and confirm some known discoveries related to HER2+ cancer patients and the gene-to-gene interaction. version:1
arxiv-1610-02084 | Computational Tradeoffs in Biological Neural Networks: Self-Stabilizing Winner-Take-All Networks | http://arxiv.org/abs/1610.02084 | id:1610.02084 author:Nancy Lynch, Cameron Musco, Merav Parter category:cs.NE cs.DC q-bio.NC  published:2016-10-06 summary:We initiate a line of investigation into biological neural networks from an algorithmic perspective. We develop a simplified but biologically plausible model for distributed computation in stochastic spiking neural networks and study tradeoffs between computation time and network complexity in this model. Our aim is to abstract real neural networks in a way that, while not capturing all interesting features, preserves high-level behavior and allows us to make biologically relevant conclusions. In this paper, we focus on the important `winner-take-all' (WTA) problem, which is analogous to a neural leader election unit: a network consisting of $n$ input neurons and $n$ corresponding output neurons must converge to a state in which a single output corresponding to a firing input (the `winner') fires, while all other outputs remain silent. Neural circuits for WTA rely on inhibitory neurons, which suppress the activity of competing outputs and drive the network towards a converged state with a single firing winner. We attempt to understand how the number of inhibitors used affects network convergence time. We show that it is possible to significantly outperform naive WTA constructions through a more refined use of inhibition, solving the problem in $O(\theta)$ rounds in expectation with just $O(\log^{1/\theta} n)$ inhibitors for any $\theta$. An alternative construction gives convergence in $O(\log^{1/\theta} n)$ rounds with $O(\theta)$ inhibitors. We compliment these upper bounds with our main technical contribution, a nearly matching lower bound for networks using $\ge \log\log n$ inhibitors. Our lower bound uses familiar indistinguishability and locality arguments from distributed computing theory. It lets us derive a number of interesting conclusions about the structure of any network solving WTA with good probability, and the use of randomness and inhibition within such a network. version:1
arxiv-1610-02067 | Stochastic Games for Smart Grid Energy Management with Prospect Prosumers | http://arxiv.org/abs/1610.02067 | id:1610.02067 author:Seyed Rasoul Etesami, Walid Saad, Narayan Mandayam, H. Vincent Poor category:cs.GT cs.IT cs.LG cs.SY math.IT  published:2016-10-06 summary:In this paper, the problem of smart grid energy management under stochastic dynamics is investigated. In the considered model, at the demand side, it is assumed that customers can act as prosumers who own renewable energy sources and can both produce and consume energy. Due to the coupling between the prosumers' decisions and the stochastic nature of renewable energy, the interaction among prosumers is formulated as a stochastic game, in which each prosumer seeks to maximize its payoff, in terms of revenues, by controlling its energy consumption and demand. In particular, the subjective behavior of prosumers is explicitly reflected into their payoff functions using prospect theory, a powerful framework that allows modeling real-life human choices. For this prospect-based stochastic game, it is shown that there always exists a stationary Nash equilibrium where the prosumers' trading policies in the equilibrium are independent of the time and their histories of the play. Moreover, a novel distributed algorithm with no information sharing among prosumers is proposed and shown to converge to an $\epsilon$-Nash equilibrium. On the other hand, at the supply side, the interaction between the utility company and the prosumers is formulated as an online optimization problem in which the utility company's goal is to learn its optimal energy allocation rules. For this case, it is shown that such an optimization problem admits a no-regret algorithm meaning that regardless of the actual outcome of the game among the prosumers, the utility company can follow a strategy that mitigates its allocation costs as if it knew the entire demand market a priori. Simulation results show the convergence of the proposed algorithms to their predicted outcomes and present new insights resulting from prospect theory that contribute toward more efficient energy management in the smart grids. version:1
arxiv-1610-02055 | Places: An Image Database for Deep Scene Understanding | http://arxiv.org/abs/1610.02055 | id:1610.02055 author:Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, Aude Oliva category:cs.CV cs.AI  published:2016-10-06 summary:The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems. version:1
arxiv-1610-02003 | Scalable Machine Translation in Memory Constrained Environments | http://arxiv.org/abs/1610.02003 | id:1610.02003 author:Paul Baltescu category:cs.CL  published:2016-10-06 summary:Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems. In recent years, mobile devices with adequate computing power have become widely available. Despite being very successful, mobile applications relying on NLP systems continue to follow a client-server architecture, which is of limited use because access to internet is often limited and expensive. The goal of this dissertation is to show how to construct a scalable machine translation system that can operate with the limited resources available on a mobile device. The main challenge for porting translation systems on mobile devices is memory usage. The amount of memory available on a mobile device is far less than what is typically available on the server side of a client-server application. In this thesis, we investigate alternatives for the two components which prevent standard translation systems from working on mobile devices due to high memory usage. We show that once these standard components are replaced with our proposed alternatives, we obtain a scalable translation system that can work on a device with limited memory. version:1
arxiv-1610-01986 | Active exploration in parameterized reinforcement learning | http://arxiv.org/abs/1610.01986 | id:1610.01986 author:Mehdi Khamassi, Costas Tzafestas category:cs.LG  published:2016-10-06 summary:Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature $\beta$ parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune $\beta$ and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm. version:1
arxiv-1610-01983 | Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks? | http://arxiv.org/abs/1610.01983 | id:1610.01983 author:Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Ram Vasudevan category:cs.CV cs.RO  published:2016-10-06 summary:Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have however relied upon massive amounts of human annotated training data. This time-consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, this paper illustrates that real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor based classification problems like those that appear in self-driving cars. version:1
