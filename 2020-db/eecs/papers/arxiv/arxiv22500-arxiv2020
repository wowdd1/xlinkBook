arxiv-1611-02025 | Presenting a New Dataset for the Timeline Generation Problem | http://arxiv.org/abs/1611.02025 | id:1611.02025 author:Xavier Holt, Will Radford, Ben Hachey category:cs.CL  published:2016-11-07 summary:The timeline generation task summarises an entity's biography by selecting stories representing key events from a large pool of relevant documents. This paper addresses the lack of a standard dataset and evaluative methodology for the problem. We present and make publicly available a new dataset of 18,793 news articles covering 39 entities. For each entity, we provide a gold standard timeline and a set of entity-related articles. We propose ROUGE as an evaluation metric and validate our dataset by showing that top Google results outperform straw-man baselines. version:1
arxiv-1611-02019 | Multi-view Generative Adversarial Networks | http://arxiv.org/abs/1611.02019 | id:1611.02019 author:Mickaël Chen, Ludovic Denoyer category:cs.LG  published:2016-11-07 summary:Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets. version:1
arxiv-1611-02010 | Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation | http://arxiv.org/abs/1611.02010 | id:1611.02010 author:Jian Du, Shaodan Ma, Yik-Chung Wu, Soummya Kar, José M. F. Moura category:stat.ML cs.IT math.IT  published:2016-11-07 summary:In networks such as the smart grid, communication networks, and social networks, local measurements/observations are scattered over a wide geographical area. Centralized inference algorithm are based on gathering all the observations at a central processing unit. However, with data explosion and ever-increasing network sizes, centralized inference suffers from large communication overhead, heavy computation burden at the center, and susceptibility to central node failure. This paper considers inference over networks using factor graphs and a distributed inference algorithm based on Gaussian belief propagation. The distributed inference involves only local computation of the information matrix and of the mean vector and message passing between neighbors. We discover and show analytically that the message information matrix converges exponentially fast to a unique positive definite limit matrix for arbitrary positive semidefinite initialization. We provide the necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator. An easily verifiable sufficient convergence condition on the topology of a factor graph is further provided. version:1
arxiv-1611-02007 | Keyphrase Annotation with Graph Co-Ranking | http://arxiv.org/abs/1611.02007 | id:1611.02007 author:Adrien Bougouin, Florian Boudin, Béatrice Daille category:cs.CL  published:2016-11-07 summary:Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods. version:1
arxiv-1611-01990 | Elliptic operator for shape analysis | http://arxiv.org/abs/1611.01990 | id:1611.01990 author:Yoni Choukroun, Alon Shtern, Ron Kimmel category:cs.GR cs.CV  published:2016-11-07 summary:Many shape analysis methods treat the geometry of an object as a metric space that can be captured by the Laplace-Beltrami operator. In this paper, we propose to adapt a classical operator from quantum mechanics to the field of shape analysis where we suggest to integrate a scalar function through a unified elliptical Hamiltonian operator. We study the addition of a potential function to the Laplacian as a generator for dual spaces in which shape processing is performed. Then, we evaluate the resulting spectral basis for different applications such as mesh compression and shape matching. The suggested operator is shown to produce better functional spaces to operate with, as demonstrated by the proposed framework that outperforms existing spectral methods, for example, when applied to shape matching benchmarks. version:1
arxiv-1611-01989 | DeepCoder: Learning to Write Programs | http://arxiv.org/abs/1611.01989 | id:1611.01989 author:Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, Daniel Tarlow category:cs.LG  published:2016-11-07 summary:We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites. version:1
arxiv-1611-01988 | Neural Functional Programming | http://arxiv.org/abs/1611.01988 | id:1611.01988 author:John K. Feser, Marc Brockschmidt, Alexander L. Gaunt, Daniel Tarlow category:cs.PL cs.LG  published:2016-11-07 summary:We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines. version:1
arxiv-1611-01972 | Fixed-point Factorized Networks | http://arxiv.org/abs/1611.01972 | id:1611.01972 author:Peisong Wang, Jian Cheng category:cs.CV cs.LG  published:2016-11-07 summary:In recent years, Deep Neural Networks (DNNs) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision, speech recognition and Natural Language Processing. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) on pre-trained models to reduce the computational complexity as well as the storage requirement of networks. Extensive experiments on large-scale ImageNet classification task show the effectiveness of our proposed method. version:1
arxiv-1611-01971 | One Class Splitting Criteria for Random Forests | http://arxiv.org/abs/1611.01971 | id:1611.01971 author:Nicolas Goix, Nicolas Drougard, Romain Brault, Maël Chiapino category:stat.ML cs.LG  published:2016-11-07 summary:Random Forests (RFs) are strong machine learning tools for classification and regression. However, they remain supervised algorithms, and no extension of RFs to the one-class setting has been proposed, except for techniques based on second-class sampling. This work fills this gap by proposing a natural methodology to extend standard splitting criteria to the one-class setting, structurally generalizing RFs to one-class classification. An extensive benchmark of seven state-of-the-art anomaly detection algorithms is also presented. This empirically demonstrates the relevance of our approach. version:1
arxiv-1611-01967 | Regularizing CNNs with Locally Constrained Decorrelations | http://arxiv.org/abs/1611.01967 | id:1611.01967 author:Pau Rodríguez, Jordi Gonzàlez, Guillem Cucurull, Josep M. Gonfaus, Xavier Roca category:cs.LG cs.NE  published:2016-11-07 summary:Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated features, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN. version:1
arxiv-1611-01964 | Log-time and Log-space Extreme Classification | http://arxiv.org/abs/1611.01964 | id:1611.01964 author:Kalina Jasinska, Nikos Karampatziakis category:cs.LG  published:2016-11-07 summary:We present LTLS, a technique for multiclass and multilabel prediction that can perform training and inference in logarithmic time and space. LTLS embeds large classification problems into simple structured prediction problems and relies on efficient dynamic programming algorithms for inference. We train LTLS with stochastic gradient descent on a number of multiclass and multilabel datasets and show that despite its small memory footprint it is often competitive with existing approaches. version:1
arxiv-1611-01962 | High-Resolution Semantic Labeling with Convolutional Neural Networks | http://arxiv.org/abs/1611.01962 | id:1611.01962 author:Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat, Pierre Alliez category:cs.CV  published:2016-11-07 summary:Convolutional neural networks (CNNs) have received increasing attention over the last few years. They were initially conceived for image categorization, i.e., the problem of assigning a semantic label to an entire input image. In this paper we address the problem of dense semantic labeling, which consists in assigning a semantic label to every pixel in an image. Since this requires a high spatial accuracy to determine where labels are assigned, categorization CNNs, intended to be highly robust to local deformations, are not directly applicable. By adapting categorization networks, many semantic labeling CNNs have been recently proposed. Our first contribution is an in-depth analysis of these architectures. We establish the desired properties of an ideal semantic labeling CNN, and assess how those methods stand with regard to these properties. We observe that even though they provide competitive results, these CNNs often underexploit properties of semantic labeling that could lead to more effective and efficient architectures. Out of these observations, we then derive a CNN framework specifically adapted to the semantic labeling problem. In addition to learning features at different resolutions, it learns how to combine these features. By integrating local and global information in an efficient and flexible manner, it outperforms previous techniques. We evaluate the proposed framework and compare it with state-of-the-art architectures on public benchmarks of high-resolution aerial image labeling. version:1
arxiv-1611-01957 | Linear Convergence of SVRG in Statistical Estimation | http://arxiv.org/abs/1611.01957 | id:1611.01957 author:Chao Qu, Huan Xu category:stat.ML cs.LG  published:2016-11-07 summary:SVRG and its variants are among the state of art optimization algorithms for the large scale machine learning problem. It is well known that SVRG converges linearly when the objective function is strongly convex. However this setup does not include several important formulations such as Lasso, group Lasso, logistic regression, among others. In this paper, we prove that, for a class of statistical M-estimators where {\em strong convexity does not hold}, SVRG can solve the formulation with {\em a linear convergence rate}. Our analysis makes use of {\em restricted strong convexity}, under which we show that SVRG converges linearly to the fundamental statistical precision of the model, i.e., the difference between true unknown parameter $\theta^*$ and the optimal solution $\hat{\theta}$ of the model. This improves previous convergence analysis on the non-strongly convex setup that achieves sub-linear convergence rate. version:1
arxiv-1611-01942 | DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing | http://arxiv.org/abs/1611.01942 | id:1611.01942 author:Shuochao Yao, Shaohan Hu, Yiran Zhao, Aston Zhang, Tarek Abdelzaher category:cs.LG cs.NE cs.NI  published:2016-11-07 summary:Mobile sensing applications usually require time-series inputs from sensors. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, DeepSense is feasible to implement on smartphones due to its moderate energy consumption and low latency version:1
arxiv-1611-01919 | Decision Tree Classification with Differential Privacy: A Survey | http://arxiv.org/abs/1611.01919 | id:1611.01919 author:Sam Fletcher, Md Zahidul Islam category:cs.DB cs.LG  published:2016-11-07 summary:Data mining information about people is becoming increasingly important in the data-driven society of the 21st century. Unfortunately, sometimes there are real-world considerations that conflict with the goals of data mining; sometimes the privacy of the people being data mined needs to be considered. This necessitates that the output of data mining algorithms be modified to preserve privacy while simultaneously not ruining the predictive power of the outputted model. Differential privacy is a strong, enforceable definition of privacy that can be used in data mining algorithms, guaranteeing that nothing will be learned about the people in the data that could not already be discovered without their participation. In this survey, we focus on one particular data mining algorithm -- decision trees -- and how differential privacy interacts with each of the components that constitute decision tree algorithms. We analyze both greedy and random decision trees, and the conflicts that arise when trying to balance privacy requirements with the accuracy of the model. version:1
arxiv-1611-01900 | Optimal rates for the regularized learning algorithms under general source condition | http://arxiv.org/abs/1611.01900 | id:1611.01900 author:S. Sivananthan Abhishake category:stat.ML 68T05  68Q32  published:2016-11-07 summary:We consider the learning algorithms under general source condition with the polynomial decay of the eigenvalues of the integral operator in vector-valued function setting. We discuss the upper convergence rates of Tikhonov regularizer under general source condition corresponding to increasing monotone index function. The convergence issues are studied for general regularization schemes by using the concept of operator monotone index functions in minimax setting. Further we also address the minimum possible error for any learning algorithm. version:1
arxiv-1611-01891 | Joint Multimodal Learning with Deep Generative Models | http://arxiv.org/abs/1611.01891 | id:1611.01891 author:Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo category:stat.ML cs.LG  published:2016-11-07 summary:We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally. version:1
arxiv-1611-01211 | Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear | http://arxiv.org/abs/1611.01211 | id:1611.01211 author:Zachary C. Lipton, Jianfeng Gao, Lihong Li, Jianshu Chen, Li Deng category:cs.LG cs.NE stat.ML  published:2016-11-03 summary:To use deep reinforcement learning in the wild, we might hope for an agent that would never make catastrophic mistakes. At the very least, we could hope that an agent would eventually learn to avoid old mistakes. Unfortunately, even in simple environments, modern deep reinforcement learning techniques are doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, state-aggregating agents may periodically relive catastrophic mistakes. We demonstrate unacceptable performance of deep Q-networks on two toy problems. We then introduce intrinsic fear, a method that mitigates these problems by avoiding dangerous states. version:2
arxiv-1611-01886 | An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax | http://arxiv.org/abs/1611.01886 | id:1611.01886 author:Wentao Huang, Kechen Zhang category:cs.LG cs.AI cs.IT math.IT q-bio.NC stat.ML  published:2016-11-07 summary:A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. From the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, allowing complete, overcomplete, or undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from image datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. version:1
arxiv-1611-01884 | AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification | http://arxiv.org/abs/1611.01884 | id:1611.01884 author:Depeng Liang, Yongdong Zhang category:cs.CL  published:2016-11-07 summary:Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling setences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long Short-Term Memory network (BLSTM). Experiment results demonstrate that our model achieves state-of-the-art results on all six tasks, including sentiment analysis, question type classification, and subjectivity classification. version:1
arxiv-1611-01875 | Challenges of Feature Selection for Big Data Analytics | http://arxiv.org/abs/1611.01875 | id:1611.01875 author:Jundong Li, Huan Liu category:cs.LG  published:2016-11-07 summary:We are surrounded by huge amounts of large-scale high dimensional data. It is desirable to reduce the dimensionality of data for many learning tasks due to the curse of dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive model, improving learning performance, and preparing clean, understandable data. Recently, some unique characteristics of big data such as data velocity and data variety present challenges to the feature selection problem. In this paper, we envision these challenges of feature selection for big data analytics. In particular, we first give a brief introduction about feature selection and then detail the challenges of feature selection for structured, heterogeneous and streaming data as well as its scalability and stability issues. At last, to facilitate and promote the feature selection research, we present an open-source feature selection repository (scikit-feature), which consists of most of current popular feature selection algorithms. version:1
arxiv-1611-01874 | Neural Machine Translation with Reconstruction | http://arxiv.org/abs/1611.01874 | id:1611.01874 author:Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, Hang Li category:cs.CL  published:2016-11-07 summary:Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems. version:1
arxiv-1611-01872 | Action2Activity: Recognizing Complex Activities from Sensor Data | http://arxiv.org/abs/1611.01872 | id:1611.01872 author:Ye Liu, Liqiang Nie, Lei Han, Luming Zhang, David S Rosenblum category:cs.CV  published:2016-11-07 summary:As compared to simple actions, activities are much more complex, but semantically consistent with a human's real life. Techniques for action recognition from sensor generated data are mature. However, there has been relatively little work on bridging the gap between actions and activities. To this end, this paper presents a novel approach for complex activity recognition comprising of two components. The first component is temporal pattern mining, which provides a mid-level feature representation for activities, encodes temporal relatedness among actions, and captures the intrinsic properties of activities. The second component is adaptive Multi-Task Learning, which captures relatedness among activities and selects discriminant features. Extensive experiments on a real-world dataset demonstrate the effectiveness of our work. version:1
arxiv-1611-01868 | Truth Discovery with Memory Network | http://arxiv.org/abs/1611.01868 | id:1611.01868 author:Luyang Li, Bing Qin, Wenjing Ren, Ting Liu category:cs.CL cs.DB  published:2016-11-07 summary:Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data (categorical data and continuous data) by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods. version:1
arxiv-1611-01867 | Latent Attention For If-Then Program Synthesis | http://arxiv.org/abs/1611.01867 | id:1611.01867 author:Xinyun Chen, Chang Liu, Richard Shin, Dawn Song, Mingcheng Chen category:cs.CL  published:2016-11-07 summary:Automatic translation from natural language descriptions into programs is a longstanding challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data. version:1
arxiv-1610-05755 | Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data | http://arxiv.org/abs/1610.05755 | id:1610.05755 author:Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, Kunal Talwar category:stat.ML cs.CR cs.LG  published:2016-10-18 summary:Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data. The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. version:3
arxiv-1611-00712 | The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables | http://arxiv.org/abs/1611.00712 | id:1611.00712 author:Chris J. Maddison, Andriy Mnih, Yee Whye Teh category:cs.LG stat.ML  published:2016-11-02 summary:The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate effectiveness of concrete relaxations on density estimation and structured prediction tasks using neural networks. version:2
arxiv-1611-01845 | Urban Distribution Grid Topology Estimation via Group Lasso | http://arxiv.org/abs/1611.01845 | id:1611.01845 author:Yizheng Liao, Yang Weng, Guangyi Liu, Ram Rajagopal category:stat.ML cs.SY math.OC  published:2016-11-06 summary:The growing penetration of distributed energy resources (DERs) in urban areas raises multiple reliability issues. The topology reconstruction is a critical step to ensure the robustness of distribution grid operation. However, the bus connectivity and network topology reconstruction are hard in distribution grids. The reasons are that 1) the branches are challenging and expensive to monitor due to underground setup; 2) the inappropriate assumption of radial topology in many studies that urban grids are mesh. To address these drawbacks, we propose a new data-driven approach to reconstruct distribution grid topology by utilizing the newly available smart meter data. Specifically, a graphical model is built to model the probabilistic relationships among different voltage measurements. With proof, the bus connectivity and topology estimation problems are formulated as a linear regression problem with least absolute shrinkage on grouped variables (Group Lasso) to deal with meshed network structures. Simulation results show highly accurate estimation in IEEE standard distribution test systems with and without loops using real smart meter data. version:1
arxiv-1611-01839 | Hierarchical Question Answering for Long Documents | http://arxiv.org/abs/1611.01839 | id:1611.01839 author:Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, Jonathan Berant category:cs.CL  published:2016-11-06 summary:Reading an article and answering questions about its content is a fundamental task for natural language understanding. While most successful neural approaches to this problem rely on recurrent neural networks (RNNs), training RNNs over long documents can be prohibitively slow. We present a novel framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance. Our approach combines a coarse, inexpensive model for selecting one or more relevant sentences and a more expensive RNN that produces the answer from those sentences. A central challenge is the lack of intermediate supervision for the coarse model, which we address using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset(Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88% against a standard sequence to sequence model. version:1
arxiv-1611-00481 | Online Multi-view Clustering with Incomplete Views | http://arxiv.org/abs/1611.00481 | id:1611.00481 author:Weixiang Shao, Lifang He, Chun-Ta Lu, Philip S. Yu category:cs.LG  published:2016-11-02 summary:In the era of big data, it is common to have data with multiple modalities or coming from multiple sources, known as "multi-view data". Multi-view clustering provides a natural way to generate clusters from such data. Since different views share some consistency and complementary information, previous works on multi-view clustering mainly focus on how to combine various numbers of views to improve clustering performance. However, in reality, each view may be incomplete, i.e., instances missing in the view. Furthermore, the size of data could be extremely huge. It is unrealistic to apply multi-view clustering in large real-world applications without considering the incompleteness of views and the memory requirement. None of previous works have addressed all these challenges simultaneously. In this paper, we propose an online multi-view clustering algorithm, OMVC, which deals with large-scale incomplete views. We model the multi-view clustering problem as a joint weighted nonnegative matrix factorization problem and process the multi-view data chunk by chunk to reduce the memory requirement. OMVC learns the latent feature matrices for all the views and pushes them towards a consensus. We further increase the robustness of the learned latent feature matrices in OMVC via lasso regularization. To minimize the influence of incompleteness, dynamic weight setting is introduced to give lower weights to the incoming missing instances in different views. More importantly, to reduce the computational time, we incorporate a faster projected gradient descent by utilizing the Hessian matrices in OMVC. Extensive experiments conducted on four real data demonstrate the effectiveness of the proposed OMVC method. version:2
arxiv-1611-01799 | Generative Adversarial Networks as Variational Training of Energy Based Models | http://arxiv.org/abs/1611.01799 | id:1611.01799 author:Shuangfei Zhai, Yu Cheng, Rogerio Feris, Zhongfei Zhang category:cs.LG  published:2016-11-06 summary:In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\mathbf{x})$ is approximated by a variational distribution $q(\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\mathbf{x})$, $q(\mathbf{x})$ is updated to maximize the lower bound; $p(\mathbf{x})$ is then updated one step with samples drawn from $q(\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\mathbf{x})$ corresponds to the discriminator and $q(\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions. \footnote{Experimental code is available at https://github.com/Shuangfei/vgan} version:1
arxiv-1611-01796 | Modular Multitask Reinforcement Learning with Policy Sketches | http://arxiv.org/abs/1611.01796 | id:1611.01796 author:Jacob Andreas, Dan Klein, Sergey Levine category:cs.LG cs.NE  published:2016-11-06 summary:We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate each task with a sequence of named subtasks, providing high-level structural relationships among tasks, but not providing the detailed guidance required by previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). Our approach associates every subtask with its own modular subpolicy, and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. This optimization is accomplished via a simple decoupled actor-critic training objective that facilitates learning common behaviors from dissimilar reward functions. We evaluate the effectiveness of our approach on a maze navigation game and a 2-D Minecraft-inspired crafting game. Both games feature extremely sparse rewards that can be obtained only after completing a number of high-level subgoals (e.g. escaping from a sequence of locked rooms or collecting and combining various ingredients in the proper order). Experiments illustrate two main advantages of our approach. First, we outperform standard baselines that learn task-specific or shared monolithic policies. Second, our method naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks. version:1
arxiv-1611-01787 | Learning to superoptimize programs | http://arxiv.org/abs/1611.01787 | id:1611.01787 author:Rudy Bunel, Alban Desmaison, M. Pawan Kumar, Philip H. S. Torr, Pushmeet Kohli category:cs.LG  published:2016-11-06 summary:Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing ("Hacker's Delight") programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization. version:1
arxiv-1611-01783 | Domain Adaptation For Formant Estimation Using Deep Learning | http://arxiv.org/abs/1611.01783 | id:1611.01783 author:Yehoshua Dissen, Joseph Keshet, Jacob Goldberger, Cynthia Clopper category:cs.CL cs.SD  published:2016-11-06 summary:In this paper we present a domain adaptation technique for formant estimation using a deep network. We first train a deep learning network on a small read speech dataset. We then freeze the parameters of the trained network and use several different datasets to train an adaptation layer that makes the obtained network universal in the sense that it works well for a variety of speakers and speech domains with very different characteristics. We evaluated our adapted network on three datasets, each of which has different speaker characteristics and speech styles. The performance of our method compares favorably with alternative methods for formant estimation. version:1
arxiv-1611-01779 | Learning to Act by Predicting the Future | http://arxiv.org/abs/1611.01779 | id:1611.01779 author:Alexey Dosovitskiy, Vladlen Koltun category:cs.LG cs.AI cs.CV  published:2016-11-06 summary:We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation allows changing the agent's goal at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments. version:1
arxiv-1611-01773 | The Shallow End: Empowering Shallower Deep-Convolutional Networks through Auxiliary Outputs | http://arxiv.org/abs/1611.01773 | id:1611.01773 author:Yong Guo, Mingkui Tan, Qingyao Wu, Jian Chen, Anton Van Den Hengel, Qinfeng Shi category:cs.CV  published:2016-11-06 summary:Convolutional neural networks (CNNs) with very deep architectures, such as the residual network (ResNet) [6], have shown encouraging results in various tasks in computer vision and machine learning. Their depth has been one of the key factors behind the great success of CNNs, with the associated gradient vanishing issue having been largely addressed by ResNet. There are other issues associated with increased depth, however. First, when networks get very deep, the supervision information may vanish due to the associated long backpropagation path. This means that intermediate layers receive less training information, which results in redundancy in models. Second, when the model becomes more complex and redundant, inference becomes more expensive. Third, very deep models require larger volumes of training data. We propose here instead an AuxNet and a new training method to propagate not only gradients but also supervision information from multiple auxiliary outputs at intermediate layers. The proposed AuxNet gives rise to a more compact network which outperforms its very deep equivalent (i.e. ResNet). For example, AuxNet with 44 layers performs better than the original ResNet with 110 layers on several benchmark data sets, i.e. CIFAR-10, CIFAR-100 and SVHN. version:1
arxiv-1611-01767 | EM Algorithm and Stochastic Control in Economics | http://arxiv.org/abs/1611.01767 | id:1611.01767 author:Steven Kou, Xianhua Peng, Xingbo Xu category:q-fin.EC math.OC stat.ML  published:2016-11-06 summary:Generalising the idea of the classical EM algorithm that is widely used for computing maximum likelihood estimates, we propose an EM-Control (EM-C) algorithm for solving multi-period finite time horizon stochastic control problems. The new algorithm sequentially updates the control policies in each time period using Monte Carlo simulation in a forward-backward manner; in other words, the algorithm goes forward in simulation and backward in optimization in each iteration. Similar to the EM algorithm, the EM-C algorithm has the monotonicity of performance improvement in each iteration, leading to good convergence properties. We demonstrate the effectiveness of the algorithm by solving stochastic control problems in the monopoly pricing of perishable assets and in the study of real business cycle. version:1
arxiv-1611-01752 | Learning a Static Analyzer from Data | http://arxiv.org/abs/1611.01752 | id:1611.01752 author:Pavol Bielik, Veselin Raychev, Martin Vechev category:cs.PL cs.LG  published:2016-11-06 summary:To be practically useful, modern static analyzers must precisely model the effect of both, statements in the programming language as well as frameworks used by the program under analysis. While important, manually addressing these challenges is difficult for at least two reasons: (i) the effects on the overall analysis can be non-trivial, and (ii) as the size and complexity of modern libraries increase, so is the number of cases the analysis must handle. In this paper we present a new, automated approach for creating static analyzers: instead of manually providing the various inference rules of the analyzer, the key idea is to learn these rules from a dataset of programs. Our method consists of two ingredients: (i) a synthesis algorithm capable of learning a candidate analyzer from a given dataset, and (ii) a counter-example guided learning procedure which generates new programs beyond those in the initial dataset, critical for discovering corner cases and ensuring the learned analysis generalizes to unseen programs. We implemented and instantiated our approach to the task of learning a points-to analysis for JavaScript, a challenging yet important problem that has received significant research attention. We show that our approach is effective: our system automatically discovered practical and useful inference rules for many corner cases that are tricky to manually identify and are missed by state-of-the-art, manually tuned solutions. version:1
arxiv-1611-01751 | Deep Convolutional Neural Network Features and the Original Image | http://arxiv.org/abs/1611.01751 | id:1611.01751 author:Connor J. Parde, Carlos Castillo, Matthew Q. Hill, Y. Ivette Colon, Swami Sankaranarayanan, Jun-Cheng Chen, Alice J. O'Toole category:cs.CV  published:2016-11-06 summary:Face recognition algorithms based on deep convolutional neural networks (DCNNs) have made progress on the task of recognizing faces in unconstrained viewing conditions. These networks operate with compact feature-based face representations derived from learning a very large number of face images. While the learned features produced by DCNNs can be highly robust to changes in viewpoint, illumination, and appearance, little is known about the nature of the face code that emerges at the top level of such networks. We analyzed the DCNN features produced by two face recognition algorithms. In the first set of experiments we used the top-level features from the DCNNs as input into linear classifiers aimed at predicting metadata about the images. The results show that the DCNN features contain surprisingly accurate information about the yaw and pitch of a face, and about whether the face came from a still image or a video frame. In the second set of experiments, we measured the extent to which individual DCNN features operated in a view-dependent or view-invariant manner. We found that view-dependent coding was a characteristic of the identities rather than the DCNN features - with some identities coded consistently in a view-dependent way and others in a view-independent way. In our third analysis, we visualized the DCNN feature space for over 24,000 images of 500 identities. Images in the center of the space were uniformly of low quality (e.g., extreme views, face occlusion, low resolution). Image quality increased monotonically as a function of distance from the origin. This result suggests that image quality information is available in the DCNN features, such that consistently average feature values reflect coding failures that reliably indicate poor or unusable images. Combined, the results offer insight into the coding mechanisms that support robust representation of faces in DCNNs. version:1
arxiv-1611-01747 | A Compare-Aggregate Model for Matching Text Sequences | http://arxiv.org/abs/1611.01747 | id:1611.01747 author:Shuohang Wang, Jing Jiang category:cs.CL cs.AI  published:2016-11-06 summary:Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general "compare-aggregate" framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network. version:1
arxiv-1611-01734 | Deep Biaffine Attention for Neural Dependency Parsing | http://arxiv.org/abs/1611.01734 | id:1611.01734 author:Timothy Dozat, Christopher D. Manning category:cs.CL cs.NE  published:2016-11-06 summary:While deep learning parsing approaches have proven very successful at finding the structure of sentences, most neural dependency parsers use neural networks only for feature extraction, and then use those features in traditional parsing algorithms. In contrast, this paper builds off recent work using general-purpose neural network components, training an attention mechanism over an LSTM to attend to the head of the phrase. We get state-of-the-art results for standard dependency parsing benchmarks, achieving 95.44% UAS and 93.76% LAS on the PTB dataset, 0.8% and 1.0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model's performance, such as reducing the value of beta2 in the Adam optimization algorithm. version:1
arxiv-1611-01731 | Deep Label Distribution Learning with Label Ambiguity | http://arxiv.org/abs/1611.01731 | id:1611.01731 author:Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, Xin Geng category:cs.CV  published:2016-11-06 summary:Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which prevents the network from over-fitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks. version:1
arxiv-1611-01730 | Validation of Tsallis Entropy In Inter-Modality Neuroimage Registration | http://arxiv.org/abs/1611.01730 | id:1611.01730 author:Henrique Tomaz Amaral-Silva, Luiz Otavio Murta-Jr, Paulo Mazzoncini de , Lauro Wichert-Ana, V. B. Surya Prasath, Colin Studholme category:cs.CV  published:2016-11-06 summary:Medical image registration plays an important role in determining topographic and morphological changes for functional diagnostic and therapeutic purposes. Manual alignment and semi-automated software still have been used; however they are subjective and make specialists spend precious time. Fully automated methods are faster and user-independent, but the critical point is registration reliability. Similarity measurement using Mutual Information (MI) with Shannon entropy (MIS) is the most common automated method that is being currently applied in medical images, although more reliable algorithms have been proposed over the last decade, suggesting improvements and different entropies; such as Studholme et al, (1999), who demonstrated that the normalization of Mutual Information (NMI) provides an invariant entropy measure for 3D medical image registration. In this paper, we described a set of experiments to evaluate the applicability of Tsallis entropy in the Mutual Information (MIT) and in the Normalized Mutual Information (NMIT) as cost functions for Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) exams registration. The effect of changing overlap in a simple image model and clinical experiments on current entropies (Entropy Correlation Coefficient - ECC, MIS and NMI) and the proposed ones (MIT and NMT) showed NMI and NMIT with Tsallis parameter close to 1 as the best options (confidence and accuracy) for CT to MRI and PET to MRI automatic neuroimaging registration. version:1
arxiv-1611-01726 | LSTM-Based System-Call Language Modeling and Robust Ensemble Method for Designing Host-Based Intrusion Detection Systems | http://arxiv.org/abs/1611.01726 | id:1611.01726 author:Gyuwan Kim, Hayoon Yi, Jangho Lee, Yunheung Paek, Sungroh Yoon category:cs.CR cs.LG  published:2016-11-06 summary:In computer security, designing a robust intrusion detection system is one of the most fundamental and important problems. In this paper, we propose a system-call language-modeling approach for designing anomaly-based host intrusion detection systems. To remedy the issue of high false-alarm rates commonly arising in conventional methods, we employ a novel ensemble method that blends multiple thresholding classifiers into a single one, making it possible to accumulate 'highly normal' sequences. The proposed system-call language model has various advantages leveraged by the fact that it can learn the semantic meaning and interactions of each system call that existing methods cannot effectively consider. Through diverse experiments on public benchmark datasets, we demonstrate the validity and effectiveness of the proposed method. Moreover, we show that our model possesses high portability, which is one of the key aspects of realizing successful intrusion detection systems. version:1
arxiv-1611-01724 | Words or Characters? Fine-grained Gating for Reading Comprehension | http://arxiv.org/abs/1611.01724 | id:1611.01724 author:Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, Ruslan Salakhutdinov category:cs.CL cs.LG  published:2016-11-06 summary:Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task. version:1
arxiv-1611-01722 | Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning | http://arxiv.org/abs/1611.01722 | id:1611.01722 author:Dilin Wang, Qiang Liu category:stat.ML cs.LG  published:2016-11-06 summary:We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results. version:1
arxiv-1611-01714 | Beyond Fine Tuning: A Modular Approach to Learning on Small Data | http://arxiv.org/abs/1611.01714 | id:1611.01714 author:Ark Anderson, Kyle Shaffer, Artem Yankov, Court D. Corley, Nathan O. Hodas category:cs.LG cs.CL  published:2016-11-06 summary:In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data. version:1
arxiv-1611-01708 | Detecting Dependencies in High-Dimensional, Sparse Databases Using Probabilistic Programming and Non-parametric Bayes | http://arxiv.org/abs/1611.01708 | id:1611.01708 author:Feras Saad, Vikash Mansinghka category:stat.ML cs.AI cs.LG  published:2016-11-05 summary:Sparse databases with hundreds of variables are commonplace. In this setting, it is both statistically and computationally challenging to detect true predictive relationships between variables and also to suppress false positives. This paper proposes a new approach to dependency detection that combines probabilistic programming, information theory, and non-parametric Bayesian modeling. The key ideas are to (i) build an ensemble of joint probability models for the whole database via approximate posterior inference in CrossCat, a non-parametric factorial mixture; (ii) identify independencies by analyzing model structures; and (iii) report the distribution on conditional mutual information induced by posterior uncertainty over the ensemble of models. This paper presents experiments showing that the approach finds relationships that pairwise correlation misses, including context-specific independencies, on databases of mathematics exam scores and global indicators of macroeconomic development. version:1
arxiv-1611-01704 | End-to-end Optimized Image Compression | http://arxiv.org/abs/1611.01704 | id:1611.01704 author:Johannes Ballé, Valero Laparra, Eero P. Simoncelli category:cs.CV cs.IT math.IT  published:2016-11-05 summary:We describe an image compression system, consisting of a nonlinear encoding transformation, a uniform quantizer, and a nonlinear decoding transformation. Like many deep neural network architectures, the transforms consist of layers of convolutional linear filters and nonlinear activation functions, but we use a joint nonlinearity that implements a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the system for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. The relaxed optimization problem resembles that of variational autoencoders, except that it must operate at any point along the rate-distortion curve, whereas the optimization of generative models aims only to minimize entropy of the data under the model. Across an independent database of test images, we find that the optimized coder exhibits significantly better rate-distortion performance than the standard JPEG and JPEG 2000 compression systems, as well as a dramatic improvement in visual quality of compressed images. version:1
arxiv-1611-01702 | TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency | http://arxiv.org/abs/1611.01702 | id:1611.01702 author:Adji B. Dieng, Chong Wang, Jianfeng Gao, John Paisley category:cs.CL cs.AI cs.LG stat.ML  published:2016-11-05 summary:In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis and report a new state-of-the-art error rate on the IMDB movie review dataset that amounts to a $13.3\%$ improvement over the previous best result. Finally TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation. version:1
arxiv-1611-01688 | Oracle-Efficient Learning and Auction Design | http://arxiv.org/abs/1611.01688 | id:1611.01688 author:Miroslav Dudík, Nika Haghtalab, Haipeng Luo, Robert E. Schapire, Vasilis Syrgkanis, Jennifer Wortman Vaughan category:cs.LG cs.DS cs.GT  published:2016-11-05 summary:We consider the design of online no-regret algorithms that are computationally efficient, given access to an offline optimization oracle. We present an algorithm we call Generalized Follow-the-Perturbed-Leader and provide conditions under which it achieves vanishing regret and is oracle-efficient. Our second main contribution is introducing a new adversarial auction-design framework for revenue maximization and applying our oracle-efficient learning results to the adaptive optimization of auctions. Our learning algorithm is a generalization of the FTPL algorithm of Kalai and Vempala that at every step plays the best-performing action subject to some perturbations. Our design uses a shared source of randomness across all actions that can be efficiently implemented using an oracle. Our work extends to oracle-efficient algorithms for contextual learning, learning with Maximal-in-Range approximation algorithms, and no-regret bidding in simultaneous auctions, answering an open problem of Daskalakis and Syrgkanis in the latter case. Our auction-design framework considers an auctioneer learning an optimal auction for a sequence of adversarially selected valuations with the goal of achieving revenue that is almost as good as the optimal auction in hindsight, among a class of auctions. We give oracle-efficient learning results for: (1) VCG auctions with bidder-specific reserves in single-parameter settings, (2) envy-free item pricing in multi-item auctions, and (3) s-level auctions of Morgenstern and Roughgarden for single-item settings. The last result leads to an approximation of the optimal Myerson auction for the stationary distribution of a Markov process, extending prior work that only gave such guarantees for the i.i.d. setting. We also extend our framework to allow the auctioneer to use side information about the bidders in the design of the optimal auction (contextual learning). version:1
arxiv-1611-01678 | Comparing learning algorithms in neural network for diagnosing cardiovascular disease | http://arxiv.org/abs/1611.01678 | id:1611.01678 author:Mirmorsal Madani category:cs.LG cs.NE  published:2016-11-05 summary:Today data mining techniques are exploited in medical science for diagnosing, overcoming and treating diseases. Neural network is one of the techniques which are widely used for diagnosis in medical field. In this article efficiency of nine algorithms, which are basis of neural network learning in diagnosing cardiovascular diseases, will be assessed. Algorithms are assessed in terms of accuracy, sensitivity, transparency, AROC and convergence rate by means of 10 fold cross validation. The results suggest that in training phase, Lonberg-M algorithm has the best efficiency in terms of all metrics, algorithm OSS has maximum accuracy in testing phase, algorithm SCG has the maximum transparency and algorithm CGB has the maximum sensitivity. version:1
arxiv-1611-01673 | Generative Multi-Adversarial Networks | http://arxiv.org/abs/1611.01673 | id:1611.01673 author:Ishan Durugkar, Ian Gemp, Sridhar Mahadevan category:cs.LG cs.MA cs.NE  published:2016-11-05 summary:Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the \emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric. version:1
arxiv-1611-01655 | Twenty (simple) questions | http://arxiv.org/abs/1611.01655 | id:1611.01655 author:Yuval Dagan, Yuval Filmus, Ariel Gabizon, Shay Moran category:cs.DM cs.DS cs.IT cs.LG math.CO math.IT  published:2016-11-05 summary:A basic combinatorial interpretation of Shannon's entropy function is via the "20 questions" game. This cooperative game is played by two players, Alice and Bob: Alice picks a distribution $\pi$ over the numbers $\{1,\ldots,n\}$, and announces it to Bob. She then chooses a number $x$ according to $\pi$, and Bob attempts to identify $x$ using as few Yes/No queries as possible, on average. An optimal strategy for the "20 questions" game is given by a Huffman code for $\pi$: Bob's questions reveal the codeword for $x$ bit by bit. This strategy finds $x$ using fewer than $H(\pi)+1$ questions on average. However, the questions asked by Bob could be arbitrary. In this paper, we investigate the following question: Are there restricted sets of questions that match the performance of Huffman codes, either exactly or approximately? Our first main result shows that for every distribution $\pi$, Bob has a strategy that uses only questions of the form "$x < c$?" and "$x = c$?", and uncovers $x$ using at most $H(\pi)+1$ questions on average, matching the performance of Huffman codes in this sense. We also give a natural set of $O(rn^{1/r})$ questions that achieve a performance of at most $H(\pi)+r$, and show that $\Omega(rn^{1/r})$ questions are required to achieve such a guarantee. Our second main result gives a set $\mathcal{Q}$ of $1.25^{n+o(n)}$ questions such that for every distribution $\pi$, Bob can implement an optimal strategy for $\pi$ using only questions from $\mathcal{Q}$. We also show that $1.25^{n-o(n)}$ questions are needed, for infinitely many $n$. If we allow a small slack of $r$ over the optimal strategy, then roughly $(rn)^{\Theta(1/r)}$ questions are necessary and sufficient. version:1
arxiv-1611-01652 | A Differentiable Physics Engine for Deep Learning in Robotics | http://arxiv.org/abs/1611.01652 | id:1611.01652 author:Jonas Degrave, Michiel Hermans, Joni Dambre, Francis wyffels category:cs.NE cs.AI cs.RO  published:2016-11-05 summary:One of the most important fields in robotics is the optimization of controllers. Currently, robots are treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. We propose an implementation of a modern physics engine, which has the ability to differentiate control parameters. This has been implemented on both CPU and GPU. We show how this speeds up the optimization process, even for small problems, and why it will scale to bigger problems. We explain why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Lastly, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software. version:1
arxiv-1611-01646 | Boosting Image Captioning with Attributes | http://arxiv.org/abs/1611.01646 | id:1611.01646 author:Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei category:cs.CV  published:2016-11-05 summary:Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard. version:1
arxiv-1611-01642 | GPU-based Pedestrian Detection for Autonomous Driving | http://arxiv.org/abs/1611.01642 | id:1611.01642 author:Victor Campmany, Sergio Silva, Antonio Espinosa, Juan Carlos Moure, David Vázquez, Antonio M. López category:cs.CV  published:2016-11-05 summary:We propose a real-time pedestrian detection system for the embedded Nvidia Tegra X1 GPU-CPU hybrid platform. The pipeline is composed by the following state-of-the-art algorithms: Histogram of Local Binary Patterns (LBP) and Histograms of Oriented Gradients (HOG) features extracted from the input image; Pyramidal Sliding Window technique for candidate generation; and Support Vector Machine (SVM) for classification. Results show a 8x speedup in the target Tegra X1 platform and a better performance/watt ratio than desktop CUDA platforms in study. version:1
arxiv-1611-01640 | What Is the Best Practice for CNNs Applied to Visual Instance Retrieval? | http://arxiv.org/abs/1611.01640 | id:1611.01640 author:Jiedong Hao, Jing Dong, Wei Wang, Tieniu Tan category:cs.CV  published:2016-11-05 summary:Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them. The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify the best choices for different factors and propose a new multi-scale image feature representation method to encode the image effectively. Finally, we show that the proposed method generalises well and outperforms the state-of-the-art methods on four typical datasets used for visual instance retrieval. version:1
arxiv-1611-01626 | PGQ: Combining policy gradient and Q-learning | http://arxiv.org/abs/1611.01626 | id:1611.01626 author:Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, Volodymyr Mnih category:cs.LG cs.AI math.OC stat.ML  published:2016-11-05 summary:Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQ', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQ. In particular, we tested PGQ on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning. version:1
arxiv-1611-01606 | Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening | http://arxiv.org/abs/1611.01606 | id:1611.01606 author:Frank S. He, Yang Liu, Alexander G. Schwing, Jian Peng category:cs.LG stat.ML  published:2016-11-05 summary:We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy. version:1
arxiv-1611-01603 | Bidirectional Attention Flow for Machine Comprehension | http://arxiv.org/abs/1611.01603 | id:1611.01603 author:Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi category:cs.CL  published:2016-11-05 summary:Machine Comprehension (MC), answering questions about a given context, re-quires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these mechanisms use attention to summarize the query and context into a single vectors, couple attentions temporally, and often form a unidirectional attention. In this paper we introduce the Bidirectional Attention Flow (BIDAF) Model, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford QA(SQuAD) and CNN/DailyMail Cloze Test datasets. version:1
arxiv-1611-01600 | Loss-aware Binarization of Deep Networks | http://arxiv.org/abs/1611.01600 | id:1611.01600 author:Lu Hou, Quanming Yao, James T. Kwok category:cs.NE cs.LG  published:2016-11-05 summary:Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximation and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks. version:1
arxiv-1611-01599 | LipNet: Sentence-level Lipreading | http://arxiv.org/abs/1611.01599 | id:1611.01599 author:Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas category:cs.LG cs.CL cs.CV  published:2016-11-05 summary:Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy. version:1
arxiv-1611-01590 | Alternating Direction Method of Multipliers for Sparse Convolutional Neural Networks | http://arxiv.org/abs/1611.01590 | id:1611.01590 author:Farkhondeh Kiaee, Christian Gagné, Mahdieh Abbasi category:cs.NE  published:2016-11-05 summary:The storage and computation requirements of Convolutional Neural Networks (CNNs) can be prohibitive for exploiting these models over low-power or embedded devices. This paper reduces the computational complexity of the CNNs by minimizing an objective function, including the recognition loss that is augmented with a sparsity-promoting penalty term. The sparsity structure of the network is identified using the Alternating Direction Method of Multipliers (ADMM), which is widely used in large optimization problems. This method alternates between promoting the sparsity of the network and optimizing the recognition performance, which allows us to exploit the two-part structure of the corresponding objective functions. In particular, we take advantage of the separability of the sparsity-inducing penalty functions to decompose the minimization problem into sub-problems that can be solved sequentially. Applying our method to a variety of state-of-the-art CNN models, our proposed method is able to simplify the original model, generating models with less computation and fewer parameters, while maintaining and often improving generalization performance. Accomplishments on a variety of models strongly verify that our proposed ADMM-based method can be a very useful tool for simplifying and improving deep CNNs. version:1
arxiv-1611-01586 | Class-prior Estimation for Learning from Positive and Unlabeled Data | http://arxiv.org/abs/1611.01586 | id:1611.01586 author:Marthinus C. du Plessis, Gang Niu, Masashi Sugiyama category:cs.LG stat.ML  published:2016-11-05 summary:We consider the problem of estimating the class prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized $L_1$-distance gives a computationally efficient algorithm with an analytic solution. The consistency, stability, and estimation error are theoretically analyzed. Finally, we experimentally demonstrate the usefulness of the proposed method. version:1
arxiv-1611-01578 | Neural Architecture Search with Reinforcement Learning | http://arxiv.org/abs/1611.01578 | id:1611.01578 author:Barret Zoph, Quoc V. Le category:cs.LG cs.AI cs.NE  published:2016-11-05 summary:Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art. version:1
arxiv-1611-01576 | Quasi-Recurrent Neural Networks | http://arxiv.org/abs/1611.01576 | id:1611.01576 author:James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher category:cs.NE cs.AI cs.CL cs.LG  published:2016-11-05 summary:Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. version:1
arxiv-1611-02272 | Neuromorphic Silicon Photonics | http://arxiv.org/abs/1611.02272 | id:1611.02272 author:Alexander N. Tait, Ellen Zhou, Thomas Ferreira de Lima, Allie X. Wu, Mitchell A. Nahmias, Bhavin J. Shastri, Paul R. Prucnal category:q-bio.NC cs.NE physics.optics  published:2016-11-05 summary:We report first observations of an integrated analog photonic network, in which connections are configured by microring weight banks, as well as the first use of electro-optic modulators as photonic neurons. A mathematical isomorphism between the silicon photonic circuit and a continuous neural model is demonstrated through dynamical bifurcation analysis. Exploiting this isomorphism, existing neural engineering tools can be adapted to silicon photonic information processing systems. A 49-node silicon photonic neural network programmed using a "neural compiler" is simulated and predicted to outperform a conventional approach 1,960-fold in a toy differential system emulation task. Photonic neural networks leveraging silicon photonic platforms could access new regimes of ultrafast information processing for radio, control, and scientific computing. version:1
arxiv-1611-01541 | Classification with Ultrahigh-Dimensional Features | http://arxiv.org/abs/1611.01541 | id:1611.01541 author:Yanming Li, Hyokyoung Hong, Jian Kang, Kevin He, Ji Zhu, Yi Li category:stat.ML cs.LG  published:2016-11-04 summary:Although much progress has been made in classification with high-dimensional features \citep{Fan_Fan:2008, JGuo:2010, CaiSun:2014, PRXu:2014}, classification with ultrahigh-dimensional features, wherein the features much outnumber the sample size, defies most existing work. This paper introduces a novel and computationally feasible multivariate screening and classification method for ultrahigh-dimensional data. Leveraging inter-feature correlations, the proposed method enables detection of marginally weak and sparse signals and recovery of the true informative feature set, and achieves asymptotic optimal misclassification rates. We also show that the proposed procedure provides more powerful discovery boundaries compared to those in \citet{CaiSun:2014} and \citet{JJin:2009}. The performance of the proposed procedure is evaluated using simulation studies and demonstrated via classification of patients with different post-transplantation renal functional types. version:1
arxiv-1611-01540 | Topology and Geometry of Deep Rectified Network Optimization Landscapes | http://arxiv.org/abs/1611.01540 | id:1611.01540 author:C. Daniel Freeman, Joan Bruna category:stat.ML cs.LG  published:2016-11-04 summary:The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important \emph{folklore} facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. These results are in accordance with empirical practice and recent literature. %Together with %recent results that rigorously establish that no gradient descent can %get stuck on saddle points, we conclude that gradient descent converges %to a global optimum in deep rectified networks. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors. version:1
arxiv-1611-00736 | Extensions and Limitations of the Neural GPU | http://arxiv.org/abs/1611.00736 | id:1611.00736 author:Eric Price, Wojciech Zaremba, Ilya Sutskever category:cs.NE cs.AI  published:2016-11-02 summary:The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires a memory efficient implementation, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with perfect accuracy. In addition, we gain insight into the Neural GPU by investigating its failure modes. We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on $000000\dots002 \times 000000\dots002$ while succeeding at $2 \times 2$. These failure modes are reminiscent of adversarial examples. version:2
arxiv-1611-01504 | Estimating Causal Direction and Confounding of Two Discrete Variables | http://arxiv.org/abs/1611.01504 | id:1611.01504 author:Krzysztof Chalupka, Frederick Eberhardt, Pietro Perona category:stat.ML cs.AI cs.LG  published:2016-11-04 summary:We propose a method to classify the causal relationship between two discrete variables given only the joint distribution of the variables, acknowledging that the method is subject to an inherent baseline error. We assume that the causal system is acyclicity, but we do allow for hidden common causes. Our algorithm presupposes that the probability distributions $P(C)$ of a cause $C$ is independent from the probability distribution $P(E\mid C)$ of the cause-effect mechanism. While our classifier is trained with a Bayesian assumption of flat hyperpriors, we do not make this assumption about our test data. This work connects to recent developments on the identifiability of causal models over continuous variables under the assumption of "independent mechanisms". Carefully-commented Python notebooks that reproduce all our experiments are available online at http://vision.caltech.edu/~kchalupk/code.html. version:1
arxiv-1611-01503 | Protein Secondary Structure Prediction Using Deep Multi-scale Convolutional Neural Networks and Next-Step Conditioning | http://arxiv.org/abs/1611.01503 | id:1611.01503 author:Akosua Busia, Jasmine Collins, Navdeep Jaitly category:cs.LG q-bio.BM  published:2016-11-04 summary:Recently developed deep learning techniques have significantly improved the accuracy of various speech and image recognition systems. In this paper we adapt some of these techniques for protein secondary structure prediction. We first train a series of deep neural networks to predict eight-class secondary structure labels given a protein's amino acid sequence information and find that using recent methods for regularization, such as dropout and weight-norm constraining, leads to measurable gains in accuracy. We then adapt recent convolutional neural network architectures--Inception, ReSNet, and DenseNet with Batch Normalization--to the problem of protein structure prediction. These convolutional architectures make heavy use of multi-scale filter layers that simultaneously compute features on several scales, and use residual connections to prevent underfitting. Using a carefully modified version of these architectures, we achieve state-of-the-art performance of 70.0% per amino acid accuracy on the public CB513 benchmark dataset. Finally, we explore additions from sequence-to-sequence learning, altering the model to make its predictions conditioned on both the protein's amino acid sequence and its past secondary structure labels. We introduce a new method of ensembling such a conditional model with our convolutional model, an approach which reaches 70.6% Q8 accuracy on CB513. We argue that these results can be further refined for larger boosts in prediction accuracy through more sophisticated attempts to control overfitting of conditional models. We aim to release the code for these experiments as part of the TensorFlow repository. version:1
arxiv-1611-00674 | Fuzzy paraphrases in learning word representations with a corpus and a lexicon | http://arxiv.org/abs/1611.00674 | id:1611.00674 author:Yuanzhi Ke, Masafumi Hagiwara category:cs.CL  published:2016-11-02 summary:There is a problem that is not carefully addressed in the previous works using lexicons or ontologies to train or improve distributed word representations: For polysemous words and utterances changing meaning in different contexts, their paraphrases or related entities in a lexicon or an ontology are unreliable and sometimes deteriorate the learning of word representations. Thus, we propose an approach to address the problem. We consider each paraphrase of a word in a lexicon not fully a paraphrase, but a fuzzy member (fuzzy paraphrase) in the paraphrase set whose membership (i.e., degree of truth) depends on the contexts. Then we propose an efficient method to use the fuzzy paraphrases to learn word embeddings. We approximately estimate the local membership of paraphrases, and train word embeddings using a lexicon jointly by replacing the words in the contexts with their paraphrases randomly subject to the membership of each paraphrase. The experimental results show that our method is efficient, overcomes the weakness of the previous related works in extracting semantic information and outperforms the previous works of learning word representations using lexicons. version:3
arxiv-1611-01487 | Sequence to Sequence Transduction with Hard Monotonic Attention | http://arxiv.org/abs/1611.01487 | id:1611.01487 author:Roee Aharoni, Yoav Goldberg category:cs.CL  published:2016-11-04 summary:We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task. version:1
arxiv-1611-01146 | Finding Approximate Local Minima for Nonconvex Optimization in Linear Time | http://arxiv.org/abs/1611.01146 | id:1611.01146 author:Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, Tengyu Ma category:math.OC cs.DS cs.NE stat.ML  published:2016-11-03 summary:We design a non-convex second-order optimization algorithm that is guaranteed to return an approximate local minimum in time which is linear in the input representation. The time complexity of our algorithm to find an approximate local minimum is even faster than that of gradient descent to find a critical point. Our algorithm applies to a general class of optimization problems including training a neural network and other non-convex objectives arising in machine learning. version:2
arxiv-1611-01484 | UMDFaces: An Annotated Face Dataset for Training Deep Networks | http://arxiv.org/abs/1611.01484 | id:1611.01484 author:Ankan Bansal, Anirudh Nanduri, Carlos Castillo, Rajeev Ranjan, Rama Chellappa category:cs.CV  published:2016-11-04 summary:Recent progress in face detection (including keypoint detection), and recognition is mainly being driven by (i) deeper convolutional neural network architectures, and (ii) larger datasets. However, most of the largest datasets are maintained by private companies and are not publicly available. The academic computer vision community needs larger and more varied datasets to make further progress. In this paper we introduce a new face dataset, called UMDFaces, which has 367,920 face annotations of 8,501 subjects. We discuss how a large dataset can be collected and annotated using human annotators and deep networks. We provide human curated bounding boxes for faces. We also provide estimated pose (roll, pitch and yaw), locations of twenty-one key-points and gender information generated by a pre-trained neural network. Finally, we compare the quality of the dataset with other publicly available face datasets at similar scales. version:1
arxiv-1611-01480 | Why comparing survival curves between two prognostic subgroups may be misleading | http://arxiv.org/abs/1611.01480 | id:1611.01480 author:Damjan Krstajic category:stat.ME stat.AP stat.ML  published:2016-11-04 summary:We consider the validation of prognostic diagnostic tests that predict two prognostic subgroups (high-risk vs low-risk) for a given disease or treatment. When comparing survival curves between two prognostic subgroups the possibility of misclassification arises, i.e. a patient predicted as high-risk might be de facto low-risk and vice versa. This is a fundamental difference from comparing survival curves between two populations (e.g. control vs treatment in RCT), where there is not an option of misclassification between members of populations. We show that there is a relationship between prognostic subgroups' survival estimates at a time point and positive and negative predictive values in the classification settings. Consequently, the prevalence needs to be taken into account when validating the survival of prognostic subgroups at a time point. Our findings question current methods of comparing survival curves between prognostic subgroups in the validation set because they do not take into account the survival rates of the population. version:1
arxiv-1611-01462 | Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling | http://arxiv.org/abs/1611.01462 | id:1611.01462 author:Hakan Inan, Khashayar Khosravi, Richard Socher category:cs.LG cs.CL stat.ML  published:2016-11-04 summary:Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5. version:1
arxiv-1611-01456 | Learning heat diffusion graphs | http://arxiv.org/abs/1611.01456 | id:1611.01456 author:Dorina Thanou, Xiaowen Dong, Daniel Kressner, Pascal Frossard category:cs.LG cs.SI stat.ML  published:2016-11-04 summary:Effective information analysis generally boils down to properly identifying the structure or geometry of the data, which is often represented by a graph. In some applications, this structure may be partly determined by design constraints or pre-determined sensing arrangements, like in road transportation networks for example. In general though, the data structure is not readily available and becomes pretty difficult to define. In particular, the global smoothness assumptions, that most of the existing works adopt, are often too general and unable to properly capture localized properties of data. In this paper, we go beyond this classical data model and rather propose to represent information as a sparse combination of localized functions that live on a data structure represented by a graph. Based on this model, we focus on the problem of inferring the connectivity that best explains the data samples at different vertices of a graph that is a priori unknown. We concentrate on the case where the observed data is actually the sum of heat diffusion processes, which is a quite common model for data on networks or other irregular structures. We cast a new graph learning problem and solve it with an efficient nonconvex optimization algorithm. Experiments on both synthetic and real world data finally illustrate the benefits of the proposed graph learning framework and confirm that the data structure can be efficiently learned from data observations only. We believe that our algorithm will help solving key questions in diverse application domains such as social and biological network analysis where it is crucial to unveil proper geometry for data understanding and inference. version:1
arxiv-1611-01455 | Ways of Conditioning Generative Adversarial Networks | http://arxiv.org/abs/1611.01455 | id:1611.01455 author:Hanock Kwak, Byoung-Tak Zhang category:cs.LG cs.AI stat.ML  published:2016-11-04 summary:The GANs are generative models whose random samples realistically reflect natural images. It also can generate samples with specific attributes by concatenating a condition vector into the input, yet research on this field is not well studied. We propose novel methods of conditioning generative adversarial networks (GANs) that achieve state-of-the-art results on MNIST and CIFAR-10. We mainly introduce two models: an information retrieving model that extracts conditional information from the samples, and a spatial bilinear pooling model that forms bilinear features derived from the spatial cross product of an image and a condition vector. These methods significantly enhance log-likelihood of test data under the conditional distributions compared to the methods of concatenation. version:1
arxiv-1611-01449 | Semi-supervised deep learning by metric embedding | http://arxiv.org/abs/1611.01449 | id:1611.01449 author:Elad Hoffer, Nir Ailon category:cs.LG  published:2016-11-04 summary:Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples. version:1
arxiv-1611-01436 | Learning Recurrent Span Representations for Extractive Question Answering | http://arxiv.org/abs/1611.01436 | id:1611.01436 author:Kenton Lee, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das category:cs.CL I.2.7  published:2016-11-04 summary:The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQuAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.'s baseline by > 50%. version:1
arxiv-1611-01427 | Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks | http://arxiv.org/abs/1611.01427 | id:1611.01427 author:Arash Ardakani, Carlo Condo, Warren J. Gross category:cs.NE cs.LG  published:2016-11-04 summary:Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks. version:1
arxiv-1611-01423 | Learning Continuous Semantic Representations of Symbolic Expressions | http://arxiv.org/abs/1611.01423 | id:1611.01423 author:Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, Charles Sutton category:cs.LG cs.AI  published:2016-11-04 summary:The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters, that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures. version:1
arxiv-1611-01421 | STDP-based spiking deep neural networks for object recognition | http://arxiv.org/abs/1611.01421 | id:1611.01421 author:Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Simon J Thorpe, Timothée Masquelier category:cs.CV  published:2016-11-04 summary:Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated - using rate-based neural networks trained with back-propagation - that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We think that the combination of STDP with latency coding is key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions. version:1
arxiv-1611-01414 | Information-Theoretic Bounds and Approximations in Neural Population Coding | http://arxiv.org/abs/1611.01414 | id:1611.01414 author:Wentao Huang, Kechen Zhang category:cs.IT cs.LG math.IT  published:2016-11-04 summary:Information theory is a powerful tool for neuroscience and other disciplines. Efficient calculation of Shannon's mutual information (MI) is a key computational step that often presents the biggest difficulty for practical applications. In this paper, we propose effective approximation methods for evaluating MI in the context of neural population coding, especially for high-dimensional inputs. We prove several information-theoretic asymptotic bounds and approximation formulas for large size neural populations. We also discuss how optimization of neural population coding based on these approximation formulas leads to a convex problem about the population density distribution in neural population parameter space. Several useful techniques, including variable transformation and dimensionality reduction, are proposed for more efficient computation of the approximations. Our numerical simulation results show that our asymptotic formulas are highly accurate for approximating MI in neural populations. For some special cases, the approximations by our formulas are exactly equal to the true MI. The approximation methods for MI may have a wide range of applications in various disciplines. version:1
arxiv-1611-01400 | Learning to Rank Scientific Documents from the Crowd | http://arxiv.org/abs/1611.01400 | id:1611.01400 author:Jesse M Lingeman, Hong Yu category:cs.IR cs.CL cs.DL cs.LG cs.SI  published:2016-11-04 summary:Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Our results show that authors' ranking differ significantly from rankings by text-similarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM-Rank) significantly surpassed state-of-the-art baseline systems. version:1
arxiv-1611-01368 | Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies | http://arxiv.org/abs/1611.01368 | id:1611.01368 author:Tal Linzen, Emmanuel Dupoux, Yoav Goldberg category:cs.CL  published:2016-11-04 summary:The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured. version:1
arxiv-1611-01353 | Information Dropout: learning optimal representations through noise | http://arxiv.org/abs/1611.01353 | id:1611.01353 author:Alessandro Achille, Stefano Soatto category:stat.ML cs.LG stat.CO  published:2016-11-04 summary:We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample. version:1
arxiv-1611-01298 | Regularized Pel-Recursive Motion Estimation Using Generalized Cross-Validation and Spatial Adaptation | http://arxiv.org/abs/1611.01298 | id:1611.01298 author:Vania V. Estrela, Luis A. Rivera, Paulo C. Beggio, Ricardo T. Lopes category:cs.CV  published:2016-11-04 summary:The computation of 2-D optical flow by means of regularized pel-recursive algorithms raises a host of issues, which include the treatment of outliers, motion discontinuities and occlusion among other problems. We propose a new approach which allows us to deal with these issues within a common framework. Our approach is based on the use of a technique called Generalized Cross-Validation to estimate the best regularization scheme for a given pixel. In our model, the regularization parameter is a matrix whose entries can account for diverse sources of error. The estimation of the motion vectors takes into consideration local properties of the image following a spatially adaptive approach where each moving pixel is supposed to have its own regularization matrix. Preliminary experiments indicate that this approach provides robust estimates of the optical flow. version:1
arxiv-1611-00938 | Fast Eigenspace Approximation using Random Signals | http://arxiv.org/abs/1611.00938 | id:1611.00938 author:Johan Paratte, Lionel Martin category:cs.DS cs.LG stat.ML  published:2016-11-03 summary:We focus in this work on the estimation of the first $k$ eigenvectors of any graph Laplacian using filtering of Gaussian random signals. We prove that we only need $k$ such signals to be able to exactly recover as many of the smallest eigenvectors, regardless of the number of nodes in the graph. In addition, we address key issues in implementing the theoretical concepts in practice using accurate approximated methods. We also propose fast algorithms both for eigenspace approximation and for the determination of the $k$th smallest eigenvalue $\lambda_k$. The latter proves to be extremely efficient under the assumption of locally uniform distribution of the eigenvalue over the spectrum. Finally, we present experiments which show the validity of our method in practice and compare it to state-of-the-art methods for clustering and visualization both on synthetic small-scale datasets and larger real-world problems of millions of nodes. We show that our method allows a better scaling with the number of nodes than all previous methods while achieving an almost perfect reconstruction of the eigenspace formed by the first $k$ eigenvectors. version:2
arxiv-1610-09461 | Fast Learning with Nonconvex L1-2 Regularization | http://arxiv.org/abs/1610.09461 | id:1610.09461 author:Quanming Yao, James T. Kwok category:cs.LG cs.NA math.NA  published:2016-10-29 summary:Convex regularizers are often used for sparse learning. They are easy to optimize, but can lead to inferior prediction performance. The difference of $\ell_1$ and $\ell_2$ ($\ell_{1-2}$) regularizer has been recently proposed as a nonconvex regularizer. It yields better recovery than both $\ell_0$ and $\ell_1$ regularizers on compressed sensing. However, how to efficiently optimize its learning problem is still challenging. The main difficulty is that both the $\ell_1$ and $\ell_2$ norms in $\ell_{1-2}$ are not differentiable, and existing optimization algorithms cannot be applied. In this paper, we show that a closed-form solution can be derived for the proximal step associated with this regularizer. We further extend the result for low-rank matrix learning and the total variation model. Experiments on both synthetic and real data sets show that the resultant accelerated proximal gradient algorithm is more efficient than other noncovex optimization algorithms. version:2
arxiv-1611-01276 | A Communication-Efficient Parallel Algorithm for Decision Tree | http://arxiv.org/abs/1611.01276 | id:1611.01276 author:Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu category:cs.LG  published:2016-11-04 summary:Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency. version:1
arxiv-1611-01268 | Semantic Noise Modeling for Better Representation Learning | http://arxiv.org/abs/1611.01268 | id:1611.01268 author:Hyo-Eun Kim, Sangheum Hwang, Kyunghyun Cho category:cs.LG cs.NE  published:2016-11-04 summary:Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a task-specific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation can be attained by maximizing the total correlation between the input, latent, and output variables. From the base model, we introduce a semantic noise modeling method which enables class-conditional perturbation on latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise while maintaining its original semantic feature. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed class-conditional perturbation process including t-SNE visualization. version:1
arxiv-1611-01260 | Learning Identity Mappings with Residual Gates | http://arxiv.org/abs/1611.01260 | id:1611.01260 author:Pedro H. P. Savarese category:cs.CV cs.LG  published:2016-11-04 summary:We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence -- fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model. version:1
arxiv-1611-01259 | Generalized Topic Modeling | http://arxiv.org/abs/1611.01259 | id:1611.01259 author:Avrim Blum, Nika Haghtalab category:cs.LG cs.CL cs.DS cs.IR  published:2016-11-04 summary:Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution $\vec a_i$ over words, and a document is generated by first selecting a mixture $\vec w$ over topics, and then generating words i.i.d. from the associated mixture $A{\vec w}$. Given a large collection of such documents, the goal is to recover the topic vectors and then to correctly classify new documents according to their topic mixture. In this work we consider a broad generalization of this framework in which words are no longer assumed to be drawn i.i.d. and instead a topic is a complex distribution over sequences of paragraphs. Since one could not hope to even represent such a distribution in general (even if paragraphs are given using some natural feature representation), we aim instead to directly learn a document classifier. That is, we aim to learn a predictor that given a new document, accurately predicts its topic mixture, without learning the distributions explicitly. We present several natural conditions under which one can do this efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be viewed as a generalization of the multi-view or co-training setting in machine learning. version:1
arxiv-1611-01242 | Answering Complicated Question Intents Expressed in Decomposed Question Sequences | http://arxiv.org/abs/1611.01242 | id:1611.01242 author:Mohit Iyyer, Wen-tau Yih, Ming-Wei Chang category:cs.CL  published:2016-11-04 summary:Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. Existing QA systems face two major problems when evaluated on our dataset: (1) handling questions that contain coreferences to previous questions or answers, and (2) matching words or phrases in a question to corresponding entries in the associated table. We conclude by proposing strategies to handle both of these issues. version:1
arxiv-1611-01239 | Reparameterization trick for discrete variables | http://arxiv.org/abs/1611.01239 | id:1611.01239 author:Seiya Tokui, Issei sato category:stat.ML cs.LG  published:2016-11-04 summary:Low-variance gradient estimation is crucial for learning directed graphical models parameterized by neural networks, where the reparameterization trick is widely used for those with continuous variables. While this technique gives low-variance gradient estimates, it has not been directly applicable to discrete variables, the sampling of which inherently requires discontinuous operations. We argue that the discontinuity can be bypassed by marginalizing out the variable of interest, which results in a new reparameterization trick for discrete variables. This reparameterization greatly reduces the variance, which is understood by regarding the method as an application of common random numbers to the estimation. The resulting estimator is theoretically guaranteed to have a variance not larger than that of the likelihood-ratio method with the optimal input-dependent baseline. We give empirical results for variational learning of sigmoid belief networks. version:1
arxiv-1611-01236 | Adversarial Machine Learning at Scale | http://arxiv.org/abs/1611.01236 | id:1611.01236 author:Alexey Kurakin, Ian Goodfellow, Samy Bengio category:cs.CV cs.CR cs.LG stat.ML  published:2016-11-04 summary:Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process. version:1
arxiv-1611-01235 | A Self-Driving Robot Using Deep Convolutional Neural Networks on Neuromorphic Hardware | http://arxiv.org/abs/1611.01235 | id:1611.01235 author:Tiffany Hwu, Jacob Isbell, Nicolas Oros, Jeffrey Krichmar category:cs.NE cs.RO  published:2016-11-04 summary:Neuromorphic computing is a promising solution for reducing the size, weight and power of mobile embedded systems. In this paper, we introduce a realization of such a system by creating the first closed-loop battery-powered communication system between an IBM TrueNorth NS1e and an autonomous Android-Based Robotics platform. Using this system, we constructed a dataset of path following behavior by manually driving the Android-Based robot along steep mountain trails and recording video frames from the camera mounted on the robot along with the corresponding motor commands. We used this dataset to train a deep convolutional neural network implemented on the TrueNorth NS1e. The NS1e, which was mounted on the robot and powered by the robot's battery, resulted in a self-driving robot that could successfully traverse a steep mountain path in real time. To our knowledge, this represents the first time the TrueNorth NS1e neuromorphic chip has been embedded on a mobile platform under closed-loop control. version:1
arxiv-1611-01232 | Deep Information Propagation | http://arxiv.org/abs/1611.01232 | id:1611.01232 author:Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, Jascha Sohl-Dickstein category:stat.ML cs.LG  published:2016-11-04 summary:We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively. version:1
arxiv-1611-01230 | Statistical Inverse Formulation of Optical Flow with Uncertainty Quantification | http://arxiv.org/abs/1611.01230 | id:1611.01230 author:Jie Sun, Erik Bollt category:stat.ML cs.CV physics.data-an  published:2016-11-04 summary:Optical flow refers to the visual motion observed between two consecutive images. Since the degree of freedom is typically much larger than the constraints imposed by the image observations, the straightforward formulation of optical flow inference is an ill-posed problem. By setting some type of additional "regularity" constraints, classical approaches formulate a well-posed optical flow inference problem in the form of a parameterized set of variational equations. In this work we build a mathematical connection, focused on optical flow methods, between classical variational optical flow approaches and Bayesian statistical inversion. A classical optical flow solution is in fact identical to a maximum a posteriori estimator under the assumptions of linear model with additive independent Gaussian noise and a Gaussian prior distribution. Unlike classical approaches, the statistical inversion approach to optical flow estimation not only allows for "point" estimates, but also provides a distribution of solutions which can be used for ensemble estimation and in particular uncertainty quantification. version:1
arxiv-1611-01224 | Sample Efficient Actor-Critic with Experience Replay | http://arxiv.org/abs/1611.01224 | id:1611.01224 author:Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas category:cs.LG  published:2016-11-03 summary:This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method. version:1
arxiv-1611-00625 | TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games | http://arxiv.org/abs/1611.00625 | id:1611.00625 author:Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, Zeming Lin, Florian Richoux, Nicolas Usunier category:cs.LG cs.AI I.2.1  published:2016-11-01 summary:We present TorchCraft, a library that enables deep learning research on Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these games from a machine learning framework, here Torch. This white paper argues for using RTS games as a benchmark for AI research, and describes the design and components of TorchCraft. version:2
arxiv-1611-01195 | Integrating Atlas and Graph Cut Methods for LV Segmentation from Cardiac Cine MRI | http://arxiv.org/abs/1611.01195 | id:1611.01195 author:Shusil Dangi, Nathan Cahill, Cristian A. Linte category:cs.CV  published:2016-11-03 summary:Magnetic Resonance Imaging (MRI) has evolved as a clinical standard-of-care imaging modality for cardiac morphology, function assessment, and guidance of cardiac interventions. All these applications rely on accurate extraction of the myocardial tissue and blood pool from the imaging data. Here we propose a framework for left ventricle (LV) segmentation from cardiac cine-MRI. First, we segment the LV blood pool using iterative graph cuts, and subsequently use this information to segment the myocardium. We formulate the segmentation procedure as an energy minimization problem in a graph subject to the shape prior obtained by label propagation from an average atlas using affine registration. The proposed framework has been validated on 30 patient cardiac cine-MRI datasets available through the STACOM LV segmentation challenge and yielded fast, robust, and accurate segmentation results. version:1
arxiv-1611-01190 | Conspiracies between Learning Algorithms, Circuit Lower Bounds and Pseudorandomness | http://arxiv.org/abs/1611.01190 | id:1611.01190 author:Igor C. Oliveira, Rahul Santhanam category:cs.CC cs.CR cs.DS cs.LG  published:2016-11-03 summary:We prove several results giving new and stronger connections between learning, circuit lower bounds and pseudorandomness. Among other results, we show a generic learning speedup lemma, equivalences between various learning models in the exponential time and subexponential time regimes, a dichotomy between learning and pseudorandomness, consequences of non-trivial learning for circuit lower bounds, Karp-Lipton theorems for probabilistic exponential time, and NC$^1$-hardness for the Minimum Circuit Size Problem. version:1
arxiv-1611-01186 | Demystifying ResNet | http://arxiv.org/abs/1611.01186 | id:1611.01186 author:Sihan Li, Jiantao Jiao, Yanjun Han, Tsachy Weissman category:cs.NE cs.LG stat.ML  published:2016-11-03 summary:We provide a theoretical explanation for the superb performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1-shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process. version:1
arxiv-1611-01179 | Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data | http://arxiv.org/abs/1611.01179 | id:1611.01179 author:Wenjing Liao, Mauro Maggioni category:stat.ML cs.IT math.IT math.ST stat.TH  published:2016-11-03 summary:We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution $\rho$ in $\mathbb{R}^D$, that is nearly supported on a $d$-dimensional set $\mathcal{M}$ - for example supported on a $d$-dimensional Riemannian manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of $\mathcal{M}$ at varying resolutions. We introduce a thresholding algorithm on the geometric wavelet coefficients, leading to what we call adaptive GMRA approximations. We show that these data-driven, empirical approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples $n$, on a wide variety of measures $\rho$, that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These approximations yield a data-driven dictionary, together with a fast transform mapping data to coefficients, and an inverse of such a map. The algorithms for both the dictionary construction and the transforms have complexity $C n \log n$ with the constant linear in $D$ and exponential in $d$. Our work therefore establishes adaptive GMRA as a fast dictionary learning algorithm with approximation guarantees. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of adaptive GMRA. version:1
arxiv-1611-01170 | PrivLogit: Efficient Privacy-preserving Logistic Regression by Tailoring Numerical Optimizers | http://arxiv.org/abs/1611.01170 | id:1611.01170 author:Wei Xie, Yang Wang, Steven M. Boker, Donald E. Brown category:cs.LG cs.CR stat.ML  published:2016-11-03 summary:Safeguarding privacy in machine learning is highly desirable, especially in collaborative studies across many organizations. Privacy-preserving distributed machine learning (based on cryptography) is popular to solve the problem. However, existing cryptographic protocols still incur excess computational overhead. Here, we make a novel observation that this is partially due to naive adoption of mainstream numerical optimization (e.g., Newton method) and failing to tailor for secure computing. This work presents a contrasting perspective: customizing numerical optimization specifically for secure settings. We propose a seemingly less-favorable optimization method that can in fact significantly accelerate privacy-preserving logistic regression. Leveraging this new method, we propose two new secure protocols for conducting logistic regression in a privacy-preserving and distributed manner. Extensive theoretical and empirical evaluations prove the competitive performance of our two secure proposals while without compromising accuracy or privacy: with speedup up to 2.3x and 8.1x, respectively, over state-of-the-art; and even faster as data scales up. Such drastic speedup is on top of and in addition to performance improvements from existing (and future) state-of-the-art cryptography. Our work provides a new way towards efficient and practical privacy-preserving logistic regression for large-scale studies which are common for modern science. version:1
arxiv-1611-01144 | Categorical Reparameterization with Gumbel-Softmax | http://arxiv.org/abs/1611.01144 | id:1611.01144 author:Eric Jang, Shixiang Gu, Ben Poole category:stat.ML cs.LG  published:2016-11-03 summary:Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. version:1
arxiv-1611-01142 | Using a Deep Reinforcement Learning Agent for Traffic Signal Control | http://arxiv.org/abs/1611.01142 | id:1611.01142 author:Wade Genders, Saiedeh Razavi category:cs.LG cs.SY  published:2016-11-03 summary:Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%. version:1
arxiv-1611-01129 | Cross: Efficient Low-rank Tensor Completion | http://arxiv.org/abs/1611.01129 | id:1611.01129 author:Anru Zhang category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2016-11-03 summary:The completion of tensors, or high-order arrays, attracts significant attention in recent research. Current literature on tensor completion primarily focuses on recovery from a set of uniformly randomly measured entries, and the required number of measurements to achieve recovery is not guaranteed to be optimal. In addition, the implementation of some previous methods are NP-hard. In this article, we propose a framework for low-rank tensor completion via a novel tensor measurement scheme we name Cross. The proposed procedure is efficient and easy to implement. In particular, we show that a third order tensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional space can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2) + r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity lower-bound. In the case of noisy measurements, we also develop a theoretical upper bound and the matching minimax lower bound for recovery error over certain classes of low-rank tensors for the proposed procedure. The results can be further extended to fourth or higher-order tensors. Simulation studies show that the method performs well under a variety of settings. Finally, the procedure is illustrated through a real dataset in neuroimaging. version:1
arxiv-1611-01101 | CogALex-V Shared Task: ROOT18 | http://arxiv.org/abs/1611.01101 | id:1611.01101 author:Emmanuele Chersoni, Giulia Rambelli, Enrico Santus category:cs.CL  published:2016-11-03 summary:In this paper, we describe ROOT 18, a classifier using the scores of several unsupervised distributional measures as features to discriminate between semantically related and unrelated words, and then to classify the related pairs according to their semantic relation (i.e. synonymy, antonymy, hypernymy, part-whole meronymy). Our classifier participated in the CogALex-V Shared Task, showing a solid performance on the first subtask, but a poor performance on the second subtask. The low scores reported on the second subtask suggest that distributional measures are not sufficient to discriminate between multiple semantic relations at once. version:1
arxiv-1611-01096 | Spectral community detection in heterogeneous large networks | http://arxiv.org/abs/1611.01096 | id:1611.01096 author:Hafiz Tiomoko Ali, Romain Couillet category:stat.ML  published:2016-11-03 summary:In this article, we study spectral methods for community detection based on $ \alpha$-parametrized normalized modularity matrix hereafter called $ {\bf L}_\alpha $ in heterogeneous graph models. We show, in a regime where community detection is not asymptotically trivial, that $ {\bf L}_\alpha $ can be well approximated by a more tractable random matrix which falls in the family of spiked random matrices. The analysis of this equivalent spiked random matrix allows us to improve spectral methods for community detection and assess their performances in the regime under study. In particular, we prove the existence of an optimal value $ \alpha_{\rm opt} $ of the parameter $ \alpha $ for which the detection of communities is best ensured and we provide an on-line estimation of $ \alpha_{\rm opt} $ only based on the knowledge of the graph adjacency matrix. Unlike classical spectral methods for community detection where clustering is performed on the eigenvectors associated with extreme eigenvalues, we show through our theoretical analysis that a regularization should instead be performed on those eigenvectors prior to clustering in heterogeneous graphs. Finally, through a deeper study of the regularized eigenvectors used for clustering, we assess the performances of our new algorithm for community detection. Numerical simulations in the course of the article show that our methods outperform state-of-the-art spectral methods on dense heterogeneous graphs. version:1
arxiv-1610-07675 | Surprisal-Driven Zoneout | http://arxiv.org/abs/1610.07675 | id:1610.07675 author:Kamil Rocki, Tomasz Kornuta, Tegan Maharaj category:cs.LG cs.AI cs.NE  published:2016-10-24 summary:We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods. version:4
arxiv-1611-00020 | Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision | http://arxiv.org/abs/1611.00020 | id:1611.00020 author:Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, Ni Lao category:cs.CL cs.AI cs.LG  published:2016-10-31 summary:Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge. version:3
arxiv-1611-01060 | A-Ward_p\b{eta}: Effective hierarchical clustering using the Minkowski metric and a fast k -means initialisation | http://arxiv.org/abs/1611.01060 | id:1611.01060 author:Renato Cordeiro de Amorim, Vladimir Makarenkov, Boris Mirkin category:cs.LG stat.ML  published:2016-11-03 summary:In this paper we make two novel contributions to hierarchical clustering. First, we introduce an anomalous pattern initialisation method for hierarchical clustering algorithms, called A-Ward, capable of substantially reducing the time they take to converge. This method generates an initial partition with a sufficiently large number of clusters. This allows the cluster merging process to start from this partition rather than from a trivial partition composed solely of singletons. Our second contribution is an extension of the Ward and Ward p algorithms to the situation where the feature weight exponent can differ from the exponent of the Minkowski distance. This new method, called A-Ward p\b{eta} , is able to generate a much wider variety of clustering solutions. We also demonstrate that its parameters can be estimated reasonably well by using a cluster validity index. We perform numerous experiments using data sets with two types of noise, insertion of noise features and blurring within-cluster values of some features. These experiments allow us to conclude: (i) our anomalous pattern initialisation method does indeed reduce the time a hierarchical clustering algorithm takes to complete, without negatively impacting its cluster recovery ability; (ii) A-Ward p\b{eta} provides better cluster recovery than both Ward and Ward p. version:1
arxiv-1611-01055 | Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter? | http://arxiv.org/abs/1611.01055 | id:1611.01055 author:Xue Bin Peng, Michiel van de Panne category:cs.LG cs.GR cs.RO  published:2016-11-03 summary:The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies. version:1
arxiv-1611-01046 | Learning to Pivot with Adversarial Networks | http://arxiv.org/abs/1611.01046 | id:1611.01046 author:Gilles Louppe, Michael Kagan, Kyle Cranmer category:stat.ML cs.LG cs.NE physics.data-an stat.ME  published:2016-11-03 summary:Many inference problems involve data generation processes that are not uniquely specified or are uncertain in some way. In a scientific context, the presence of several plausible data generation processes is often associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution is invariant to the unknown value of the (categorical or continuous) nuisance parameters that parametrizes this family of generation processes. In this work, we introduce a flexible training procedure based on adversarial networks for enforcing the pivotal property on a predictive model. We derive theoretical results showing that the proposed algorithm tends towards a minimax solution corresponding to a predictive model that is both optimal and independent of the nuisance parameters (if that models exists) or for which one can tune the trade-off between power and robustness. Finally, we demonstrate the effectiveness of this approach with a toy example and an example from particle physics. version:1
arxiv-1611-00995 | An empirical study for Vietnamese dependency parsing | http://arxiv.org/abs/1611.00995 | id:1611.00995 author:Dat Quoc Nguyen, Mark Dras, Mark Johnson category:cs.CL  published:2016-11-03 summary:This paper presents an empirical comparison of different dependency parsers for Vietnamese, which has some unusual characteristics such as copula drop and verb serialization. Experimental results show that the neural network-based parsers perform significantly better than the traditional parsers. We report the highest parsing scores published to date for Vietnamese with the labeled attachment score (LAS) at 73.53% and the unlabeled attachment score (UAS) at 80.66%. version:1
arxiv-1611-00962 | Multitask Protein Function Prediction Through Task Dissimilarity | http://arxiv.org/abs/1611.00962 | id:1611.00962 author:Marco Frasca, Nicolò Cesa Bianchi category:stat.ML cs.LG q-bio.QM 68Q01 I.5; J.3  published:2016-11-03 summary:Automated protein function prediction is a challenging problem with distinctive features, such as the hierarchical organization of protein functions and the scarcity of annotated proteins for most biological functions. We propose a multitask learning algorithm addressing both issues. Unlike standard multitask algorithms, which use task (protein functions) similarity information as a bias to speed up learning, we show that dissimilarity information enforces separation of rare class labels from frequent class labels, and for this reason is better suited for solving unbalanced protein function prediction problems. We support our claim by showing that a multitask extension of the label propagation algorithm empirically works best when the task relatedness information is represented using a dissimilarity matrix as opposed to a similarity matrix. Moreover, the experimental comparison carried out on three model organism shows that our method has a more stable performance in both "protein-centric" and "function-centric" evaluation settings. version:1
arxiv-1611-00960 | Adaptive mixed norm optical flow estimation | http://arxiv.org/abs/1611.00960 | id:1611.00960 author:Vania V. Estrela, Matthias O. Franz, Ricardo T. Lopes, G. P. De Araujo category:cs.CV  published:2016-11-03 summary:The pel-recursive computation of 2-D optical flow has been extensively studied in computer vision to estimate motion from image sequences, but it still raises a wealth of issues, such as the treatment of outliers, motion discontinuities and occlusion. It relies on spatio-temporal brightness variations due to motion. Our proposed adaptive regularized approach deals with these issues within a common framework. It relies on the use of a data-driven technique called Mixed Norm (MN) to estimate the best motion vector for a given pixel. In our model, various types of noise can be handled, representing different sources of error. The motion vector estimation takes into consideration local image properties and it results from the minimization of a mixed norm functional with a regularization parameter depending on the kurtosis. This parameter determines the relative importance of the fourth norm and makes the functional convex. The main advantage of the developed procedure is that no knowledge of the noise distribution is necessary. Experiments indicate that this approach provides robust estimates of the optical flow. version:1
arxiv-1611-00953 | High-dimensional regression over disease subgroups | http://arxiv.org/abs/1611.00953 | id:1611.00953 author:Frank Dondelinger, Sach Mukherjee, The Alzheimer's Dise category:stat.AP stat.ML  published:2016-11-03 summary:We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where disease subtypes, for example, may differ with respect to underlying regression models, but sample sizes at the subgroup-level may be limited. We focus on the case in which subgroup-specific models may be expected to be similar but not necessarily identical. Our approach is to treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an $\ell_1$ term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and cancer datasets. These examples demonstrate the gains our approach can offer in terms of prediction and the ability to estimate subgroup-specific sparsity patterns. version:1
arxiv-1611-00939 | Recent Advances in Transient Imaging: A Computer Graphics and Vision Perspective | http://arxiv.org/abs/1611.00939 | id:1611.00939 author:Adrian Jarabo, Belen Masia, Julio Marco, Diego Gutierrez category:cs.CV cs.GR  published:2016-11-03 summary:Transient imaging has recently made a huge impact in the computer graphics and computer vision fields. By capturing, reconstructing, or simulating light transport at extreme temporal resolutions, researchers have proposed novel techniques to show movies of light in motion, see around corners, detect objects in highly-scattering media, or infer material properties from a distance, to name a few. The key idea is to leverage the wealth of information in the temporal domain at the pico or nanosecond resolution, information usually lost during the capture-time temporal integration. This paper presents recent advances in this field of transient imaging from a graphics and vision perspective, including capture techniques, analysis, applications and simulation. version:1
arxiv-1611-00931 | Rough Set Based Color Channel Selection | http://arxiv.org/abs/1611.00931 | id:1611.00931 author:Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler category:cs.CV  published:2016-11-03 summary:Color channel selection is essential for accurate segmentation of sky and clouds in images obtained from ground-based sky cameras. Most prior works in cloud segmentation use threshold based methods on color channels selected in an ad-hoc manner. In this letter, we propose the use of rough sets for color channel selection in visible-light images. Our proposed approach assesses color channels with respect to their contribution for segmentation, and identifies the most effective ones. version:1
arxiv-1611-00483 | Detecting Context Dependent Messages in a Conversational Environment | http://arxiv.org/abs/1611.00483 | id:1611.00483 author:Chaozhuo Li, Yu Wu, Wei Wu, Chen Xing, Zhoujun Li, Ming Zhou category:cs.CL  published:2016-11-02 summary:While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to consider the linguistic context of an input text in the generation process. The task is challenging, as messages in a conversational environment are short and informal, and evidence that can indicate a message is context dependent is scarce. After a study of social conversation data crawled from the web, we observed that some characteristics estimated from the responses of messages are discriminative for identifying context dependent messages. With the characteristics as weak supervision, we propose using a Long Short Term Memory (LSTM) network to learn a classifier. Our method carries out text representation and classifier learning in a unified framework. Experimental results show that the proposed method can significantly outperform baseline methods on accuracy of classification. version:2
arxiv-1611-00898 | Low Rank Approximation with Entrywise $\ell_1$-Norm Error | http://arxiv.org/abs/1611.00898 | id:1611.00898 author:Zhao Song, David P. Woodruff, Peilin Zhong category:cs.DS cs.CC cs.LG  published:2016-11-03 summary:We study the $\ell_1$-low rank approximation problem, where for a given $n \times d$ matrix $A$ and approximation factor $\alpha \geq 1$, the goal is to output a rank-$k$ matrix $\widehat{A}$ for which $$\ A-\widehat{A}\ _1 \leq \alpha \cdot \min_{\textrm{rank-}k\textrm{ matrices}~A'}\ A-A'\ _1,$$ where for an $n \times d$ matrix $C$, we let $\ C\ _1 = \sum_{i=1}^n \sum_{j=1}^d C_{i,j} $. This error measure is known to be more robust than the Frobenius norm in the presence of outliers and is indicated in models where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple places if there are any approximation algorithms. We give the first provable approximation algorithms for $\ell_1$-low rank approximation, showing that it is possible to achieve approximation factor $\alpha = (\log d) \cdot \mathrm{poly}(k)$ in $\mathrm{nnz}(A) + (n+d) \mathrm{poly}(k)$ time, where $\mathrm{nnz}(A)$ denotes the number of non-zero entries of $A$. If $k$ is constant, we further improve the approximation ratio to $O(1)$ with a $\mathrm{poly}(nd)$-time algorithm. Under the Exponential Time Hypothesis, we show there is no $\mathrm{poly}(nd)$-time algorithm achieving a $(1+\frac{1}{\log^{1+\gamma}(nd)})$-approximation, for $\gamma > 0$ an arbitrarily small constant, even when $k = 1$. We give a number of additional results for $\ell_1$-low rank approximation: nearly tight upper and lower bounds for column subset selection, CUR decompositions, extensions to low rank approximation with respect to $\ell_p$-norms for $1 \leq p < 2$ and earthmover distance, low-communication distributed protocols and low-memory streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. We also give a preliminary empirical evaluation. version:1
arxiv-1611-00890 | Maximizing Investment Value of Small-Scale PV in a Smart Grid Environment | http://arxiv.org/abs/1611.00890 | id:1611.00890 author:Jeremy Every, Li Li, Youguang G. Guo, David G. Dorrell category:math.OC cs.AI cs.NE  published:2016-11-03 summary:Determining the optimal size and orientation of small-scale residential based PV arrays will become increasingly complex in the future smart grid environment with the introduction of smart meters and dynamic tariffs. However consumers can leverage the availability of smart meter data to conduct a more detailed exploration of PV investment options for their particular circumstances. In this paper, an optimization method for PV orientation and sizing is proposed whereby maximizing the PV investment value is set as the defining objective. Solar insolation and PV array models are described to form the basis of the PV array optimization strategy. A constrained particle swarm optimization algorithm is selected due to its strong performance in non-linear applications. The optimization algorithm is applied to real-world metered data to quantify the possible investment value of a PV installation under different energy retailers and tariff structures. The arrangement with the highest value is determined to enable prospective small-scale PV investors to select the most cost-effective system. version:1
arxiv-1611-00873 | Extracting Actionability from Machine Learning Models by Sub-optimal Deterministic Planning | http://arxiv.org/abs/1611.00873 | id:1611.00873 author:Qiang Lyu, Yixin Chen, Zhaorong Li, Zhicheng Cui, Ling Chen, Xing Zhang, Haihua Shen category:cs.AI cs.LG  published:2016-11-03 summary:A main focus of machine learning research has been improving the generalization accuracy and efficiency of prediction models. Many models such as SVM, random forest, and deep neural nets have been proposed and achieved great success. However, what emerges as missing in many applications is actionability, i.e., the ability to turn prediction results into actions. For example, in applications such as customer relationship management, clinical prediction, and advertisement, the users need not only accurate prediction, but also actionable instructions which can transfer an input to a desirable goal (e.g., higher profit repays, lower morbidity rates, higher ads hit rates). Existing effort in deriving such actionable knowledge is few and limited to simple action models which restricted to only change one attribute for each action. The dilemma is that in many real applications those action models are often more complex and harder to extract an optimal solution. In this paper, we propose a novel approach that achieves actionability by combining learning with planning, two core areas of AI. In particular, we propose a framework to extract actionable knowledge from random forest, one of the most widely used and best off-the-shelf classifiers. We formulate the actionability problem to a sub-optimal action planning (SOAP) problem, which is to find a plan to alter certain features of a given input so that the random forest would yield a desirable output, while minimizing the total costs of actions. Technically, the SOAP problem is formulated in the SAS+ planning formalism, and solved using a Max-SAT based approach. Our experimental results demonstrate the effectiveness and efficiency of the proposed approach on a personal credit dataset and other benchmarks. Our work represents a new application of automated planning on an emerging and challenging machine learning paradigm. version:1
arxiv-1611-00866 | Tensor Decomposition via Variational Auto-Encoder | http://arxiv.org/abs/1611.00866 | id:1611.00866 author:Bin Liu, Zenglin Xu, Yingming Li category:stat.ML  published:2016-11-03 summary:Tensor decomposition is an important technique for capturing the high-order interactions among multiway data. Multi-linear tensor composition methods, such as the Tucker decomposition and the CANDECOMP/PARAFAC (CP), assume that the complex interactions among objects are multi-linear, and are thus insufficient to represent nonlinear relationships in data. Another assumption of these methods is that a predefined rank should be known. However, the rank of tensors is hard to estimate, especially for cases with missing values. To address these issues, we design a Bayesian generative model for tensor decomposition. Different from the traditional Bayesian methods, the high-order interactions of tensor entries are modeled with variational auto-encoder. The proposed model takes advantages of Neural Networks and nonparametric Bayesian models, by replacing the multi-linear product in traditional Bayesian tensor decomposition with a complex nonlinear function (via Neural Networks) whose parameters can be learned from data. Experimental results on synthetic data and real-world chemometrics tensor data have demonstrated that our new model can achieve significantly higher prediction performance than the state-of-the-art tensor decomposition approaches. version:1
arxiv-1611-00864 | Recurrent Neural Networks for Spatiotemporal Dynamics of Intrinsic Networks from fMRI Data | http://arxiv.org/abs/1611.00864 | id:1611.00864 author:R Devon Hjelm, Sergey M. Plis, Vince Calhoun category:cs.NE q-bio.NC  published:2016-11-03 summary:Functional magnetic resonance imaging (fMRI) of temporally-coherent blood oxygenization level-dependent (BOLD) signal provides an effective means of analyzing functionally coherent patterns in the brain. Intrinsic networks and functional connectivity are important outcomes of fMRI studies and are central to understanding brain function and making diagnoses. The most popular method for separating INs, independent component analysis, begins with the assumption that the data is a mixture of maximally independent sources. ICA is trainable through one of many relatively simple optimization routines that maximize non-Gaussianity or minimize mutual information. Although fMRI data is a time series, ICA, as with other popular linear methods for separating INs, is order-agnostic in time: each multivariate signal at each time step is treated as i.i.d.. ICA in its common use in the field employs the same parameterization across subjects, which allows for either temporal or spatial variability, but not both. In order to overcome shortcomings of temporal ICA in lack of dynamics and subject-wise/temporal variability of spatial maps, but without abandoning the fundamental strengths of ICA, we combine recurrent neural networks (RNNs) with an ICA objective. The resulting model naturally represents temporal and spatial dynamics---having subject-wise and temporally variable spatial maps---and is easily trainable using gradient descent and back-propagation. version:1
arxiv-1611-00862 | Quantile Reinforcement Learning | http://arxiv.org/abs/1611.00862 | id:1611.00862 author:Hugo Gilbert, Paul Weng category:cs.LG cs.AI  published:2016-11-03 summary:In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire. version:1
arxiv-1611-00601 | Ordinal Common-sense Inference | http://arxiv.org/abs/1611.00601 | id:1611.00601 author:Sheng Zhang, Rachel Rudinger, Kevin Duh, Benjamin Van Durme category:cs.CL  published:2016-11-02 summary:Humans have the capacity to draw common-sense inferences from natural language: various things that are likely but not certain to hold based on established discourse, and are rarely stated explicitly. We propose an evaluation of automated common-sense inference based on an extension of recognizing textual entailment: predicting ordinal human responses of subjective likelihood of an inference holding in a given context. We describe a framework for extracting common-sense knowledge for corpora, which is then used to construct a dataset for this ordinal entailment task, which we then use to train and evaluate a sequence to sequence neural network model. Further, we annotate subsets of previously established datasets via our ordinal annotation protocol in order to then analyze the distinctions between these and what we have constructed. version:2
arxiv-1611-00851 | An All-In-One Convolutional Neural Network for Face Analysis | http://arxiv.org/abs/1611.00851 | id:1611.00851 author:Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo, Rama Chellappa category:cs.CV  published:2016-11-03 summary:We present a multi-purpose algorithm for simultaneous face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and face recognition using a single deep convolutional neural network (CNN). The proposed method employs a multi-task learning framework that regularizes the shared parameters of CNN and builds a synergy among different domains and tasks. Extensive experiments show that the network has a better understanding of face and achieves state-of-the-art result for most of these tasks. version:1
arxiv-1611-00850 | Optical Flow Estimation using a Spatial Pyramid Network | http://arxiv.org/abs/1611.00850 | id:1611.00850 author:Anurag Ranjan, Michael J. Black category:cs.CV  published:2016-11-03 summary:We learn to compute optical flow by combining a classical coarse-to-fine flow approach with deep learning. Specifically we adopt a spatial-pyramid formulation to deal with large motions. According to standard practice, we warp one image of a pair at each pyramid level by the current flow estimate and compute an update to the flow field. Instead of the standard minimization of an objective function at each pyramid level, we train a deep neural network at each level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First the network is much simpler and much smaller; our Spatial Pyramid Network (SPyNet) is 96% smaller than FlowNet in terms of model parameters. Because it is so small, the method could be made to run on a cell phone or other embedded system. Second, since the flow is assumed to be small at each level ($< 1$ pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the filters that we learn appear similar to classical spatio-temporal filters, possibly giving insight into how to improve the method further. Our results are more accurate than FlowNet in most cases and suggest a new direction of combining classical flow methods with deep learning. version:1
arxiv-1611-00838 | Initialization and Coordinate Optimization for Multi-way Matching | http://arxiv.org/abs/1611.00838 | id:1611.00838 author:Da Tang, Tony Jebara category:stat.ML cs.CV cs.LG  published:2016-11-02 summary:We consider the problem of consistently matching multiple sets of elements to each other, which is a common task in fields such as computer vision. To solve the underlying NP-hard objective, existing methods often relax or approximate it, but end up with unsatisfying empirical performances due to their inexact objectives. We propose a coordinate update algorithm that directly solves the exact objective. By using the pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its Maximum Spanning Tree, our algorithm successfully avoids bad local optima. Theoretically, with high probability our algorithm could guarantee to solve this problem optimally on data with reasonable noise. Empirically, our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets. version:1
arxiv-1611-00829 | Multidimensional Binary Search for Contextual Decision-Making | http://arxiv.org/abs/1611.00829 | id:1611.00829 author:Ilan Lobel, Renato Paes Leme, Adrian Vladu category:cs.DS cs.LG  published:2016-11-02 summary:We consider a multidimensional search problem that is motivated by questions in contextual decision-making, such as dynamic pricing and personalized medicine. Nature selects a state from a $d$-dimensional unit ball and then generates a sequence of $d$-dimensional directions. We are given access to the directions, but not access to the state. After receiving a direction, we have to guess the value of the dot product between the state and the direction. Our goal is to minimize the number of times when our guess is more than $\epsilon$ away from the true answer. We construct a polynomial time algorithm that we call Projected Volume achieving regret $O(d\log(d/\epsilon))$, which is optimal up to a $\log d$ factor. The algorithm combines a volume cutting strategy with a new geometric technique that we call cylindrification. version:1
arxiv-1611-00822 | Learning Deep Embeddings with Histogram Loss | http://arxiv.org/abs/1611.00822 | id:1611.00822 author:Evgeniya Ustinova, Victor Lempitsky category:cs.CV  published:2016-11-02 summary:We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives. version:1
arxiv-1611-00817 | Gaussian Processes for Survival Analysis | http://arxiv.org/abs/1611.00817 | id:1611.00817 author:Tamara Fernández, Nicolás Rivera, Yee Whye Teh category:stat.ML  published:2016-11-02 summary:We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests. version:1
arxiv-1611-01390 | Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures | http://arxiv.org/abs/1611.01390 | id:1611.01390 author:Jonathan Vacher, Andrew Isaac Meso, Laurent U. Perrinet, Gabriel Peyré category:q-bio.NC cs.CV  published:2016-11-02 summary:A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The present study details the complete formulation of such a generative model intended to probe visual motion perception. It is first derived in a set of axiomatic steps constrained by biological plausibility. We then extend previous contributions by detailing three equivalent formulations of the Gaussian dynamic texture model. First, the composite dynamic textures are constructed by the random aggregation of warped patterns, which can be viewed as 3D Gaussian fields. Second, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real time, on-the-fly, texture synthesis using time-discretized auto- regressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log-likelihood of the probability density. The log-likelihoods are finally essential for the construction of a Bayesian inference framework. We use the model to probe speed perception in humans psychophysically using zoom-like changes in stimulus spatial frequency content. The likelihood is contained within the genera- tive model and we chose a slow speed prior consistent with previous literature. We then validated the fitting process of the model using synthesized data. The human data replicates previous findings that relative perceived speed is positively biased by spatial frequency increments. The effect cannot be fully accounted for by previous models, but the current prior acting on the spatio-temporal likelihoods has proved necessary in accounting for the perceptual bias. version:1
arxiv-1611-00801 | A FOFE-based Local Detection Approach for Named Entity Recognition and Mention Detection | http://arxiv.org/abs/1611.00801 | id:1611.00801 author:Mingbin Xu, Hui Jiang category:cs.CL  published:2016-11-02 summary:In this paper, we study a novel approach for named entity recognition (NER) and mention detection in natural language processing. Instead of treating NER as a sequence labelling problem, we propose a new local detection approach, which rely on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Afterwards, a simple feedforward neural network is used to reject or predict entity label for each individual fragment. The proposed method has been evaluated in several popular NER and mention detection tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our methods have yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labelling methods. version:1
arxiv-1611-00798 | Cross-validation based Nonlinear Shrinkage | http://arxiv.org/abs/1611.00798 | id:1611.00798 author:Daniel Bartz category:stat.ML  published:2016-11-02 summary:Many machine learning algorithms require precise estimates of covariance matrices. The sample covariance matrix performs poorly in high-dimensional settings, which has stimulated the development of alternative methods, the majority based on factor models and shrinkage. Recent work of Ledoit and Wolf has extended the shrinkage framework to Nonlinear Shrinkage (NLS), a more powerful covariance estimator based on Random Matrix Theory. Our contribution shows that, contrary to claims in the literature, cross-validation based covariance matrix estimation (CVC) yields comparable performance at strongly reduced complexity and runtime. On two real world data sets, we show that the CVC estimator yields superior results than competing shrinkage and factor based methods. version:1
arxiv-1611-00740 | Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of Dimensionality | http://arxiv.org/abs/1611.00740 | id:1611.00740 author:Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, Qianli Liao category:cs.LG  published:2016-11-02 summary:The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures. version:1
arxiv-1611-00714 | Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization | http://arxiv.org/abs/1611.00714 | id:1611.00714 author:Alexander Jung, Alfred O. Hero III, Alexandru Mara, Sabeur Aridhi category:cs.LG cs.DC  published:2016-11-02 summary:We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterovs optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks. version:1
arxiv-1611-00710 | Deep counter networks for asynchronous event-based processing | http://arxiv.org/abs/1611.00710 | id:1611.00710 author:Jonathan Binas, Giacomo Indiveri, Michael Pfeiffer category:cs.NE cs.LG  published:2016-11-02 summary:Despite their advantages in terms of computational resources, latency, and power consumption, event-based implementations of neural networks have not been able to achieve the same performance figures as their equivalent state-of-the-art deep network models. We propose counter neurons as minimal spiking neuron models which only require addition and comparison operations, thus avoiding costly multiplications. We show how inference carried out in deep counter networks converges to the same accuracy levels as are achieved with state-of-the-art conventional networks. As their event-based style of computation leads to reduced latency and sparse updates, counter networks are ideally suited for efficient compact and low-power hardware implementation. We present theory and training methods for counter networks, and demonstrate on the MNIST benchmark that counter networks converge quickly, both in terms of time and number of operations required, to state-of-the-art classification accuracy. version:1
arxiv-1611-00336 | Stochastic Variational Deep Kernel Learning | http://arxiv.org/abs/1611.00336 | id:1611.00336 author:Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing category:stat.ML cs.LG stat.ME  published:2016-11-01 summary:Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet. version:2
arxiv-1610-09996 | End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension | http://arxiv.org/abs/1610.09996 | id:1610.09996 author:Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, Bowen Zhou category:cs.CL  published:2016-10-31 summary:This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR achieves state-of-the-art exact match and F1 scores on the SQuAD dataset. version:2
arxiv-1611-00684 | Wearable Vision Detection of Environmental Fall Risks using Convolutional Neural Networks | http://arxiv.org/abs/1611.00684 | id:1611.00684 author:Mina Nouredanesh, Andrew McCormick, Sunil L. Kukreja, James Tung category:cs.CV  published:2016-11-02 summary:In this paper, a method to detect environmental hazards related to a fall risk using a mobile vision system is proposed. First-person perspective videos are proposed to provide objective evidence on cause and circumstances of perturbed balance during activities of daily living, targeted to seniors. A classification problem was defined with 12 total classes of potential fall risks, including slope changes (e.g., stairs, curbs, ramps) and surfaces (e.g., gravel, grass, concrete). Data was collected using a chest-mounted GoPro camera. We developed a convolutional neural network for automatic feature extraction, reduction, and classification of frames. Initial results, with a mean square error of 8%, are promising. version:1
arxiv-1611-00683 | Improving variational methods via pairwise linear response identities | http://arxiv.org/abs/1611.00683 | id:1611.00683 author:Jack Raymond, Federico Ricci-Tersenghi category:stat.ML cond-mat.dis-nn  published:2016-11-02 summary:Inference methods are often formulated as variational approximations: these approximations allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima. version:1
arxiv-1610-09650 | Deep Model Compression: Distilling Knowledge from Noisy Teachers | http://arxiv.org/abs/1610.09650 | id:1610.09650 author:Bharat Bhusan Sau, Vineeth N. Balasubramanian category:cs.LG  published:2016-10-30 summary:The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach. version:2
arxiv-1610-10025 | Function Weighted Metric Discovery for Unreliable Functions | http://arxiv.org/abs/1610.10025 | id:1610.10025 author:Alexander Cloninger category:stat.ML  published:2016-10-31 summary:We consider building a function adapted diffusion operator high dimensional data $X$ when the function $F$ can only be evaluated on large subsets of the data, and possibly only depends on a small subset of the features. Our method breaks $X$ up into hierarchical trees, and determines the importance of each feature in each subset of the population. The resulting metric $\rho_F$ is then used to define a localized filtration of $F$ and estimation values of $F$ at a finer scale than it is reliable naively. We apply this method in several cases in which $F$ is unreliable a priori, and specifically use it to determine personalized risk and treatment effectiveness in drug trials. We validate the model on several synthetic datasets. version:2
arxiv-1610-09322 | Homotopy Analysis for Tensor PCA | http://arxiv.org/abs/1610.09322 | id:1610.09322 author:Anima Anandkumar, Yuan Deng, Rong Ge, Hossein Mobahi category:stat.ML cs.LG  published:2016-10-28 summary:Developing efficient and guaranteed nonconvex algorithms has been an important challenge in modern machine learning. Algorithms with good empirical performance such as stochastic gradient descent often lack theoretical guarantees. In this paper, we analyze the class of homotopy or continuation methods for global optimization of nonconvex functions. These methods start from an objective function that is efficient to optimize (e.g. convex), and progressively modify it to obtain the required objective, and the solutions are passed along the homotopy path. For the challenging problem of tensor PCA, we prove global convergence of the homotopy method in the "high noise" regime. The signal-to-noise requirement for our algorithm is tight in the sense that it matches the recovery guarantee for the best degree-4 sum-of-squares algorithm. In addition, we prove a phase transition along the homotopy path for tensor PCA. This allows to simplify the homotopy method to a local search algorithm, viz., tensor power iterations, with a specific initialization and a noise injection procedure, while retaining the theoretical guarantees. version:3
arxiv-1611-00565 | Learning Methods for Dynamic Topic Modeling in Automated Behaviour Analysis | http://arxiv.org/abs/1611.00565 | id:1611.00565 author:Olga Isupova, Danil Kuzin, Lyudmila Mihaylova category:stat.ML  published:2016-11-02 summary:Semi-supervised and unsupervised systems provide operators with invaluable support and can tremendously reduce the operators load. In the light of the necessity to process large volumes of video data and provide autonomous decisions, this work proposes new learning algorithms for activity analysis in video. The activities and behaviours are described by a dynamic topic model. Two novel learning algorithms based on the expectation maximisation approach and variational Bayes inference are proposed. Theoretical derivations of the posterior of model parameters are given. The designed learning algorithms are compared with the Gibbs sampling inference scheme introduced earlier in the literature. A detailed comparison of the learning algorithms is presented on real video data. We also propose an anomaly localisation procedure, elegantly embedded in the topic modeling framework. The proposed framework can be applied to a number of areas, including transportation systems, security and surveillance. version:1
arxiv-1611-00555 | Sensitivity Maps of the Hilbert-Schmidt Independence Criterion | http://arxiv.org/abs/1611.00555 | id:1611.00555 author:Adrián Pérez-Suay, Gustau Camps-Valls category:stat.ML  published:2016-11-02 summary:Kernel dependence measures yield accurate estimates of nonlinear relations between random variables, and they are also endorsed with solid theoretical properties and convergence rates. Besides, the empirical estimates are easy to compute in closed form just involving linear algebra operations. However, they are hampered by two important problems: the high computational cost involved, as two kernel matrices of the sample size have to be computed and stored, and the interpretability of the measure, which remains hidden behind the implicit feature map. We here address these two issues. We introduce the Sensitivity Maps (SMs) for the Hilbert-Schmidt independence criterion (HSIC). Sensitivity maps allow us to explicitly analyze and visualize the relative relevance of both examples and features on the dependence measure. We also present the randomized HSIC (RHSIC) and its corresponding sensitivity maps to cope with large scale problems. We build upon the framework of random features and the Bochner's theorem to approximate the involved kernels in the canonical HSIC. The power of the RHSIC measure scales favourably with the number of samples, and it approximates HSIC and the sensitivity maps efficiently. Convergence bounds of both the measure and the sensitivity map are also provided. Our proposal is illustrated in synthetic examples, and challenging real problems of dependence estimation, feature selection, and causal inference from empirical data. version:1
arxiv-1611-00544 | A nonparametric HMM for genetic imputation and coalescent inference | http://arxiv.org/abs/1611.00544 | id:1611.00544 author:Lloyd T. Elliott, Yee Whye Teh category:stat.AP stat.ML  published:2016-11-02 summary:Genetic sequence data are well described by hidden Markov models (HMMs) in which latent states correspond to clusters of similar mutation patterns. Theory from statistical genetics suggests that these HMMs are nonhomogeneous (their transition probabilities vary along the chromosome) and have large support for self transitions. We develop a new nonparametric model of genetic sequence data, based on the hierarchical Dirichlet process, which supports these self transitions and nonhomogeneity. Our model provides a parameterization of the genetic process that is more parsimonious than other more general nonparametric models which have previously been applied to population genetics. We provide truncation-free MCMC inference for our model using a new auxiliary sampling scheme for Bayesian nonparametric HMMs. In a series of experiments on male X chromosome data from the Thousand Genomes Project and also on data simulated from a population bottleneck we show the benefits of our model over the popular finite model fastPHASE, which can itself be seen as a parametric truncation of our model. We find that the number of HMM states found by our model is correlated with the time to the most recent common ancestor in population bottlenecks. This work demonstrates the flexibility of Bayesian nonparametrics applied to large and complex genetic data. version:1
arxiv-1611-00514 | The Intelligent Voice 2016 Speaker Recognition System | http://arxiv.org/abs/1611.00514 | id:1611.00514 author:Abbas Khosravani, Cornelius Glackin, Nazim Dugan, Gérard Chollet, Nigel Cannings category:cs.SD cs.CL stat.ML  published:2016-11-02 summary:This paper presents the Intelligent Voice (IV) system submitted to the NIST 2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this year was on developing speaker recognition technology which is robust for novel languages that are much more heterogeneous than those used in the current state-of-the-art, using significantly less training data, that does not contain meta-data from those languages. The system is based on the state-of-the-art i-vector/PLDA which is developed on the fixed training condition, and the results are reported on the protocol defined on the development set of the challenge. version:1
arxiv-1611-00472 | Towards Sub-Word Level Compositions for Sentiment Analysis of Hindi-English Code Mixed Text | http://arxiv.org/abs/1611.00472 | id:1611.00472 author:Ameya Prabhu, Aditya Joshi, Manish Shrivastava, Vasudeva Varma category:cs.CL  published:2016-11-02 summary:Sentiment analysis (SA) using code-mixed data from social media has several applications in opinion mining ranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media. In this paper, we introduce learning sub-word level representations in LSTM (Subword-LSTM) architecture instead of character-level or word-level representations. This linguistic prior in our architecture enables us to learn the information about sentiment value of important morphemes. This also seems to work well in highly noisy text containing misspellings as shown in our experiments which is demonstrated in morpheme-level feature maps learned by our model. Also, we hypothesize that encoding this linguistic prior in the Subword-LSTM architecture leads to the superior performance. Our system attains accuracy 4-5% greater than traditional approaches on our dataset, and also outperforms the available system for sentiment analysis in Hi-En code-mixed text by 18%. version:1
arxiv-1611-00471 | Dual Attention Networks for Multimodal Reasoning and Matching | http://arxiv.org/abs/1611.00471 | id:1611.00471 author:Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim category:cs.CV  published:2016-11-02 summary:We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. First, the reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). Second, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching. version:1
arxiv-1611-00468 | CRF-CNN: Modeling Structured Information in Human Pose Estimation | http://arxiv.org/abs/1611.00468 | id:1611.00468 author:Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang category:cs.CV  published:2016-11-02 summary:Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks. Finally, a neural network implementation of end-to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets. version:1
arxiv-1611-00760 | Quantum Laplacian Eigenmap | http://arxiv.org/abs/1611.00760 | id:1611.00760 author:Yiming Huang, Xiaoyu Li category:quant-ph cs.LG  published:2016-11-02 summary:Laplacian eigenmap algorithm is a typical nonlinear model for dimensionality reduction in classical machine learning. We propose an efficient quantum Laplacian eigenmap algorithm to exponentially speed up the original counterparts. In our work, we demonstrate that the Hermitian chain product proposed in quantum linear discriminant analysis (arXiv:1510.00113,2015) can be applied to implement quantum Laplacian eigenmap algorithm. While classical Laplacian eigenmap algorithm requires polynomial time to solve the eigenvector problem, our algorithm is able to exponentially speed up nonlinear dimensionality reduction. version:1
arxiv-1611-00457 | Structure vs. Language: Investigating the Multi-factors of Asymmetric Opinions on Online Social Interrelationship with a Case Study | http://arxiv.org/abs/1611.00457 | id:1611.00457 author:Bo Wang, Yingjun Sun, Yuan Wang category:cs.SI cs.CL  published:2016-11-02 summary:Though current researches often study the properties of online social relationship from an objective view, we also need to understand individuals' subjective opinions on their interrelationships in social computing studies. Inspired by the theories from sociolinguistics, the latest work indicates that interactive language can reveal individuals' asymmetric opinions on their interrelationship. In this work, in order to explain the opinions' asymmetry on interrelationship with more latent factors, we extend the investigation from single relationship to the structural context in online social network. We analyze the correlation between interactive language features and the structural context of interrelationships. The structural context of vertex, edges and triangles in social network are considered. With statistical analysis on Enron email dataset, we find that individuals' opinions (measured by interactive language features) on their interrelationship are related to some of their important structural context in social network. This result can help us to understand and measure the individuals' opinions on their interrelationship with more intrinsic information. version:1
arxiv-1611-00454 | Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks | http://arxiv.org/abs/1611.00454 | id:1611.00454 author:Hao Wang, Xingjian Shi, Dit-Yan Yeung category:cs.LG cs.AI cs.CL cs.CV stat.ML  published:2016-11-02 summary:Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information. version:1
arxiv-1611-00448 | Natural-Parameter Networks: A Class of Probabilistic Neural Networks | http://arxiv.org/abs/1611.00448 | id:1611.00448 author:Hao Wang, Xingjian Shi, Dit-Yan Yeung category:cs.LG cs.AI cs.CL cs.CV stat.ML  published:2016-11-02 summary:Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance. version:1
arxiv-1611-00440 | And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election | http://arxiv.org/abs/1611.00440 | id:1611.00440 author:Elvyna Tunggawan, Yustinus Eko Soelistio category:cs.IR cs.CL cs.SI  published:2016-11-02 summary:This paper describes a Naive-Bayesian predictive model for 2016 U.S. Presidential Election based on Twitter data. We use 33,708 tweets gathered since December 16, 2015 until February 29, 2016. We introduce a simpler data preprocessing method to label the data and train the model. The model achieves 95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie Sanders as Republican and Democratic nominee respectively. It achieves a comparable result to those in its competitor methods. version:1
arxiv-1611-00429 | Distributed Mean Estimation with Limited Communication | http://arxiv.org/abs/1611.00429 | id:1611.00429 author:Ananda Theertha Suresh, Felix X. Yu, H. Brendan McMahan, Sanjiv Kumar category:cs.LG  published:2016-11-02 summary:Motivated by the need for distributed optimization algorithms with low communication cost, we study communication efficient algorithms to perform distributed mean estimation. We study scenarios in which each client sends one bit per dimension. We first show that for $d$ dimensional data with $n$ clients, a naive stochastic rounding approach yields a mean squared error $\Theta(d/n)$. We then show by applying a structured random rotation of the data (an $\mathcal{O}(d \log d)$ algorithm), the error can be reduced to $\mathcal{O}((\log d)/n)$. The algorithms and the analysis make no distributional assumptions on the data. version:1
arxiv-1611-00421 | Flood-Filling Networks | http://arxiv.org/abs/1611.00421 | id:1611.00421 author:Michał Januszewski, Jeremy Maitin-Shepard, Peter Li, Jörgen Kornfeld, Winfried Denk, Viren Jain category:cs.CV  published:2016-11-01 summary:State-of-the-art image segmentation algorithms generally consist of at least two successive and distinct computations: a boundary detection process that uses local image information to classify image locations as boundaries between objects, followed by a pixel grouping step such as watershed or connected components that clusters pixels into segments. Prior work has varied the complexity and approach employed in these two steps, including the incorporation of multi-layer neural networks to perform boundary prediction, and the use of global optimizations during pixel clustering. We propose a unified and end-to-end trainable machine learning approach, flood-filling networks, in which a recurrent 3d convolutional network directly produces individual segments from a raw image. The proposed approach robustly segments images with an unknown and variable number of objects as well as highly variable object sizes. We demonstrate the approach on a challenging 3d image segmentation task, connectomic reconstruction from volume electron microscopy data, on which flood-filling neural networks substantially improve accuracy over other state-of-the-art methods. The proposed approach can replace complex multi-step segmentation pipelines with a single neural network that is learned end-to-end. version:1
arxiv-1611-00393 | Solving Visual Madlibs with Multiple Cues | http://arxiv.org/abs/1611.00393 | id:1611.00393 author:Tatiana Tommasi, Arun Mallya, Bryan Plummer, Svetlana Lazebnik, Alexander C. Berg, Tamara L. Berg category:cs.CV  published:2016-11-01 summary:This paper presents an approach for answering fill-in-the-blank multiple choice questions from the Visual Madlibs dataset. Instead of generic and commonly used representations trained on the ImageNet dataset, our approach employs a combination of networks trained for specialized tasks such as scene recognition, person activity classification, and attribute prediction. We also present a method for localizing phrases from candidate answers in order to provide spatial support for feature extraction. We map each of these features, together with candidate answers, to a joint embedding space through normalized canonical correlation analysis (CCA). Finally, we solve an optimization problem to learn to combine CCA scores from multiple cues to select the best answer. Extensive experimental results show a significant improvement over the previous state of the art and confirm that answering questions from a wide range of types benefits from examining a variety of image cues and carefully choosing the spatial support of feature extraction. version:1
arxiv-1611-00384 | The Deep Journey from Content to Collaborative Filtering | http://arxiv.org/abs/1611.00384 | id:1611.00384 author:Oren Barkan, Noam Koenigstein, Eylon Yogev category:cs.IR cs.CL cs.LG  published:2016-11-01 summary:In Recommender Systems research, algorithms are often characterized as either Collaborative Filtering (CF) or Content Based (CB). CF algorithms are trained using a dataset of user explicit or implicit preferences while CB algorithms are typically based on item profiles. These approaches harness very different data sources hence the resulting recommended items are generally also very different. This paper presents a novel model that serves as a bridge from items content into their CF representations. We introduce a multiple input deep regression model to predict the CF latent embedding vectors of items based on their textual description and metadata. We showcase the effectiveness of the proposed model by predicting the CF vectors of movies and apps based on their textual descriptions. Finally, we show that the model can be further improved by incorporating metadata such as the movie release year and tags which contribute to a higher accuracy. version:1
arxiv-1610-07472 | Distilling Information Reliability and Source Trustworthiness from Digital Traces | http://arxiv.org/abs/1610.07472 | id:1610.07472 author:Behzad Tabibian, Isabel Valera, Mehrdad Farajtabar, Le Song, Bernhard Schölkopf, Manuel Gomez-Rodriguez category:cs.SI stat.ML  published:2016-10-24 summary:Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their content. These evaluations can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy evaluations, often biased, to distill a robust, unbiased and interpretable measure of both notions? In this paper, we argue that the temporal traces left by these noisy evaluations give cues on the reliability of the information and the trustworthiness of the sources. Then, we propose a temporal point process modeling framework that links these temporal traces to robust, unbiased and interpretable notions of information reliability and source trustworthiness. Furthermore, we develop an efficient convex optimization procedure to learn the parameters of the model from historical traces. Experiments on real-world data gathered from Wikipedia and Stack Overflow show that our modeling framework accurately predicts evaluation events, provides an interpretable measure of information reliability and source trustworthiness, and yields interesting insights about real-world events. version:2
arxiv-1611-00379 | The Machine Learning Algorithm as Creative Musical Tool | http://arxiv.org/abs/1611.00379 | id:1611.00379 author:Rebecca Fiebrink, Baptiste Caramiaux category:cs.HC cs.LG  published:2016-11-01 summary:Machine learning is the capacity of a computational system to learn structures from datasets in order to make predictions on newly seen data. Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine learning, and human-computer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts. We motivate a new understanding of learning algorithms as human-computer interfaces. We show that, like other interfaces, learning algorithms can be characterised by the ways their affordances intersect with goals of human users. We also argue that the nature of interaction between users and algorithms impacts the usability and usefulness of those algorithms in profound ways. This human-centred view of machine learning motivates our concluding discussion of what it means to employ machine learning as a creative tool. version:1
arxiv-1611-00356 | Using Artificial Intelligence to Identify State Secrets | http://arxiv.org/abs/1611.00356 | id:1611.00356 author:Renato Rocha Souza, Flavio Codeco Coelho, Rohan Shah, Matthew Connelly category:cs.CY cs.CL cs.LG  published:2016-11-01 summary:Whether officials can be trusted to protect national security information has become a matter of great public controversy, reigniting a long-standing debate about the scope and nature of official secrecy. The declassification of millions of electronic records has made it possible to analyze these issues with greater rigor and precision. Using machine-learning methods, we examined nearly a million State Department cables from the 1970s to identify features of records that are more likely to be classified, such as international negotiations, military operations, and high-level communications. Even with incomplete data, algorithms can use such features to identify 90% of classified cables with <11% false positives. But our results also show that there are longstanding problems in the identification of sensitive information. Error analysis reveals many examples of both overclassification and underclassification. This indicates both the need for research on inter-coder reliability among officials as to what constitutes classified material and the opportunity to develop recommender systems to better manage both classification and declassification. version:1
arxiv-1611-00354 | Faster decoding for subword level Phrase-based SMT between related languages | http://arxiv.org/abs/1611.00354 | id:1611.00354 author:Anoop Kunchukuttan, Pushpak Bhattacharyya category:cs.CL  published:2016-11-01 summary:A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy. version:1
arxiv-1611-00350 | Computationally Efficient Influence Maximization in Stochastic and Adversarial Models: Algorithms and Analysis | http://arxiv.org/abs/1611.00350 | id:1611.00350 author:Justin Khim, Varun Jog, Po-Ling Loh category:cs.SI cs.LG stat.ML  published:2016-11-01 summary:We consider the problem of influence maximization in fixed networks, for both stochastic and adversarial contagion models. The common goal is to select a subset of nodes of a specified size to infect so that the number of infected nodes at the conclusion of the epidemic is as large as possible. In the stochastic setting, the epidemic spreads according to a general triggering model, which includes the popular linear threshold and independent cascade models. We establish upper and lower bounds for the influence of an initial subset of nodes in the network, where the influence is defined as the expected number of infected nodes. Although the problem of exact influence computation is NP-hard in general, our bounds may be evaluated efficiently, leading to scalable algorithms for influence maximization with rigorous theoretical guarantees. In the adversarial spreading setting, an adversary is allowed to specify the edges through which contagion may spread, and the player chooses sets of nodes to infect in successive rounds. Both the adversary and player may behave stochastically, but we limit the adversary to strategies that are oblivious of the player's actions. We establish upper and lower bounds on the minimax pseudo-regret in both undirected and directed networks. version:1
arxiv-1611-00347 | Surpassing Gradient Descent Provably: A Cyclic Incremental Method with Linear Convergence Rate | http://arxiv.org/abs/1611.00347 | id:1611.00347 author:Aryan Mokhtari, Mert Gürbüzbalaban, Alejandro Ribeiro category:math.OC cs.LG  published:2016-11-01 summary:Recently, there has been growing interest in developing optimization methods for solving large-scale machine learning problems. Most of these problems boil down to the problem of minimizing an average of a finite set of smooth and strongly convex functions where the number of functions $n$ is large. Gradient descent method (GD) is successful in minimizing convex problems at a fast linear rate; however, it is not applicable to the considered large-scale optimization setting because of the high computational complexity. Incremental methods resolve this drawback of gradient methods by replacing the required gradient for the descent direction with an incremental gradient approximation. They operate by evaluating one gradient per iteration and executing the average of the $n$ available gradients as a gradient approximate. Although, incremental methods reduce the computational cost of GD, their convergence rates do not justify their advantage relative to GD in terms of the total number of gradient evaluations until convergence. In this paper, we introduce a Double Incremental Aggregated Gradient method (DIAG) that computes the gradient of only one function at each iteration, which is chosen based on a cyclic scheme, and uses the aggregated average gradient of all the functions to approximate the full gradient. The iterates of the proposed DIAG method uses averages of both iterates and gradients in oppose to classic incremental methods that utilize gradient averages but do not utilize iterate averages. We prove that not only the proposed DIAG method converges linearly to the optimal solution, but also its linear convergence factor justifies the advantage of incremental methods on GD. In particular, we prove that the worst case performance of DIAG is better than the worst case performance of GD. version:1
arxiv-1611-00340 | Variational Bayes In Private Settings (VIPS) | http://arxiv.org/abs/1611.00340 | id:1611.00340 author:Mijung Park, James Foulds, Kamalika Chaudhuri, Max Welling category:stat.ML cs.CR  published:2016-11-01 summary:We provide a general framework for privacy-preserving variational Bayes (VB) for a large class of probabilistic models, called the conjugate exponential (CE) family. Our primary observation is that when models are in the CE family, we can privatise the variational posterior distributions simply by perturbing the expected sufficient statistics of the complete-data likelihood. For widely used non-CE models with binomial likelihoods (e.g., logistic regression), we exploit the P{\'o}lya-Gamma data augmentation scheme to bring such models into the CE family, such that inferences in the modified model resemble the original (private) variational Bayes algorithm as closely as possible. The iterative nature of variational Bayes presents a further challenge for privacy preservation, as each iteration increases the amount of noise needed. We overcome this challenge by combining: (1) a relaxed notion of differential privacy, called {\it{concentrated differential privacy}}, which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect resulting from subsampling mini-batches from large-scale data in stochastic learning. We empirically demonstrate the effectiveness of our method in CE and non-CE models including latent Dirichlet allocation (LDA) and Bayesian logistic regression, evaluated on real-world datasets. version:1
arxiv-1611-00328 | The $χ$-Divergence for Approximate Inference | http://arxiv.org/abs/1611.00328 | id:1611.00328 author:Adji B. Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David M. Blei category:stat.ML cs.LG stat.CO stat.ME  published:2016-11-01 summary:Variational inference enables Bayesian analysis for complex probabilistic models with massive data sets. It works by positing a family of distributions and finding the member in the family that is closest to the posterior. While successful, variational methods can run into pathologies; for example, they typically underestimate posterior uncertainty. We propose CHI-VI, a complementary algorithm to traditional variational inference with KL($q$ $p$) and an alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes the $\chi$-divergence from the posterior to the family of approximating distributions. In EP, only local minimization of the KL($p$ $q$) objective is possible. In contrast, CHI-VI optimizes a well-defined global objective. It directly minimizes an upper bound to the model evidence that equivalently minimizes the $\chi$-divergence. In experiments, we illustrate the utility of the upper bound for sandwich estimating the model evidence. We also compare several probabilistic models and a Cox process for basketball data. We find CHI-VI often yields better classification error rates and better posterior uncertainty. version:1
arxiv-1611-00326 | Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech Detection | http://arxiv.org/abs/1611.00326 | id:1611.00326 author:Pengfei Sun, Jun Qin category:cs.SD cs.LG stat.ML  published:2016-11-01 summary:In this letter, we propose enhanced factored three way restricted Boltzmann machines (EFTW-RBMs) for speech detection. The proposed model incorporates conditional feature learning by multiplying the dynamical state of the third unit, which allows a modulation over the visible-hidden node pairs. Instead of stacking previous frames of speech as the third unit in a recursive manner, the correlation related weighting coefficients are assigned to the contextual neighboring frames. Specifically, a threshold function is designed to capture the long-term features and blend the globally stored speech structure. A factored low rank approximation is introduced to reduce the parameters of the three-dimensional interaction tensor, on which non-negative constraint is imposed to address the sparsity characteristic. The validations through the area-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our approach outperforms several existing 1D and 2D (i.e., time and time-frequency domain) speech detection algorithms in various noisy environments. version:1
arxiv-1610-03017 | Fully Character-Level Neural Machine Translation without Explicit Segmentation | http://arxiv.org/abs/1610.03017 | id:1610.03017 author:Jason Lee, Kyunghyun Cho, Thomas Hofmann category:cs.CL cs.LG  published:2016-10-10 summary:Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment. version:2
arxiv-1611-00303 | Semi-Supervised Radio Signal Identification | http://arxiv.org/abs/1611.00303 | id:1611.00303 author:Timothy J. O'Shea, Nathan West, Matthew Vondal, T. Charles Clancy category:cs.LG cs.IT math.IT stat.ML  published:2016-11-01 summary:Radio recognition in complex multi-user environments is an important tool for optimizing spectrum utilization, identifying and minimizing interference, and enforcing spectrum policy. Radio data is readily available and easy to obtain, but labeled data is often scarce making supervised learning strategies difficult and time consuming to curate. We demonstrate that semi-supervised learning techniques can be used to scale learning beyond supervised datasets, allowing for both discerning and recalling radio signals of interest by using sparse signal representations based on both unsupervised and supervised methods for nonlinear feature learning. version:1
arxiv-1611-00301 | Recurrent Neural Radio Anomaly Detection | http://arxiv.org/abs/1611.00301 | id:1611.00301 author:Timothy J O'Shea, T. Charles Clancy, Robert W. McGwier category:cs.LG  published:2016-11-01 summary:We introduce a powerful recurrent neural network based method for novelty detection to the application of detecting radio anomalies. This approach holds promise in significantly increasing the ability of naive anomaly detection to detect small anomalies in highly complex complexity multi-user radio bands. We demonstrate the efficacy of this approach on a number of common real over the air radio communications bands of interest and quantify detection performance in terms of probability of detection an false alarm rates across a range of interference to band power ratios and compare to baseline methods. version:1
arxiv-1610-06906 | Review of Action Recognition and Detection Methods | http://arxiv.org/abs/1610.06906 | id:1610.06906 author:Soo Min Kang, Richard P. Wildes category:cs.CV  published:2016-10-21 summary:In computer vision, action recognition refers to the act of classifying an action that is present in a given video and action detection involves locating actions of interest in space and/or time. Videos, which contain photometric information (e.g. RGB, intensity values) in a lattice structure, contain information that can assist in identifying the action that has been imaged. The process of action recognition and detection often begins with extracting useful features and encoding them to ensure that the features are specific to serve the task of action recognition and detection. Encoded features are then processed through a classifier to identify the action class and their spatial and/or temporal locations. In this report, a thorough review of various action recognition and detection algorithms in computer vision is provided by analyzing the two-step process of a typical action recognition and detection algorithm: (i) extraction and encoding of features, and (ii) classifying features into action classes. In efforts to ensure that computer vision-based algorithms reach the capabilities that humans have of identifying actions irrespective of various nuisance variables that may be present within the field of view, the state-of-the-art methods are reviewed and some remaining problems are addressed in the final chapter. version:2
arxiv-1611-00261 | Causal Compression | http://arxiv.org/abs/1611.00261 | id:1611.00261 author:Aleksander Wieczorek, Volker Roth category:stat.ML  published:2016-11-01 summary:We propose a new method of discovering causal relationships in temporal data based on the notion of causal compression. To this end, we adopt the Pearlian graph setting and the directed information as an information theoretic tool for quantifying causality. We introduce chain rule for directed information and use it to motivate causal sparsity. We show two applications of the proposed method: causal time series segmentation which selects time points capturing the incoming and outgoing causal flow between time points belonging to different signals, and causal bipartite graph recovery. We prove that modelling of causality in the adopted set-up only requires estimating the copula density of the data distribution and thus does not depend on its marginals. We evaluate the method on time resolved gene expression data. version:1
arxiv-1611-00260 | Surrogate-Assisted Partial Order-based Evolutionary Optimisation | http://arxiv.org/abs/1611.00260 | id:1611.00260 author:Vanessa Volz, Günter Rudolph, Boris Naujoks category:cs.NE  published:2016-11-01 summary:In this paper, we propose a novel approach (SAPEO) to support the survival selection process in multi-objective evolutionary algorithms with surrogate models - it dynamically chooses individuals to evaluate exactly based on the model uncertainty and the distinctness of the population. We introduce variants that differ in terms of the risk they allow when doing survival selection. Here, the anytime performance of different SAPEO variants is evaluated in conjunction with an SMS-EMOA using the BBOB bi-objective benchmark. We compare the obtained results with the performance of the regular SMS-EMOA, as well as another surrogate-assisted approach. The results open up general questions about the applicability and required conditions for surrogate-assisted multi-objective evolutionary algorithms to be tackled in the future. version:1
arxiv-1611-00255 | Stationary time-vertex signal processing | http://arxiv.org/abs/1611.00255 | id:1611.00255 author:Andreas Loukas, Nathanaël Perraudin category:cs.LG cs.DS stat.ML  published:2016-11-01 summary:The goal of this paper is to improve learning for multivariate processes whose structure is dependent on some known graph topology. Typically, the graph information is incorporated to the learning process via a smoothness assumption postulating that the values supported on well connected vertices exhibit small variations. We argue that smoothness is not enough. To capture the behavior of complex interconnected systems, such as transportation and biological networks, it is important to train expressive models, being able to reproduce a wide range of graph and temporal behaviors. Motivated by this need, this paper puts forth a novel definition of time-vertex wide-sense stationarity, or joint stationarity for short. We believe that the proposed definition is natural, at it elegantly relates to existing definitions of stationarity in the time and vertex domains. We use joint stationarity to regularize learning and to reduce computational complexity in both estimation and recovery tasks. In particular, we show that for any jointly stationary process: (a) one can learn the covariance structure from O(1) samples, and (b) can solve MMSE recovery problems, such as interpolation, denoising, forecasting, in complexity that is linear to the edges and timesteps. Experiments with three datasets suggest that joint stationarity can yield significant accuracy improvements in the reconstruction effort. version:1
arxiv-1611-00218 | Sliding Dictionary Based Sparse Representation For Action Recognition | http://arxiv.org/abs/1611.00218 | id:1611.00218 author:Yashas Annadani, D L Rakshith, Soma Biswas category:cs.CV  published:2016-11-01 summary:The task of action recognition has been in the forefront of research, given its applications in gaming, surveillance and health care. In this work, we propose a simple, yet very effective approach which works seamlessly for both offline and online action recognition using the skeletal joints. We construct a sliding dictionary which has the training data along with their time stamps. This is used to compute the sparse coefficients of the input action sequence which is divided into overlapping windows and each window gives a probability score for each action class. In addition, we compute another simple feature, which calibrates each of the action sequences to the training sequences, and models the deviation of the action from the each of the training data. Finally, a score level fusion of the two heterogeneous but complementary features for each window is obtained and the scores for the available windows are successively combined to give the confidence scores of each action class. This way of combining the scores makes the approach suitable for scenarios where only part of the sequence is available. Extensive experimental evaluation on three publicly available datasets shows the effectiveness of the proposed approach for both offline and online action recognition tasks. version:1
arxiv-1610-08249 | Universality of Bayesian mixture predictors | http://arxiv.org/abs/1610.08249 | id:1610.08249 author:Daniil Ryabko category:math.ST cs.IT cs.LG math.IT stat.TH  published:2016-10-26 summary:The problem is that of sequential probability forecasting for finite-valued time series. The data is generated by an unknown probability distribution over the space of all one-way infinite sequences. It is known that this measure belongs to a given set C, but the latter is completely arbitrary (uncountably infinite, without any structure given). The performance is measured with asymptotic average log loss. In this work it is shown that the minimax asymptotic performance is always attainable, and it is attained by a convex combination of a countably many measures from the set C (a Bayesian mixture). This was previously only known for the case when the best achievable asymptotic error is 0. This also contrasts previous results that show that in the non-realizable case all Bayesian mixtures may be suboptimal, while there is a predictor that achieves the optimal performance. version:2
arxiv-1610-09559 | Rawlsian Fairness for Machine Learning | http://arxiv.org/abs/1610.09559 | id:1610.09559 author:Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, Aaron Roth category:cs.LG  published:2016-10-29 summary:Motivated by concerns that automated decision-making procedures can unintentionally lead to discriminatory behavior, we study a technical definition of fairness modeled after John Rawls' notion of "fair equality of opportunity". In the context of a simple model of online decision making, we give an algorithm that satisfies this fairness constraint, while still being able to learn at a rate that is comparable to (but necessarily worse than) that of the best algorithms absent a fairness constraint. We prove a regret bound for fair algorithms in the linear contextual bandit framework that is a significant improvement over our technical companion paper [16], which gives black-box reductions in a more general setting. We analyze our algorithms both theoretically and experimentally. Finally, we introduce the notion of a "discrimination index", and show that standard algorithms for our problem exhibit structured discriminatory behavior, whereas the "fair" algorithms we develop do not. version:2
arxiv-1611-00196 | Recurrent Neural Network Language Model Adaptation Derived Document Vector | http://arxiv.org/abs/1611.00196 | id:1611.00196 author:Wei Li, Brian Kan, Wing Mak category:cs.CL  published:2016-11-01 summary:In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document, and they can be important in some NLP tasks such as genre classification. This paper proposes a novel distributed vector representation of a document: a simple recurrent-neural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document. The new document vectors are labeled as DV-RNN and DV-LSTM respectively. We believe that our new document vectors can capture some high-level sequential information in the documents, which other current document representations fail to capture. The new document vectors were evaluated in the genre classification of documents in three corpora: the Brown Corpus, the BNC Baby Corpus and an artificially created Penn Treebank dataset. Their classification performances are compared with the performance of TF-IDF vector and the state-of-the-art distributed memory model of paragraph vector (PV-DM). The results show that DV-LSTM significantly outperforms TF-IDF and PV-DM in most cases, and combinations of the proposed document vectors with TF-IDF or PV-DM may further improve performance. version:1
arxiv-1611-00179 | Dual Learning for Machine Translation | http://arxiv.org/abs/1611.00179 | id:1611.00179 author:Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma category:cs.CL  published:2016-11-01 summary:While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task. version:1
arxiv-1610-08694 | CogALex-V Shared Task: LexNET - Integrated Path-based and Distributional Method for the Identification of Semantic Relations | http://arxiv.org/abs/1610.08694 | id:1610.08694 author:Vered Shwartz, Ido Dagan category:cs.CL  published:2016-10-27 summary:We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing concrete semantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task. version:3
arxiv-1611-00175 | Robust Spectral Inference for Joint Stochastic Matrix Factorization | http://arxiv.org/abs/1611.00175 | id:1611.00175 author:Moontae Lee, David Bindel, David Mimno category:cs.LG cs.AI  published:2016-11-01 summary:Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality. version:1
arxiv-1611-00170 | Online Maximum Likelihood Estimation of the Parameters of Partially Observed Diffusion Processes | http://arxiv.org/abs/1611.00170 | id:1611.00170 author:Simone Carlo Surace, Jean-Pascal Pfister category:math.OC math.PR stat.ML  published:2016-11-01 summary:We revisit the problem of estimating the parameters of a partially observed stochastic process $(X_t,Y_t)$ with a continuous time parameter, where $X_t$ is the hidden state process and $Y_t$ is the observed process. The estimation is to be done online, i.e. the parameter estimate should be updated recursively based on the observation filtration $\sigma\{Y_s, s\leq t\}$. Online parameter estimation is a challenging problem that needs to be solved for designing adaptive filters and for stochastic control in all cases where the system is unknown or changing over time, with applications in robotics, neuroscience, or finance. Here, we use the representation of the log-likelihood function in terms of the Radon-Nikodym derivative of the probability measure restricted to the observation (the observation likelihood) with respect to a reference measure under which $Y_t$ is a Wiener process. This log-likelihood can be computed by using the stochastic filter. Using stochastic gradient ascent on the likelihood function, we obtain an algorithm for the time evolution of the parameter estimate. Although this approach is based on theoretical results that have been known for several decades, this explicit method of recursive parameter estimation has remained unpublished. version:1
arxiv-1611-00148 | Best-Buddies Tracking | http://arxiv.org/abs/1611.00148 | id:1611.00148 author:Shaul Oron, Denis Suhanov, Shai Avidan category:cs.CV  published:2016-11-01 summary:Best-Buddies Tracking (BBT) applies the Best-Buddies Similarity measure (BBS) to the problem of model-free online tracking. BBS was introduced as a similarity measure between two point sets and was shown to be very effective for template matching. Originally, BBS was designed to work with point sets of equal size, and we propose a modification that lets it handle point sets of different size. The modified BBS is better suited to handle scale changes in the template size, as well as support a variable number of template images. We embed the modified BBS in a particle filter framework and obtain good results on a number of standard benchmarks. version:1
arxiv-1611-00144 | Product-based Neural Networks for User Response Prediction | http://arxiv.org/abs/1611.00144 | id:1611.00144 author:Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, Jun Wang category:cs.LG cs.IR  published:2016-11-01 summary:Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics. version:1
arxiv-1611-00138 | MusicMood: Predicting the mood of music from song lyrics using machine learning | http://arxiv.org/abs/1611.00138 | id:1611.00138 author:Sebastian Raschka category:cs.LG cs.CL cs.IR  published:2016-11-01 summary:Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics. version:1
arxiv-1611-00137 | Embedding Deep Metric for Person Re-identication A Study Against Large Variations | http://arxiv.org/abs/1611.00137 | id:1611.00137 author:Hailin Shi, Yang Yang, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Weishi Zheng, Stan Z. Li category:cs.CV cs.LG  published:2016-11-01 summary:Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)'s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification. version:1
arxiv-1611-00135 | A Benchmark Dataset and Saliency-guided Stacked Autoencoder for Video-based Salient Object Detection | http://arxiv.org/abs/1611.00135 | id:1611.00135 author:Jia Li, Changqun Xia, Xiaowu Chen category:cs.CV  published:2016-11-01 summary:Image-based salient object detection (SOD) has been extensively studied in the past decades. However, video-based SOD is much less explored since there lacks large-scale video datasets within which salient objects are unambiguously defined and annotated. Toward this end, this paper proposes a video-based SOD dataset that consists of 200 videos (64 minutes). In constructing the dataset, we manually annotate all objects and regions over 7,650 uniformly sampled keyframes and collect the eye-tracking data of 23 subjects that free-view all videos. From the user data, we find salient objects in video can be defined as objects that consistently pop-out throughout the video, and objects with such attributes can be unambiguously annotated by combining manually annotated object/region masks with eye-tracking data of multiple subjects. To the best of our knowledge, it is currently the largest dataset for video-based salient object detection. Based on the dataset, this paper proposes an unsupervised approach for video-based SOD by using a saliency-guided stacked autoencoder. In the proposed approach, spatiotemporal saliency cues are first extracted at pixel, superpixel and object levels. With these saliency cues, a stacked autoencoder is unsupervisedly trained which can automatically infer a saliency score for each pixel by progressively encoding the high-dimensional saliency cues gathered from the pixel and its spatiotemporal neighbors. Experimental results show that the proposed approach outperforms 19 image-based and 5 video-based models on the proposed dataset. Moreover, the comprehensive benchmarking results show that the proposed dataset is very challenging and have the potential to greatly boost the development of video-based SOD. version:1
arxiv-1610-09935 | Knowledge Questions from Knowledge Graphs | http://arxiv.org/abs/1610.09935 | id:1610.09935 author:Dominic Seyler, Mohamed Yahya, Klaus Berberich category:cs.CL  published:2016-10-31 summary:We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiple-choice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach. version:2
arxiv-1610-04325 | Hadamard Product for Low-rank Bilinear Pooling | http://arxiv.org/abs/1610.04325 | id:1610.04325 author:Jin-Hwa Kim, Kyoung-Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang category:cs.CV cs.AI cs.NE  published:2016-10-14 summary:Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property. version:2
arxiv-1611-00126 | Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings | http://arxiv.org/abs/1611.00126 | id:1611.00126 author:Shufeng Xiong category:cs.CL  published:2016-11-01 summary:Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods. version:1
arxiv-1610-09736 | A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction | http://arxiv.org/abs/1610.09736 | id:1610.09736 author:Eunhee Kang, Junhong Min, Jong Chul Ye category:cs.CV  published:2016-10-31 summary:Due to the potential risk of inducing cancers, radiation dose of X-ray CT should be reduced for routine patient scanning. However, in low-dose X-ray CT, severe artifacts usually occur due to photon starvation, beamhardening, etc, which decrease the reliability of diagnosis. Thus, high quality reconstruction from low-dose X-ray CT data has become one of the important research topics in CT community. Conventional model-based denoising approaches are, however, computationally very expensive, and image domain denoising approaches hardly deal with CT specific noise patterns. To address these issues, we propose an algorithm using a deep convolutional neural network (CNN), which is applied to wavelet transform coefficients of low-dose CT images. Specifically, by using a directional wavelet transform for extracting directional component of artifacts and exploiting the intra- and inter-band correlations, our deep network can effectively suppress CT specific noises. Moreover, our CNN is designed to have various types of residual learning architecture for faster network training and better denoising. Experimental results confirm that the proposed algorithm effectively removes complex noise patterns of CT images, originated from the reduced X-ray dose. In addition, we show that wavelet domain CNN is efficient in removing the noises from low-dose CT compared to an image domain CNN. Our results were rigorously evaluated by several radiologists and won the second place award in 2016 AAPM Low-Dose CT Grand Challenge. To the best of our knowledge, this work is the first deep learning architecture for low-dose CT reconstruction that has been rigorously evaluated and proven for its efficacy. version:2
arxiv-1610-09033 | Operator Variational Inference | http://arxiv.org/abs/1610.09033 | id:1610.09033 author:Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei category:stat.ML cs.LG stat.CO stat.ME  published:2016-10-27 summary:Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images. version:2
arxiv-1611-00068 | RNN Approaches to Text Normalization: A Challenge | http://arxiv.org/abs/1611.00068 | id:1611.00068 author:Richard Sproat, Navdeep Jaitly category:cs.CL  published:2016-10-31 summary:This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. version:1
arxiv-1611-00065 | Bayesian Adaptive Data Analysis Guarantees from Subgaussianity | http://arxiv.org/abs/1611.00065 | id:1611.00065 author:Sam Elder category:cs.LG math.PR stat.ML  published:2016-10-31 summary:The new field of adaptive data analysis seeks to provide algorithms and provable guarantees for models of machine learning that allow researchers to reuse their data, which normally falls outside of the usual statistical paradigm of static data analysis. In 2014, Dwork, Feldman, Hardt, Pitassi, Reingold and Roth introduced one potential model and proposed several solutions based on differential privacy. In previous work in 2016, we described a problem with this model and instead proposed a Bayesian variant, but also found that the analogous Bayesian methods cannot achieve the same statistical guarantees as in the static case. In this paper, we prove the first positive results for the Bayesian model, showing that with a Dirichlet prior, the posterior mean algorithm indeed matches the statistical guarantees of the static case. We conjecture that this is true for any conjugate prior from the exponential family, but can only prove this in special cases. The main ingredient, Theorem 4, shows that the $\text{Beta}(\alpha,\beta)$ distribution is subgaussian with variance proxy $O(1/(\alpha+\beta+1))$, a concentration result also of independent interest. Unlike most moment-based concentration techniques, which bound the centered moments, our proof utilizes a simple condition on the raw moments of a positive random variable. version:1
arxiv-1611-00058 | Kernel Bandwidth Selection for SVDD: Peak Criterion Approach for Large Data | http://arxiv.org/abs/1611.00058 | id:1611.00058 author:Sergiy Peredriy, Deovrat Kakde, Arin Chaudhuri category:cs.LG stat.ML G.3; G.4; I.2.6  published:2016-10-31 summary:Support Vector Data Description (SVDD) provides a useful approach to construct a description of multivariate data for single-class classification and outlier detection with various practical applications. Gaussian kernel used in SVDD formulation allows flexible data description defined by observations designated as support vectors. The data boundary of such description is non-spherical and conforms to the geometric features of the data. By varying the Gaussian kernel bandwidth parameter, the SVDD-generated boundary can be made either smoother (more spherical) or tighter/jagged. The former case may lead to under-fitting, whereas the latter may result in overfitting. Peak criterion has been proposed to select an optimal value of the kernel bandwidth to strike the balance between the data boundary smoothness and its ability to capture the general geometric shape of the data. Peak criterion involves training SVDD at various values of the kernel bandwidth parameter. When training datasets are large, the time required to obtain the optimal value of the Gaussian kernel bandwidth parameter according to Peak method can become prohibitively large. This paper proposes an extension of Peak method for the case of large data. The proposed method gives good results when applied to several datasets. Two existing alternative methods of computing the Gaussian kernel bandwidth parameter (Coefficient of Variation and Distance to the Farthest Neighbor) were modified to allow comparison with the proposed method on convergence. Empirical comparison demonstrates the advantage of the proposed method. version:1
arxiv-1611-00050 | Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks | http://arxiv.org/abs/1611.00050 | id:1611.00050 author:Eder Santana, Matthew Emigh, Pablo Zerges, Jose C Principe category:cs.LG cs.CV  published:2016-10-31 summary:We propose a convolutional recurrent neural network, with Winner-Take-All dropout for high dimensional unsupervised feature learning in multi-dimensional time series. We apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature, including the Deep Predictive Coding Networks previously proposed by Chalasani and Principe.Our contributions can be summarized as a scalable reinterpretation of the Deep Predictive Coding Networks trained end-to-end with backpropagation through time, an extension of the previously proposed Winner-Take-All Autoencoders to sequences in time, and a new technique for initializing and regularizing convolutional-recurrent neural networks. version:1
arxiv-1611-00035 | Full-Capacity Unitary Recurrent Neural Networks | http://arxiv.org/abs/1611.00035 | id:1611.00035 author:Scott Wisdom, Thomas Powers, John R. Hershey, Jonathan Le Roux, Les Atlas category:stat.ML cs.LG cs.NE  published:2016-10-31 summary:Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs. version:1
arxiv-1610-10099 | Neural Machine Translation in Linear Time | http://arxiv.org/abs/1610.10099 | id:1610.10099 author:Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu category:cs.CL cs.LG  published:2016-10-31 summary:We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences. version:1
arxiv-1610-10087 | Tensor Switching Networks | http://arxiv.org/abs/1610.10087 | id:1610.10087 author:Chuan-Yung Tsai, Andrew Saxe, David Cox category:cs.NE cs.LG stat.ML  published:2016-10-31 summary:We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks. version:1
arxiv-1610-09307 | Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated | http://arxiv.org/abs/1610.09307 | id:1610.09307 author:Namrata Vaswani, Han Guo category:cs.LG stat.ML  published:2016-10-28 summary:Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as "data-dependent noise". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes. version:2
arxiv-1610-10064 | The Case for Temporal Transparency: Detecting Policy Change Events in Black-Box Decision Making Systems | http://arxiv.org/abs/1610.10064 | id:1610.10064 author:Miguel Ferreira, Muhammad Bilal Zafar, Krishna P. Gummadi category:stat.ML cs.CY  published:2016-10-31 summary:Bringing transparency to black-box decision making systems (DMS) has been a topic of increasing research interest in recent years. Traditional active and passive approaches to make these systems transparent are often limited by scalability and/or feasibility issues. In this paper, we propose a new notion of black-box DMS transparency, named, temporal transparency, whose goal is to detect if/when the DMS policy changes over time, and is mostly invariant to the drawbacks of traditional approaches. We map our notion of temporal transparency to time series changepoint detection methods, and develop a framework to detect policy changes in real-world DMS's. Experiments on New York Stop-question-and-frisk dataset reveal a number of publicly announced and unannounced policy changes, highlighting the utility of our framework. version:1
arxiv-1610-10060 | Optimization for Large-Scale Machine Learning with Distributed Features and Observations | http://arxiv.org/abs/1610.10060 | id:1610.10060 author:Alexandros Nathan, Diego Klabjan category:stat.ML cs.LG  published:2016-10-31 summary:As the size of modern data sets exceeds the disk and memory capacities of a single computer, machine learning practitioners have resorted to parallel and distributed computing. Given that optimization is one of the pillars of machine learning and predictive modeling, distributed optimization methods have recently garnered ample attention in the literature. Although previous research has mostly focused on settings where either the observations, or features of the problem at hand are stored in distributed fashion, the situation where both are partitioned across the nodes of a computer cluster (doubly distributed) has barely been studied. In this work we propose two doubly distributed optimization algorithms. The first one falls under the umbrella of distributed dual coordinate ascent methods, while the second one belongs to the class of stochastic gradient/coordinate descent hybrid methods. We conduct numerical experiments in Spark using real-world and simulated data sets and study the scaling properties of our methods. Our empirical evaluation of the proposed algorithms demonstrates the out-performance of a block distributed ADMM method, which, to the best of our knowledge is the only other existing doubly distributed optimization algorithm. version:1
arxiv-1610-10048 | Bi-modal First Impressions Recognition using Temporally Ordered Deep Audio and Stochastic Visual Features | http://arxiv.org/abs/1610.10048 | id:1610.10048 author:Arulkumar Subramaniam, Vismay Patel, Ashish Mishra, Prashanth Balasubramanian, Anurag Mittal category:cs.CV  published:2016-10-31 summary:We propose a novel approach for First Impressions Recognition in terms of the Big Five personality-traits from short videos. The Big Five personality traits is a model to describe human personality using five broad categories: Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness. We train two bi-modal end-to-end deep neural network architectures using temporally ordered audio and novel stochastic visual features from few frames, without over-fitting. We empirically show that the trained models perform exceptionally well, even after training from a small sub-portions of inputs. Our method is evaluated in ChaLearn LAP 2016 Apparent Personality Analysis (APA) competition using ChaLearn LAP APA2016 dataset and achieved excellent performance. version:1
arxiv-1610-10042 | ConfocalGN : a minimalistic confocal image simulator | http://arxiv.org/abs/1610.10042 | id:1610.10042 author:Serge Dmitrieff, François Nédélec category:cs.CV q-bio.QM  published:2016-10-31 summary:Image analysis has a central importance in applied and fundamental science. Robustness is a much needed feature of image analysis tools, but is hard to evaluate in the absence of knowledge of the ground truth. We developed a very simple confocal image generator to simulate confocal microscopy images from a known ground truth. The software can analyze a sample image to derivate noise parameters and implement them directly in the image simulation. This provides a fast and realistic way to assert the quality and robustness of an image analysis procedure. version:1
arxiv-1610-10033 | A Detailed Rubric for Motion Segmentation | http://arxiv.org/abs/1610.10033 | id:1610.10033 author:Pia Bideau, Erik Learned-Miller category:cs.CV  published:2016-10-31 summary:Motion segmentation is currently an active area of research in computer Vision. The task of comparing different methods of motion segmentation is complicated by the fact that researchers may use subtly different definitions of the problem. Questions such as "Which objects are moving?", "What is background?", and "How can we use motion of the camera to segment objects, whether they are static or moving?" are clearly related to each other, but lead to different algorithms, and imply different versions of the ground truth. This report has two goals. The first is to offer a precise definition of motion segmentation so that the intent of an algorithm is as well-defined as possible. The second is to report on new versions of three previously existing data sets that are compatible with this definition. We hope that this more detailed definition, and the three data sets that go with it, will allow more meaningful comparisons of certain motion segmentation methods. version:1
arxiv-1610-09995 | Generating Sentiment Lexicons for German Twitter | http://arxiv.org/abs/1610.09995 | id:1610.09995 author:Uladzimir Sidarenka, Manfred Stede category:cs.CL  published:2016-10-31 summary:Despite a substantial progress made in developing new sentiment lexicon generation (SLG) methods for English, the task of transferring these approaches to other languages and domains in a sound way still remains open. In this paper, we contribute to the solution of this problem by systematically comparing semi-automatic translations of common English polarity lists with the results of the original automatic SLG algorithms, which were applied directly to German data. We evaluate these lexicons on a corpus of 7,992 manually annotated tweets. In addition to that, we also collate the results of dictionary- and corpus-based SLG methods in order to find out which of these paradigms is better suited for the inherently noisy domain of social media. Our experiments show that semi-automatic translations notably outperform automatic systems (reaching a macro-averaged F1-score of 0.589), and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches (whose best F1-scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre. version:1
arxiv-1610-09982 | Sentiment Analysis of Review Datasets Using Naive Bayes and K-NN Classifier | http://arxiv.org/abs/1610.09982 | id:1610.09982 author:Lopamudra Dey, Sanjay Chakraborty, Anuraag Biswas, Beepa Bose, Sweta Tiwari category:cs.IR cs.CL  published:2016-10-31 summary:The advent of Web 2.0 has led to an increase in the amount of sentimental content available in the Web. Such content is often found in social media web sites in the form of movie or product reviews, user comments, testimonials, messages in discussion forums etc. Timely discovery of the sentimental or opinionated web content has a number of advantages, the most important of all being monetization. Understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements, recommendation systems and analysis of market trends. The focus of our project is sentiment focussed web crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same. We use statistical methods to capture elements of subjective style and the sentence polarity. The paper elaborately discusses two supervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Naive Bayes and compares their overall accuracy, precisions as well as recall values. It was seen that in case of movie reviews Naive Bayes gave far better results than K-NN but for hotel reviews these algorithms gave lesser, almost same accuracies. version:1
arxiv-1610-09975 | Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition | http://arxiv.org/abs/1610.09975 | id:1610.09975 author:Hagen Soltau, Hank Liao, Hasim Sak category:cs.CL cs.LG cs.NE  published:2016-10-31 summary:We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units. version:1
arxiv-1610-09964 | Ontology Verbalization using Semantic-Refinement | http://arxiv.org/abs/1610.09964 | id:1610.09964 author:Vinu E. V, P Sreenivasa Kumar category:cs.AI cs.CL  published:2016-10-31 summary:We propose a rule-based technique to generate redundancy-free NL descriptions of OWL entities.The existing approaches which address the problem of verbalizing OWL ontologies generate NL text segments which are close to their counterpart OWL statements.Some of these approaches also perform grouping and aggregating of these NL text segments to generate a more fluent and comprehensive form of the content.Restricting our attention to description of individuals and concepts, we find that the approach currently followed in the available tools is that of determining the set of all logical conditions that are satisfied by the given individual/concept name and translate these conditions verbatim into corresponding NL descriptions.Human-understandability of such descriptions is affected by the presence of repetitions and redundancies, as they have high fidelity to their OWL representation.In the literature, no efforts had been taken to remove redundancies and repetitions at the logical-level before generating the NL descriptions of entities and we find this to be the main reason for lack of readability of the generated text.Herein, we propose a technique called semantic-refinement(SR) to generate meaningful and easily-understandable descriptions of individuals and concepts of a given OWLontology.We identify the combinations of OWL/DL constructs that lead to repetitive/redundant descriptions and propose a series of refinement rules to rewrite the conditions that are satisfied by an individual/concept in a meaning-preserving manner.The reduced set of conditions are then employed for generating NL descriptions.Our experiments show that, SR leads to significantly improved descriptions of ontology entities.We also test the effectiveness and usefulness of the the generated descriptions for the purpose of validating the ontologies and find that the proposed technique is indeed helpful in the context. version:1
arxiv-1610-08336 | The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM | http://arxiv.org/abs/1610.08336 | id:1610.08336 author:Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, Davide Scaramuzza category:cs.RO cs.CV  published:2016-10-26 summary:New vision sensors, such as the Dynamic and Active-pixel Vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called "events") and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we also provide inertial measurements and ground truth from a motion-capture system. All the data are released both as standard text files and binary files (i.e., rosbag). This paper provides an overview of the available data and details a simulator that we release open-source to create synthetic event-camera data. version:2
arxiv-1610-09915 | Complex-Valued Kernel Methods for Regression | http://arxiv.org/abs/1610.09915 | id:1610.09915 author:Rafael Boloix-Tortosa, Juan José Murillo-Fuentes, Irene Santos Velázquez, Fernando Pérez-Cruz category:stat.ML cs.LG  published:2016-10-31 summary:Usually, complex-valued RKHS are presented as an straightforward application of the real-valued case. In this paper we prove that this procedure yields a limited solution for regression. We show that another kernel, here denoted as pseudo kernel, is needed to learn any function in complex-valued fields. Accordingly, we derive a novel RKHS to include it, the widely RKHS (WRKHS). When the pseudo-kernel cancels, WRKHS reduces to complex-valued RKHS of previous approaches. We address the kernel and pseudo-kernel design, paying attention to the kernel and the pseudo-kernel being complex-valued. In the experiments included we report remarkable improvements in simple scenarios where real a imaginary parts have different similitude relations for given inputs or cases where real and imaginary parts are correlated. In the context of these novel results we revisit the problem of non-linear channel equalization, to show that the WRKHS helps to design more efficient solutions. version:1
arxiv-1610-09914 | Named Entity Recognition for Novel Types by Transfer Learning | http://arxiv.org/abs/1610.09914 | id:1610.09914 author:Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, Timothy Baldwin category:cs.CL  published:2016-10-31 summary:In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch. version:1
arxiv-1610-09908 | Joint Large-Scale Motion Estimation and Image Reconstruction | http://arxiv.org/abs/1610.09908 | id:1610.09908 author:Hendrik Dirks category:cs.CV math.OC 68U10  65K10  65M06 I.4; G.1.6; G.4  published:2016-10-31 summary:This article describes the implementation of the joint motion estimation and image reconstruction framework presented by Burger, Dirks and Sch\"onlieb and extends this framework to large-scale motion between consecutive image frames. The variational framework uses displacements between consecutive frames based on the optical flow approach to improve the image reconstruction quality on the one hand and the motion estimation quality on the other. The energy functional consists of a data-fidelity term with a general operator that connects the input sequence to the solution, it has a total variation term for the image sequence and is connected to the underlying flow using an optical flow term. Additional spatial regularity for the flow is modeled by a total variation regularizer for both components of the flow. The numerical minimization is performed in an alternating manner using primal-dual techniques. The resulting schemes are presented as pseudo-code together with a short numerical evaluation. version:1
arxiv-1610-09903 | Learning Runtime Parameters in Computer Systems with Delayed Experience Injection | http://arxiv.org/abs/1610.09903 | id:1610.09903 author:Michael Schaarschmidt, Felix Gessert, Valentin Dalibard, Eiko Yoneki category:cs.LG I.2.6; H.2.4  published:2016-10-31 summary:Learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem. This paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints. Cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous deep reinforcement learning to learn optimal cache expirations for HTTP caching in content delivery networks. To this end, we introduce a technique for asynchronous experience management called delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Evaluation results show that our approach based on normalized advantage functions and asynchronous CPU-only training outperforms a statistical estimator. version:1
arxiv-1610-09900 | Inference Compilation and Universal Probabilistic Programming | http://arxiv.org/abs/1610.09900 | id:1610.09900 author:Tuan Anh Le, Atilim Gunes Baydin, Frank Wood category:cs.AI cs.LG stat.ML 68T37  68T05 G.3; I.2.6  published:2016-10-31 summary:We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do "compilation of inference" because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference. version:1
arxiv-1610-09893 | LightRNN: Memory and Computation-Efficient Recurrent Neural Networks | http://arxiv.org/abs/1610.09893 | id:1610.09893 author:Xiang Li, Tao Qin, Jian Yang, Tie-Yan Liu category:cs.CL cs.LG  published:2016-10-31 summary:Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need $2 \sqrt{ V }$ vectors to represent a vocabulary of $ V $ unique words, which are far less than the $ V $ vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm \emph{LightRNN} to reflect its very small model size and very high training speed. version:1
arxiv-1610-09889 | Chinese Poetry Generation with Planning based Neural Network | http://arxiv.org/abs/1610.09889 | id:1610.09889 author:Zhe Wang, Wei He, Hua Wu, Haiyang Wu, Wei Li, Haifeng Wang, Enhong Chen category:cs.CL cs.AI  published:2016-10-31 summary:Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user's writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user's intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets. version:1
arxiv-1610-09887 | Depth Separation in ReLU Networks for Approximating Smooth Non-Linear Functions | http://arxiv.org/abs/1610.09887 | id:1610.09887 author:Itay Safran, Ohad Shamir category:cs.LG cs.NE stat.ML  published:2016-10-31 summary:We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on $[0,1]^d$, which can be approximated to accuracy $\epsilon$ by ReLU networks of depth and width $\mathcal{O}(\text{poly}(\log(1/\epsilon)))$, cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least $\Omega(1/\epsilon)$. version:1
arxiv-1611-00800 | Temporal Matrix Completion with Locally Linear Latent Factors for Medical Applications | http://arxiv.org/abs/1611.00800 | id:1611.00800 author:Frodo Kin Sun Chan, Andy J Ma, Pong C Yuen, Terry Cheuk-Fung Yip, Yee-Kit Tse, Vincent Wai-Sun Wong, Grace Lai-Hung Wong category:cs.LG cs.CV stat.ML  published:2016-10-31 summary:Regular medical records are useful for medical practitioners to analyze and monitor patient health status especially for those with chronic disease, but such records are usually incomplete due to unpunctuality and absence of patients. In order to resolve the missing data problem over time, tensor-based model is suggested for missing data imputation in recent papers because this approach makes use of low rank tensor assumption for highly correlated data. However, when the time intervals between records are long, the data correlation is not high along temporal direction and such assumption is not valid. To address this problem, we propose to decompose a matrix with missing data into its latent factors. Then, the locally linear constraint is imposed on these factors for matrix completion in this paper. By using a publicly available dataset and two medical datasets collected from hospital, experimental results show that the proposed algorithm achieves the best performance by comparing with the existing methods. version:1
arxiv-1610-09882 | A Survey of Brain Inspired Technologies for Engineering | http://arxiv.org/abs/1610.09882 | id:1610.09882 author:Jarryd Son, Amit Kumar Mishra category:cs.AI cs.NE  published:2016-10-31 summary:Cognitive engineering is a multi-disciplinary field and hence it is difficult to find a review article consolidating the leading developments in the field. The in-credible pace at which technology is advancing pushes the boundaries of what is achievable in cognitive engineering. There are also differing approaches to cognitive engineering brought about from the multi-disciplinary nature of the field and the vastness of possible applications. Thus research communities require more frequent reviews to keep up to date with the latest trends. In this paper we shall dis-cuss some of the approaches to cognitive engineering holistically to clarify the reasoning behind the different approaches and to highlight their strengths and weaknesses. We shall then show how developments from seemingly disjointed views could be integrated to achieve the same goal of creating cognitive machines. By reviewing the major contributions in the different fields and showing the potential for a combined approach, this work intends to assist the research community in devising more unified methods and techniques for developing cognitive machines. version:1
arxiv-1611-00228 | Application Specific Instrumentation (ASIN): A Bio-inspired Paradigm to Instrumentation using recognition before detection | http://arxiv.org/abs/1611.00228 | id:1611.00228 author:Amit Kumar Mishra category:cs.OH cs.LG  published:2016-10-31 summary:In this paper we present a new scheme for instrumentation, which has been inspired by the way small mammals sense their environment. We call this scheme Application Specific Instrumentation (ASIN). A conventional instrumentation system focuses on gathering as much information about the scene as possible. This, usually, is a generic system whose data can be used by another system to take a specific action. ASIN fuses these two steps into one. The major merit of the proposed scheme is that it uses low resolution sensors and much less computational overhead to give good performance for a highly specialised application version:1
arxiv-1610-09838 | Analysis of Nonstationary Time Series Using Locally Coupled Gaussian Processes | http://arxiv.org/abs/1610.09838 | id:1610.09838 author:Luca Ambrogioni, Eric Maris category:stat.ML  published:2016-10-31 summary:The analysis of nonstationary time series is of great importance in many scientific fields such as physics and neuroscience. In recent years, Gaussian process regression has attracted substantial attention as a robust and powerful method for analyzing time series. In this paper, we introduce a new framework for analyzing nonstationary time series using locally stationary Gaussian process analysis with parameters that are coupled through a hidden Markov model. The main advantage of this framework is that arbitrary complex nonstationary covariance functions can be obtained by combining simpler stationary building blocks whose hidden parameters can be estimated in closed-form. We demonstrate the flexibility of the method by analyzing two examples of synthetic nonstationary signals: oscillations with time varying frequency and time series with two dynamical states. Finally, we report an example application on real magnetoencephalographic measurements of brain activity. version:1
arxiv-1610-09816 | Robust Gait Recognition by Integrating Inertial and RGBD Sensors | http://arxiv.org/abs/1610.09816 | id:1610.09816 author:Qin Zou, Lihao Ni, Qian Wang, Qingquan Li, Song Wang category:cs.CV  published:2016-10-31 summary:Gait has been considered as a promising and unique biometric for person identification. Traditionally, gait data are collected using either color sensors, such as a CCD camera, depth sensors, such as a Microsoft Kinect, or inertial sensors, such as an accelerometer. However, a single type of sensors may only capture part of the dynamic gait features and make the gait recognition sensitive to complex covariate conditions, leading to fragile gait-based person identification systems. In this paper, we propose to combine all three types of sensors for gait data collection and gait recognition, which can be used for important identification applications, such as identity recognition to access a restricted building or area. We propose two new algorithms, namely EigenGait and TrajGait, to extract gait features from the inertial data and the RGBD (color and depth) data, respectively. Specifically, EigenGait extracts general gait dynamics from the accelerometer readings in the eigenspace and TrajGait extracts more detailed sub-dynamics by analyzing 3D dense trajectories. Finally, both extracted features are fed into a supervised classifier for gait recognition and person identification. Experiments on 50 subjects, with comparisons to several other state-of-the-art gait-recognition approaches, show that the proposed approach can achieve higher recognition accuracy and robustness. version:1
arxiv-1610-09799 | Experiments with POS Tagging Code-mixed Indian Social Media Text | http://arxiv.org/abs/1610.09799 | id:1610.09799 author:Prakash B. Pimpale, Raj Nath Patel category:cs.CL  published:2016-10-31 summary:This paper presents Centre for Development of Advanced Computing Mumbai's (CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For Code-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON 2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te) languages mixed with English (en). In this paper, we have described our approaches to the POS tagging techniques, we exploited for this task. Machine learning has been used to POS tag the mixed language text. For POS tagging, distributed representations of words in vector space (word2vec) for feature extraction and Log-linear models have been tried. We report our work on all three languages hi, bn, and te mixed with en. version:1
arxiv-1610-09780 | Flexible Models for Microclustering with Application to Entity Resolution | http://arxiv.org/abs/1610.09780 | id:1610.09780 author:Giacomo Zanella, Brenda Betancourt, Hanna Wallach, Jeffrey Miller, Abbas Zaidi, Rebecca C. Steorts category:stat.ME math.ST stat.AP stat.ML stat.TH  published:2016-10-31 summary:Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets. version:1
arxiv-1610-09778 | DPPred: An Effective Prediction Framework with Concise Discriminative Patterns | http://arxiv.org/abs/1610.09778 | id:1610.09778 author:Jingbo Shang, Meng Jiang, Wenzhu Tong, Jinfeng Xiao, Jian Peng, Jiawei Han category:cs.LG cs.AI  published:2016-10-31 summary:In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data. In this paper, we propose a novel Discriminative Pattern-based Prediction framework (DPPred) to accomplish the prediction tasks by taking their advantages of both effectiveness and interpretability. Specifically, DPPred adopts the concise discriminative patterns that are on the prefix paths from the root to leaf nodes in the tree-based models. DPPred selects a limited number of the useful discriminative patterns by searching for the most effective pattern combination to fit generalized linear models. Extensive experiments show that in many scenarios, DPPred provides competitive accuracy with the state-of-the-art as well as the valuable interpretability for developers and experts. In particular, taking a clinical application dataset as a case study, our DPPred outperforms the baselines by using only 40 concise discriminative patterns out of a potentially exponentially large set of patterns. version:1
arxiv-1610-09769 | Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks | http://arxiv.org/abs/1610.09769 | id:1610.09769 author:Jingbo Shang, Meng Qu, Jialu Liu, Lance M. Kaplan, Jiawei Han, Jian Peng category:cs.SI cs.LG  published:2016-10-31 summary:Most real-world data can be modeled as heterogeneous information networks (HINs) consisting of vertices of multiple types and their relationships. Search for similar vertices of the same type in large HINs, such as bibliographic networks and business-review networks, is a fundamental problem with broad applications. Although similarity search in HINs has been studied previously, most existing approaches neither explore rich semantic information embedded in the network structures nor take user's preference as a guidance. In this paper, we re-examine similarity search in HINs and propose a novel embedding-based framework. It models vertices as low-dimensional vectors to explore network structure-embedded similarity. To accommodate user preferences at defining similarity semantics, our proposed framework, ESim, accepts user-defined meta-paths as guidance to learn vertex vectors in a user-preferred embedding space. Moreover, an efficient and parallel sampling-based optimization algorithm has been developed to learn embeddings in large-scale HINs. Extensive experiments on real-world large-scale HINs demonstrate a significant improvement on the effectiveness of ESim over several state-of-the-art algorithms as well as its scalability. version:1
arxiv-1610-09766 | A New Distance Measure for Non-Identical Data with Application to Image Classification | http://arxiv.org/abs/1610.09766 | id:1610.09766 author:Muthukaruppan Swaminathan, Pankaj Kumar Yadav, Obdulio Piloto, Tobias Sjöblom, Ian Cheong category:cs.CV  published:2016-10-31 summary:Distance measures are part and parcel of many computer vision algorithms. The underlying assumption in all existing distance measures is that feature elements are independent and identically distributed. However, in real-world settings, data generally originate from heterogeneous sources even if they do possess a common data-generating mechanism. Since these sources are not identically distributed by necessity, the assumption of identical distribution is inappropriate. Here, we use statistical analysis to show that feature elements of local image descriptors are indeed non-identically distributed. To test the effect of omitting the unified distribution assumption, we created a new distance measure called the Poisson-Binomial Radius (PBR). PBR is a bin-to-bin distance which accounts for the dispersion of bin-to-bin information. PBR's performance was evaluated on twelve benchmark data sets covering six different classification and recognition applications: texture, material, leaf, scene, ear biometrics and category-level image classification. Results from these experiments demonstrate that PBR outperforms state-of-the-art distance measures for most of the data sets and achieves comparable performance on the rest, suggesting that accounting for different distributions in distance measures can improve performance in classification and recognition tasks. version:1
arxiv-1610-09730 | Active Learning from Imperfect Labelers | http://arxiv.org/abs/1610.09730 | id:1610.09730 author:Songbai Yan, Kamalika Chaudhuri, Tara Javidi category:cs.LG stat.ML  published:2016-10-30 summary:We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity. version:1
arxiv-1610-09726 | The Multi-fidelity Multi-armed Bandit | http://arxiv.org/abs/1610.09726 | id:1610.09726 author:Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, Barnabás Póczos category:cs.LG  published:2016-10-30 summary:We study a variant of the classical stochastic $K$-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available. For example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences. We formalise this task as a multi-fidelity bandit, where, at each time step, the forecaster may choose to play an arm at any one of $M$ fidelities. The highest fidelity (desired outcome) expends cost $\lambda^{(m)}$. The $m^{\text{th}}$ fidelity (an approximation) expends $\lambda^{(m)} < \lambda^{(M)}$ and returns a biased estimate of the highest fidelity. We develop MF-UCB, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations. For instance, in the above online advertising example, MF-UCB would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates. We complement this result with a lower bound and show that MF-UCB is nearly optimal under certain conditions. version:1
arxiv-1610-09722 | Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources | http://arxiv.org/abs/1610.09722 | id:1610.09722 author:Jason Naradowsky, Sebastian Riedel category:cs.CL  published:2016-10-30 summary:In order to extract event information from text, a machine reading model must learn to accurately read and interpret the ways in which that information is expressed. But it must also, as the human reader must, aggregate numerous individual value hypotheses into a single coherent global analysis, applying global constraints which reflect prior knowledge of the domain. In this work we focus on the task of extracting plane crash event information from clusters of related news articles whose labels are derived via distant supervision. Unlike previous machine reading work, we assume that while most target values will occur frequently in most clusters, they may also be missing or incorrect. We introduce a novel neural architecture to explicitly model the noisy nature of the data and to deal with these aforementioned learning issues. Our models are trained end-to-end and achieve an improvement of more than 12.1 F$_1$ over previous work, despite using far less linguistic annotation. We apply factor graph constraints to promote more coherent event analyses, with belief propagation inference formulated within the transitions of a recurrent neural network. We show this technique additionally improves maximum F$_1$ by up to 2.8 points, resulting in a relative improvement of $50\%$ over the previous state-of-the-art. version:1
arxiv-1610-09716 | Doubly Convolutional Neural Networks | http://arxiv.org/abs/1610.09716 | id:1610.09716 author:Shuangfei Zhai, Yu Cheng, Weining Lu, Zhongfei Zhang category:cs.LG  published:2016-10-30 summary:Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy. version:1
arxiv-1610-09712 | Real-Time Image Distortion Correction: Analysis and Evaluation of FPGA-Compatible Algorithms | http://arxiv.org/abs/1610.09712 | id:1610.09712 author:Paolo Di Febbo, Stefano Mattoccia, Carlo Dal Mutto category:cs.CV  published:2016-10-30 summary:Image distortion correction is a critical pre-processing step for a variety of computer vision and image processing algorithms. Standard real-time software implementations are generally not suited for direct hardware porting, so appropriated versions need to be designed in order to obtain implementations deployable on FPGAs. In this paper, hardware-compatible techniques for image distortion correction are introduced and analyzed in details. The considered solutions are compared in terms of output quality by using a geometrical-error-based approach, with particular emphasis on robustness with respect to increasing lens distortion. The required amount of hardware resources is also estimated for each considered approach. version:1
arxiv-1611-00252 | Improving a Credit Scoring Model by Incorporating Bank Statement Derived Features | http://arxiv.org/abs/1611.00252 | id:1611.00252 author:Rory P. Bunker, Wenjun Zhang, M. Asif Naeem category:cs.LG  published:2016-10-30 summary:In this paper, we investigate the extent to which features derived from bank statements provided by loan applicants, and which are not declared on an application form, can enhance a credit scoring model for a New Zealand lending company. Exploring the potential of such information to improve credit scoring models in this manner has not been studied previously. We construct a baseline model based solely on the existing scoring features obtained from the loan application form, and a second baseline model based solely on the new bank statement-derived features. A combined feature model is then created by augmenting the application form features with the new bank statement derived features. Our experimental results using ROC analysis show that a combined feature model performs better than both of the two baseline models, and show that a number of the bank statement-derived features have value in improving the credit scoring model. The target data set used for modelling was highly imbalanced, and Naive Bayes was found to be the best performing model, and outperformed a number of other classifiers commonly used in credit scoring, suggesting its potential for future use on highly imbalanced data sets. version:1
arxiv-1610-09704 | Feature-Augmented Neural Networks for Patient Note De-identification | http://arxiv.org/abs/1610.09704 | id:1610.09704 author:Ji Young Lee, Franck Dernoncourt, Ozlem Uzuner, Peter Szolovits category:cs.CL cs.NE stat.ML  published:2016-10-30 summary:Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain. version:1
arxiv-1610-09659 | Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering | http://arxiv.org/abs/1610.09659 | id:1610.09659 author:Gautier Marti, Sebastien Andler, Frank Nielsen, Philippe Donnat category:stat.ML  published:2016-10-30 summary:We propose a methodology to explore and measure the pairwise correlations that exist between variables in a dataset. The methodology leverages copulas for encoding dependence between two variables, state-of-the-art optimal transport for providing a relevant geometry to the copulas, and clustering for summarizing the main dependence patterns found between the variables. Some of the clusters centers can be used to parameterize a novel dependence coefficient which can target or forget specific dependence patterns. Finally, we illustrate and benchmark the methodology on several datasets. Code and numerical experiments are available online for reproducible research. version:1
arxiv-1610-09652 | Visual Tracking via Boolean Map Representations | http://arxiv.org/abs/1610.09652 | id:1610.09652 author:Kaihua Zhang, Qingshan Liu, Ming-Hsuan Yang category:cs.CV  published:2016-10-30 summary:In this paper, we present a simple yet effective Boolean map based representation that exploits connectivity cues for visual tracking. We describe a target object with histogram of oriented gradients and raw color features, of which each one is characterized by a set of Boolean maps generated by uniformly thresholding their values. The Boolean maps effectively encode multi-scale connectivity cues of the target with different granularities. The fine-grained Boolean maps capture spatially structural details that are effective for precise target localization while the coarse-grained ones encode global shape information that are robust to large target appearance variations. Finally, all the Boolean maps form together a robust representation that can be approximated by an explicit feature map of the intersection kernel, which is fed into a logistic regression classifier with online update, and the target location is estimated within a particle filter framework. The proposed representation scheme is computationally efficient and facilitates achieving favorable performance in terms of accuracy and robustness against the state-of-the-art tracking methods on a large benchmark dataset of 50 image sequences. version:1
arxiv-1610-09645 | Accurate Deep Representation Quantization with Gradient Snapping Layer for Similarity Search | http://arxiv.org/abs/1610.09645 | id:1610.09645 author:Shicong Liu, Hongtao Lu category:cs.CV  published:2016-10-30 summary:Recent advance of large scale similarity search involves using deeply learned representations to improve the search accuracy and use vector quantization methods to increase the search speed. However, how to learn deep representations that strongly preserve similarities between data pairs and can be accurately quantized via vector quantization remains a challenging task. Existing methods simply leverage quantization loss and similarity loss, which result in unexpectedly biased back-propagating gradients and affect the search performances. To this end, we propose a novel gradient snapping layer (GSL) to directly regularize the back-propagating gradient towards a neighboring codeword, the generated gradients are un-biased for reducing similarity loss and also propel the learned representations to be accurately quantized. Joint deep representation and vector quantization learning can be easily performed by alternatively optimize the quantization codebook and the deep neural network. The proposed framework is compatible with various existing vector quantization approaches. Experimental results demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art large scale similarity search methods. version:1
arxiv-1610-09641 | Auxiliary gradient-based sampling algorithms | http://arxiv.org/abs/1610.09641 | id:1610.09641 author:Michalis K. Titsias, Omiros Papaspiliopoulos category:stat.ML stat.CO  published:2016-10-30 summary:We introduce a new family of MCMC samplers that combine the strategic use of auxiliary variables and Gibbs sampling with simple Taylor expansions of the target density. Our approach permits the marginalisation over the auxiliary variables yielding marginal samplers, or the augmentation of the auxiliary variables, yielding auxiliary samplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and preconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special cases of the marginal samplers we propose. We prove that marginal schemes are superior in terms of asymptotic variance, but demonstrate that the acceptance ratios in auxiliary schemes can be computed an order of magnitude faster. In the context of latent Gaussian models we propose new auxiliary and marginal samplers whose implementation requires a single tuning parameter, which can be found automatically during the transient phase. Extensive experimentation shows that the increase in efficiency (measured as effective sample size per unit of computing time) relative to (optimised implementations of) state-of-the-art methods such as pCNL or elliptical slice sampling ranges from 10-fold in binary classification problems to 25-fold in log-Gaussian Cox processes to 100-fold in Gaussian process regression. We provide some insights on this remarkable improvement in terms of the way alternative samplers try to approximate the eigenvalues of the target by shrinking those of the Gaussian prior. Finally, we introduce a novel MCMC sampling scheme for jointly sampling the latent Gaussian and its hyperparameters that builds upon the auxiliary samplers. version:1
arxiv-1610-09639 | Compact Deep Convolutional Neural Networks With Coarse Pruning | http://arxiv.org/abs/1610.09639 | id:1610.09639 author:Sajid Anwar, Wonyong Sung category:cs.LG cs.NE  published:2016-10-30 summary:The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns furhter increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple and generic strategy to choose the least adversarial pruning masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a network with both granularities. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the missclassification rate of the baseline network. version:1
arxiv-1610-09625 | Discovering containment: from infants to machines | http://arxiv.org/abs/1610.09625 | id:1610.09625 author:Shimon Ullman, Nimrod Dorfman, Daniel Harari category:q-bio.NC cs.CV cs.LG  published:2016-10-30 summary:Current artificial learning systems can recognize thousands of visual categories, or play Go at a champion"s level, but cannot explain infants learning, in particular the ability to learn complex concepts without guidance, in a specific order. A notable example is the category of 'containers' and the notion of containment, one of the earliest spatial relations to be learned, starting already at 2.5 months, and preceding other common relations (e.g., support). Such spontaneous unsupervised learning stands in contrast with current highly successful computational models, which learn in a supervised manner, that is, by using large data sets of labeled examples. How can meaningful concepts be learned without guidance, and what determines the trajectory of infant learning, making some notions appear consistently earlier than others? version:1
arxiv-1610-09615 | Compressed Learning: A Deep Neural Network Approach | http://arxiv.org/abs/1610.09615 | id:1610.09615 author:Amir Adler, Michael Elad, Michael Zibulevsky category:cs.CV  published:2016-10-30 summary:Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art. version:1
arxiv-1610-09609 | Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene | http://arxiv.org/abs/1610.09609 | id:1610.09609 author:Keyu Lu, Jian Li, Xiangjing An, Hangen He category:cs.CV cs.AI cs.NE  published:2016-10-30 summary:Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter based deep network which is suitable for the object detection tasks in traffic scene. In this approach, we first decompose a object detection task into several easier local regression tasks. Then, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. To reduce the consumption of storage and computing resources, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Additionally, we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. Finally, we perform several experiments to validate the performance of our proposed approach. Experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state-of-the-art. version:1
arxiv-1610-09608 | A Theoretical Study of The Relationship Between Whole An ELM Network and Its Subnetworks | http://arxiv.org/abs/1610.09608 | id:1610.09608 author:Enmei Tu, Guanghao Zhang, Lily Rachmawati, Eshan Rajabally, Guang-Bin Huang category:cs.LG cs.NE  published:2016-10-30 summary:A biological neural network is constituted by numerous subnetworks and modules with different functionalities. For an artificial neural network, the relationship between a network and its subnetworks is also important and useful for both theoretical and algorithmic research, i.e. it can be exploited to develop incremental network training algorithm or parallel network training algorithm. In this paper we explore the relationship between an ELM neural network and its subnetworks. To the best of our knowledge, we are the first to prove a theorem that shows an ELM neural network can be scattered into subnetworks and its optimal solution can be constructed recursively by the optimal solutions of these subnetworks. Based on the theorem we also present two algorithms to train a large ELM neural network efficiently: one is a parallel network training algorithm and the other is an incremental network training algorithm. The experimental results demonstrate the usefulness of the theorem and the validity of the developed algorithms. version:1
arxiv-1610-09600 | Sparse interpretable estimators for cyclic arrival rates | http://arxiv.org/abs/1610.09600 | id:1610.09600 author:Ningyuan Chen, Donald K. K. Lee, Sahand Negahban category:stat.ML 62M15  90B22  60G55  published:2016-10-30 summary:Exploiting the fact that most arrival processes exhibit cyclic (though not necessarily periodic) behaviour, we propose a simple and computationally efficient procedure for estimating the intensity of a nonhomogeneous Poisson process. The estimator is interpretable because it is a simple sum of p sinusoids, although p and the frequency, amplitude, and phase of each wave are not known and need to be estimated. This results in a flexible specification that is suitable for use in modeling as well as in high resolution simulations. Our estimation procedure sits in between classic periodogram methods and atomic/total variation norm thresholding. A novel aspect of our approach is the use of window functions with fast decaying spectral tails, which we show improves frequency resolution as well as prevent strong frequency signals from masking weaker ones nearby. Interestingly the prolate spheriodal window that is usually considered optimal in signal processing is suboptimal for frequency recovery. Under suitable conditions, finite sample performance guarantees for our procedure can be derived that resolve open questions and expand existing results in spectral estimation literature. Preliminary simulations suggest that our procedure can potentially be extended to Cox processes which are used to model overdispersion, although a formal analysis is left for future research. We apply our procedure to arrivals data from an emergency department at an academic hospital in the United States to examine differences in arrival patterns by illness severity. version:1
arxiv-1610-09590 | A Scalable and Robust Framework for Intelligent Real-time Video Surveillance | http://arxiv.org/abs/1610.09590 | id:1610.09590 author:Shreenath Dutt, Ankita Kalra category:cs.CV cs.DC  published:2016-10-30 summary:In this paper, we present an intelligent, reliable and storage-efficient video surveillance system using Apache Storm and OpenCV. As a Storm topology, we have added multiple information extraction modules that only write important content to the disk. Our topology is extensible, capable of adding novel algorithms as per the use case without affecting the existing ones, since all the processing is independent of each other. This framework is also highly scalable and fault tolerant, which makes it a best option for organisations that need to monitor a large network of surveillance cameras. version:1
arxiv-1610-09585 | Conditional Image Synthesis With Auxiliary Classifier GANs | http://arxiv.org/abs/1610.09585 | id:1610.09585 author:Augustus Odena, Christopher Olah, Jonathon Shlens category:stat.ML cs.CV  published:2016-10-30 summary:Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. version:1
arxiv-1610-09582 | Diversity Promoting Online Sampling for Streaming Video Summarization | http://arxiv.org/abs/1610.09582 | id:1610.09582 author:Rushil Anirudh, Ahnaf Masroor, Pavan Turaga category:cs.CV  published:2016-10-29 summary:Many applications benefit from sampling algorithms where a small number of well chosen samples are used to generalize different properties of a large dataset. In this paper, we use diverse sampling for streaming video summarization. Several emerging applications support streaming video, but existing summarization algorithms need access to the entire video which requires a lot of memory and computational power. We propose a memory efficient and computationally fast, online algorithm that uses competitive learning for diverse sampling. Our algorithm is a generalization of online K-means such that the cost function reduces clustering error, while also ensuring a diverse set of samples. The diversity is measured as the volume of a convex hull around the samples. Finally, the performance of the proposed algorithm is measured against human users for 50 videos in the VSUMM dataset. The algorithm performs better than batch mode summarization, while requiring significantly lower memory and computational requirements. version:1
arxiv-1610-09565 | Sequence-to-sequence neural network models for transliteration | http://arxiv.org/abs/1610.09565 | id:1610.09565 author:Mihaela Rosca, Thomas Breuel category:cs.CL  published:2016-10-29 summary:Transliteration is a key component of machine translation systems and software internationalization. This paper demonstrates that neural sequence-to-sequence models obtain state of the art or close to state of the art results on existing datasets. In an effort to make machine transliteration accessible, we open source a new Arabic to English transliteration dataset and our trained models. version:1
arxiv-1610-09555 | TensorLy: Tensor Learning in Python | http://arxiv.org/abs/1610.09555 | id:1610.09555 author:Jean Kossaifi, Yannis Panagakis, Maja Pantic category:cs.LG  published:2016-10-29 summary:Tensor methods are gaining increasing traction in machine learning. However, there are scant to no resources available to perform tensor learning and decomposition in Python. To answer this need we developed TensorLy. TensorLy is a state of the art general purpose library for tensor learning. Written in Python, it aims at following the same standards adopted by the main projects of the Python scientific community and fully integrating with these. It allows for fast and straightforward tensor decomposition and learning and comes with exhaustive tests, thorough documentation and minimal dependencies. It can be easily extended and its BSD licence makes it suitable for both academic and commercial applications. TensorLy is available at https://github.com/tensorly/tensorly version:1
arxiv-1610-09543 | FEAST: An Automated Feature Selection Framework for Compilation Tasks | http://arxiv.org/abs/1610.09543 | id:1610.09543 author:Pai-Shun Ting, Chun-Chen Tu, Pin-Yu Chen, Ya-Yun Lo, Shin-Ming Cheng category:cs.PL cs.LG cs.SE  published:2016-10-29 summary:The success of the application of machine-learning techniques to compilation tasks can be largely attributed to the recent development and advancement of program characterization, a process that numerically or structurally quantifies a target program. While great achievements have been made in identifying key features to characterize programs, choosing a correct set of features for a specific compiler task remains an ad hoc procedure. In order to guarantee a comprehensive coverage of features, compiler engineers usually need to select excessive number of features. This, unfortunately, would potentially lead to a selection of multiple similar features, which in turn could create a new problem of bias that emphasizes certain aspects of a program's characteristics, hence reducing the accuracy and performance of the target compiler task. In this paper, we propose FEAture Selection for compilation Tasks (FEAST), an efficient and automated framework for determining the most relevant and representative features from a feature pool. Specifically, FEAST utilizes widely used statistics and machine-learning tools, including LASSO, sequential forward and backward selection, for automatic feature selection, and can in general be applied to any numerical feature set. This paper further proposes an automated approach to compiler parameter assignment for assessing the performance of FEAST. Intensive experimental results demonstrate that, under the compiler parameter assignment task, FEAST can achieve comparable results with about 18% of features that are automatically selected from the entire feature pool. We also inspect these selected features and discuss their roles in program execution. version:1
arxiv-1610-09540 | Solving Large-scale Systems of Random Quadratic Equations via Stochastic Truncated Amplitude Flow | http://arxiv.org/abs/1610.09540 | id:1610.09540 author:Gang Wang, Georgios B. Giannakis, Jie Chen category:cs.IT math.IT math.OC stat.ML  published:2016-10-29 summary:A novel approach termed \emph{stochastic truncated amplitude flow} (STAF) is developed to reconstruct an unknown $n$-dimensional real-/complex-valued signal $\bm{x}$ from $m$ `phaseless' quadratic equations of the form $\psi_i= \langle\bm{a}_i,\bm{x}\rangle $. This problem, also known as phase retrieval from magnitude-only information, is \emph{NP-hard} in general. Adopting an amplitude-based nonconvex formulation, STAF leads to an iterative solver comprising two stages: s1) Orthogonality-promoting initialization through a stochastic variance reduced gradient algorithm; and, s2) A series of iterative refinements of the initialization using stochastic truncated gradient iterations. Both stages involve a single equation per iteration, thus rendering STAF a simple, scalable, and fast approach amenable to large-scale implementations that is useful when $n$ is large. When $\{\bm{a}_i\}_{i=1}^m$ are independent Gaussian, STAF provably recovers exactly any $\bm{x}\in\mathbb{R}^n$ exponentially fast based on order of $n$ quadratic equations. STAF is also robust in the presence of additive noise of bounded support. Simulated tests involving real Gaussian $\{\bm{a}_i\}$ vectors demonstrate that STAF empirically reconstructs any $\bm{x}\in\mathbb{R}^n$ exactly from about $2.3n$ magnitude-only measurements, outperforming state-of-the-art approaches and narrowing the gap from the information-theoretic number of equations $m=2n-1$. Extensive experiments using synthetic data and real images corroborate markedly improved performance of STAF over existing alternatives. version:1
arxiv-1610-04834 | Location Sensitive Deep Convolutional Neural Networks for Segmentation of White Matter Hyperintensities | http://arxiv.org/abs/1610.04834 | id:1610.04834 author:Mohsen Ghafoorian, Nico Karssemeijer, Tom Heskes, Inge van Uden, Clara Sanchez, Geert Litjens, Frank-Erik de Leeuw, Bram van Ginneken, Elena Marchiori, Bram Platel category:cs.CV  published:2016-10-16 summary:The anatomical location of imaging features is of crucial importance for accurate diagnosis in many medical tasks. Convolutional neural networks (CNN) have had huge successes in computer vision, but they lack the natural ability to incorporate the anatomical location in their decision making process, hindering success in some medical image analysis tasks. In this paper, to integrate the anatomical location information into the network, we propose several deep CNN architectures that consider multi-scale patches or take explicit location features while training. We apply and compare the proposed architectures for segmentation of white matter hyperintensities in brain MR images on a large dataset. As a result, we observe that the CNNs that incorporate location information substantially outperform a conventional segmentation method with hand-crafted features as well as CNNs that do not integrate location information. On a test set of 46 scans, the best configuration of our networks obtained a Dice score of 0.791, compared to 0.797 for an independent human observer. Performance levels of the machine and the independent human observer were not statistically significantly different (p-value=0.17). version:2
arxiv-1610-09520 | Multi-Camera Occlusion and Sudden-Appearance-Change Detection Using Hidden Markovian Chains | http://arxiv.org/abs/1610.09520 | id:1610.09520 author:Xudong Ma category:cs.CV  published:2016-10-29 summary:This paper was originally submitted to Xinova as a response to a Request for Invention (RFI) on new event monitoring methods. In this paper, a new object tracking algorithm using multiple cameras for surveillance applications is proposed. The proposed system can detect sudden-appearance-changes and occlusions using a hidden Markovian statistical model. The experimental results confirm that our system detect the sudden-appearance changes and occlusions reliably. version:1
arxiv-1610-09516 | Finding Street Gang Members on Twitter | http://arxiv.org/abs/1610.09516 | id:1610.09516 author:Lakshika Balasuriya, Sanjaya Wijeratne, Derek Doran, Amit Sheth category:cs.SI cs.CL cs.CY cs.IR  published:2016-10-29 summary:Most street gang members use Twitter to intimidate others, to present outrageous images and statements to the world, and to share recent illegal activities. Their tweets may thus be useful to law enforcement agencies to discover clues about recent crimes or to anticipate ones that may occur. Finding these posts, however, requires a method to discover gang member Twitter profiles. This is a challenging task since gang members represent a very small population of the 320 million Twitter users. This paper studies the problem of automatically finding gang members on Twitter. It outlines a process to curate one of the largest sets of verifiable gang member profiles that have ever been studied. A review of these profiles establishes differences in the language, images, YouTube links, and emojis gang members use compared to the rest of the Twitter population. Features from this review are used to train a series of supervised classifiers. Our classifier achieves a promising F1 score with a low false positive rate. version:1
arxiv-1610-09513 | Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences | http://arxiv.org/abs/1610.09513 | id:1610.09513 author:Daniel Neil, Michael Pfeiffer, Shih-Chii Liu category:cs.LG  published:2016-10-29 summary:Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime. version:1
arxiv-1610-09512 | Contextual Decision Processes with Low Bellman Rank are PAC-Learnable | http://arxiv.org/abs/1610.09512 | id:1610.09512 author:Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire category:cs.LG stat.ML  published:2016-10-29 summary:This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new formulation, called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a new complexity measure, the Bellman Rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman Rank. The algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The algorithm uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation. version:1
arxiv-1610-07442 | Deep Multi-scale Location-aware 3D Convolutional Neural Networks for Automated Detection of Lacunes of Presumed Vascular Origin | http://arxiv.org/abs/1610.07442 | id:1610.07442 author:Mohsen Ghafoorian, Nico Karssemeijer, Tom Heskes, Mayra Bergkamp, Joost Wissink, Jiri Obels, Karlijn Keizer, Frank-Erik de Leeuw, Bram van Ginneken, Elena Marchiori, Bram Platel category:cs.CV  published:2016-10-24 summary:Lacunes of presumed vascular origin (lacunes) are associated with an increased risk of stroke, gait impairment, and dementia and are a primary imaging feature of the small vessel disease. Quantification of lacunes may be of great importance to elucidate the mechanisms behind neuro-degenerative disorders and is recommended as part of study standards for small vessel disease research. However, due to the different appearance of lacunes in various brain regions and the existence of other similar-looking structures, such as perivascular spaces, manual annotation is a difficult, elaborative and subjective task, which can potentially be greatly improved by reliable and consistent computer-aided detection (CAD) routines. In this paper, we propose an automated two-stage method using deep convolutional neural networks (CNN). We show that this method has good performance and can considerably benefit readers. We first use a fully convolutional neural network to detect initial candidates. In the second step, we employ a 3D CNN as a false positive reduction tool. As the location information is important to the analysis of candidate structures, we further equip the network with contextual information using multi-scale analysis and integration of explicit location features. We trained, validated and tested our networks on a large dataset of 1075 cases obtained from two different studies. Subsequently, we conducted an observer study with four trained observers and compared our method with them using a free-response operating characteristic analysis. Shown on a test set of 111 cases, the resulting CAD system exhibits performance similar to the trained human observers and achieves a sensitivity of 0.974 with 0.13 false positives per slice. A feasibility study also showed that a trained human observer would considerably benefit once aided by the CAD system. version:2
arxiv-1610-09498 | A MAP-MRF filter for phase-sensitive coil combination in autocalibrating partially parallel susceptibility weighted MRI | http://arxiv.org/abs/1610.09498 | id:1610.09498 author:Sreekanth Madhusoodhanan, Joseph Suresh Paul category:cs.CV  published:2016-10-29 summary:A statistical approach for combination of channel phases is developed for optimizing the Contrast-to-Noise Ratio (CNR) in Susceptibility Weighted Images (SWI) acquired using autocalibrating partially parallel techniques. The unwrapped phase images of each coil are filtered using local random field based probabilistic weights, derived using energy functions representative of noisy sensitivity and tissue information pertaining to venous structure in the individual channel phase images. The channel energy functions are obtained as functions of local image intensities, first or second order clique phase difference and a threshold scaling parameter dependent on the input noise level. Whereas the expectation of the individual energy functions with respect to the noise distribution in clique phase differences is to be maximized for optimal filtering, the expectation of tissue energy function decreases and noise energy function increases with increase in threshold scale parameter. The optimum scaling parameter is shown to occur at the point where expectations of both energy functions contribute to the largest possible extent. It is shown that implementation of the filter in the same lines as that of Iterated Conditional Modes (ICM) algorithm provides structural enhancement in the coil combined phase, with reduced noise amplification. Application to simulated and in vivo multi-channel SWI shows that CNR of combined phase obtained using MAP-MRF filter is higher as compared to that of coil combination using weighted average. version:1
arxiv-1610-09493 | Machine learning methods for accurate delineation of tumors in PET images | http://arxiv.org/abs/1610.09493 | id:1610.09493 author:Jakub Czakon, Filip Drapejkowski, Grzegorz Zurek, Piotr Giedziun, Jacek Zebrowski, Witold Dyrka category:cs.CV cs.NE  published:2016-10-29 summary:In oncology, Positron Emission Tomography imaging is widely used in diagnostics of cancer metastases, in monitoring of progress in course of the cancer treatment, and in planning radiotherapeutic interventions. Accurate and reproducible delineation of the tumor in the Positron Emission Tomography scans remains a difficult task, despite being crucial for delivering appropriate radiation dose, minimizing adverse side-effects of the therapy, and reliable evaluation of treatment. In this piece of research we attempt to solve the problem of automated delineation of the tumor using 3d implementations of the spatial distance weighted fuzzy c-means, the deep convolutional neural network and a dictionary model. The methods, in diverse ways, combine intensity and spatial information. version:1
arxiv-1610-09491 | SDP Relaxation with Randomized Rounding for Energy Disaggregation | http://arxiv.org/abs/1610.09491 | id:1610.09491 author:Kiarash Shaloudegi, András György, Csaba Szepesvári, Wilsun Xu category:cs.LG  published:2016-10-29 summary:We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method. version:1
arxiv-1610-09490 | A general multiblock method for structured variable selection | http://arxiv.org/abs/1610.09490 | id:1610.09490 author:Tommy Löfstedt, Fouad Hadj-Selem, Vincent Guillemot, Cathy Philippe, Nicolas Raymond, Edouard Duchesney, Vincent Frouin, Arthur Tenenhaus category:stat.ML  published:2016-10-29 summary:Regularised canonical correlation analysis was recently extended to more than two sets of variables by the multiblock method Regularised generalised canonical correlation analysis (RGCCA). Further, Sparse GCCA (SGCCA) was proposed to address the issue of variable selection. However, for technical reasons, the variable selection offered by SGCCA was restricted to a covariance link between the blocks (i.e., with $\tau=1$). One of the main contributions of this paper is to go beyond the covariance link and to propose an extension of SGCCA for the full RGCCA model (i.e., with $\tau\in[0, 1]$). In addition, we propose an extension of SGCCA that exploits structural relationships between variables within blocks. Specifically, we propose an algorithm that allows structured and sparsity-inducing penalties to be included in the RGCCA optimisation problem. The proposed multiblock method is illustrated on a real three-block high-grade glioma data set, where the aim is to predict the location of the brain tumours, and on a simulated data set, where the aim is to illustrate the method's ability to reconstruct the true underlying weight vectors. version:1
arxiv-1610-07830 | On the convergence rate of the three operator splitting scheme | http://arxiv.org/abs/1610.07830 | id:1610.07830 author:Fabian Pedregosa category:stat.ML math.OC  published:2016-10-25 summary:The three operator splitting scheme was recently proposed by [Davis and Yin, 2015] as a method to optimize composite objective functions with one convex smooth term and two convex (possibly non-smooth) terms for which we have access to their proximity operator. In this short note we provide an alternative proof for the sublinear and linear rate of convergence of this method. version:2
arxiv-1610-09463 | Sparse Signal Recovery for Binary Compressed Sensing by Majority Voting Neural Networks | http://arxiv.org/abs/1610.09463 | id:1610.09463 author:Daisuke Ito, Tadashi Wadayama category:cs.IT cs.LG math.IT stat.ML  published:2016-10-29 summary:In this paper, we propose majority voting neural networks for sparse signal recovery in binary compressed sensing. The majority voting neural network is composed of several independently trained feedforward neural networks employing the sigmoid function as an activation function. Our empirical study shows that a choice of a loss function used in training processes for the network is of prime importance. We found a loss function suitable for sparse signal recovery, which includes a cross entropy-like term and an $L_1$ regularized term. From the experimental results, we observed that the majority voting neural network achieves excellent recovery performance, which is approaching the optimal performance as the number of component nets grows. The simple architecture of the majority voting neural networks would be beneficial for both software and hardware implementations. version:1
arxiv-1610-09460 | Building Energy Load Forecasting using Deep Neural Networks | http://arxiv.org/abs/1610.09460 | id:1610.09460 author:Daniel L. Marino, Kasun Amarasinghe, Milos Manic category:cs.NE  published:2016-10-29 summary:Ensuring sustainability demands more efficient energy management with minimized energy wastage. Therefore, the power grid of the future should provide an unprecedented level of flexibility in energy management. To that end, intelligent decision making requires accurate predictions of future energy demand/load, both at aggregate and individual site level. Thus, energy load forecasting have received increased attention in the recent past, however has proven to be a difficult problem. This paper presents a novel energy load forecasting methodology based on Deep Neural Networks, specifically Long Short Term Memory (LSTM) algorithms. The presented work investigates two variants of the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to Sequence (S2S) architecture. Both methods were implemented on a benchmark data set of electricity consumption data from one residential customer. Both architectures where trained and tested on one hour and one-minute time-step resolution datasets. Experimental results showed that the standard LSTM failed at one-minute resolution data while performing well in one-hour resolution data. It was shown that S2S architecture performed well on both datasets. Further, it was shown that the presented methods produced comparable results with the other deep learning methods for energy forecasting in literature. version:1
arxiv-1610-09455 | Selective De-noising of Sparse-Coloured Images | http://arxiv.org/abs/1610.09455 | id:1610.09455 author:Arjun Chaudhuri category:cs.CV  published:2016-10-29 summary:Since time immemorial, noise has been a constant source of disturbance to the various entities known to mankind. Noise models of different kinds have been developed to study noise in more detailed fashion over the years. Image processing, particularly, has extensively implemented several algorithms to reduce noise in photographs and pictorial documents to alleviate the effect of noise. Images with sparse colours-lesser number of distinct colours in them-are common nowadays, especially in astronomy and astrophysics where black and white colours form the main components. Additive noise of Gaussian type is the most common form of noise to be studied and analysed in majority of communication channels, namely-satellite links, mobile base station to local cellular tower communication channel,et. al. Most of the time, we encounter images from astronomical sources being distorted with noise maximally as they travel long distance from telescopes in outer space to Earth. Considering Additive White Gaussian Noise(AWGN) to be the common noise in these long distance channels, this paper provides an insight and an algorithmic approach to pixel-specific de-noising of sparse-coloured images affected by AWGN. The paper concludes with some essential future avenues and applications of this de-noising method in industry and academia. version:1
arxiv-1610-09451 | KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics | http://arxiv.org/abs/1610.09451 | id:1610.09451 author:Evan R. Sparks, Shivaram Venkataraman, Tomer Kaftan, Michael J. Franklin, Benjamin Recht category:cs.LG cs.DC  published:2016-10-29 summary:Modern advanced analytics applications make use of machine learning techniques and contain multiple steps of domain-specific and general-purpose processing with high resource requirements. We present KeystoneML, a system that captures and optimizes the end-to-end large-scale machine learning applications for high-throughput training in a distributed environment with a high-level API. This approach offers increased ease of use and higher performance over existing systems for large scale learning. We demonstrate the effectiveness of KeystoneML in achieving high quality statistical accuracy and scalable training using real world datasets in several domains. By optimizing execution KeystoneML achieves up to 15x training throughput over unoptimized execution on a real image classification application. version:1
arxiv-1610-09428 | Beyond Exchangeability: The Chinese Voting Process | http://arxiv.org/abs/1610.09428 | id:1610.09428 author:Moontae Lee, Seok Hyun Jin, David Mimno category:cs.LG cs.IR cs.SI  published:2016-10-28 summary:Many online communities present user-contributed responses such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes. We propose the Chinese Voting Process (CVP) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases. We evaluate this model on Amazon product reviews and more than 80 StackExchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities. version:1
arxiv-1610-09420 | Dynamic matrix recovery from incomplete observations under an exact low-rank constraint | http://arxiv.org/abs/1610.09420 | id:1610.09420 author:Liangbei Xu, Mark A. Davenport category:stat.ML cs.LG  published:2016-10-28 summary:Low-rank matrix factorizations arise in a wide variety of applications -- including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the {\em matrix sensing} and {\em matrix completion} observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results. version:1
arxiv-1610-09414 | Learning Adaptive Parameter Tuning for Image Processing | http://arxiv.org/abs/1610.09414 | id:1610.09414 author:Jingming Dong, Iuri Frosio, Jan Kautz category:cs.CV  published:2016-10-28 summary:The non-stationary nature of image characteristics calls for adaptive processing, based on the local image content. We propose a simple and flexible method to learn local tuning of parameters in adaptive image processing: we extract simple local features from an image and learn the relation between these features and the optimal filtering parameters. Learning is performed by optimizing a user defined cost function (any image quality metric) on a training set. We apply our method to three classical problems (denoising, demosaicing and deblurring) and we show the effectiveness of the learned parameter modulation strategies. We also show that these strategies are consistent with theoretical results from the literature. version:1
arxiv-1610-09386 | Detecting Breast Cancer using a Compressive Sensing Unmixing Algorithm | http://arxiv.org/abs/1610.09386 | id:1610.09386 author:Richard Obermeier, Jose Angel Martinez-Lorenzo category:cs.CV math.OC  published:2016-10-28 summary:Traditional breast cancer imaging methods using microwave Nearfield Radar Imaging (NRI) seek to recover the complex permittivity of the tissues at each voxel in the imaging region. This approach is suboptimal, in that it does not directly consider the permittivity values that healthy and cancerous breast tissues typically have. In this paper, we describe a novel unmixing algorithm for detecting breast cancer. In this approach, the breast tissue is separated into three components, low water content (LWC), high water content (HWC), and cancerous tissues, and the goal of the optimization procedure is to recover the mixture proportions for each component. By utilizing this approach in a hybrid DBT / NRI system, the unmixing reconstruction process can be posed as a sparse recovery problem, such that compressive sensing (CS) techniques can be employed. A numerical analysis is performed, which demonstrates that cancerous lesions can be detected from their mixture proportion under the appropriate conditions. version:1
arxiv-1611-01511 | Algorithms for Fitting the Constrained Lasso | http://arxiv.org/abs/1611.01511 | id:1611.01511 author:Brian R. Gaines, Hua Zhou category:stat.ML stat.CO  published:2016-10-28 summary:We compare alternative computing strategies for solving the constrained lasso problem. As its name suggests, the constrained lasso extends the widely-used lasso to handle linear constraints, which allow the user to incorporate prior information into the model. In addition to quadratic programming, we employ the alternating direction method of multipliers (ADMM) and also derive an efficient solution path algorithm. Through both simulations and real data examples, we compare the different algorithms and provide practical recommendations in terms of efficiency and accuracy for various sizes of data. We also show that, for an arbitrary penalty matrix, the generalized lasso can be transformed to a constrained lasso, while the converse is not true. Thus, our methods can also be used for estimating a generalized lasso, which has wide-ranging applications. Code for implementing the algorithms is freely available in the Matlab toolbox SparseReg. version:1
arxiv-1610-09334 | Real-time Online Action Detection Forests using Spatio-temporal Contexts | http://arxiv.org/abs/1610.09334 | id:1610.09334 author:Seungryul Baek, Kwang In Kim, Tae-Kyun Kim category:cs.CV  published:2016-10-28 summary:Online action detection (OAD) is challenging since 1) robust yet computationally expensive features cannot be straightforwardly used due to the real-time processing requirements and 2) the localization and classification of actions have to be performed even before they are fully observed. We propose a new random forest (RF)-based online action detection framework that addresses these challenges. Our algorithm uses computationally efficient skeletal joint features. High accuracy is achieved by using robust convolutional neural network (CNN)-based features which are extracted from the raw RGBD images, plus the temporal relationships between the current frame of interest, and the past and future frames. While these high-quality features are not available in real-time testing scenario, we demonstrate that they can be effectively exploited in training RF classifiers: We use these spatio-temporal contexts to craft RF's new split functions improving RFs' leaf node statistics. Experiments with challenging MSRAction3D, G3D, and OAD datasets demonstrate that our algorithm significantly improves the accuracy over the state-of-the-art online action detection algorithms while achieving the real-time efficiency of existing skeleton-based RF classifiers. version:1
arxiv-1610-09333 | Word Embeddings for the Construction Domain | http://arxiv.org/abs/1610.09333 | id:1610.09333 author:Antoine J. -P. Tixier, Michalis Vazirgiannis, Matthew R. Hallowell category:cs.CL  published:2016-10-28 summary:We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful construction-specific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work. version:1
arxiv-1610-09300 | Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods | http://arxiv.org/abs/1610.09300 | id:1610.09300 author:Antoine Gautier, Quynh Nguyen, Matthias Hein category:cs.LG math.OC stat.ML  published:2016-10-28 summary:The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets. version:1
arxiv-1610-09278 | The TUM LapChole dataset for the M2CAI 2016 workflow challenge | http://arxiv.org/abs/1610.09278 | id:1610.09278 author:Ralf Stauder, Daniel Ostler, Michael Kranzfelder, Sebastian Koller, Hubertus Feußner, Nassir Navab category:cs.CV  published:2016-10-28 summary:In this technical report we present our collected dataset of laparoscopic cholecystectomies (LapChole). Laparoscopic videos of a total of 20 surgeries were recorded and annotated with surgical phase labels, of which 15 were randomly pre-determined as training data, while the remaining 5 videos are selected as test data. This dataset was later included as part of the M2CAI 2016 workflow detection challenge during MICCAI 2016 in Athens. version:1
arxiv-1611-02730 | Robust Cardiac Motion Estimation using Ultrafast Ultrasound Data: A Low-Rank-Topology-Preserving Approach | http://arxiv.org/abs/1611.02730 | id:1611.02730 author:Angelica I. Aviles, Thomas Widlak, Alicia Casals, Maartje M. Nillesen, Habib Ammari category:cs.CV  published:2016-10-26 summary:Cardiac motion estimation is an important diagnostic tool to detect heart diseases and it has been explored with modalities such as MRI and conventional ultrasound (US) sequences. US cardiac motion estimation still presents challenges because of the complex motion patterns and the presence of noise. In this work, we propose a novel approach to estimate the cardiac motion using ultrafast ultrasound data. -- Our solution is based on a variational formulation characterized by the L2-regularized class. The displacement is represented by a lattice of b-splines and we ensure robustness by applying a maximum likelihood type estimator. While this is an important part of our solution, the main highlight of this paper is to combine a low-rank data representation with topology preservation. Low-rank data representation (achieved by finding the k-dominant singular values of a Casorati Matrix arranged from the data sequence) speeds up the global solution and achieves noise reduction. On the other hand, topology preservation (achieved by monitoring the Jacobian determinant) allows to radically rule out distortions while carefully controlling the size of allowed expansions and contractions. Our variational approach is carried out on a realistic dataset as well as on a simulated one. We demonstrate how our proposed variational solution deals with complex deformations through careful numerical experiments. While maintaining the accuracy of the solution, the low-rank preprocessing is shown to speed up the convergence of the variational problem. Beyond cardiac motion estimation, our approach is promising for the analysis of other organs that experience motion. version:1
arxiv-1611-03109 | Energy-efficient Machine Learning in Silicon: A Communications-inspired Approach | http://arxiv.org/abs/1611.03109 | id:1611.03109 author:Naresh R. Shanbhag category:cs.LG cs.AR  published:2016-10-25 summary:This position paper advocates a communications-inspired approach to the design of machine learning systems on energy-constrained embedded `always-on' platforms. The communications-inspired approach has two versions - 1) a deterministic version where existing low-power communication IC design methods are repurposed, and 2) a stochastic version referred to as Shannon-inspired statistical information processing employing information-based metrics, statistical error compensation (SEC), and retraining-based methods to implement ML systems on stochastic circuit/device fabrics operating at the limits of energy-efficiency. The communications-inspired approach has the potential to fully leverage the opportunities afforded by ML algorithms and applications in order to address the challenges inherent in their deployment on energy-constrained platforms. version:1
arxiv-1611-02174 | Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation | http://arxiv.org/abs/1611.02174 | id:1611.02174 author:Yiyi Liao, Lichao Huang, Yue Wang, Sarath Kodagoda, Yinan Yu, Yong Liu category:cs.CV cs.RO  published:2016-10-17 summary:Many standard robotic platforms are equipped with at least a fixed 2D laser range finder and a monocular camera. Although those platforms do not have sensors for 3D depth sensing capability, knowledge of depth is an essential part in many robotics activities. Therefore, recently, there is an increasing interest in depth estimation using monocular images. As this task is inherently ambiguous, the data-driven estimated depth might be unreliable in robotics applications. In this paper, we have attempted to improve the precision of monocular depth estimation by introducing 2D planar observation from the remaining laser range finder without extra cost. Specifically, we construct a dense reference map from the sparse laser range data, redefining the depth estimation task as estimating the distance between the real and the reference depth. To solve the problem, we construct a novel residual of residual neural network, and tightly combine the classification and regression losses for continuous depth estimation. Experimental results suggest that our method achieves considerable promotion compared to the state-of-the-art methods on both NYUD2 and KITTI, validating the effectiveness of our method on leveraging the additional sensory information. We further demonstrate the potential usage of our method in obstacle avoidance where our methodology provides comprehensive depth information compared to the solution using monocular camera or 2D laser range finder alone. version:1
arxiv-1611-02302 | Quantum spectral analysis: frequency at time | http://arxiv.org/abs/1611.02302 | id:1611.02302 author:Mario Mastriani category:cs.CV  published:2016-10-11 summary:A quantum time-dependent spectrum analysis, or simply, quantum spectral analysis (QuSA) is presented in this work, and it is based on Schrodinger equation, which is a partial differential equation that describes how the quantum state of a non-relativistic physical system changes with time. In classic world is named frequency at time (FAT), which is presented here in opposition and as a complement of traditional spectral analysis frequency-dependent based on Fourier theory. Besides, FAT is a metric, which assesses the impact of the flanks of a signal on its frequency spectrum, which is not taken into account by Fourier theory and even less in real time. Even more, and unlike all derived tools from Fourier Theory (i.e., continuous, discrete, fast, short-time, fractional and quantum Fourier Transform, as well as, Gabor) FAT has the following advantages: a) compact support with excellent energy output treatment, b) low computational cost, O(N) for signals and O(N2) for images, c) it does not have phase uncertainties (indeterminate phase for magnitude = 0) as Discrete and Fast Fourier Transform (DFT, FFT, respectively), d) among others. In fact, FAT constitutes one side of a triangle (which from now on is closed) and it consists of the original signal in time, spectral analysis based on Fourier Theory and FAT. Thus a toolbox is completed, which it is essential for all applications of Digital Signal Processing (DSP) and Digital Image Processing (DIP); and, even, in the latter, FAT allows edge detection (which is called flank detection in case of signals), denoising, despeckling, compression, and superresolution of still images. Such applications include signals intelligence and imagery intelligence. On the other hand, we will present other DIP tools, which are also derived from the Schrodinger equation. version:1
arxiv-1611-05788 | Data Science in Service of Performing Arts: Applying Machine Learning to Predicting Audience Preferences | http://arxiv.org/abs/1611.05788 | id:1611.05788 author:Jacob Abernethy, Cyrus Anderson, Alex Chojnacki, Chengyu Dai, John Dryden, Eric Schwartz, Wenbo Shen, Jonathan Stroud, Laura Wendlandt, Sheng Yang, Daniel Zhang category:stat.AP cs.DB cs.LG  published:2016-09-30 summary:Performing arts organizations aim to enrich their communities through the arts. To do this, they strive to match their performance offerings to the taste of those communities. Success relies on understanding audience preference and predicting their behavior. Similar to most e-commerce or digital entertainment firms, arts presenters need to recommend the right performance to the right customer at the right time. As part of the Michigan Data Science Team (MDST), we partnered with the University Musical Society (UMS), a non-profit performing arts presenter housed in the University of Michigan, Ann Arbor. We are providing UMS with analysis and business intelligence, utilizing historical individual-level sales data. We built a recommendation system based on collaborative filtering, gaining insights into the artistic preferences of customers, along with the similarities between performances. To better understand audience behavior, we used statistical methods from customer-base analysis. We characterized customer heterogeneity via segmentation, and we modeled customer cohorts to understand and predict ticket purchasing patterns. Finally, we combined statistical modeling with natural language processing (NLP) to explore the impact of wording in program descriptions. These ongoing efforts provide a platform to launch targeted marketing campaigns, helping UMS carry out its mission by allocating its resources more efficiently. Celebrating its 138th season, UMS is a 2014 recipient of the National Medal of Arts, and it continues to enrich communities by connecting world-renowned artists with diverse audiences, especially students in their formative years. We aim to contribute to that mission through data science and customer analytics. version:1
arxiv-1610-04261 | Improved phase-unwrapping method using geometric constraints | http://arxiv.org/abs/1610.04261 | id:1610.04261 author:Guangliang Du, Minmin Wang, Canlin Zhou, Shuchun Si, Hui Li, Zhenkun Lei, Yanjie Li category:cs.CV  published:2016-09-28 summary:Conventional dual-frequency fringe projection algorithm often suffers from phase unwrapping failure when the frequency ratio between the high frequency and the low one is too large. Zhang et.al. proposed an enhanced two-frequency phase-shifting method to use geometric constraints of digital fringe projection(DFP) to reduce the noise impact due to the large frequency ratio. However, this method needs to calibrate the DFP system and calculate the minimum phase map at the nearest position from the camera perspective, these procedures are are relatively complex and more time-cosuming. In this paper, we proposed an improved method, which eliminates the system calibration and determination in Zhang's method,meanwhile does not need to use the low frequency fringe pattern. In the proposed method,we only need a set of high frequency fringe patterns to measure the object after the high frequency is directly estimated by the experiment. Thus the proposed method can simplify the procedure and improve the speed. Finally, the experimental evaluation is conducted to prove the validity of the proposed method.The results demonstrate that the proposed method can overcome the main disadvantages encountered by Zhang's method. version:1
arxiv-1609-05521 | Playing FPS Games with Deep Reinforcement Learning | http://arxiv.org/abs/1609.05521 | id:1609.05521 author:Guillaume Lample, Devendra Singh Chaplot category:cs.AI cs.LG  published:2016-09-18 summary:Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios. version:1
arxiv-1610-05267 | Rule Extraction Algorithm for Deep Neural Networks: A Review | http://arxiv.org/abs/1610.05267 | id:1610.05267 author:Tameru Hailesilassie category:cs.CV  published:2016-09-16 summary:Despite the highest classification accuracy in wide varieties of application areas, artificial neural network has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of neural network in data mining and decision system. This drawback is the reason why researchers have proposed many rule extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN) is achieving a profound result over the standard neural network for classification and recognition problems. It is a hot machine learning area proven both useful and innovative. This paper has thoroughly reviewed various rule extraction algorithms, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these algorithms based on the neural network structure with which the algorithm is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction algorithm from DNN. version:1
arxiv-1610-08495 | Adaptive matching pursuit for sparse signal recovery | http://arxiv.org/abs/1610.08495 | id:1610.08495 author:Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga category:cs.LG stat.ML  published:2016-09-12 summary:Spike and Slab priors have been of much recent interest in signal processing as a means of inducing sparsity in Bayesian inference. Applications domains that benefit from the use of these priors include sparse recovery, regression and classification. It is well-known that solving for the sparse coefficient vector to maximize these priors results in a hard non-convex and mixed integer programming problem. Most existing solutions to this optimization problem either involve simplifying assumptions/relaxations or are computationally expensive. We propose a new greedy and adaptive matching pursuit (AMP) algorithm to directly solve this hard problem. Essentially, in each step of the algorithm, the set of active elements would be updated by either adding or removing one index, whichever results in better improvement. In addition, the intermediate steps of the algorithm are calculated via an inexpensive Cholesky decomposition which makes the algorithm much faster. Results on simulated data sets as well as real-world image recovery challenges confirm the benefits of the proposed AMP, particularly in providing a superior cost-quality trade-off over existing alternatives. version:1
arxiv-1611-03341 | Fast Algorithm of High-resolution Microwave Imaging Using the Non-parametric Generalized Reflectivity Model | http://arxiv.org/abs/1611.03341 | id:1611.03341 author:Long Gang Wang, Lianlin Li, Tie Jun Cui category:cs.CV  published:2016-09-12 summary:This paper presents an efficient algorithm of high-resolution microwave imaging based on the concept of generalized reflectivity. The contribution made in this paper is two-fold. We introduce the concept of non-parametric generalized reflectivity (GR, for short) as a function of operational frequencies and view angles, etc. The GR extends the conventional Born-based imaging model, i.e., single-scattering model, into that accounting for more realistic interaction between the electromagnetic wavefield and imaged scene. Afterwards, the GR-based microwave imaging is formulated in the convex of sparsity-regularized optimization. Typically, the sparsity-regularized optimization requires the implementation of iterative strategy, which is computationally expensive, especially for large-scale problems. To break this bottleneck, we convert the imaging problem into the problem of physics-driven image processing by introducing a dual transformation. Moreover, this image processing is performed over overlapping patches, which can be efficiently solved in the parallel or distributed manner. In this way, the proposed high-resolution imaging methodology could be applicable to large-scale microwave imaging problems. Selected simulation results are provided to demonstrate the state-of-art performance of proposed methodology. version:1
arxiv-1610-01502 | Template shape estimation: correcting an asymptotic bias | http://arxiv.org/abs/1610.01502 | id:1610.01502 author:Nina Miolane, Susan Holmes, Xavier Pennec category:cs.CV math.DG  published:2016-09-06 summary:We use tools from geometric statistics to analyze the usual estimation procedure of a template shape. This applies to shapes from landmarks, curves, surfaces, images etc. We demonstrate the asymptotic bias of the template shape estimation using the stratified geometry of the shape space. We give a Taylor expansion of the bias with respect to a parameter $\sigma$ describing the measurement error on the data. We propose two bootstrap procedures that quantify the bias and correct it, if needed. They are applicable for any type of shape data. We give a rule of thumb to provide intuition on whether the bias has to be corrected. This exhibits the parameters that control the bias' magnitude. We illustrate our results on simulated and real shape data. version:1
arxiv-1611-00591 | Deep Neural Networks for HDR imaging | http://arxiv.org/abs/1611.00591 | id:1611.00591 author:Kshiteej Sheth category:cs.CV cs.LG cs.NE  published:2016-09-04 summary:We propose novel methods of solving two tasks using Convolutional Neural Networks, firstly the task of generating HDR map of a static scene using differently exposed LDR images of the scene captured using conventional cameras and secondly the task of finding an optimal tone mapping operator that would give a better score on the TMQI metric compared to the existing methods. We quantitatively show the performance of our networks and illustrate the cases where our networks performs good as well as bad. version:1
arxiv-1610-01857 | Go With the Flow, on Jupiter and Snow. Coherence From Video Data without Trajectories | http://arxiv.org/abs/1610.01857 | id:1610.01857 author:Abd AlRahman AlMomani, Erik M. Bollt category:physics.data-an stat.ML  published:2016-08-25 summary:Viewing a data set such as the clouds of Jupiter, coherence is readily apparent to human observers, especially the Great Red Spot, but also other great storms and persistent structures. There are now many different definitions and perspectives mathematically describing coherent structures, but we will take an image processing perspective here. We describe an image processing perspective inference of coherent sets from a fluidic system directly from image data, without attempting to first model underlying flow fields, related to a concept in image processing called motion tracking. In contrast to standard spectral methods for image processing which are generally related to a symmetric affinity matrix, leading to standard spectral graph theory, we need a not symmetric affinity which arises naturally from the underlying arrow of time. We develop an anisotropic, directed diffusion operator corresponding to flow on a directed graph, from a directed affinity matrix developed with coherence in mind, and corresponding spectral graph theory from the graph Laplacian. Our examples will include partitioning the weather and cloud structures of Jupiter, and a local to Potsdam, N.Y. lake-effect snow event on Earth, as well as the the benchmark test double-gyre system. version:1
