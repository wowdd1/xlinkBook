arxiv-1207-2714 | Clustering based approach extracting collocations | http://arxiv.org/abs/1207.2714 | id:1207.2714 author:Mohamed Achraf Ben Mohamed, Mounir Zrigui, Mohsen Maraoui category:cs.CL  published:2012-07-11 summary:The following study presents a collocation extraction approach based on clustering technique. This study uses a combination of several classical measures which cover all aspects of a given corpus then it suggests separating bigrams found in the corpus in several disjoint groups according to the probability of presence of collocations. This will allow excluding groups where the presence of collocations is very unlikely and thus reducing in a meaningful way the search space. version:1
arxiv-1207-2697 | Genetic agent approach for improving on-the-fly web map generalization | http://arxiv.org/abs/1207.2697 | id:1207.2697 author:Brahim lejdel, Okba kazar category:cs.MA cs.CG cs.NE  published:2012-07-11 summary:The utilization of web mapping becomes increasingly important in the domain of cartography. Users want access to spatial data on the web specific to their needs. For this reason, different approaches were appeared for generating on-the-fly the maps demanded by users, but those not suffice for guide a flexible and efficient process. Thus, new approach must be developed for improving this process according to the user needs. This work focuses on defining a new strategy which improves on-the-fly map generalization process and resolves the spatial conflicts. This approach uses the multiple representation and cartographic generalization. The map generalization process is based on the implementation of multi- agent system where each agent was equipped with a genetic patrimony. version:1
arxiv-1207-4172 | Variational Chernoff Bounds for Graphical Models | http://arxiv.org/abs/1207.4172 | id:1207.4172 author:Pradeep Ravikumar, John Lafferty category:cs.LG stat.ML  published:2012-07-11 summary:Recent research has made significant progress on the problem of bounding log partition functions for exponential family graphical models. Such bounds have associated dual parameters that are often used as heuristic estimates of the marginal probabilities required in inference and learning. However these variational estimates do not give rigorous bounds on marginal probabilities, nor do they give estimates for probabilities of more general events than simple marginals. In this paper we build on this recent work by deriving rigorous upper and lower bounds on event probabilities for graphical models. Our approach is based on the use of generalized Chernoff bounds to express bounds on event probabilities in terms of convex optimization problems; these optimization problems, in turn, require estimates of generalized log partition functions. Simulations indicate that this technique can result in useful, rigorous bounds to complement the heuristic variational estimates, with comparable computational cost. version:1
arxiv-1207-4169 | The Author-Topic Model for Authors and Documents | http://arxiv.org/abs/1207.4169 | id:1207.4169 author:Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, Padhraic Smyth category:cs.IR cs.LG stat.ML  published:2012-07-11 summary:We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output. version:1
arxiv-1207-4167 | Predictive State Representations: A New Theory for Modeling Dynamical Systems | http://arxiv.org/abs/1207.4167 | id:1207.4167 author:Satinder Singh, Michael James, Matthew Rudary category:cs.AI cs.LG  published:2012-07-11 summary:Modeling dynamical systems, both for control purposes and to make predictions about their behavior, is ubiquitous in science and engineering. Predictive state representations (PSRs) are a recently introduced class of models for discrete-time dynamical systems. The key idea behind PSRs and the closely related OOMs (Jaeger's observable operator models) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system. This makes PSRs rather different from history-based models such as nth-order Markov models and hidden-state-based models such as HMMs and POMDPs. We introduce an interesting construct, the systemdynamics matrix, and show how PSRs can be derived simply from it. We also use this construct to show formally that PSRs are more general than both nth-order Markov models and HMMs/POMDPs. Finally, we discuss the main difference between PSRs and OOMs and conclude with directions for future work. version:1
arxiv-1207-4164 | Factored Latent Analysis for far-field tracking data | http://arxiv.org/abs/1207.4164 | id:1207.4164 author:Chris Stauffer category:cs.LG stat.ML  published:2012-07-11 summary:This paper uses Factored Latent Analysis (FLA) to learn a factorized, segmental representation for observations of tracked objects over time. Factored Latent Analysis is latent class analysis in which the observation space is subdivided and each aspect of the original space is represented by a separate latent class model. One could simply treat these factors as completely independent and ignore their interdependencies or one could concatenate them together and attempt to learn latent class structure for the complete observation space. Alternatively, FLA allows the interdependencies to be exploited in estimating an effective model, which is also capable of representing a factored latent state. In this paper, FLA is used to learn a set of factored latent classes to represent different modalities of observations of tracked objects. Different characteristics of the state of tracked objects are each represented by separate latent class models, including normalized size, normalized speed, normalized direction, and position. This model also enables effective temporal segmentation of these sequences. This method is data-driven, unsupervised using only pairwise observation statistics. This data-driven and unsupervised activity classi- fication technique exhibits good performance in multiple challenging environments. version:1
arxiv-1207-4158 | On the Choice of Regions for Generalized Belief Propagation | http://arxiv.org/abs/1207.4158 | id:1207.4158 author:Max Welling category:cs.AI cs.LG  published:2012-07-11 summary:Generalized belief propagation (GBP) has proven to be a promising technique for approximate inference tasks in AI and machine learning. However, the choice of a good set of clusters to be used in GBP has remained more of an art then a science until this day. This paper proposes a sequential approach to adding new clusters of nodes and their interactions (i.e. "regions") to the approximation. We first review and analyze the recently introduced region graphs and find that three kinds of operations ("split", "merge" and "death") leave the free energy and (under some conditions) the fixed points of GBP invariant. This leads to the notion of "weakly irreducible" regions as the natural candidates to be added to the approximation. Computational complexity of the GBP algorithm is controlled by restricting attention to regions with small "region-width". Combining the above with an efficient (i.e. local in the graph) measure to predict the improved accuracy of GBP leads to the sequential "region pursuit" algorithm for adding new regions bottom-up to the region graph. Experiments show that this algorithm can indeed perform close to optimally. version:1
arxiv-1207-4157 | An Integrated, Conditional Model of Information Extraction and Coreference with Applications to Citation Matching | http://arxiv.org/abs/1207.4157 | id:1207.4157 author:Ben Wellner, Andrew McCallum, Fuchun Peng, Michael Hay category:cs.LG cs.DL cs.IR stat.ML  published:2012-07-11 summary:Although information extraction and coreference resolution appear together in many applications, most current systems perform them as ndependent steps. This paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models. We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning. On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields. version:1
arxiv-1207-4156 | Graph partition strategies for generalized mean field inference | http://arxiv.org/abs/1207.4156 | id:1207.4156 author:Eric P. Xing, Michael I. Jordan, Stuart Russell category:cs.LG stat.ML  published:2012-07-11 summary:An autonomous variational inference algorithm for arbitrary graphical models requires the ability to optimize variational approximations over the space of model parameters as well as over the choice of tractable families used for the variational approximation. In this paper, we present a novel combination of graph partitioning algorithms with a generalized mean field (GMF) inference algorithm. This combination optimizes over disjoint clustering of variables and performs inference using those clusters. We provide a formal analysis of the relationship between the graph cut and the GMF approximation, and explore several graph partition strategies empirically. Our empirical results provide rather clear support for a weighted version of MinCut as a useful clustering algorithm for GMF inference, which is consistent with the implications from the formal analysis. version:1
arxiv-1207-4155 | Similarity-Driven Cluster Merging Method for Unsupervised Fuzzy Clustering | http://arxiv.org/abs/1207.4155 | id:1207.4155 author:Xuejian Xiong, Kap Chan, Kian Lee Tan category:cs.LG stat.ML  published:2012-07-11 summary:In this paper, a similarity-driven cluster merging method is proposed for unsuper-vised fuzzy clustering. The cluster merging method is used to resolve the problem of cluster validation. Starting with an overspecified number of clusters in the data, pairs of similar clusters are merged based on the proposed similarity-driven cluster merging criterion. The similarity between clusters is calculated by a fuzzy cluster similarity matrix, while an adaptive threshold is used for merging. In addition, a modified generalized ob- jective function is used for prototype-based fuzzy clustering. The function includes the p-norm distance measure as well as principal components of the clusters. The number of the principal components is determined automatically from the data being clustered. The properties of this unsupervised fuzzy clustering algorithm are illustrated by several experiments. version:1
arxiv-1207-4152 | Maximum Entropy for Collaborative Filtering | http://arxiv.org/abs/1207.4152 | id:1207.4152 author:Lawrence Zitnick, Takeo Kanade category:cs.IR cs.LG  published:2012-07-11 summary:Within the task of collaborative filtering two challenges for computing conditional probabilities exist. First, the amount of training data available is typically sparse with respect to the size of the domain. Thus, support for higher-order interactions is generally not present. Second, the variables that we are conditioning upon vary for each query. That is, users label different variables during each query. For this reason, there is no consistent input to output mapping. To address these problems we purpose a maximum entropy approach using a non-standard measure of entropy. This approach can be simplified to solving a set of linear equations that can be efficiently solved. version:1
arxiv-1207-4151 | PAC-learning bounded tree-width Graphical Models | http://arxiv.org/abs/1207.4151 | id:1207.4151 author:Mukund Narasimhan, Jeff A. Bilmes category:cs.LG cs.DS stat.ML  published:2012-07-11 summary:We show that the class of strongly connected graphical models with treewidth at most k can be properly efficiently PAC-learnt with respect to the Kullback-Leibler Divergence. Previous approaches to this problem, such as those of Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by reducing it to a combinatorial optimization problem. However, for k > 1, this problem is NP-complete ([15]), and so unless P=NP, these approaches will take exponential amounts of time. Our approach differs significantly from these, in that it first attempts to find approximate conditional independencies by solving (polynomially many) submodular optimization problems, and then using a dynamic programming formulation to combine the approximate conditional independence information to derive a graphical model with underlying graph of the tree-width specified. This gives us an efficient (polynomial time in the number of random variables) PAC-learning algorithm which requires only polynomial number of samples of the true distribution, and only polynomial running time. version:1
arxiv-1207-4149 | From Fields to Trees | http://arxiv.org/abs/1207.4149 | id:1207.4149 author:Firas Hamze, Nando de Freitas category:stat.CO cs.LG  published:2012-07-11 summary:We present new MCMC algorithms for computing the posterior distributions and expectations of the unknown variables in undirected graphical models with regular structure. For demonstration purposes, we focus on Markov Random Fields (MRFs). By partitioning the MRFs into non-overlapping trees, it is possible to compute the posterior distribution of a particular tree exactly by conditioning on the remaining tree. These exact solutions allow us to construct efficient blocked and Rao-Blackwellised MCMC algorithms. We show empirically that tree sampling is considerably more efficient than other partitioned sampling schemes and the naive Gibbs sampler, even in cases where loopy belief propagation fails to converge. We prove that tree sampling exhibits lower variance than the naive Gibbs sampler and other naive partitioning schemes using the theoretical measure of maximal correlation. We also construct new information theory tools for comparing different MCMC schemes and show that, under these, tree sampling is more efficient. version:1
arxiv-1207-4148 | Dynamical Systems Trees | http://arxiv.org/abs/1207.4148 | id:1207.4148 author:Andrew Howard, Tony S. Jebara category:cs.LG stat.ML  published:2012-07-11 summary:We propose dynamical systems trees (DSTs) as a flexible class of models for describing multiple processes that interact via a hierarchy of aggregating parent chains. DSTs extend Kalman filters, hidden Markov models and nonlinear dynamical systems to an interactive group scenario. Various individual processes interact as communities and sub-communities in a tree structure that is unrolled in time. To accommodate nonlinear temporal activity, each individual leaf process is modeled as a dynamical system containing discrete and/or continuous hidden states with discrete and/or Gaussian emissions. Subsequent higher level parent processes act like hidden Markov models and mediate the interaction between leaf processes or between other parent processes in the hierarchy. Aggregator chains are parents of child processes that they combine and mediate, yielding a compact overall parameterization. We provide tractable inference and learning algorithms for arbitrary DST topologies via an efficient structured mean-field algorithm. The diverse applicability of DSTs is demonstrated by experiments on gene expression data and by modeling group behavior in the setting of an American football game. version:1
arxiv-1207-4146 | A Bayesian Approach toward Active Learning for Collaborative Filtering | http://arxiv.org/abs/1207.4146 | id:1207.4146 author:Rong Jin, Luo Si category:cs.LG cs.IR stat.ML  published:2012-07-11 summary:Collaborative filtering is a useful technique for exploiting the preference patterns of a group of users to predict the utility of items for the active user. In general, the performance of collaborative filtering depends on the number of rated examples given by the active user. The more the number of rated examples given by the active user, the more accurate the predicted ratings will be. Active learning provides an effective way to acquire the most informative rated examples from active users. Previous work on active learning for collaborative filtering only considers the expected loss function based on the estimated model, which can be misleading when the estimated model is inaccurate. This paper takes one step further by taking into account of the posterior distribution of the estimated model, which results in more robust active learning algorithm. Empirical studies with datasets of movie ratings show that when the number of ratings from the active user is restricted to be small, active learning methods only based on the estimated model don't perform well while the active learning method using the model distribution achieves substantially better performance. version:1
arxiv-1207-4144 | A Generative Bayesian Model for Aggregating Experts' Probabilities | http://arxiv.org/abs/1207.4144 | id:1207.4144 author:Joseph Kahn category:cs.LG stat.ML  published:2012-07-11 summary:In order to improve forecasts, a decisionmaker often combines probabilities given by various sources, such as human experts and machine learning classifiers. When few training data are available, aggregation can be improved by incorporating prior knowledge about the event being forecasted and about salient properties of the experts. To this end, we develop a generative Bayesian aggregation model for probabilistic classi cation. The model includes an event-specific prior, measures of individual experts' bias, calibration, accuracy, and a measure of dependence betweeen experts. Rather than require absolute measures, we show that aggregation may be expressed in terms of relative accuracy between experts. The model results in a weighted logarithmic opinion pool (LogOps) that satis es consistency criteria such as the external Bayesian property. We derive analytic solutions for independent and for exchangeable experts. Empirical tests demonstrate the model's use, comparing its accuracy with other aggregation methods. version:1
arxiv-1207-4142 | Conditional Chow-Liu Tree Structures for Modeling Discrete-Valued Vector Time Series | http://arxiv.org/abs/1207.4142 | id:1207.4142 author:Sergey Kirshner, Padhraic Smyth, Andrew Robertson category:cs.LG stat.ML  published:2012-07-11 summary:We consider the problem of modeling discrete-valued vector time series data using extensions of Chow-Liu tree models to capture both dependencies across time and dependencies across variables. Conditional Chow-Liu tree models are introduced, as an extension to standard Chow-Liu trees, for modeling conditional rather than joint densities. We describe learning algorithms for such models and show how they can be used to learn parsimonious representations for the output distributions in hidden Markov models. These models are applied to the important problem of simulating and forecasting daily precipitation occurrence for networks of rain stations. To demonstrate the effectiveness of the models, we compare their performance versus a number of alternatives using historical precipitation data from Southwestern Australia and the Western United States. We illustrate how the structure and parameters of the models can be used to provide an improved meteorological interpretation of such data. version:1
arxiv-1207-4139 | An Extended Cencov-Campbell Characterization of Conditional Information Geometry | http://arxiv.org/abs/1207.4139 | id:1207.4139 author:Guy Lebanon category:cs.LG stat.ML  published:2012-07-11 summary:We formulate and prove an axiomatic characterization of conditional information geometry, for both the normalized and the nonnormalized cases. This characterization extends the axiomatic derivation of the Fisher geometry by Cencov and Campbell to the cone of positive conditional models, and as a special case to the manifold of conditional distributions. Due to the close connection between the conditional I-divergence and the product Fisher information metric the characterization provides a new axiomatic interpretation of the primal problems underlying logistic regression and AdaBoost. version:1
arxiv-1207-4138 | Active Model Selection | http://arxiv.org/abs/1207.4138 | id:1207.4138 author:Omid Madani, Daniel J. Lizotte, Russell Greiner category:cs.LG stat.ML  published:2012-07-11 summary:Classical learning assumes the learner is given a labeled data sample, from which it learns a model. The field of Active Learning deals with the situation where the learner begins not with a training sample, but instead with resources that it can use to obtain information to help identify the optimal model. To better understand this task, this paper presents and analyses the simplified "(budgeted) active model selection" version, which captures the pure exploration aspect of many active learning problems in a clean and simple problem formulation. Here the learner can use a fixed budget of "model probes" (where each probe evaluates the specified model on a random indistinguishable instance) to identify which of a given set of possible models has the highest expected accuracy. Our goal is a policy that sequentially determines which model to probe next, based on the information observed so far. We present a formal description of this task, and show that it is NPhard in general. We then investigate a number of algorithms for this task, including several existing ones (eg, "Round-Robin", "Interval Estimation", "Gittins") as well as some novel ones (e.g., "Biased-Robin"), describing first their approximation properties and then their empirical performance on various problem instances. We observe empirically that the simple biased-robin algorithm significantly outperforms the other algorithms in the case of identical costs and priors. version:1
arxiv-1207-4134 | Bayesian Learning in Undirected Graphical Models: Approximate MCMC algorithms | http://arxiv.org/abs/1207.4134 | id:1207.4134 author:Iain Murray, Zoubin Ghahramani category:cs.LG stat.ML  published:2012-07-11 summary:Bayesian learning in undirected graphical models computing posterior distributions over parameters and predictive quantities is exceptionally difficult. We conjecture that for general undirected models, there are no tractable MCMC (Markov Chain Monte Carlo) schemes giving the correct equilibrium distribution over parameters. While this intractability, due to the partition function, is familiar to those performing parameter optimisation, Bayesian learning of posterior distributions over undirected model parameters has been unexplored and poses novel challenges. we propose several approximate MCMC schemes and test on fully observed binary models (Boltzmann machines) for a small coronary heart disease data set and larger artificial systems. While approximations must perform well on the model, their interaction with the sampling scheme is also important. Samplers based on variational mean- field approximations generally performed poorly, more advanced methods using loopy propagation, brief sampling and stochastic dynamics lead to acceptable parameter posteriors. Finally, we demonstrate these techniques on a Markov random field with hidden variables. version:1
arxiv-1207-4133 | "Ideal Parent" Structure Learning for Continuous Variable Networks | http://arxiv.org/abs/1207.4133 | id:1207.4133 author:Iftach Nachman, Gal Elidan, Nir Friedman category:cs.LG stat.ML  published:2012-07-11 summary:In recent years, there is a growing interest in learning Bayesian networks with continuous variables. Learning the structure of such networks is a computationally expensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks with hidden variables. We present a general method for significantly speeding the structure search algorithm for continuous variable networks with common parametric distributions. Importantly, our method facilitates the addition of new hidden variables into the network structure efficiently. We demonstrate the method on several data sets, both for learning structure on fully observable data, and for introducing new hidden variables during structure search. version:1
arxiv-1207-4132 | MOB-ESP and other Improvements in Probability Estimation | http://arxiv.org/abs/1207.4132 | id:1207.4132 author:Rodney Nielsen category:cs.LG cs.AI stat.ML  published:2012-07-11 summary:A key prerequisite to optimal reasoning under uncertainty in intelligent systems is to start with good class probability estimates. This paper improves on the current best probability estimation trees (Bagged-PETs) and also presents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using several benchmark datasets and multiple metrics. These experiments show that MOB-ESP outputs significantly more accurate class probabilities than either the baseline BPETs algorithm or the enhanced version presented here (EB-PETs). These results are based on metrics closely associated with the average accuracy of the predictions. MOB-ESP also provides much better probability rankings than B-PETs. The paper further suggests how these estimation techniques can be applied in concert with a broader category of classifiers. version:1
arxiv-1207-4131 | Exponential Families for Conditional Random Fields | http://arxiv.org/abs/1207.4131 | id:1207.4131 author:Yasemin Altun, Alex Smola, Thomas Hofmann category:cs.LG stat.ML  published:2012-07-11 summary:In this paper we de ne conditional random elds in reproducing kernel Hilbert spaces and show connections to Gaussian Process classi cation. More speci cally, we prove decomposition results for undirected graphical models and we give constructions for kernels. Finally we present e cient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited e ciently in the optimization process. version:1
arxiv-1207-4129 | Recovering Articulated Object Models from 3D Range Data | http://arxiv.org/abs/1207.4129 | id:1207.4129 author:Dragomir Anguelov, Daphne Koller, Hoi-Cheung Pang, Praveen Srinivasan, Sebastian Thrun category:cs.CV  published:2012-07-11 summary:We address the problem of unsupervised learning of complex articulated object models from 3D range data. We describe an algorithm whose input is a set of meshes corresponding to different configurations of an articulated object. The algorithm automatically recovers a decomposition of the object into approximately rigid parts, the location of the parts in the different object instances, and the articulated object skeleton linking the parts. Our algorithm first registers allthe meshes using an unsupervised non-rigid technique described in a companion paper. It then segments the meshes using a graphical model that captures the spatial contiguity of parts. The segmentation is done using the EM algorithm, iterating between finding a decomposition of the object into rigid parts, and finding the location of the parts in the object instances. Although the graphical model is densely connected, the object decomposition step can be performed optimally and efficiently, allowing us to identify a large number of object parts while avoiding local maxima. We demonstrate the algorithm on real world datasets, recovering a 15-part articulated model of a human puppet from just 7 different puppet configurations, as well as a 4 part model of a fiexing arm where significant non-rigid deformation was present. version:1
arxiv-1207-4125 | Applying Discrete PCA in Data Analysis | http://arxiv.org/abs/1207.4125 | id:1207.4125 author:Wray L. Buntine, Aleks Jakulin category:cs.LG stat.ML  published:2012-07-11 summary:Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval. version:1
arxiv-1207-4118 | Iterative Conditional Fitting for Gaussian Ancestral Graph Models | http://arxiv.org/abs/1207.4118 | id:1207.4118 author:Mathias Drton, Thomas S. Richardson category:stat.ME cs.LG stat.ML  published:2012-07-11 summary:Ancestral graph models, introduced by Richardson and Spirtes (2002), generalize both Markov random fields and Bayesian networks to a class of graphs with a global Markov property that is closed under conditioning and marginalization. By design, ancestral graphs encode precisely the conditional independence structures that can arise from Bayesian networks with selection and unobserved (hidden/latent) variables. Thus, ancestral graph models provide a potentially very useful framework for exploratory model selection when unobserved variables might be involved in the data-generating process but no particular hidden structure can be specified. In this paper, we present the Iterative Conditional Fitting (ICF) algorithm for maximum likelihood estimation in Gaussian ancestral graph models. The name reflects that in each step of the procedure a conditional distribution is estimated, subject to constraints, while a marginal distribution is held fixed. This approach is in duality to the well-known Iterative Proportional Fitting algorithm, in which marginal distributions are fitted while conditional distributions are held fixed. version:1
arxiv-1207-4113 | On-line Prediction with Kernels and the Complexity Approximation Principle | http://arxiv.org/abs/1207.4113 | id:1207.4113 author:Alex Gammerman, Yuri Kalnishkan, Vladimir Vovk category:cs.LG stat.ML  published:2012-07-11 summary:The paper describes an application of Aggregating Algorithm to the problem of regression. It generalizes earlier results concerned with plain linear regression to kernel techniques and presents an on-line algorithm which performs nearly as well as any oblivious kernel predictor. The paper contains the derivation of an estimate on the performance of this algorithm. The estimate is then used to derive an application of the Complexity Approximation Principle to kernel methods. version:1
arxiv-1207-4112 | Algebraic Statistics in Model Selection | http://arxiv.org/abs/1207.4112 | id:1207.4112 author:Luis David Garcia category:cs.LG stat.ML  published:2012-07-11 summary:We develop the necessary theory in computational algebraic geometry to place Bayesian networks into the realm of algebraic statistics. We present an algebra{statistics dictionary focused on statistical modeling. In particular, we link the notion of effiective dimension of a Bayesian network with the notion of algebraic dimension of a variety. We also obtain the independence and non{independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables, via a generating set of an ideal of polynomials associated to the network. These results extend previous work on the subject. Finally, the relevance of these results for model selection is discussed. version:1
arxiv-1207-4110 | The Minimum Information Principle for Discriminative Learning | http://arxiv.org/abs/1207.4110 | id:1207.4110 author:Amir Globerson, Naftali Tishby category:cs.LG stat.ML  published:2012-07-11 summary:Exponential models of distributions are widely used in machine learning for classiffication and modelling. It is well known that they can be interpreted as maximum entropy models under empirical expectation constraints. In this work, we argue that for classiffication tasks, mutual information is a more suitable information theoretic measure to be optimized. We show how the principle of minimum mutual information generalizes that of maximum entropy, and provides a comprehensive framework for building discriminative classiffiers. A game theoretic interpretation of our approach is then given, and several generalization bounds provided. We present iterative algorithms for solving the minimum information problem and its convex dual, and demonstrate their performance on various classiffication tasks. The results show that minimum information classiffiers outperform the corresponding maximum entropy models. version:1
arxiv-1207-2630 | Nugget Discovery with a Multi-objective Cultural Algorithm | http://arxiv.org/abs/1207.2630 | id:1207.2630 author:Sujatha Srinivasan, Sivakumar Ramakrishnan category:cs.NE I.5.2; I.2.0  published:2012-07-11 summary:Partial classification popularly known as nugget discovery comes under descriptive knowledge discovery. It involves mining rules for a target class of interest. Classification "If-Then" rules are the most sought out by decision makers since they are the most comprehensible form of knowledge mined by data mining techniques. The rules have certain properties namely the rule metrics which are used to evaluate them. Mining rules with user specified properties can be considered as a multi-objective optimization problem since the rules have to satisfy more than one property to be used by the user. Cultural algorithm (CA) with its knowledge sources have been used in solving many optimization problems. However research gap exists in using cultural algorithm for multi-objective optimization of rules. In the current study a multi-objective cultural algorithm is proposed for partial classification. Results of experiments on benchmark data sets reveal good performance. version:1
arxiv-1207-2602 | A Novel Approach Coloured Object Tracker with Adaptive Model and Bandwidth using Mean Shift Algorithm | http://arxiv.org/abs/1207.2602 | id:1207.2602 author:Seyed Amir Mohammadi, Mohammad Reza Mahzoun category:cs.CV  published:2012-07-11 summary:The traditional color-based mean-shift tracking algorithm is popular among tracking methods due to its simple and efficient procedure, however, the lack of dynamism in its target model makes it unsuitable for tracking objects which have changes in their sizes and shapes. In this paper, we propose a fast novel threephase colored object tracker algorithm based on mean shift idea while utilizing adaptive model. The proposed method can improve the mentioned weaknesses of the original mean-shift algorithm. The experimental results show that the new method is feasible, robust and has acceptable speed in comparison with other algorithms.15 page, version:1
arxiv-1207-2600 | Efficient Prediction of DNA-Binding Proteins Using Machine Learning | http://arxiv.org/abs/1207.2600 | id:1207.2600 author:Sokyna Qatawneh, Afaf Alneaimi, Thamer Rawashdeh, Mmohammad Muhairat, Rami Qahwaji, Stan Ipson category:cs.CV q-bio.QM  published:2012-07-11 summary:DNA-binding proteins are a class of proteins which have a specific or general affinity to DNA and include three important components: transcription factors; nucleases, and histones. DNA-binding proteins also perform important roles in many types of cellular activities. In this paper we describe machine learning systems for the prediction of DNA- binding proteins where a Support Vector Machine and a Cascade Correlation Neural Network are optimized and then compared to determine the learning algorithm that achieves the best prediction performance. The information used for classification is derived from characteristics that include overall charge, patch size and amino acids composition. In total 121 DNA- binding proteins and 238 non-binding proteins are used to build and evaluate the system. For SVM using the ANOVA Kernel with Jack-knife evaluation, an accuracy of 86.7% has been achieved with 91.1% for sensitivity and 85.3% for specificity. For CCNN optimized over the entire dataset with Jack knife evaluation we report an accuracy of 75.4%, while the values of specificity and sensitivity achieved were 72.3% and 82.6%, respectively. version:1
arxiv-1207-2597 | Automated Training and Maintenance through Kinect | http://arxiv.org/abs/1207.2597 | id:1207.2597 author:Saket Warade, Jagannath Aghav, Petitpierre Claude, Sandeep Udayagiri category:cs.CV cs.ET cs.GR cs.HC  published:2012-07-11 summary:In this paper, we have worked on reducing burden on mechanic involving complex automobile maintenance activities that are performed in centralised workshops. We have presented a system prototype that combines Augmented Reality with Kinect. With the use of Kinect, very high quality sensors are available at considerably low costs, thus reducing overall expenditure for system design. The system can be operated either in Speech mode or in Gesture mode. The system can be controlled by various audio commands if user opts for Speech mode. The same controlling can also be done by using a set of Gestures in Gesture mode. Gesture recognition is the task performed by Kinect system. This system, bundled with RGB and Depth camera, processes the skeletal data by keeping track of 20 different body joints. Recognizing Gestures is done by verifying user movements and checking them against predefined condition. Augmented Reality module captures real-time image data streams from high resolution camera. This module then generates 3D model that is superimposed on real time data. version:1
arxiv-1207-2537 | Face Recognition Algorithms based on Transformed Shape Features | http://arxiv.org/abs/1207.2537 | id:1207.2537 author:Sambhunath Biswas, Amrita Biswas category:cs.CV  published:2012-07-11 summary:Human face recognition is, indeed, a challenging task, especially under the illumination and pose variations. We examine in the present paper effectiveness of two simple algorithms using coiflet packet and Radon transforms to recognize human faces from some databases of still gray level images, under the environment of illumination and pose variations. Both the algorithms convert 2-D gray level training face images into their respective depth maps or physical shape which are subsequently transformed by Coiflet packet and Radon transforms to compute energy for feature extraction. Experiments show that such transformed shape features are robust to illumination and pose variations. With the features extracted, training classes are optimally separated through linear discriminant analysis (LDA), while classification for test face images is made through a k-NN classifier, based on L1 norm and Mahalanobis distance measures. Proposed algorithms are then tested on face images that differ in illumination,expression or pose separately, obtained from three databases,namely, ORL, Yale and Essex-Grimace databases. Results, so obtained, are compared with two different existing algorithms.Performance using Daubechies wavelets is also examined. It is seen that the proposed Coiflet packet and Radon transform based algorithms have significant performance, especially under different illumination conditions and pose variation. Comparison shows the proposed algorithms are superior. version:1
arxiv-1110-2755 | Efficient Tracking of Large Classes of Experts | http://arxiv.org/abs/1110.2755 | id:1110.2755 author:András Gyorgy, Tamás Linder, Gábor Lugosi category:cs.LG cs.IT math.IT 68Q32  68P30 I.2.6; E.4  published:2011-10-12 summary:In the framework of prediction of individual sequences, sequential prediction methods are to be constructed that perform nearly as well as the best expert from a given class. We consider prediction strategies that compete with the class of switching strategies that can segment a given sequence into several blocks, and follow the advice of a different "base" expert in each block. As usual, the performance of the algorithm is measured by the regret defined as the excess loss relative to the best switching strategy selected in hindsight for the particular sequence to be predicted. In this paper we construct prediction strategies of low computational cost for the case where the set of base experts is large. In particular we provide a method that can transform any prediction algorithm $\A$ that is designed for the base class into a tracking algorithm. The resulting tracking algorithm can take advantage of the prediction performance and potential computational efficiency of $\A$ in the sense that it can be implemented with time and space complexity only $O(n^{\gamma} \ln n)$ times larger than that of $\A$, where $n$ is the time horizon and $\gamma \ge 0$ is a parameter of the algorithm. With $\A$ properly chosen, our algorithm achieves a regret bound of optimal order for $\gamma>0$, and only $O(\ln n)$ times larger than the optimal order for $\gamma=0$ for all typical regret bound types we examined. For example, for predicting binary sequences with switching parameters under the logarithmic loss, our method achieves the optimal $O(\ln n)$ regret rate with time complexity $O(n^{1+\gamma}\ln n)$ for any $\gamma\in (0,1)$. version:3
arxiv-1207-2491 | A Spectral Learning Approach to Range-Only SLAM | http://arxiv.org/abs/1207.2491 | id:1207.2491 author:Byron Boots, Geoffrey J. Gordon category:cs.LG cs.RO stat.ML  published:2012-07-10 summary:We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, and many MHT ones, our approach does not need to linearize a transition or measurement model; such linearizations can cause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly for the highly non-Gaussian posteriors encountered in range-only SLAM. We provide a theoretical analysis of our method, including finite-sample error bounds. Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost. version:1
arxiv-1207-2440 | Non-Convex Rank Minimization via an Empirical Bayesian Approach | http://arxiv.org/abs/1207.2440 | id:1207.2440 author:David Wipf category:stat.ML cs.CV cs.IT math.IT  published:2012-07-10 summary:In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candes et al., 2011) can be viewed as a special case. version:1
arxiv-1107-0399 | Vision-Based Navigation I: A navigation filter for fusing DTM/correspondence updates | http://arxiv.org/abs/1107.0399 | id:1107.0399 author:Oleg Kupervasser, Vladimir Voronov category:cs.CV cs.AI 68T45  published:2011-07-02 summary:An algorithm for pose and motion estimation using corresponding features in images and a digital terrain map is proposed. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. The utilization of data is shown to improve the robustness and accuracy of the inertial navigation algorithm. Extended Kalman filter was used to combine results of inertial navigation algorithm and proposed vision-based navigation algorithm. The feasibility of this algorithms is established through numerical simulations. version:4
arxiv-1207-2426 | A Multi-Agents Architecture to Learn Vision Operators and their Parameters | http://arxiv.org/abs/1207.2426 | id:1207.2426 author:Issam Qaffou, Mohammed Sadgal, Abdelaziz Elfazziki category:cs.CV  published:2012-07-10 summary:In a vision system, every task needs that the operators to apply should be {\guillemotleft} well chosen {\guillemotright} and their parameters should be also {\guillemotleft} well adjusted {\guillemotright}. The diversity of operators and the multitude of their parameters constitute a big challenge for users. As it is very difficult to make the {\guillemotleft} right {\guillemotright} choice, lack of a specific rule, many disadvantages appear and affect the computation time and especially the quality of results. In this paper we present a multi-agent architecture to learn the best operators to apply and their best parameters for a class of images. Our architecture consists of three types of agents: User Agent, Operator Agent and Parameter Agent. The User Agent determines the phases of treatment, a library of operators and the possible values of their parameters. The Operator Agent constructs all possible combinations of operators and the Parameter Agent, the core of the architecture, adjusts the parameters of each combination by treating a large number of images. Through the reinforcement learning mechanism, our architecture does not consider only the system opportunities but also the user preferences. version:1
arxiv-1207-2422 | Dual-Space Analysis of the Sparse Linear Model | http://arxiv.org/abs/1207.2422 | id:1207.2422 author:David Wipf, Yi Wu category:stat.ML cs.CV cs.IT math.IT  published:2012-07-10 summary:Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients. These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters. Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation. The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space. This perspective is useful because some analyses or extensions are more conducive to development in one space or the other. Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit. As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I. In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II. For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations. version:1
arxiv-1207-2268 | Improvement of ISOM by using filter | http://arxiv.org/abs/1207.2268 | id:1207.2268 author:Imen Chaabouni, Wiem Fourati, Med Salim Bouhlel category:cs.MM cs.CV  published:2012-07-10 summary:Image compression helps in storing the transmitted data in proficient way by decreasing its redundancy. This technique helps in transferring more digital or multimedia data over internet as it increases the storage space. It is important to maintain the image quality even if it is compressed to certain extent. Depend upon this the image compression is classified into two categories : lossy and lossless image compression. There are many lossy digital image compression techniques exists. Among this Incremental Self Organizing Map is a familiar one. The good pictures quality can be retrieved if image denoising technique is used for compression and also provides better compression ratio. Image denoising is an important pre-processing step for many image analysis and computer vision system. It refers to the task of recovering a good estimate of the true image from a degraded observation without altering and changing useful structure in the image such as discontinuities and edges. Many approaches have been proposed to remove the noise effectively while preserving the original image details and features as much as possible. This paper proposes a technique for image compression using Incremental Self Organizing Map (ISOM) with Discret Wavelet Transform (DWT) by applying filtering techniques which play a crucial role in enhancing the quality of a reconstructed image. The experimental result shows that the proposed technique obtained better compression ratio value. version:1
arxiv-1207-2265 | Challenges for Distributional Compositional Semantics | http://arxiv.org/abs/1207.2265 | id:1207.2265 author:Daoud Clarke category:cs.CL cs.AI  published:2012-07-10 summary:This paper summarises the current state-of-the art in the study of compositionality in distributional semantics, and major challenges for this area. We single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory. Once suitable theories have been developed, algorithms will be needed to apply the theory to tasks. Evaluation is a major problem; we single out application to recognising textual entailment and machine translation for this purpose. version:1
arxiv-1207-2253 | A Genetic Algorithm Approach for Solving a Flexible Job Shop Scheduling Problem | http://arxiv.org/abs/1207.2253 | id:1207.2253 author:Sayedmohammadreza Vaghefinezhad, Kuan Yew Wong category:math.OC cs.NE  published:2012-07-10 summary:Flexible job shop scheduling has been noticed as an effective manufacturing system to cope with rapid development in today's competitive environment. Flexible job shop scheduling problem (FJSSP) is known as a NP-hard problem in the field of optimization. Considering the dynamic state of the real world makes this problem more and more complicated. Most studies in the field of FJSSP have only focused on minimizing the total makespan. In this paper, a mathematical model for FJSSP has been developed. The objective function is maximizing the total profit while meeting some constraints. Time-varying raw material costs and selling prices and dissimilar demands for each period, have been considered to decrease gaps between reality and the model. A manufacturer that produces various parts of gas valves has been used as a case study. Its scheduling problem for multi-part, multi-period, and multi-operation with parallel machines has been solved by using genetic algorithm (GA). The best obtained answer determines the economic amount of production by different machines that belong to predefined operations for each part to satisfy customer demand in each period. version:1
arxiv-1102-1465 | An Introduction to Artificial Prediction Markets for Classification | http://arxiv.org/abs/1102.1465 | id:1102.1465 author:Adrian Barbu, Nathan Lay category:stat.ML cs.LG math.ST stat.TH  published:2011-02-07 summary:Prediction markets are used in real life to predict outcomes of interest such as presidential elections. This paper presents a mathematical theory of artificial prediction markets for supervised learning of conditional probability estimators. The artificial prediction market is a novel method for fusing the prediction information of features or trained classifiers, where the fusion result is the contract price on the possible outcomes. The market can be trained online by updating the participants' budgets using training examples. Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions. Efficient numerical algorithms are presented for solving these equations. The obtained artificial prediction market is shown to be a maximum likelihood estimator. It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods. Furthermore, the market mechanism allows the aggregation of specialized classifiers that participate only on specific instances. Experimental comparisons show that the artificial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI datasets. Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost's detection rate from 79.6% to 81.2% at 3 false positives/volume. version:6
arxiv-1006-2899 | Approximated Structured Prediction for Learning Large Scale Graphical Models | http://arxiv.org/abs/1006.2899 | id:1006.2899 author:Tamir Hazan, Raquel Urtasun category:cs.LG cs.AI  published:2010-06-15 summary:This manuscripts contains the proofs for "A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction". version:2
arxiv-1111-7100 | Determining a rotation of a tetrahedron from a projection | http://arxiv.org/abs/1111.7100 | id:1111.7100 author:Richard J. Gardner, Paolo Gronchi, Thorsten Theobald category:math.MG cs.CG cs.CV  published:2011-11-30 summary:The following problem, arising from medical imaging, is addressed: Suppose that $T$ is a known tetrahedron in $\R^3$ with centroid at the origin. Also known is the orthogonal projection $U$ of the vertices of the image $\phi T$ of $T$ under an unknown rotation $\phi$ about the origin. Under what circumstances can $\phi$ be determined from $T$ and $U$? version:2
arxiv-1206-6381 | Shortest path distance in random k-nearest neighbor graphs | http://arxiv.org/abs/1206.6381 | id:1206.6381 author:Morteza Alamgir, Ulrike von Luxburg category:cs.LG stat.ML  published:2012-06-27 summary:Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R^d. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs. version:2
arxiv-1207-1977 | Estimating a Causal Order among Groups of Variables in Linear Models | http://arxiv.org/abs/1207.1977 | id:1207.1977 author:Doris Entner, Patrik O. Hoyer category:stat.ML cs.LG stat.ME  published:2012-07-09 summary:The machine learning community has recently devoted much attention to the problem of inferring causal relationships from statistical data. Most of this work has focused on uncovering connections among scalar random variables. We generalize existing methods to apply to collections of multi-dimensional random vectors, focusing on techniques applicable to linear models. The performance of the resulting algorithms is evaluated and compared in simulations, which show that our methods can, in many cases, provide useful information on causal relationships even for relatively small sample sizes. version:1
arxiv-1207-1965 | Forecasting electricity consumption by aggregating specialized experts | http://arxiv.org/abs/1207.1965 | id:1207.1965 author:Marie Devaine, Pierre Gaillard, Yannig Goude, Gilles Stoltz category:stat.ML cs.LG stat.AP  published:2012-07-09 summary:We consider the setting of sequential prediction of arbitrary sequences based on specialized experts. We first provide a review of the relevant literature and present two theoretical contributions: a general analysis of the specialist aggregation rule of Freund et al. (1997) and an adaptation of fixed-share rules of Herbster and Warmuth (1998) in this setting. We then apply these rules to the sequential short-term (one-day-ahead) forecasting of electricity consumption; to do so, we consider two data sets, a Slovakian one and a French one, respectively concerned with hourly and half-hourly predictions. We follow a general methodology to perform the stated empirical studies and detail in particular tuning issues of the learning parameters. The introduced aggregation rules demonstrate an improved accuracy on the data sets at hand; the improvements lie in a reduced mean squared error but also in a more robust behavior with respect to large occasional errors. version:1
arxiv-1207-1922 | Spatial And Spectral Quality Evaluation Based On Edges Regions Of Satellite Image Fusion | http://arxiv.org/abs/1207.1922 | id:1207.1922 author:Firouz Abdullah Al-Wassai, N. V. Kalyankar, Ali A. Al-Zaky category:cs.CV  published:2012-07-08 summary:The Quality of image fusion is an essential determinant of the value of processing images fusion for many applications. Spatial and spectral qualities are the two important indexes that used to evaluate the quality of any fused image. However, the jury is still out of fused image's benefits if it compared with its original images. In addition, there is a lack of measures for assessing the objective quality of the spatial resolution for the fusion methods. Therefore, an objective quality of the spatial resolution assessment for fusion images is required. Most important details of the image are in edges regions, but most standards of image estimation do not depend upon specifying the edges in the image and measuring their edges. However, they depend upon the general estimation or estimating the uniform region, so this study deals with new method proposed to estimate the spatial resolution by Contrast Statistical Analysis (CSA) depending upon calculating the contrast of the edge, non edge regions and the rate for the edges regions. Specifying the edges in the image is made by using Soble operator with different threshold values. In addition, estimating the color distortion added by image fusion based on Histogram Analysis of the edge brightness values of all RGB-color bands and Lcomponent. version:1
arxiv-1207-1915 | Nonparametric Edge Detection in Speckled Imagery | http://arxiv.org/abs/1207.1915 | id:1207.1915 author:Edwin Girón, Alejandro C. Frery, Francisco Cribari-Neto category:stat.AP cs.CV stat.ML  published:2012-07-08 summary:We address the issue of edge detection in Synthetic Aperture Radar imagery. In particular, we propose nonparametric methods for edge detection, and numerically compare them to an alternative method that has been recently proposed in the literature. Our results show that some of the proposed methods display superior results and are computationally simpler than the existing method. An application to real (not simulated) data is presented and discussed. version:1
arxiv-1207-1888 | Keeping greed good: sparse regression under design uncertainty with application to biomass characterization | http://arxiv.org/abs/1207.1888 | id:1207.1888 author:David J. Biagioni, Ryan Elmore, Wesley Jones category:stat.AP stat.CO stat.ME stat.ML  published:2012-07-08 summary:In this paper, we consider the classic measurement error regression scenario in which our independent, or design, variables are observed with several sources of additive noise. We will show that our motivating example's replicated measurements on both the design and dependent variables may be leveraged to enhance a sparse regression algorithm. Specifically, we estimate the variance and use it to scale our design variables. We demonstrate the efficacy of scaling from several points of view and validate it empirically with a biomass characterization data set using two of the most widely used sparse algorithms: least angle regression (LARS) and the Dantzig selector (DS). version:1
arxiv-1207-1847 | Finding Structure in Text, Genome and Other Symbolic Sequences | http://arxiv.org/abs/1207.1847 | id:1207.1847 author:Ted Dunning category:cs.CL cs.IR  published:2012-07-08 summary:The statistical methods derived and described in this thesis provide new ways to elucidate the structural properties of text and other symbolic sequences. Generically, these methods allow detection of a difference in the frequency of a single feature, the detection of a difference between the frequencies of an ensemble of features and the attribution of the source of a text. These three abstract tasks suffice to solve problems in a wide variety of settings. Furthermore, the techniques described in this thesis can be extended to provide a wide range of additional tests beyond the ones described here. A variety of applications for these methods are examined in detail. These applications are drawn from the area of text analysis and genetic sequence analysis. The textually oriented tasks include finding interesting collocations and cooccurent phrases, language identification, and information retrieval. The biologically oriented tasks include species identification and the discovery of previously unreported long range structure in genes. In the applications reported here where direct comparison is possible, the performance of these new methods substantially exceeds the state of the art. Overall, the methods described here provide new and effective ways to analyse text and other symbolic sequences. Their particular strength is that they deal well with situations where relatively little data are available. Since these methods are abstract in nature, they can be applied in novel situations with relative ease. version:1
arxiv-1207-1765 | Object Recognition with Multi-Scale Pyramidal Pooling Networks | http://arxiv.org/abs/1207.1765 | id:1207.1765 author:Jonathan Masci, Ueli Meier, Gabriel Fricout, Jürgen Schmidhuber category:cs.CV cs.NE  published:2012-07-07 summary:We present a Multi-Scale Pyramidal Pooling Network, featuring a novel pyramidal pooling layer at multiple scales and a novel encoding layer. Thanks to the former the network does not require all images of a given classification task to be of equal size. The encoding layer improves generalisation performance in comparison to similar neural network architectures, especially when training data is scarce. We evaluate and compare our system to convolutional neural networks and state-of-the-art computer vision methods on various benchmark datasets. We also present results on industrial steel defect classification, where existing architectures are not applicable because of the constraint on equally sized input images. The proposed architecture can be seen as a fully supervised hierarchical bag-of-features extension that is trained online and can be fine-tuned for any given task. version:1
arxiv-0811-4413 | A Spectral Algorithm for Learning Hidden Markov Models | http://arxiv.org/abs/0811.4413 | id:0811.4413 author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:cs.LG cs.AI  published:2008-11-26 summary:Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications. version:6
arxiv-1205-4810 | Safe Exploration in Markov Decision Processes | http://arxiv.org/abs/1205.4810 | id:1205.4810 author:Teodor Mihai Moldovan, Pieter Abbeel category:cs.LG  published:2012-05-22 summary:In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods. version:3
arxiv-1207-1687 | Sequential detection of multiple change points in networks: a graphical model approach | http://arxiv.org/abs/1207.1687 | id:1207.1687 author:Arash Ali Amini, XuanLong Nguyen category:math.ST stat.ML stat.TH  published:2012-07-06 summary:We propose a probabilistic formulation that enables sequential detection of multiple change points in a network setting. We present a class of sequential detection rules for certain functionals of change points (minimum among a subset), and prove their asymptotic optimality properties in terms of expected detection delay time. Drawing from graphical model formalism, the sequential detection rules can be implemented by a computationally efficient message-passing protocol which may scale up linearly in network size and in waiting time. The effectiveness of our inference algorithm is demonstrated by simulations. version:1
arxiv-1207-0052 | The Complexity of Learning Principles and Parameters Grammars | http://arxiv.org/abs/1207.0052 | id:1207.0052 author:Jacob Andreas category:cs.FL cs.CL  published:2012-06-30 summary:We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the "Principles and Parameters" framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P. version:3
arxiv-1207-1649 | Analysis of Multi-Scale Fractal Dimension to Classify Human Motion | http://arxiv.org/abs/1207.1649 | id:1207.1649 author:Núbia Rosa da Silva, Odemir Martinez Bruno category:cs.CV  published:2012-07-06 summary:In recent years there has been considerable interest in human action recognition. Several approaches have been developed in order to enhance the automatic video analysis. Although some developments have been achieved by the computer vision community, the properly classification of human motion is still a hard and challenging task. The objective of this study is to investigate the use of 3D multi-scale fractal dimension to recognize motion patterns in videos. In order to develop a robust strategy for human motion classification, we proposed a method where the Fourier transform is used to calculate the derivative in which all data points are deemed. Our results shown that different accuracy rates can be found for different databases. We believe that in specific applications our results are the first step to develop an automatic monitoring system, which can be applied in security systems, traffic monitoring, biology, physical therapy, cardiovascular disease among many others. version:1
arxiv-1207-1551 | An Innovative Skin Detection Approach Using Color Based Image Retrieval Technique | http://arxiv.org/abs/1207.1551 | id:1207.1551 author:Shervan Fekri-Ershad, Mohammad Saberi, Farshad Tajeripour category:cs.CV  published:2012-07-06 summary:From The late 90th, "Skin Detection" becomes one of the major problems in image processing. If "Skin Detection" will be done in high accuracy, it can be used in many cases as face recognition, Human Tracking and etc. Until now so many methods were presented for solving this problem. In most of these methods, color space was used to extract feature vector for classifying pixels, but the most of them have not good accuracy in detecting types of skin. The proposed approach in this paper is based on "Color based image retrieval" (CBIR) technique. In this method, first by means of CBIR method and image tiling and considering the relation between pixel and its neighbors, a feature vector would be defined and then with using a training step, detecting the skin in the test stage. The result shows that the presenting approach, in addition to its high accuracy in detecting type of skin, has no sensitivity to illumination intensity and moving face orientation. version:1
arxiv-1207-1522 | Multimodal similarity-preserving hashing | http://arxiv.org/abs/1207.1522 | id:1207.1522 author:Jonathan Masci, Michael M. Bronstein, Alexander A. Bronstein, Jürgen Schmidhuber category:cs.CV cs.NE  published:2012-07-06 summary:We introduce an efficient computational framework for hashing data belonging to multiple modalities into a single representation space where they become mutually comparable. The proposed approach is based on a novel coupled siamese neural network architecture and allows unified treatment of intra- and inter-modality similarity learning. Unlike existing cross-modality similarity learning approaches, our hashing functions are not limited to binarized linear projections and can assume arbitrarily complex forms. We show experimentally that our method significantly outperforms state-of-the-art hashing approaches on multimedia retrieval tasks. version:1
arxiv-1207-1315 | An experimental study of exhaustive solutions for the Mastermind puzzle | http://arxiv.org/abs/1207.1315 | id:1207.1315 author:J. J. Merelo, Antonio M. Mora, Carlos Cotta, Thomas P. Runarsson category:cs.NE math.OC  published:2012-07-05 summary:Mastermind is in essence a search problem in which a string of symbols that is kept secret must be found by sequentially playing strings that use the same alphabet, and using the responses that indicate how close are those other strings to the secret one as hints. Although it is commercialized as a game, it is a combinatorial problem of high complexity, with applications on fields that range from computer security to genomics. As such a kind of problem, there are no exact solutions; even exhaustive search methods rely on heuristics to choose, at every step, strings to get the best possible hint. These methods mostly try to play the move that offers the best reduction in search space size in the next step; this move is chosen according to an empirical score. However, in this paper we will examine several state of the art exhaustive search methods and show that another factor, the presence of the actual solution among the candidate moves, or, in other words, the fact that the actual solution has the highest score, plays also a very important role. Using that, we will propose new exhaustive search approaches that obtain results which are comparable to the classic ones, and besides, are better suited as a basis for non-exhaustive search strategies such as evolutionary algorithms, since their behavior in a series of key indicators is better than the classical algorithms. version:1
arxiv-1202-5695 | Training Restricted Boltzmann Machines on Word Observations | http://arxiv.org/abs/1202.5695 | id:1202.5695 author:George E. Dahl, Ryan P. Adams, Hugo Larochelle category:cs.LG stat.ML  published:2012-02-25 summary:The restricted Boltzmann machine (RBM) is a flexible tool for modeling complex data, however there have been significant computational difficulties in using RBMs to model high-dimensional multinomial observations. In natural language processing applications, words are naturally modeled by K-ary discrete distributions, where K is determined by the vocabulary size and can easily be in the hundreds of thousands. The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K-way softmax visible units during block Gibbs updates, an operation that takes time linear in K. In this work, we address this issue by employing a more general class of Markov chain Monte Carlo operators on the visible units, yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n-grams using larger vocabularies than previously feasible and using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter. version:2
arxiv-1207-1119 | On unified view of nullspace-type conditions for recoveries associated with general sparsity structures | http://arxiv.org/abs/1207.1119 | id:1207.1119 author:Anatoli Juditsky, Fatma Kilinc Karzan, Arkadi Nemirovski category:math.OC cs.IT math.IT stat.ML  published:2012-07-04 summary:We discuss a general notion of "sparsity structure" and associated recoveries of a sparse signal from its linear image of reduced dimension possibly corrupted with noise. Our approach allows for uni?ed treatment of (a) the "usual sparsity" and "usual $\ell_1$ recovery," (b) block-sparsity with possibly overlapping blocks and associated block-$\ell_1$ recovery, and (c) low-rank-oriented recovery by nuclear norm minimization. The proposed recovery routines are natural extensions of the usual $\ell_1$ minimization used in Compressed Sensing. Specifically we present nullspace-type sufficient conditions for the recovery to be precise on sparse signals in the noiseless case. Then we derive error bounds for imperfect (nearly sparse signal, presence of observation noise, etc.) recovery under these conditions. In all of these cases, we present efficiently verifiable sufficient conditions for the validity of the associated nullspace properties. version:1
arxiv-1207-1429 | Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks | http://arxiv.org/abs/1207.1429 | id:1207.1429 author:Marc Teyssier, Daphne Koller category:cs.LG cs.AI stat.ML  published:2012-07-04 summary:One of the basic tasks for Bayesian networks (BNs) is that of learning a network structure from data. The BN-learning problem is NP-hard, so the standard solution is heuristic search. Many approaches have been proposed for this task, but only a very small number outperform the baseline of greedy hill-climbing with tabu lists; moreover, many of the proposed algorithms are quite complex and hard to implement. In this paper, we propose a very simple and easy-to-implement method for addressing this task. Our approach is based on the well-known fact that the best network (of bounded in-degree) consistent with a given node ordering can be found very efficiently. We therefore propose a search not over the space of structures, but over the space of orderings, selecting for each ordering the best network consistent with it. This search space is much smaller, makes more global search steps, has a lower branching factor, and avoids costly acyclicity checks. We present results for this algorithm on both synthetic and real data sets, evaluating both the score of the network found and in the running time. We show that ordering-based search outperforms the standard baseline, and is competitive with recent algorithms that are much harder to implement. version:1
arxiv-1207-1423 | Mining Associated Text and Images with Dual-Wing Harmoniums | http://arxiv.org/abs/1207.1423 | id:1207.1423 author:Eric P. Xing, Rong Yan, Alexander G. Hauptmann category:cs.LG cs.DB stat.ML  published:2012-07-04 summary:We propose a multi-wing harmonium model for mining multimedia data that extends and improves on earlier models based on two-layer random fields, which capture bidirectional dependencies between hidden topic aspects and observed inputs. This model can be viewed as an undirected counterpart of the two-layer directed models such as LDA for similar tasks, but bears significant difference in inference/learning cost tradeoffs, latent topic representations, and topic mixing mechanisms. In particular, our model facilitates efficient inference and robust topic mixing, and potentially provides high flexibilities in modeling the latent topic spaces. A contrastive divergence and a variational algorithm are derived for learning. We specialized our model to a dual-wing harmonium for captioned images, incorporating a multivariate Poisson for word-counts and a multivariate Gaussian for color histogram. We present empirical results on the applications of this model to classification, retrieval and image annotation on news video collections, and we report an extensive comparison with various extant models. version:1
arxiv-1207-1421 | A Function Approximation Approach to Estimation of Policy Gradient for POMDP with Structured Policies | http://arxiv.org/abs/1207.1421 | id:1207.1421 author:Huizhen Yu category:cs.LG stat.ML  published:2012-07-04 summary:We consider the estimation of the policy gradient in partially observable Markov decision processes (POMDP) with a special class of structured policies that are finite-state controllers. We show that the gradient estimation can be done in the Actor-Critic framework, by making the critic compute a "value" function that does not depend on the states of POMDP. This function is the conditional mean of the true value function that depends on the states. We show that the critic can be implemented using temporal difference (TD) methods with linear function approximations, and the analytical results on TD and Actor-Critic can be transfered to this case. Although Actor-Critic algorithms have been used extensively in Markov decision processes (MDP), up to now they have not been proposed for POMDP as an alternative to the earlier proposal GPOMDP algorithm, an actor-only method. Furthermore, we show that the same idea applies to semi-Markov problems with a subset of finite-state controllers. version:1
arxiv-1207-1420 | Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars | http://arxiv.org/abs/1207.1420 | id:1207.1420 author:Luke S. Zettlemoyer, Michael Collins category:cs.CL  published:2012-07-04 summary:This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains. version:1
arxiv-1207-1417 | The DLR Hierarchy of Approximate Inference | http://arxiv.org/abs/1207.1417 | id:1207.1417 author:Michal Rosen-Zvi, Michael I. Jordan, Alan Yuille category:cs.LG stat.ML  published:2012-07-04 summary:We propose a hierarchy for approximate inference based on the Dobrushin, Lanford, Ruelle (DLR) equations. This hierarchy includes existing algorithms, such as belief propagation, and also motivates novel algorithms such as factorized neighbors (FN) algorithms and variants of mean field (MF) algorithms. In particular, we show that extrema of the Bethe free energy correspond to approximate solutions of the DLR equations. In addition, we demonstrate a close connection between these approximate algorithms and Gibbs sampling. Finally, we compare and contrast various of the algorithms in the DLR hierarchy on spin-glass problems. The experiments show that algorithms higher up in the hierarchy give more accurate results when they converge but tend to be less stable. version:1
arxiv-1207-1414 | Two-Way Latent Grouping Model for User Preference Prediction | http://arxiv.org/abs/1207.1414 | id:1207.1414 author:Eerika Savia, Kai Puolamaki, Janne Sinkkonen, Samuel Kaski category:cs.IR cs.LG stat.ML  published:2012-07-04 summary:We introduce a novel latent grouping model for predicting the relevance of a new document to a user. The model assumes a latent group structure for both users and documents. We compared the model against a state-of-the-art method, the User Rating Profile model, where only users have a latent group structure. We estimate both models by Gibbs sampling. The new method predicts relevance more accurately for new documents that have few known ratings. The reason is that generalization over documents then becomes necessary and hence the twoway grouping is profitable. version:1
arxiv-1207-1413 | Discovery of non-gaussian linear causal models using ICA | http://arxiv.org/abs/1207.1413 | id:1207.1413 author:Shohei Shimizu, Aapo Hyvarinen, Yutaka Kano, Patrik O. Hoyer category:cs.LG cs.MS stat.ML  published:2012-07-04 summary:In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000). Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis (ICA), and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data. version:1
arxiv-1207-1409 | Piecewise Training for Undirected Models | http://arxiv.org/abs/1207.1409 | id:1207.1409 author:Charles Sutton, Andrew McCallum category:cs.LG stat.ML  published:2012-07-04 summary:For many large undirected models that arise in real-world applications, exact maximumlikelihood training is intractable, because it requires computing marginal distributions of the model. Conditional training is even more difficult, because the partition function depends not only on the parameters, but also on the observed input, requiring repeated inference over each training example. An appealing idea for such models is to independently train a local undirected classifier over each clique, afterwards combining the learned weights into a single global model. In this paper, we show that this piecewise method can be justified as minimizing a new family of upper bounds on the log partition function. On three natural-language data sets, piecewise training is more accurate than pseudolikelihood, and often performs comparably to global training using belief propagation. version:1
arxiv-1207-1406 | A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance | http://arxiv.org/abs/1207.1406 | id:1207.1406 author:Andrew McCallum, Kedar Bellare, Fernando Pereira category:cs.LG cs.AI  published:2012-07-04 summary:The need to measure sequence similarity arises in information extraction, object identity, data mining, biological sequence analysis, and other domains. This paper presents discriminative string-edit CRFs, a finitestate conditional random field model for edit sequences between strings. Conditional random fields have advantages over generative approaches to this problem, such as pair HMMs or the work of Ristad and Yianilos, because as conditionally-trained methods, they enable the use of complex, arbitrary actions and features of the input strings. As in generative models, the training data does not have to specify the edit sequences between the given string pairs. Unlike generative models, however, our model is trained on both positive and negative instances of string pairs. We present positive experimental results on several data sets. version:1
arxiv-1207-1404 | A submodular-supermodular procedure with applications to discriminative structure learning | http://arxiv.org/abs/1207.1404 | id:1207.1404 author:Mukund Narasimhan, Jeff A. Bilmes category:cs.LG cs.DS stat.ML  published:2012-07-04 summary:In this paper, we present an algorithm for minimizing the difference between two submodular functions using a variational framework which is based on (an extension of) the concave-convex procedure [17]. Because several commonly used metrics in machine learning, like mutual information and conditional mutual information, are submodular, the problem of minimizing the difference of two submodular problems arises naturally in many machine learning applications. Two such applications are learning discriminatively structured graphical models and feature selection under computational complexity constraints. A commonly used metric for measuring discriminative capacity is the EAR measure which is the difference between two conditional mutual information terms. Feature selection taking complexity considerations into account also fall into this framework because both the information that a set of features provide and the cost of computing and using the features can be modeled as submodular functions. This problem is NP-hard, and we give a polynomial time heuristic for it. We also present results on synthetic data to show that classifiers based on discriminative graphical models using this algorithm can significantly outperform classifiers based on generative graphical models. version:1
arxiv-1207-1403 | Obtaining Calibrated Probabilities from Boosting | http://arxiv.org/abs/1207.1403 | id:1207.1403 author:Alexandru Niculescu-Mizil, Richard A. Caruana category:cs.LG stat.ML  published:2012-07-04 summary:Boosted decision trees typically yield good accuracy, precision, and ROC area. However, because the outputs from boosting are not well calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. We empirically demonstrate why AdaBoost predicts distorted probabilities and examine three calibration methods for correcting this distortion: Platt Scaling, Isotonic Regression, and Logistic Correction. We also experiment with boosting using log-loss instead of the usual exponential loss. Experiments show that Logistic Correction and boosting with log-loss work well when boosting weak models such as decision stumps, but yield poor performance when boosting more complex models such as full decision trees. Platt Scaling and Isotonic Regression, however, significantly improve the probabilities predicted by version:1
arxiv-1207-1396 | Toward Practical N2 Monte Carlo: the Marginal Particle Filter | http://arxiv.org/abs/1207.1396 | id:1207.1396 author:Mike Klaas, Nando de Freitas, Arnaud Doucet category:stat.CO cs.LG stat.ML  published:2012-07-04 summary:Sequential Monte Carlo techniques are useful for state estimation in non-linear, non-Gaussian dynamic models. These methods allow us to approximate the joint posterior distribution using sequential importance sampling. In this framework, the dimension of the target distribution grows with each time step, thus it is necessary to introduce some resampling steps to ensure that the estimates provided by the algorithm have a reasonable variance. In many applications, we are only interested in the marginal filtering distribution which is defined on a space of fixed dimension. We present a Sequential Monte Carlo algorithm called the Marginal Particle Filter which operates directly on the marginal distribution, hence avoiding having to perform importance sampling on a space of growing dimension. Using this idea, we also derive an improved version of the auxiliary particle filter. We show theoretic and empirical results which demonstrate a reduction in variance over conventional particle filtering, and present techniques for reducing the cost of the marginal particle filter with N particles from O(N2) to O(N logN). version:1
arxiv-1207-1393 | Learning about individuals from group statistics | http://arxiv.org/abs/1207.1393 | id:1207.1393 author:Hendrik Kuck, Nando de Freitas category:cs.LG stat.ML  published:2012-07-04 summary:We propose a new problem formulation which is similar to, but more informative than, the binary multiple-instance learning problem. In this setting, we are given groups of instances (described by feature vectors) along with estimates of the fraction of positively-labeled instances per group. The task is to learn an instance level classifier from this information. That is, we are trying to estimate the unknown binary labels of individuals from knowledge of group statistics. We propose a principled probabilistic model to solve this problem that accounts for uncertainty in the parameters and in the unknown individual labels. This model is trained with an efficient MCMC algorithm. Its performance is demonstrated on both synthetic and real-world data arising in general object recognition. version:1
arxiv-1207-1387 | Learning Bayesian Network Parameters with Prior Knowledge about Context-Specific Qualitative Influences | http://arxiv.org/abs/1207.1387 | id:1207.1387 author:Ad Feelders, Linda C. van der Gaag category:cs.AI cs.LG stat.ML  published:2012-07-04 summary:We present a method for learning the parameters of a Bayesian network with prior knowledge about the signs of influences between variables. Our method accommodates not just the standard signs, but provides for context-specific signs as well. We show how the various signs translate into order constraints on the network parameters and how isotonic regression can be used to compute order-constrained estimates from the available data. Our experimental results show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution, especially when only a small sample of data is available. Moreover, the computed estimates are guaranteed to be consistent with the specified signs, thereby resulting in a network that is more likely to be accepted by experts in its domain of application. version:1
arxiv-1207-1382 | Maximum Margin Bayesian Networks | http://arxiv.org/abs/1207.1382 | id:1207.1382 author:Yuhong Guo, Dana Wilkinson, Dale Schuurmans category:cs.LG stat.ML  published:2012-07-04 summary:We consider the problem of learning Bayesian network classifiers that maximize the marginover a set of classification variables. We find that this problem is harder for Bayesian networks than for undirected graphical models like maximum margin Markov networks. The main difficulty is that the parameters in a Bayesian network must satisfy additional normalization constraints that an undirected graphical model need not respect. These additional constraints complicate the optimization task. Nevertheless, we derive an effective training algorithm that solves the maximum margin training problem for a range of Bayesian network topologies, and converges to an approximate solution for arbitrary network topologies. Experimental results show that the method can demonstrate improved generalization performance over Markov networks when the directed graphical structure encodes relevant knowledge. In practice, the training technique allows one to combine prior knowledge expressed as a directed (causal) model with state of the art discriminative learning methods. version:1
arxiv-1207-1380 | Bayes Blocks: An Implementation of the Variational Bayesian Building Blocks Framework | http://arxiv.org/abs/1207.1380 | id:1207.1380 author:Markus Harva, Tapani Raiko, Antti Honkela, Harri Valpola, Juha Karhunen category:cs.MS cs.LG stat.ML  published:2012-07-04 summary:A software library for constructing and learning probabilistic models is presented. The library offers a set of building blocks from which a large variety of static and dynamic models can be built. These include hierarchical models for variances of other variables and many nonlinear models. The underlying variational Bayesian machinery, providing for fast and robust estimation but being mathematically rather involved, is almost completely hidden from the user thus making it very easy to use the library. The building blocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables and computational nodes which can be combined rather freely. version:1
arxiv-1207-1379 | On the Detection of Concept Changes in Time-Varying Data Stream by Testing Exchangeability | http://arxiv.org/abs/1207.1379 | id:1207.1379 author:Shen-Shyang Ho, Harry Wechsler category:cs.LG stat.ML  published:2012-07-04 summary:A martingale framework for concept change detection based on testing data exchangeability was recently proposed (Ho, 2005). In this paper, we describe the proposed change-detection test based on the Doob's Maximal Inequality and show that it is an approximation of the sequential probability ratio test (SPRT). The relationship between the threshold value used in the proposed test and its size and power is deduced from the approximation. The mean delay time before a change is detected is estimated using the average sample number of a SPRT. The performance of the test using various threshold values is examined on five different data stream scenarios simulated using two synthetic data sets. Finally, experimental results show that the test is effective in detecting changes in time-varying data streams simulated using three benchmark data sets. version:1
arxiv-1207-1367 | Belief Updating and Learning in Semi-Qualitative Probabilistic Networks | http://arxiv.org/abs/1207.1367 | id:1207.1367 author:Cassio Polpo de Campos, Fabio Gagliardi Cozman category:cs.AI stat.ML  published:2012-07-04 summary:This paper explores semi-qualitative probabilistic networks (SQPNs) that combine numeric and qualitative information. We first show that exact inferences with SQPNs are NPPP-Complete. We then show that existing qualitative relations in SQPNs (plus probabilistic logic and imprecise assessments) can be dealt effectively through multilinear programming. We then discuss learning: we consider a maximum likelihood method that generates point estimates given a SQPN and empirical data, and we describe a Bayesian-minded method that employs the Imprecise Dirichlet Model to generate set-valued estimates. version:1
arxiv-1207-1366 | Learning Factor Graphs in Polynomial Time & Sample Complexity | http://arxiv.org/abs/1207.1366 | id:1207.1366 author:Pieter Abbeel, Daphne Koller, Andrew Y. Ng category:cs.LG stat.ML  published:2012-07-04 summary:We study computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded factor size and bounded connectivity can be learned in polynomial time and polynomial number of samples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Unlike maximum likelihood estimation, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. version:1
arxiv-1207-1364 | Learning from Sparse Data by Exploiting Monotonicity Constraints | http://arxiv.org/abs/1207.1364 | id:1207.1364 author:Eric E. Altendorf, Angelo C. Restificar, Thomas G. Dietterich category:cs.LG stat.ML  published:2012-07-04 summary:When training data is sparse, more domain knowledge must be incorporated into the learning algorithm in order to reduce the effective size of the hypothesis space. This paper builds on previous work in which knowledge about qualitative monotonicities was formally represented and incorporated into learning algorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm). We show how to interpret knowledge of qualitative influences, and in particular of monotonicities, as constraints on probability distributions, and to incorporate this knowledge into Bayesian network learning algorithms. We show that this yields improved accuracy, particularly with very small training sets (e.g. less than 10 examples). version:1
arxiv-1207-1019 | PAC-Bayesian Majority Vote for Late Classifier Fusion | http://arxiv.org/abs/1207.1019 | id:1207.1019 author:Emilie Morvant, Amaury Habrard, Stéphane Ayache category:stat.ML cs.CV cs.LG cs.MM  published:2012-07-04 summary:A lot of attention has been devoted to multimedia indexing over the past few years. In the literature, we often consider two kinds of fusion schemes: The early fusion and the late fusion. In this paper we focus on late classifier fusion, where one combines the scores of each modality at the decision level. To tackle this problem, we investigate a recent and elegant well-founded quadratic program named MinCq coming from the Machine Learning PAC-Bayes theory. MinCq looks for the weighted combination, over a set of real-valued functions seen as voters, leading to the lowest misclassification rate, while making use of the voters' diversity. We provide evidence that this method is naturally adapted to late fusion procedure. We propose an extension of MinCq by adding an order- preserving pairwise loss for ranking, helping to improve Mean Averaged Precision measure. We confirm the good behavior of the MinCq-based fusion approaches with experiments on a real image benchmark. version:1
arxiv-1207-1358 | Unsupervised spectral learning | http://arxiv.org/abs/1207.1358 | id:1207.1358 author:Susan Shortreed, Marina Meila category:cs.LG stat.ML  published:2012-07-04 summary:In spectral clustering and spectral image segmentation, the data is partioned starting from a given matrix of pairwise similarities S. the matrix S is constructed by hand, or learned on a separate training set. In this paper we show how to achieve spectral clustering in unsupervised mode. Our algorithm starts with a set of observed pairwise features, which are possible components of an unknown, parametric similarity function. This function is learned iteratively, at the same time as the clustering of the data. The algorithm shows promosing results on synthetic and real data. version:1
arxiv-1110-3907 | AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem | http://arxiv.org/abs/1110.3907 | id:1110.3907 author:Peng Sun, Mark D. Reid, Jie Zhou category:stat.ML cs.AI cs.CV  published:2011-10-18 summary:This paper presents an improvement to model learning when using multi-class LogitBoost for classification. Motivated by the statistical view, LogitBoost can be seen as additive tree regression. Two important factors in this setting are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the dense Hessian matrices that arise when computing tree node split gain and node value fittings. In general, this setting is too complicated for a tractable model learning algorithm. However, too aggressive simplification of the setting may lead to degraded performance. For example, the original LogitBoost is outperformed by ABC-LogitBoost due to the latter's more careful treatment of the above two factors. In this paper we propose techniques to address the two main difficulties of the LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block coordinate descent that exploits the dense Hessian when computing tree split gain and node values. Higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the ABC-LogitBoost implementations. version:3
arxiv-1207-0833 | Relational Data Mining Through Extraction of Representative Exemplars | http://arxiv.org/abs/1207.0833 | id:1207.0833 author:Frédéric Blanchard, Michel Herbin category:cs.AI cs.IR stat.ML  published:2012-07-03 summary:With the growing interest on Network Analysis, Relational Data Mining is becoming an emphasized domain of Data Mining. This paper addresses the problem of extracting representative elements from a relational dataset. After defining the notion of degree of representativeness, computed using the Borda aggregation procedure, we present the extraction of exemplars which are the representative elements of the dataset. We use these concepts to build a network on the dataset. We expose the main properties of these notions and we propose two typical applications of our framework. The first application consists in resuming and structuring a set of binary images and the second in mining co-authoring relation in a research team. version:1
arxiv-1207-0784 | Web-Based Benchmark for Keystroke Dynamics Biometric Systems: A Statistical Analysis | http://arxiv.org/abs/1207.0784 | id:1207.0784 author:Romain Giot, Mohamad El-Abed, Christophe Rosenberger category:cs.LG  published:2012-07-03 summary:Most keystroke dynamics studies have been evaluated using a specific kind of dataset in which users type an imposed login and password. Moreover, these studies are optimistics since most of them use different acquisition protocols, private datasets, controlled environment, etc. In order to enhance the accuracy of keystroke dynamics' performance, the main contribution of this paper is twofold. First, we provide a new kind of dataset in which users have typed both an imposed and a chosen pairs of logins and passwords. In addition, the keystroke dynamics samples are collected in a web-based uncontrolled environment (OS, keyboards, browser, etc.). Such kind of dataset is important since it provides us more realistic results of keystroke dynamics' performance in comparison to the literature (controlled environment, etc.). Second, we present a statistical analysis of well known assertions such as the relationship between performance and password size, impact of fusion schemes on system overall performance, and others such as the relationship between performance and entropy. We put into obviousness in this paper some new results on keystroke dynamics in realistic conditions. version:1
arxiv-1207-0783 | Hybrid Template Update System for Unimodal Biometric Systems | http://arxiv.org/abs/1207.0783 | id:1207.0783 author:Romain Giot, Christophe Rosenberger, Bernadette Dorizzi category:cs.LG  published:2012-07-03 summary:Semi-supervised template update systems allow to automatically take into account the intra-class variability of the biometric data over time. Such systems can be inefficient by including too many impostor's samples or skipping too many genuine's samples. In the first case, the biometric reference drifts from the real biometric data and attracts more often impostors. In the second case, the biometric reference does not evolve quickly enough and also progressively drifts from the real biometric data. We propose a hybrid system using several biometric sub-references in order to increase per- formance of self-update systems by reducing the previously cited errors. The proposition is validated for a keystroke- dynamics authentication system (this modality suffers of high variability over time) on two consequent datasets from the state of the art. version:1
arxiv-1207-0771 | Polarimetric SAR Image Smoothing with Stochastic Distances | http://arxiv.org/abs/1207.0771 | id:1207.0771 author:Leonardo Torres, Antonio C. Medeiros, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML  published:2012-07-03 summary:Polarimetric Synthetic Aperture Radar (PolSAR) images are establishing as an important source of information in remote sensing applications. The most complete format this type of imaging produces consists of complex-valued Hermitian matrices in every image coordinate and, as such, their visualization is challenging. They also suffer from speckle noise which reduces the signal-to-noise ratio. Smoothing techniques have been proposed in the literature aiming at preserving different features and, analogously, projections from the cone of Hermitian positive matrices to different color representation spaces are used for enhancing certain characteristics. In this work we propose the use of stochastic distances between models that describe this type of data in a Nagao-Matsuyama-type of smoothing technique. The resulting images are shown to present good visualization properties (noise reduction with preservation of fine details) in all the considered visualization spaces. version:1
arxiv-1207-0757 | Generalized Statistical Complexity of SAR Imagery | http://arxiv.org/abs/1207.0757 | id:1207.0757 author:Eliana S. de Almeida, Antonio Carlos de Medeiros, Osvaldo A. Rosso, Alejandro C. Frery category:cs.IT cs.GR math.IT stat.AP stat.ML  published:2012-07-03 summary:A new generalized Statistical Complexity Measure (SCM) was proposed by Rosso et al in 2010. It is a functional that captures the notions of order/disorder and of distance to an equilibrium distribution. The former is computed by a measure of entropy, while the latter depends on the definition of a stochastic divergence. When the scene is illuminated by coherent radiation, image data is corrupted by speckle noise, as is the case of ultrasound-B, sonar, laser and Synthetic Aperture Radar (SAR) sensors. In the amplitude and intensity formats, this noise is multiplicative and non-Gaussian requiring, thus, specialized techniques for image processing and understanding. One of the most successful family of models for describing these images is the Multiplicative Model which leads, among other probability distributions, to the G0 law. This distribution has been validated in the literature as an expressive and tractable model, deserving the "universal" denomination for its ability to describe most types of targets. In order to compute the statistical complexity of a site in an image corrupted by speckle noise, we assume that the equilibrium distribution is that of fully developed speckle, namely the Gamma law in intensity format, which appears in areas with little or no texture. We use the Shannon entropy along with the Hellinger distance to measure the statistical complexity of intensity SAR images, and we show that it is an expressive feature capable of identifying many types of targets. version:1
arxiv-1207-0742 | The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling | http://arxiv.org/abs/1207.0742 | id:1207.0742 author:Marc Dymetman, Guillaume Bouchard, Simon Carter category:cs.AI cs.CL cs.LG  published:2012-07-03 summary:Most current sampling algorithms for high-dimensional distributions are based on MCMC techniques and are approximate in the sense that they are valid only asymptotically. Rejection sampling, on the other hand, produces valid samples, but is unrealistically slow in high-dimension spaces. The OS* algorithm that we propose is a unified approach to exact optimization and sampling, based on incremental refinements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We show that the choice of the refinement can be done in a way that ensures tractability in high-dimension spaces, and we present first experiments in two different settings: inference in high-order HMMs and in large discrete graphical models. version:1
arxiv-1207-1115 | Inferring land use from mobile phone activity | http://arxiv.org/abs/1207.1115 | id:1207.1115 author:Jameson L. Toole, Michael Ulm, Dietmar Bauer, Marta C. Gonzalez category:stat.ML cs.LG physics.data-an physics.soc-ph H.2.8  published:2012-07-03 summary:Understanding the spatiotemporal distribution of people within a city is crucial to many planning applications. Obtaining data to create required knowledge, currently involves costly survey methods. At the same time ubiquitous mobile sensors from personal GPS devices to mobile phones are collecting massive amounts of data on urban systems. The locations, communications, and activities of millions of people are recorded and stored by new information technologies. This work utilizes novel dynamic data, generated by mobile phone users, to measure spatiotemporal changes in population. In the process, we identify the relationship between land use and dynamic population over the course of a typical week. A machine learning classification algorithm is used to identify clusters of locations with similar zoned uses and mobile phone activity patterns. It is shown that the mobile phone data is capable of delivering useful information on actual land use that supplements zoning regulations. version:1
arxiv-1207-0704 | Speckle Reduction using Stochastic Distances | http://arxiv.org/abs/1207.0704 | id:1207.0704 author:Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML  published:2012-07-03 summary:This paper presents a new approach for filter design based on stochastic distances and tests between distributions. A window is defined around each pixel, samples are compared and only those which pass a goodness-of-fit test are used to compute the filtered value. The technique is applied to intensity Synthetic Aperture Radar (SAR) data, using the Gamma model with varying number of looks allowing, thus, changes in heterogeneity. Modified Nagao-Matsuyama windows are used to define the samples. The proposal is compared with the Lee's filter which is considered a standard, using a protocol based on simulation. Among the criteria used to quantify the quality of filters, we employ the equivalent number of looks (related to the signal-to-noise ratio), line contrast, and edge preservation. Moreover, we also assessed the filters by the Universal Image Quality Index and the Pearson's correlation between edges. version:1
arxiv-1207-0702 | Meme as Building Block for Evolutionary Optimization of Problem Instances | http://arxiv.org/abs/1207.0702 | id:1207.0702 author:Liang Feng, Yew Soon Ong, Ah Hwee Tan, Ivor Wai-Hung Tsang category:cs.NE  published:2012-07-03 summary:A significantly under-explored area of evolutionary optimization in the literature is the study of optimization methodologies that can evolve along with the problems solved. Particularly, present evolutionary optimization approaches generally start their search from scratch or the ground-zero state of knowledge, independent of how similar the given new problem of interest is to those optimized previously. There has thus been the apparent lack of automated knowledge transfers and reuse across problems. Taking the cue, this paper introduces a novel Memetic Computational Paradigm for search, one that models after how human solves problems, and embarks on a study towards intelligent evolutionary optimization of problems through the transfers of structured knowledge in the form of memes learned from previous problem-solving experiences, to enhance future evolutionary searches. In particular, the proposed memetic search paradigm is composed of four culture-inspired operators, namely, Meme Learning, Meme Selection, Meme Variation and Meme Imitation. The learning operator mines for memes in the form of latent structures derived from past experiences of problem-solving. The selection operator identifies the fit memes that replicate and transmit across problems, while the variation operator introduces innovations into the memes. The imitation operator, on the other hand, defines how fit memes assimilate into the search process of newly encountered problems, thus gearing towards efficient and effective evolutionary optimization. Finally, comprehensive studies on two widely studied challenging well established NP-hard routing problem domains, particularly, the capacitated vehicle routing (CVR) and capacitated arc routing (CAR), confirm the high efficacy of the proposed memetic computational search paradigm for intelligent evolutionary optimization of problems. version:1
arxiv-1207-0677 | Local Water Diffusion Phenomenon Clustering From High Angular Resolution Diffusion Imaging (HARDI) | http://arxiv.org/abs/1207.0677 | id:1207.0677 author:Romain Giot, Christophe Charrier, Maxime Descoteaux category:cs.LG cs.CV  published:2012-07-03 summary:The understanding of neurodegenerative diseases undoubtedly passes through the study of human brain white matter fiber tracts. To date, diffusion magnetic resonance imaging (dMRI) is the unique technique to obtain information about the neural architecture of the human brain, thus permitting the study of white matter connections and their integrity. However, a remaining challenge of the dMRI community is to better characterize complex fiber crossing configurations, where diffusion tensor imaging (DTI) is limited but high angular resolution diffusion imaging (HARDI) now brings solutions. This paper investigates the development of both identification and classification process of the local water diffusion phenomenon based on HARDI data to automatically detect imaging voxels where there are single and crossing fiber bundle populations. The technique is based on knowledge extraction processes and is validated on a dMRI phantom dataset with ground truth. version:1
arxiv-1203-1007 | Agnostic System Identification for Model-Based Reinforcement Learning | http://arxiv.org/abs/1203.1007 | id:1203.1007 author:Stephane Ross, J. Andrew Bagnell category:cs.LG cs.AI cs.SY stat.ML  published:2012-03-05 summary:A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature. version:2
arxiv-1207-0658 | On the origin of long-range correlations in texts | http://arxiv.org/abs/1207.0658 | id:1207.0658 author:Eduardo G. Altmann, Giampaolo Cristadoro, Mirko Degli Esposti category:physics.data-an cs.CL physics.soc-ph  published:2012-07-03 summary:The complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language. In order to retain and convey such a high dimensional information, the statistical properties of our linguistic output has to be highly correlated in time. An example are the robust observations, still largely not understood, of correlations on arbitrary long scales in literary texts. In this paper we explain how long-range correlations flow from highly structured linguistic levels down to the building blocks of a text (words, letters, etc..). By combining calculations and data analysis we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text. The mechanisms we identify are fairly general and can be equally applied to other hierarchical settings. version:1
arxiv-1207-0580 | Improving neural networks by preventing co-adaptation of feature detectors | http://arxiv.org/abs/1207.0580 | id:1207.0580 author:Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov category:cs.NE cs.CV cs.LG  published:2012-07-03 summary:When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition. version:1
arxiv-1010-1526 | Time Series Classification by Class-Specific Mahalanobis Distance Measures | http://arxiv.org/abs/1010.1526 | id:1010.1526 author:Zoltán Prekopcsák, Daniel Lemire category:cs.LG  published:2010-10-07 summary:To classify time series by nearest neighbors, we need to specify or learn one or several distance measures. We consider variations of the Mahalanobis distance measures which rely on the inverse covariance matrix of the data. Unfortunately --- for time series data --- the covariance matrix has often low rank. To alleviate this problem we can either use a pseudoinverse, covariance shrinking or limit the matrix to its diagonal. We review these alternatives and benchmark them against competitive methods such as the related Large Margin Nearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW) distance. As we expected, we find that the DTW is superior, but the Mahalanobis distance measures are one to two orders of magnitude faster. To get best results with Mahalanobis distance measures, we recommend learning one distance measure per class using either covariance shrinking or the diagonal approach. version:6
arxiv-1207-0396 | Applying Deep Belief Networks to Word Sense Disambiguation | http://arxiv.org/abs/1207.0396 | id:1207.0396 author:Peratham Wiriyathammabhum, Boonserm Kijsirikul, Hiroya Takamura, Manabu Okumura category:cs.CL cs.LG  published:2012-07-02 summary:In this paper, we applied a novel learning algorithm, namely, Deep Belief Networks (DBN) to word sense disambiguation (WSD). DBN is a probabilistic generative model composed of multiple layers of hidden units. DBN uses Restricted Boltzmann Machine (RBM) to greedily train layer by layer as a pretraining. Then, a separate fine tuning step is employed to improve the discriminative power. We compared DBN with various state-of-the-art supervised learning algorithms in WSD such as Support Vector Machine (SVM), Maximum Entropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel Principal Component Analysis (KPCA). We used all words in the given paragraph, surrounding context words and part-of-speech of surrounding words as our knowledge sources. We conducted our experiment on the SENSEVAL-2 data set. We observed that DBN outperformed all other learning algorithms. version:1
arxiv-1207-0369 | More Effective Crossover Operators for the All-Pairs Shortest Path Problem | http://arxiv.org/abs/1207.0369 | id:1207.0369 author:Benjamin Doerr, Daniel Johannsen, Timo Kötzing, Frank Neumann, Madeleine Theile category:cs.NE  published:2012-07-02 summary:The all-pairs shortest path problem is the first non-artificial problem for which it was shown that adding crossover can significantly speed up a mutation-only evolutionary algorithm. Recently, the analysis of this algorithm was refined and it was shown to have an expected optimization time (w.r.t. the number of fitness evaluations) of $\Theta(n^{3.25}(\log n)^{0.25})$. In contrast to this simple algorithm, evolutionary algorithms used in practice usually employ refined recombination strategies in order to avoid the creation of infeasible offspring. We study extensions of the basic algorithm by two such concepts which are central in recombination, namely \emph{repair mechanisms} and \emph{parent selection}. We show that repairing infeasible offspring leads to an improved expected optimization time of $\mathord{O}(n^{3.2}(\log n)^{0.2})$. As a second part of our study we prove that choosing parents that guarantee feasible offspring results in an even better optimization time of $\mathord{O}(n^{3}\log n)$. Both results show that already simple adjustments of the recombination operator can asymptotically improve the runtime of evolutionary algorithms. version:1
arxiv-1201-2605 | Autonomous Cleaning of Corrupted Scanned Documents - A Generative Modeling Approach | http://arxiv.org/abs/1201.2605 | id:1201.2605 author:Zhenwen Dai, Jörg Lücke category:cs.CV cs.LG  published:2012-01-12 summary:We study the task of cleaning scanned text documents that are strongly corrupted by dirt such as manual line strokes, spilled ink etc. We aim at autonomously removing dirt from a single letter-size page based only on the information the page contains. Our approach, therefore, has to learn character representations without supervision and requires a mechanism to distinguish learned representations from irregular patterns. To learn character representations, we use a probabilistic generative model parameterizing pattern features, feature variances, the features' planar arrangements, and pattern frequencies. The latent variables of the model describe pattern class, pattern position, and the presence or absence of individual pattern features. The model parameters are optimized using a novel variational EM approximation. After learning, the parameters represent, independently of their absolute position, planar feature arrangements and their variances. A quality measure defined based on the learned representation then allows for an autonomous discrimination between regular character patterns and the irregular patterns making up the dirt. The irregular patterns can thus be removed to clean the document. For a full Latin alphabet we found that a single page does not contain sufficiently many character examples. However, even if heavily corrupted by dirt, we show that a page containing a lower number of character types can efficiently and autonomously be cleaned solely based on the structural regularity of the characters it contains. In different examples using characters from different alphabets, we demonstrate generality of the approach and discuss its implications for future developments. version:2
arxiv-1110-4304 | Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression | http://arxiv.org/abs/1110.4304 | id:1110.4304 author:Ján Dolinský, Kei Hirose, Sadanori Konishi category:stat.ML stat.ME  published:2011-10-19 summary:Echo state network (ESN) is viewed as a temporal non-orthogonal expansion with pseudo-random parameters. Such expansions naturally give rise to regressors of various relevance to a teacher output. We illustrate that often only a certain amount of the generated echo-regressors effectively explain the variance of the teacher output and also that sole local regularization is not able to provide in-depth information concerning the importance of the generated regressors. The importance is therefore determined by a joint calculation of the individual variance contributions and Bayesian relevance using locally regularized orthogonal forward regression (LROFR) algorithm. This information can be advantageously used in a variety of ways for an in-depth analysis of an ESN structure and its state-space parameters in relation to the unknown dynamics of the underlying problem. We present locally regularized linear readout built using LROFR. The readout may have a different dimensionality than an ESN model itself, and besides improving robustness and accuracy of an ESN it relates the echo-regressors to different features of the training data and may determine what type of an additional readout is suitable for a task at hand. Moreover, as flexibility of the linear readout has limitations and might sometimes be insufficient for certain tasks, we also present a radial basis function (RBF) readout built using LROFR. It is a flexible and parsimonious readout with excellent generalization abilities and is a viable alternative to readouts based on a feed-forward neural network (FFNN) or an RBF net built using relevance vector machine (RVM). version:3
arxiv-0911-3280 | Automated languages phylogeny from Levenshtein distance | http://arxiv.org/abs/0911.3280 | id:0911.3280 author:Maurizio Serva category:cs.CL q-bio.PE q-bio.QM  published:2009-11-17 summary:Languages evolve over time in a process in which reproduction, mutation and extinction are all possible, similar to what happens to living organisms. Using this similarity it is possible, in principle, to build family trees which show the degree of relatedness between languages. The method used by modern glottochronology, developed by Swadesh in the 1950s, measures distances from the percentage of words with a common historical origin. The weak point of this method is that subjective judgment plays a relevant role. Recently we proposed an automated method that avoids the subjectivity, whose results can be replicated by studies that use the same database and that doesn't require a specific linguistic knowledge. Moreover, the method allows a quick comparison of a large number of languages. We applied our method to the Indo-European and Austronesian families, considering in both cases, fifty different languages. The resulting trees are similar to those of previous studies, but with some important differences in the position of few languages and subgroups. We believe that these differences carry new information on the structure of the tree and on the phylogenetic relationships within families. version:7
arxiv-1207-0268 | Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses | http://arxiv.org/abs/1207.0268 | id:1207.0268 author:Shivani Agarwal category:cs.LG stat.ML I.2.6  published:2012-07-02 summary:The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we show that such (non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011). version:1
arxiv-1203-1078 | Sequential Design for Computer Experiments with a Flexible Bayesian Additive Model | http://arxiv.org/abs/1203.1078 | id:1203.1078 author:Hugh Chipman, Pritam Ranjan, Weiwei Wang category:stat.ME stat.ML  published:2012-03-06 summary:In computer experiments, a mathematical model implemented on a computer is used to represent complex physical phenomena. These models, known as computer simulators, enable experimental study of a virtual representation of the complex phenomena. Simulators can be thought of as complex functions that take many inputs and provide an output. Often these simulators are themselves expensive to compute, and may be approximated by "surrogate models" such as statistical regression models. In this paper we consider a new kind of surrogate model, a Bayesian ensemble of trees (Chipman et al. 2010), with the specific goal of learning enough about the simulator that a particular feature of the simulator can be estimated. We focus on identifying the simulator's global minimum. Utilizing the Bayesian version of the Expected Improvement criterion (Jones et al. 1998), we show that this ensemble is particularly effective when the simulator is ill-behaved, exhibiting nonstationarity or abrupt changes in the response. A number of illustrations of the approach are given, including a tidal power application. version:2
arxiv-1207-0170 | Single parameter galaxy classification: The Principal Curve through the multi-dimensional space of galaxy properties | http://arxiv.org/abs/1207.0170 | id:1207.0170 author:M. Taghizadeh-Popp, S. Heinis, A. S. Szalay category:astro-ph.CO cs.CV stat.ML  published:2012-07-01 summary:We propose to describe the variety of galaxies from SDSS by using only one affine parameter. To this aim, we build the Principal Curve (P-curve) passing through the spine of the data point cloud, considering the eigenspace derived from Principal Component Analysis of morphological, physical and photometric galaxy properties. Thus, galaxies can be labeled, ranked and classified by a single arc length value of the curve, measured at the unique closest projection of the data points on the P-curve. We find that the P-curve has a "W" letter shape with 3 turning points, defining 4 branches that represent distinct galaxy populations. This behavior is controlled mainly by 2 properties, namely u-r and SFR. We further present the variations of several galaxy properties as a function of arc length. Luminosity functions variate from steep Schechter fits at low arc length, to double power law and ending in Log-normal fits at high arc length. Galaxy clustering shows increasing autocorrelation power at large scales as arc length increases. PCA analysis allowed to find peculiar galaxy populations located apart from the main cloud of data points, such as small red galaxies dominated by a disk, of relatively high stellar mass-to-light ratio and surface mass density. The P-curve allows not only dimensionality reduction, but also provides supporting evidence for relevant physical models and scenarios in extragalactic astronomy: 1) Evidence for the hierarchical merging scenario in the formation of a selected group of red massive galaxies. These galaxies present a log-normal r-band luminosity function, which might arise from multiplicative processes involved in this scenario. 2) Connection between the onset of AGN activity and star formation quenching, which appears in green galaxies when transitioning from blue to red populations. (Full abstract in downloadable version) version:1
arxiv-1207-0151 | Differentiable Pooling for Hierarchical Feature Learning | http://arxiv.org/abs/1207.0151 | id:1207.0151 author:Matthew D. Zeiler, Rob Fergus category:cs.CV cs.LG  published:2012-06-30 summary:We introduce a parametric form of pooling, based on a Gaussian, which can be optimized alongside the features in a single global objective function. By contrast, existing pooling schemes are based on heuristics (e.g. local maximum) and have no clear link to the cost function of the model. Furthermore, the variables of the Gaussian explicitly store location information, distinct from the appearance captured by the features, thus providing a what/where decomposition of the input signal. Although the differentiable pooling scheme can be incorporated in a wide range of hierarchical models, we demonstrate it in the context of a Deconvolutional Network model (Zeiler et al. ICCV 2011). We also explore a number of secondary issues within this model and present detailed experiments on MNIST digits. version:1
arxiv-1204-0991 | Distributed Robust Power System State Estimation | http://arxiv.org/abs/1204.0991 | id:1204.0991 author:Vassilis Kekatos, Georgios B. Giannakis category:stat.ML math.OC  published:2012-04-04 summary:Deregulation of energy markets, penetration of renewables, advanced metering capabilities, and the urge for situational awareness, all call for system-wide power system state estimation (PSSE). Implementing a centralized estimator though is practically infeasible due to the complexity scale of an interconnection, the communication bottleneck in real-time monitoring, regional disclosure policies, and reliability issues. In this context, distributed PSSE methods are treated here under a unified and systematic framework. A novel algorithm is developed based on the alternating direction method of multipliers. It leverages existing PSSE solvers, respects privacy policies, exhibits low communication load, and its convergence to the centralized estimates is guaranteed even in the absence of local observability. Beyond the conventional least-squares based PSSE, the decentralized framework accommodates a robust state estimator. By exploiting interesting links to the compressive sampling advances, the latter jointly estimates the state and identifies corrupted measurements. The novel algorithms are numerically evaluated using the IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate that the attainable accuracy can be reached within a few inter-area exchanges, while largest residual tests are outperformed. version:2
arxiv-1203-0697 | Learning High-Dimensional Mixtures of Graphical Models | http://arxiv.org/abs/1203.0697 | id:1203.0697 author:A. Anandkumar, D. Hsu, F. Huang, S. M. Kakade category:stat.ML cs.AI cs.LG  published:2012-03-04 summary:We consider unsupervised estimation of mixtures of discrete graphical models, where the class variable corresponding to the mixture components is hidden and each mixture component over the observed variables can have a potentially different Markov graph structure and parameters. We propose a novel approach for estimating the mixture components, and our output is a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. Our method is efficient when the union graph, which is the union of the Markov graphs of the mixture components, has sparse vertex separators between any pair of observed variables. This includes tree mixtures and mixtures of bounded degree graphs. For such models, we prove that our method correctly recovers the union graph structure and the tree structures corresponding to maximum-likelihood tree approximations of the mixture components. The sample and computational complexities of our method scale as $\poly(p, r)$, for an $r$-component mixture of $p$-variate graphical models. We further extend our results to the case when the union graph has sparse local separators between any pair of observed variables, such as mixtures of locally tree-like graphs, and the mixture components are in the regime of correlation decay. version:2
arxiv-1204-1664 | Optimally-Weighted Herding is Bayesian Quadrature | http://arxiv.org/abs/1204.1664 | id:1204.1664 author:Ferenc Huszár, David Duvenaud category:stat.ML math.NA G.1.4  published:2012-04-07 summary:Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate. version:2
arxiv-1207-0099 | Density-Difference Estimation | http://arxiv.org/abs/1207.0099 | id:1207.0099 author:Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus Christoffel du Plessis, Song Liu, Ichiro Takeuchi category:cs.LG stat.ML  published:2012-06-30 summary:We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. The usefulness of the proposed method is also demonstrated experimentally. version:1
arxiv-1205-2874 | Decoupling Exploration and Exploitation in Multi-Armed Bandits | http://arxiv.org/abs/1205.2874 | id:1205.2874 author:Orly Avner, Shie Mannor, Ohad Shamir category:cs.LG  published:2012-05-13 summary:We consider a multi-armed bandit problem where the decision maker can explore and exploit different arms at every round. The exploited arm adds to the decision maker's cumulative reward (without necessarily observing the reward) while the explored arm reveals its value. We devise algorithms for this setup and show that the dependence on the number of arms, k, can be much better than the standard square root of k dependence, depending on the behavior of the arms' reward sequences. For the important case of piecewise stationary stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling policy, which we show is essential to the success of any algorithm in the adversarial setup. Finally, we show some simulation results on an ultra-wide band channel selection inspired setting indicating the applicability of our algorithms. version:3
arxiv-1207-0057 | Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders | http://arxiv.org/abs/1207.0057 | id:1207.0057 author:Yoshua Bengio, Guillaume Alain, Salah Rifai category:cs.LG stat.ML  published:2012-06-30 summary:Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper contributes to the mathematical understanding of this phenomenon and helps define better justified sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density. First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures estimators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed sampling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder. version:1
arxiv-1206-7112 | A Hybrid Method for Distance Metric Learning | http://arxiv.org/abs/1206.7112 | id:1206.7112 author:Yi-Hao Kao, Benjamin Van Roy, Daniel Rubin, Jiajing Xu, Jessica Faruque, Sandy Napel category:cs.LG cs.IR stat.ML  published:2012-06-29 summary:We consider the problem of learning a measure of distance among vectors in a feature space and propose a hybrid method that simultaneously learns from similarity ratings assigned to pairs of vectors and class labels assigned to individual vectors. Our method is based on a generative model in which class labels can provide information that is not encoded in feature vectors but yet relates to perceived similarity between objects. Experiments with synthetic data as well as a real medical image retrieval problem demonstrate that leveraging class labels through use of our method improves retrieval performance significantly. version:1
arxiv-1206-1208 | Cumulative Step-size Adaptation on Linear Functions: Technical Report | http://arxiv.org/abs/1206.1208 | id:1206.1208 author:Alexandre Adrien Chotard, Anne Auger, Nikolaus Hansen category:cs.LG  published:2012-06-06 summary:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied. version:2
arxiv-1207-7244 | Visual Vocabulary Learning and Its Application to 3D and Mobile Visual Search | http://arxiv.org/abs/1207.7244 | id:1207.7244 author:Liujuan Cao category:cs.CV  published:2012-06-29 summary:In this technical report, we review related works and recent trends in visual vocabulary based web image search, object recognition, mobile visual search, and 3D object retrieval. Especial focuses would be also given for the recent trends in supervised/unsupervised vocabulary optimization, compact descriptor for visual search, as well as in multi-view based 3D object representation. version:1
arxiv-1207-3749 | Preliminary Design of Debris Removal Missions by Means of Simplified Models for Low-Thrust, Many-Revolution Transfers | http://arxiv.org/abs/1207.3749 | id:1207.3749 author:Federico Zuiani, Massimiliano Vasile category:math.OC cs.NE  published:2012-06-29 summary:This paper presents a novel approach for the preliminary design of Low-Thrust, many-revolution transfers. The main feature of the novel approach is a considerable reduction in the control parameters and a consequent gain in computational speed. Each spiral is built by using a predefined pattern for thrust direction and switching structure. The pattern is then optimised to minimise propellant consumption and transfer time. The variation of the orbital elements due to the thrust is computed analytically from a first-order solution of the perturbed Keplerian motion. The proposed approach allows for a realistic estimation of {\Delta}V and time of flight required to transfer a spacecraft between two arbitrary orbits. Eccentricity and plane changes are both accounted for. The novel approach is applied here to the design of missions for the removal of space debris by means of an Ion Beam Shepherd Spacecraft. In particular, two slightly different variants of the proposed low-thrust control model are used for the different phases of the mission. Thanks to their low computational cost they can be included in a multiobjective optimisation problem in which the sequence and timing of the removal of five pieces of debris are optimised to minimise propellant consumption and mission duration. version:1
arxiv-1005-4717 | Smoothing proximal gradient method for general structured sparse regression | http://arxiv.org/abs/1005.4717 | id:1005.4717 author:Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing category:stat.ML cs.LG math.OC stat.AP stat.CO  published:2010-05-26 summary:We study the problem of estimating high-dimensional regression models regularized by a structured sparsity-inducing penalty that encodes prior structural information on either the input or output variables. We consider two widely adopted types of penalties of this kind as motivating examples: (1) the general overlapping-group-lasso penalty, generalized from the group-lasso penalty; and (2) the graph-guided-fused-lasso penalty, generalized from the fused-lasso penalty. For both types of penalties, due to their nonseparability and nonsmoothness, developing an efficient optimization method remains a challenging problem. In this paper we propose a general optimization approach, the smoothing proximal gradient (SPG) method, which can solve structured sparse regression problems with any smooth convex loss under a wide spectrum of structured sparsity-inducing penalties. Our approach combines a smoothing technique with an effective proximal gradient method. It achieves a convergence rate significantly faster than the standard first-order methods, subgradient methods, and is much more scalable than the most widely used interior-point methods. The efficiency and scalability of our method are demonstrated on both simulation experiments and real genetic data sets. version:4
arxiv-1206-6883 | Learning Neighborhoods for Metric Learning | http://arxiv.org/abs/1206.6883 | id:1206.6883 author:Jun Wang, Adam Woznica, Alexandros Kalousis category:cs.LG  published:2012-06-28 summary:Metric learning methods have been shown to perform well on different learning tasks. Many of them rely on target neighborhood relationships that are computed in the original feature space and remain fixed throughout learning. As a result, the learned metric reflects the original neighborhood relations. We propose a novel formulation of the metric learning problem in which, in addition to the metric, the target neighborhood relations are also learned in a two-step iterative approach. The new formulation can be seen as a generalization of many existing metric learning methods. The formulation includes a target neighbor assignment rule that assigns different numbers of neighbors to instances according to their quality; `high quality' instances get more neighbors. We experiment with two of its instantiations that correspond to the metric learning algorithms LMNN and MCML and compare it to other metric learning methods on a number of datasets. The experimental results show state-of-the-art performance and provide evidence that learning the neighborhood relations does improve predictive performance. version:1
arxiv-1206-6735 | Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing | http://arxiv.org/abs/1206.6735 | id:1206.6735 author:Shay B. Cohen, Carlos Gómez-Rodríguez, Giorgio Satta category:cs.CL cs.AI  published:2012-06-28 summary:We present a novel technique to remove spurious ambiguity from transition systems for dependency parsing. Our technique chooses a canonical sequence of transition operations (computation) for a given dependency tree. Our technique can be applied to a large class of bottom-up transition systems, including for instance Nivre (2004) and Attardi (2006). version:1
arxiv-1206-6722 | Piecewise Linear Topology, Evolutionary Algorithms, and Optimization Problems | http://arxiv.org/abs/1206.6722 | id:1206.6722 author:Andrew Clark category:cs.NE math.GN math.OC I.6.1  published:2012-06-28 summary:Schemata theory, Markov chains, and statistical mechanics have been used to explain how evolutionary algorithms (EAs) work. Incremental success has been achieved with all of these methods, but each has been stymied by limitations related to its less-than-global view. We show that moving the investigation into topological space improves our understanding of why EAs work. version:1
arxiv-1112-0467 | Merging Belief Propagation and the Mean Field Approximation: A Free Energy Approach | http://arxiv.org/abs/1112.0467 | id:1112.0467 author:Erwin Riegler, Gunvor Elisabeth Kirkelund, Carles Navarro Manchón, Mihai-Alin Badiu, Bernard Henry Fleury category:cs.IT math.IT stat.ML  published:2011-12-02 summary:We present a joint message passing approach that combines belief propagation and the mean field approximation. Our analysis is based on the region-based free energy approximation method proposed by Yedidia et al. We show that the message passing fixed-point equations obtained with this combination correspond to stationary points of a constrained region-based free energy approximation. Moreover, we present a convergent implementation of these message passing fixedpoint equations provided that the underlying factor graph fulfills certain technical conditions. In addition, we show how to include hard constraints in the part of the factor graph corresponding to belief propagation. Finally, we demonstrate an application of our method to iterative channel estimation and decoding in an orthogonal frequency division multiplexing (OFDM) system. version:3
arxiv-1204-3251 | Plug-in martingales for testing exchangeability on-line | http://arxiv.org/abs/1204.3251 | id:1204.3251 author:Valentina Fedorova, Alex Gammerman, Ilia Nouretdinov, Vladimir Vovk category:cs.LG stat.ME 62G10 I.2.6  published:2012-04-15 summary:A standard assumption in machine learning is the exchangeability of data, which is equivalent to assuming that the examples are generated from the same probability distribution independently. This paper is devoted to testing the assumption of exchangeability on-line: the examples arrive one by one, and after receiving each example we would like to have a valid measure of the degree to which the assumption of exchangeability has been falsified. Such measures are provided by exchangeability martingales. We extend known techniques for constructing exchangeability martingales and show that our new method is competitive with the martingales introduced before. Finally we investigate the performance of our testing method on two benchmark datasets, USPS and Statlog Satellite data; for the former, the known techniques give satisfactory results, but for the latter our new more flexible method becomes necessary. version:2
arxiv-1206-6230 | Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena | http://arxiv.org/abs/1206.6230 | id:1206.6230 author:Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan, Ali Oran, Patrick Jaillet, John M. Dolan, Gaurav S. Sukhatme category:cs.LG cs.AI cs.DC cs.MA cs.RO  published:2012-06-27 summary:The problem of modeling and predicting spatiotemporal traffic phenomena over an urban road network is important to many traffic applications such as detecting and forecasting congestion hotspots. This paper presents a decentralized data fusion and active sensing (D2FAS) algorithm for mobile sensors to actively explore the road network to gather and assimilate the most informative data for predicting the traffic phenomenon. We analyze the time and communication complexity of D2FAS and demonstrate that it can scale well with a large number of observations and sensors. We provide a theoretical guarantee on its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the Gaussian process (GP) model: The computation of such a sparse approximate GP model can thus be parallelized and distributed among the mobile sensors (in a Google-like MapReduce paradigm), thereby achieving efficient and scalable prediction. We also theoretically guarantee its active sensing performance that improves under various practical environmental conditions. Empirical evaluation on real-world urban road network data shows that our D2FAS algorithm is significantly more time-efficient and scalable than state-of-the-art centralized algorithms while achieving comparable predictive performance. version:2
arxiv-1112-5016 | A Scalable Bootstrap for Massive Data | http://arxiv.org/abs/1112.5016 | id:1112.5016 author:Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan category:stat.ME stat.CO stat.ML  published:2011-12-21 summary:The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets---which are increasingly prevalent---the computation of bootstrap-based quantities can be prohibitively demanding computationally. While variants such as subsampling and the $m$ out of $n$ bootstrap can be used in principle to reduce the cost of bootstrap computations, we find that these methods are generally not robust to specification of hyperparameters (such as the number of subsampled data points), and they often require use of more prior information (such as rates of convergence of estimators) than the bootstrap. As an alternative, we introduce the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient means of assessing the quality of estimators. BLB is well suited to modern parallel and distributed computing architectures and furthermore retains the generic applicability and statistical efficiency of the bootstrap. We demonstrate BLB's favorable statistical performance via a theoretical analysis elucidating the procedure's properties, as well as a simulation study comparing BLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In addition, we present results from a large-scale distributed implementation of BLB demonstrating its computational superiority on massive data, a method for adaptively selecting BLB's hyperparameters, an empirical study applying BLB to several real datasets, and an extension of BLB to time series data. version:2
arxiv-1206-6519 | A Permutation Approach to Testing Interactions in Many Dimensions | http://arxiv.org/abs/1206.6519 | id:1206.6519 author:Noah Simon, Robert Tibshirani category:stat.ML stat.CO stat.ME  published:2012-06-27 summary:To date, testing interactions in high dimensions has been a challenging task. Existing methods often have issues with sensitivity to modeling assumptions and heavily asymptotic nominal p-values. To help alleviate these issues, we propose a permutation-based method for testing marginal interactions with a binary response. Our method searches for pairwise correlations which differ between classes. In this manuscript, we compare our method on real and simulated data to the standard approach of running many pairwise logistic models. On simulated data our method finds more significant interactions at a lower false discovery rate (especially in the presence of main effects). On real genomic data, although there is no gold standard, our method finds apparent signal and tells a believable story, while logistic regression does not. We also give asymptotic consistency results under not too restrictive assumptions. version:1
arxiv-1206-6514 | Investigation of Color Constancy for Ubiquitous Wireless LAN/Camera Positioning: An Initial Outcome | http://arxiv.org/abs/1206.6514 | id:1206.6514 author:Wan Mohd Yaakob Wan Bejuri, Mohd Murtadha Mohamad, Maimunah Sapri, Mohd Adly Rosly category:cs.CV cs.HC  published:2012-06-27 summary:This paper present our color constancy investigation in the hybridization of Wireless LAN and Camera positioning in the mobile phone. Five typical color constancy schemes are analyzed in different location environment. The results can be used to combine with RF signals from Wireless LAN positioning by using model fitting approach in order to establish absolute positioning output. There is no conventional searching algorithm required, thus it is expected to reduce the complexity of computation. Finally we present our preliminary results to illustrate the indoor positioning algorithm performance evaluation for an indoor environment set-up. version:1
arxiv-1206-6380 | Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring | http://arxiv.org/abs/1206.6380 | id:1206.6380 author:Sungjin Ahn, Anoop Korattikara, Max Welling category:cs.LG stat.CO stat.ML  published:2012-06-27 summary:In this paper we address the following question: Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?. An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in. version:1
arxiv-1206-6382 | High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains | http://arxiv.org/abs/1206.6382 | id:1206.6382 author:Majid Janzamin, Animashree Anandkumar category:cs.LG stat.ML  published:2012-06-27 summary:In this paper, we present a novel framework incorporating a combination of sparse models in different domains. We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We provide efficient methods for decomposition of the data into two domains, \viz Markov and independence domains. We characterize a set of sufficient conditions for identifiability and model consistency. Our decomposition method is based on a simple modification of the popular $\ell_1$-penalized maximum-likelihood estimator ($\ell_1$-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples $n$ scales as $n = \Omega(d^2 \log p)$, where $p$ is the number of variables and $d$ is the maximum node degree in the Markov model. Our conditions for recovery are comparable to those of $\ell_1$-MLE for consistent estimation of a sparse Markov model, and thus, we guarantee successful high-dimensional estimation of a richer class of models under comparable conditions. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation. version:1
arxiv-1206-6383 | Feature Selection via Probabilistic Outputs | http://arxiv.org/abs/1206.6383 | id:1206.6383 author:Andrea Danyluk, Nicholas Arnosti category:cs.LG stat.ML  published:2012-06-27 summary:This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by \citet{shen} and a complementary approach proposed below. We develop a theoretical framework to analyze each criterion and show that both estimate the spread (across all values of a given feature) of the probability that an example belongs to the positive class. Based on our analysis, we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions. version:1
arxiv-1206-6384 | Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization | http://arxiv.org/abs/1206.6384 | id:1206.6384 author:Haim Avron, Satyen Kale, Shiva Kasiviswanathan, Vikas Sindhwani category:cs.LG stat.ML  published:2012-06-27 summary:We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization. Unlike existing approaches, our method executes very cheap iterations by combining low-rank stochastic subgradients with efficient incremental SVD updates, made possible by highly optimized and parallelizable dense linear algebra operations on small matrices. Our practical algorithms always maintain a low-rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings. Empirical comparisons confirm that our approach is highly competitive with several recently proposed state-of-the-art solvers for such problems. version:1
arxiv-1206-6385 | Improved Estimation in Time Varying Models | http://arxiv.org/abs/1206.6385 | id:1206.6385 author:Doina Precup, Philip Bachman category:cs.LG stat.ME stat.ML  published:2012-06-27 summary:Locally adapted parameterizations of a model (such as locally weighted regression) are expressive but often suffer from high variance. We describe an approach for reducing the variance, based on the idea of estimating simultaneously a transformed space for the model, as well as locally adapted parameterizations in this new space. We present a new problem formulation that captures this idea and illustrate it in the important context of time varying models. We develop an algorithm for learning a set of bases for approximating a time varying sparse network; each learned basis constitutes an archetypal sparse network structure. We also provide an extension for learning task-driven bases. We present empirical results on synthetic data sets, as well as on a BCI EEG classification task. version:1
arxiv-1206-6386 | How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing | http://arxiv.org/abs/1206.6386 | id:1206.6386 author:Yoram Bachrach, Thore Graepel, Tom Minka, John Guiver category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:We propose a new probabilistic graphical model that jointly models the difficulties of questions, the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings. We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario. version:1
arxiv-1206-6387 | Fast classification using sparse decision DAGs | http://arxiv.org/abs/1206.6387 | id:1206.6387 author:Djalel Benbouzid, Robert Busa-Fekete, Balazs Kegl category:cs.LG stat.ML  published:2012-06-27 summary:In this paper we propose an algorithm that builds sparse decision DAGs (directed acyclic graphs) from a list of base classifiers provided by an external learning method such as AdaBoost. The basic idea is to cast the DAG design task as a Markov decision process. Each instance can decide to use or to skip each base classifier, based on the current state of the classifier being built. The result is a sparse decision DAG where the base classifiers are selected in a data-dependent way. The method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off. The algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks, and it clearly outperforms them when there is a small number of base classifiers. Unlike cascades, it is also readily applicable for multi-class classification. Using the multi-class setup, we show on a benchmark web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker. version:1
arxiv-1206-6388 | Canonical Trends: Detecting Trend Setters in Web Data | http://arxiv.org/abs/1206.6388 | id:1206.6388 author:Felix Biessmann, Jens-Michalis Papaioannou, Mikio Braun, Andreas Harth category:cs.LG cs.SI stat.ML  published:2012-06-27 summary:Much information available on the web is copied, reused or rephrased. The phenomenon that multiple web sources pick up certain information is often called trend. A central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend. We present a simple and efficient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others. We validate our approach on real data collected from influential technology news feeds. version:1
arxiv-1206-6390 | Incorporating Causal Prior Knowledge as Path-Constraints in Bayesian Networks and Maximal Ancestral Graphs | http://arxiv.org/abs/1206.6390 | id:1206.6390 author:Giorgos Borboudakis, Ioannis Tsamardinos category:cs.AI cs.CE cs.LG  published:2012-06-27 summary:We consider the incorporation of causal knowledge about the presence or absence of (possibly indirect) causal relations into a causal model. Such causal relations correspond to directed paths in a causal model. This type of knowledge naturally arises from experimental data, among others. Specifically, we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral Graphs and their Markov equivalence classes: Partially Directed Acyclic Graphs and Partially Oriented Ancestral Graphs. We introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models. In simulated experiments, we show that often considering even a few causal facts leads to a significant number of new inferences. In a case study, we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network. The code is available at mensxmachina.org. version:1
arxiv-1206-6391 | Gaussian Process Quantile Regression using Expectation Propagation | http://arxiv.org/abs/1206.6391 | id:1206.6391 author:Alexis Boukouvalas, Remi Barillec, Dan Cornford category:stat.ME cs.LG stat.AP  published:2012-06-27 summary:Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables. We present a new framework for direct quantile regression where a Gaussian process model is learned, minimising the expected tilted loss function. The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm. We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets. The method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full Gaussian process probabilistic framework. version:1
arxiv-1206-6392 | Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription | http://arxiv.org/abs/1206.6392 | id:1206.6392 author:Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent category:cs.LG cs.SD stat.ML  published:2012-06-27 summary:We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription. version:1
arxiv-1206-6393 | Local Loss Optimization in Operator Models: A New Insight into Spectral Learning | http://arxiv.org/abs/1206.6393 | id:1206.6393 author:Borja Balle, Ariadna Quattoni, Xavier Carreras category:cs.LG stat.ML  published:2012-06-27 summary:This paper re-visits the spectral method for learning latent variable models defined in terms of observable operators. We give a new perspective on the method, showing that operators can be recovered by minimizing a loss defined on a finite subset of the domain. A non-convex optimization similar to the spectral method is derived. We also propose a regularized convex relaxation of this optimization. We show that in practice the availabilty of a continuous regularization parameter (in contrast with the discrete number of states in the original method) allows a better trade-off between accuracy and model complexity. We also prove that in general, a randomized strategy for choosing the local loss will succeed with high probability. version:1
arxiv-1206-6394 | Nonparametric Link Prediction in Dynamic Networks | http://arxiv.org/abs/1206.6394 | id:1206.6394 author:Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan category:cs.LG cs.SI stat.ML  published:2012-06-27 summary:We propose a non-parametric link prediction algorithm for a sequence of graph snapshots over time. The model predicts links based on the features of its endpoints, as well as those of the local neighborhood around the endpoints. This allows for different types of neighborhoods in a graph, each with its own dynamics (e.g, growing or shrinking communities). We prove the consistency of our estimator, and give a fast implementation based on locality-sensitive hashing. Experiments with simulated as well as five real-world dynamic graphs show that we outperform the state of the art, especially when sharp fluctuations or non-linearities are present. version:1
arxiv-1206-6395 | Convergence Rates for Differentially Private Statistical Estimation | http://arxiv.org/abs/1206.6395 | id:1206.6395 author:Kamalika Chaudhuri, Daniel Hsu category:cs.LG cs.CR stat.ML  published:2012-06-27 summary:Differential privacy is a cryptographically-motivated definition of privacy which has gained significant attention over the past few years. Differentially private solutions enforce privacy by adding random noise to a function computed over the data, and the challenge in designing such algorithms is to control the added noise in order to optimize the privacy-accuracy-sample size tradeoff. This work studies differentially-private statistical estimation, and shows upper and lower bounds on the convergence rates of differentially private approximations to statistical estimators. Our results reveal a formal connection between differential privacy and the notion of Gross Error Sensitivity (GES) in robust statistics, by showing that the convergence rate of any differentially private approximation to an estimator that is accurate over a large class of distributions has to grow with the GES of the estimator. We then provide an upper bound on the convergence rate of a differentially private approximation to an estimator with bounded range and bounded GES. We show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy. version:1
arxiv-1206-6396 | Joint Optimization and Variable Selection of High-dimensional Gaussian Processes | http://arxiv.org/abs/1206.6396 | id:1206.6396 author:Bo Chen, Rui Castro, Andreas Krause category:cs.LG stat.ML  published:2012-06-27 summary:Maximizing high-dimensional, non-convex functions through noisy observations is a notoriously hard problem, but one that arises in many applications. In this paper, we tackle this challenge by modeling the unknown function as a sample from a high-dimensional Gaussian process (GP) distribution. Assuming that the unknown function only depends on few relevant variables, we show that it is possible to perform joint variable selection and GP optimization. We provide strong performance guarantees for our algorithm, bounding the sample complexity of variable selection, and as well as providing cumulative regret bounds. We further provide empirical evidence on the effectiveness of our algorithm on several benchmark optimization problems. version:1
arxiv-1206-6397 | Communications Inspired Linear Discriminant Analysis | http://arxiv.org/abs/1206.6397 | id:1206.6397 author:Minhua Chen, William Carson, Miguel Rodrigues, Robert Calderbank, Lawrence Carin category:cs.LG stat.ML  published:2012-06-27 summary:We study the problem of supervised linear dimensionality reduction, taking an information-theoretic viewpoint. The linear projection matrix is designed by maximizing the mutual information between the projected signal and the class label (based on a Shannon entropy measure). By harnessing a recent theoretical result on the gradient of mutual information, the above optimization problem can be solved directly using gradient descent, without requiring simplification of the objective function. Theoretical analysis and empirical comparison are made between the proposed method and two closely related methods (Linear Discriminant Analysis and Information Discriminant Analysis), and comparisons are also made with a method in which Renyi entropy is used to define the mutual information (in this case the gradient may be computed simply, under a special parameter setting). Relative to these alternative approaches, the proposed method achieves promising results on real datasets. version:1
arxiv-1206-6399 | Demand-Driven Clustering in Relational Domains for Predicting Adverse Drug Events | http://arxiv.org/abs/1206.6399 | id:1206.6399 author:Jesse Davis, Vitor Santos Costa, Peggy Peissig, Michael Caldwell, Elizabeth Berg, David Page category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:Learning from electronic medical records (EMR) is challenging due to their relational nature and the uncertain dependence between a patient's past and future health status. Statistical relational learning is a natural fit for analyzing EMRs but is less adept at handling their inherent latent structure, such as connections between related medications or diseases. One way to capture the latent structure is via a relational clustering of objects. We propose a novel approach that, instead of pre-clustering the objects, performs a demand-driven clustering during learning. We evaluate our algorithm on three real-world tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication. We find that our approach is more accurate than performing no clustering, pre-clustering, and using expert-constructed medical heterarchies. version:1
arxiv-1206-6400 | Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret | http://arxiv.org/abs/1206.6400 | id:1206.6400 author:Raman Arora, Ofer Dekel, Ambuj Tewari category:cs.LG stat.ML  published:2012-06-27 summary:Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm's ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm's actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm's performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary's memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret. version:1
arxiv-1206-6401 | Consistent Multilabel Ranking through Univariate Losses | http://arxiv.org/abs/1206.6401 | id:1206.6401 author:Krzysztof Dembczynski, Wojciech Kotlowski, Eyke Huellermeier category:cs.LG stat.ML  published:2012-06-27 summary:We consider the problem of rank loss minimization in the setting of multilabel classification, which is usually tackled by means of convex surrogate losses defined on pairs of labels. Very recently, this approach was put into question by a negative result showing that commonly used pairwise surrogate losses, such as exponential and logistic losses, are inconsistent. In this paper, we show a positive result which is arguably surprising in light of the previous one: the simpler univariate variants of exponential and logistic surrogates (i.e., defined on single labels) are consistent for rank loss minimization. Instead of directly proving convergence, we give a much stronger result by deriving regret bounds and convergence rates. The proposed losses suggest efficient and scalable algorithms, which are tested experimentally. version:1
arxiv-1206-6402 | Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization | http://arxiv.org/abs/1206.6402 | id:1206.6402 author:Thomas Desautels, Andreas Krause, Joel Burdick category:cs.LG stat.ML  published:2012-06-27 summary:Can one parallelize complex exploration exploitation tradeoffs? As an example, consider the problem of optimal high-throughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multi-armed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications. version:1
arxiv-1206-6403 | Two Step CCA: A new spectral method for estimating vector models of words | http://arxiv.org/abs/1206.6403 | id:1206.6403 author:Paramveer Dhillon, Jordan Rodu, Dean Foster, Lyle Ungar category:cs.CL cs.LG  published:2012-06-27 summary:Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank "dictionary" by an eigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification. version:1
arxiv-1206-6404 | Policy Gradients with Variance Related Risk Criteria | http://arxiv.org/abs/1206.6404 | id:1206.6404 author:Dotan Di Castro, Aviv Tamar, Shie Mannor category:cs.LG cs.CY math.OC stat.ML  published:2012-06-27 summary:Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-to-go in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem. version:1
arxiv-1206-6405 | Bounded Planning in Passive POMDPs | http://arxiv.org/abs/1206.6405 | id:1206.6405 author:Roy Fox, Naftali Tishby category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum. version:1
arxiv-1206-6406 | Bayesian Optimal Active Search and Surveying | http://arxiv.org/abs/1206.6406 | id:1206.6406 author:Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, Richard Mann category:cs.LG stat.ML  published:2012-06-27 summary:We consider two active binary-classification problems with atypical objectives. In the first, active search, our goal is to actively uncover as many members of a given class as possible. In the second, active surveying, our goal is to actively query points to ultimately predict the proportion of a given class. Numerous real-world problems can be framed in these terms, and in either case typical model-based concerns such as generalization error are only of secondary importance. We approach these problems via Bayesian decision theory; after choosing natural utility functions, we derive the optimal policies. We provide three contributions. In addition to introducing the active surveying problem, we extend previous work on active search in two ways. First, we prove a novel theoretical result, that less-myopic approximations to the optimal policy can outperform more-myopic approximations by any arbitrary degree. We then derive bounds that for certain models allow us to reduce (in practice dramatically) the exponential search space required by a na?ve implementation of the optimal policy, enabling further lookahead while still ensuring that optimal decisions are always made. version:1
arxiv-1206-6407 | Large-Scale Feature Learning With Spike-and-Slab Sparse Coding | http://arxiv.org/abs/1206.6407 | id:1206.6407 author:Ian Goodfellow, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML  published:2012-06-27 summary:We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models? Transfer Learning Challenge. version:1
arxiv-1206-6408 | Sequential Nonparametric Regression | http://arxiv.org/abs/1206.6408 | id:1206.6408 author:Haijie Gu, John Lafferty category:stat.ME astro-ph.IM cs.LG  published:2012-06-27 summary:We present algorithms for nonparametric regression in settings where the data are obtained sequentially. While traditional estimators select bandwidths that depend upon the sample size, for sequential data the effective sample size is dynamically changing. We propose a linear time algorithm that adjusts the bandwidth for each new data point, and show that the estimator achieves the optimal minimax rate of convergence. We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function. We provide simulations that confirm the theoretical results, and demonstrate the effectiveness of the methods. version:1
arxiv-1206-6409 | Scaling Up Coordinate Descent Algorithms for Large $\ell_1$ Regularization Problems | http://arxiv.org/abs/1206.6409 | id:1206.6409 author:Chad Scherrer, Mahantesh Halappanavar, Ambuj Tewari, David Haglin category:cs.LG cs.DC stat.ML  published:2012-06-27 summary:We present a generic framework for parallel coordinate descent (CD) algorithms that includes, as special cases, the original sequential algorithms Cyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm. We introduce two novel parallel algorithms that are also special cases---Thread-Greedy CD and Coloring-Based CD---and give performance measurements for an OpenMP implementation of these. version:1
arxiv-1206-6410 | On the Partition Function and Random Maximum A-Posteriori Perturbations | http://arxiv.org/abs/1206.6410 | id:1206.6410 author:Tamir Hazan, Tommi Jaakkola category:cs.LG stat.ML  published:2012-06-27 summary:In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical "high signal - high coupling" regime that results in ragged energy landscapes difficult for alternative approaches. version:1
arxiv-1206-6411 | On the Difficulty of Nearest Neighbor Search | http://arxiv.org/abs/1206.6411 | id:1206.6411 author:Junfeng He, Sanjiv Kumar, Shih-Fu Chang category:cs.LG cs.DB cs.IR stat.ML  published:2012-06-27 summary:Fast approximate nearest neighbor (NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure. version:1
arxiv-1206-6412 | A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound | http://arxiv.org/abs/1206.6412 | id:1206.6412 author:Ming Ji, Tianbao Yang, Binbin Lin, Rong Jin, Jiawei Han category:cs.LG stat.ML  published:2012-06-27 summary:In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study. version:1
arxiv-1206-6413 | A Convex Relaxation for Weakly Supervised Classifiers | http://arxiv.org/abs/1206.6413 | id:1206.6413 author:Armand Joulin, Francis Bach category:cs.LG stat.ML  published:2012-06-27 summary:This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning as well as on clustering tasks. version:1
arxiv-1206-6414 | The Nonparametric Metadata Dependent Relational Model | http://arxiv.org/abs/1206.6414 | id:1206.6414 author:Dae Il Kim, Michael Hughes, Erik Sudderth category:cs.LG cs.SI stat.ML  published:2012-06-27 summary:We introduce the nonparametric metadata dependent relational (NMDR) model, a Bayesian nonparametric stochastic block model for network data. The NMDR allows the entities associated with each node to have mixed membership in an unbounded collection of latent communities. Learned regression models allow these memberships to depend on, and be predicted from, arbitrary node metadata. We develop efficient MCMC algorithms for learning NMDR models from partially observed node relationships. Retrospective MCMC methods allow our sampler to work directly with the infinite stick-breaking representation of the NMDR, avoiding the need for finite truncations. Our results demonstrate recovery of useful latent communities from real-world social and ecological networks, and the usefulness of metadata in link prediction tasks. version:1
arxiv-1206-6415 | The Big Data Bootstrap | http://arxiv.org/abs/1206.6415 | id:1206.6415 author:Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael Jordan category:cs.LG stat.ML  published:2012-06-27 summary:The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB's behavior, including a study of its statistical correctness, its large-scale implementation and performance, selection of hyperparameters, and performance on real data. version:1
arxiv-1206-6416 | An Infinite Latent Attribute Model for Network Data | http://arxiv.org/abs/1206.6416 | id:1206.6416 author:Konstantina Palla, David Knowles, Zoubin Ghahramani category:cs.LG stat.ML  published:2012-06-27 summary:Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a "flat" clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks. version:1
arxiv-1206-6417 | Learning Task Grouping and Overlap in Multi-task Learning | http://arxiv.org/abs/1206.6417 | id:1206.6417 author:Abhishek Kumar, Hal Daume III category:cs.LG stat.ML  published:2012-06-27 summary:In the paradigm of multi-task learning, mul- tiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learn- ing that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combi- nation of a finite number of underlying basis tasks. The coefficients of the linear combina- tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on on the assumption that task pa- rameters within a group lie in a low dimen- sional subspace but allows the tasks in differ- ent groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods. version:1
arxiv-1206-6418 | Learning Invariant Representations with Local Transformations | http://arxiv.org/abs/1206.6418 | id:1206.6418 author:Kihyuk Sohn, Honglak Lee category:cs.LG cs.CV stat.ML  published:2012-06-27 summary:Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains. version:1
arxiv-1206-6419 | Cross-Domain Multitask Learning with Latent Probit Models | http://arxiv.org/abs/1206.6419 | id:1206.6419 author:Shaobo Han, Xuejun Liao, Lawrence Carin category:cs.LG stat.ML  published:2012-06-27 summary:Learning multiple tasks across heterogeneous domains is a challenging problem since the feature space may not be the same for different tasks. We assume the data in multiple tasks are generated from a latent common domain via sparse domain transforms and propose a latent probit model (LPM) to jointly learn the domain transforms, and the shared probit classifier in the common domain. To learn meaningful task relatedness and avoid over-fitting in classification, we introduce sparsity in the domain transforms matrices, as well as in the common classifier. We derive theoretical bounds for the estimation error of the classifier in terms of the sparsity of domain transforms. An expectation-maximization algorithm is derived for learning the LPM. The effectiveness of the approach is demonstrated on several real datasets. version:1
arxiv-1206-6420 | Distributed Parameter Estimation via Pseudo-likelihood | http://arxiv.org/abs/1206.6420 | id:1206.6420 author:Qiang Liu, Alexander Ihler category:cs.LG cs.DC stat.ML  published:2012-06-27 summary:Estimating statistical models within sensor networks requires distributed algorithms, in which both data and computation are distributed across the nodes of the network. We propose a general approach for distributed learning based on combining local estimators defined by pseudo-likelihood components, encompassing a number of combination methods, and provide both theoretical and experimental analysis. We show that simple linear combination or max-voting methods, when combined with second-order information, are statistically competitive with more advanced and costly joint optimization. Our algorithms have many attractive properties including low communication and computational cost and "any-time" behavior. version:1
arxiv-1206-6421 | Structured Learning from Partial Annotations | http://arxiv.org/abs/1206.6421 | id:1206.6421 author:Xinghua Lou, Fred Hamprecht category:cs.LG stat.ML  published:2012-06-27 summary:Structured learning is appropriate when predicting structured outputs such as trees, graphs, or sequences. Most prior work requires the training set to consist of complete trees, graphs or sequences. Specifying such detailed ground truth can be tedious or infeasible for large outputs. Our main contribution is a large margin formulation that makes structured learning from only partially annotated data possible. The resulting optimization problem is non-convex, yet can be efficiently solve by concave-convex procedure (CCCP) with novel speedup strategies. We apply our method to a challenging tracking-by-assignment problem of a variable number of divisible objects. On this benchmark, using only 25% of a full annotation we achieve a performance comparable to a model learned with a full annotation. Finally, we offer a unifying perspective of previous work using the hinge, ramp, or max loss for structured learning, followed by an empirical comparison on their practical performance. version:1
arxiv-1206-6422 | An Online Boosting Algorithm with Theoretical Justifications | http://arxiv.org/abs/1206.6422 | id:1206.6422 author:Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu category:cs.LG stat.ML  published:2012-06-27 summary:We study the task of online boosting--combining online weak learners into an online strong learner. While batch boosting has a sound theoretical foundation, online boosting deserves more study from the theoretical perspective. In this paper, we carefully compare the differences between online and batch boosting, and propose a novel and reasonable assumption for the online weak learner. Based on the assumption, we design an online boosting algorithm with a strong theoretical guarantee by adapting from the offline SmoothBoost algorithm that matches the assumption closely. We further tackle the task of deciding the number of weak learners using established theoretical results for online convex programming and predicting with expert advice. Experiments on real-world data sets demonstrate that the proposed algorithm compares favorably with existing online boosting algorithms. version:1
arxiv-1206-6423 | A Joint Model of Language and Perception for Grounded Attribute Learning | http://arxiv.org/abs/1206.6423 | id:1206.6423 author:Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, Dieter Fox category:cs.CL cs.LG cs.RO  published:2012-06-27 summary:As robots become more ubiquitous and capable, it becomes ever more important to enable untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world. In this paper, we present an approach for joint learning of language and perception models for grounded attribute induction. Our perception model includes attribute classifiers, for example to detect object color and shape, and the language model is based on a probabilistic categorial grammar that enables the construction of rich, compositional meaning representations. The approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace. We demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes. version:1
arxiv-1206-6424 | Anytime Marginal MAP Inference | http://arxiv.org/abs/1206.6424 | id:1206.6424 author:Denis Maua, Cassio De Campos category:cs.AI stat.ML  published:2012-06-27 summary:This paper presents a new anytime algorithm for the marginal MAP problem in graphical models. The algorithm is described in detail, its complexity and convergence rate are studied, and relations to previous theoretical results for the problem are discussed. It is shown that the algorithm runs in polynomial-time if the underlying graph of the model has bounded tree-width, and that it provides guarantees to the lower and upper bounds obtained within a fixed amount of computational resources. Experiments with both real and synthetic generated models highlight its main characteristics and show that it compares favorably against Park and Darwiche's systematic search, particularly in the case of problems with many MAP variables and moderate tree-width. version:1
arxiv-1206-6425 | Sparse Stochastic Inference for Latent Dirichlet allocation | http://arxiv.org/abs/1206.6425 | id:1206.6425 author:David Mimno, Matt Hoffman, David Blei category:cs.LG stat.ML  published:2012-06-27 summary:We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models. version:1
arxiv-1206-6426 | A Fast and Simple Algorithm for Training Neural Probabilistic Language Models | http://arxiv.org/abs/1206.6426 | id:1206.6426 author:Andriy Mnih, Yee Whye Teh category:cs.CL cs.LG  published:2012-06-27 summary:In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset. version:1
arxiv-1206-6427 | Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients | http://arxiv.org/abs/1206.6427 | id:1206.6427 author:Iftekhar Naim, Daniel Gildea category:cs.LG stat.ML  published:2012-06-27 summary:The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic anti-annealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach. version:1
arxiv-1206-6428 | A Binary Classification Framework for Two-Stage Multiple Kernel Learning | http://arxiv.org/abs/1206.6428 | id:1206.6428 author:Abhishek Kumar, Alexandru Niculescu-Mizil, Koray Kavukcuoglu, Hal Daume III category:cs.LG stat.ML  published:2012-06-27 summary:With the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. In this context, the Multiple Kernel Learning (MKL) problem of finding a combination of pre-specified base kernels that is suitable for the task at hand has received significant attention from researchers. In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel. Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler, and, arguably, more accessible to practitioners. Experiments on nine data sets from different domains show that, despite its simplicity, the proposed technique compares favorably with current leading MKL approaches. version:1
arxiv-1206-6429 | Incorporating Domain Knowledge in Matching Problems via Harmonic Analysis | http://arxiv.org/abs/1206.6429 | id:1206.6429 author:Deepti Pachauri, Maxwell Collins, Vikas SIngh, Risi Kondor category:cs.LG cs.CV stat.ML  published:2012-06-27 summary:Matching one set of objects to another is a ubiquitous task in machine learning and computer vision that often reduces to some form of the quadratic assignment problem (QAP). The QAP is known to be notoriously hard, both in theory and in practice. Here, we investigate if this difficulty can be mitigated when some additional piece of information is available: (a) that all QAP instances of interest come from the same application, and (b) the correct solution for a set of such QAP instances is given. We propose a new approach to accelerate the solution of QAPs based on learning parameters for a modified objective function from prior QAP instances. A key feature of our approach is that it takes advantage of the algebraic structure of permutations, in conjunction with special methods for optimizing functions over the symmetric group Sn in Fourier space. Experiments show that in practical domains the new method can outperform existing approaches. version:1
arxiv-1206-6430 | Variational Bayesian Inference with Stochastic Search | http://arxiv.org/abs/1206.6430 | id:1206.6430 author:John Paisley, David Blei, Michael Jordan category:cs.LG stat.CO stat.ML  published:2012-06-27 summary:Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP. version:1
arxiv-1206-6431 | Exact Maximum Margin Structure Learning of Bayesian Networks | http://arxiv.org/abs/1206.6431 | id:1206.6431 author:Robert Peharz, Franz Pernkopf category:cs.LG stat.ML  published:2012-06-27 summary:Recently, there has been much interest in finding globally optimal Bayesian network structures. These techniques were developed for generative scores and can not be directly extended to discriminative scores, as desired for classification. In this paper, we propose an exact method for finding network structures maximizing the probabilistic soft margin, a successfully applied discriminative score. Our method is based on branch-and-bound techniques within a linear programming framework and maintains an any-time solution, together with worst-case sub-optimality bounds. We apply a set of order constraints for enforcing the network structure to be acyclic, which allows a compact problem representation and the use of general-purpose optimization techniques. In classification experiments, our methods clearly outperform generatively trained network structures and compete with support vector machines. version:1
arxiv-1206-6432 | Sparse Support Vector Infinite Push | http://arxiv.org/abs/1206.6432 | id:1206.6432 author:Alain Rakotomamonjy category:cs.LG cs.CE stat.ML  published:2012-06-27 summary:In this paper, we address the problem of embedded feature selection for ranking on top of the list problems. We pose this problem as a regularized empirical risk minimization with $p$-norm push loss function ($p=\infty$) and sparsity inducing regularizers. We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer. Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator. Experimental results on toy, DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function. version:1
arxiv-1206-6433 | Copula Mixture Model for Dependency-seeking Clustering | http://arxiv.org/abs/1206.6433 | id:1206.6433 author:Melanie Rey, Volker Roth category:stat.ME cs.LG stat.ML  published:2012-06-27 summary:We introduce a copula mixture model to perform dependency-seeking clustering when co-occurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results. version:1
arxiv-1206-6434 | A Generative Process for Sampling Contractive Auto-Encoders | http://arxiv.org/abs/1206.6434 | id:1206.6434 author:Salah Rifai, Yoshua Bengio, Yann Dauphin, Pascal Vincent category:cs.LG stat.ML  published:2012-06-27 summary:The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error. version:1
arxiv-1206-6435 | Rethinking Collapsed Variational Bayes Inference for LDA | http://arxiv.org/abs/1206.6435 | id:1206.6435 author:Issei Sato, Hiroshi Nakagawa category:cs.LG stat.ML  published:2012-06-27 summary:We propose a novel interpretation of the collapsed variational Bayes inference with a zero-order Taylor expansion approximation, called CVB0 inference, for latent Dirichlet allocation (LDA). We clarify the properties of the CVB0 inference by using the alpha-divergence. We show that the CVB0 inference is composed of two different divergence projections: alpha=1 and -1. This interpretation will help shed light on CVB0 works. version:1
arxiv-1206-6436 | Efficient Structured Prediction with Latent Variables for General Graphical Models | http://arxiv.org/abs/1206.6436 | id:1206.6436 author:Alexander Schwing, Tamir Hazan, Marc Pollefeys, Raquel Urtasun category:cs.LG stat.ML  published:2012-06-27 summary:In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields. version:1
arxiv-1206-6437 | Large Scale Variational Bayesian Inference for Structured Scale Mixture Models | http://arxiv.org/abs/1206.6437 | id:1206.6437 author:Young Jun Ko, Matthias Seeger category:cs.CV cs.LG stat.ML  published:2012-06-27 summary:Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial "sparse" methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with non-factorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors. version:1
arxiv-1206-6438 | Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation | http://arxiv.org/abs/1206.6438 | id:1206.6438 author:Yuan Shi, Fei Sha category:cs.LG stat.ML  published:2012-06-27 summary:We study the problem of unsupervised domain adaptation, which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain. Many existing approaches first learn domain-invariant features and then construct classifiers with them. We propose a novel approach that jointly learn the both. Specifically, while the method identifies a feature space where data in the source and the target domains are similarly distributed, it also learns the feature space discriminatively, optimizing an information-theoretic metric as an proxy to the expected misclassification error on the target domain. We show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain. Empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies. version:1
arxiv-1206-6439 | Gap Filling in the Plant Kingdom---Trait Prediction Using Hierarchical Probabilistic Matrix Factorization | http://arxiv.org/abs/1206.6439 | id:1206.6439 author:Hanhuai Shan, Jens Kattge, Peter Reich, Arindam Banerjee, Franziska Schrodt, Markus Reichstein category:cs.CE cs.LG stat.AP  published:2012-06-27 summary:Plant traits are a key to understanding and predicting the adaptation of ecosystems to environmental changes, which motivates the TRY project aiming at constructing a global database for plant traits and becoming a standard resource for the ecological community. Despite its unprecedented coverage, a large percentage of missing data substantially constrains joint trait analysis. Meanwhile, the trait data is characterized by the hierarchical phylogenetic structure of the plant kingdom. While factorization based matrix completion techniques have been widely used to address the missing data problem, traditional matrix factorization methods are unable to leverage the phylogenetic structure. We propose hierarchical probabilistic matrix factorization (HPMF), which effectively uses hierarchical phylogenetic information for trait prediction. We demonstrate HPMF's high accuracy, effectiveness of incorporating hierarchical structure and ability to capture trait correlation through experiments. version:1
arxiv-1206-6440 | Predicting Preference Flips in Commerce Search | http://arxiv.org/abs/1206.6440 | id:1206.6440 author:Or Sheffet, Nina Mishra, Samuel Ieong category:cs.LG stat.ML  published:2012-06-27 summary:Traditional approaches to ranking in web search follow the paradigm of rank-by-score: a learned function gives each query-URL combination an absolute score and URLs are ranked according to this score. This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other. Scoring contradicts prior work in behavioral economics that showed that users' preferences between two items depend not only on the items but also on the presented alternatives. Thus, for the same query, users' preference between items A and B depends on the presence/absence of item C. We propose a new model of ranking, the Random Shopper Model, that allows and explains such behavior. In this model, each feature is viewed as a Markov chain over the items to be ranked, and the goal is to find a weighting of the features that best reflects their importance. We show that our model can be learned under the empirical risk minimization framework, and give an efficient learning algorithm. Experiments on commerce search logs demonstrate that our algorithm outperforms scoring-based approaches including regression and listwise ranking. version:1
arxiv-1206-6441 | A Topic Model for Melodic Sequences | http://arxiv.org/abs/1206.6441 | id:1206.6441 author:Athina Spiliopoulou, Amos Storkey category:cs.LG cs.IR stat.ML  published:2012-06-27 summary:We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre. This is a challenging task as one needs to capture not only the rich temporal structure evident in music, but also the complex statistical dependencies among different music components. To address this problem we introduce the Variable-gram Topic Model, which couples the latent topic formalism with a systematic model for contextual information. We evaluate the model on next-step prediction. Additionally, we present a novel way of model evaluation, where we directly compare model samples with data sequences using the Maximum Mean Discrepancy of string kernels, to assess how close is the model distribution to the data distribution. We show that the model has the highest performance under both evaluation measures when compared to LDA, the Topic Bigram and related non-topic models. version:1
arxiv-1206-6442 | Minimizing The Misclassification Error Rate Using a Surrogate Convex Loss | http://arxiv.org/abs/1206.6442 | id:1206.6442 author:Shai Ben-David, David Loker, Nathan Srebro, Karthik Sridharan category:cs.LG stat.ML  published:2012-06-27 summary:We carefully study how well minimizing convex surrogate loss functions, corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. In particular, we show that amongst all convex surrogate losses, the hinge loss gives essentially the best possible bound, of all convex loss functions, for the misclassification error rate of the resulting linear predictor in terms of the best possible margin error rate. We also provide lower bounds for specific convex surrogates that show how different commonly used losses qualitatively differ from each other. version:1
arxiv-1206-6444 | Statistical Linear Estimation with Penalized Estimators: an Application to Reinforcement Learning | http://arxiv.org/abs/1206.6444 | id:1206.6444 author:Bernardo Avila Pires, Csaba Szepesvari category:cs.LG stat.ML  published:2012-06-27 summary:Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coefficients of a linear system to be solved are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coefficients. Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared. We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid data-splitting. A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients, thus allowing the complete separation of the analysis of the stochastic properties of these errors. We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning. version:1
arxiv-1206-6445 | Deep Lambertian Networks | http://arxiv.org/abs/1206.6445 | id:1206.6445 author:Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton category:cs.CV cs.LG stat.ML  published:2012-06-27 summary:Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reflectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition. version:1
arxiv-1206-6446 | Agglomerative Bregman Clustering | http://arxiv.org/abs/1206.6446 | id:1206.6446 author:Matus Telgarsky, Sanjoy Dasgupta category:cs.LG stat.ML  published:2012-06-27 summary:This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions. version:1
arxiv-1206-6447 | Small-sample Brain Mapping: Sparse Recovery on Spatially Correlated Designs with Randomization and Clustering | http://arxiv.org/abs/1206.6447 | id:1206.6447 author:Gael Varoquaux, Alexandre Gramfort, Bertrand Thirion category:cs.LG cs.CV stat.AP stat.ML  published:2012-06-27 summary:Functional neuroimaging can measure the brain?s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets. version:1
arxiv-1206-6448 | Online Alternating Direction Method | http://arxiv.org/abs/1206.6448 | id:1206.6448 author:Huahua Wang, Arindam Banerjee category:cs.LG stat.ML  published:2012-06-27 summary:Online optimization has emerged as powerful tool in large scale optimization. In this paper, we introduce efficient online algorithms based on the alternating directions method (ADM). We introduce a new proof technique for ADM in the batch setting, which yields the O(1/T) convergence rate of ADM and forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether the solution needs to lie in the feasible set or not. In both settings, we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions. Preliminary results are presented to illustrate the performance of the proposed algorithms. version:1
arxiv-1206-6449 | Monte Carlo Bayesian Reinforcement Learning | http://arxiv.org/abs/1206.6449 | id:1206.6449 author:Yi Wang, Kok Sung Won, David Hsu, Wee Sun Lee category:cs.LG stat.ML  published:2012-06-27 summary:Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with point-based approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance. version:1
arxiv-1206-6450 | Conditional Sparse Coding and Grouped Multivariate Regression | http://arxiv.org/abs/1206.6450 | id:1206.6450 author:Min Xu, John Lafferty category:cs.LG stat.ML  published:2012-06-27 summary:We study the problem of multivariate regression where the data are naturally grouped, and a regression matrix is to be estimated for each group. We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups, and a sparse linear combination of the dictionary elements is estimated to form a model within each group. We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group. It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience. We propose an algorithm for conditional sparse coding, analyze its theoretical properties in terms of predictive accuracy, and present the results of simulation and brain imaging experiments that compare the new technique to reduced rank regression. version:1
arxiv-1206-6451 | The Greedy Miser: Learning under Test-time Budgets | http://arxiv.org/abs/1206.6451 | id:1206.6451 author:Zhixiang Xu, Kilian Weinberger, Olivier Chapelle category:cs.LG stat.ML  published:2012-06-27 summary:As machine learning algorithms enter applications in industrial settings, there is increased interest in controlling their cpu-time during testing. The cpu-time consists of the running time of the algorithm and the extraction time of the features. The latter can vary drastically when the feature set is diverse. In this paper, we propose an algorithm, the Greedy Miser, that incorporates the feature extraction cost during training to explicitly minimize the cpu-time during testing. The algorithm is a straightforward extension of stage-wise regression and is equally suitable for regression or multi-class classification. Compared to prior work, it is significantly more cost-effective and scales to larger data sets. version:1
arxiv-1206-6452 | Smoothness and Structure Learning by Proxy | http://arxiv.org/abs/1206.6452 | id:1206.6452 author:Benjamin Yackley, Terran Lane category:cs.LG math.OC stat.ML  published:2012-06-27 summary:As data sets grow in size, the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account. For instance, Bayesian networks, the model chosen in this paper, have a super-exponentially large search space for a fixed number of variables. One possible method to alleviate this problem is to use a proxy, such as a Gaussian Process regressor, in place of the true scoring function, training it on a selection of sampled networks. We prove here that the use of such a proxy is well-founded, as we can bound the smoothness of a commonly-used scoring function for Bayesian network structure learning. We show here that, compared to an identical search strategy using the network?s exact scores, our proxy-based search is able to get equivalent or better scores on a number of data sets in a fraction of the time. version:1
arxiv-1206-6453 | Adaptive Canonical Correlation Analysis Based On Matrix Manifolds | http://arxiv.org/abs/1206.6453 | id:1206.6453 author:Florian Yger, Maxime Berar, Gilles Gasso, Alain Rakotomamonjy category:cs.LG stat.ML  published:2012-06-27 summary:In this paper, we formulate the Canonical Correlation Analysis (CCA) problem on matrix manifolds. This framework provides a natural way for dealing with matrix constraints and tools for building efficient algorithms even in an adaptive setting. Finally, an adaptive CCA algorithm is proposed and applied to a change detection problem in EEG signals. version:1
arxiv-1206-6454 | Hierarchical Exploration for Accelerating Contextual Bandits | http://arxiv.org/abs/1206.6454 | id:1206.6454 author:Yisong Yue, Sue Ann Hong, Carlos Guestrin category:cs.LG stat.ML  published:2012-06-27 summary:Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation. version:1
arxiv-1206-6455 | Regularizers versus Losses for Nonlinear Dimensionality Reduction: A Factored View with New Convex Relaxations | http://arxiv.org/abs/1206.6455 | id:1206.6455 author:Yaoliang Yu, James Neufeld, Ryan Kiros, Xinhua Zhang, Dale Schuurmans category:cs.LG stat.ML  published:2012-06-27 summary:We demonstrate that almost all non-parametric dimensionality reduction methods can be expressed by a simple procedure: regularized loss minimization plus singular value truncation. By distinguishing the role of the loss and regularizer in such a process, we recover a factored perspective that reveals some gaps in the current literature. Beyond identifying a useful new loss for manifold unfolding, a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction. These regularizers can be applied to any loss. version:1
arxiv-1206-6456 | Lognormal and Gamma Mixed Negative Binomial Regression | http://arxiv.org/abs/1206.6456 | id:1206.6456 author:Mingyuan Zhou, Lingbo Li, David Dunson, Lawrence Carin category:stat.AP cs.LG stat.ME  published:2012-06-27 summary:In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples. version:1
arxiv-1206-6457 | Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations | http://arxiv.org/abs/1206.6457 | id:1206.6457 author:Nando de Freitas, Alex Smola, Masrour Zoghi category:cs.LG stat.ML  published:2012-06-27 summary:This paper analyzes the problem of Gaussian process (GP) bandits with deterministic observations. The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al, 2010). For GPs with Gaussian observation noise, with variance strictly greater than zero, Srinivas et al proved that the regret vanishes at the approximate rate of $O(1/\sqrt{t})$, where t is the number of observations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymptotically according to $O(e^{-\frac{\tau t}{(\ln t)^{d/4}}})$ with high probability. Here, d is the dimension of the search space and tau is a constant that depends on the behaviour of the objective function near its global maximum. version:1
arxiv-1206-6458 | Batch Active Learning via Coordinated Matching | http://arxiv.org/abs/1206.6458 | id:1206.6458 author:Javad Azimi, Alan Fern, Xiaoli Zhang-Fern, Glencora Borradaile, Brent Heeringa category:cs.LG stat.ML  published:2012-06-27 summary:Most prior work on active learning of classifiers has focused on sequentially selecting one unlabeled example at a time to be labeled in order to reduce the overall labeling effort. In many scenarios, however, it is desirable to label an entire batch of examples at once, for example, when labels can be acquired in parallel. This motivates us to study batch active learning, which iteratively selects batches of $k>1$ examples to be labeled. We propose a novel batch active learning method that leverages the availability of high-quality and efficient sequential active-learning policies by attempting to approximate their behavior when applied for $k$ steps. Specifically, our algorithm first uses Monte-Carlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over $k$ step executions. The algorithm then attempts to select a set of $k$ examples that best matches this distribution, leading to a combinatorial optimization problem that we term "bounded coordinated matching". While we show this problem is NP-hard in general, we give an efficient greedy solution, which inherits approximation bounds from supermodular minimization theory. Our experimental results on eight benchmark datasets show that the proposed approach is highly effective version:1
arxiv-1206-6459 | Bayesian Conditional Cointegration | http://arxiv.org/abs/1206.6459 | id:1206.6459 author:Chris Bracegirdle, David Barber category:cs.CE cs.LG stat.ME  published:2012-06-27 summary:Cointegration is an important topic for time-series, and describes a relationship between two series in which a linear combination is stationary. Classically, the test for cointegration is based on a two stage process in which first the linear relation between the series is estimated by Ordinary Least Squares. Subsequently a unit root test is performed on the residuals. A well-known deficiency of this classical approach is that it can lead to erroneous conclusions about the presence of cointegration. As an alternative, we present a framework for estimating whether cointegration exists using Bayesian inference which is empirically superior to the classical approach. Finally, we apply our technique to model segmented cointegration in which cointegration may exist only for limited time. In contrast to previous approaches our model makes no restriction on the number of possible cointegration segments. version:1
arxiv-1206-6460 | Output Space Search for Structured Prediction | http://arxiv.org/abs/1206.6460 | id:1206.6460 author:Janardhan Rao Doppa, Alan Fern, Prasad Tadepalli category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time-bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we define the limited-discrepancy search space over structured outputs, which is able to leverage powerful classification learning algorithms to improve the search space quality. Second, we give a generic cost function learning approach, where the key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state-of-the-art structured-prediction performance. version:1
arxiv-1206-6461 | On the Sample Complexity of Reinforcement Learning with a Generative Model | http://arxiv.org/abs/1206.6461 | id:1206.6461 author:Mohammad Gheshlaghi Azar, Remi Munos, Bert Kappen category:cs.LG stat.ML  published:2012-06-27 summary:We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \gamma\in[0,1) only O(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) samples are required to find an \epsilon-optimal estimation of the action-value function with the probability 1-\delta. We also prove a matching lower bound of \Theta (N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, \epsilon, \delta and 1/(1-\gamma). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1-\gamma). version:1
arxiv-1206-6462 | Learning Object Arrangements in 3D Scenes using Human Context | http://arxiv.org/abs/1206.6462 | id:1206.6462 author:Yun Jiang, Marcus Lim, Ashutosh Saxena category:cs.LG cs.CV cs.RO stat.ML  published:2012-06-27 summary:We consider the problem of learning object arrangements in a 3D scene. The key idea here is to learn how objects relate to human poses based on their affordances, ease of use and reachability. In contrast to modeling object-object relationships, modeling human-object relationships scales linearly in the number of objects. We design appropriate density functions based on 3D spatial features to capture this. We learn the distribution of human poses in a scene using a variant of the Dirichlet process mixture model that allows sharing of the density function parameters across the same object types. Then we can reason about arrangements of the objects in the room based on these meaningful human poses. In our extensive experiments on 20 different rooms with a total of 47 objects, our algorithm predicted correct placements with an average error of 1.6 meters from ground truth. In arranging five real scenes, it received a score of 4.3/5 compared to 3.7 for the best baseline method. version:1
arxiv-1206-6463 | An Iterative Locally Linear Embedding Algorithm | http://arxiv.org/abs/1206.6463 | id:1206.6463 author:Deguang Kong, Chris H. Q. Ding, Heng Huang, Feiping Nie category:cs.LG stat.ML  published:2012-06-27 summary:Local Linear embedding (LLE) is a popular dimension reduction method. In this paper, we first show LLE with nonnegative constraint is equivalent to the widely used Laplacian embedding. We further propose to iterate the two steps in LLE repeatedly to improve the results. Thirdly, we relax the kNN constraint of LLE and present a sparse similarity learning algorithm. The final Iterative LLE combines these three improvements. Extensive experiment results show that iterative LLE algorithm significantly improve both classification and clustering results. version:1
arxiv-1206-6465 | Bayesian Efficient Multiple Kernel Learning | http://arxiv.org/abs/1206.6465 | id:1206.6465 author:Mehmet Gonen category:cs.LG stat.ML  published:2012-06-27 summary:Multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources. Most of the previous research on such methods is focused on the computational efficiency issue. However, it is still not feasible to combine many kernels using existing Bayesian approaches due to their high time complexity. We propose a fully conjugate Bayesian formulation and derive a deterministic variational approximation, which allows us to combine hundreds or thousands of kernels very efficiently. We briefly explain how the proposed method can be extended for multiclass learning and semi-supervised learning. Experiments with large numbers of kernels on benchmark data sets show that our inference method is quite fast, requiring less than a minute. On one bioinformatics and three image recognition data sets, our method outperforms previously reported results with better generalization performance. version:1
arxiv-1206-6466 | Utilizing Static Analysis and Code Generation to Accelerate Neural Networks | http://arxiv.org/abs/1206.6466 | id:1206.6466 author:Lawrence McAfee, Kunle Olukotun category:cs.NE cs.MS cs.PL  published:2012-06-27 summary:As datasets continue to grow, neural network (NN) applications are becoming increasingly limited by both the amount of available computational power and the ease of developing high-performance applications. Researchers often must have expert systems knowledge to make their algorithms run efficiently. Although available computing power increases rapidly each year, algorithm efficiency is not able to keep pace due to the use of general purpose compilers, which are not able to fully optimize specialized application domains. Within the domain of NNs, we have the added knowledge that network architecture remains constant during training, meaning the architecture's data structure can be statically optimized by a compiler. In this paper, we present SONNC, a compiler for NNs that utilizes static analysis to generate optimized parallel code. We show that SONNC's use of static optimizations make it able to outperform hand-optimized C++ code by up to 7.8X, and MATLAB code by up to 24X. Additionally, we show that use of SONNC significantly reduces code complexity when using structurally sparse networks. version:1
arxiv-1206-6467 | Semi-Supervised Collective Classification via Hybrid Label Regularization | http://arxiv.org/abs/1206.6467 | id:1206.6467 author:Luke McDowell, David Aha category:cs.LG stat.ML  published:2012-06-27 summary:Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for "collective classification" (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of "label regularization" to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies. version:1
arxiv-1206-6468 | Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation | http://arxiv.org/abs/1206.6468 | id:1206.6468 author:Gautham Mysore, Maneesh Sahani category:cs.LG cs.SD stat.ML  published:2012-06-27 summary:The past decade has seen substantial work on the use of non-negative matrix factorization and its probabilistic counterparts for audio source separation. Although able to capture audio spectral structure well, these models neglect the non-stationarity and temporal dynamics that are important properties of audio. The recently proposed non-negative factorial hidden Markov model (N-FHMM) introduces a temporal dimension and improves source separation performance. However, the factorial nature of this model makes the complexity of inference exponential in the number of sound sources. Here, we present a Bayesian variant of the N-FHMM suited to an efficient variational inference algorithm, whose complexity is linear in the number of sound sources. Our algorithm performs comparably to exact inference in the original N-FHMM but is significantly faster. In typical configurations of the N-FHMM, our method achieves around a 30x increase in speed. version:1
arxiv-1206-6469 | Inferring Latent Structure From Mixed Real and Categorical Relational Data | http://arxiv.org/abs/1206.6469 | id:1206.6469 author:Esther Salazar, Matthew Cain, Elise Darling, Stephen Mitroff, Lawrence Carin category:cs.LG stat.ML  published:2012-06-27 summary:We consider analysis of relational data (a matrix), in which the rows correspond to subjects (e.g., people) and the columns correspond to attributes. The elements of the matrix may be a mix of real and categorical. Each subject and attribute is characterized by a latent binary feature vector, and an inferred matrix maps each row-column pair of binary feature vectors to an observed matrix element. The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low-rank covariance matrix, and the Gaussian random variables are mapped to latent binary features via a probit link. The same type construction is applied jointly to the columns. The model infers latent, low-dimensional binary features associated with each row and each column, as well correlation structure between all rows and between all columns. version:1
arxiv-1206-6470 | A Combinatorial Algebraic Approach for the Identifiability of Low-Rank Matrix Completion | http://arxiv.org/abs/1206.6470 | id:1206.6470 author:Franz Kiraly, Ryota Tomioka category:cs.LG cs.DM cs.NA stat.ML  published:2012-06-27 summary:In this paper, we review the problem of matrix completion and expose its intimate relations with algebraic geometry, combinatorics and graph theory. We present the first necessary and sufficient combinatorial conditions for matrices of arbitrary rank to be identifiable from a set of matrix entries, yielding theoretical constraints and new algorithms for the problem of matrix completion. We conclude by algorithmically evaluating the tightness of the given conditions and algorithms for practically relevant matrix sizes, showing that the algebraic-combinatoric approach can lead to improvements over state-of-the-art matrix completion methods. version:1
arxiv-1206-6471 | On Causal and Anticausal Learning | http://arxiv.org/abs/1206.6471 | id:1206.6471 author:Bernhard Schoelkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, Joris Mooij category:cs.LG stat.ML  published:2012-06-27 summary:We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results. version:1
arxiv-1206-6472 | An Efficient Approach to Sparse Linear Discriminant Analysis | http://arxiv.org/abs/1206.6472 | id:1206.6472 author:Luis Francisco Sanchez Merchante, Yves Grandvalet, Gerrad Govaert category:cs.LG stat.ML  published:2012-06-27 summary:We present a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis (LDA). Our proposal, is based on penalized Optimal Scoring. It has an exact equivalence with penalized LDA, contrary to the multi-class approaches based on the regression of class indicator that have been proposed so far. Sparsity is obtained thanks to a group-Lasso penalty that selects the same features in all discriminant directions. Our experiments demonstrate that this approach generates extremely parsimonious models without compromising prediction performances. Besides prediction, the resulting sparse discriminant directions are also amenable to low-dimensional representations of data. Our algorithm is highly efficient for medium to large number of variables, and is thus particularly well suited to the analysis of gene expression data. version:1
arxiv-1206-6473 | Compositional Planning Using Optimal Option Models | http://arxiv.org/abs/1206.6473 | id:1206.6473 author:David Silver, Kamil Ciosek category:cs.AI cs.LG  published:2012-06-27 summary:In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macro-operators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intra- and inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals. version:1
arxiv-1206-6474 | Estimation of Simultaneously Sparse and Low Rank Matrices | http://arxiv.org/abs/1206.6474 | id:1206.6474 author:Emile Richard, Pierre-Andre Savalle, Nicolas Vayatis category:cs.DS cs.LG cs.NA stat.ML  published:2012-06-27 summary:The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves $\ell_1$-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets. version:1
arxiv-1206-6476 | Similarity Learning for Provably Accurate Sparse Linear Classification | http://arxiv.org/abs/1206.6476 | id:1206.6476 author:Aurelien Bellet, Amaury Habrard, Marc Sebban category:cs.LG stat.ML  published:2012-06-27 summary:In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers. version:1
arxiv-1206-6477 | Discovering Support and Affiliated Features from Very High Dimensions | http://arxiv.org/abs/1206.6477 | id:1206.6477 author:Yiteng Zhai, Mingkui Tan, Ivor Tsang, Yew Soon Ong category:cs.LG stat.ML  published:2012-06-27 summary:In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method. version:1
arxiv-1206-6478 | Maximum Margin Output Coding | http://arxiv.org/abs/1206.6478 | id:1206.6478 author:Yi Zhang, Jeff Schneider category:cs.LG stat.ML  published:2012-06-27 summary:In this paper we study output coding for multi-label prediction. For a multi-label output coding to be discriminative, it is important that codewords for different label vectors are significantly different from each other. In the meantime, unlike in traditional coding theory, codewords in output coding are to be predicted from the input, so it is also critical to have a predictable label encoding. To find output codes that are both discriminative and predictable, we first propose a max-margin formulation that naturally captures these two properties. We then convert it to a metric learning formulation, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. Without a label structure for tractable inference, we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method for optimization. In our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification. version:1
arxiv-1206-6479 | The Landmark Selection Method for Multiple Output Prediction | http://arxiv.org/abs/1206.6479 | id:1206.6479 author:Krishnakumar Balasubramanian, Guy Lebanon category:cs.LG stat.ML  published:2012-06-27 summary:Conditional modeling x \to y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset y_L of the dimensions of y, and proceed by modeling (i) x \to y_L and (ii) y_L \to y. Composing these two models, we obtain a conditional model x \to y that possesses convenient statistical properties. Multi-label classification and multivariate regression experiments on several datasets show that this model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods. version:1
arxiv-1206-6480 | A Dantzig Selector Approach to Temporal Difference Learning | http://arxiv.org/abs/1206.6480 | id:1206.6480 author:Matthieu Geist, Bruno Scherrer, Alessandro Lazaric, Mohammad Ghavamzadeh category:cs.LG stat.ML  published:2012-06-27 summary:LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, L1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are well-suited for high-dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed--point problem, its integration with L1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks. version:1
arxiv-1206-6481 | Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning | http://arxiv.org/abs/1206.6481 | id:1206.6481 author:Yuhong Guo, Min Xiao category:cs.CL cs.IR cs.LG  published:2012-06-27 summary:In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods. version:1
arxiv-1206-6482 | Modeling Images using Transformed Indian Buffet Processes | http://arxiv.org/abs/1206.6482 | id:1206.6482 author:Ke Zhai, Yuening Hu, Sinead Williamson, Jordan Boyd-Graber category:cs.CV cs.LG stat.ML  published:2012-06-27 summary:Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images. version:1
arxiv-1206-6483 | Subgraph Matching Kernels for Attributed Graphs | http://arxiv.org/abs/1206.6483 | id:1206.6483 author:Nils Kriege, Petra Mutzel category:cs.LG stat.ML  published:2012-06-27 summary:We propose graph kernels based on subgraph matchings, i.e. structure-preserving bijections between subgraphs. While recently proposed kernels based on common subgraphs (Wale et al., 2008; Shervashidze et al., 2009) in general can not be applied to attributed graphs, our approach allows to rate mappings of subgraphs by a flexible scoring scheme comparing vertex and edge attributes by kernels. We show that subgraph matching kernels generalize several known kernels. To compute the kernel we propose a graph-theoretical algorithm inspired by a classical relation between common subgraphs of two graphs and cliques in their product graph observed by Levi (1973). Encouraging experimental results on a classification task of real-world graphs are presented. version:1
arxiv-1206-6484 | Apprenticeship Learning for Model Parameters of Partially Observable Environments | http://arxiv.org/abs/1206.6484 | id:1206.6484 author:Takaki Makino, Johane Takeuchi category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:We consider apprenticeship learning, i.e., having an agent learn a task by observing an expert demonstrating the task in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment. version:1
arxiv-1206-6485 | Greedy Algorithms for Sparse Reinforcement Learning | http://arxiv.org/abs/1206.6485 | id:1206.6485 author:Christopher Painter-Wakefield, Ronald Parr category:cs.LG stat.ML  published:2012-06-27 summary:Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on $L_1$ regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the $L_1$ regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing $L_1$ regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems. version:1
arxiv-1206-6486 | Flexible Modeling of Latent Task Structures in Multitask Learning | http://arxiv.org/abs/1206.6486 | id:1206.6486 author:Alexandre Passos, Piyush Rai, Jacques Wainer, Hal Daume III category:cs.LG stat.ML  published:2012-06-27 summary:Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the "right" latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, mean-regularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and real-world datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method. version:1
arxiv-1206-6487 | An Adaptive Algorithm for Finite Stochastic Partial Monitoring | http://arxiv.org/abs/1206.6487 | id:1206.6487 author:Gabor Bartok, Navid Zolghadr, Csaba Szepesvari category:cs.LG cs.GT stat.ML  published:2012-06-27 summary:We present a new anytime algorithm that achieves near-optimal regret for any instance of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for both "easy" and "hard" problems. For easy problems, it additionally achieves logarithmic individual regret. Most importantly, the algorithm is adaptive in the sense that if the opponent strategy is in an "easy region" of the strategy space then the regret grows as if the problem was easy. As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al. (2011). version:1
arxiv-1206-6488 | The Nonparanormal SKEPTIC | http://arxiv.org/abs/1206.6488 | id:1206.6488 author:Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman category:stat.ME cs.LG stat.ML  published:2012-06-27 summary:We propose a semiparametric approach, named nonparanormal skeptic, for estimating high dimensional undirected graphical models. In terms of modeling, we consider the nonparanormal family proposed by Liu et al (2009). In terms of estimation, we exploit nonparametric rank-based correlation coefficient estimators including the Spearman's rho and Kendall's tau. In high dimensional settings, we prove that the nonparanormal skeptic achieves the optimal parametric rate of convergence in both graph and parameter estimation. This result suggests that the nonparanormal graphical models are a safe replacement of the Gaussian graphical models, even when the data are Gaussian. version:1
arxiv-1206-6361 | Learning Markov Network Structure using Brownian Distance Covariance | http://arxiv.org/abs/1206.6361 | id:1206.6361 author:Ehsan Khoshgnauz category:stat.ML cs.LG  published:2012-06-27 summary:In this paper, we present a simple non-parametric method for learning the structure of undirected graphs from data that drawn from an underlying unknown distribution. We propose to use Brownian distance covariance to estimate the conditional independences between the random variables and encodes pairwise Markov graph. This framework can be applied in high-dimensional setting, where the number of parameters much be larger than the sample size. version:1
arxiv-1206-6878 | Efficient Selection of Disambiguating Actions for Stereo Vision | http://arxiv.org/abs/1206.6878 | id:1206.6878 author:Monika Schaeffer, Ron Parr category:cs.CV  published:2012-06-27 summary:In many domains that involve the use of sensors, such as robotics or sensor networks, there are opportunities to use some form of active sensing to disambiguate data from noisy or unreliable sensors. These disambiguating actions typically take time and expend energy. One way to choose the next disambiguating action is to select the action with the greatest expected entropy reduction, or information gain. In this work, we consider active sensing in aid of stereo vision for robotics. Stereo vision is a powerful sensing technique for mobile robots, but it can fail in scenes that lack strong texture. In such cases, a structured light source, such as vertical laser line can be used for disambiguation. By treating the stereo matching problem as a specially structured HMM-like graphical model, we demonstrate that for a scan line with n columns and maximum stereo disparity d, the entropy minimizing aim point for the laser can be selected in O(nd) time - cost no greater than the stereo algorithm itself. In contrast, a typical HMM formulation would suggest at least O(nd^2) time for the entropy calculation alone. version:1
arxiv-1206-6873 | Variable noise and dimensionality reduction for sparse Gaussian processes | http://arxiv.org/abs/1206.6873 | id:1206.6873 author:Edward Snelson, Zoubin Ghahramani category:cs.LG stat.ML  published:2012-06-27 summary:The sparse pseudo-input Gaussian process (SPGP) is a new approximation method for speeding up GP regression in the case of a large number of data points N. The approximation is controlled by the gradient optimization of a small set of M `pseudo-inputs', thereby reducing complexity from N^3 to NM^2. One limitation of the SPGP is that this optimization space becomes impractically big for high dimensional data sets. This paper addresses this limitation by performing automatic dimensionality reduction. A projection of the input space to a low dimensional space is learned in a supervised manner, alongside the pseudo-inputs, which now live in this reduced space. The paper also investigates the suitability of the SPGP for modeling data with input-dependent noise. A further extension of the model is made to make it even more powerful in this regard - we learn an uncertainty parameter for each pseudo-input. The combination of sparsity, reduced dimension, and input-dependent noise makes it possible to apply GPs to much larger and more complex data sets than was previously practical. We demonstrate the benefits of these methods on several synthetic and real world problems. version:1
arxiv-1206-6872 | A Self-Supervised Terrain Roughness Estimator for Off-Road Autonomous Driving | http://arxiv.org/abs/1206.6872 | id:1206.6872 author:David Stavens, Sebastian Thrun category:cs.CV cs.LG cs.RO  published:2012-06-27 summary:We present a machine learning approach for estimating the second derivative of a drivable surface, its roughness. Robot perception generally focuses on the first derivative, obstacle detection. However, the second derivative is also important due to its direct relation (with speed) to the shock the vehicle experiences. Knowing the second derivative allows a vehicle to slow down in advance of rough terrain. Estimating the second derivative is challenging due to uncertainty. For example, at range, laser readings may be so sparse that significant information about the surface is missing. Also, a high degree of precision is required in projecting laser readings. This precision may be unavailable due to latency or error in the pose estimation. We model these sources of error as a multivariate polynomial. Its coefficients are learned using the shock data as ground truth -- the accelerometers are used to train the lasers. The resulting classifier operates on individual laser readings from a road surface described by a 3D point cloud. The classifier identifies sections of road where the second derivative is likely to be large. Thus, the vehicle can slow down in advance, reducing the shock it experiences. The algorithm is an evolution of one we used in the 2005 DARPA Grand Challenge. We analyze it using data from that route. version:1
arxiv-1206-6871 | Ranking by Dependence - A Fair Criteria | http://arxiv.org/abs/1206.6871 | id:1206.6871 author:Harald Steck category:cs.LG stat.ML  published:2012-06-27 summary:Estimating the dependences between random variables, and ranking them accordingly, is a prevalent problem in machine learning. Pursuing frequentist and information-theoretic approaches, we first show that the p-value and the mutual information can fail even in simplistic situations. We then propose two conditions for regularizing an estimator of dependence, which leads to a simple yet effective new measure. We discuss its advantages and compare it to well-established model-selection criteria. Apart from that, we derive a simple constraint for regularizing parameter estimates in a graphical model. This results in an analytical approximation for the optimal value of the equivalent sample size, which agrees very well with the more involved Bayesian approach in our experiments. version:1
arxiv-1206-6870 | Incremental Model-based Learners With Formal Learning-Time Guarantees | http://arxiv.org/abs/1206.6870 | id:1206.6870 author:Alexander L. Strehl, Lihong Li, Michael L. Littman category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:Model-based learning algorithms have been shown to use experience efficiently when learning to solve Markov Decision Processes (MDPs) with finite state and action spaces. However, their high computational cost due to repeatedly solving an internal model inhibits their use in large-scale problems. We propose a method based on real-time dynamic programming (RTDP) to speed up two model-based algorithms, RMAX and MBIE (model-based interval estimation), resulting in computationally much faster algorithms with little loss compared to existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX and RTDP-IE, have considerably smaller computational demands than RMAX and MBIE. We develop a general theoretical framework that allows us to prove that both are efficient learners in a PAC (probably approximately correct) sense. We also present an experimental evaluation of these new algorithms that helps quantify the tradeoff between computational and experience demands. version:1
arxiv-1206-6868 | Bayesian Random Fields: The Bethe-Laplace Approximation | http://arxiv.org/abs/1206.6868 | id:1206.6868 author:Max Welling, Sridevi Parise category:cs.LG stat.ML  published:2012-06-27 summary:While learning the maximum likelihood value of parameters of an undirected graphical model is hard, modelling the posterior distribution over parameters given data is harder. Yet, undirected models are ubiquitous in computer vision and text modelling (e.g. conditional random fields). But where Bayesian approaches for directed models have been very successful, a proper Bayesian treatment of undirected models in still in its infant stages. We propose a new method for approximating the posterior of the parameters given data based on the Laplace approximation. This approximation requires the computation of the covariance matrix over features which we compute using the linear response approximation based in turn on loopy belief propagation. We develop the theory for conditional and 'unconditional' random fields with or without hidden variables. In the conditional setting we introduce a new variant of bagging suitable for structured domains. Here we run the loopy max-product algorithm on a 'super-graph' composed of graphs for individual models sampled from the posterior and connected by constraints. Experiments on real world data validate the proposed methods. version:1
arxiv-1206-6865 | A Non-Parametric Bayesian Method for Inferring Hidden Causes | http://arxiv.org/abs/1206.6865 | id:1206.6865 author:Frank Wood, Thomas Griffiths, Zoubin Ghahramani category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:We present a non-parametric Bayesian approach to structure learning with hidden causes. Previous Bayesian treatments of this problem define a prior over the number of hidden causes and use algorithms such as reversible jump Markov chain Monte Carlo to move between solutions. In contrast, we assume that the number of hidden causes is unbounded, but only a finite number influence observable variables. This makes it possible to use a Gibbs sampler to approximate the distribution over causal structures. We evaluate the performance of both approaches in discovering hidden causes in simulated data, and use our non-parametric approach to discover hidden causes in a real medical dataset. version:1
arxiv-1206-6864 | Infinite Hidden Relational Models | http://arxiv.org/abs/1206.6864 | id:1206.6864 author:Zhao Xu, Volker Tresp, Kai Yu, Hans-Peter Kriegel category:cs.AI cs.DB cs.LG  published:2012-06-27 summary:In many cases it makes sense to model a relationship symmetrically, not implying any particular directionality. Consider the classical example of a recommendation system where the rating of an item by a user should symmetrically be dependent on the attributes of both the user and the item. The attributes of the (known) relationships are also relevant for predicting attributes of entities and for predicting attributes of new relations. In recommendation systems, the exploitation of relational attributes is often referred to as collaborative filtering. Again, in many applications one might prefer to model the collaborative effect in a symmetrical way. In this paper we present a relational model, which is completely symmetrical. The key innovation is that we introduce for each entity (or object) an infinite-dimensional latent variable as part of a Dirichlet process (DP) model. We discuss inference in the model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant process. We extend the Chinese restaurant process to be applicable to relational modeling. Our approach is evaluated in three applications. One is a recommendation system based on the MovieLens data set. The second application concerns the prediction of the function of yeast genes/proteins on the data set of KDD Cup 2001 using a multi-relational model. The third application involves a relational medical domain. The experimental results show that our model gives significantly improved estimates of attributes describing relationships or entities in complex relational models. version:1
arxiv-1206-6863 | Bayesian Multicategory Support Vector Machines | http://arxiv.org/abs/1206.6863 | id:1206.6863 author:Zhihua Zhang, Michael I. Jordan category:cs.LG stat.ML  published:2012-06-27 summary:We show that the multi-class support vector machine (MSVM) proposed by Lee et. al. (2004), can be viewed as a MAP estimation procedure under an appropriate probabilistic interpretation of the classifier. We also show that this interpretation can be extended to a hierarchical Bayesian architecture and to a fully-Bayesian inference procedure for multi-class classification based on data augmentation. We present empirical results that show that the advantages of the Bayesian formalism are obtained without a loss in classification accuracy. version:1
arxiv-1206-6862 | On the Number of Samples Needed to Learn the Correct Structure of a Bayesian Network | http://arxiv.org/abs/1206.6862 | id:1206.6862 author:Or Zuk, Shiri Margel, Eytan Domany category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:Bayesian Networks (BNs) are useful tools giving a natural and compact representation of joint probability distributions. In many applications one needs to learn a Bayesian Network (BN) from data. In this context, it is important to understand the number of samples needed in order to guarantee a successful learning. Previous work have studied BNs sample complexity, yet it mainly focused on the requirement that the learned distribution will be close to the original distribution which generated the data. In this work, we study a different aspect of the learning, namely the number of samples needed in order to learn the correct structure of the network. We give both asymptotic results, valid in the large sample limit, and experimental results, demonstrating the learning behavior for feasible sample sizes. We show that structure learning is a more difficult task, compared to approximating the correct distribution, in the sense that it requires a much larger number of samples, regardless of the computational power available for the learner. version:1
arxiv-1206-6860 | Predicting Conditional Quantiles via Reduction to Classification | http://arxiv.org/abs/1206.6860 | id:1206.6860 author:John Langford, Roberto Oliveira, Bianca Zadrozny category:cs.LG stat.ML  published:2012-06-27 summary:We show how to reduce the process of predicting general order statistics (and the median in particular) to solving classification. The accompanying theoretical statement shows that the regret of the classifier bounds the regret of the quantile regression under a quantile loss. We also test this reduction empirically against existing quantile regression methods on large real-world datasets and discover that it provides state-of-the-art performance. version:1
arxiv-1206-6858 | Sequential Document Representations and Simplicial Curves | http://arxiv.org/abs/1206.6858 | id:1206.6858 author:Guy Lebanon category:cs.IR cs.LG  published:2012-06-27 summary:The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present a continuous and differentiable sequential document representation that goes beyond the bag of words assumption, and yet is efficient and effective. This representation employs smooth curves in the multinomial simplex to account for sequential information. We discuss the representation and its geometric properties and demonstrate its applicability for the task of text classification. version:1
arxiv-1206-6857 | Faster Gaussian Summation: Theory and Experiment | http://arxiv.org/abs/1206.6857 | id:1206.6857 author:Dongryeol Lee, Alexander G. Gray category:cs.LG cs.NA stat.ML  published:2012-06-27 summary:We provide faster algorithms for the problem of Gaussian summation, which occurs in many machine learning methods. We develop two new extensions - an O(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and a new error control scheme integrating any arbitrary approximation method - within the best discretealgorithmic framework using adaptive hierarchical data structures. We rigorously evaluate these techniques empirically in the context of optimal bandwidth selection in kernel density estimation, revealing the strengths and weaknesses of current state-of-the-art approaches for the first time. Our results demonstrate that the new error control scheme yields improved performance, whereas the series expansion approach is only effective in low dimensions (five or less). version:1
arxiv-1205-4213 | Online Structured Prediction via Coactive Learning | http://arxiv.org/abs/1205.4213 | id:1205.4213 author:Pannaga Shivaswamy, Thorsten Joachims category:cs.LG cs.AI cs.IR  published:2012-05-18 summary:We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved -- but not necessarily optimal -- object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have ${\cal O}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search. version:2
arxiv-1206-6852 | Structured Priors for Structure Learning | http://arxiv.org/abs/1206.6852 | id:1206.6852 author:Vikash Mansinghka, Charles Kemp, Thomas Griffiths, Joshua Tenenbaum category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:Traditional approaches to Bayes net structure learning typically assume little regularity in graph structure other than sparseness. However, in many cases, we expect more systematicity: variables in real-world systems often group into classes that predict the kinds of probabilistic dependencies they participate in. Here we capture this form of prior knowledge in a hierarchical Bayesian framework, and exploit it to enable structure learning and type discovery from small datasets. Specifically, we present a nonparametric generative model for directed acyclic graphs as a prior for Bayes net structure learning. Our model assumes that variables come in one or more classes and that the prior probability of an edge existing between two variables is a function only of their classes. We derive an MCMC algorithm for simultaneous inference of the number of classes, the class assignments of variables, and the Bayes net structure over variables. For several realistic, sparse datasets, we show that the bias towards systematicity of connections provided by our model yields more accurate learned networks than a traditional, uniform prior approach, and that the classes found by our model are appropriate. version:1
arxiv-1206-6851 | A compact, hierarchical Q-function decomposition | http://arxiv.org/abs/1206.6851 | id:1206.6851 author:Bhaskara Marthi, Stuart Russell, David Andre category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:Previous work in hierarchical reinforcement learning has faced a dilemma: either ignore the values of different possible exit states from a subroutine, thereby risking suboptimal behavior, or represent those values explicitly thereby incurring a possibly large representation cost because exit values refer to nonlocal aspects of the world (i.e., all subsequent rewards). This paper shows that, in many cases, one can avoid both of these problems. The solution is based on recursively decomposing the exit value function in terms of Q-functions at higher levels of the hierarchy. This leads to an intuitively appealing runtime architecture in which a parent subroutine passes to its child a value function on the exit states and the child reasons about how its choices affect the exit value. We also identify structural conditions on the value function and transition distributions that allow much more concise representations of exit state distributions, leading to further state abstraction. In essence, the only variables whose exit values need be considered are those that the parent cares about and the child affects. We demonstrate the utility of our algorithms on a series of increasingly complex environments. version:1
arxiv-1206-6847 | Identifying the Relevant Nodes Without Learning the Model | http://arxiv.org/abs/1206.6847 | id:1206.6847 author:Jose M. Pena, Roland Nilsson, Johan Björkegren, Jesper Tegnér category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:We propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes. Our method is simple, effcient, consistent, and does not require learning a Bayesian network first. Therefore, our method can be applied to high-dimensional databases, e.g. gene expression databases. version:1
arxiv-1206-6846 | Approximate Separability for Weak Interaction in Dynamic Systems | http://arxiv.org/abs/1206.6846 | id:1206.6846 author:Avi Pfeffer category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:One approach to monitoring a dynamic system relies on decomposition of the system into weakly interacting subsystems. An earlier paper introduced a notion of weak interaction called separability, and showed that it leads to exact propagation of marginals for prediction. This paper addresses two questions left open by the earlier paper: can we define a notion of approximate separability that occurs naturally in practice, and do separability and approximate separability lead to accurate monitoring? The answer to both questions is afirmative. The paper also analyzes the structure of approximately separable decompositions, and provides some explanation as to why these models perform well. version:1
arxiv-1206-6845 | Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick Breaking Representation | http://arxiv.org/abs/1206.6845 | id:1206.6845 author:Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling category:stat.ME cs.LG stat.ML  published:2012-06-27 summary:Nonparametric Bayesian approaches to clustering, information retrieval, language modeling and object recognition have recently shown great promise as a new paradigm for unsupervised data analysis. Most contributions have focused on the Dirichlet process mixture models or extensions thereof for which efficient Gibbs samplers exist. In this paper we explore Gibbs samplers for infinite complexity mixture models in the stick breaking representation. The advantage of this representation is improved modeling flexibility. For instance, one can design the prior distribution over cluster sizes or couple multiple infinite mixture models (e.g. over time) at the level of their parameters (i.e. the dependent Dirichlet process model). However, Gibbs samplers for infinite mixture models (as recently introduced in the statistics literature) seem to mix poorly over cluster labels. Among others issues, this can have the adverse effect that labels for the same cluster in coupled mixture models are mixed up. We introduce additional moves in these samplers to improve mixing over cluster labels and to bring clusters into correspondence. An application to modeling of storm trajectories is used to illustrate these ideas. version:1
arxiv-1206-6842 | Chi-square Tests Driven Method for Learning the Structure of Factored MDPs | http://arxiv.org/abs/1206.6842 | id:1206.6842 author:Thomas Degris, Olivier Sigaud, Pierre-Henri Wuillemin category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:SDYNA is a general framework designed to address large stochastic reinforcement learning problems. Unlike previous model based methods in FMDPs, it incrementally learns the structure and the parameters of a RL problem using supervised learning techniques. Then, it integrates decision-theoric planning algorithms based on FMDPs to compute its policy. SPITI is an instanciation of SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the reward function and the Dynamic Bayesian Networks with local structures representing the transition function of the problem. These representations are used by an incremental version of the Structured Value Iteration algorithm. In order to learn the structure, SPITI uses Chi-Square tests to detect the independence between two probability distributions. Thus, we study the relation between the threshold used in the Chi-Square test, the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value. We show that, on stochastic problems, one can tune the threshold so as to generate both a compact model and an efficient policy. Then, we show that SPITI, while keeping its model compact, uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large RL problem with an unknown structure. We also introduce a new measure based on Chi-Square to qualify the accuracy of the model learned by SPITI. We qualitatively show that the generalization property in SPITI within the FMDP framework may prevent an exponential growth of the time required to learn the structure of large stochastic RL problems. version:1
arxiv-1206-6838 | Continuous Time Markov Networks | http://arxiv.org/abs/1206.6838 | id:1206.6838 author:Tal El-Hay, Nir Friedman, Daphne Koller, Raz Kupferman category:cs.AI cs.LG  published:2012-06-27 summary:A central task in many applications is reasoning about processes that change in a continuous time. The mathematical framework of Continuous Time Markov Processes provides the basic foundations for modeling such systems. Recently, Nodelman et al introduced continuous time Bayesian networks (CTBNs), which allow a compact representation of continuous-time processes over a factored state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a different type of continuous-time dynamics. In many real life processes, such as biological and chemical systems, the dynamics of the process can be naturally described as an interplay between two forces - the tendency of each entity to change its state, and the overall fitness or energy function of the entire system. In our model, the first force is described by a continuous-time proposal process that suggests possible local changes to the state of the system at different rates. The second force is represented by a Markov network that encodes the fitness, or desirability, of different states; a proposed local change is then accepted with a probability that is a function of the change in the fitness distribution. We show that the fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal process whose stationary distribution has a compact graphical representation. This allows us to naturally capture a different type of structure in complex dynamical processes, such as evolving biological sequences. We describe the semantics of the representation, its basic properties, and how it compares to CTBNs. We also provide algorithms for learning such models from data, and discuss its applicability to biological sequence evolution. version:1
arxiv-1206-6833 | Matrix Tile Analysis | http://arxiv.org/abs/1206.6833 | id:1206.6833 author:Inmar Givoni, Vincent Cheung, Brendan J. Frey category:cs.LG cs.CE cs.NA stat.ML  published:2012-06-27 summary:Many tasks require finding groups of elements in a matrix of numbers, symbols or class likelihoods. One approach is to use efficient bi- or tri-linear factorization techniques including PCA, ICA, sparse matrix factorization and plaid analysis. These techniques are not appropriate when addition and multiplication of matrix elements are not sensibly defined. More directly, methods like bi-clustering can be used to classify matrix elements, but these methods make the overly-restrictive assumption that the class of each element is a function of a row class and a column class. We introduce a general computational problem, `matrix tile analysis' (MTA), which consists of decomposing a matrix into a set of non-overlapping tiles, each of which is defined by a subset of usually nonadjacent rows and columns. MTA does not require an algebra for combining tiles, but must search over discrete combinations of tile assignments. Exact MTA is a computationally intractable integer programming problem, but we describe an approximate iterative technique and a computationally efficient sum-product relaxation of the integer program. We compare the effectiveness of these methods to PCA and plaid on hundreds of randomly generated tasks. Using double-gene-knockout data, we show that MTA finds groups of interacting yeast genes that have biologically-related functions. version:1
arxiv-1206-6832 | Convex Structure Learning for Bayesian Networks: Polynomial Feature Selection and Approximate Ordering | http://arxiv.org/abs/1206.6832 | id:1206.6832 author:Yuhong Guo, Dale Schuurmans category:cs.LG stat.ML  published:2012-06-27 summary:We present a new approach to learning the structure and parameters of a Bayesian network based on regularized estimation in an exponential family representation. Here we show that, given a fixed variable order, the optimal structure and parameters can be learned efficiently, even without restricting the size of the parent sets. We then consider the problem of optimizing the variable order for a given set of features. This is still a computationally hard problem, but we present a convex relaxation that yields an optimal 'soft' ordering in polynomial time. One novel aspect of the approach is that we do not perform a discrete search over DAG structures, nor over variable orders, but instead solve a continuous relaxation that can then be rounded to obtain a valid network structure. We conduct an experimental comparison against standard structure search procedures over standard objectives, which cope with local minima, and evaluate the advantages of using convex relaxations that reduce the effects of local minima. version:1
arxiv-1206-6830 | The AI&M Procedure for Learning from Incomplete Data | http://arxiv.org/abs/1206.6830 | id:1206.6830 author:Manfred Jaeger category:stat.ME cs.AI cs.LG  published:2012-06-27 summary:We investigate methods for parameter learning from incomplete data that is not missing at random. Likelihood-based methods then require the optimization of a profile likelihood that takes all possible missingness mechanisms into account. Optimzing this profile likelihood poses two main difficulties: multiple (local) maxima, and its very high-dimensional parameter space. In this paper a new method is presented for optimizing the profile likelihood that addresses the second difficulty: in the proposed AI&M (adjusting imputation and mazimization) procedure the optimization is performed by operations in the space of data completions, rather than directly in the parameter space of the profile likelihood. We apply the AI&M method to learning parameters for Bayesian networks. The method is compared against conservative inference, which takes into account each possible data completion, and against EM. The results indicate that likelihood-based inference is still feasible in the case of unknown missingness mechanisms, and that conservative inference is unnecessarily weak. On the other hand, our results also provide evidence that the EM algorithm is still quite effective when the data is not missing at random. version:1
arxiv-1206-6828 | Advances in exact Bayesian structure discovery in Bayesian networks | http://arxiv.org/abs/1206.6828 | id:1206.6828 author:Mikko Koivisto category:cs.LG cs.AI stat.ML  published:2012-06-27 summary:We consider a Bayesian method for learning the Bayesian network structure from complete data. Recently, Koivisto and Sood (2004) presented an algorithm that for any single edge computes its marginal posterior probability in O(n 2^n) time, where n is the number of attributes; the number of parents per attribute is bounded by a constant. In this paper we show that the posterior probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n) total time. This result is achieved by a forward-backward technique and fast Moebius transform algorithms, which are of independent interest. The resulting speedup by a factor of about n^2 allows us to experimentally study the statistical power of learning moderate-size networks. We report results from a simulation study that covers data sets with 20 to 10,000 records over 5 to 25 discrete attributes version:1
arxiv-1206-6824 | Gene Expression Time Course Clustering with Countably Infinite Hidden Markov Models | http://arxiv.org/abs/1206.6824 | id:1206.6824 author:Matthew Beal, Praveen Krishnamurthy category:cs.LG cs.CE stat.ML  published:2012-06-27 summary:Most existing approaches to clustering gene expression time course data treat the different time points as independent dimensions and are invariant to permutations, such as reversal, of the experimental time course. Approaches utilizing HMMs have been shown to be helpful in this regard, but are hampered by having to choose model architectures with appropriate complexities. Here we propose for a clustering application an HMM with a countably infinite state space; inference in this model is possible by recasting it in the hierarchical Dirichlet process (HDP) framework (Teh et al. 2006), and hence we call it the HDP-HMM. We show that the infinite model outperforms model selection methods over finite models, and traditional time-independent methods, as measured by a variety of external and internal indices for clustering on two large publicly available data sets. Moreover, we show that the infinite models utilize more hidden states and employ richer architectures (e.g. state-to-state transitions) without the damaging effects of overfitting. version:1
arxiv-1206-6815 | Discriminative Learning via Semidefinite Probabilistic Models | http://arxiv.org/abs/1206.6815 | id:1206.6815 author:Koby Crammer, Amir Globerson category:cs.LG stat.ML  published:2012-06-27 summary:Discriminative linear models are a popular tool in machine learning. These can be generally divided into two types: The first is linear classifiers, such as support vector machines, which are well studied and provide state-of-the-art results. One shortcoming of these models is that their output (known as the 'margin') is not calibrated, and cannot be translated naturally into a distribution over the labels. Thus, it is difficult to incorporate such models as components of larger systems, unlike probabilistic based approaches. The second type of approach constructs class conditional distributions using a nonlinearity (e.g. log-linear models), but is occasionally worse in terms of classification error. We propose a supervised learning method which combines the best of both approaches. Specifically, our method provides a distribution over the labels, which is a linear function of the model parameters. As a consequence, differences between probabilities are linear functions, a property which most probabilistic models (e.g. log-linear) do not have. Our model assumes that classes correspond to linear subspaces (rather than to half spaces). Using a relaxed projection operator, we construct a measure which evaluates the degree to which a given vector 'belongs' to a subspace, resulting in a distribution over labels. Interestingly, this view is closely related to similar concepts in quantum detection theory. The resulting models can be trained either to maximize the margin or to optimize average likelihood measures. The corresponding optimization problems are semidefinite programs which can be solved efficiently. We illustrate the performance of our algorithm on real world datasets, and show that it outperforms 2nd order kernel methods. version:1
arxiv-1206-6814 | An Empirical Comparison of Algorithms for Aggregating Expert Predictions | http://arxiv.org/abs/1206.6814 | id:1206.6814 author:Varsha Dani, Omid Madani, David M Pennock, Sumit Sanghai, Brian Galebach category:cs.AI cs.LG  published:2012-06-27 summary:Predicting the outcomes of future events is a challenging problem for which a variety of solution methods have been explored and attempted. We present an empirical comparison of a variety of online and offline adaptive algorithms for aggregating experts' predictions of the outcomes of five years of US National Football League games (1319 games) using expert probability elicitations obtained from an Internet contest called ProbabilitySports. We find that it is difficult to improve over simple averaging of the predictions in terms of prediction accuracy, but that there is room for improvement in quadratic loss. Somewhat surprisingly, a Bayesian estimation algorithm which estimates the variance of each expert's prediction exhibits the most consistent superior performance over simple averaging among our collection of algorithms. version:1
arxiv-1206-6813 | A concentration theorem for projections | http://arxiv.org/abs/1206.6813 | id:1206.6813 author:Sanjoy Dasgupta, Daniel Hsu, Nakul Verma category:cs.LG stat.ML  published:2012-06-27 summary:X in R^D has mean zero and finite second moments. We show that there is a precise sense in which almost all linear projections of X into R^d (for d < D) look like a scale-mixture of spherical Gaussians -- specifically, a mixture of distributions N(0, sigma^2 I_d) where the weight of the particular sigma component is P ( X ^2 = sigma^2 D). The extent of this effect depends upon the ratio of d to D, and upon a particular coefficient of eccentricity of X's distribution. We explore this result in a variety of experiments. version:1
arxiv-1206-6262 | Scaling Life-long Off-policy Learning | http://arxiv.org/abs/1206.6262 | id:1206.6262 author:Adam White, Joseph Modayil, Richard S. Sutton category:cs.AI cs.LG  published:2012-06-27 summary:We pursue a life-long learning approach to artificial intelligence that makes extensive use of reinforcement learning algorithms. We build on our prior work with general value functions (GVFs) and the Horde architecture. GVFs have been shown able to represent a wide variety of facts about the world's dynamics that may be useful to a long-lived agent (Sutton et al. 2011). We have also previously shown scaling - that thousands of on-policy GVFs can be learned accurately in real-time on a mobile robot (Modayil, White & Sutton 2011). That work was limited in that it learned about only one policy at a time, whereas the greatest potential benefits of life-long learning come from learning about many policies in parallel, as we explore in this paper. Many new challenges arise in this off-policy learning setting. To deal with convergence and efficiency challenges, we utilize the recently introduced GTD({\lambda}) algorithm. We show that GTD({\lambda}) with tile coding can simultaneously learn hundreds of predictions for five simple target policies while following a single random behavior policy, assessing accuracy with interspersed on-policy tests. To escape the need for the tests, which preclude further scaling, we introduce and empirically vali- date two online estimators of the off-policy objective (MSPBE). Finally, we use the more efficient of the two estimators to demonstrate off-policy learning at scale - the learning of value functions for one thousand policies in real time on a physical robot. This ability constitutes a significant step towards scaling life-long off-policy learning. version:1
arxiv-1206-6196 | Discrete Elastic Inner Vector Spaces with Application in Time Series and Sequence Mining | http://arxiv.org/abs/1206.6196 | id:1206.6196 author:Pierre-François Marteau, Nicolas Bonnel, Gilbas Ménier category:cs.LG cs.DB  published:2012-06-27 summary:This paper proposes a framework dedicated to the construction of what we call discrete elastic inner product allowing one to embed sets of non-uniformly sampled multivariate time series or sequences of varying lengths into inner product space structures. This framework is based on a recursive definition that covers the case of multiple embedded time elastic dimensions. We prove that such inner products exist in our general framework and show how a simple instance of this inner product class operates on some prospective applications, while generalizing the Euclidean inner product. Classification experimentations on time series and symbolic sequences datasets demonstrate the benefits that we can expect by embedding time series or sequences into elastic inner spaces rather than into classical Euclidean spaces. These experiments show good accuracy when compared to the euclidean distance or even dynamic programming algorithms while maintaining a linear algorithmic complexity at exploitation stage, although a quadratic indexing phase beforehand is required. version:1
arxiv-1206-6141 | Directed Time Series Regression for Control | http://arxiv.org/abs/1206.6141 | id:1206.6141 author:Yi-Hao Kao, Benjamin Van Roy category:cs.LG cs.SY stat.ML  published:2012-06-26 summary:We propose directed time series regression, a new approach to estimating parameters of time-series models for use in certainty equivalent model predictive control. The approach combines merits of least squares regression and empirical optimization. Through a computational study involving a stochastic version of a well known inverted pendulum balancing problem, we demonstrate that directed time series regression can generate significant improvements in controller performance over either of the aforementioned alternatives. version:1
arxiv-1107-4623 | A Unifying Analysis of Projected Gradient Descent for $\ell_p$-constrained Least Squares | http://arxiv.org/abs/1107.4623 | id:1107.4623 author:Sohail Bahmani, Bhiksha Raj category:math.NA cs.IT math.IT math.OC stat.ML  published:2011-07-22 summary:In this paper we study the performance of the Projected Gradient Descent(PGD) algorithm for $\ell_{p}$-constrained least squares problems that arise in the framework of Compressed Sensing. Relying on the Restricted Isometry Property, we provide convergence guarantees for this algorithm for the entire range of $0\leq p\leq1$, that include and generalize the existing results for the Iterative Hard Thresholding algorithm and provide a new accuracy guarantee for the Iterative Soft Thresholding algorithm as special cases. Our results suggest that in this group of algorithms, as $p$ increases from zero to one, conditions required to guarantee accuracy become stricter and robustness to noise deteriorates. version:5
arxiv-1206-6038 | Predictive Approaches For Gaussian Process Classifier Model Selection | http://arxiv.org/abs/1206.6038 | id:1206.6038 author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG stat.ML  published:2012-06-26 summary:In this paper we consider the problem of Gaussian process classifier (GPC) model selection with different Leave-One-Out (LOO) Cross Validation (CV) based optimization criteria and provide a practical algorithm using LOO predictive distributions with such criteria to select hyperparameters. Apart from the standard average negative logarithm of predictive probability (NLP), we also consider smoothed versions of criteria such as F-measure and Weighted Error Rate (WER), which are useful for handling imbalanced data. Unlike the regression case, LOO predictive distributions for the classifier case are intractable. We use approximate LOO predictive distributions arrived from Expectation Propagation (EP) approximation. We conduct experiments on several real world benchmark datasets. When the NLP criterion is used for optimizing the hyperparameters, the predictive approaches show better or comparable NLP generalization performance with existing GPC approaches. On the other hand, when the F-measure criterion is used, the F-measure generalization performance improves significantly on several datasets. Overall, the EP-based predictive algorithm comes out as an excellent choice for GP classifier model selection with different optimization criteria. version:1
arxiv-1206-6030 | An Additive Model View to Sparse Gaussian Process Classifier Design | http://arxiv.org/abs/1206.6030 | id:1206.6030 author:Sundararajan Sellamanickam, Shirish Shevade category:cs.LG stat.ML  published:2012-06-26 summary:We consider the problem of designing a sparse Gaussian process classifier (SGPC) that generalizes well. Viewing SGPC design as constructing an additive model like in boosting, we present an efficient and effective SGPC design method to perform a stage-wise optimization of a predictive loss function. We introduce new methods for two key components viz., site parameter estimation and basis vector selection in any SGPC design. The proposed adaptive sampling based basis vector selection method aids in achieving improved generalization performance at a reduced computational cost. This method can also be used in conjunction with any other site parameter estimation methods. It has similar computational and storage complexities as the well-known information vector machine and is suitable for large datasets. The hyperparameters can be determined by optimizing a predictive loss function. The experimental results show better generalization performance of the proposed basis vector selection method on several benchmark datasets, particularly for relatively smaller basis vector set sizes or on difficult datasets. version:1
arxiv-1206-6015 | Transductive Classification Methods for Mixed Graphs | http://arxiv.org/abs/1206.6015 | id:1206.6015 author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG stat.ML  published:2012-06-26 summary:In this paper we provide a principled approach to solve a transductive classification problem involving a similar graph (edges tend to connect nodes with same labels) and a dissimilar graph (edges tend to connect nodes with opposing labels). Most of the existing methods, e.g., Information Regularization (IR), Weighted vote Relational Neighbor classifier (WvRN) etc, assume that the given graph is only a similar graph. We extend the IR and WvRN methods to deal with mixed graphs. We evaluate the proposed extensions on several benchmark datasets as well as two real world datasets and demonstrate the usefulness of our ideas. version:1
arxiv-1010-4504 | Reading Dependencies from Covariance Graphs | http://arxiv.org/abs/1010.4504 | id:1010.4504 author:Jose M. Peña category:stat.ML cs.AI math.ST stat.TH  published:2010-10-21 summary:The covariance graph (aka bi-directed graph) of a probability distribution $p$ is the undirected graph $G$ where two nodes are adjacent iff their corresponding random variables are marginally dependent in $p$. In this paper, we present a graphical criterion for reading dependencies from $G$, under the assumption that $p$ satisfies the graphoid properties as well as weak transitivity and composition. We prove that the graphical criterion is sound and complete in certain sense. We argue that our assumptions are not too restrictive. For instance, all the regular Gaussian probability distributions satisfy them. version:3
arxiv-1206-5915 | Graph Based Classification Methods Using Inaccurate External Classifier Information | http://arxiv.org/abs/1206.5915 | id:1206.5915 author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG  published:2012-06-26 summary:In this paper we consider the problem of collectively classifying entities where relational information is available across the entities. In practice inaccurate class distribution for each entity is often available from another (external) classifier. For example this distribution could come from a classifier built using content features or a simple dictionary. Given the relational and inaccurate external classifier information, we consider two graph based settings in which the problem of collective classification can be solved. In the first setting the class distribution is used to fix labels to a subset of nodes and the labels for the remaining nodes are obtained like in a transductive setting. In the other setting the class distributions of all nodes are used to define the fitting function part of a graph regularized objective function. We define a generalized objective function that handles both the settings. Methods like harmonic Gaussian field and local-global consistency (LGC) reported in the literature can be seen as special cases. We extend the LGC and weighted vote relational neighbor classification (WvRN) methods to support usage of external classifier information. We also propose an efficient least squares regularization (LSR) based method and relate it to information regularization methods. All the methods are evaluated on several benchmark and real world datasets. Considering together speed, robustness and accuracy, experimental results indicate that the LSR and WvRN-extension methods perform better than other methods. version:1
arxiv-1206-5882 | Exact Recovery of Sparsely-Used Dictionaries | http://arxiv.org/abs/1206.5882 | id:1206.5882 author:Daniel A. Spielman, Huan Wang, John Wright category:cs.LG cs.IT math.IT  published:2012-06-26 summary:We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that $O (n \log n)$ samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms. version:1
arxiv-1206-5851 | A meta-analysis of state-of-the-art electoral prediction from Twitter data | http://arxiv.org/abs/1206.5851 | id:1206.5851 author:Daniel Gayo-Avello category:cs.SI cs.CL cs.CY physics.soc-ph  published:2012-06-25 summary:Electoral prediction from Twitter data is an appealing research topic. It seems relatively straightforward and the prevailing view is overly optimistic. This is problematic because while simple approaches are assumed to be good enough, core problems are not addressed. Thus, this paper aims to (1) provide a balanced and critical review of the state of the art; (2) cast light on the presume predictive power of Twitter data; and (3) depict a roadmap to push forward the field. Hence, a scheme to characterize Twitter prediction methods is proposed. It covers every aspect from data collection to performance evaluation, through data processing and vote inference. Using that scheme, prior research is analyzed and organized to explain the main approaches taken up to date but also their weaknesses. This is the first meta-analysis of the whole body of research regarding electoral prediction from Twitter data. It reveals that its presumed predictive power regarding electoral prediction has been rather exaggerated: although social media may provide a glimpse on electoral outcomes current research does not provide strong evidence to support it can replace traditional polls. Finally, future lines of research along with a set of requirements they must fulfill are provided. version:1
arxiv-1205-6031 | Towards a Mathematical Foundation of Immunology and Amino Acid Chains | http://arxiv.org/abs/1205.6031 | id:1205.6031 author:Wen-Jun Shen, Hau-San Wong, Quan-Wu Xiao, Xin Guo, Stephen Smale category:stat.ML cs.LG q-bio.GN  published:2012-05-28 summary:We attempt to set a mathematical foundation of immunology and amino acid chains. To measure the similarities of these chains, a kernel on strings is defined using only the sequence of the chains and a good amino acid substitution matrix (e.g. BLOSUM62). The kernel is used in learning machines to predict binding affinities of peptides to human leukocyte antigens DR (HLA-DR) molecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsen et.al. 2010) benchmark databases, our algorithm achieves the state-of-the-art performance. The kernel is also used to define a distance on an HLA-DR allele set based on which a clustering analysis precisely recovers the serotype classifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010). These results suggest that our kernel relates well the chain structure of both peptides and HLA-DR molecules to their biological functions, and that it offers a simple, powerful and promising methodology to immunology and amino acid chain studies. version:2
arxiv-1206-5559 | Speeding up the construction of slow adaptive walks | http://arxiv.org/abs/1206.5559 | id:1206.5559 author:Susan Khor category:cs.NE  published:2012-06-25 summary:An algorithm (bliss) is proposed to speed up the construction of slow adaptive walks. Slow adaptive walks are adaptive walks biased towards closer points or smaller move steps. They were previously introduced to explore a search space, e.g. to detect potential local optima or to assess the ruggedness of a fitness landscape. To avoid the quadratic cost of computing Hamming distance (HD) for all-pairs of strings in a set in order to find the set of closest strings for each string, strings are sorted and clustered by bliss such that similar strings are more likely to get paired off for HD computation. To efficiently arrange the strings by similarity, bliss employs the idea of shared non-overlapping position specific subsequences between strings which is inspired by an alignment-free protein sequence comparison algorithm. Tests are performed to evaluate the quality of b-walks, i.e. slow adaptive walks constructed from the output of bliss, on enumerated search spaces. Finally, b-walks are applied to explore larger search spaces with the help of Wang-Landau sampling. version:1
arxiv-1112-5980 | Search space analysis with Wang-Landau sampling and slow adaptive walks | http://arxiv.org/abs/1112.5980 | id:1112.5980 author:Susan Khor category:cs.NE  published:2011-12-27 summary:Two complementary techniques for analyzing search spaces are proposed: (i) an algorithm to detect search points with potential to be local optima; and (ii) a slightly adjusted Wang-Landau sampling algorithm to explore larger search spaces. The detection algorithm assumes that local optima are points which are easier to reach and harder to leave by a slow adaptive walker. A slow adaptive walker moves to a nearest fitter point. Thus, points with larger outgoing step sizes relative to incoming step sizes are marked using the local optima score formulae as potential local optima points (PLOPs). Defining local optima in these more general terms allows their detection within the closure of a subset of a search space, and the sampling of a search space unshackled by a particular move set. Tests are done with NK and HIFF problems to confirm that PLOPs detected in the manner proposed retain characteristics of local optima, and that the adjusted Wang-Landau samples are more representative of the search space than samples produced by choosing points uniformly at random. While our approach shows promise, more needs to be done to reduce its computation cost that it may pave a way toward analyzing larger search spaces of practical meaning. version:2
arxiv-1112-2988 | Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns | http://arxiv.org/abs/1112.2988 | id:1112.2988 author:Tsvi Achler category:cs.CV  published:2011-12-13 summary:Matching animal-like flexibility in recognition and the ability to quickly incorporate new information remains difficult. Limits are yet to be adequately addressed in neural models and recognition algorithms. This work proposes a configuration for recognition that maintains the same function of conventional algorithms but avoids combinatorial problems. Feedforward recognition algorithms such as classical artificial neural networks and machine learning algorithms are known to be subject to catastrophic interference and forgetting. Modifying or learning new information (associations between patterns and labels) causes loss of previously learned information. I demonstrate using mathematical analysis how supervised generative models, with feedforward and feedback connections, can emulate feedforward algorithms yet avoid catastrophic interference and forgetting. Learned information in generative models is stored in a more intuitive form that represents the fixed points or solutions of the network and moreover displays similar difficulties as cognitive phenomena. Brain-like capabilities and limits associated with generative models suggest the brain may perform recognition and store information using a similar approach. Because of the central role of recognition, progress understanding the underlying principles may reveal significant insight on how to better study and integrate with the brain. version:2
arxiv-1206-5384 | Keyphrase Based Arabic Summarizer (KPAS) | http://arxiv.org/abs/1206.5384 | id:1206.5384 author:Tarek El-Shishtawy, Fatma El-Ghannam category:cs.CL cs.AI  published:2012-06-23 summary:This paper describes a computationally inexpensive and efficient generic summarization algorithm for Arabic texts. The algorithm belongs to extractive summarization family, which reduces the problem into representative sentences identification and extraction sub-problems. Important keyphrases of the document to be summarized are identified employing combinations of statistical and linguistic features. The sentence extraction algorithm exploits keyphrases as the primary attributes to rank a sentence. The present experimental work, demonstrates different techniques for achieving various summarization goals including: informative richness, coverage of both main and auxiliary topics, and keeping redundancy to a minimum. A scoring scheme is then adopted that balances between these summarization goals. To evaluate the resulted Arabic summaries with well-established systems, aligned English/Arabic texts are used through the experiments. version:1
arxiv-1206-5360 | Analysis of a Nature Inspired Firefly Algorithm based Back-propagation Neural Network Training | http://arxiv.org/abs/1206.5360 | id:1206.5360 author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.AI cs.NE  published:2012-06-23 summary:Optimization algorithms are normally influenced by meta-heuristic approach. In recent years several hybrid methods for optimization are developed to find out a better solution. The proposed work using meta-heuristic Nature Inspired algorithm is applied with back-propagation method to train a feed-forward neural network. Firefly algorithm is a nature inspired meta-heuristic algorithm, and it is incorporated into back-propagation algorithm to achieve fast and improved convergence rate in training feed-forward neural network. The proposed technique is tested over some standard data set. It is found that proposed method produces an improved convergence within very few iteration. This performance is also analyzed and compared to genetic algorithm based back-propagation. It is observed that proposed method consumes less time to converge and providing improved convergence rate with minimum feed-forward neural network design. version:1
arxiv-1206-1088 | Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior | http://arxiv.org/abs/1206.1088 | id:1206.1088 author:Yutian Chen, Max Welling category:stat.ML cs.LG  published:2012-06-05 summary:In recent years a number of methods have been developed for automatically learning the (sparse) connectivity structure of Markov Random Fields. These methods are mostly based on L1-regularized optimization which has a number of disadvantages such as the inability to assess model uncertainty and expensive cross-validation to find the optimal regularization parameter. Moreover, the model's predictive performance may degrade dramatically with a suboptimal value of the regularization parameter (which is sometimes desirable to induce sparseness). We propose a fully Bayesian approach based on a "spike and slab" prior (similar to L0 regularization) that does not suffer from these shortcomings. We develop an approximate MCMC method combining Langevin dynamics and reversible jump MCMC to conduct inference in this model. Experiments show that the proposed model learns a good combination of the structure and parameter values without the need for separate hyper-parameter tuning. Moreover, the model's predictive performance is much more robust than L1-based methods with hyper-parameter settings that induce highly sparse model structures. version:2
arxiv-1109-0882 | Moving Object Detection by Detecting Contiguous Outliers in the Low-Rank Representation | http://arxiv.org/abs/1109.0882 | id:1109.0882 author:Xiaowei Zhou, Can Yang, Weichuan Yu category:cs.CV  published:2011-09-05 summary:Object detection is a fundamental step for automated video analysis in many vision applications. Object detection in a video is usually performed by object detectors or background subtraction techniques. Often, an object detector requires manually labeled examples to train a binary classifier, while background subtraction needs a training sequence that contains no objects to build a background model. To automate the analysis, object detection without a separate training phase becomes a critical task. People have tried to tackle this task by using motion information. But existing motion-based methods are usually limited when coping with complex scenarios such as nonrigid motion and dynamic background. In this paper, we show that above challenges can be addressed in a unified framework named DEtecting Contiguous Outliers in the LOw-rank Representation (DECOLOR). This formulation integrates object detection and background learning into a single process of optimization, which can be solved by an alternating algorithm efficiently. We explain the relations between DECOLOR and other sparsity-based methods. Experiments on both simulated data and real sequences demonstrate that DECOLOR outperforms the state-of-the-art approaches and it can work effectively on a wide range of complex scenarios. version:2
arxiv-1206-5157 | Leaf vein segmentation using Odd Gabor filters and morphological operations | http://arxiv.org/abs/1206.5157 | id:1206.5157 author:Vini Katyal, Aviral category:cs.CV cs.AI  published:2012-06-22 summary:Leaf vein forms the basis of leaf characterization and classification. Different species have different leaf vein patterns. It is seen that leaf vein segmentation will help in maintaining a record of all the leaves according to their specific pattern of veins thus provide an effective way to retrieve and store information regarding various plant species in database as well as provide an effective means to characterize plants on the basis of leaf vein structure which is unique for every species. The algorithm proposes a new way of segmentation of leaf veins with the use of Odd Gabor filters and the use of morphological operations for producing a better output. The Odd Gabor filter gives an efficient output and is robust and scalable as compared with the existing techniques as it detects the fine fiber like veins present in leaves much more efficiently. version:1
arxiv-1206-5102 | Hidden Markov Models with mixtures as emission distributions | http://arxiv.org/abs/1206.5102 | id:1206.5102 author:Stevenn Volant, Caroline Bérard, Marie-Laure Martin-Magniette, Stéphane Robin category:stat.ML cs.LG stat.CO  published:2012-06-22 summary:In unsupervised classification, Hidden Markov Models (HMM) are used to account for a neighborhood structure between observations. The emission distributions are often supposed to belong to some parametric family. In this paper, a semiparametric modeling where the emission distributions are a mixture of parametric distributions is proposed to get a higher flexibility. We show that the classical EM algorithm can be adapted to infer the model parameters. For the initialisation step, starting from a large number of components, a hierarchical method to combine them into the hidden states is proposed. Three likelihood-based criteria to select the components to be combined are discussed. To estimate the number of hidden states, BIC-like criteria are derived. A simulation study is carried out both to determine the best combination between the merging criteria and the model selection criteria and to evaluate the accuracy of classification. The proposed method is also illustrated using a biological dataset from the model plant Arabidopsis thaliana. A R package HMMmix is freely available on the CRAN. version:1
arxiv-1206-5065 | A generic framework for video understanding applied to group behavior recognition | http://arxiv.org/abs/1206.5065 | id:1206.5065 author:Sofia Zaidenberg, Bernard Boulay, François Bremond category:cs.CV  published:2012-06-22 summary:This paper presents an approach to detect and track groups of people in video-surveillance applications, and to automatically recognize their behavior. This method keeps track of individuals moving together by maintaining a spacial and temporal group coherence. First, people are individually detected and tracked. Second, their trajectories are analyzed over a temporal window and clustered using the Mean-Shift algorithm. A coherence value describes how well a set of people can be described as a group. Furthermore, we propose a formal event description language. The group events recognition approach is successfully validated on 4 camera views from 3 datasets: an airport, a subway, a shopping center corridor and an entrance hall. version:1
arxiv-1206-4968 | Convergence of the Continuous Time Trajectories of Isotropic Evolution Strategies on Monotonic C^2-composite Functions | http://arxiv.org/abs/1206.4968 | id:1206.4968 author:Youhei Akimoto, Anne Auger, Nikolaus Hansen category:cs.NE math.OC  published:2012-06-21 summary:The Information-Geometric Optimization (IGO) has been introduced as a unified framework for stochastic search algorithms. Given a parametrized family of probability distributions on the search space, the IGO turns an arbitrary optimization problem on the search space into an optimization problem on the parameter space of the probability distribution family and defines a natural gradient ascent on this space. From the natural gradients defined over the entire parameter space we obtain continuous time trajectories which are the solutions of an ordinary differential equation (ODE). Via discretization, the IGO naturally defines an iterated gradient ascent algorithm. Depending on the chosen distribution family, the IGO recovers several known algorithms such as the pure rank-\mu update CMA-ES. Consequently, the continuous time IGO-trajectory can be viewed as an idealization of the original algorithm. In this paper we study the continuous time trajectories of the IGO given the family of isotropic Gaussian distributions. These trajectories are a deterministic continuous time model of the underlying evolution strategy in the limit for population size to infinity and change rates to zero. On functions that are the composite of a monotone and a convex-quadratic function, we prove the global convergence of the solution of the ODE towards the global optimum. We extend this result to composites of monotone and twice continuously differentiable functions and prove local convergence towards local optima. version:1
arxiv-1206-4958 | A Pointillism Approach for Natural Language Processing of Social Media | http://arxiv.org/abs/1206.4958 | id:1206.4958 author:Peiyou Song, Anhei Shu, Anyu Zhou, Dan Wallach, Jedidiah R. Crandall category:cs.IR cs.CL cs.SI  published:2012-06-21 summary:The Chinese language poses challenges for natural language processing based on the unit of a word even for formal uses of the Chinese language, social media only makes word segmentation in Chinese even more difficult. In this document we propose a pointillism approach to natural language processing. Rather than words that have individual meanings, the basic unit of a pointillism approach is trigrams of characters. These grams take on meaning in aggregate when they appear together in a way that is correlated over time. Our results from three kinds of experiments show that when words and topics do have a meme-like trend, they can be reconstructed from only trigrams. For example, for 4-character idioms that appear at least 99 times in one day in our data, the unconstrained precision (that is, precision that allows for deviation from a lexicon when the result is just as correct as the lexicon version of the word or phrase) is 0.93. For longer words and phrases collected from Wiktionary, including neologisms, the unconstrained precision is 0.87. We consider these results to be very promising, because they suggest that it is feasible for a machine to reconstruct complex idioms, phrases, and neologisms with good precision without any notion of words. Thus the colorful and baroque uses of language that typify social media in challenging languages such as Chinese may in fact be accessible to machines. version:1
arxiv-1202-2770 | Multi-Level Error-Resilient Neural Networks with Learning | http://arxiv.org/abs/1202.2770 | id:1202.2770 author:Amir Hesam Salavati, Amin Karbasi category:cs.NE cs.AI cs.IT math.IT  published:2012-02-13 summary:The problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons. An ideal neural network should include three components simultaneously: a learning algorithm, a large pattern retrieval capacity and resilience against noise. Prior works in this area usually improve one or two aspects at the cost of the third. Our work takes a step forward in closing this gap. More specifically, we show that by forcing natural constraints on the set of learning patterns, we can drastically improve the retrieval capacity of our neural network. Moreover, we devise a learning algorithm whose role is to learn those patterns satisfying the above mentioned constraints. Finally we show that our neural network can cope with a fair amount of noise. version:4
arxiv-1206-4866 | Portraits of Julius Caesar: a proposal for 3D analysis | http://arxiv.org/abs/1206.4866 | id:1206.4866 author:Amelia Carolina Sparavigna category:cs.CV  published:2012-06-21 summary:Here I suggest the use of a 3D scanning and rendering to create some virtual copies of ancient artifacts to study and compare them. In particular, this approach could be interesting for some roman marble busts, two of which are portraits of Julius Caesar, and the third is a realistic portrait of a man recently found at Arles, France. The comparison of some images indicates that a three-dimensional visualization is necessary. version:1
arxiv-1203-5443 | Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA | http://arxiv.org/abs/1203.5443 | id:1203.5443 author:Martin Pelikan, Mark W. Hauschild, Pier Luca Lanzi category:cs.NE cs.AI cs.LG I.2.6; I.2.8; G.1.6  published:2012-03-24 summary:An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups. version:2
arxiv-1204-0566 | The Kernelized Stochastic Batch Perceptron | http://arxiv.org/abs/1204.0566 | id:1204.0566 author:Andrew Cotter, Shai Shalev-Shwartz, Nathan Srebro category:cs.LG  published:2012-04-03 summary:We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives. version:2
arxiv-1104-4595 | Scaled Sparse Linear Regression | http://arxiv.org/abs/1104.4595 | id:1104.4595 author:Tingni Sun, Cun-Hui Zhang category:stat.ML math.ST stat.TH  published:2011-04-24 summary:Scaled sparse linear regression jointly estimates the regression coefficients and noise level in a linear model. It chooses an equilibrium with a sparse regression method by iteratively estimating the noise level via the mean residual square and scaling the penalty in proportion to the estimated noise level. The iterative algorithm costs little beyond the computation of a path or grid of the sparse regression estimator for penalty levels above a proper threshold. For the scaled lasso, the algorithm is a gradient descent in a convex minimization of a penalized joint loss function for the regression coefficients and noise level. Under mild regularity conditions, we prove that the scaled lasso simultaneously yields an estimator for the noise level and an estimated coefficient vector satisfying certain oracle inequalities for prediction, the estimation of the noise level and the regression coefficients. These inequalities provide sufficient conditions for the consistency and asymptotic normality of the noise level estimator, including certain cases where the number of variables is of greater order than the sample size. Parallel results are provided for the least squares estimation after model selection by the scaled lasso. Numerical results demonstrate the superior performance of the proposed methods over an earlier proposal of joint convex minimization. version:2
arxiv-1206-5293 | On Sensitivity of the MAP Bayesian Network Structure to the Equivalent Sample Size Parameter | http://arxiv.org/abs/1206.5293 | id:1206.5293 author:Tomi Silander, Petri Kontkanen, Petri Myllymaki category:cs.LG stat.ML  published:2012-06-20 summary:BDeu marginal likelihood score is a popular model selection criterion for selecting a Bayesian network structure based on sample data. This non-informative scoring criterion assigns same score for network structures that encode same independence statements. However, before applying the BDeu score, one must determine a single parameter, the equivalent sample size alpha. Unfortunately no generally accepted rule for determining the alpha parameter has been suggested. This is disturbing, since in this paper we show through a series of concrete experiments that the solution of the network structure optimization problem is highly sensitive to the chosen alpha parameter value. Based on these results, we are able to give explanations for how and why this phenomenon happens, and discuss ideas for solving this problem. version:1
arxiv-1206-5291 | Improved Dynamic Schedules for Belief Propagation | http://arxiv.org/abs/1206.5291 | id:1206.5291 author:Charles Sutton, Andrew McCallum category:cs.LG cs.AI stat.ML  published:2012-06-20 summary:Belief propagation and its variants are popular methods for approximate inference, but their running time and even their convergence depend greatly on the schedule used to send the messages. Recently, dynamic update schedules have been shown to converge much faster on hard networks than static schedules, namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithm wastes message updates: many messages are computed solely to determine their priority, and are never actually performed. In this paper, we show that estimating the residual, rather than calculating it directly, leads to significant decreases in the number of messages required for convergence, and in the total running time. The residual is estimated using an upper bound based on recent work on message errors in BP. On both synthetic and real-world networks, this dramatically decreases the running time of BP, in some cases by a factor of five, without affecting the quality of the solution. version:1
arxiv-1206-5290 | Imitation Learning with a Value-Based Prior | http://arxiv.org/abs/1206.5290 | id:1206.5290 author:Umar Syed, Robert E. Schapire category:cs.LG cs.AI stat.ML  published:2012-06-20 summary:The goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior. Accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor. We present a novel approach to encoding prior knowledge about the correct behavior, where we assume that this prior knowledge takes the form of a Markov Decision Process (MDP) that is used by the apprentice as a rough and imperfect model of the mentor's behavior. Specifically, taking a Bayesian approach, we treat the value of a policy in this modeling MDP as the log prior probability of the policy. In other words, we assume a priori that the mentor's behavior is likely to be a high value policy in the modeling MDP, though quite possibly different from the optimal policy. We describe an efficient algorithm that, given a modeling MDP and a set of demonstrations by a mentor, provably converges to a stationary point of the log posterior of the mentor's policy, where the posterior is computed with respect to the "value based" prior. We also present empirical evidence that this prior does in fact speed learning of the mentor's policy, and is an improvement in our experiments over similar previous methods. version:1
arxiv-1206-5286 | MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies | http://arxiv.org/abs/1206.5286 | id:1206.5286 author:Yair Weiss, Chen Yanover, Talya Meltzer category:cs.AI cs.LG stat.ML  published:2012-06-20 summary:Finding the most probable assignment (MAP) in a general graphical model is known to be NP hard but good approximations have been attained with max-product belief propagation (BP) and its variants. In particular, it is known that using BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties. In this paper we extend the setting under which BP can be used to provably extract the MAP. We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single-cycle, tree reweighted BP and many other BP variants. We show that when there are no ties, fixed-points of convex max-product BP will provably give the MAP solution. We also show that convex sum-product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem. Finally, we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties. In experiments, we show that our theorems allow us to find the MAP in many real-world instances of graphical models where exact inference using junction-tree is impossible. version:1
arxiv-1206-5283 | Bayesian Active Distance Metric Learning | http://arxiv.org/abs/1206.5283 | id:1206.5283 author:Liu Yang, Rong Jin, Rahul Sukthankar category:cs.LG stat.ML  published:2012-06-20 summary:Distance metric learning is an important component for many tasks, such as statistical classification and content-based image retrieval. Existing approaches for learning distance metrics from pairwise constraints typically suffer from two major problems. First, most algorithms only offer point estimation of the distance metric and can therefore be unreliable when the number of training examples is small. Second, since these algorithms generally select their training examples at random, they can be inefficient if labeling effort is limited. This paper presents a Bayesian framework for distance metric learning that estimates a posterior distribution for the distance metric from labeled pairwise constraints. We describe an efficient algorithm based on the variational method for the proposed Bayesian approach. Furthermore, we apply the proposed Bayesian framework to active distance metric learning by selecting those unlabeled example pairs with the greatest uncertainty in relative distance. Experiments in classification demonstrate that the proposed framework achieves higher classification accuracy and identifies more informative training examples than the non-Bayesian approach and state-of-the-art distance metric learning algorithms. version:1
arxiv-1206-5282 | A Characterization of Markov Equivalence Classes for Directed Acyclic Graphs with Latent Variables | http://arxiv.org/abs/1206.5282 | id:1206.5282 author:Jiji Zhang category:stat.ME cs.LG stat.ML  published:2012-06-20 summary:Different directed acyclic graphs (DAGs) may be Markov equivalent in the sense that they entail the same conditional independence relations among the observed variables. Meek (1995) characterizes Markov equivalence classes for DAGs (with no latent variables) by presenting a set of orientation rules that can correctly identify all arrow orientations shared by all DAGs in a Markov equivalence class, given a member of that class. For DAG models with latent variables, maximal ancestral graphs (MAGs) provide a neat representation that facilitates model search. Earlier work (Ali et al. 2005) has identified a set of orientation rules sufficient to construct all arrowheads common to a Markov equivalence class of MAGs. In this paper, we provide extra rules sufficient to construct all common tails as well. We end up with a set of orientation rules sound and complete for identifying commonalities across a Markov equivalence class of MAGs, which is particularly useful for causal inference. version:1
arxiv-1206-5281 | Learning Selectively Conditioned Forest Structures with Applications to DBNs and Classification | http://arxiv.org/abs/1206.5281 | id:1206.5281 author:Brian D. Ziebart, Anind K. Dey, J Andrew Bagnell category:cs.LG stat.ML  published:2012-06-20 summary:Dealing with uncertainty in Bayesian Network structures using maximum a posteriori (MAP) estimation or Bayesian Model Averaging (BMA) is often intractable due to the superexponential number of possible directed, acyclic graphs. When the prior is decomposable, two classes of graphs where efficient learning can take place are tree structures, and fixed-orderings with limited in-degree. We show how MAP estimates and BMA for selectively conditioned forests (SCF), a combination of these two classes, can be computed efficiently for ordered sets of variables. We apply SCFs to temporal data to learn Dynamic Bayesian Networks having an intra-timestep forest and inter-timestep limited in-degree structure, improving model accuracy over DBNs without the combination of structures. We also apply SCFs to Bayes Net classification to learn selective forest augmented Naive Bayes classifiers. We argue that the built-in feature selection of selective augmented Bayes classifiers makes them preferable to similar non-selective classifiers based on empirical evidence. version:1
arxiv-1206-5278 | Fast Nonparametric Conditional Density Estimation | http://arxiv.org/abs/1206.5278 | id:1206.5278 author:Michael P. Holmes, Alexander G. Gray, Charles Lee Isbell category:stat.ME cs.LG stat.ML  published:2012-06-20 summary:Conditional density estimation generalizes regression by modeling a full density f(yjx) rather than only the expected value E(yjx). This is important for many tasks, including handling multi-modality and generating prediction intervals. Though fundamental and widely applicable, nonparametric conditional density estimators have received relatively little attention from statisticians and little or none from the machine learning community. None of that work has been applied to greater than bivariate data, presumably due to the computational difficulty of data-driven bandwidth selection. We describe the double kernel conditional density estimator and derive fast dual-tree-based algorithms for bandwidth selection using a maximum likelihood criterion. These techniques give speedups of up to 3.8 million in our experiments, and enable the first applications to previously intractable large multivariate datasets, including a redshift prediction problem from the Sloan Digital Sky Survey. version:1
