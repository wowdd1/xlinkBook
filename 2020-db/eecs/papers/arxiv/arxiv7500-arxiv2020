arxiv-1411-0652 | Clustering memes in social media streams | http://arxiv.org/abs/1411.0652 | id:1411.0652 author:Mohsen JafariAsbagh, Emilio Ferrara, Onur Varol, Filippo Menczer, Alessandro Flammini category:cs.SI cs.CY cs.LG physics.soc-ph  published:2014-11-03 summary:The problem of clustering content in social media has pervasive applications, including the identification of discussion topics, event detection, and content recommendation. Here we describe a streaming framework for online detection and clustering of memes in social media, specifically Twitter. A pre-clustering procedure, namely protomeme detection, first isolates atomic tokens of information carried by the tweets. Protomemes are thereafter aggregated, based on multiple similarity measures, to obtain memes as cohesive groups of tweets reflecting actual concepts or topics of discussion. The clustering algorithm takes into account various dimensions of the data and metadata, including natural language, the social network, and the patterns of information diffusion. As a result, our system can build clusters of semantically, structurally, and topically related tweets. The clustering process is based on a variant of Online K-means that incorporates a memory mechanism, used to "forget" old memes and replace them over time with the new ones. The evaluation of our framework is carried out by using a dataset of Twitter trending topics. Over a one-week period, we systematically determined whether our algorithm was able to recover the trending hashtags. We show that the proposed method outperforms baseline algorithms that only use content features, as well as a state-of-the-art event detection method that assumes full knowledge of the underlying follower network. We finally show that our online learning framework is flexible, due to its independence of the adopted clustering algorithm, and best suited to work in a streaming scenario. version:1
arxiv-1411-0630 | Active Inference for Binary Symmetric Hidden Markov Models | http://arxiv.org/abs/1411.0630 | id:1411.0630 author:Armen E. Allahverdyan, Aram Galstyan category:stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT  published:2014-11-03 summary:We consider active maximum a posteriori (MAP) inference problem for Hidden Markov Models (HMM), where, given an initial MAP estimate of the hidden sequence, we select to label certain states in the sequence to improve the estimation accuracy of the remaining states. We develop an analytical approach to this problem for the case of binary symmetric HMMs, and obtain a closed form solution that relates the expected error reduction to model parameters under the specified active inference scheme. We then use this solution to determine most optimal active inference scheme in terms of error reduction, and examine the relation of those schemes to heuristic principles of uncertainty reduction and solution unicity. version:1
arxiv-1411-0602 | Factorbird - a Parameter Server Approach to Distributed Matrix Factorization | http://arxiv.org/abs/1411.0602 | id:1411.0602 author:Sebastian Schelter, Venu Satuluri, Reza Zadeh category:cs.LG  published:2014-11-03 summary:We present Factorbird, a prototype of a parameter server approach for factorizing large matrices with Stochastic Gradient Descent-based algorithms. We designed Factorbird to meet the following desiderata: (a) scalability to tall and wide matrices with dozens of billions of non-zeros, (b) extensibility to different kinds of models and loss functions as long as they can be optimized using Stochastic Gradient Descent (SGD), and (c) adaptability to both batch and streaming scenarios. Factorbird uses a parameter server in order to scale to models that exceed the memory of an individual machine, and employs lock-free Hogwild!-style learning with a special partitioning scheme to drastically reduce conflicting updates. We also discuss other aspects of the design of our system such as how to efficiently grid search for hyperparameters at scale. We present experiments of Factorbird on a matrix built from a subset of Twitter's interaction graph, consisting of more than 38 billion non-zeros and about 200 million rows and columns, which is to the best of our knowledge the largest matrix on which factorization results have been reported in the literature. version:1
arxiv-1411-0591 | Bayesian feature selection with strongly-regularizing priors maps to the Ising Model | http://arxiv.org/abs/1411.0591 | id:1411.0591 author:Charles K. Fisher, Pankaj Mehta category:cond-mat.stat-mech cs.LG stat.ML  published:2014-11-03 summary:Identifying small subsets of features that are relevant for prediction and/or classification tasks is a central problem in machine learning and statistics. The feature selection task is especially important, and computationally difficult, for modern datasets where the number of features can be comparable to, or even exceed, the number of samples. Here, we show that feature selection with Bayesian inference takes a universal form and reduces to calculating the magnetizations of an Ising model, under some mild conditions. Our results exploit the observation that the evidence takes a universal form for strongly-regularizing priors --- priors that have a large effect on the posterior probability even in the infinite data limit. We derive explicit expressions for feature selection for generalized linear models, a large class of statistical techniques that include linear and logistic regression. We illustrate the power of our approach by analyzing feature selection in a logistic regression-based classifier trained to distinguish between the letters B and D in the notMNIST dataset. version:1
arxiv-1411-0588 | On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using GATE | http://arxiv.org/abs/1411.0588 | id:1411.0588 author:Nadezhda Borisova, Grigor Iliev, Elena Karashtranova category:cs.CL  published:2014-11-03 summary:In this article, we describe an approach for automatic detection of noun-adjective agreement errors in Bulgarian texts by explaining the necessary steps required to develop a simple Java-based language processing application. For this purpose, we use the GATE language processing framework, which is capable of analyzing texts in Bulgarian language and can be embedded in software applications, accessed through a set of Java APIs. In our example application we also demonstrate how to use the functionality of GATE to perform regular expressions over annotations for detecting agreement errors in simple noun phrases formed by two words - attributive adjective and a noun, where the attributive adjective precedes the noun. The provided code samples can also be used as a starting point for implementing natural language processing functionalities in software applications related to language processing tasks like detection, annotation and retrieval of word groups meeting a specific set of criteria. version:1
arxiv-1411-0582 | Affective Facial Expression Processing via Simulation: A Probabilistic Model | http://arxiv.org/abs/1411.0582 | id:1411.0582 author:Jonathan Vitale, Mary-Anne Williams, Benjamin Johnston, Giuseppe Boccignone category:cs.CV  published:2014-11-03 summary:Understanding the mental state of other people is an important skill for intelligent agents and robots to operate within social environments. However, the mental processes involved in `mind-reading' are complex. One explanation of such processes is Simulation Theory - it is supported by a large body of neuropsychological research. Yet, determining the best computational model or theory to use in simulation-style emotion detection, is far from being understood. In this work, we use Simulation Theory and neuroscience findings on Mirror-Neuron Systems as the basis for a novel computational model, as a way to handle affective facial expressions. The model is based on a probabilistic mapping of observations from multiple identities onto a single fixed identity (`internal transcoding of external stimuli'), and then onto a latent space (`phenomenological response'). Together with the proposed architecture we present some promising preliminary results version:1
arxiv-1405-3222 | Efficient Implementations of the Generalized Lasso Dual Path Algorithm | http://arxiv.org/abs/1405.3222 | id:1405.3222 author:Taylor Arnold, Ryan Tibshirani category:stat.CO cs.LG stat.ML  published:2014-05-13 summary:We consider efficient implementations of the generalized lasso dual path algorithm of Tibshirani and Taylor (2011). We first describe a generic approach that covers any penalty matrix D and any (full column rank) matrix X of predictor variables. We then describe fast implementations for the special cases of trend filtering problems, fused lasso problems, and sparse fused lasso problems, both with X=I and a general matrix X. These specialized implementations offer a considerable improvement over the generic implementation, both in terms of numerical stability and efficiency of the solution path computation. These algorithms are all available for use in the genlasso R package, which can be found in the CRAN repository. version:2
arxiv-1303-0665 | Personalized News Recommendation with Context Trees | http://arxiv.org/abs/1303.0665 | id:1303.0665 author:Florent Garcin, Christos Dimitrakakis, Boi Faltings category:cs.IR cs.LG stat.ML  published:2013-03-04 summary:The profusion of online news articles makes it difficult to find interesting articles, a problem that can be assuaged by using a recommender system to bring the most relevant news stories to readers. However, news recommendation is challenging because the most relevant articles are often new content seen by few users. In addition, they are subject to trends and preference changes over time, and in many cases we do not have sufficient information to profile the reader. In this paper, we introduce a class of news recommendation systems based on context trees. They can provide high-quality news recommendation to anonymous visitors based on present browsing behaviour. We show that context-tree recommender systems provide good prediction accuracy and recommendation novelty, and they are sufficiently flexible to capture the unique properties of news articles. version:2
arxiv-1411-0541 | Distributed Submodular Maximization | http://arxiv.org/abs/1411.0541 | id:1411.0541 author:Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause category:cs.LG cs.AI cs.DC cs.IR  published:2014-11-03 summary:Many large-scale machine learning problems -- clustering, non-parametric learning, kernel machines, etc. -- require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol GreeDi, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show that under certain natural conditions, performance close to the centralized approach can be achieved. We begin with monotone submodular maximization subject to a cardinality constraint, and then extend this approach to obtain approximation guarantees for (not necessarily monotone) submodular maximization subject to more general constraints including matroid or knapsack constraints.In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar based clustering on tens of millions of examples using Hadoop. version:1
arxiv-1406-2390 | Unsupervised Deep Haar Scattering on Graphs | http://arxiv.org/abs/1406.2390 | id:1406.2390 author:Xu Chen, Xiuyuan Cheng, Stéphane Mallat category:cs.LG cs.CV  published:2014-06-09 summary:The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere. version:2
arxiv-1309-0326 | Tagging Scientific Publications using Wikipedia and Natural Language Processing Tools. Comparison on the ArXiv Dataset | http://arxiv.org/abs/1309.0326 | id:1309.0326 author:Michał Łopuszyński, Łukasz Bolikowski category:cs.CL cs.DL  published:2013-09-02 summary:In this work, we compare two simple methods of tagging scientific publications with labels reflecting their content. As a first source of labels Wikipedia is employed, second label set is constructed from the noun phrases occurring in the analyzed corpus. We examine the statistical properties and the effectiveness of both approaches on the dataset consisting of abstracts from 0.7 million of scientific documents deposited in the ArXiv preprint collection. We believe that obtained tags can be later on applied as useful document features in various machine learning tasks (document similarity, clustering, topic modelling, etc.). version:3
arxiv-1408-3731 | Unsupervised Keyword Extraction from Polish Legal Texts | http://arxiv.org/abs/1408.3731 | id:1408.3731 author:Michał Jungiewicz, Michał Łopuszyński category:cs.CL  published:2014-08-16 summary:In this work, we present an application of the recently proposed unsupervised keyword extraction algorithm RAKE to a corpus of Polish legal texts from the field of public procurement. RAKE is essentially a language and domain independent method. Its only language-specific input is a stoplist containing a set of non-content words. The performance of the method heavily depends on the choice of such a stoplist, which should be domain adopted. Therefore, we complement RAKE algorithm with an automatic approach to selecting non-content words, which is based on the statistical properties of term distribution. version:2
arxiv-1411-0442 | Non Binary Local Gradient Contours for Face Recognition | http://arxiv.org/abs/1411.0442 | id:1411.0442 author:Abdullah Gubbi, Mohammad Fazle Azeem, M Sharmila Kumari category:cs.CV  published:2014-11-03 summary:As the features from the traditional Local Binary Patterns (LBP) and Local Directional Patterns (LDP) are found to be ineffective for face recognition, we have proposed a new approach derived on the basis of Information sets whereby the loss of information that occurs during the binarization is eliminated. The information sets expand the scope of fuzzy sets by connecting the attribute and the corresponding membership function value as a product. Since face is having smooth texture in a limited area, the extracted features must be highly discernible. To limit the number of features, we consider only the non overlapping windows. By the application of the information set theory we can reduce the number of feature of an image. The derived features are shown to work fairly well over eigenface, fisherface and LBP methods. version:1
arxiv-1411-0439 | Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature | http://arxiv.org/abs/1411.0439 | id:1411.0439 author:Tom Gunter, Michael A. Osborne, Roman Garnett, Philipp Hennig, Stephen J. Roberts category:stat.ML  published:2014-11-03 summary:We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples. version:1
arxiv-1411-0392 | Sparsity Constrained Graph Regularized NMF for Spectral Unmixing of Hyperspectral Data | http://arxiv.org/abs/1411.0392 | id:1411.0392 author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV  published:2014-11-03 summary:Hyperspectral images contain mixed pixels due to low spatial resolution of hyperspectral sensors. Mixed pixels are pixels containing more than one distinct material called endmembers. The presence percentages of endmembers in mixed pixels are called abundance fractions. Spectral unmixing problem refers to decomposing these pixels into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fractions, nonnegative matrix factorization methods (NMF) have been widely used for solving spectral unmixing problem. In this paper we have used graph regularized NMF (GNMF) method combined with sparseness constraint to decompose mixed pixels in hyperspectral imagery. This method preserves the geometrical structure of data while representing it in low dimensional space. Adaptive regularization parameter based on temperature schedule in simulated annealing method also has been used in this paper for the sparseness term. Proposed algorithm is applied on synthetic and real datasets. Synthetic data is generated based on endmembers from USGS spectral library. AVIRIS Cuprite dataset is used as real dataset for evaluation of proposed method. Results are quantified based on spectral angle distance (SAD) and abundance angle distance (AAD) measures. Results in comparison with other methods show that the proposed method can unmix data more effectively. Specifically for the Cuprite dataset, performance of the proposed method is approximately 10% better than the VCA and Sparse NMF in terms of root mean square of SAD. version:1
arxiv-1406-4905 | Variational Gaussian Process State-Space Models | http://arxiv.org/abs/1406.4905 | id:1406.4905 author:Roger Frigola, Yutian Chen, Carl E. Rasmussen category:cs.LG cs.RO cs.SY stat.ML  published:2014-06-18 summary:State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series. version:2
arxiv-1407-5055 | Adaptive Image Denoising by Targeted Databases | http://arxiv.org/abs/1407.5055 | id:1407.5055 author:Enming Luo, Stanley H. Chan, Truong Q. Nguyen category:cs.CV stat.ME  published:2014-06-30 summary:We propose a data-dependent denoising procedure to restore noisy images. Different from existing denoising algorithms which search for patches from either the noisy image or a generic database, the new algorithm finds patches from a database that contains only relevant patches. We formulate the denoising problem as an optimal filter design problem and make two contributions. First, we determine the basis function of the denoising filter by solving a group sparsity minimization problem. The optimization formulation generalizes existing denoising algorithms and offers systematic analysis of the performance. Improvement methods are proposed to enhance the patch search process. Second, we determine the spectral coefficients of the denoising filter by considering a localized Bayesian prior. The localized prior leverages the similarity of the targeted database, alleviates the intensive Bayesian computation, and links the new method to the classical linear minimum mean squared error estimation. We demonstrate applications of the proposed method in a variety of scenarios, including text images, multiview images and face images. Experimental results show the superiority of the new algorithm over existing methods. version:3
arxiv-1411-0347 | Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares | http://arxiv.org/abs/1411.0347 | id:1411.0347 author:Mert Pilanci, Martin J. Wainwright category:math.OC cs.IT cs.LG math.IT stat.ML  published:2014-11-03 summary:We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including $\ell_1$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment. version:1
arxiv-1408-1143 | Machine learning for many-body physics: The case of the Anderson impurity model | http://arxiv.org/abs/1408.1143 | id:1408.1143 author:Louis-François Arsenault, Alejandro Lopez-Bezanilla, O. Anatole von Lilienfeld, Andrew J. Millis category:cond-mat.str-el stat.ML  published:2014-08-05 summary:Machine learning methods are applied to finding the Green's function of the Anderson impurity model, a basic model system of quantum many-body condensed-matter physics. Different methods of parametrizing the Green's function are investigated; a representation in terms of Legendre polynomials is found to be superior due to its limited number of coefficients and its applicability to state of the art methods of solution. The dependence of the errors on the size of the training set is determined. The results indicate that a machine learning approach to dynamical mean-field theory may be feasible. version:2
arxiv-1401-0730 | What is usual in unusual videos? Trajectory snippet histograms for discovering unusualness | http://arxiv.org/abs/1401.0730 | id:1401.0730 author:Ahmet Iscen, Anil Armagan, Pinar Duygulu category:cs.CV  published:2014-01-03 summary:Unusual events are important as being possible indicators of undesired consequences. Moreover, unusualness in everyday life activities may also be amusing to watch as proven by the popularity of such videos shared in social media. Discovery of unusual events in videos is generally attacked as a problem of finding usual patterns, and then separating the ones that do not resemble to those. In this study, we address the problem from the other side, and try to answer what type of patterns are shared among unusual videos that make them resemble to each other regardless of the ongoing event. With this challenging problem at hand, we propose a novel descriptor to encode the rapid motions in videos utilizing densely extracted trajectories. The proposed descriptor, which is referred to as trajectory snipped histograms, is used to distinguish unusual videos from usual videos, and further exploited to discover snapshots in which unusualness happen. Experiments on domain specific people falling videos and unrestricted funny videos show the effectiveness of our method in capturing unusualness. version:2
arxiv-1411-0288 | A General Framework for Mixed Graphical Models | http://arxiv.org/abs/1411.0288 | id:1411.0288 author:Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Yulia Baker, Ying-Wooi Wan, Zhandong Liu category:math.ST stat.ML stat.TH  published:2014-11-02 summary:"Mixed Data" comprising a large number of heterogeneous variables (e.g. count, binary, continuous, skewed continuous, among other data types) are prevalent in varied areas such as genomics and proteomics, imaging genetics, national security, social networking, and Internet advertising. There have been limited efforts at statistically modeling such mixed data jointly, in part because of the lack of computationally amenable multivariate distributions that can capture direct dependencies between such mixed variables of different types. In this paper, we address this by introducing a novel class of Block Directed Markov Random Fields (BDMRFs). Using the basic building block of node-conditional univariate exponential families from Yang et al. (2012), we introduce a class of mixed conditional random field distributions, that are then chained according to a block-directed acyclic graph to form our class of Block Directed Markov Random Fields (BDMRFs). The Markov independence graph structure underlying a BDMRF thus has both directed and undirected edges. We introduce conditions under which these distributions exist and are normalizable, study several instances of our models, and propose scalable penalized conditional likelihood estimators with statistical guarantees for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing expression data and mutation data demonstrate the versatility of our methods. version:1
arxiv-1411-0282 | Noisy Matrix Completion under Sparse Factor Models | http://arxiv.org/abs/1411.0282 | id:1411.0282 author:Akshay Soni, Swayambhoo Jain, Jarvis Haupt, Stefano Gonella category:stat.ML cs.IT math.IT stat.AP  published:2014-11-02 summary:This paper examines a general class of noisy matrix completion tasks where the goal is to estimate a matrix from observations obtained at a subset of its entries, each of which is subject to random noise or corruption. Our specific focus is on settings where the matrix to be estimated is well-approximated by a product of two (a priori unknown) matrices, one of which is sparse. Such structural models - referred to here as "sparse factor models" - have been widely used, for example, in subspace clustering applications, as well as in contemporary sparse modeling and dictionary learning tasks. Our main theoretical contributions are estimation error bounds for sparsity-regularized maximum likelihood estimators for problems of this form, which are applicable to a number of different observation noise or corruption models. Several specific implications are examined, including scenarios where observations are corrupted by additive Gaussian noise or additive heavier-tailed (Laplace) noise, Poisson-distributed observations, and highly-quantized (e.g., one-bit) observations. We also propose a simple algorithmic approach based on the alternating direction method of multipliers for these tasks, and provide experimental evidence to support our error analyses. version:1
arxiv-1311-3755 | Deterministic Bayesian Information Fusion and the Analysis of its Performance | http://arxiv.org/abs/1311.3755 | id:1311.3755 author:Gaurav Thakur category:math.ST stat.ML stat.TH 62C10  49K30  46N30  published:2013-11-15 summary:This paper develops a mathematical and computational framework for analyzing the expected performance of Bayesian data fusion, or joint statistical inference, within a sensor network. We use variational techniques to obtain the posterior expectation as the optimal fusion rule under a deterministic constraint and a quadratic cost, and study the smoothness and other properties of its classification performance. For a certain class of fusion problems, we prove that this fusion rule is also optimal in a much wider sense and satisfies strong asymptotic convergence results. We show how these results apply to a variety of examples with Gaussian, exponential and other statistics, and discuss computational methods for determining the fusion system's performance in more general, large-scale problems. These results are motivated by studying the performance of fusing multi-modal radar and acoustic sensors for detecting explosive substances, but have broad applicability to other Bayesian decision problems. version:4
arxiv-1411-0247 | Random feedback weights support learning in deep neural networks | http://arxiv.org/abs/1411.0247 | id:1411.0247 author:Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman category:q-bio.NC cs.NE  published:2014-11-02 summary:The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits. version:1
arxiv-1411-0217 | Cuckoo Search Inspired Hybridization of the Nelder-Mead Simplex Algorithm Applied to Optimization of Photovoltaic Cells | http://arxiv.org/abs/1411.0217 | id:1411.0217 author:Raka Jovanovic, Sabre Kais, Fahhad H. Alharbi category:cs.NE  published:2014-11-02 summary:A new hybridization of the Cuckoo Search (CS) is developed and applied to optimize multi-cell solar systems; namely multi-junction and split spectrum cells. The new approach consists of combining the CS with the Nelder-Mead method. More precisely, instead of using single solutions as nests for the CS, we use the concept of a simplex which is used in the Nelder-Mead algorithm. This makes it possible to use the flip operation introduces in the Nelder-Mead algorithm instead of the Levy flight which is a standard part of the CS. In this way, the hybridized algorithm becomes more robust and less sensitive to parameter tuning which exists in CS. The goal of our work was to optimize the performance of multi-cell solar systems. Although the underlying problem consists of the minimization of a function of a relatively small number of parameters, the difficulty comes from the fact that the evaluation of the function is complex and only a small number of evaluations is possible. In our test, we show that the new method has a better performance when compared to similar but more compex hybridizations of Nelder-Mead algorithm using genetic algorithms or particle swarm optimization on standard benchmark functions. Finally, we show that the new method outperforms some standard meta-heuristics for the problem of interest. version:1
arxiv-1201-0862 | Extension of SBL Algorithms for the Recovery of Block Sparse Signals with Intra-Block Correlation | http://arxiv.org/abs/1201.0862 | id:1201.0862 author:Zhilin Zhang, Bhaskar D. Rao category:stat.ML stat.ME  published:2012-01-04 summary:We examine the recovery of block sparse signals and extend the framework in two important directions; one by exploiting signals' intra-block correlation and the other by generalizing signals' block structure. We propose two families of algorithms based on the framework of block sparse Bayesian learning (BSBL). One family, directly derived from the BSBL framework, requires knowledge of the block structure. Another family, derived from an expanded BSBL framework, is based on a weaker assumption on the block structure, and can be used when the block structure is completely unknown. Using these algorithms we show that exploiting intra-block correlation is very helpful in improving recovery performance. These algorithms also shed light on how to modify existing algorithms or design new ones to exploit such correlation and improve performance. version:5
arxiv-1206-3493 | Compressed Sensing of EEG for Wireless Telemonitoring with Low Energy Consumption and Inexpensive Hardware | http://arxiv.org/abs/1206.3493 | id:1206.3493 author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao category:stat.AP cs.IT math.IT stat.ML  published:2012-06-13 summary:Telemonitoring of electroencephalogram (EEG) through wireless body-area networks is an evolving direction in personalized medicine. Among various constraints in designing such a system, three important constraints are energy consumption, data compression, and device cost. Conventional data compression methodologies, although effective in data compression, consumes significant energy and cannot reduce device cost. Compressed sensing (CS), as an emerging data compression methodology, is promising in catering to these constraints. However, EEG is non-sparse in the time domain and also non-sparse in transformed domains (such as the wavelet domain). Therefore, it is extremely difficult for current CS algorithms to recover EEG with the quality that satisfies the requirements of clinical diagnosis and engineering applications. Recently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method to the CS problem. This study introduces the technique to the telemonitoring of EEG. Experimental results show that its recovery quality is better than state-of-the-art CS algorithms, and sufficient for practical use. These results suggest that BSBL is very promising for telemonitoring of EEG and other non-sparse physiological signals. version:3
arxiv-1205-1287 | Compressed Sensing for Energy-Efficient Wireless Telemonitoring of Noninvasive Fetal ECG via Block Sparse Bayesian Learning | http://arxiv.org/abs/1205.1287 | id:1205.1287 author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao category:stat.ML cs.LG stat.AP  published:2012-05-07 summary:Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. The design of a telemonitoring system via a wireless body-area network with low energy consumption for ambulatory use is highly desirable. As an emerging technique, compressed sensing (CS) shows great promise in compressing/reconstructing data with low energy consumption. However, due to some specific characteristics of raw FECG recordings such as non-sparsity and strong noise contamination, current CS algorithms generally fail in this application. This work proposes to use the block sparse Bayesian learning (BSBL) framework to compress/reconstruct non-sparse raw FECG recordings. Experimental results show that the framework can reconstruct the raw recordings with high quality. Especially, the reconstruction does not destroy the interdependence relation among the multichannel recordings. This ensures that the independent component analysis decomposition of the reconstructed recordings has high fidelity. Furthermore, the framework allows the use of a sparse binary sensing matrix with much fewer nonzero entries to compress recordings. Particularly, each column of the matrix can contain only two nonzero entries. This shows the framework, compared to other algorithms such as current CS algorithms and wavelet algorithms, can greatly reduce code execution in CPU in the data compression stage. version:7
arxiv-1307-3581 | Image color transfer to evoke different emotions based on color combinations | http://arxiv.org/abs/1307.3581 | id:1307.3581 author:Li He, Hairong Qi, Russell Zaretzki category:cs.CV cs.GR  published:2013-07-12 summary:In this paper, a color transfer framework to evoke different emotions for images based on color combinations is proposed. The purpose of this color transfer is to change the "look and feel" of images, i.e., evoking different emotions. Colors are confirmed as the most attractive factor in images. In addition, various studies in both art and science areas have concluded that other than single color, color combinations are necessary to evoke specific emotions. Therefore, we propose a novel framework to transfer color of images based on color combinations, using a predefined color emotion model. The contribution of this new framework is three-fold. First, users do not need to provide reference images as used in traditional color transfer algorithms. In most situations, users may not have enough aesthetic knowledge or path to choose desired reference images. Second, because of the usage of color combinations instead of single color for emotions, a new color transfer algorithm that does not require an image library is proposed. Third, again because of the usage of color combinations, artifacts that are normally seen in traditional frameworks using single color are avoided. We present encouraging results generated from this new framework and its potential in several possible applications including color transfer of photos and paintings. version:2
arxiv-1312-3516 | Density Estimation in Infinite Dimensional Exponential Families | http://arxiv.org/abs/1312.3516 | id:1312.3516 author:Bharath Sriperumbudur, Kenji Fukumizu, Revant Kumar, Arthur Gretton, Aapo Hyvärinen category:math.ST stat.ME stat.ML stat.TH  published:2013-12-12 summary:In this paper, we consider an infinite dimensional exponential family, $\mathcal{P}$ of probability densities, which are parametrized by functions in a reproducing kernel Hilbert space, $H$ and show it to be quite rich in the sense that a broad class of densities on $\mathbb{R}^d$ can be approximated arbitrarily well in Kullback-Leibler (KL) divergence by elements in $\mathcal{P}$. The main goal of the paper is to estimate an unknown density, $p_0$ through an element in $\mathcal{P}$. Standard techniques like maximum likelihood estimation (MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing the KL divergence between $p_0$ and $\mathcal{P}$, do not yield practically useful estimators because of their inability to efficiently handle the log-partition function. Instead, we propose an estimator, $\hat{p}_n$ based on minimizing the \emph{Fisher divergence}, $J(p_0\Vert p)$ between $p_0$ and $p\in \mathcal{P}$, which involves solving a simple finite-dimensional linear system. When $p_0\in\mathcal{P}$, we show that the proposed estimator is consistent, and provide a convergence rate of $n^{-\min\left\{\frac{2}{3},\frac{2\beta+1}{2\beta+2}\right\}}$ in Fisher divergence under the smoothness assumption that $\log p_0\in\mathcal{R}(C^\beta)$ for some $\beta\ge 0$, where $C$ is a certain Hilbert-Schmidt operator on $H$ and $\mathcal{R}(C^\beta)$ denotes the image of $C^\beta$. We also investigate the misspecified case of $p_0\notin\mathcal{P}$ and show that $J(p_0\Vert\hat{p}_n)\rightarrow \inf_{p\in\mathcal{P}}J(p_0\Vert p)$ as $n\rightarrow\infty$, and provide a rate for this convergence under a similar smoothness condition as above. Through numerical simulations we demonstrate that the proposed estimator outperforms the non-parametric kernel density estimator, and that the advantage with the proposed estimator grows as $d$ increases. version:3
arxiv-1411-0189 | Synchronization Clustering based on a Linearized Version of Vicsek model | http://arxiv.org/abs/1411.0189 | id:1411.0189 author:Xinquan Chen category:cs.LG cs.DB  published:2014-11-02 summary:This paper presents a kind of effective synchronization clustering method based on a linearized version of Vicsek model. This method can be represented by an Effective Synchronization Clustering algorithm (ESynC), an Improved version of ESynC algorithm (IESynC), a Shrinking Synchronization Clustering algorithm based on another linear Vicsek model (SSynC), and an effective Multi-level Synchronization Clustering algorithm (MSynC). After some analysis and comparisions, we find that ESynC algorithm based on the Linearized version of the Vicsek model has better synchronization effect than SynC algorithm based on an extensive Kuramoto model and a similar synchronization clustering algorithm based on the original Vicsek model. By simulated experiments of some artificial data sets, we observe that ESynC algorithm, IESynC algorithm, and SSynC algorithm can get better synchronization effect although it needs less iterative times and less time than SynC algorithm. In some simulations, we also observe that IESynC algorithm and SSynC algorithm can get some improvements in time cost than ESynC algorithm. At last, it gives some research expectations to popularize this algorithm. version:1
arxiv-1403-3083 | A Novel Method to Extract Rocks from Mars Images | http://arxiv.org/abs/1403.3083 | id:1403.3083 author:Shuliang Wang, Yasen Chen category:cs.CV  published:2014-03-13 summary:In this paper, a novel method is proposed to extract rocks from Martian surface images by using 8 data field. It models the interaction between two pixels of an image in the context of imagery 9 characteristics. First, foreground rocks are differed from background information by binarizing 10 image on roughly partitioned images. Second, foreground rocks are grouped into clusters by 11 locating the centers and edges of clusters in data field via hierarchical grids. Third, the target 12 rocks are discovered for the Mars Exploration Rover (MER) to keep healthy paths. The 13 experiment with images taken by MER shows the proposed method is practical and potential. version:2
arxiv-1406-3824 | Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing | http://arxiv.org/abs/1406.3824 | id:1406.3824 author:Yuchen Zhang, Xi Chen, Dengyong Zhou, Michael I. Jordan category:stat.ML  published:2014-06-15 summary:Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods. version:3
arxiv-1411-0169 | Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms | http://arxiv.org/abs/1411.0169 | id:1411.0169 author:Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS math.ST stat.TH  published:2014-11-01 summary:Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. We consider the problem of {\em density estimation}, in which a learning algorithm is given i.i.d. draws from $p$ and must (with high probability) output a hypothesis distribution that is close to $p$. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function. In more detail, for any $k$ and $\epsilon$, we give an algorithm that makes $\tilde{O}(k/\epsilon^2)$ draws from $p$, runs in $\tilde{O}(k/\epsilon^2)$ time, and outputs a hypothesis distribution $h$ that is piecewise constant with $O(k \log^2(1/\epsilon))$ pieces. With high probability the hypothesis $h$ satisfies $d_{\mathrm{TV}}(p,h) \leq C \cdot \mathrm{opt}_k(p) + \epsilon$, where $d_{\mathrm{TV}}$ denotes the total variation distance (statistical distance), $C$ is a universal constant, and $\mathrm{opt}_k(p)$ is the smallest total variation distance between $p$ and any $k$-piecewise constant distribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The "approximation factor" $C$ in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of $k$ and $\epsilon$ can achieve $C<2$ regardless of what kind of hypothesis distribution it uses. version:1
arxiv-1411-0161 | Entropy of Overcomplete Kernel Dictionaries | http://arxiv.org/abs/1411.0161 | id:1411.0161 author:Paul Honeine category:cs.IT cs.CV cs.LG cs.NE math.IT stat.ML  published:2014-11-01 summary:In signal analysis and synthesis, linear approximation theory considers a linear decomposition of any given signal in a set of atoms, collected into a so-called dictionary. Relevant sparse representations are obtained by relaxing the orthogonality condition of the atoms, yielding overcomplete dictionaries with an extended number of atoms. More generally than the linear decomposition, overcomplete kernel dictionaries provide an elegant nonlinear extension by defining the atoms through a mapping kernel function (e.g., the gaussian kernel). Models based on such kernel dictionaries are used in neural networks, gaussian processes and online learning with kernels. The quality of an overcomplete dictionary is evaluated with a diversity measure the distance, the approximation, the coherence and the Babel measures. In this paper, we develop a framework to examine overcomplete kernel dictionaries with the entropy from information theory. Indeed, a higher value of the entropy is associated to a further uniform spread of the atoms over the space. For each of the aforementioned diversity measures, we derive lower bounds on the entropy. Several definitions of the entropy are examined, with an extensive analysis in both the input space and the mapped feature space. version:1
arxiv-1411-0130 | A Two-phase Decision Support Framework for the Automatic Screening of Digital Fundus Images | http://arxiv.org/abs/1411.0130 | id:1411.0130 author:Balint Antal, Andras Hajdu, Zsuzsanna Maros-Szabo, Zsolt Torok, Adrienne Csutak, Tunde Peto category:cs.CV  published:2014-11-01 summary:In this paper we give a brief review on the present status of automated detection systems describe for the screening of diabetic retinopathy. We further detail an enhanced detection procedure that consists of two steps. First, a pre-screening algorithm is considered to classify the input digital fundus images based on the severity of abnormalities. If an image is found to be seriously abnormal, it will not be analysed further with robust lesion detector algorithms. As a further improvement, we introduce a novel feature extraction approach based on clinical observations. The second step of the proposed method detects regions of interest with possible lesions on the images that previously passed the pre-screening step. These regions will serve as input to the specific lesion detectors for detailed analysis. This procedure can increase the computational performance of a screening system. Experimental results show that both two steps of the proposed approach are capable to efficiently exclude a large amount of data from further processing, thus, to decrease the computational burden of the automatic screening system. version:1
arxiv-1411-0126 | Detection of texts in natural images | http://arxiv.org/abs/1411.0126 | id:1411.0126 author:Gowtham Rangarajan Raman category:cs.CV  published:2014-11-01 summary:A framework that makes use of Connected components and supervised Support machine to recognise texts is proposed. The image is preprocessed and and edge graph is calculated using a probabilistic framework to compensate for photometric noise. Connected components over the resultant image is calculated, which is bounded and then pruned using geometric constraints. Finally a Gabor Feature based SVM is used to classify the presence of text in the candidates. The proposed method was tested with ICDAR 10 dataset and few other images available on the internet. It resulted in a recall and precision metric of 0.72 and 0.88 comfortably better than the benchmark Eiphstein's algorithm. The proposed method recorded a 0.70 and 0.74 in natural images which is significantly better than current methods on natural images. The proposed method also scales almost linearly for high resolution, cluttered images. version:1
arxiv-1404-5475 | Combining pattern-based CRFs and weighted context-free grammars | http://arxiv.org/abs/1404.5475 | id:1404.5475 author:Rustem Takhanov, Vladimir Kolmogorov category:cs.FL cs.DS cs.LG I.2.7  published:2014-04-22 summary:We consider two models for the sequence labeling (tagging) problem. The first one is a {\em Pattern-Based Conditional Random Field }(\PB), in which the energy of a string (chain labeling) $x=x_1\ldots x_n\in D^n$ is a sum of terms over intervals $[i,j]$ where each term is non-zero only if the substring $x_i\ldots x_j$ equals a prespecified word $w\in \Lambda$. The second model is a {\em Weighted Context-Free Grammar }(\WCFG) frequently used for natural language processing. \PB and \WCFG encode local and non-local interactions respectively, and thus can be viewed as complementary. We propose a {\em Grammatical Pattern-Based CRF model }(\GPB) that combines the two in a natural way. We argue that it has certain advantages over existing approaches such as the {\em Hybrid model} of Bened{\'i} and Sanchez that combines {\em $\mbox{$N$-grams}$} and \WCFGs. The focus of this paper is to analyze the complexity of inference tasks in a \GPB such as computing MAP. We present a polynomial-time algorithm for general \GPBs and a faster version for a special case that we call {\em Interaction Grammars}. version:2
arxiv-1411-0085 | Complex Events Recognition under Uncertainty in a Sensor Network | http://arxiv.org/abs/1411.0085 | id:1411.0085 author:Atul Kanaujia, Tae Eun Choe, Hongli Deng category:cs.CV  published:2014-11-01 summary:Automated extraction of semantic information from a network of sensors for cognitive analysis and human-like reasoning is a desired capability in future ground surveillance systems. We tackle the problem of complex decision making under uncertainty in network information environment, where lack of effective visual processing tools, incomplete domain knowledge frequently cause uncertainty in the visual primitives, leading to sub-optimal decisions. While state-of-the-art vision techniques exist in detecting visual entities (humans, vehicles and scene elements) in an image, a missing functionality is the ability to merge the information to reveal meaningful information for high level inference. In this work, we develop a probabilistic first order predicate logic(FOPL) based reasoning system for recognizing complex events in synchronized stream of videos, acquired from sensors with non-overlapping fields of view. We adopt Markov Logic Network(MLN) as a tool to model uncertainty in observations, and fuse information extracted from heterogeneous data in a probabilistically consistent way. MLN overcomes strong dependence on pure empirical learning by incorporating domain knowledge, in the form of user-defined rules and confidences associated with them. This work demonstrates that the MLN based decision control system can be made scalable to model statistical relations between a variety of entities and over long video sequences. Experiments with real-world data, under a variety of settings, illustrate the mathematical soundness and wide-ranging applicability of our approach. version:1
arxiv-1411-0073 | Learning Mixed Multinomial Logit Model from Ordinal Data | http://arxiv.org/abs/1411.0073 | id:1411.0073 author:Sewoong Oh, Devavrat Shah category:stat.ML  published:2014-11-01 summary:Motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of MultiNomial Logit (MNL) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question. In case of single MNL models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible. However, even learning mixture with two MNL components is infeasible in general. Given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. We present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular, a mixture of $r$ MNL components over $n$ objects can be learnt using samples whose size scales polynomially in $n$ and $r$ (concretely, $r^{3.5}n^3(log n)^4$, with $r\ll n^{2/7}$ when the model parameters are sufficiently incoherent). The algorithm has two phases: first, learn the pair-wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using Rank Centrality introduced by Negahban et al. In the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available. version:1
arxiv-1402-6455 | Sparse principal component regression with adaptive loading | http://arxiv.org/abs/1402.6455 | id:1402.6455 author:Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko Shiroishi category:stat.ML stat.ME 62H25  62J07  published:2014-02-26 summary:Principal component regression (PCR) is a two-stage procedure that selects some principal components and then constructs a regression model regarding them as new explanatory variables. Note that the principal components are obtained from only explanatory variables and not considered with the response variable. To address this problem, we propose the sparse principal component regression (SPCR) that is a one-stage procedure for PCR. SPCR enables us to adaptively obtain sparse principal component loadings that are related to the response variable and select the number of principal components simultaneously. SPCR can be obtained by the convex optimization problem for each of parameters with the coordinate descent algorithm. Monte Carlo simulations and real data analyses are performed to illustrate the effectiveness of SPCR. version:4
arxiv-1310-2931 | Feedback Detection for Live Predictors | http://arxiv.org/abs/1310.2931 | id:1310.2931 author:Stefan Wager, Nick Chamandy, Omkar Muralidharan, Amir Najmi category:stat.ME cs.LG stat.ML  published:2013-10-10 summary:A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine. version:2
arxiv-1407-5035 | LSDA: Large Scale Detection Through Adaptation | http://arxiv.org/abs/1407.5035 | id:1407.5035 author:Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko category:cs.CV  published:2014-07-18 summary:A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at version:3
arxiv-1406-1853 | Model-based Reinforcement Learning and the Eluder Dimension | http://arxiv.org/abs/1406.1853 | id:1406.1853 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG  published:2014-06-07 summary:We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds. version:2
arxiv-1403-3741 | Near-optimal Reinforcement Learning in Factored MDPs | http://arxiv.org/abs/1403.3741 | id:1403.3741 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG  published:2014-03-15 summary:Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP, where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. This implies $T = \Omega(SA)$ time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, $S$ and $A$ can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a \emph{factored} MDP, it is possible to achieve regret that scales polynomially in the number of \emph{parameters} encoding the factored MDP, which may be exponentially smaller than $S$ or $A$. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored). version:3
arxiv-1406-5298 | Semi-Supervised Learning with Deep Generative Models | http://arxiv.org/abs/1406.5298 | id:1406.5298 author:Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling category:cs.LG stat.ML  published:2014-06-20 summary:The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning. version:2
arxiv-1411-0022 | Generalized Adaptive Dictionary Learning via Domain Shift Minimization | http://arxiv.org/abs/1411.0022 | id:1411.0022 author:Varun Panaganti category:cs.CV  published:2014-10-31 summary:Visual data driven dictionaries have been successfully employed for various object recognition and classification tasks. However, the task becomes more challenging if the training and test data are from contrasting domains. In this paper, we propose a novel and generalized approach towards learning an adaptive and common dictionary for multiple domains. Precisely, we project the data from different domains onto a low dimensional space while preserving the intrinsic structure of data from each domain. We also minimize the domain-shift among the data from each pair of domains. Simultaneously, we learn a common adaptive dictionary. Our algorithm can also be modified to learn class-specific dictionaries which can be used for classification. We additionally propose a discriminative manifold regularization which imposes the intrinsic structure of class specific features onto the sparse coefficients. Experiments on image classification show that our approach fares better compared to the existing state-of-the-art methods. version:1
arxiv-1411-0007 | Rapid Adaptation of POS Tagging for Domain Specific Uses | http://arxiv.org/abs/1411.0007 | id:1411.0007 author:John E. Miller, Michael Bloodgood, Manabu Torii, K. Vijay-Shanker category:cs.CL cs.LG stat.ML  published:2014-10-31 summary:Part-of-speech (POS) tagging is a fundamental component for performing natural language tasks such as parsing, information extraction, and question answering. When POS taggers are trained in one domain and applied in significantly different domains, their performance can degrade dramatically. We present a methodology for rapid adaptation of POS taggers to new domains. Our technique is unsupervised in that a manually annotated corpus for the new domain is not necessary. We use suffix information gathered from large amounts of raw text as well as orthographic information to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain. version:1
arxiv-1411-6591 | A Latent Source Model for Online Collaborative Filtering | http://arxiv.org/abs/1411.6591 | id:1411.6591 author:Guy Bresler, George H. Chen, Devavrat Shah category:cs.LG cs.IR stat.ML  published:2014-10-31 summary:Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the "online" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work). version:1
arxiv-1410-8864 | Greedy Subspace Clustering | http://arxiv.org/abs/1410.8864 | id:1410.8864 author:Dohyung Park, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT  published:2014-10-31 summary:We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity between subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost. version:1
arxiv-1407-3289 | Altitude Training: Strong Bounds for Single-Layer Dropout | http://arxiv.org/abs/1407.3289 | id:1407.3289 author:Stefan Wager, William Fithian, Sida Wang, Percy Liang category:stat.ML cs.LG math.ST stat.TH  published:2014-07-11 summary:Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions. version:2
arxiv-1409-6440 | A non-linear learning & classification algorithm that achieves full training accuracy with stellar classification accuracy | http://arxiv.org/abs/1409.6440 | id:1409.6440 author:Rashid Khogali category:cs.CV cs.LG  published:2014-09-23 summary:A fast Non-linear and non-iterative learning and classification algorithm is synthesized and validated. This algorithm named the "Reverse Ripple Effect(R.R.E)", achieves 100% learning accuracy but is computationally expensive upon classification. The R.R.E is a (deterministic) algorithm that super imposes Gaussian weighted functions on training points. In this work, the R.R.E algorithm is compared against known learning and classification techniques/algorithms such as: the Perceptron Criterion algorithm, Linear Support Vector machines, the Linear Fisher Discriminant and a simple Neural Network. The classification accuracy of the R.R.E algorithm is evaluated using simulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzed under linearly and non-linearly separable data sets. For the comparison with the Neural Network, the classical XOR problem is considered. version:2
arxiv-1410-8783 | Supervised learning model for parsing Arabic language | http://arxiv.org/abs/1410.8783 | id:1410.8783 author:Nabil Khoufi, Chafik Aloulou, Lamia Hadrich Belguith category:cs.CL cs.LG I.2.7  published:2014-10-31 summary:Parsing the Arabic language is a difficult task given the specificities of this language and given the scarcity of digital resources (grammars and annotated corpora). In this paper, we suggest a method for Arabic parsing based on supervised machine learning. We used the SVMs algorithm to select the syntactic labels of the sentence. Furthermore, we evaluated our parser following the cross validation method by using the Penn Arabic Treebank. The obtained results are very encouraging. version:1
arxiv-1410-8750 | Learning Mixtures of Ranking Models | http://arxiv.org/abs/1410.8750 | id:1410.8750 author:Pranjal Awasthi, Avrim Blum, Or Sheffet, Aravindan Vijayaraghavan category:cs.LG  published:2014-10-31 summary:This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a Mallows Mixture Model. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings. Before this work, even the question of identifiability in the case of a mixture of two Mallows models was unresolved. version:1
arxiv-1410-8675 | Partition-wise Linear Models | http://arxiv.org/abs/1410.8675 | id:1410.8675 author:Hidekazu Oiwa, Ryohei Fujimaki category:stat.ML cs.LG  published:2014-10-31 summary:Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models. version:1
arxiv-1410-8668 | Experiments to Improve Named Entity Recognition on Turkish Tweets | http://arxiv.org/abs/1410.8668 | id:1410.8668 author:Dilek Küçük, Ralf Steinberger category:cs.CL  published:2014-10-31 summary:Social media texts are significant information sources for several application areas including trend analysis, event monitoring, and opinion mining. Unfortunately, existing solutions for tasks such as named entity recognition that perform well on formal texts usually perform poorly when applied to social media texts. In this paper, we report on experiments that have the purpose of improving named entity recognition on Turkish tweets, using two different annotated data sets. In these experiments, starting with a baseline named entity recognition system, we adapt its recognition rules and resources to better fit Twitter language by relaxing its capitalization constraint and by diacritics-based expansion of its lexical resources, and we employ a simplistic normalization scheme on tweets to observe the effects of these on the overall named entity recognition performance on Turkish tweets. The evaluation results of the system with these different settings are provided with discussions of these results. version:1
arxiv-1410-8623 | Addressing the non-functional requirements of computer vision systems: A case study | http://arxiv.org/abs/1410.8623 | id:1410.8623 author:Shannon Fenn, Alexandre Mendes, David Budden category:cs.CV cs.RO cs.SE  published:2014-10-31 summary:Computer vision plays a major role in the robotics industry, where vision data is frequently used for navigation and high-level decision making. Although there is significant research in algorithms and functional requirements, there is a comparative lack of emphasis on how best to map these abstract concepts onto an appropriate software architecture. In this study, we distinguish between the functional and non-functional requirements of a computer vision system. Using a RoboCup humanoid robot system as a case study, we propose and develop a software architecture that fulfills the latter criteria. The modifiability of the proposed architecture is demonstrated by detailing a number of feature detection algorithms and emphasizing which aspects of the underlying framework were modified to support their integration. To demonstrate portability, we port our vision system (designed for an application-specific DARwIn-OP humanoid robot) to a general-purpose, Raspberry Pi computer. We evaluate performance on both platforms and compare them to a vision system optimised for functional requirements only. The architecture and implementation presented in this study provide a highly generalisable framework for computer vision system design that is of particular benefit in research and development, competition and other environments in which rapid system evolution is necessary. version:1
arxiv-1406-1222 | Discovering Structure in High-Dimensional Data Through Correlation Explanation | http://arxiv.org/abs/1406.1222 | id:1406.1222 author:Greg Ver Steeg, Aram Galstyan category:cs.LG cs.AI stat.ML  published:2014-06-04 summary:We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language. version:2
arxiv-1410-8620 | A Comparison of learning algorithms on the Arcade Learning Environment | http://arxiv.org/abs/1410.8620 | id:1410.8620 author:Aaron Defazio, Thore Graepel category:cs.LG cs.AI  published:2014-10-31 summary:Reinforcement learning agents have traditionally been evaluated on small toy problems. With advances in computing power and the advent of the Arcade Learning Environment, it is now possible to evaluate algorithms on diverse and difficult problems within a consistent framework. We discuss some challenges posed by the arcade learning environment which do not manifest in simpler environments. We then provide a comparison of model-free, linear learning algorithms on this challenging problem set. version:1
arxiv-1410-8586 | DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks | http://arxiv.org/abs/1410.8586 | id:1410.8586 author:Tao Chen, Damian Borth, Trevor Darrell, Shih-Fu Chang category:cs.CV cs.LG cs.MM cs.NE H.3.3  published:2014-10-30 summary:This paper introduces a visual sentiment concept classification method based on deep convolutional neural networks (CNNs). The visual sentiment concepts are adjective noun pairs (ANPs) automatically discovered from the tags of web photos, and can be utilized as effective statistical cues for detecting emotions depicted in the images. Nearly one million Flickr images tagged with these ANPs are downloaded to train the classifiers of the concepts. We adopt the popular model of deep convolutional neural networks which recently shows great performance improvement on classifying large-scale web-based image dataset such as ImageNet. Our deep CNNs model is trained based on Caffe, a newly developed deep learning framework. To deal with the biased training data which only contains images with strong sentiment and to prevent overfitting, we initialize the model with the model weights trained from ImageNet. Performance evaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called DeepSentiBank) is significantly improved in both annotation accuracy and retrieval performance, compared to its predecessors which mainly use binary SVM classification models. version:1
arxiv-1410-8581 | Semi-Automatic Construction of a Domain Ontology for Wind Energy Using Wikipedia Articles | http://arxiv.org/abs/1410.8581 | id:1410.8581 author:Dilek Küçük, Yusuf Arslan category:cs.CL cs.CE  published:2014-10-30 summary:Domain ontologies are important information sources for knowledge-based systems. Yet, building domain ontologies from scratch is known to be a very labor-intensive process. In this study, we present our semi-automatic approach to building an ontology for the domain of wind energy which is an important type of renewable energy with a growing share in electricity generation all over the world. Related Wikipedia articles are first processed in an automated manner to determine the basic concepts of the domain together with their properties and next the concepts, properties, and relationships are organized to arrive at the ultimate ontology. We also provide pointers to other engineering ontologies which could be utilized together with the proposed wind energy ontology in addition to its prospective application areas. The current study is significant as, to the best of our knowledge, it proposes the first considerably wide-coverage ontology for the wind energy domain and the ontology is built through a semi-automatic process which makes use of the related Web resources, thereby reducing the overall cost of the ontology building process. version:1
arxiv-1410-8580 | An Online Algorithm for Learning Selectivity to Mixture Means | http://arxiv.org/abs/1410.8580 | id:1410.8580 author:Matthew Lawlor, Steven Zucker category:q-bio.NC cs.LG  published:2014-10-30 summary:We develop a biologically-plausible learning rule called Triplet BCM that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule, and provides a novel interpretation of classical BCM as performing a kind of tensor decomposition. It achieves a substantial generalization over classical BCM by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide complete proofs of convergence of this learning rule, and an extended discussion of the connection between BCM and tensor learning. version:1
arxiv-1410-8577 | An Ensemble-based System for Microaneurysm Detection and Diabetic Retinopathy Grading | http://arxiv.org/abs/1410.8577 | id:1410.8577 author:Balint Antal, Andras Hajdu category:cs.CV cs.AI stat.AP stat.ML  published:2014-10-30 summary:Reliable microaneurysm detection in digital fundus images is still an open issue in medical image processing. We propose an ensemble-based framework to improve microaneurysm detection. Unlike the well-known approach of considering the output of multiple classifiers, we propose a combination of internal components of microaneurysm detectors, namely preprocessing methods and candidate extractors. We have evaluated our approach for microaneurysm detection in an online competition, where this algorithm is currently ranked as first and also on two other databases. Since microaneurysm detection is decisive in diabetic retinopathy grading, we also tested the proposed method for this task on the publicly available Messidor database, where a promising AUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification based on the presence or absence of the microaneurysms. version:1
arxiv-1410-8576 | An ensemble-based system for automatic screening of diabetic retinopathy | http://arxiv.org/abs/1410.8576 | id:1410.8576 author:Balint Antal, Andras Hajdu category:cs.CV cs.LG stat.AP stat.ML  published:2014-10-30 summary:In this paper, an ensemble-based method for the screening of diabetic retinopathy (DR) is proposed. This approach is based on features extracted from the output of several retinal image processing algorithms, such as image-level (quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms, exudates) and anatomical (macula, optic disc) components. The actual decision about the presence of the disease is then made by an ensemble of machine learning classifiers. We have tested our approach on the publicly available Messidor database, where 90% sensitivity, 91% specificity and 90% accuracy and 0.989 AUC are achieved in a disease/no-disease setting. These results are highly competitive in this field and suggest that retinal image processing is a valid approach for automatic DR screening. version:1
arxiv-1406-0531 | Causal Inference through a Witness Protection Program | http://arxiv.org/abs/1406.0531 | id:1406.0531 author:Ricardo Silva, Robin Evans category:stat.ML  published:2014-06-02 summary:One of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest "weak" paths in a unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of "path cancellations" that imply conditional independencies but do not rule out the existence of confounding causal paths. The outcome is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice along with other default tools in observational studies. version:2
arxiv-1410-8553 | A random forest system combination approach for error detection in digital dictionaries | http://arxiv.org/abs/1410.8553 | id:1410.8553 author:Michael Bloodgood, Peng Ye, Paul Rodrigues, David Zajic, David Doermann category:cs.CL cs.LG stat.ML  published:2014-10-30 summary:When digitizing a print bilingual dictionary, whether via optical character recognition or manual entry, it is inevitable that errors are introduced into the electronic version that is created. We investigate automating the process of detecting errors in an XML representation of a digitized print dictionary using a hybrid approach that combines rule-based, feature-based, and language model-based methods. We investigate combining methods and show that using random forests is a promising approach. We find that in isolation, unsupervised methods rival the performance of supervised methods. Random forests typically require training data so we investigate how we can apply random forests to combine individual base methods that are themselves unsupervised without requiring large amounts of training data. Experiments reveal empirically that a relatively small amount of data is sufficient and can potentially be further reduced through specific selection criteria. version:1
arxiv-1410-8749 | What a Nasty day: Exploring Mood-Weather Relationship from Twitter | http://arxiv.org/abs/1410.8749 | id:1410.8749 author:Jiwei Li, Xun Wang, Eduard Hovy category:cs.SI cs.CL  published:2014-10-30 summary:While it has long been believed in psychology that weather somehow influences human's mood, the debates have been going on for decades about how they are correlated. In this paper, we try to study this long-lasting topic by harnessing a new source of data compared from traditional psychological researches: Twitter. We analyze 2 years' twitter data collected by twitter API which amounts to $10\%$ of all postings and try to reveal the correlations between multiple dimensional structure of human mood with meteorological effects. Some of our findings confirm existing hypotheses, while others contradict them. We are hopeful that our approach, along with the new data source, can shed on the long-going debates on weather-mood correlation. version:1
arxiv-1407-1785 | Novel methods for multilinear data completion and de-noising based on tensor-SVD | http://arxiv.org/abs/1407.1785 | id:1407.1785 author:Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer category:cs.CV  published:2014-07-07 summary:In this paper we propose novel methods for completion (from limited samples) and de-noising of multilinear (tensor) data and as an application consider 3-D and 4- D (color) video data completion and de-noising. We exploit the recently proposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, the notion of multilinear rank and a related tensor nuclear norm was proposed in [11] to characterize informational and structural complexity of multilinear data. We first show that videos with linear camera motion can be represented more efficiently using t-SVD compared to the approaches based on vectorizing or flattening of the tensors. Since efficiency in representation implies efficiency in recovery, we outline a tensor nuclear norm penalized algorithm for video completion from missing entries. Application of the proposed algorithm for video recovery from missing entries is shown to yield a superior performance over existing methods. We also consider the problem of tensor robust Principal Component Analysis (PCA) for de-noising 3-D video data from sparse random corruptions. We show superior performance of our method compared to the matrix robust PCA adapted to this setting as proposed in [4]. version:2
arxiv-1406-1856 | A Drifting-Games Analysis for Online Learning and Applications to Boosting | http://arxiv.org/abs/1406.1856 | id:1406.1856 author:Haipeng Luo, Robert E. Schapire category:cs.LG  published:2014-06-07 summary:We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round. version:2
arxiv-1406-4498 | Extracting information from S-curves of language change | http://arxiv.org/abs/1406.4498 | id:1406.4498 author:Fakhteh Ghanbarnejad, Martin Gerlach, Jose M. Miotto, Eduardo G. Altmann category:physics.soc-ph cs.CL physics.data-an  published:2014-06-17 summary:It is well accepted that adoption of innovations are described by S-curves (slow start, accelerating period, and slow end). In this paper, we analyze how much information on the dynamics of innovation spreading can be obtained from a quantitative description of S-curves. We focus on the adoption of linguistic innovations for which detailed databases of written texts from the last 200 years allow for an unprecedented statistical precision. Combining data analysis with simulations of simple models (e.g., the Bass dynamics on complex networks) we identify signatures of endogenous and exogenous factors in the S-curves of adoption. We propose a measure to quantify the strength of these factors and three different methods to estimate it from S-curves. We obtain cases in which the exogenous factors are dominant (in the adoption of German orthographic reforms and of one irregular verb) and cases in which endogenous factors are dominant (in the adoption of conventions for romanization of Russian names and in the regularization of most studied verbs). These results show that the shape of S-curve is not universal and contains information on the adoption mechanism. (published at "J. R. Soc. Interface, vol. 11, no. 101, (2014) 1044"; DOI: http://dx.doi.org/10.1098/rsif.2014.1044) version:2
arxiv-1410-8420 | Learning circuits with few negations | http://arxiv.org/abs/1410.8420 | id:1410.8420 author:Eric Blais, Clément L. Canonne, Igor C. Oliveira, Rocco A. Servedio, Li-Yang Tan category:cs.CC cs.DM cs.LG  published:2014-10-30 summary:Monotone Boolean functions, and the monotone Boolean circuits that compute them, have been intensively studied in complexity theory. In this paper we study the structure of Boolean functions in terms of the minimum number of negations in any circuit computing them, a complexity measure that interpolates between monotone functions and the class of all functions. We study this generalization of monotonicity from the vantage point of learning theory, giving near-matching upper and lower bounds on the uniform-distribution learnability of circuits in terms of the number of negations they contain. Our upper bounds are based on a new structural characterization of negation-limited circuits that extends a classical result of A. A. Markov. Our lower bounds, which employ Fourier-analytic tools from hardness amplification, give new results even for circuits with no negations (i.e. monotone functions). version:1
arxiv-1410-8372 | On Estimating $L_2^2$ Divergence | http://arxiv.org/abs/1410.8372 | id:1410.8372 author:Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman category:stat.ML  published:2014-10-30 summary:We give a comprehensive theoretical characterization of a nonparametric estimator for the $L_2^2$ divergence between two continuous distributions. We first bound the rate of convergence of our estimator, showing that it is $\sqrt{n}$-consistent provided the densities are sufficiently smooth. In this smooth regime, we then show that our estimator is asymptotically normal, construct asymptotic confidence intervals, and establish a Berry-Ess\'{e}en style inequality characterizing the rate of convergence to normality. We also show that this estimator is minimax optimal. version:1
arxiv-1407-7644 | Estimating the Accuracies of Multiple Classifiers Without Labeled Data | http://arxiv.org/abs/1407.7644 | id:1407.7644 author:Ariel Jaffe, Boaz Nadler, Yuval Kluger category:stat.ML cs.LG  published:2014-07-29 summary:In various situations one is given only the predictions of multiple classifiers over a large unlabeled test data. This scenario raises the following questions: Without any labeled data and without any a-priori knowledge about the reliability of these different classifiers, is it possible to consistently and computationally efficiently estimate their accuracies? Furthermore, also in a completely unsupervised manner, can one construct a more accurate unsupervised ensemble classifier? In this paper, focusing on the binary case, we present simple, computationally efficient algorithms to solve these questions. Furthermore, under standard classifier independence assumptions, we prove our methods are consistent and study their asymptotic error. Our approach is spectral, based on the fact that the off-diagonal entries of the classifiers' covariance matrix and 3-d tensor are rank-one. We illustrate the competitive performance of our algorithms via extensive experiments on both artificial and real datasets. version:2
arxiv-1410-8326 | Towards Learning Object Affordance Priors from Technical Texts | http://arxiv.org/abs/1410.8326 | id:1410.8326 author:Nicholas H. Kirk category:cs.LG cs.AI cs.CL cs.RO 68T05  published:2014-10-30 summary:Everyday activities performed by artificial assistants can potentially be executed naively and dangerously given their lack of common sense knowledge. This paper presents conceptual work towards obtaining prior knowledge on the usual modality (passive or active) of any given entity, and their affordance estimates, by extracting high-confidence ability modality semantic relations (X can Y relationship) from non-figurative texts, by analyzing co-occurrence of grammatical instances of subjects and verbs, and verbs and objects. The discussion includes an outline of the concept, potential and limitations, and possible feature and learning framework adoption. version:1
arxiv-1403-2330 | Subspace Clustering by Exploiting a Low-Rank Representation with a Symmetric Constraint | http://arxiv.org/abs/1403.2330 | id:1403.2330 author:Jie Chen, Zhang Yi category:cs.CV  published:2014-03-07 summary:In this paper, we propose a low-rank representation with symmetric constraint (LRRSC) method for robust subspace clustering. Given a collection of data points approximately drawn from multiple subspaces, the proposed technique can simultaneously recover the dimension and members of each subspace. LRRSC extends the original low-rank representation algorithm by integrating a symmetric constraint into the low-rankness property of high-dimensional data representation. The symmetric low-rank representation, which preserves the subspace structures of high-dimensional data, guarantees weight consistency for each pair of data points so that highly correlated data points of subspaces are represented together. Moreover, it can be efficiently calculated by solving a convex optimization problem. We provide a rigorous proof for minimizing the nuclear-norm regularized least square problem with a symmetric constraint. The affinity matrix for spectral clustering can be obtained by further exploiting the angular information of the principal directions of the symmetric low-rank representation. This is a critical step towards evaluating the memberships between data points. Experimental results on benchmark databases demonstrate the effectiveness and robustness of LRRSC compared with several state-of-the-art subspace clustering algorithms. version:2
arxiv-1412-6141 | Efficient Decision-Making by Volume-Conserving Physical Object | http://arxiv.org/abs/1412.6141 | id:1412.6141 author:Song-Ju Kim, Masashi Aono, Etsushi Nameda category:cs.AI cs.LG nlin.AO physics.data-an  published:2014-10-30 summary:We demonstrate that any physical object, as long as its volume is conserved when coupled with suitable operations, provides a sophisticated decision-making capability. We consider the problem of finding, as accurately and quickly as possible, the most profitable option from a set of options that gives stochastic rewards. These decisions are made as dictated by a physical object, which is moved in a manner similar to the fluctuations of a rigid body in a tug-of-war game. Our analytical calculations validate statistical reasons why our method exhibits higher efficiency than conventional algorithms. version:1
arxiv-1411-0024 | Robust sketching for multiple square-root LASSO problems | http://arxiv.org/abs/1411.0024 | id:1411.0024 author:Vu Pham, Laurent El Ghaoui, Arturo Fernandez category:math.OC cs.LG cs.SY stat.ML  published:2014-10-30 summary:Many learning tasks, such as cross-validation, parameter search, or leave-one-out analysis, involve multiple instances of similar problems, each instance sharing a large part of learning data with the others. We introduce a robust framework for solving multiple square-root LASSO problems, based on a sketch of the learning data that uses low-rank approximations. Our approach allows a dramatic reduction in computational effort, in effect reducing the number of observations from $m$ (the number of observations to start with) to $k$ (the number of singular values retained in the low-rank model), while not sacrificing---sometimes even improving---the statistical performance. Theoretical analysis, as well as numerical experiments on both synthetic and real data, illustrate the efficiency of the method in large scale applications. version:1
arxiv-1410-8251 | Notes on Noise Contrastive Estimation and Negative Sampling | http://arxiv.org/abs/1410.8251 | id:1410.8251 author:Chris Dyer category:cs.LG  published:2014-10-30 summary:Estimating the parameters of probabilistic models of language such as maxent models and probabilistic neural models is computationally difficult since it involves evaluating partition functions by summing over an entire vocabulary, which may be millions of word types in size. Two closely related strategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al., 2012; Goldberg and Levy, 2014)---have emerged as popular solutions to this computational problem, but some confusion remains as to which is more appropriate and when. This document explicates their relationships to each other and to other estimation techniques. The analysis shows that, although they are superficially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while negative sampling is best understood as a family of binary classification models that are useful for learning word representations but not as a general-purpose estimator. version:1
arxiv-1401-0733 | ConceptVision: A Flexible Scene Classification Framework | http://arxiv.org/abs/1401.0733 | id:1401.0733 author:Ahmet Iscen, Eren Golge, Ilker Sarac, Pinar Duygulu category:cs.CV  published:2014-01-03 summary:We introduce ConceptVision, a method that aims for high accuracy in categorizing large number of scenes, while keeping the model relatively simpler and efficient for scalability. The proposed method combines the advantages of both low-level representations and high-level semantic categories, and eliminates the distinctions between different levels through the definition of concepts. The proposed framework encodes the perspectives brought through different concepts by considering them in concept groups. Different perspectives are ensembled for the final decision. Extensive experiments are carried out on benchmark datasets to test the effects of different concepts, and methods used to ensemble. Comparisons with state-of-the-art studies show that we can achieve better results with incorporation of concepts in different levels with different perspectives. version:2
arxiv-1410-8149 | Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling | http://arxiv.org/abs/1410.8149 | id:1410.8149 author:Paul Rodrigues, David Zajic, David Doermann, Michael Bloodgood, Peng Ye category:cs.CL cs.LG  published:2014-10-29 summary:Dictionaries are often developed using tools that save to Extensible Markup Language (XML)-based standards. These standards often allow high-level repeating elements to represent lexical entries, and utilize descendants of these repeating elements to represent the structure within each lexical entry, in the form of an XML tree. In many cases, dictionaries are published that have errors and inconsistencies that are expensive to find manually. This paper discusses a method for dictionary writers to quickly audit structural regularity across entries in a dictionary by using statistical language modeling. The approach learns the patterns of XML nodes that could occur within an XML tree, and then calculates the probability of each XML tree in the dictionary against these patterns to look for entries that diverge from the norm. version:1
arxiv-1410-8043 | High-Performance Distributed ML at Scale through Parameter Server Consistency Models | http://arxiv.org/abs/1410.8043 | id:1410.8043 author:Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, Eric P. Xing category:cs.LG stat.ML  published:2014-10-29 summary:As Machine Learning (ML) applications increase in data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands. Unfortunately, effective use of clusters for ML requires considerable expertise in writing distributed code, while highly-abstracted frameworks like Hadoop have not, in practice, approached the performance seen in specialized ML implementations. The recent Parameter Server (PS) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML applications into distributed ones, while maintaining high throughput through relaxed "consistency models" that allow inconsistent parameter reads. However, due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output; at the same time, there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput. Motivated by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models. We then use the gleaned insights to improve a consistency model using an "eager" PS communication mechanism, and implement it as a new PS system that enables ML algorithms to reach their solution more quickly. version:1
arxiv-1410-8034 | Latent Feature Based FM Model For Rating Prediction | http://arxiv.org/abs/1410.8034 | id:1410.8034 author:Xudong Liu, Bin Zhang, Ting Zhang, Chang Liu category:cs.LG cs.IR stat.ML 68-XX H.2.8  published:2014-10-29 summary:Rating Prediction is a basic problem in Recommender System, and one of the most widely used method is Factorization Machines(FM). However, traditional matrix factorization methods fail to utilize the benefit of implicit feedback, which has been proved to be important in Rating Prediction problem. In this work, we consider a specific situation, movie rating prediction, where we assume that watching history has a big influence on his/her rating behavior on an item. We introduce two models, Latent Dirichlet Allocation(LDA) and word2vec, both of which perform state-of-the-art results in training latent features. Based on that, we propose two feature based models. One is the Topic-based FM Model which provides the implicit feedback to the matrix factorization. The other is the Vector-based FM Model which expresses the order info of watching history. Empirical results on three datasets demonstrate that our method performs better than the baseline model and confirm that Vector-based FM Model usually works better as it contains the order info. version:1
arxiv-1410-8808 | A Semantic Web of Know-How: Linked Data for Community-Centric Tasks | http://arxiv.org/abs/1410.8808 | id:1410.8808 author:Paolo Pareti, Ewan Klein, Adam Barker category:cs.AI cs.CL  published:2014-10-29 summary:This paper proposes a novel framework for representing community know-how on the Semantic Web. Procedural knowledge generated by web communities typically takes the form of natural language instructions or videos and is largely unstructured. The absence of semantic structure impedes the deployment of many useful applications, in particular the ability to discover and integrate know-how automatically. We discuss the characteristics of community know-how and argue that existing knowledge representation frameworks fail to represent it adequately. We present a novel framework for representing the semantic structure of community know-how and demonstrate the feasibility of our approach by providing a concrete implementation which includes a method for automatically acquiring procedural knowledge for real-world tasks. version:1
arxiv-1410-7922 | Extended Dynamic Programming and Fast Multidimensional Search Algorithm for Energy Minization in Stereo and Motion | http://arxiv.org/abs/1410.7922 | id:1410.7922 author:Mikhail G. Mozerov category:cs.CV  published:2014-10-29 summary:This paper presents a novel extended dynamic programming approach for energy minimization (EDP) to solve the correspondence problem for stereo and motion. A significant speedup is achieved using a recursive minimum search strategy (RMS). The mentioned speedup is particularly important if the disparity space is 2D as well as 3D. The proposed RMS can also be applied in the well-known dynamic programming (DP) approach for stereo and motion. In this case, the general 2D problem of the global discrete energy minimization is reduced to several mutually independent sub-problems of the one-dimensional minimization. The EDP method is used when the approximation of the general 2D discrete energy minimization problem is considered. Then the RMS algorithm is an essential part of the EDP method. Using the EDP algorithm we obtain a lower energy bound than the graph cuts (GC) expansion technique on stereo and motion problems. The proposed calculation scheme possesses natural parallelism and can be realized on graphics processing unit (GPU) platforms, and can be potentially restricted further by the number of scanlines in the image plane. Furthermore, the RMS and EDP methods can be used in any optimization problem where the objective function meets specific conditions in the smoothness term. version:1
arxiv-1410-7890 | Global Bandits with Holder Continuity | http://arxiv.org/abs/1410.7890 | id:1410.7890 author:Onur Atan, Cem Tekin, Mihaela van der Schaar category:cs.LG  published:2014-10-29 summary:Standard Multi-Armed Bandit (MAB) problems assume that the arms are independent. However, in many application scenarios, the information obtained by playing an arm provides information about the remainder of the arms. Hence, in such applications, this informativeness can and should be exploited to enable faster convergence to the optimal solution. In this paper, we introduce and formalize the Global MAB (GMAB), in which arms are globally informative through a global parameter, i.e., choosing an arm reveals information about all the arms. We propose a greedy policy for the GMAB which always selects the arm with the highest estimated expected reward, and prove that it achieves bounded parameter-dependent regret. Hence, this policy selects suboptimal arms only finitely many times, and after a finite number of initial time steps, the optimal arm is selected in all of the remaining time steps with probability one. In addition, we also study how the informativeness of the arms about each other's rewards affects the speed of learning. Specifically, we prove that the parameter-free (worst-case) regret is sublinear in time, and decreases with the informativeness of the arms. We also prove a sublinear in time Bayesian risk bound for the GMAB which reduces to the well-known Bayesian risk bound for linearly parameterized bandits when the arms are fully informative. GMABs have applications ranging from drug and treatment discovery to dynamic pricing. version:1
arxiv-1410-7883 | Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired by C. elegans Chemotaxis | http://arxiv.org/abs/1410.7883 | id:1410.7883 author:Shibani Santurkar, Bipin Rajendran category:cs.NE q-bio.NC  published:2014-10-29 summary:We demonstrate a spiking neural network for navigation motivated by the chemotaxis network of Caenorhabditis elegans. Our network uses information regarding temporal gradients in the tracking variable's concentration to make navigational decisions. The gradient information is determined by mimicking the underlying mechanisms of the ASE neurons of C. elegans. Simulations show that our model is able to forage and track a target set-point in extremely noisy environments. We develop a VLSI implementation for the main gradient detector neurons, which could be integrated with standard comparator circuitry to develop a robust circuit for navigation and contour tracking. version:1
arxiv-1410-7881 | A neural circuit for navigation inspired by C. elegans Chemotaxis | http://arxiv.org/abs/1410.7881 | id:1410.7881 author:Shibani Santurkar, Bipin Rajendran category:cs.NE q-bio.NC  published:2014-10-29 summary:We develop an artificial neural circuit for contour tracking and navigation inspired by the chemotaxis of the nematode Caenorhabditis elegans. In order to harness the computational advantages spiking neural networks promise over their non-spiking counterparts, we develop a network comprising 7-spiking neurons with non-plastic synapses which we show is extremely robust in tracking a range of concentrations. Our worm uses information regarding local temporal gradients in sodium chloride concentration to decide the instantaneous path for foraging, exploration and tracking. A key neuron pair in the C. elegans chemotaxis network is the ASEL & ASER neuron pair, which capture the gradient of concentration sensed by the worm in their graded membrane potentials. The primary sensory neurons for our network are a pair of artificial spiking neurons that function as gradient detectors whose design is adapted from a computational model of the ASE neuron pair in C. elegans. Simulations show that our worm is able to detect the set-point with approximately four times higher probability than the optimal memoryless Levy foraging model. We also show that our spiking neural network is much more efficient and noise-resilient while navigating and tracking a contour, as compared to an equivalent non-spiking network. We demonstrate that our model is extremely robust to noise and with slight modifications can be used for other practical applications such as obstacle avoidance. Our network model could also be extended for use in three-dimensional contour tracking or obstacle avoidance. version:1
arxiv-1410-7875 | Faster graphical model identification of tandem mass spectra using peptide word lattices | http://arxiv.org/abs/1410.7875 | id:1410.7875 author:Shengjie Wang, John T. Halloran, Jeff A. Bilmes, William S. Noble category:q-bio.MN stat.ML  published:2014-10-29 summary:Liquid chromatography coupled with tandem mass spectrometry, also known as shotgun proteomics, is a widely-used high-throughput technology for identifying proteins in complex biological samples. Analysis of the tens of thousands of fragmentation spectra produced by a typical shotgun proteomics experiment begins by assigning to each observed spectrum the peptide hypothesized to be responsible for generating the spectrum, typically done by searching each spectrum against a database of peptides. We have recently described a machine learning method---Dynamic Bayesian Network for Rapid Identification of Peptides (DRIP)---that not only achieves state-of-the-art spectrum identification performance on a variety of datasets but also provides a trainable model capable of returning valuable auxiliary information regarding specific peptide-spectrum matches. In this work, we present two significant improvements to DRIP. First, we describe how to use word lattices, which are widely used in natural language processing, to significantly speed up DRIP's computations. To our knowledge, all existing shotgun proteomics search engines compute independent scores between a given observed spectrum and each possible candidate peptide from the database. The key idea of the word lattice is to represent the set of candidate peptides in a single data structure, thereby allowing sharing of redundant computations among the different candidates. We demonstrate that using lattices in conjunction with DRIP leads to speedups on the order of tens across yeast and worm data sets. Second, we introduce a variant of DRIP that uses a discriminative training framework, performing maximum mutual entropy estimation rather than maximum likelihood estimation. This modification improves DRIP's statistical power, enabling us to increase the number of identified spectrum at a 1% false discovery rate on yeast and worm data sets. version:1
arxiv-1307-0293 | A Direct Estimation of High Dimensional Stationary Vector Autoregressions | http://arxiv.org/abs/1307.0293 | id:1307.0293 author:Fang Han, Huanran Lu, Han Liu category:stat.ML  published:2013-07-01 summary:The vector autoregressive (VAR) model is a powerful tool in modeling complex time series and has been exploited in many fields. However, fitting high dimensional VAR model poses some unique challenges: On one hand, the dimensionality, caused by modeling a large number of time series and higher order autoregressive processes, is usually much higher than the time series length; On the other hand, the temporal dependence structure in the VAR model gives rise to extra theoretical challenges. In high dimensions, one popular approach is to assume the transition matrix is sparse and fit the VAR model using the "least squares" method with a lasso-type penalty. In this manuscript, we propose an alternative way in estimating the VAR model. The main idea is, via exploiting the temporal dependence structure, to formulate the estimating problem into a linear program. There is instant advantage for the proposed approach over the lasso-type estimators: The estimation equation can be decomposed into multiple sub-equations and accordingly can be efficiently solved in a parallel fashion. In addition, our method brings new theoretical insights into the VAR model analysis. So far the theoretical results developed in high dimensions (e.g., Song and Bickel (2011) and Kock and Callot (2012)) mainly pose assumptions on the design matrix of the formulated regression problems. Such conditions are indirect about the transition matrices and not transparent. In contrast, our results show that the operator norm of the transition matrices plays an important role in estimation accuracy. We provide explicit rates of convergence for both estimation and prediction. In addition, we provide thorough experiments on both synthetic and real-world equity data to show that there are empirical advantages of our method over the lasso-type estimators in both parameter estimation and forecasting. version:3
arxiv-1410-7852 | A Markov Decision Process Analysis of the Cold Start Problem in Bayesian Information Filtering | http://arxiv.org/abs/1410.7852 | id:1410.7852 author:Xiaoting Zhao, Peter I. Frazier category:cs.LG cs.IR math.OC  published:2014-10-29 summary:We consider the information filtering problem, in which we face a stream of items, and must decide which ones to forward to a user to maximize the number of relevant items shown, minus a penalty for each irrelevant item shown. Forwarding decisions are made separately in a personalized way for each user. We focus on the cold-start setting for this problem, in which we have limited historical data on the user's preferences, and must rely on feedback from forwarded articles to learn which the fraction of items relevant to the user in each of several item categories. Performing well in this setting requires trading exploration vs. exploitation, forwarding items that are likely to be irrelevant, to allow learning that will improve later performance. In a Bayesian setting, and using Markov decision processes, we show how the Bayes-optimal forwarding algorithm can be computed efficiently when the user will examine each forwarded article, and how an upper bound on the Bayes-optimal procedure and a heuristic index policy can be obtained for the setting when the user will examine only a limited number of forwarded items. We present results from simulation experiments using parameters estimated using historical data from arXiv.org. version:1
arxiv-1410-7787 | Correcting Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language | http://arxiv.org/abs/1410.7787 | id:1410.7787 author:David Zajic, Michael Maxwell, David Doermann, Paul Rodrigues, Michael Bloodgood category:cs.CL  published:2014-10-28 summary:We describe a paradigm for combining manual and automatic error correction of noisy structured lexicographic data. Modifications to the structure and underlying text of the lexicographic data are expressed in a simple, interpreted programming language. Dictionary Manipulation Language (DML) commands identify nodes by unique identifiers, and manipulations are performed using simple commands such as create, move, set text, etc. Corrected lexicons are produced by applying sequences of DML commands to the source version of the lexicon. DML commands can be written manually to repair one-off errors or generated automatically to correct recurring problems. We discuss advantages of the paradigm for the task of editing digital bilingual dictionaries. version:1
arxiv-1410-1184 | Graphical LASSO Based Model Selection for Time Series | http://arxiv.org/abs/1410.1184 | id:1410.1184 author:Alexander Jung, Gabor Hannak, Norbert Görtz category:stat.ML  published:2014-10-05 summary:We propose a novel graphical model selection (GMS) scheme for high-dimensional stationary time series or discrete time process. The method is based on a natural generalization of the graphical LASSO (gLASSO), introduced originally for GMS based on i.i.d. samples, and estimates the conditional independence graph (CIG) of a time series from a finite length observation. The gLASSO for time series is defined as the solution of an l1-regularized maximum (approximate) likelihood problem. We solve this optimization problem using the alternating direction method of multipliers (ADMM). Our approach is nonparametric as we do not assume a finite dimensional (e.g., an autoregressive) parametric model for the observed process. Instead, we require the process to be sufficiently smooth in the spectral domain. For Gaussian processes, we characterize the performance of our method theoretically by deriving an upper bound on the probability that our algorithm fails to correctly identify the CIG. Numerical experiments demonstrate the ability of our method to recover the correct CIG from a limited amount of samples. version:3
arxiv-1410-7762 | A hierarchical framework for object recognition | http://arxiv.org/abs/1410.7762 | id:1410.7762 author:Reza Moazzezi category:cs.CV  published:2014-10-28 summary:Object recognition in the presence of background clutter and distractors is a central problem both in neuroscience and in machine learning. However, the performance level of the models that are inspired by cortical mechanisms, including deep networks such as convolutional neural networks and deep belief networks, is shown to significantly decrease in the presence of noise and background objects [19, 24]. Here we develop a computational framework that is hierarchical, relies heavily on key properties of the visual cortex including mid-level feature selectivity in visual area V4 and Inferotemporal cortex (IT) [4, 9, 12, 18], high degrees of selectivity and invariance in IT [13, 17, 18] and the prior knowledge that is built into cortical circuits (such as the emergence of edge detector neurons in primary visual cortex before the onset of the visual experience) [1, 21], and addresses the problem of object recognition in the presence of background noise and distractors. Our approach is specifically designed to address large deformations, allows flexible communication between different layers of representation and learns highly selective filters from a small number of training examples. version:1
arxiv-1410-1141 | On the Computational Efficiency of Training Neural Networks | http://arxiv.org/abs/1410.1141 | id:1410.1141 author:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir category:cs.LG cs.AI stat.ML  published:2014-10-05 summary:It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks. version:2
arxiv-1410-7730 | New similarity index based on entropy and group theory | http://arxiv.org/abs/1410.7730 | id:1410.7730 author:Yasel Garcés, Esley Torres, Osvaldo Pereira, Roberto Rodríguez category:cs.CV  published:2014-10-28 summary:In this work, we propose a new similarity index for images considering the entropy function and group theory. This index considers an algebraic group of images, it is defined by an inner law that provides a novel approach for the subtraction of images. Through an equivalence relationship in the field of images, we prove the existence of the quotient group, on which the new similarity index is defined. We also present the main properties of the new index, and the immediate application thereof as a stopping criterion of the "Mean Shift Iterative Algorithm". version:1
arxiv-1306-4478 | Finite Element Based Tracking of Deforming Surfaces | http://arxiv.org/abs/1306.4478 | id:1306.4478 author:Stefanie Wuhrer, Jochen Lang, Motahareh Tekieh, Chang Shu category:cs.CV cs.GR  published:2013-06-19 summary:We present an approach to robustly track the geometry of an object that deforms over time from a set of input point clouds captured from a single viewpoint. The deformations we consider are caused by applying forces to known locations on the object's surface. Our method combines the use of prior information on the geometry of the object modeled by a smooth template and the use of a linear finite element method to predict the deformation. This allows the accurate reconstruction of both the observed and the unobserved sides of the object. We present tracking results for noisy low-quality point clouds acquired by either a stereo camera or a depth camera, and simulations with point clouds corrupted by different error terms. We show that our method is also applicable to large non-linear deformations. version:3
arxiv-1410-7709 | Anomaly Detection Framework Using Rule Extraction for Efficient Intrusion Detection | http://arxiv.org/abs/1410.7709 | id:1410.7709 author:Antti Juvonen, Tuomo Sipola category:cs.LG cs.CR  published:2014-10-28 summary:Huge datasets in cyber security, such as network traffic logs, can be analyzed using machine learning and data mining methods. However, the amount of collected data is increasing, which makes analysis more difficult. Many machine learning methods have not been designed for big datasets, and consequently are slow and difficult to understand. We address the issue of efficient network traffic classification by creating an intrusion detection framework that applies dimensionality reduction and conjunctive rule extraction. The system can perform unsupervised anomaly detection and use this information to create conjunctive rules that classify huge amounts of traffic in real time. We test the implemented system with the widely used KDD Cup 99 dataset and real-world network logs to confirm that the performance is satisfactory. This system is transparent and does not work like a black box, making it intuitive for domain experts, such as network administrators. version:1
arxiv-1410-7660 | Non-convex Robust PCA | http://arxiv.org/abs/1410.7660 | id:1410.7660 author:Praneeth Netrapalli, U N Niranjan, Sujay Sanghavi, Animashree Anandkumar, Prateek Jain category:cs.IT cs.LG math.IT stat.ML  published:2014-10-28 summary:We propose a new method for robust PCA -- the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is {\em non-convex} but easy to compute. In spite of this non-convexity, we establish exact recovery of the low-rank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For an $m \times n$ input matrix ($m \leq n)$, our method has a running time of $O(r^2mn)$ per iteration, and needs $O(\log(1/\epsilon))$ iterations to reach an accuracy of $\epsilon$. This is close to the running time of simple PCA via the power method, which requires $O(rmn)$ per iteration, and $O(\log(1/\epsilon))$ iterations. In contrast, existing methods for robust PCA, which are based on convex optimization, have $O(m^2n)$ complexity per iteration, and take $O(1/\epsilon)$ iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations. version:1
arxiv-1410-7613 | A Short Image Series Based Scheme for Time Series Digital Image Correlation | http://arxiv.org/abs/1410.7613 | id:1410.7613 author:Xian Wang, Shaopeng Ma category:physics.optics cs.CV 78Mxx  published:2014-10-28 summary:A new scheme for digital image correlation, i.e., short time series DIC (STS-DIC) is proposed. Instead of processing the original deformed speckle images individually, STS-DIC combines several adjacent deformed speckle images from a short time series and then processes the averaged image, for which deformation continuity over time is introduced. The deformation of several adjacent images is assumed to be linear in time and a new spatial-temporal displacement representation method with eight unknowns is presented based on the subset-based representation method. Then, the model of STS-DIC is created and a solving scheme is developed based on the Newton-Raphson iteration. The proposed method is verified for numerical and experimental cases. The results show that the proposed STS-DIC greatly improves the accuracy of traditional DIC, both under simple and complicated deformation conditions, while retaining acceptable actual computational cost. version:1
arxiv-1311-3494 | Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation | http://arxiv.org/abs/1311.3494 | id:1311.3494 author:Ohad Shamir category:cs.LG stat.ML  published:2013-11-14 summary:Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for several different settings. version:6
arxiv-1410-7596 | Fast Algorithms for Online Stochastic Convex Programming | http://arxiv.org/abs/1410.7596 | id:1410.7596 author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG cs.DS math.OC F.1.2; G.1.6  published:2014-10-28 summary:We introduce the online stochastic Convex Programming (CP) problem, a very general version of stochastic online problems which allows arbitrary concave objectives and convex feasibility constraints. Many well-studied problems like online stochastic packing and covering, online stochastic matching with concave returns, etc. form a special case of online stochastic CP. We present fast algorithms for these problems, which achieve near-optimal regret guarantees for both the i.i.d. and the random permutation models of stochastic inputs. When applied to the special case online packing, our ideas yield a simpler and faster primal-dual algorithm for this well studied problem, which achieves the optimal competitive ratio. Our techniques make explicit the connection of primal-dual paradigm and online learning to online stochastic CP. version:1
arxiv-1410-7580 | Robust Piecewise-Constant Smoothing: M-Smoother Revisited | http://arxiv.org/abs/1410.7580 | id:1410.7580 author:Linchao Bao, Qingxiong Yang category:cs.CV  published:2014-10-28 summary:A robust estimator, namely M-smoother, for piecewise-constant smoothing is revisited in this paper. Starting from its generalized formulation, we propose a numerical scheme/framework for solving it via a series of weighted-average filtering (e.g., box filtering, Gaussian filtering, bilateral filtering, and guided filtering). Because of the equivalence between M-smoother and local-histogram-based filters (such as median filter and mode filter), the proposed framework enables fast approximation of histogram filters via a number of box filtering or Gaussian filtering. In addition, high-quality piecewise-constant smoothing can be achieved via a number of bilateral filtering or guided filtering integrated in the proposed framework. Experiments on depth map denoising show the effectiveness of our framework. version:1
arxiv-1410-7550 | Learning deep dynamical models from image pixels | http://arxiv.org/abs/1410.7550 | id:1410.7550 author:Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:stat.ML cs.LG cs.NE cs.SY  published:2014-10-28 summary:Modeling dynamical systems is important in many disciplines, e.g., control, robotics, or neurotechnology. Commonly the state of these systems is not directly observed, but only available through noisy and potentially high-dimensional observations. In these cases, system identification, i.e., finding the measurement mapping and the transition mapping (system dynamics) in latent space can be challenging. For linear system dynamics and measurement mappings efficient solutions for system identification are available. However, in practical applications, the linearity assumptions does not hold, requiring non-linear system identification techniques. If additionally the observations are high-dimensional (e.g., images), non-linear system identification is inherently hard. To address the problem of non-linear system identification from high-dimensional observations, we combine recent advances in deep learning and system identification. In particular, we jointly learn a low-dimensional embedding of the observation by means of deep auto-encoders and a predictive transition model in this low-dimensional space. We demonstrate that our model enables learning good predictive models of dynamical systems from pixel information only. version:1
arxiv-1309-1539 | Practical Matrix Completion and Corruption Recovery using Proximal Alternating Robust Subspace Minimization | http://arxiv.org/abs/1309.1539 | id:1309.1539 author:Yu-Xiang Wang, Choon Meng Lee, Loong-Fah Cheong, Kim-Chuan Toh category:cs.CV  published:2013-09-06 summary:Low-rank matrix completion is a problem of immense practical importance. Recent works on the subject often use nuclear norm as a convex surrogate of the rank function. Despite its solid theoretical foundation, the convex version of the problem often fails to work satisfactorily in real-life applications. Real data often suffer from very few observations, with support not meeting the random requirements, ubiquitous presence of noise and potentially gross corruptions, sometimes with these simultaneously occurring. This paper proposes a Proximal Alternating Robust Subspace Minimization (PARSuMi) method to tackle the three problems. The proximal alternating scheme explicitly exploits the rank constraint on the completed matrix and uses the $\ell_0$ pseudo-norm directly in the corruption recovery step. We show that the proposed method for the non-convex and non-smooth model converges to a stationary point. Although it is not guaranteed to find the global optimal solution, in practice we find that our algorithm can typically arrive at a good local minimizer when it is supplied with a reasonably good starting point based on convex optimization. Extensive experiments with challenging synthetic and real data demonstrate that our algorithm succeeds in a much larger range of practical problems where convex optimization fails, and it also outperforms various state-of-the-art algorithms. version:2
arxiv-1207-3269 | The Price of Privacy in Untrusted Recommendation Engines | http://arxiv.org/abs/1207.3269 | id:1207.3269 author:Siddhartha Banerjee, Nidhi Hegde, Laurent Massoulié category:cs.LG cs.IT math.IT  published:2012-07-13 summary:Recent increase in online privacy concerns prompts the following question: can a recommender system be accurate if users do not entrust it with their private data? To answer this, we study the problem of learning item-clusters under local differential privacy, a powerful, formal notion of data privacy. We develop bounds on the sample-complexity of learning item-clusters from privatized user inputs. Significantly, our results identify a sample-complexity separation between learning in an information-rich and an information-scarce regime, thereby highlighting the interaction between privacy and the amount of information (ratings) available to each user. In the information-rich regime, where each user rates at least a constant fraction of items, a spectral clustering approach is shown to achieve a sample-complexity lower bound derived from a simple information-theoretic argument based on Fano's inequality. However, the information-scarce regime, where each user rates only a vanishing fraction of items, is found to require a fundamentally different approach both for lower bounds and algorithms. To this end, we develop new techniques for bounding mutual information under a notion of channel-mismatch, and also propose a new algorithm, MaxSense, and show that it achieves optimal sample-complexity in this setting. The techniques we develop for bounding mutual information may be of broader interest. To illustrate this, we show their applicability to $(i)$ learning based on 1-bit sketches, and $(ii)$ adaptive learning, where queries can be adapted based on answers to past queries. version:2
arxiv-1405-0558 | The Falling Factorial Basis and Its Statistical Applications | http://arxiv.org/abs/1405.0558 | id:1405.0558 author:Yu-Xiang Wang, Alex Smola, Ryan J. Tibshirani category:stat.ML  published:2014-05-03 summary:We study a novel spline-like basis, which we name the "falling factorial basis", bearing many similarities to the classic truncated power basis. The advantage of the falling factorial basis is that it enables rapid, linear-time computations in basis matrix multiplication and basis matrix inversion. The falling factorial functions are not actually splines, but are close enough to splines that they provably retain some of the favorable properties of the latter functions. We examine their application in two problems: trend filtering over arbitrary input points, and a higher-order variant of the two-sample Kolmogorov-Smirnov test. version:2
arxiv-1410-7414 | Fast Function to Function Regression | http://arxiv.org/abs/1410.7414 | id:1410.7414 author:Junier Oliva, Willie Neiswanger, Barnabas Poczos, Eric Xing, Jeff Schneider category:stat.ML cs.LG  published:2014-10-27 summary:We analyze the problem of regression when both input covariates and output responses are functions from a nonparametric function class. Function to function regression (FFR) covers a large range of interesting applications including time-series prediction problems, and also more general tasks like studying a mapping between two separate types of distributions. However, previous nonparametric estimators for FFR type problems scale badly computationally with the number of input/output pairs in a data-set. Given the complexity of a mapping between general functions it may be necessary to consider large data-sets in order to achieve a low estimation risk. To address this issue, we develop a novel scalable nonparametric estimator, the Triple-Basis Estimator (3BE), which is capable of operating over datasets with many instances. To the best of our knowledge, the 3BE is the first nonparametric FFR estimator that can scale to massive datasets. We analyze the 3BE's risk and derive an upperbound rate. Furthermore, we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators in various real-world data-sets. version:1
arxiv-1410-7372 | Feature Selection through Minimization of the VC dimension | http://arxiv.org/abs/1410.7372 | id:1410.7372 author:Jayadeva, Sanjit S. Batra, Siddharth Sabharwal category:cs.LG 68T05  68T10  68Q32 I.5.1; I.5.2  published:2014-10-27 summary:Feature selection involes identifying the most relevant subset of input features, with a view to improving generalization of predictive models by reducing overfitting. Directly searching for the most relevant combination of attributes is NP-hard. Variable selection is of critical importance in many applications, such as micro-array data analysis, where selecting a small number of discriminative features is crucial to developing useful models of disease mechanisms, as well as for prioritizing targets for drug discovery. The recently proposed Minimal Complexity Machine (MCM) provides a way to learn a hyperplane classifier by minimizing an exact (\boldmath{$\Theta$}) bound on its VC dimension. It is well known that a lower VC dimension contributes to good generalization. For a linear hyperplane classifier in the input space, the VC dimension is upper bounded by the number of features; hence, a linear classifier with a small VC dimension is parsimonious in the set of features it employs. In this paper, we use the linear MCM to learn a classifier in which a large number of weights are zero; features with non-zero weights are the ones that are chosen. Selected features are used to learn a kernel SVM classifier. On a number of benchmark datasets, the features chosen by the linear MCM yield comparable or better test set accuracy than when methods such as ReliefF and FCBF are used for the task. The linear MCM typically chooses one-tenth the number of attributes chosen by the other methods; on some very high dimensional datasets, the MCM chooses about $0.6\%$ of the features; in comparison, ReliefF and FCBF choose 70 to 140 times more features, thus demonstrating that minimizing the VC dimension may provide a new, and very effective route for feature selection and for learning sparse representations. version:1
arxiv-1410-7371 | A General Statistic Framework for Genome-based Disease Risk Prediction | http://arxiv.org/abs/1410.7371 | id:1410.7371 author:L. Ma, N. Lin, C. I. Amos, M. M. Xiong category:stat.ML  published:2014-10-27 summary:Advances of modern sensing and sequencing technologies generate a deluge of high dimensional space-temporal physiological and next-generation sequencing (NGS) data. Physiological traits are observed either as continuous random functions, or on a dense grid and referred to as function-valued traits. Both physiological and NGS data are highly correlated data with their inherent order, spacing, and functional nature which are ignored by traditional summary-based univariate and multivariate regression methods designed for quantitative genetic analysis of scalar trait and common variants. To capture morphological and dynamic features of the data and utilize their dependent structure, we propose a functional linear model (FLM) in which a trait curve is modeled as a response function, the genetic variation in a genomic region or gene is modeled as a functional predictor, and the genetic effects are modeled as a function of both time and genomic position (FLMF) for genetic analysis of function-valued trait with both GWAS and NGS data. By extensive simulations, we demonstrate that the FLMF has the correct type 1 error rates and much higher power to detect association than the existing methods. The FLMF is applied to sleep data from Starr County health studies where oxygen saturation were measured in 22,670 seconds on average for 833 individuals. We found 65 genes that were significantly associated with oxygen saturation functional trait with P-values ranging from 2.40E-06 to 2.53E-21. The results clearly demonstrate that the FLMF substantially outperforms the traditional genetic models with scalar trait. version:1
arxiv-1410-7279 | Topology Adaptive Graph Estimation in High Dimensions | http://arxiv.org/abs/1410.7279 | id:1410.7279 author:Johannes Lederer, Christian Müller category:stat.ML stat.ME  published:2014-10-27 summary:We introduce Graphical TREX (GTREX), a novel method for graph estimation in high-dimensional Gaussian graphical models. By conducting neighborhood selection with TREX, GTREX avoids tuning parameters and is adaptive to the graph topology. We compare GTREX with standard methods on a new simulation set-up that is designed to assess accurately the strengths and shortcomings of different methods. These simulations show that a neighborhood selection scheme based on Lasso and an optimal (in practice unknown) tuning parameter outperforms other standard methods over a large spectrum of scenarios. Moreover, we show that GTREX can rival this scheme and, therefore, can provide competitive graph estimation without the need for tuning parameter calibration. version:1
arxiv-1410-7265 | An Unsupervised Ensemble-based Markov Random Field Approach to Microscope Cell Image Segmentation | http://arxiv.org/abs/1410.7265 | id:1410.7265 author:Balint Antal, Bence Remenyik, Andras Hajdu category:cs.CV cs.AI q-bio.QM  published:2014-10-27 summary:In this paper, we propose an approach to the unsupervised segmentation of images using Markov Random Field. The proposed approach is based on the idea of Bit Plane Slicing. We use the planes as initial labellings for an ensemble of segmentations. With pixelwise voting, a robust segmentation approach can be achieved, which we demonstrate on microscope cell images. We tested our approach on a publicly available database, where it proven to be competitive with other methods and manual segmentation. version:1
arxiv-1410-7252 | Iris Biometric System using a hybrid approach | http://arxiv.org/abs/1410.7252 | id:1410.7252 author:Abhimanyu Sarin, Dr. Jagadish Nayak category:cs.CV 47G20  published:2014-10-27 summary:Iris Recognition Systems are ocular- based biometric devices used primarily for security reasons. The complexity and the randomness of the Iris, amongst various other factors, ensure that this biometric system is inarguably an exact and reliable method of identification. The algorithm is responsible for automatic localization and segmentation of boundaries using circular Hough Transform, noise reductions, image enhancement and feature extraction across numerous distinct images present in the database. This paper delves into the various kinds of techniques required to approximate the pupillary and limbic boundaries of the enrolled iris image, captured using a suitable image acquisition device and perform feature extraction on the normalized iris image with the help of Haar Wavelets to encode the input data into a binary string format. These techniques were validated using images from the CASIA database, and various other procedures were also tried and tested. version:1
arxiv-1410-7241 | A Greedy Homotopy Method for Regression with Nonconvex Constraints | http://arxiv.org/abs/1410.7241 | id:1410.7241 author:Fabian L. Wauthier, Peter Donnelly category:stat.ML stat.ME 62J99 (Primary)  published:2014-10-27 summary:Constrained least squares regression is an essential tool for high-dimensional data analysis. Given a partition $\mathcal{G}$ of input variables, this paper considers a particular class of nonconvex constraint functions that encourage the linear model to select a small number of variables from a small number of groups in $\mathcal{G}$. Such constraints are relevant in many practical applications, such as Genome-Wide Association Studies (GWAS). Motivated by the efficiency of the Lasso homotopy method, we present RepLasso, a greedy homotopy algorithm that tries to solve the induced sequence of nonconvex problems by solving a sequence of suitably adapted convex surrogate problems. We prove that in some situations RepLasso recovers the global minima of the nonconvex problem. Moreover, even if it does not recover global minima, we prove that in relevant cases it will still do no worse than the Lasso in terms of support and signed support recovery, while in practice outperforming it. We show empirically that the strategy can also be used to improve over other Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitis highlights our method's practical utility. version:1
arxiv-1406-6140 | Offline Handwritten MODI Character Recognition Using HU, Zernike Moments and Zoning | http://arxiv.org/abs/1406.6140 | id:1406.6140 author:Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L. Yannawar category:cs.CV  published:2014-06-24 summary:HOCR is abbreviated as Handwritten Optical Character Recognition. HOCR is a process of recognition of different handwritten characters from a digital image of documents. Handwritten automatic character recognition has attracted many researchers all over the world to contribute handwritten character recognition domain. Shape identification and feature extraction is very important part of any character recognition system and success of method is highly dependent on selection of features. However feature extraction is the most important step in defining the shape of the character as precisely and as uniquely as possible. This is indeed the most important step and complex task as well and achieved success by using invariance property, irrespective of position and orientation. Zernike moments describes shape, identify rotation invariant due to its Orthogonality property. MODI is an ancient script of India had cursive and complex representation of characters. The work described in this paper presents efficiency of Zernike moments over Hu 7 moment with zoning for automatic recognition of handwritten MODI characters. Offline approach is used in this paper because MODI Script was very popular and widely used for writing purpose till 19th century before Devanagari was officially adopted. version:4
arxiv-1404-1151 | Recognition of Handwritten MODI Numerals using Hu and Zernike features | http://arxiv.org/abs/1404.1151 | id:1404.1151 author:Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L. Yannawar category:cs.CV  published:2014-04-04 summary:Handwritten automatic character recognition has attracted many researchers all over the world to contribute automatic character recognition domain. Shape identification and feature extraction is very important part of any character recognition system and success of method is highly dependent on selection of features. However feature extraction is the most important step in defining the shape of the character as precisely and as uniquely as possible. This is indeed the most important step and complex task as well and achieved success by using invariance property, irrespective of position and orientation. Zernike moments describes shape, identify rotation invariant due to its Orthogonality property. MODI is an ancient script of India had cursive and complex representation of characters. The work described in this paper presents efficiency of Zernike moments over Hus moment for automatic recognition of handwritten MODI numerals. version:3
arxiv-1410-7211 | A method for context-based adaptive QRS clustering in real-time | http://arxiv.org/abs/1410.7211 | id:1410.7211 author:Daniel Castro, Paulo Félix, Jesús Presedo category:cs.CV physics.med-ph  published:2014-10-27 summary:Continuous follow-up of heart condition through long-term electrocardiogram monitoring is an invaluable tool for diagnosing some cardiac arrhythmias. In such context, providing tools for fast locating alterations of normal conduction patterns is mandatory and still remains an open issue. This work presents a real-time method for adaptive clustering QRS complexes from multilead ECG signals that provides the set of QRS morphologies that appear during an ECG recording. The method processes the QRS complexes sequentially, grouping them into a dynamic set of clusters based on the information content of the temporal context. The clusters are represented by templates which evolve over time and adapt to the QRS morphology changes. Rules to create, merge and remove clusters are defined along with techniques for noise detection in order to avoid their proliferation. To cope with beat misalignment, Derivative Dynamic Time Warping is used. The proposed method has been validated against the MIT-BIH Arrhythmia Database and the AHA ECG Database showing a global purity of 98.56% and 99.56%, respectively. Results show that our proposal not only provides better results than previous offline solutions but also fulfills real-time requirements. version:1
arxiv-1306-0543 | Predicting Parameters in Deep Learning | http://arxiv.org/abs/1306.0543 | id:1306.0543 author:Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas category:cs.LG cs.NE stat.ML  published:2013-06-03 summary:We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. version:2
arxiv-1410-7182 | Analysis of Named Entity Recognition and Linking for Tweets | http://arxiv.org/abs/1410.7182 | id:1410.7182 author:Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke van Erp, Genevieve Gorrell, Raphaël Troncy, Johann Petrak, Kalina Bontcheva category:cs.CL  published:2014-10-27 summary:Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisy texts, what the main sources of error are, and which problems should be further investigated to improve the state of the art. version:1
arxiv-1410-7164 | Directional Bilateral Filters | http://arxiv.org/abs/1410.7164 | id:1410.7164 author:Manasij Venkatesh, Chandra Sekhar Seelamantula category:cs.CV  published:2014-10-27 summary:We propose a bilateral filter with a locally controlled domain kernel for directional edge-preserving smoothing. Traditional bilateral filters use a range kernel, which is responsible for edge preservation, and a fixed domain kernel that performs smoothing. Our intuition is that orientation and anisotropy of image structures should be incorporated into the domain kernel while smoothing. For this purpose, we employ an oriented Gaussian domain kernel locally controlled by a structure tensor. The oriented domain kernel combined with a range kernel forms the directional bilateral filter. The two kernels assist each other in effectively suppressing the influence of the outliers while smoothing. To find the optimal parameters of the directional bilateral filter, we propose the use of Stein's unbiased risk estimate (SURE). We test the capabilities of the kernels separately as well as together, first on synthetic images, and then on real endoscopic images. The directional bilateral filter has better denoising performance than the Gaussian bilateral filter at various noise levels in terms of peak signal-to-noise ratio (PSNR). version:1
arxiv-1301-5356 | Efficient MRF Energy Propagation for Video Segmentation via Bilateral Filters | http://arxiv.org/abs/1301.5356 | id:1301.5356 author:Ozan Sener, Kemal Ugur, A. Aydin Alatan category:cs.CV  published:2013-01-22 summary:Segmentation of an object from a video is a challenging task in multimedia applications. Depending on the application, automatic or interactive methods are desired; however, regardless of the application type, efficient computation of video object segmentation is crucial for time-critical applications; specifically, mobile and interactive applications require near real-time efficiencies. In this paper, we address the problem of video segmentation from the perspective of efficiency. We initially redefine the problem of video object segmentation as the propagation of MRF energies along the temporal domain. For this purpose, a novel and efficient method is proposed to propagate MRF energies throughout the frames via bilateral filters without using any global texture, color or shape model. Recently presented bi-exponential filter is utilized for efficiency, whereas a novel technique is also developed to dynamically solve graph-cuts for varying, non-lattice graphs in general linear filtering scenario. These improvements are experimented for both automatic and interactive video segmentation scenarios. Moreover, in addition to the efficiency, segmentation quality is also tested both quantitatively and qualitatively. Indeed, for some challenging examples, significant time efficiency is observed without loss of segmentation quality. version:3
arxiv-1410-7100 | Estimating the intrinsic dimension in fMRI space via dataset fractal analysis - Counting the `cpu cores' of the human brain | http://arxiv.org/abs/1410.7100 | id:1410.7100 author:Harris V. Georgiou category:cs.AI cs.CV q-bio.NC stat.ML  published:2014-10-27 summary:Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool for localizing and analyzing brain activity. This study focuses on one very important aspect of the functional properties of human brain, specifically the estimation of the level of parallelism when performing complex cognitive tasks. Using fMRI as the main modality, the human brain activity is investigated through a purely data-driven signal processing and dimensionality analysis approach. Specifically, the fMRI signal is treated as a multi-dimensional data space and its intrinsic `complexity' is studied via dataset fractal analysis and blind-source separation (BSS) methods. One simulated and two real fMRI datasets are used in combination with Independent Component Analysis (ICA) and fractal analysis for estimating the intrinsic (true) dimensionality, in order to provide data-driven experimental evidence on the number of independent brain processes that run in parallel when visual or visuo-motor tasks are performed. Although this number is can not be defined as a strict threshold but rather as a continuous range, when a specific activation level is defined, a corresponding number of parallel processes or the casual equivalent of `cpu cores' can be detected in normal human brain activity. version:1
arxiv-1410-7098 | Concavity of reweighted Kikuchi approximation | http://arxiv.org/abs/1410.7098 | id:1410.7098 author:Po-Ling Loh, Andre Wibisono category:stat.ML math.ST stat.TH  published:2014-10-26 summary:We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach. version:1
arxiv-1411-1668 | On Chord and Sagitta in ${\mathbb Z}^2$: An Analysis towards Fast and Robust Circular Arc Detection | http://arxiv.org/abs/1411.1668 | id:1411.1668 author:Sahadev Bera, Shyamosree Pal, Partha Bhowmick, Bhargab B. Bhattacharya category:cs.CG cs.CV  published:2014-10-26 summary:Although chord and sagitta, when considered in tandem, may reflect many underlying geometric properties of circles on the Euclidean plane, their implications on the digital plane are not yet well-understood. In this paper, we explore some of their fundamental properties on the digital plane that have a strong bearing on the unsupervised detection of circles and circular arcs in a digital image. We show that although the chord-and-sagitta properties of a real circle do not readily migrate to the digital plane, they can indeed be used for the analysis in the discrete domain based on certain bounds on their deviations, which are derived from the real domain. In particular, we derive an upper bound on the circumferential angular deviation of a point in the context of chord property, and an upper bound on the relative error in radius estimation with regard to the sagitta property. Using these two bounds, we design a novel algorithm for the detection and parameterization of circles and circular arcs, which does not require any heuristic initialization or manual tuning. The chord property is deployed for the detection of circular arcs, whereas the sagitta property is used to estimate their centers and radii. Finally, to improve the accuracy of estimation, the notion of restricted Hough transform is used. Experimental results demonstrate superior efficiency and robustness of the proposed methodology compared to existing techniques. version:1
arxiv-1410-7057 | Sparse Distributed Learning via Heterogeneous Diffusion Adaptive Networks | http://arxiv.org/abs/1410.7057 | id:1410.7057 author:Bijit Kumar Das, Mrityunjoy Chakraborty, Jerónimo Arenas-García category:cs.LG cs.DC cs.SY stat.ML  published:2014-10-26 summary:In-network distributed estimation of sparse parameter vectors via diffusion LMS strategies has been studied and investigated in recent years. In all the existing works, some convex regularization approach has been used at each node of the network in order to achieve an overall network performance superior to that of the simple diffusion LMS, albeit at the cost of increased computational overhead. In this paper, we provide analytical as well as experimental results which show that the convex regularization can be selectively applied only to some chosen nodes keeping rest of the nodes sparsity agnostic, while still enjoying the same optimum behavior as can be realized by deploying the convex regularization at all the nodes. Due to the incorporation of unregularized learning at a subset of nodes, less computational cost is needed in the proposed approach. We also provide a guideline for selection of the sparsity aware nodes and a closed form expression for the optimum regularization parameter. version:1
arxiv-1407-3716 | Performance Guarantees for Schatten-$p$ Quasi-Norm Minimization in Recovery of Low-Rank Matrices | http://arxiv.org/abs/1407.3716 | id:1407.3716 author:Mohammadreza Malek-Mohammadi, Massoud Babaie-Zadeh, Mikael Skoglund category:cs.IT math.IT stat.ML  published:2014-07-14 summary:We address some theoretical guarantees for Schatten-$p$ quasi-norm minimization ($p \in (0,1]$) in recovering low-rank matrices from compressed linear measurements. Firstly, using null space properties of the measurement operator, we provide a sufficient condition for exact recovery of low-rank matrices. This condition guarantees unique recovery of matrices of ranks equal or larger than what is guaranteed by nuclear norm minimization. Secondly, this sufficient condition leads to a theorem proving that all restricted isometry property (RIP) based sufficient conditions for $\ell_p$ quasi-norm minimization generalize to Schatten-$p$ quasi-norm minimization. Based on this theorem, we provide a few RIP-based recovery conditions. version:2
arxiv-1211-7012 | Learning-Assisted Automated Reasoning with Flyspeck | http://arxiv.org/abs/1211.7012 | id:1211.7012 author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO  published:2012-11-29 summary:The considerable mathematical knowledge encoded by the Flyspeck project is combined with external automated theorem provers (ATPs) and machine-learning premise selection methods trained on the proofs, producing an AI system capable of answering a wide range of mathematical queries automatically. The performance of this architecture is evaluated in a bootstrapping scenario emulating the development of Flyspeck from axioms to the last theorem, each time using only the previous theorems and proofs. It is shown that 39% of the 14185 theorems could be proved in a push-button mode (without any high-level advice and user interaction) in 30 seconds of real time on a fourteen-CPU workstation. The necessary work involves: (i) an implementation of sound translations of the HOL Light logic to ATP formalisms: untyped first-order, polymorphic typed first-order, and typed higher-order, (ii) export of the dependency information from HOL Light and ATP proofs for the machine learners, and (iii) choice of suitable representations and methods for learning from previous proofs, and their integration as advisors with HOL Light. This work is described and discussed here, and an initial analysis of the body of proofs that were found fully automatically is provided. version:3
arxiv-1410-7029 | A Novel Statistical Method Based on Dynamic Models for Classification | http://arxiv.org/abs/1410.7029 | id:1410.7029 author:Lerong Li, Momiao Xiong category:stat.ML  published:2014-10-26 summary:Realizations of stochastic process are often observed temporal data or functional data. There are growing interests in classification of dynamic or functional data. The basic feature of functional data is that the functional data have infinite dimensions and are highly correlated. An essential issue for classifying dynamic and functional data is how to effectively reduce their dimension and explore dynamic feature. However, few statistical methods for dynamic data classification have directly used rich dynamic features of the data. We propose to use second order ordinary differential equation (ODE) to model dynamic process and principal differential analysis to estimate constant or time-varying parameters in the ODE. We examine differential dynamic properties of the dynamic system across different conditions including stability and transient-response, which determine how the dynamic systems maintain their functions and performance under a broad range of random internal and external perturbations. We use the parameters in the ODE as features for classifiers. As a proof of principle, the proposed methods are applied to classifying normal and abnormal QRS complexes in the electrocardiogram (ECG) data analysis, which is of great clinical values in diagnosis of cardiovascular diseases. We show that the ODE-based classification methods in QRS complex classification outperform the currently widely used neural networks with Fourier expansion coefficients of the functional data as their features. We expect that the dynamic model-based classification methods may open a new avenue for functional data classification. version:1
arxiv-1409-1612 | Semantic clustering of Russian web search results: possibilities and problems | http://arxiv.org/abs/1409.1612 | id:1409.1612 author:Andrey Kutuzov category:cs.CL cs.IR  published:2014-09-04 summary:The paper deals with word sense induction from lexical co-occurrence graphs. We construct such graphs on large Russian corpora and then apply this data to cluster Mail.ru Search results according to meanings of the query. We compare different methods of performing such clustering and different source corpora. Models of applying distributional semantics to big linguistic data are described. version:2
arxiv-1407-0208 | A Bayes consistent 1-NN classifier | http://arxiv.org/abs/1407.0208 | id:1407.0208 author:Aryeh Kontorovich, Roi Weiss category:cs.LG stat.ML  published:2014-07-01 summary:We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-NN enjoys considerable statistical and algorithmic advantages over the k-NN classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported. version:2
arxiv-1108-4324 | Sparse Estimation using Bayesian Hierarchical Prior Modeling for Real and Complex Linear Models | http://arxiv.org/abs/1108.4324 | id:1108.4324 author:Niels Lovmand Pedersen, Carles Navarro Manchón, Mihai-Alin Badiu, Dmitriy Shutin, Bernard Henri Fleury category:stat.ML  published:2011-08-22 summary:In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have been used to model sparsity-inducing priors that realize a class of concave penalty functions for the regression task in real-valued signal models. Motivated by the relative scarcity of formal tools for SBL in complex-valued models, this paper proposes a GSM model - the Bessel K model - that induces concave penalty functions for the estimation of complex sparse signals. The properties of the Bessel K model are analyzed when it is applied to Type I and Type II estimation. This analysis reveals that, by tuning the parameters of the mixing pdf different penalty functions are invoked depending on the estimation type used, the value of the noise variance, and whether real or complex signals are estimated. Using the Bessel K model, we derive a sparse estimator based on a modification of the expectation-maximization algorithm formulated for Type II estimation. The estimator includes as a special instance the algorithms proposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical results show the superiority of the proposed estimator over these state-of-the-art estimators in terms of convergence speed, sparseness, reconstruction error, and robustness in low and medium signal-to-noise ratio regimes. version:3
arxiv-1410-6996 | Improved depth imaging by constrained full-waveform inversion | http://arxiv.org/abs/1410.6996 | id:1410.6996 author:Musa Maharramov, Biondo Biondi category:physics.geo-ph cs.CV  published:2014-10-26 summary:We propose a formulation of full-wavefield inversion (FWI) as a constrained optimization problem, and describe a computationally efficient technique for solving constrained full-wavefield inversion (CFWI). The technique is based on using a total-variation regularization method, with the regularization weighted in favor of constraining deeper subsurface model sections. The method helps to promote "edge-preserving" blocky model inversion where fitting the seismic data alone fails to adequately constrain the model. The method is demonstrated on synthetic datasets with added noise, and is shown to enhance the sharpness of the inverted model and correctly reposition mispositioned reflectors by better constraining the velocity model at depth. version:1
arxiv-1410-6990 | Local Rademacher Complexity for Multi-label Learning | http://arxiv.org/abs/1410.6990 | id:1410.6990 author:Chang Xu, Tongliang Liu, Dacheng Tao, Chao Xu category:stat.ML cs.LG  published:2014-10-26 summary:We analyze the local Rademacher complexity of empirical risk minimization (ERM)-based multi-label learning algorithms, and in doing so propose a new algorithm for multi-label learning. Rather than using the trace norm to regularize the multi-label predictor, we instead minimize the tail sum of the singular values of the predictor in multi-label learning. Benefiting from the use of the local Rademacher complexity, our algorithm, therefore, has a sharper generalization error bound and a faster convergence rate. Compared to methods that minimize over all singular values, concentrating on the tail singular values results in better recovery of the low-rank structure of the multi-label predictor, which plays an import role in exploiting label correlations. We propose a new conditional singular value thresholding algorithm to solve the resulting objective function. Empirical studies on real-world datasets validate our theoretical results and demonstrate the effectiveness of the proposed algorithm. version:1
arxiv-1410-6984 | Fully Automated Myocardial Infarction Classification using Ordinary Differential Equations | http://arxiv.org/abs/1410.6984 | id:1410.6984 author:Getie Zewdie, Momiao Xiong category:stat.ML  published:2014-10-26 summary:Portable, Wearable and Wireless electrocardiogram (ECG) Systems have the potential to be used as point-of-care for cardiovascular disease diagnostic systems. Such wearable and wireless ECG systems require automatic detection of cardiovascular disease. Even in the primary care, automation of ECG diagnostic systems will improve efficiency of ECG diagnosis and reduce the minimal training requirement of local healthcare workers. However, few fully automatic myocardial infarction (MI) disease detection algorithms have well been developed. This paper presents a novel automatic MI classification algorithm using second order ordinary differential equation (ODE) with time varying coefficients, which simultaneously captures morphological and dynamic feature of highly correlated ECG signals. By effectively estimating the unobserved state variables and the parameters of the second order ODE, the accuracy of the classification was significantly improved. The estimated time varying coefficients of the second order ODE were used as an input to the support vector machine (SVM) for the MI classification. The proposed method was applied to the PTB diagnostic ECG database within Physionet. The overall sensitivity, specificity, and classification accuracy of 12 lead ECGs for MI binary classifications were 98.7%, 96.4% and 98.3%, respectively. We also found that even using one lead ECG signals, we can reach accuracy as high as 97%. Multiclass MI classification is a challenging task but the developed ODE approach for 12 lead ECGs coupled with multiclass SVM reached 96.4% accuracy for classifying 5 subgroups of MI and healthy controls. version:1
arxiv-1410-6975 | Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering | http://arxiv.org/abs/1410.6975 | id:1410.6975 author:Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski category:cs.LG  published:2014-10-26 summary:In this paper, we compare three initialization schemes for the KMEANS clustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and 3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k needs to be set by the user of the algorithms. (Kang 2013) recently proposed a novel use of determinantal point processes for sampling the initial centroids for the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide any evaluation establishing that KMEANSD++ is better than other algorithms. In this paper, we show that the performance of KMEANSD++ is comparable to KMEANS++ (both of which are better than KMEANSRAND) with KMEANSD++ having an additional that it can automatically approximate the value of k. version:1
arxiv-1410-6909 | A Framework for On-Line Devanagari Handwritten Character Recognition | http://arxiv.org/abs/1410.6909 | id:1410.6909 author:Sunil Kumar Kopparapu, Lajish V. L category:cs.CV  published:2014-10-25 summary:The main challenge in on-line handwritten character recognition in Indian lan- guage is the large size of the character set, larger similarity between different characters in the script and the huge variation in writing style. In this paper we propose a framework for on-line handwitten script recognition taking cues from speech signal processing literature. The framework is based on identify- ing strokes, which in turn lead to recognition of handwritten on-line characters rather that the conventional character identification. Though the framework is described for Devanagari script, the framework is general and can be applied to any language. The proposed platform consists of pre-processing, feature extraction, recog- nition and post processing like the conventional character recognition but ap- plied to strokes. The on-line Devanagari character recognition reduces to one of recognizing one of 69 primitives and recognition of a character is performed by recognizing a sequence of such primitives. We further show the impact of noise removal on on-line raw data which is usually noisy. The use of Fuzzy Direc- tional Features to enhance the accuracy of stroke recognition is also described. The recognition results are compared with commonly used directional features in literature using several classifiers. version:1
arxiv-1410-7382 | Modified Mel Filter Bank to Compute MFCC of Subsampled Speech | http://arxiv.org/abs/1410.7382 | id:1410.7382 author:Kiran Kumar Bhuvanagiri, Sunil Kumar Kopparapu category:cs.CL cs.SD  published:2014-10-25 summary:Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used speech features in most speech and speaker recognition applications. In this work, we propose a modified Mel filter bank to extract MFCCs from subsampled speech. We also propose a stronger metric which effectively captures the correlation between MFCCs of original speech and MFCC of resampled speech. It is found that the proposed method of filter bank construction performs distinguishably well and gives recognition performance on resampled speech close to recognition accuracies on original speech. version:1
arxiv-1410-6903 | Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech | http://arxiv.org/abs/1410.6903 | id:1410.6903 author:Laxmi Narayana M., Sunil Kumar Kopparapu category:cs.SD cs.CL  published:2014-10-25 summary:Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used speech features in most speech and speaker recognition applications. In this paper, we study the effect of resampling a speech signal on these speech features. We first derive a relationship between the MFCC param- eters of the resampled speech and the MFCC parameters of the original speech. We propose six methods of calculating the MFCC parameters of downsampled speech by transforming the Mel filter bank used to com- pute MFCC of the original speech. We then experimentally compute the MFCC parameters of the down sampled speech using the proposed meth- ods and compute the Pearson coefficient between the MFCC parameters of the downsampled speech and that of the original speech to identify the most effective choice of Mel-filter band that enables the computed MFCC of the resampled speech to be as close as possible to the original speech sample MFCC. version:1
arxiv-1410-6880 | Screening Rules for Overlapping Group Lasso | http://arxiv.org/abs/1410.6880 | id:1410.6880 author:Seunghak Lee, Eric P. Xing category:stat.ML cs.LG  published:2014-10-25 summary:Recently, to solve large-scale lasso and group lasso problems, screening rules have been developed, the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others. However, screening for overlapping group lasso remains an open challenge because the overlaps between groups make it infeasible to test each group independently. In this paper, we develop screening rules for overlapping group lasso. To address the challenge arising from groups with overlaps, we take into account overlapping groups only if they are inclusive of the group being tested, and then we derive screening rules, adopting the dual polytope projection approach. This strategy allows us to screen each group independently of each other. In our experiments, we demonstrate the efficiency of our screening rules on various datasets. version:1
arxiv-1410-6830 | Clustering Words by Projection Entropy | http://arxiv.org/abs/1410.6830 | id:1410.6830 author:Işık Barış Fidaner, Ali Taylan Cemgil category:cs.CL cs.LG  published:2014-10-24 summary:We apply entropy agglomeration (EA), a recently introduced algorithm, to cluster the words of a literary text. EA is a greedy agglomerative procedure that minimizes projection entropy (PE), a function that can quantify the segmentedness of an element set. To apply it, the text is reduced to a feature allocation, a combinatorial object to represent the word occurences in the text's paragraphs. The experiment results demonstrate that EA, despite its reduction and simplicity, is useful in capturing significant relationships among the words in the text. This procedure was implemented in Python and published as a free software: REBUS. version:1
arxiv-1312-5258 | On the Challenges of Physical Implementations of RBMs | http://arxiv.org/abs/1312.5258 | id:1312.5258 author:Vincent Dumoulin, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG  published:2013-12-18 summary:Restricted Boltzmann machines (RBMs) are powerful machine learning models, but learning and some kinds of inference in the model require sampling-based approximations, which, in classical digital computers, are implemented using expensive MCMC. Physical computation offers the opportunity to reduce the cost of sampling by building physical systems whose natural dynamics correspond to drawing samples from the desired RBM distribution. Such a system avoids the burn-in and mixing cost of a Markov chain. However, hardware implementations of this variety usually entail limitations such as low-precision and limited range of the parameters and restrictions on the size and topology of the RBM. We conduct software simulations to determine how harmful each of these restrictions is. Our simulations are designed to reproduce aspects of the D-Wave quantum computer, but the issues we investigate arise in most forms of physical computation. version:2
arxiv-1410-6776 | Online and Stochastic Gradient Methods for Non-decomposable Loss Functions | http://arxiv.org/abs/1410.6776 | id:1410.6776 author:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain category:cs.LG stat.ML  published:2014-10-24 summary:Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, prec@k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method. version:1
arxiv-1410-6736 | On The Effect of Hyperedge Weights On Hypergraph Learning | http://arxiv.org/abs/1410.6736 | id:1410.6736 author:Sheng Huang, Ahmed Elgammal, Dan Yang category:cs.CV  published:2014-10-24 summary:Hypergraph is a powerful representation in several computer vision, machine learning and pattern recognition problems. In the last decade, many researchers have been keen to develop different hypergraph models. In contrast, no much attention has been paid to the design of hyperedge weights. However, many studies on pairwise graphs show that the choice of edge weight can significantly influence the performances of such graph algorithms. We argue that this also applies to hypegraphs. In this paper, we empirically discuss the influence of hyperedge weight on hypegraph learning via proposing three novel hyperedge weights from the perspectives of geometry, multivariate statistical analysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE, Sheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar to graph learning, several representative hyperedge weighting schemes can be concluded by our experimental studies. Moreover, the experiments also demonstrate that the combinations of such weighting schemes and conventional hypergraph models can get very promising classification and clustering performances in comparison with some recent state-of-the-art algorithms. version:1
arxiv-1406-2582 | Probabilistic ODE Solvers with Runge-Kutta Means | http://arxiv.org/abs/1406.2582 | id:1406.2582 author:Michael Schober, David Duvenaud, Philipp Hennig category:stat.ML cs.LG cs.NA math.NA  published:2014-06-10 summary:Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions. version:2
arxiv-1410-6604 | Median Selection Subset Aggregation for Parallel Inference | http://arxiv.org/abs/1410.6604 | id:1410.6604 author:Xiangyu Wang, Peichao Peng, David Dunson category:stat.ML cs.DC stat.CO stat.ME  published:2014-10-24 summary:For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the `median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors. version:1
arxiv-1410-6532 | A Novel Visual Word Co-occurrence Model for Person Re-identification | http://arxiv.org/abs/1410.6532 | id:1410.6532 author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV  published:2014-10-24 summary:Person re-identification aims to maintain the identity of an individual in diverse locations through different non-overlapping camera views. The problem is fundamentally challenging due to appearance variations resulting from differing poses, illumination and configurations of camera views. To deal with these difficulties, we propose a novel visual word co-occurrence model. We first map each pixel of an image to a visual word using a codebook, which is learned in an unsupervised manner. The appearance transformation between camera views is encoded by a co-occurrence matrix of visual word joint distributions in probe and gallery images. Our appearance model naturally accounts for spatial similarities and variations caused by pose, illumination & configuration change across camera views. Linear SVMs are then trained as classifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campus benchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on the Cumulative Match Characteristic (CMC) curves, and beats the state-of-the-art results by 10.44% and 22.27%. version:1
arxiv-1410-4599 | Non-parametric Bayesian Learning with Deep Learning Structure and Its Applications in Wireless Networks | http://arxiv.org/abs/1410.4599 | id:1410.4599 author:Erte Pan, Zhu Han category:cs.LG cs.NE cs.NI stat.ML  published:2014-10-16 summary:In this paper, we present an infinite hierarchical non-parametric Bayesian model to extract the hidden factors over observed data, where the number of hidden factors for each layer is unknown and can be potentially infinite. Moreover, the number of layers can also be infinite. We construct the model structure that allows continuous values for the hidden factors and weights, which makes the model suitable for various applications. We use the Metropolis-Hastings method to infer the model structure. Then the performance of the algorithm is evaluated by the experiments. Simulation results show that the model fits the underlying structure of simulated data. version:2
arxiv-1410-0925 | A Framework for the Volumetric Integration of Depth Images | http://arxiv.org/abs/1410.0925 | id:1410.0925 author:Victor Adrian Prisacariu, Olaf Kähler, Ming Ming Cheng, Carl Yuheng Ren, Julien Valentin, Philip H. S. Torr, Ian D. Reid, David W. Murray category:cs.CV  published:2014-10-03 summary:Volumetric models have become a popular representation for 3D scenes in recent years. One of the breakthroughs leading to their popularity was KinectFusion, where the focus is on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a truncated signed distance function leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is also memory-intensive and limits the applicability to small scale reconstructions. Several avenues have been explored for overcoming this limitation. With the aim of summarizing them and providing for a fast and flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The core idea is that individual steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the needs of the user. Along with the framework we also provide a set of components for scalable reconstruction: two implementations of camera trackers, based on RGB data and on depth data, two representations of the 3D volumetric data, a dense volume and one based on hashes of subblocks, and an optional module for swapping subblocks in and out of the typically limited GPU memory. version:3
arxiv-1410-6472 | Foreground-Background Segmentation Based on Codebook and Edge Detector | http://arxiv.org/abs/1410.6472 | id:1410.6472 author:Mikaël A. Mousse, Eugène C. Ezin, Cina Motamed category:cs.CV  published:2014-10-23 summary:Background modeling techniques are used for moving object detection in video. Many algorithms exist in the field of object detection with different purposes. In this paper, we propose an improvement of moving object detection based on codebook segmentation. We associate the original codebook algorithm with an edge detection algorithm. Our goal is to prove the efficiency of using an edge detection algorithm with a background modeling algorithm. Throughout our study, we compared the quality of the moving object detection when codebook segmentation algorithm is associated with some standard edge detectors. In each case, we use frame-based metrics for the evaluation of the detection. The different results are presented and analyzed. version:1
arxiv-1410-6447 | Density-Based Region Search with Arbitrary Shape for Object Localization | http://arxiv.org/abs/1410.6447 | id:1410.6447 author:Ji Zhao, Deyu Meng, Jiayi Ma category:cs.CV  published:2014-10-23 summary:Region search is widely used for object localization. Typically, the region search methods project the score of a classifier into an image plane, and then search the region with the maximal score. The recently proposed region search methods, such as efficient subwindow search and efficient region search, %which localize objects from the score distribution on an image are much more efficient than sliding window search. However, for some classifiers and tasks, the projected scores are nearly all positive, and hence maximizing the score of a region results in localizing nearly the entire images as objects, which is meaningless. In this paper, we observe that the large scores are mainly concentrated on or around objects. Based on this observation, we propose a method, named level set maximum-weight connected subgraph (LS-MWCS), which localizes objects with arbitrary shapes by searching regions with the densest score rather than the maximal score. The region density can be controlled by a parameter flexibly. And we prove an important property of the proposed LS-MWCS, which guarantees that the region with the densest score can be searched. Moreover, the LS-MWCS can be efficiently optimized by belief propagation. The method is evaluated on the problem of weakly-supervised object localization, and the quantitative results demonstrate the superiorities of our LS-MWCS compared to other state-of-the-art methods. version:1
arxiv-1410-4343 | Enhanced Multiobjective Evolutionary Algorithm based on Decomposition for Solving the Unit Commitment Problem | http://arxiv.org/abs/1410.4343 | id:1410.4343 author:Anupam Trivedi, Kunal Pal, Chiranjib Saha, Dipti Srinivasan category:cs.NE 68T04  published:2014-10-16 summary:The unit commitment (UC) problem is a nonlinear, high-dimensional, highly constrained, mixed-integer power system optimization problem and is generally solved in the literature considering minimizing the system operation cost as the only objective. However, due to increasing environmental concerns, the recent attention has shifted to incorporating emission in the problem formulation. In this paper, a multi-objective evolutionary algorithm based on decomposition (MOEA/D) is proposed to solve the UC problem as a multi-objective optimization problem considering minimizing cost and emission as the multiple objec- tives. Since, UC problem is a mixed-integer optimization problem consisting of binary UC variables and continuous power dispatch variables, a novel hybridization strategy is proposed within the framework of MOEA/D such that genetic algorithm (GA) evolves the binary variables while differential evolution (DE) evolves the continuous variables. Further, a novel non-uniform weight vector distribution strategy is proposed and a parallel island model based on combination of MOEA/D with uniform and non-uniform weight vector distribution strategy is implemented to enhance the performance of the presented algorithm. Extensive case studies are presented on different test systems and the effectiveness of the proposed hybridization strategy, the non-uniform weight vector distribution strategy and parallel island model is verified through stringent simulated results. Further, exhaustive benchmarking against the algorithms proposed in the literature is presented to demonstrate the superiority of the proposed algorithm in obtaining significantly better converged and uniformly distributed trade-off solutions. version:2
arxiv-1410-6413 | Initialization of multilayer forecasting artifical neural networks | http://arxiv.org/abs/1410.6413 | id:1410.6413 author:Vladimir V. Bochkarev, Yulia S. Maslennikova category:cs.NE stat.ME 62M45  62M10  68T05 I.5.1  published:2014-10-23 summary:In this paper, a new method was developed for initialising artificial neural networks predicting dynamics of time series. Initial weighting coefficients were determined for neurons analogously to the case of a linear prediction filter. Moreover, to improve the accuracy of the initialization method for a multilayer neural network, some variants of decomposition of the transformation matrix corresponding to the linear prediction filter were suggested. The efficiency of the proposed neural network prediction method by forecasting solutions of the Lorentz chaotic system is shown in this paper. version:1
arxiv-1410-6387 | On Lower and Upper Bounds in Smooth Strongly Convex Optimization - A Unified Approach via Linear Iterative Methods | http://arxiv.org/abs/1410.6387 | id:1410.6387 author:Yossi Arjevani category:math.OC cs.LG  published:2014-10-23 summary:In this thesis we develop a novel framework to study smooth and strongly convex optimization algorithms, both deterministic and stochastic. Focusing on quadratic functions we are able to examine optimization algorithms as a recursive application of linear operators. This, in turn, reveals a powerful connection between a class of optimization algorithms and the analytic theory of polynomials whereby new lower and upper bounds are derived. In particular, we present a new and natural derivation of Nesterov's well-known Accelerated Gradient Descent method by employing simple 'economic' polynomials. This rather natural interpretation of AGD contrasts with earlier ones which lacked a simple, yet solid, motivation. Lastly, whereas existing lower bounds are only valid when the dimensionality scales with the number of iterations, our lower bound holds in the natural regime where the dimensionality is fixed. version:1
arxiv-1410-6382 | Attribute Efficient Linear Regression with Data-Dependent Sampling | http://arxiv.org/abs/1410.6382 | id:1410.6382 author:Doron Kukliansky, Ohad Shamir category:cs.LG stat.ML  published:2014-10-23 summary:In this paper we analyze a budgeted learning setting, in which the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for ridge and lasso linear regression, which utilize the geometry of the data by a novel data-dependent sampling scheme. When the learner has prior knowledge on the second moments of the attributes, the optimal sampling probabilities can be calculated precisely, and result in data-dependent improvements factors for the excess risk over the state-of-the-art that may be as large as $O(\sqrt{d})$, where $d$ is the problem's dimension. Moreover, under reasonable assumptions our algorithms can use less attributes than full-information algorithms, which is the main concern in budgeted learning settings. To the best of our knowledge, these are the first algorithms able to do so in our setting. Where no such prior knowledge is available, we develop a simple estimation technique that given a sufficient amount of training examples, achieves similar improvements. We complement our theoretical analysis with experiments on several data sets which support our claims. version:1
arxiv-1409-5241 | Subspace Alignment For Domain Adaptation | http://arxiv.org/abs/1409.5241 | id:1409.5241 author:Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars category:cs.CV  published:2014-09-18 summary:In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces spanned by eigenvectors. Our method seeks a domain invariant feature space by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We present two approaches to determine the only hyper-parameter in our method corresponding to the size of the subspaces. In the first approach we tune the size of subspaces using a theoretical bound on the stability of the obtained result. In the second approach, we use maximum likelihood estimation to determine the subspace size, which is particularly useful for high dimensional data. Apart from PCA, we propose a subspace creation method that outperform partial least squares (PLS) and linear discriminant analysis (LDA) in domain adaptation. We test our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods. version:2
arxiv-1410-6271 | A General Stochastic Algorithmic Framework for Minimizing Expensive Black Box Objective Functions Based on Surrogate Models and Sensitivity Analysis | http://arxiv.org/abs/1410.6271 | id:1410.6271 author:Yilun Wang, Christine A. Shoemaker category:stat.ML 90C26  90C56 G.1.6; G.3; I.2.8  published:2014-10-23 summary:We are focusing on bound constrained global optimization problems, whose objective functions are computationally expensive black-box functions and have multiple local minima. The recently popular Metric Stochastic Response Surface (MSRS) algorithm proposed by \cite{Regis2007SRBF} based on adaptive or sequential learning based on response surfaces is revisited and further extended for better performance in case of higher dimensional problems. Specifically, we propose a new way to generate the candidate points which the next function evaluation point is picked from according to the metric criteria, based on a new definition of distance, and prove the global convergence of the corresponding. Correspondingly, a more adaptive implementation of MSRS, named "SO-SA", is presented. "SO-SA" is is more likely to perturb those most sensitive coordinates when generating the candidate points, instead of perturbing all coordinates simultaneously. Numerical experiments on both synthetic problems and real problems demonstrate the advantages of our new algorithm, compared with many state of the art alternatives.} version:1
arxiv-1410-6264 | Capturing spatial interdependence in image features: the counting grid, an epitomic representation for bags of features | http://arxiv.org/abs/1410.6264 | id:1410.6264 author:Alessandro Perina, Nebojsa Jojic category:cs.CV stat.ML  published:2014-10-23 summary:In recent scene recognition research images or large image regions are often represented as disorganized "bags" of features which can then be analyzed using models originally developed to capture co-variation of word counts in text. However, image feature counts are likely to be constrained in different ways than word counts in text. For example, as a camera pans upwards from a building entrance over its first few floors and then further up into the sky Fig. 1, some feature counts in the image drop while others rise -- only to drop again giving way to features found more often at higher elevations. The space of all possible feature count combinations is constrained both by the properties of the larger scene and the size and the location of the window into it. To capture such variation, in this paper we propose the use of the counting grid model. This generative model is based on a grid of feature counts, considerably larger than any of the modeled images, and considerably smaller than the real estate needed to tile the images next to each other tightly. Each modeled image is assumed to have a representative window in the grid in which the feature counts mimic the feature distribution in the image. We provide a learning procedure that jointly maps all images in the training set to the counting grid and estimates the appropriate local counts in it. Experimentally, we demonstrate that the resulting representation captures the space of feature count combinations more accurately than the traditional models, not only when the input images come from a panning camera, but even when modeling images of different scenes from the same category. version:1
arxiv-1404-4774 | Online Group Feature Selection | http://arxiv.org/abs/1404.4774 | id:1404.4774 author:Wang Jing, Zhao Zhong-Qiu, Hu Xuegang, Cheung Yiu-ming, Wang Meng, Wu Xindong category:cs.CV  published:2014-04-18 summary:Online feature selection with dynamic features has become an active research area in recent years. However, in some real-world applications such as image analysis and email spam filtering, features may arrive by groups. Existing online feature selection methods evaluate features individually, while existing group feature selection methods cannot handle online processing. Motivated by this, we formulate the online group feature selection problem, and propose a novel selection approach for this problem. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we use spectral analysis to select discriminative features in each group when it arrives. In the inter-group selection, we use Lasso to select a globally optimal subset of features. This 2-stage procedure continues until there are no more features to come or some predefined stopping conditions are met. Extensive experiments conducted on benchmark and real-world data sets demonstrate that our proposed approach outperforms other state-of-the-art online feature selection methods. version:3
arxiv-1401-3409 | Low-Rank Modeling and Its Applications in Image Analysis | http://arxiv.org/abs/1401.3409 | id:1401.3409 author:Xiaowei Zhou, Can Yang, Hongyu Zhao, Weichuan Yu category:cs.CV cs.LG stat.ML  published:2014-01-15 summary:Low-rank modeling generally refers to a class of methods that solve problems by representing variables of interest as low-rank matrices. It has achieved great success in various fields including computer vision, data mining, signal processing and bioinformatics. Recently, much progress has been made in theories, algorithms and applications of low-rank modeling, such as exact low-rank matrix recovery via convex programming and matrix completion applied to collaborative filtering. These advances have brought more and more attentions to this topic. In this paper, we review the recent advance of low-rank modeling, the state-of-the-art algorithms, and related applications in image analysis. We first give an overview to the concept of low-rank modeling and challenging problems in this area. Then, we summarize the models and algorithms for low-rank matrix recovery and illustrate their advantages and limitations with numerical experiments. Next, we introduce a few applications of low-rank modeling in the context of image analysis. Finally, we conclude this paper with some discussions. version:3
arxiv-1410-6126 | Motion Estimation via Robust Decomposition with Constrained Rank | http://arxiv.org/abs/1410.6126 | id:1410.6126 author:German Ros, Jose Alvarez, Julio Guerrero category:cs.CV  published:2014-10-22 summary:In this work, we address the problem of outlier detection for robust motion estimation by using modern sparse-low-rank decompositions, i.e., Robust PCA-like methods, to impose global rank constraints. Robust decompositions have shown to be good at splitting a corrupted matrix into an uncorrupted low-rank matrix and a sparse matrix, containing outliers. However, this process only works when matrices have relatively low rank with respect to their ambient space, a property not met in motion estimation problems. As a solution, we propose to exploit the partial information present in the decomposition to decide which matches are outliers. We provide evidences showing that even when it is not possible to recover an uncorrupted low-rank matrix, the resulting information can be exploited for outlier detection. To this end we propose the Robust Decomposition with Constrained Rank (RD-CR), a proximal gradient based method that enforces the rank constraints inherent to motion estimation. We also present a general framework to perform robust estimation for stereo Visual Odometry, based on our RD-CR and a simple but effective compressed optimization method that achieves high performance. Our evaluation on synthetic data and on the KITTI dataset demonstrates the applicability of our approach in complex scenarios and it yields state-of-the-art performance. version:1
arxiv-1401-0304 | Learning without Concentration | http://arxiv.org/abs/1401.0304 | id:1401.0304 author:Shahar Mendelson category:cs.LG stat.ML  published:2014-01-01 summary:We obtain sharp bounds on the performance of Empirical Risk Minimization performed in a convex class and with respect to the squared loss, without assuming that class members and the target are bounded functions or have rapidly decaying tails. Rather than resorting to a concentration-based argument, the method used here relies on a `small-ball' assumption and thus holds for classes consisting of heavy-tailed functions and for heavy-tailed targets. The resulting estimates scale correctly with the `noise level' of the problem, and when applied to the classical, bounded scenario, always improve the known bounds. version:2
arxiv-1311-2524 | Rich feature hierarchies for accurate object detection and semantic segmentation | http://arxiv.org/abs/1311.2524 | id:1311.2524 author:Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik category:cs.CV  published:2013-11-11 summary:Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn. version:5
arxiv-1410-6095 | Online Energy Price Matrix Factorization for Power Grid Topology Tracking | http://arxiv.org/abs/1410.6095 | id:1410.6095 author:Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick category:stat.ML cs.LG math.OC stat.AP  published:2014-10-22 summary:Grid security and open markets are two major smart grid goals. Transparency of market data facilitates a competitive and efficient energy environment, yet it may also reveal critical physical system information. Recovering the grid topology based solely on publicly available market data is explored here. Real-time energy prices are calculated as the Lagrange multipliers of network-constrained economic dispatch; that is, via a linear program (LP) typically solved every 5 minutes. Granted the grid Laplacian is a parameter of this LP, one could infer such a topology-revealing matrix upon observing successive LP dual outcomes. The matrix of spatio-temporal prices is first shown to factor as the product of the inverse Laplacian times a sparse matrix. Leveraging results from sparse matrix decompositions, topology recovery schemes with complementary strengths are subsequently formulated. Solvers scalable to high-dimensional and streaming market data are devised. Numerical validation using real load data on the IEEE 30-bus grid provide useful input for current and future market designs. version:1
arxiv-1410-6093 | Cosine Similarity Measure According to a Convex Cost Function | http://arxiv.org/abs/1410.6093 | id:1410.6093 author:Osman Gunay, Cem Emre Akbas, A. Enis Cetin category:cs.LG  published:2014-10-22 summary:In this paper, we describe a new vector similarity measure associated with a convex cost function. Given two vectors, we determine the surface normals of the convex function at the vectors. The angle between the two surface normals is the similarity measure. Convex cost function can be the negative entropy function, total variation (TV) function and filtered variation function. The convex cost function need not be differentiable everywhere. In general, we need to compute the gradient of the cost function to compute the surface normals. If the gradient does not exist at a given vector, it is possible to use the subgradients and the normal producing the smallest angle between the two vectors is used to compute the similarity measure. version:1
arxiv-1406-1440 | Bayesian matrix completion: prior specification | http://arxiv.org/abs/1406.1440 | id:1406.1440 author:Pierre Alquier, Vincent Cottet, Nicolas Chopin, Judith Rousseau category:stat.ML math.ST stat.CO stat.TH  published:2014-06-05 summary:Low-rank matrix estimation from incomplete measurements recently received increased attention due to the emergence of several challenging applications, such as recommender systems; see in particular the famous Netflix challenge. While the behaviour of algorithms based on nuclear norm minimization is now well understood, an as yet unexplored avenue of research is the behaviour of Bayesian algorithms in this context. In this paper, we briefly review the priors used in the Bayesian literature for matrix completion. A standard approach is to assign an inverse gamma prior to the singular values of a certain singular value decomposition of the matrix of interest; this prior is conjugate. However, we show that two other types of priors (again for the singular values) may be conjugate for this model: a gamma prior, and a discrete prior. Conjugacy is very convenient, as it makes it possible to implement either Gibbs sampling or Variational Bayes. Interestingly enough, the maximum a posteriori for these different priors is related to the nuclear norm minimization problems. We also compare all these priors on simulated datasets, and on the classical MovieLens and Netflix datasets. version:3
arxiv-1410-6031 | Demixed principal component analysis of population activity in higher cortical areas reveals independent representation of task parameters | http://arxiv.org/abs/1410.6031 | id:1410.6031 author:Dmitry Kobak, Wieland Brendel, Christos Constantinidis, Claudia E. Feierstein, Adam Kepecs, Zachary F. Mainen, Ranulfo Romo, Xue-Lian Qi, Naoshige Uchida, Christian K. Machens category:q-bio.NC stat.ML  published:2014-10-22 summary:Neurons in higher cortical areas, such as the prefrontal cortex, are known to be tuned to a variety of sensory and motor variables. The resulting diversity of neural tuning often obscures the represented information. Here we introduce a novel dimensionality reduction technique, demixed principal component analysis (dPCA), which automatically discovers and highlights the essential features in complex population activities. We reanalyze population data from the prefrontal areas of rats and monkeys performing a variety of working memory and decision-making tasks. In each case, dPCA summarizes the relevant features of the population response in a single figure. The population activity is decomposed into a few demixed components that capture most of the variance in the data and that highlight dynamic tuning of the population to various task parameters, such as stimuli, decisions, rewards, etc. Moreover, dPCA reveals strong, condition-independent components of the population activity that remain unnoticed with conventional approaches. version:1
arxiv-1410-5926 | Salient Object Detection: A Discriminative Regional Feature Integration Approach | http://arxiv.org/abs/1410.5926 | id:1410.5926 author:Huaizu Jiang, Zejian Yuan, Ming-Ming Cheng, Yihong Gong, Nanning Zheng, Jingdong Wang category:cs.CV  published:2014-10-22 summary:Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score. Saliency scores across multiple levels are finally fused to produce the saliency map. The contributions lie in two-fold. One is that we propose a discriminate regional feature integration approach for salient object detection. Compared with existing heuristic models, our proposed method is able to automatically integrate high-dimensional regional saliency features and choose discriminative ones. The other is that by investigating standard generic region properties as well as two widely studied concepts for salient object detection, i.e., regional contrast and backgroundness, our approach significantly outperforms state-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate that our method runs as fast as most existing algorithms. version:1
arxiv-1410-5920 | Active Regression by Stratification | http://arxiv.org/abs/1410.5920 | id:1410.5920 author:Sivan Sabato, Remi Munos category:stat.ML cs.LG  published:2014-10-22 summary:We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches the optimal risk using piecewise constant approximations. version:1
arxiv-1401-7702 | A Spectral Framework for Anomalous Subgraph Detection | http://arxiv.org/abs/1401.7702 | id:1401.7702 author:Benjamin A. Miller, Michelle S. Beard, Patrick J. Wolfe, Nadya T. Bliss category:cs.SI stat.ML  published:2014-01-29 summary:A wide variety of application domains are concerned with data consisting of entities and their relationships or connections, formally represented as graphs. Within these diverse application areas, a common problem of interest is the detection of a subset of entities whose connectivity is anomalous with respect to the rest of the data. While the detection of such anomalous subgraphs has received a substantial amount of attention, no application-agnostic framework exists for analysis of signal detectability in graph-based data. In this paper, we describe a framework that enables such analysis using the principal eigenspace of a graph's residuals matrix, commonly called the modularity matrix in community detection. Leveraging this analytical tool, we show that the framework has a natural power metric in the spectral norm of the anomalous subgraph's adjacency matrix (signal power) and of the background graph's residuals matrix (noise power). We propose several algorithms based on spectral properties of the residuals matrix, with more computationally expensive techniques providing greater detection power. Detection and identification performance are presented for a number of signal and noise models, including clusters and bipartite foregrounds embedded into simple random backgrounds as well as graphs with community structure and realistic degree distributions. The trends observed verify intuition gleaned from other signal processing areas, such as greater detection power when the signal is embedded within a less active portion of the background. We demonstrate the utility of the proposed techniques in detecting small, highly anomalous subgraphs in real graphs derived from Internet traffic and product co-purchases. version:2
arxiv-1410-5894 | Vehicle Detection and Tracking Techniques: A Concise Review | http://arxiv.org/abs/1410.5894 | id:1410.5894 author:Raad Ahmed Hadi, Ghazali Sulong, Loay Edwar George category:cs.CV  published:2014-10-22 summary:Vehicle detection and tracking applications play an important role for civilian and military applications such as in highway traffic surveillance control, management and urban traffic planning. Vehicle detection process on road are used for vehicle tracking, counts, average speed of each individual vehicle, traffic analysis and vehicle categorizing objectives and may be implemented under different environments changes. In this review, we present a concise overview of image processing methods and analysis tools which used in building these previous mentioned applications that involved developing traffic surveillance systems. More precisely and in contrast with other reviews, we classified the processing methods under three categories for more clarification to explain the traffic systems. version:1
arxiv-1410-6414 | A Parallel and Efficient Algorithm for Learning to Match | http://arxiv.org/abs/1410.6414 | id:1410.6414 author:Jingbo Shang, Tianqi Chen, Hang Li, Zhengdong Lu, Yong Yu category:cs.LG cs.AI  published:2014-10-22 summary:Many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains, including collaborative filtering, link prediction, image tagging, and web search. Machine learning techniques, referred to as learning-to-match in this paper, have been successfully applied to the problems. Among them, a class of state-of-the-art methods, named feature-based matrix factorization, formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model. Unfortunately, making those algorithms scale to real world problems is challenging, and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks. In this paper, we tackle this challenge with a novel parallel and efficient algorithm for feature-based matrix factorization. Our algorithm, based on coordinate descent, can easily handle hundreds of millions of instances and features on a single machine. The key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters, with guaranteed convergence on minimizing the original objective function. Experimental results demonstrate that the proposed method is effective on a wide range of matching problems, with efficiency significantly improved upon the baselines while accuracy retained unchanged. version:1
arxiv-1410-5884 | Mean-Field Networks | http://arxiv.org/abs/1410.5884 | id:1410.5884 author:Yujia Li, Richard Zemel category:cs.LG stat.ML  published:2014-10-21 summary:The mean field algorithm is a widely used approximate inference algorithm for graphical models whose exact inference is intractable. In each iteration of mean field, the approximate marginals for each variable are updated by getting information from the neighbors. This process can be equivalently converted into a feedforward network, with each layer representing one iteration of mean field and with tied weights on all layers. This conversion enables a few natural extensions, e.g. untying the weights in the network. In this paper, we study these mean field networks (MFNs), and use them as inference tools as well as discriminative models. Preliminary experiment results show that MFNs can learn to do inference very efficiently and perform significantly better than mean field as discriminative models. version:1
arxiv-1410-5877 | Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation | http://arxiv.org/abs/1410.5877 | id:1410.5877 author:Michael Bloodgood, Chris Callison-Burch category:cs.CL cs.LG stat.ML  published:2014-10-21 summary:We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement. version:1
arxiv-1410-5861 | Compositional Structure Learning for Action Understanding | http://arxiv.org/abs/1410.5861 | id:1410.5861 author:Ran Xu, Gang Chen, Caiming Xiong, Wei Chen, Jason J. Corso category:cs.CV  published:2014-10-21 summary:The focus of the action understanding literature has predominately been classification, how- ever, there are many applications demanding richer action understanding such as mobile robotics and video search, with solutions to classification, localization and detection. In this paper, we propose a compositional model that leverages a new mid-level representation called compositional trajectories and a locally articulated spatiotemporal deformable parts model (LALSDPM) for fully action understanding. Our methods is advantageous in capturing the variable structure of dynamic human activity over a long range. First, the compositional trajectories capture long-ranging, frequently co-occurring groups of trajectories in space time and represent them in discriminative hierarchies, where human motion is largely separated from camera motion; second, LASTDPM learns a structured model with multi-layer deformable parts to capture multiple levels of articulated motion. We implement our methods and demonstrate state of the art performance on all three problems: action detection, localization, and recognition. version:1
arxiv-1410-5850 | A Fast Hybrid Primal Heuristic for Multiband Robust Capacitated Network Design with Multiple Time Periods | http://arxiv.org/abs/1410.5850 | id:1410.5850 author:Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj category:math.OC cs.DS cs.NE  published:2014-10-21 summary:We investigate the Robust Multiperiod Network Design Problem, a generalization of the Capacitated Network Design Problem (CNDP) that, besides establishing flow routing and network capacity installation as in a canonical CNDP, also considers a planning horizon made up of multiple time periods and protection against fluctuations in traffic volumes. As a remedy against traffic volume uncertainty, we propose a Robust Optimization model based on Multiband Robustness (B\"using and D'Andreagiovanni, 2012), a refinement of classical Gamma-Robustness by Bertsimas and Sim that uses a system of multiple deviation bands. Since the resulting optimization problem may prove very challenging even for instances of moderate size solved by a state-of-the-art optimization solver, we propose a hybrid primal heuristic that combines a randomized fixing strategy inspired by ant colony optimization, which exploits information coming from linear relaxations of the problem, and an exact large neighbourhood search. Computational experiments on a set of realistic instances from the SNDlib show that our original heuristic can run fast and produce solutions of extremely high quality associated with low optimality gaps. version:1
arxiv-1410-5792 | Generalized Compression Dictionary Distance as Universal Similarity Measure | http://arxiv.org/abs/1410.5792 | id:1410.5792 author:Andrey Bogomolov, Bruno Lepri, Fabio Pianesi category:stat.ML cs.AI cs.CC cs.IT math.IT  published:2014-10-21 summary:We present a new similarity measure based on information theoretic measures which is superior than Normalized Compression Distance for clustering problems and inherits the useful properties of conditional Kolmogorov complexity. We show that Normalized Compression Dictionary Size and Normalized Compression Dictionary Entropy are computationally more efficient, as the need to perform the compression itself is eliminated. Also they scale linearly with exponential vector size growth and are content independent. We show that normalized compression dictionary distance is compressor independent, if limited to lossless compressors, which gives space for optimizations and implementation speed improvement for real-time and big data applications. The introduced measure is applicable for machine learning tasks of parameter-free unsupervised clustering, supervised learning such as classification and regression, feature selection, and is applicable for big data problems with order of magnitude speed increase. version:1
arxiv-1410-5816 | Daily Stress Recognition from Mobile Phone Data, Weather Conditions and Individual Traits | http://arxiv.org/abs/1410.5816 | id:1410.5816 author:Andrey Bogomolov, Bruno Lepri, Michela Ferron, Fabio Pianesi, Alex, Pentland category:cs.CY cs.LG physics.data-an stat.AP stat.ML  published:2014-10-21 summary:Research has proven that stress reduces quality of life and causes many diseases. For this reason, several researchers devised stress detection systems based on physiological parameters. However, these systems require that obtrusive sensors are continuously carried by the user. In our paper, we propose an alternative approach providing evidence that daily stress can be reliably recognized based on behavioral metrics, derived from the user's mobile phone activity and from additional indicators, such as the weather conditions (data pertaining to transitory properties of the environment) and the personality traits (data concerning permanent dispositions of individuals). Our multifactorial statistical model, which is person-independent, obtains the accuracy score of 72.28% for a 2-class daily stress recognition problem. The model is efficient to implement for most of multimedia applications due to highly reduced low-dimensional feature space (32d). Moreover, we identify and discuss the indicators which have strong predictive power. version:1
arxiv-1410-5684 | Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods | http://arxiv.org/abs/1410.5684 | id:1410.5684 author:Saahil Ognawala, Justin Bayer category:stat.ML cs.LG  published:2014-10-21 summary:Advancements in parallel processing have lead to a surge in multilayer perceptrons' (MLP) applications and deep learning in the past decades. Recurrent Neural Networks (RNNs) give additional representational power to feedforward MLPs by providing a way to treat sequential data. However, RNNs are hard to train using conventional error backpropagation methods because of the difficulty in relating inputs over many time-steps. Regularization approaches from MLP sphere, like dropout and noisy weight training, have been insufficiently applied and tested on simple RNNs. Moreover, solutions have been proposed to improve convergence in RNNs but not enough to improve the long term dependency remembering capabilities thereof. In this study, we aim to empirically evaluate the remembering and generalization ability of RNNs on polyphonic musical datasets. The models are trained with injected noise, random dropout, norm-based regularizers and their respective performances compared to well-initialized plain RNNs and advanced regularization methods like fast-dropout. We conclude with evidence that training with noise does not improve performance as conjectured by a few works in RNN optimization before ours. version:1
arxiv-1410-5652 | Improvement of PSO algorithm by memory based gradient search - application in inventory management | http://arxiv.org/abs/1410.5652 | id:1410.5652 author:Tamás Varga, András Király, János Abonyi category:cs.NE  published:2014-10-21 summary:Advanced inventory management in complex supply chains requires effective and robust nonlinear optimization due to the stochastic nature of supply and demand variations. Application of estimated gradients can boost up the convergence of Particle Swarm Optimization (PSO) algorithm but classical gradient calculation cannot be applied to stochastic and uncertain systems. In these situations Monte-Carlo (MC) simulation can be applied to determine the gradient. We developed a memory based algorithm where instead of generating and evaluating new simulated samples the stored and shared former function evaluations of the particles are sampled to estimate the gradients by local weighted least squares regression. The performance of the resulted regional gradient-based PSO is verified by several benchmark problems and in a complex application example where optimal reorder points of a supply chain are determined. version:1
arxiv-1402-3144 | A Robust Ensemble Approach to Learn From Positive and Unlabeled Data Using SVM Base Models | http://arxiv.org/abs/1402.3144 | id:1402.3144 author:Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor category:stat.ML cs.LG G.3; I.2.6; I.5.1  published:2014-02-13 summary:We present a novel approach to learn binary classifiers when only positive and unlabeled instances are available (PU learning). This problem is routinely cast as a supervised task with label noise in the negative set. We use an ensemble of SVM models trained on bootstrap resamples of the training data for increased robustness against label noise. The approach can be considered in a bagging framework which provides an intuitive explanation for its mechanics in a semi-supervised setting. We compared our method to state-of-the-art approaches in simulations using multiple public benchmark data sets. The included benchmark comprises three settings with increasing label noise: (i) fully supervised, (ii) PU learning and (iii) PU learning with false positives. Our approach shows a marginal improvement over existing methods in the second setting and a significant improvement in the third. version:2
arxiv-1410-5610 | Universality of Power Law Coding for Principal Neurons | http://arxiv.org/abs/1410.5610 | id:1410.5610 author:Gabriele Scheler category:q-bio.NC cs.NE  published:2014-10-21 summary:In this paper we document distributions for spike rates, synaptic weights and neural gains for principal neurons in various tissues and under different behavioral conditions. We find a remarkable consistency of a power-law, specifically lognormal, distribution across observations from auditory or visual cortex as well as midbrain nuclei, cerebellar Purkinje cells and striatal medium spiny neurons. An exception is documented for fast-spiking interneurons, as non-coding neurons, which seem to follow a normal distribution. The difference between strongly recurrent and transfer connectivity (cortex vs. striatum and cerebellum), or the level of activation (low in cortex, high in Purkinje cells and midbrain nuclei) seems to be irrelevant for these distributions. This has certain implications on neural coding. In particular, logarithmic scale distribution of neuronal output appears as a structural phenomenon that is always present in coding neurons. We also report data for a lognormal distribution of synaptic strengths in cortex, cerebellum and hippocampus and for intrinsic excitability in striatum, cortex and cerebellum. We present a neural model for gain, weights and spike rates, specifically matching the width of distributions. We discuss the data from the perspective of a hierarchical coding scheme with few sparse or top-level features and many additional distributed low-level features. Logarithmic-scale coding may solve an access problem by combining a local modular structure with few high frequency contact points. Computational models may need to incorporate these observations as primary constraints. More data are needed to consolidate the observations. version:1
arxiv-1410-5600 | Mobility Enhancement for Elderly | http://arxiv.org/abs/1410.5600 | id:1410.5600 author:Ramviyas Parasuraman category:cs.CV cs.RO  published:2014-10-21 summary:Loss of Mobility is a common handicap to senior citizens. It denies them the ease of movement they would like to have like outdoor visits, movement in hospitals, social outgoings, but more seriously in the day to day in-house routine functions necessary for living etc. Trying to overcome this handicap by means of servant or domestic help and simple wheel chairs is not only costly in the long run, but forces the senior citizen to be at the mercy of sincerity of domestic helps and also the consequent loss of dignity. In order to give a dignified life, the mobility obtained must be at the complete discretion, will and control of the senior citizen. This can be provided only by a reasonably sophisticated and versatile wheel chair, giving enhanced ability of vision, hearing through man-machine interface, and sensor aided navigation and control. More often than not senior people have poor vision which makes it difficult for them to maker visual judgement and so calls for the use of Artificial Intelligence in visual image analysis and guided navigation systems. In this project, we deal with two important enhancement features for mobility enhancement, Audio command and Vision aided obstacle detection and navigation. We have implemented speech recognition algorithm using template of stored words for identifying the voice command given by the user. This frees the user of an agile hand to operate joystick or mouse control. Also, we have developed a new appearance based obstacle detection system using stereo-vision cameras which estimates the distance of nearest obstacle to the wheel chair and takes necessary action. This helps user in making better judgement of route and navigate obstacles. The main challenge in this project is how to navigate in an unknown/unfamiliar environment by avoiding obstacles. version:1
arxiv-1410-5137 | On Iterative Hard Thresholding Methods for High-dimensional M-Estimation | http://arxiv.org/abs/1410.5137 | id:1410.5137 author:Prateek Jain, Ambuj Tewari, Purushottam Kar category:cs.LG stat.ML  published:2014-10-20 summary:The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard $L_0$ constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. We also extend our analysis to a large family of "fully corrective methods" that includes two-stage and partial hard-thresholding algorithms. We show that our results hold for the problem of sparse regression, as well as low-rank matrix recovery. version:2
arxiv-1410-5557 | Where do goals come from? A Generic Approach to Autonomous Goal-System Development | http://arxiv.org/abs/1410.5557 | id:1410.5557 author:Matthias Rolf, Minoru Asada category:cs.LG cs.AI  published:2014-10-21 summary:Goals express agents' intentions and allow them to organize their behavior based on low-dimensional abstractions of high-dimensional world states. How can agents develop such goals autonomously? This paper proposes a detailed conceptual and computational account to this longstanding problem. We argue to consider goals as high-level abstractions of lower-level intention mechanisms such as rewards and values, and point out that goals need to be considered alongside with a detection of the own actions' effects. We propose Latent Goal Analysis as a computational learning formulation thereof, and show constructively that any reward or value function can by explained by goals and such self-detection as latent mechanisms. We first show that learned goals provide a highly effective dimensionality reduction in a practical reinforcement learning problem. Then, we investigate a developmental scenario in which entirely task-unspecific rewards induced by visual saliency lead to self and goal representations that constitute goal-directed reaching. version:1
arxiv-1410-5524 | Learning to Rank Binary Codes | http://arxiv.org/abs/1410.5524 | id:1410.5524 author:Jie Feng, Wei Liu, Yan Wang category:cs.CV  published:2014-10-21 summary:Binary codes have been widely used in vision problems as a compact feature representation to achieve both space and time advantages. Various methods have been proposed to learn data-dependent hash functions which map a feature vector to a binary code. However, considerable data information is inevitably lost during the binarization step which also causes ambiguity in measuring sample similarity using Hamming distance. Besides, the learned hash functions cannot be changed after training, which makes them incapable of adapting to new data outside the training data set. To address both issues, in this paper we propose a flexible bitwise weight learning framework based on the binary codes obtained by state-of-the-art hashing methods, and incorporate the learned weights into the weighted Hamming distance computation. We then formulate the proposed framework as a ranking problem and leverage the Ranking SVM model to offline tackle the weight learning. The framework is further extended to an online mode which updates the weights at each time new data comes, thereby making it scalable to large and dynamic data sets. Extensive experimental results demonstrate significant performance gains of using binary codes with bitwise weighting in image retrieval tasks. It is appealing that the online weight learning leads to comparable accuracy with its offline counterpart, which thus makes our approach practical for realistic applications. version:1
arxiv-1410-5522 | Variational Reformulation of Bayesian Inverse Problems | http://arxiv.org/abs/1410.5522 | id:1410.5522 author:Panagiotis Tsilifis, Ilias Bilionis, Ioannis Katsounaros, Nicholas Zabaras category:stat.ML  published:2014-10-21 summary:The classical approach to inverse problems is based on the optimization of a misfit function. Despite its computational appeal, such an approach suffers from many shortcomings, e.g., non-uniqueness of solutions, modeling prior knowledge, etc. The Bayesian formalism to inverse problems avoids most of the difficulties encountered by the optimization approach, albeit at an increased computational cost. In this work, we use information theoretic arguments to cast the Bayesian inference problem in terms of an optimization problem. The resulting scheme combines the theoretical soundness of fully Bayesian inference with the computational efficiency of a simple optimization. version:1
arxiv-1410-5491 | Using Mechanical Turk to Build Machine Translation Evaluation Sets | http://arxiv.org/abs/1410.5491 | id:1410.5491 author:Michael Bloodgood, Chris Callison-Burch category:cs.CL cs.LG stat.ML  published:2014-10-20 summary:Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield. version:1
arxiv-1410-5467 | Machine Learning of Coq Proof Guidance: First Experiments | http://arxiv.org/abs/1410.5467 | id:1410.5467 author:Cezary Kaliszyk, Lionel Mamane, Josef Urban category:cs.LO cs.LG  published:2014-10-20 summary:We report the results of the first experiments with learning proof dependencies from the formalizations done with the Coq system. We explain the process of obtaining the dependencies from the Coq proofs, the characterization of formulas that is used for the learning, and the evaluation method. Various machine learning methods are compared on a dataset of 5021 toplevel Coq proofs coming from the CoRN repository. The best resulting method covers on average 75% of the needed proof dependencies among the first 100 predictions, which is a comparable performance of such initial experiments on other large-theory corpora. version:1
arxiv-1410-5392 | Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling for Gaussian Graphical Models | http://arxiv.org/abs/1410.5392 | id:1410.5392 author:Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng category:cs.DS cs.LG cs.NA math.NA stat.CO stat.ML  published:2014-10-20 summary:Motivated by a sampling problem basic to computational statistical inference, we develop a nearly optimal algorithm for a fundamental problem in spectral graph theory and numerical analysis. Given an $n\times n$ SDDM matrix ${\bf \mathbf{M}}$, and a constant $-1 \leq p \leq 1$, our algorithm gives efficient access to a sparse $n\times n$ linear operator $\tilde{\mathbf{C}}$ such that $${\mathbf{M}}^{p} \approx \tilde{\mathbf{C}} \tilde{\mathbf{C}}^\top.$$ The solution is based on factoring ${\bf \mathbf{M}}$ into a product of simple and sparse matrices using squaring and spectral sparsification. For ${\mathbf{M}}$ with $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and polylogarithmic depth on a parallel machine with $m$ processors. This gives the first sampling algorithm that only requires nearly linear work and $n$ i.i.d. random univariate Gaussian samples to generate i.i.d. random samples for $n$-dimensional Gaussian random fields with SDDM precision matrices. For sampling this natural subclass of Gaussian random fields, it is optimal in the randomness and nearly optimal in the work and parallel complexity. In addition, our sampling algorithm can be directly extended to Gaussian random fields with SDD precision matrices. version:1
arxiv-1410-5362 | Prediction of Synchrostate Transitions in EEG Signals Using Markov Chain Models | http://arxiv.org/abs/1410.5362 | id:1410.5362 author:Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik Maharatna category:q-bio.NC physics.med-ph stat.AP stat.ML  published:2014-10-20 summary:This paper proposes a stochastic model using the concept of Markov chains for the inter-state transitions of the millisecond order quasi-stable phase synchronized patterns or synchrostates, found in multi-channel Electroencephalogram (EEG) signals. First and second order transition probability matrices are estimated for Markov chain modelling from 100 trials of 128-channel EEG signals during two different face perception tasks. Prediction accuracies with such finite Markov chain models for synchrostate transition are also compared, under a data-partitioning based cross-validation scheme. version:1
arxiv-1410-7795 | Classification of Autism Spectrum Disorder Using Supervised Learning of Brain Connectivity Measures Extracted from Synchrostates | http://arxiv.org/abs/1410.7795 | id:1410.7795 author:Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik Maharatna, Fabio Apicella, Federico Sicca category:physics.med-ph cs.CV stat.AP stat.ML  published:2014-10-20 summary:Objective. The paper investigates the presence of autism using the functional brain connectivity measures derived from electro-encephalogram (EEG) of children during face perception tasks. Approach. Phase synchronized patterns from 128-channel EEG signals are obtained for typical children and children with autism spectrum disorder (ASD). The phase synchronized states or synchrostates temporally switch amongst themselves as an underlying process for the completion of a particular cognitive task. We used 12 subjects in each group (ASD and typical) for analyzing their EEG while processing fearful, happy and neutral faces. The minimal and maximally occurring synchrostates for each subject are chosen for extraction of brain connectivity features, which are used for classification between these two groups of subjects. Among different supervised learning techniques, we here explored the discriminant analysis and support vector machine both with polynomial kernels for the classification task. Main results. The leave one out cross-validation of the classification algorithm gives 94.7% accuracy as the best performance with corresponding sensitivity and specificity values as 85.7% and 100% respectively. Significance. The proposed method gives high classification accuracies and outperforms other contemporary research results. The effectiveness of the proposed method for classification of autistic and typical children suggests the possibility of using it on a larger population to validate it for clinical practice. version:1
arxiv-1410-5801 | Artifact reduction in multichannel pervasive EEG using hybrid WPT-ICA and WPT-EMD signal decomposition techniques | http://arxiv.org/abs/1410.5801 | id:1410.5801 author:Valentina Bono, Wasifa Jamal, Saptarshi Das, Koushik Maharatna category:physics.med-ph cs.LG stat.AP stat.ME  published:2014-10-20 summary:In order to reduce the muscle artifacts in multi-channel pervasive Electroencephalogram (EEG) signals, we here propose and compare two hybrid algorithms by combining the concept of wavelet packet transform (WPT), empirical mode decomposition (EMD) and Independent Component Analysis (ICA). The signal cleaning performances of WPT-EMD and WPT-ICA algorithms have been compared using a signal-to-noise ratio (SNR)-like criterion for artifacts. The algorithms have been tested on multiple trials of four different artifact cases viz. eye-blinking and muscle artifacts including left and right hand movement and head-shaking. version:1
arxiv-1410-5102 | On Bootstrapping Machine Learning Performance Predictors via Analytical Models | http://arxiv.org/abs/1410.5102 | id:1410.5102 author:Diego Didona, Paolo Romano category:cs.PF cs.LG  published:2014-10-19 summary:Performance modeling typically relies on two antithetic methodologies: white box models, which exploit knowledge on system's internals and capture its dynamics using analytical approaches, and black box techniques, which infer relations among the input and output variables of a system based on the evidences gathered during an initial training phase. In this paper we investigate a technique, which we name Bootstrapping, which aims at reconciling these two methodologies and at compensating the cons of the one with the pros of the other. We thoroughly analyze the design space of this gray box modeling technique, and identify a number of algorithmic and parametric trade-offs which we evaluate via two realistic case studies, a Key-Value Store and a Total Order Broadcast service. version:1
arxiv-1410-5078 | Learning Vague Concepts for the Semantic Web | http://arxiv.org/abs/1410.5078 | id:1410.5078 author:Paolo Pareti, Ewan Klein category:cs.AI cs.CL  published:2014-10-19 summary:Ontologies can be a powerful tool for structuring knowledge, and they are currently the subject of extensive research. Updating the contents of an ontology or improving its interoperability with other ontologies is an important but difficult process. In this paper, we focus on the presence of vague concepts, which are pervasive in natural language, within the framework of formal ontologies. We will adopt a framework in which vagueness is captured via numerical restrictions that can be automatically adjusted. Since updating vague concepts, either through ontology alignment or ontology evolution, can lead to inconsistent sets of axioms, we define and implement a method to detecting and repairing such inconsistencies in a local fashion. version:1
arxiv-1410-5058 | Dense 3D Face Correspondence | http://arxiv.org/abs/1410.5058 | id:1410.5058 author:Syed Zulqarnain Gilani, Faisal Shafait, Ajmal Mian category:cs.CV  published:2014-10-19 summary:We present an algorithm that automatically establishes dense correspondences between a large number of 3D faces. Starting from automatically detected sparse correspondences on the convex hull of 3D faces, the algorithm triangulates existing correspondences and expands them iteratively along the triangle edges. New correspondences are established by matching keypoints on the geodesic patches along the triangle edges and the process is repeated. After exhausting keypoint matches, further correspondences are established by evolving level set geodesic curves from the centroids of large triangles. A deformable model (K3DM) is constructed from the dense corresponded faces and an algorithm is proposed for morphing the K3DM to fit unseen faces. This algorithm iterates between rigid alignment of an unseen face followed by regularized morphing of the deformable model. We have extensively evaluated the proposed algorithms on synthetic data and real 3D faces from the FRGCv2 and BU3DFE databases using quantitative and qualitative benchmarks. Our algorithm achieved dense correspondences with a mean localization error of 1.28mm on synthetic faces and detected 18 anthropometric landmarks on unseen real faces from the FRGCv2 database with 3mm precision. Furthermore, our deformable model fitting algorithm achieved 99.8% gender classification and 98.3% face recognition accuracy on the FRGCv2 database. version:1
arxiv-1310-1341 | Director Field Model of the Primary Visual Cortex for Contour Detection | http://arxiv.org/abs/1310.1341 | id:1310.1341 author:Vijay Singh, Martin Tchernookov, Rebecca Butterfield, Ilya Nemenman category:q-bio.NC cs.CV  published:2013-10-04 summary:We aim to build the simplest possible model capable of detecting long, noisy contours in a cluttered visual scene. For this, we model the neural dynamics in the primate primary visual cortex in terms of a continuous director field that describes the average rate and the average orientational preference of active neurons at a particular point in the cortex. We then use a linear-nonlinear dynamical model with long range connectivity patterns to enforce long-range statistical context present in the analyzed images. The resulting model has substantially fewer degrees of freedom than traditional models, and yet it can distinguish large contiguous objects from the background clutter by suppressing the clutter and by filling-in occluded elements of object contours. This results in high-precision, high-recall detection of large objects in cluttered scenes. Parenthetically, our model has a direct correspondence with the Landau - de Gennes theory of nematic liquid crystal in two dimensions. version:2
arxiv-1410-4984 | Gaussian Process Models with Parallelization and GPU acceleration | http://arxiv.org/abs/1410.4984 | id:1410.4984 author:Zhenwen Dai, Andreas Damianou, James Hensman, Neil Lawrence category:cs.DC cs.LG stat.ML  published:2014-10-18 summary:In this work, we present an extension of Gaussian process (GP) models with sophisticated parallelization and GPU acceleration. The parallelization scheme arises naturally from the modular computational structure w.r.t. datapoints in the sparse Gaussian process formulation. Additionally, the computational bottleneck is implemented with GPU acceleration for further speed up. Combining both techniques allows applying Gaussian process models to millions of datapoints. The efficiency of our algorithm is demonstrated with a synthetic dataset. Its source code has been integrated into our popular software library GPy. version:1
arxiv-1410-4966 | The Visualization of Change in Word Meaning over Time using Temporal Word Embeddings | http://arxiv.org/abs/1410.4966 | id:1410.4966 author:Chiraag Lala, Shay B. Cohen category:cs.CL  published:2014-10-18 summary:We describe a visualization tool that can be used to view the change in meaning of words over time. The tool makes use of existing (static) word embedding datasets together with a timestamped $n$-gram corpus to create {\em temporal} word embeddings. version:1
arxiv-1204-4200 | Discrete Dynamical Genetic Programming in XCS | http://arxiv.org/abs/1204.4200 | id:1204.4200 author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY I.2.6  published:2012-04-18 summary:A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such discrete dynamical systems within XCS to solve a number of well-known test problems. version:2
arxiv-1410-5784 | Optimal Feature Selection from VMware ESXi 5.1 Feature Set | http://arxiv.org/abs/1410.5784 | id:1410.5784 author:Amartya Hatua category:cs.DC cs.LG  published:2014-10-18 summary:A study of VMware ESXi 5.1 server has been carried out to find the optimal set of parameters which suggest usage of different resources of the server. Feature selection algorithms have been used to extract the optimum set of parameters of the data obtained from VMware ESXi 5.1 server using esxtop command. Multiple virtual machines (VMs) are running in the mentioned server. K-means algorithm is used for clustering the VMs. The goodness of each cluster is determined by Davies Bouldin index and Dunn index respectively. The best cluster is further identified by the determined indices. The features of the best cluster are considered into a set of optimal parameters. version:1
arxiv-1405-7085 | Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds | http://arxiv.org/abs/1405.7085 | id:1405.7085 author:Raef Bassily, Adam Smith, Abhradeep Thakurta category:cs.LG cs.CR stat.ML  published:2014-05-27 summary:In this paper, we initiate a systematic investigation of differentially private algorithms for convex empirical risk minimization. Various instantiations of this problem have been studied before. We provide new algorithms and matching lower bounds for private ERM assuming only that each data point's contribution to the loss function is Lipschitz bounded and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even match the optimal non-private running time (as measured by oracle complexity). We give separate algorithms (and lower bounds) for $(\epsilon,0)$- and $(\epsilon,\delta)$-differential privacy; perhaps surprisingly, the techniques used for designing optimal algorithms in the two cases are completely different. Our lower bounds apply even to very simple, smooth function families, such as linear and quadratic functions. This implies that algorithms from previous work can be used to obtain optimal error rates, under the additional assumption that the contributions of each data point to the loss function is smooth. We show that simple approaches to smoothing arbitrary loss functions (in order to apply previous techniques) do not yield optimal error rates. In particular, optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median. version:2
arxiv-1410-4868 | A Modality Lexicon and its use in Automatic Tagging | http://arxiv.org/abs/1410.4868 | id:1410.4868 author:Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Nathaniel W. Filardo, Lori Levin, Christine Piatko category:cs.CL I.2.7  published:2014-10-17 summary:This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one tagger---a structure-based tagger---results in precision around 86% (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data. version:1
arxiv-1410-4863 | Arabic Language Text Classification Using Dependency Syntax-Based Feature Selection | http://arxiv.org/abs/1410.4863 | id:1410.4863 author:Yannis Haralambous, Yassir Elidrissi, Philippe Lenca category:cs.CL  published:2014-10-17 summary:We study the performance of Arabic text classification combining various techniques: (a) tfidf vs. dependency syntax, for feature selection and weighting; (b) class association rules vs. support vector machines, for classification. The Arabic text is used in two forms: rootified and lightly stemmed. The results we obtain show that lightly stemmed text leads to better performance than rootified text; that class association rules are better suited for small feature sets obtained by dependency syntax constraints; and, finally, that support vector machines are better suited for large feature sets based on morphological feature selection criteria. version:1
arxiv-1410-5703 | Robust Multidimensional Mean-Payoff Games are Undecidable | http://arxiv.org/abs/1410.5703 | id:1410.5703 author:Yaron Velner category:cs.LO cs.LG  published:2014-10-17 summary:Mean-payoff games play a central role in quantitative synthesis and verification. In a single-dimensional game a weight is assigned to every transition and the objective of the protagonist is to assure a non-negative limit-average weight. In the multidimensional setting, a weight vector is assigned to every transition and the objective of the protagonist is to satisfy a boolean condition over the limit-average weight of each dimension, e.g., $\LimAvg(x_1) \leq 0 \vee \LimAvg(x_2)\geq 0 \wedge \LimAvg(x_3) \geq 0$. We recently proved that when one of the players is restricted to finite-memory strategies then the decidability of determining the winner is inter-reducible with Hilbert's Tenth problem over rationals (a fundamental long-standing open problem). In this work we allow arbitrary (infinite-memory) strategies for both players and we show that the problem is undecidable. version:1
arxiv-1410-4828 | Generalized Conditional Gradient for Sparse Estimation | http://arxiv.org/abs/1410.4828 | id:1410.4828 author:Yaoliang Yu, Xinhua Zhang, Dale Schuurmans category:math.OC cs.LG stat.ML  published:2014-10-17 summary:Structured sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. In this paper we investigate the generalized conditional gradient (GCG) algorithm for solving structured sparse optimization problems---demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. After providing a comprehensive overview of the convergence properties of GCG, we develop efficient methods for evaluating polar operators, a subroutine that is required in each GCG iteration. In particular, we show how the polar operator can be efficiently evaluated in two important scenarios: dictionary learning and structured sparse estimation. A further improvement is achieved by interleaving GCG with fixed-rank local subspace optimization. A series of experiments on matrix completion, multi-class classification, multi-view dictionary learning and overlapping group lasso shows that the proposed method can significantly reduce the training cost of current alternatives. version:1
arxiv-1410-4821 | Convex Optimization in Julia | http://arxiv.org/abs/1410.4821 | id:1410.4821 author:Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, Stephen Boyd category:math.OC cs.MS stat.ML  published:2014-10-17 summary:This paper describes Convex, a convex optimization modeling framework in Julia. Convex translates problems from a user-friendly functional language into an abstract syntax tree describing the problem. This concise representation of the global structure of the problem allows Convex to infer whether the problem complies with the rules of disciplined convex programming (DCP), and to pass the problem to a suitable solver. These operations are carried out in Julia using multiple dispatch, which dramatically reduces the time required to verify DCP compliance and to parse a problem into conic form. Convex then automatically chooses an appropriate backend solver to solve the conic form problem. version:1
arxiv-1410-4792 | Variational Bayes for Merging Noisy Databases | http://arxiv.org/abs/1410.4792 | id:1410.4792 author:Tamara Broderick, Rebecca C. Steorts category:stat.ME stat.ML  published:2014-10-17 summary:Bayesian entity resolution merges together multiple, noisy databases and returns the minimal collection of unique individuals represented, together with their true, latent record values. Bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final, resolved database. However, existing Bayesian methods for entity resolution use Markov monte Carlo method (MCMC) approximations and are too slow to run on modern databases containing millions or billions of records. Instead, we propose applying variational approximations to allow scalable Bayesian inference in these models. We derive a coordinate-ascent approximation for mean-field variational Bayes, qualitatively compare our algorithm to existing methods, note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution, and discuss directions for future work in this domain. version:1
arxiv-1410-4777 | A Hierarchical Multi-Output Nearest Neighbor Model for Multi-Output Dependence Learning | http://arxiv.org/abs/1410.4777 | id:1410.4777 author:Richard G. Morris, Tony Martinez, Michael R. Smith category:stat.ML cs.LG  published:2014-10-17 summary:Multi-Output Dependence (MOD) learning is a generalization of standard classification problems that allows for multiple outputs that are dependent on each other. A primary issue that arises in the context of MOD learning is that for any given input pattern there can be multiple correct output patterns. This changes the learning task from function approximation to relation approximation. Previous algorithms do not consider this problem, and thus cannot be readily applied to MOD problems. To perform MOD learning, we introduce the Hierarchical Multi-Output Nearest Neighbor model (HMONN) that employs a basic learning model for each output and a modified nearest neighbor approach to refine the initial results. version:1
arxiv-1410-4744 | mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting | http://arxiv.org/abs/1410.4744 | id:1410.4744 author:Jakub Konečný, Jie Liu, Peter Richtárik, Martin Takáč category:cs.LG stat.ML  published:2014-10-17 summary:We propose a mini-batching scheme for improving the theoretical complexity and practical performance of semi-stochastic gradient descent applied to the problem of minimizing a strongly convex composite function represented as the sum of an average of a large number of smooth convex functions, and simple nonsmooth convex function. Our method first performs a deterministic step (computation of the gradient of the objective function at the starting point), followed by a large number of stochastic steps. The process is repeated a few times with the last iterate becoming the new starting point. The novelty of our method is in introduction of mini-batching into the computation of stochastic steps. In each step, instead of choosing a single function, we sample $b$ functions, compute their gradients, and compute the direction based on this. We analyze the complexity of the method and show that the method benefits from two speedup effects. First, we prove that as long as $b$ is below a certain threshold, we can reach predefined accuracy with less overall work than without mini-batching. Second, our mini-batching scheme admits a simple parallel implementation, and hence is suitable for further acceleration by parallelization. version:1
arxiv-1411-1375 | Heuristic algorithm for 1D and 2D unfolding | http://arxiv.org/abs/1411.1375 | id:1411.1375 author:Yordan Karadzhov category:physics.data-an stat.ML  published:2014-10-17 summary:A very simple heuristic approach to the unfolding problem will be described. An iterative algorithm starts with an empty histogram and every iteration aims to add one entry to this histogram. The entry to be added is selected according to a criteria which includes a $\chi^2$ test and a regularization. After a relatively small number of iterations (500 - 1000) the growing reconstructed distribution converges to the true distribution. version:1
arxiv-1403-1349 | Learning Soft Linear Constraints with Application to Citation Field Extraction | http://arxiv.org/abs/1403.1349 | id:1403.1349 author:Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum category:cs.CL cs.DL cs.IR  published:2014-03-06 summary:Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft constraints, where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance. On the other hand, for imposing hard constraints, dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference. We extend the technique to perform prediction subject to soft constraints. Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset. version:2
arxiv-1410-4673 | KCRC-LCD: Discriminative Kernel Collaborative Representation with Locality Constrained Dictionary for Visual Categorization | http://arxiv.org/abs/1410.4673 | id:1410.4673 author:Weiyang Liu, Zhiding Yu, Lijia Lu, Yandong Wen, Hui Li, Yuexian Zou category:cs.CV cs.LG  published:2014-10-17 summary:We consider the image classification problem via kernel collaborative representation classification with locality constrained dictionary (KCRC-LCD). Specifically, we propose a kernel collaborative representation classification (KCRC) approach in which kernel method is used to improve the discrimination ability of collaborative representation classification (CRC). We then measure the similarities between the query and atoms in the global dictionary in order to construct a locality constrained dictionary (LCD) for KCRC. In addition, we discuss several similarity measure approaches in LCD and further present a simple yet effective unified similarity measure whose superiority is validated in experiments. There are several appealing aspects associated with LCD. First, LCD can be nicely incorporated under the framework of KCRC. The LCD similarity measure can be kernelized under KCRC, which theoretically links CRC and LCD under the kernel method. Second, KCRC-LCD becomes more scalable to both the training set size and the feature dimension. Example shows that KCRC is able to perfectly classify data with certain distribution, while conventional CRC fails completely. Comprehensive experiments on many public datasets also show that KCRC-LCD is a robust discriminative classifier with both excellent performance and good scalability, being comparable or outperforming many other state-of-the-art approaches. version:1
arxiv-1410-4470 | MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex Optimization | http://arxiv.org/abs/1410.4470 | id:1410.4470 author:Raviteja Vemulapalli, Vinay Praneeth Boda, Rama Chellappa category:cs.CV cs.LG  published:2014-10-16 summary:In the recent past, automatic selection or combination of kernels (or features) based on multiple kernel learning (MKL) approaches has been receiving significant attention from various research communities. Though MKL has been extensively studied in the context of support vector machines (SVM), it is relatively less explored for ratio-trace problems. In this paper, we show that MKL can be formulated as a convex optimization problem for a general class of ratio-trace problems that encompasses many popular algorithms used in various computer vision applications. We also provide an optimization procedure that is guaranteed to converge to the global optimum of the proposed optimization problem. We experimentally demonstrate that the proposed MKL approach, which we refer to as MKL-RT, can be successfully used to select features for discriminative dimensionality reduction and cross-modal retrieval. We also show that the proposed convex MKL-RT approach performs better than the recently proposed non-convex MKL-DR approach. version:2
arxiv-1410-4622 | Robust Topological Feature Extraction for Mapping of Environments using Bio-Inspired Sensor Networks | http://arxiv.org/abs/1410.4622 | id:1410.4622 author:Alireza Dirafzoon, Edgar Lobaton category:cs.RO cs.SY math.AT stat.ML  published:2014-10-17 summary:In this paper, we exploit minimal sensing information gathered from biologically inspired sensor networks to perform exploration and mapping in an unknown environment. A probabilistic motion model of mobile sensing nodes, inspired by motion characteristics of cockroaches, is utilized to extract weak encounter information in order to build a topological representation of the environment. Neighbor to neighbor interactions among the nodes are exploited to build point clouds representing spatial features of the manifold characterizing the environment based on the sampled data. To extract dominant features from sampled data, topological data analysis is used to produce persistence intervals for features, to be used for topological mapping. In order to improve robustness characteristics of the sampled data with respect to outliers, density based subsampling algorithms are employed. Moreover, a robust scale-invariant classification algorithm for persistence diagrams is proposed to provide a quantitative representation of desired features in the data. Furthermore, various strategies for defining encounter metrics with different degrees of information regarding agents' motion are suggested to enhance the precision of the estimation and classification performance of the topological method. version:1
arxiv-1408-0881 | Volumes of logistic regression models with applications to model selection | http://arxiv.org/abs/1408.0881 | id:1408.0881 author:James G. Dowty category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH  published:2014-08-05 summary:Logistic regression models with $n$ observations and $q$ linearly-independent covariates are shown to have Fisher information volumes which are bounded below by $\pi^q$ and above by ${n \choose q} \pi^q$. This is proved with a novel generalization of the classical theorems of Pythagoras and de Gua, which is of independent interest. The finding that the volume is always finite is new, and it implies that the volume can be directly interpreted as a measure of model complexity. The volume is shown to be a continuous function of the design matrix $X$ at generic $X$, but to be discontinuous in general. This means that models with sparse design matrices can be significantly less complex than nearby models, so the resulting model-selection criterion prefers sparse models. This is analogous to the way that $\ell^1$-regularisation tends to prefer sparse model fits, though in our case this behaviour arises spontaneously from general principles. Lastly, an unusual topological duality is shown to exist between the ideal boundaries of the natural and expectation parameter spaces of logistic regression models. version:3
arxiv-1410-5330 | An Overview of General Performance Metrics of Binary Classifier Systems | http://arxiv.org/abs/1410.5330 | id:1410.5330 author:Sebastian Raschka category:cs.LG  published:2014-10-17 summary:This document provides a brief overview of different metrics and terminology that is used to measure the performance of binary classification systems. version:1
arxiv-1410-4604 | Domain-Independent Optimistic Initialization for Reinforcement Learning | http://arxiv.org/abs/1410.4604 | id:1410.4604 author:Marlos C. Machado, Sriram Srinivasan, Michael Bowling category:cs.LG cs.AI  published:2014-10-16 summary:In Reinforcement Learning (RL), it is common to use optimistic initialization of value functions to encourage exploration. However, such an approach generally depends on the domain, viz., the scale of the rewards must be known, and the feature representation must have a constant norm. We present a simple approach that performs optimistic initialization with less dependence on the domain. version:1
arxiv-1410-4573 | Learning a hyperplane regressor by minimizing an exact bound on the VC dimension | http://arxiv.org/abs/1410.4573 | id:1410.4573 author:Jayadeva, Suresh Chandra, Siddarth Sabharwal, Sanjit S. Batra category:cs.LG 68T05  68T10  68Q32 I.5.1; I.5.2  published:2014-10-16 summary:The capacity of a learning machine is measured by its Vapnik-Chervonenkis dimension, and learning machines with a low VC dimension generalize better. It is well known that the VC dimension of SVMs can be very large or unbounded, even though they generally yield state-of-the-art learning performance. In this paper, we show how to learn a hyperplane regressor by minimizing an exact, or \boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed as the Minimal Complexity Machine (MCM) Regressor, involves solving a simple linear programming problem. Experimental results show, that on a number of benchmark datasets, the proposed approach yields regressors with error rates much less than those obtained with conventional SVM regresssors, while often using fewer support vectors. On some benchmark datasets, the number of support vectors is less than one tenth the number used by SVMs, indicating that the MCM does indeed learn simpler representations. version:1
arxiv-1410-4521 | Reconstructive Sparse Code Transfer for Contour Detection and Semantic Labeling | http://arxiv.org/abs/1410.4521 | id:1410.4521 author:Michael Maire, Stella X. Yu, Pietro Perona category:cs.CV  published:2014-10-16 summary:We frame the task of predicting a semantic labeling as a sparse reconstruction procedure that applies a target-specific learned transfer function to a generic deep sparse code representation of an image. This strategy partitions training into two distinct stages. First, in an unsupervised manner, we learn a set of generic dictionaries optimized for sparse coding of image patches. We train a multilayer representation via recursive sparse dictionary learning on pooled codes output by earlier layers. Second, we encode all training images with the generic dictionaries and learn a transfer function that optimizes reconstruction of patches extracted from annotated ground-truth given the sparse codes of their corresponding image patches. At test time, we encode a novel image using the generic dictionaries and then reconstruct using the transfer function. The output reconstruction is a semantic labeling of the test image. Applying this strategy to the task of contour detection, we demonstrate performance competitive with state-of-the-art systems. Unlike almost all prior work, our approach obviates the need for any form of hand-designed features or filters. To illustrate general applicability, we also show initial results on semantic part labeling of human faces. The effectiveness of our approach opens new avenues for research on deep sparse representations. Our classifiers utilize this representation in a novel manner. Rather than acting on nodes in the deepest layer, they attach to nodes along a slice through multiple layers of the network in order to make predictions about local patches. Our flexible combination of a generatively learned sparse representation with discriminatively trained transfer classifiers extends the notion of sparse reconstruction to encompass arbitrary semantic labeling tasks. version:1
arxiv-1410-4441 | Improve CAPTCHA's Security Using Gaussian Blur Filter | http://arxiv.org/abs/1410.4441 | id:1410.4441 author:Ariyan Zarei category:cs.CV  published:2014-10-16 summary:Providing security for webservers against unwanted and automated registrations has become a big concern. To prevent these kinds of false registrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based or visual CAPTCHAs are very common. Actually visual CAPTCHA is an image containing a sequence of characters. So far most of visual CAPTCHAs, in order to resist against OCR programs, use some common implementations such as wrapping the characters, random placement and rotations of characters, etc. In this paper we applied Gaussian Blur filter, which is an image transformation, to visual CAPTCHAs to reduce their readability by OCR programs. We concluded that this technique made CAPTCHAs almost unreadable for OCR programs but, their readability by human users still remained high. version:1
arxiv-1403-7806 | Unbiased Black-Box Complexities of Jump Functions | http://arxiv.org/abs/1403.7806 | id:1403.7806 author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE F.2.2  published:2014-03-30 summary:We analyze the unbiased black-box complexity of jump functions with small, medium, and large sizes of the fitness plateau surrounding the optimal solution. Among other results, we show that when the jump size is $(1/2 - \varepsilon)n$, that is, only a small constant fraction of the fitness values is visible, then the unbiased black-box complexities for arities $3$ and higher are of the same order as those for the simple \textsc{OneMax} function. Even for the extreme jump function, in which all but the two fitness values $n/2$ and $n$ are blanked out, polynomial-time mutation-based (i.e., unary unbiased) black-box optimization algorithms exist. This is quite surprising given that for the extreme jump function almost the whole search space (all but a $\Theta(n^{-1/2})$ fraction) is a plateau of constant fitness. To prove these results, we introduce new tools for the analysis of unbiased black-box complexities, for example, selecting the new parent individual not by comparing the fitnesses of the competing search points, but also by taking into account the (empirical) expected fitnesses of their offspring. version:2
arxiv-1410-3864 | Multi-Agent Shape Formation and Tracking Inspired from a Social Foraging Dynamics | http://arxiv.org/abs/1410.3864 | id:1410.3864 author:Debdipta Goswami, Chiranjib Saha, Kunal Pal, Swagatam Das category:cs.NE cs.RO 70F04  published:2014-10-14 summary:Principle of Swarm Intelligence has recently found widespread application in formation control and automated tracking by the automated multi-agent system. This article proposes an elegant and effective method inspired by foraging dynamics to produce geometric-patterns by the search agents. Starting from a random initial orientation, it is investigated how the foraging dynamics can be modified to achieve convergence of the agents on the desired pattern with almost uniform density. Guided through the proposed dynamics, the agents can also track a moving point by continuously circulating around the point. An analytical treatment supported with computer simulation results is provided to better understand the convergence behaviour of the system. version:2
arxiv-1410-4341 | Implicit segmentation of Kannada characters in offline handwriting recognition using hidden Markov models | http://arxiv.org/abs/1410.4341 | id:1410.4341 author:Manasij Venkatesh, Vikas Majjagi, Deepu Vijayasenan category:cs.LG cs.CV  published:2014-10-16 summary:We describe a method for classification of handwritten Kannada characters using Hidden Markov Models (HMMs). Kannada script is agglutinative, where simple shapes are concatenated horizontally to form a character. This results in a large number of characters making the task of classification difficult. Character segmentation plays a significant role in reducing the number of classes. Explicit segmentation techniques suffer when overlapping shapes are present, which is common in the case of handwritten text. We use HMMs to take advantage of the agglutinative nature of Kannada script, which allows us to perform implicit segmentation of characters along with recognition. All the experiments are performed on the Chars74k dataset that consists of 657 handwritten characters collected across multiple users. Gradient-based features are extracted from individual characters and are used to train character HMMs. The use of implicit segmentation technique at the character level resulted in an improvement of around 10%. This system also outperformed an existing system tested on the same dataset by around 16%. Analysis based on learning curves showed that increasing the training data could result in better accuracy. Accordingly, we collected additional data and obtained an improvement of 4% with 6 additional samples. version:1
arxiv-1410-7679 | Super-resolution method using sparse regularization for point-spread function recovery | http://arxiv.org/abs/1410.7679 | id:1410.7679 author:Fred Maurice Ngolè Mboula, Jean-Luc Starck, Samuel Ronayette, Koryo Okumura, Jérôme Amiaux category:cs.CV astro-ph.IM  published:2014-10-16 summary:In large-scale spatial surveys, such as the forthcoming ESA Euclid mission, images may be undersampled due to the optical sensors sizes. Therefore, one may consider using a super-resolution (SR) method to recover aliased frequencies, prior to further analysis. This is particularly relevant for point-source images, which provide direct measurements of the instrument point-spread function (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse, which is an SR algorithm using a sparse analysis prior. We show that such a prior provides significant improvements over existing methods, especially on low SNR PSFs. version:1
arxiv-1211-3966 | Lasso Screening Rules via Dual Polytope Projection | http://arxiv.org/abs/1211.3966 | id:1211.3966 author:Jie Wang, Peter Wonka, Jieping Ye category:cs.LG stat.ML  published:2012-11-16 summary:Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have $0$ components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no "exact" screening rule for group Lasso. We have evaluated our screening rule using synthetic and real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso. version:3
arxiv-1410-4210 | Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets | http://arxiv.org/abs/1410.4210 | id:1410.4210 author:Jie Wang, Jieping Ye category:cs.LG  published:2014-10-15 summary:Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel Two-Layer Feature REduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method---called DPC (DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude. version:1
arxiv-1410-4176 | Learning Distributed Word Representations for Natural Logic Reasoning | http://arxiv.org/abs/1410.4176 | id:1410.4176 author:Samuel R. Bowman, Christopher Potts, Christopher D. Manning category:cs.CL  published:2014-10-15 summary:Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models' ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics. version:1
arxiv-1408-0872 | Open-set Person Re-identification | http://arxiv.org/abs/1408.0872 | id:1408.0872 author:Shengcai Liao, Zhipeng Mo, Jianqing Zhu, Yang Hu, Stan Z. Li category:cs.CV  published:2014-08-05 summary:Person re-identification is becoming a hot research for developing both machine learning algorithms and video surveillance applications. The task of person re-identification is to determine which person in a gallery has the same identity to a probe image. This task basically assumes that the subject of the probe image belongs to the gallery, that is, the gallery contains this person. However, in practical applications such as searching a suspect in a video, this assumption is usually not true. In this paper, we consider the open-set person re-identification problem, which includes two sub-tasks, detection and identification. The detection sub-task is to determine the presence of the probe subject in the gallery, and the identification sub-task is to determine which person in the gallery has the same identity as the accepted probe. We present a database collected from a video surveillance setting of 6 cameras, with 200 persons and 7,413 images segmented. Based on this database, we develop a benchmark protocol for evaluating the performance under the open-set person re-identification scenario. Several popular metric learning algorithms for person re-identification have been evaluated as baselines. From the baseline performance, we observe that the open-set person re-identification problem is still largely unresolved, thus further attention and effort is needed. version:2
arxiv-1304-5409 | Separating the Real from the Synthetic: Minutiae Histograms as Fingerprints of Fingerprints | http://arxiv.org/abs/1304.5409 | id:1304.5409 author:Carsten Gottschlich, Stephan Huckemann category:cs.CV cs.AI cs.DB  published:2013-04-19 summary:In this study we show that by the current state-of-the-art synthetically generated fingerprints can easily be discriminated from real fingerprints. We propose a method based on second order extended minutiae histograms (MHs) which can distinguish between real and synthetic prints with very high accuracy. MHs provide a fixed-length feature vector for a fingerprint which are invariant under rotation and translation. This 'test of realness' can be applied to synthetic fingerprints produced by any method. In this work, tests are conducted on the 12 publicly available databases of FVC2000, FVC2002 and FVC2004 which are well established benchmarks for evaluating the performance of fingerprint recognition algorithms; 3 of these 12 databases consist of artificial fingerprints generated by the SFinGe software. Additionally, we evaluate the discriminative performance on a database of synthetic fingerprints generated by the software of Bicz versus real fingerprint images. We conclude with suggestions for the improvement of synthetic fingerprint generation. version:3
arxiv-1410-4062 | Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms for Machine Learning | http://arxiv.org/abs/1410.4062 | id:1410.4062 author:Emanuele Frandi, Ricardo Nanculef, Johan Suykens category:stat.ML cs.LG cs.NA math.OC  published:2014-10-15 summary:Frank-Wolfe algorithms for convex minimization have recently gained considerable attention from the Optimization and Machine Learning communities, as their properties make them a suitable choice in a variety of applications. However, as each iteration requires to optimize a linear model, a clever implementation is crucial to make such algorithms viable on large-scale datasets. For this purpose, approximation strategies based on a random sampling have been proposed by several researchers. In this work, we perform an experimental study on the effectiveness of these techniques, analyze possible alternatives and provide some guidelines based on our results. version:1
arxiv-1410-4017 | Online Tracking of Skin Colour Regions Against a Complex Background | http://arxiv.org/abs/1410.4017 | id:1410.4017 author:Subhadip Basu, S. Chakraborty, K. Mukherjee, S. K. Pandit category:cs.CV  published:2014-10-15 summary:Online tracking of human activity against a complex background is a challenging task for many applications. In this paper, we have developed a robust technique for localizing skin colour regions from unconstrained image frames. A simple and fast segmentation algorithm is used to train a multiplayer perceptron (MLP) for detection of skin colours. Stepper motors are synchronized with the MLP to track the movement of the skin colour regions. version:1
arxiv-1410-4013 | A two-pass fuzzy-geno approach to pattern classification | http://arxiv.org/abs/1410.4013 | id:1410.4013 author:Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2014-10-15 summary:The work presents an extension of the fuzzy approach to 2-D shape recognition [1] through refinement of initial or coarse classification decisions under a two pass approach. In this approach, an unknown pattern is classified by refining possible classification decisions obtained through coarse classification of the same. To build a fuzzy model of a pattern class horizontal and vertical fuzzy partitions on the sample images of the class are optimized using genetic algorithm. To make coarse classification decisions about an unknown pattern, the fuzzy representation of the pattern is compared with models of all pattern classes through a specially designed similarity measure. Coarse classification decisions are refined in the second pass to obtain the final classification decision of the unknown pattern. To do so, optimized horizontal and vertical fuzzy partitions are again created on certain regions of the image frame, specific to each group of similar type of pattern classes. It is observed through experiments that the technique improves the overall recognition rate from 86.2%, in the first pass, to 90.4% after the second pass, with 500 training samples of handwritten digits. version:1
arxiv-1410-4012 | Online interpretation of numeric sign language using 2-d skeletal model | http://arxiv.org/abs/1410.4012 | id:1410.4012 author:Subhadip Basu, S. Dey, K. Mukherjee, T. S. Jana category:cs.CV  published:2014-10-15 summary:Gesturing is one of the natural modes of human communication. Signs produced by gestures can have a basic meaning coupled with additional information that is layered over the basic meaning of the sign. Sign language is an important example of communicative gestures that are highly structured and well accepted across the world as a communication medium for deaf and dumb. In this paper, an online recognition scheme is proposed to interpret the standard numeric sign language comprising of 10 basic hand symbols. A web camera is used to capture the real time hand movements as input to the system. The basic meaning of the hand gesture is extracted from the input data frame by analysing the shape of the hand, considering its orientation, movement and location to be fixed. The input hand shape is processed to identify the palm structure, fingertips and their relative positions and the presence of the extended thumb. A 2-dimensional skeletal model is generated from the acquired shape information to represent and subsequently interpret the basic meaning of the hand gesture. version:1
arxiv-1410-4009 | Thompson sampling with the online bootstrap | http://arxiv.org/abs/1410.4009 | id:1410.4009 author:Dean Eckles, Maurits Kaptein category:cs.LG stat.CO stat.ML 68W27  62L05 G.3; I.2.6  published:2014-10-15 summary:Thompson sampling provides a solution to bandit problems in which new observations are allocated to arms with the posterior probability that an arm is optimal. While sometimes easy to implement and asymptotically optimal, Thompson sampling can be computationally demanding in large scale bandit problems, and its performance is dependent on the model fit to the observed data. We introduce bootstrap Thompson sampling (BTS), a heuristic method for solving bandit problems which modifies Thompson sampling by replacing the posterior distribution used in Thompson sampling by a bootstrap distribution. We first explain BTS and show that the performance of BTS is competitive to Thompson sampling in the well-studied Bernoulli bandit case. Subsequently, we detail why BTS using the online bootstrap is more scalable than regular Thompson sampling, and we show through simulation that BTS is more robust to a misspecified error distribution. BTS is an appealing modification of Thompson sampling, especially when samples from the posterior are otherwise not available or are costly. version:1
arxiv-1410-3970 | Shape and Color Object Tracking for Real-Time Robotic Navigation | http://arxiv.org/abs/1410.3970 | id:1410.3970 author:Haythem Ghazouani category:cs.CV  published:2014-10-15 summary:This paper presents a real-time approach for single-colored ball detection and tracking. The approach consists of two main phases. In a first offline calibration phase, the intrinsic parameters of the camera and the radial distortion are estimated, and a classification of colors is learned from a sample image of colored balls. The second phase consists of four main steps: (1) color segmentation of the input image into several regions based on the offline classification, (2) robust estimation of the circle parameters (3) refinement of the circle parameters, and (4) ball tracking. The experimental results showed that the approach presents a good compromise between suitability for real-time navigation and robustness to occlusions, background congestion and colors interference in the scene. version:1
arxiv-1402-2058 | Probabilistic Interpretation of Linear Solvers | http://arxiv.org/abs/1402.2058 | id:1402.2058 author:Philipp Hennig category:math.OC cs.LG cs.NA math.NA math.PR stat.ML 90C53  65F10  published:2014-02-10 summary:This manuscript proposes a probabilistic framework for algorithms that iteratively solve unconstrained linear problems $Bx = b$ with positive definite $B$ for $x$. The goal is to replace the point estimates returned by existing methods with a Gaussian posterior belief over the elements of the inverse of $B$, which can be used to estimate errors. Recent probabilistic interpretations of the secant family of quasi-Newton optimization algorithms are extended. Combined with properties of the conjugate gradient algorithm, this leads to uncertainty-calibrated methods with very limited cost overhead over conjugate gradients, a self-contained novel interpretation of the quasi-Newton and conjugate gradient algorithms, and a foundation for new nonlinear optimization methods. version:2
arxiv-1410-3935 | A Logic-based Approach to Generatively Defined Discriminative Modeling | http://arxiv.org/abs/1410.3935 | id:1410.3935 author:Taisuke Sato, Keiichi Kubota, Yoshitaka Kameya category:cs.LG  published:2014-10-15 summary:Conditional random fields (CRFs) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively. Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs in machine learning to compare generative and discriminative models and choose the best model. We implemented our approach as the D-PRISM language by modifying PRISM, a logic-based probabilistic modeling language for generative modeling, while exploiting its dynamic programming mechanism for efficient probability computation. We tested D-PRISM with logistic regression, a linear-chain CRF and a CRF-CFG and empirically confirmed their excellent discriminative performance compared to their generative counterparts, i.e.\ naive Bayes, an HMM and a PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF versions of Bayesian network classifiers and probabilistic left-corner grammars respectively and easily implementable in D-PRISM. We empirically showed that they outperform their generative counterparts as expected. version:1
arxiv-1410-3932 | Detection of Salient Regions in Crowded Scenes | http://arxiv.org/abs/1410.3932 | id:1410.3932 author:Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino category:cs.CV  published:2014-10-15 summary:The increasing number of cameras and a handful of human operators to monitor the video inputs from hundreds of cameras leave the system ill equipped to fulfil the task of detecting anomalies. Thus, there is a dire need to automatically detect regions that require immediate attention for a more effective and proactive surveillance. We propose a framework that utilises the temporal variations in the flow field of a crowd scene to automatically detect salient regions, while eliminating the need to have prior knowledge of the scene or training. We deem the flow fields to be a dynamic system and adopt the stability theory of dynamical systems, to determine the motion dynamics within a given area. In the context of this work, salient regions refer to areas with high motion dynamics, where points in a particular region are unstable. Experimental results on public, crowd scenes have shown the effectiveness of the proposed method in detecting salient regions which correspond to unstable flow, occlusions, bottlenecks, entries and exits. version:1
arxiv-1410-3915 | Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective | http://arxiv.org/abs/1410.3915 | id:1410.3915 author:Neil Shah, Alex Beutel, Brian Gallagher, Christos Faloutsos category:cs.LG cs.IR cs.SI  published:2014-10-15 summary:How can we detect suspicious users in large online networks? Online popularity of a user or product (via follows, page-likes, etc.) can be monetized on the premise of higher ad click-through rates or increased sales. Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck. Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent (but sometimes honest) users. However, small-scale, stealthy attacks may go unnoticed due to the nature of low-rank eigenanalysis used in practice. In this work, we take an adversarial approach to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods and propose fBox, an algorithm designed to catch small-scale, stealth attacks that slip below the radar. Our algorithm has the following desirable properties: (a) it has theoretical underpinnings, (b) it is shown to be highly effective on real data and (c) it is scalable (linear on the input size). We evaluate fBox on a large, public 41.7 million node, 1.5 billion edge who-follows-whom social graph from Twitter in 2010 and with high precision identify many suspicious accounts which have persisted without suspension even to this day. version:1
arxiv-1410-3910 | High Order Structure Descriptors for Scene Images | http://arxiv.org/abs/1410.3910 | id:1410.3910 author:Wenya Zhu, Xiankai Lu, Tao Xu, Ziyi Zhao category:cs.CV  published:2014-10-15 summary:Structure information is ubiquitous in natural scene images and it plays an important role in scene representation. In this paper, third order structure statistics (TOSS) and fourth order structure statistics (FOSS) are exploited to encode higher order structure information. Afterwards, based on the radial and normal slice of TOSS and FOSS, we propose the high order structure feature: third order structure feature (TOSF) and fourth order structure feature (FOSF). It is well known that scene images are well characterized by particular arrangements of their local structures, we divide the scene image into the non-overlapping sub-regions and compute the proposed higher order structural features among them. Then a scene classification is performed by using SVM classifier with these higher order structure features. The experimental results show that higher order structure statistics can deliver image structure information well and its spatial envelope has strong discriminative ability. version:1
arxiv-1410-3905 | Efficient Image Categorization with Sparse Fisher Vector | http://arxiv.org/abs/1410.3905 | id:1410.3905 author:Xiankai Lu, Zheng Fang, Tao Xu, Haiting Zhang, Hongya Tuo category:cs.CV  published:2014-10-15 summary:In object recognition, Fisher vector (FV) representation is one of the state-of-art image representations ways at the expense of dense, high dimensional features and increased computation time. A simplification of FV is attractive, so we propose Sparse Fisher vector (SFV). By incorporating locality strategy, we can accelerate the Fisher coding step in image categorization which is implemented from a collective of local descriptors. Combining with pooling step, we explore the relationship between coding step and pooling step to give a theoretical explanation about SFV. Experiments on benchmark datasets have shown that SFV leads to a speedup of several-fold of magnitude compares with FV, while maintaining the categorization performance. In addition, we demonstrate how SFV preserves the consistence in representation of similar local features. version:1
arxiv-1410-3886 | Tighter Low-rank Approximation via Sampling the Leveraged Element | http://arxiv.org/abs/1410.3886 | id:1410.3886 author:Srinadh Bhojanapalli, Prateek Jain, Sujay Sanghavi category:cs.DS cs.LG stat.ML  published:2014-10-14 summary:In this work, we propose a new randomized algorithm for computing a low-rank approximation to a given matrix. Taking an approach different from existing literature, our method first involves a specific biased sampling, with an element being chosen based on the leverage scores of its row and column, and then involves weighted alternating minimization over the factored form of the intended low-rank matrix, to minimize error only on these samples. Our method can leverage input sparsity, yet produce approximations in {\em spectral} (as opposed to the weaker Frobenius) norm; this combines the best aspects of otherwise disparate current results, but with a dependence on the condition number $\kappa = \sigma_1/\sigma_r$. In particular we require $O(nnz(M) + \frac{n\kappa^2 r^5}{\epsilon^2})$ computations to generate a rank-$r$ approximation to $M$ in spectral norm. In contrast, the best existing method requires $O(nnz(M)+ \frac{nr^2}{\epsilon^4})$ time to compute an approximation in Frobenius norm. Besides the tightness in spectral norm, we have a better dependence on the error $\epsilon$. Our method is naturally and highly parallelizable. Our new approach enables two extensions that are interesting on their own. The first is a new method to directly compute a low-rank approximation (in efficient factored form) to the product of two given matrices; it computes a small random set of entries of the product, and then executes weighted alternating minimization (as before) on these. The sampling strategy is different because now we cannot access leverage scores of the product matrix (but instead have to work with input matrices). The second extension is an improved algorithm with smaller communication complexity for the distributed PCA setting (where each server has small set of rows of the matrix, and want to compute low rank approximation with small amount of communication with other servers). version:1
arxiv-1406-2237 | Reducing the Effects of Detrimental Instances | http://arxiv.org/abs/1406.2237 | id:1406.2237 author:Michael R. Smith, Tony Martinez category:stat.ML cs.LG  published:2014-06-09 summary:Not all instances in a data set are equally beneficial for inducing a model of the data. Some instances (such as outliers or noise) can be detrimental. However, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. Many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. In this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. We call our method of identifying and weighting detrimental instances reduced detrimental instance learning (RDIL). We examine RIDL on a set of 54 data sets and 5 learning algorithms and compare RIDL with other weighting and filtering approaches. RDIL is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (MLPs). Our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them. version:2
arxiv-1403-5403 | A Non-Local Structure Tensor Based Approach for Multicomponent Image Recovery Problems | http://arxiv.org/abs/1403.5403 | id:1403.5403 author:Giovanni Chierchia, Nelly Pustelnik, Beatrice Pesquet-Popescu, Jean-Christophe Pesquet category:cs.CV cs.NA math.OC  published:2014-03-21 summary:Non-Local Total Variation (NLTV) has emerged as a useful tool in variational methods for image recovery problems. In this paper, we extend the NLTV-based regularization to multicomponent images by taking advantage of the Structure Tensor (ST) resulting from the gradient of a multicomponent image. The proposed approach allows us to penalize the non-local variations, jointly for the different components, through various $\ell_{1,p}$ matrix norms with $p \ge 1$. To facilitate the choice of the hyper-parameters, we adopt a constrained convex optimization approach in which we minimize the data fidelity term subject to a constraint involving the ST-NLTV regularization. The resulting convex optimization problem is solved with a novel epigraphical projection method. This formulation can be efficiently implemented thanks to the flexibility offered by recent primal-dual proximal algorithms. Experiments are carried out for multispectral and hyperspectral images. The results demonstrate the interest of introducing a non-local structure tensor regularization and show that the proposed approach leads to significant improvements in terms of convergence speed over current state-of-the-art methods. version:2
arxiv-1410-3831 | An exact mapping between the Variational Renormalization Group and Deep Learning | http://arxiv.org/abs/1410.3831 | id:1410.3831 author:Pankaj Mehta, David J. Schwab category:stat.ML cond-mat.stat-mech cs.LG cs.NE  published:2014-10-14 summary:Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data. version:1
arxiv-1410-3791 | POLYGLOT-NER: Massive Multilingual Named Entity Recognition | http://arxiv.org/abs/1410.3791 | id:1410.3791 author:Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL cs.LG I.2.7; I.2.6  published:2014-10-14 summary:The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance. Our method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise. Our evaluation is two fold: First, we demonstrate the system performance on human annotated datasets. Second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation. version:1
arxiv-1410-3756 | Crowd Saliency Detection via Global Similarity Structure | http://arxiv.org/abs/1410.3756 | id:1410.3756 author:Mei Kuan Lim, Ven Jyn Kok, Chen Change Loy, Chee Seng Chan category:cs.CV stat.ML  published:2014-10-14 summary:It is common for CCTV operators to overlook inter- esting events taking place within the crowd due to large number of people in the crowded scene (i.e. marathon, rally). Thus, there is a dire need to automate the detection of salient crowd regions acquiring immediate attention for a more effective and proactive surveillance. This paper proposes a novel framework to identify and localize salient regions in a crowd scene, by transforming low-level features extracted from crowd motion field into a global similarity structure. The global similarity structure representation allows the discovery of the intrinsic manifold of the motion dynamics, which could not be captured by the low-level representation. Ranking is then performed on the global similarity structure to identify a set of extrema. The proposed approach is unsupervised so learning stage is eliminated. Experimental results on public datasets demonstrates the effectiveness of exploiting such extrema in identifying salient regions in various crowd scenarios that exhibit crowding, local irregular motion, and unique motion areas such as sources and sinks. version:1
arxiv-1410-3752 | Enhanced Random Forest with Image/Patch-Level Learning for Image Understanding | http://arxiv.org/abs/1410.3752 | id:1410.3752 author:Wai Lam Hoo, Tae-Kyun Kim, Yuru Pei, Chee Seng Chan category:cs.CV stat.ML  published:2014-10-14 summary:Image understanding is an important research domain in the computer vision due to its wide real-world applications. For an image understanding framework that uses the Bag-of-Words model representation, the visual codebook is an essential part. Random forest (RF) as a tree-structure discriminative codebook has been a popular choice. However, the performance of the RF can be degraded if the local patch labels are poorly assigned. In this paper, we tackle this problem by a novel way to update the RF codebook learning for a more discriminative codebook with the introduction of the soft class labels, estimated from the pLSA model based on a feedback scheme. The feedback scheme is performed on both the image and patch levels respectively, which is in contrast to the state- of-the-art RF codebook learning that focused on either image or patch level only. Experiments on 15-Scene and C-Pascal datasets had shown the effectiveness of the proposed method in image understanding task. version:1
arxiv-1410-3751 | A Fusion Approach for Efficient Human Skin Detection | http://arxiv.org/abs/1410.3751 | id:1410.3751 author:Wei Ren Tan, Chee Seng Chan, Pratheepan Yogarajah, Joan Condell category:cs.CV stat.ML  published:2014-10-14 summary:A reliable human skin detection method that is adaptable to different human skin colours and illu- mination conditions is essential for better human skin segmentation. Even though different human skin colour detection solutions have been successfully applied, they are prone to false skin detection and are not able to cope with the variety of human skin colours across different ethnic. Moreover, existing methods require high computational cost. In this paper, we propose a novel human skin de- tection approach that combines a smoothed 2D histogram and Gaussian model, for automatic human skin detection in colour image(s). In our approach an eye detector is used to refine the skin model for a specific person. The proposed approach reduces computational costs as no training is required; and it improves the accuracy of skin detection despite wide variation in ethnicity and illumination. To the best of our knowledge, this is the first method to employ fusion strategy for this purpose. Qualitative and quantitative results on three standard public datasets and a comparison with state-of-the-art methods have shown the effectiveness and robustness of the proposed approach. version:1
arxiv-1410-3748 | Zero-Shot Object Recognition System based on Topic Model | http://arxiv.org/abs/1410.3748 | id:1410.3748 author:Wai Lam Hoo, Chee Seng Chan category:cs.CV stat.ML  published:2014-10-14 summary:Object recognition systems usually require fully complete manually labeled training data to train the classifier. In this paper, we study the problem of object recognition where the training samples are missing during the classifier learning stage, a task also known as zero-shot learning. We propose a novel zero-shot learning strategy that utilizes the topic model and hierarchical class concept. Our proposed method advanced where cumbersome human annotation stage (i.e. attribute-based classification) is eliminated. We achieve comparable performance with state-of-the-art algorithms in four public datasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), and Animals with Attributes (49.65%) when unseen classes exist in the classification task. version:1
arxiv-1410-3744 | Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking | http://arxiv.org/abs/1410.3744 | id:1410.3744 author:Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino category:cs.CV cs.NE  published:2014-10-14 summary:Conventional tracking solutions are not feasible in handling abrupt motion as they are based on smooth motion assumption or an accurate motion model. Abrupt motion is not subject to motion continuity and smoothness. To assuage this, we deem tracking as an optimisation problem and propose a novel abrupt motion tracker that based on swarm intelligence - the SwaTrack. Unlike existing swarm-based filtering methods, we first of all introduce an optimised swarm-based sampling strategy to tradeoff between the exploration and exploitation of the search space in search for the optimal proposal distribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allow on the fly tuning of the best mean and variance of the distribution for sampling. Such innovating idea of combining these strategies in an ingenious way in the PSO framework to handle the abrupt motion, which so far no existing works are found. Experimental results in both quantitative and qualitative had shown the effectiveness of the proposed method in tracking abrupt motions. version:1
arxiv-1410-3726 | Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding | http://arxiv.org/abs/1410.3726 | id:1410.3726 author:Chern Hong Lim, Anhar Risnumawan, Chee Seng Chan category:cs.CV cs.AI cs.IR  published:2014-10-14 summary:Ambiguity or uncertainty is a pervasive element of many real world decision making processes. Variation in decisions is a norm in this situation when the same problem is posed to different subjects. Psychological and metaphysical research had proven that decision making by human is subjective. It is influenced by many factors such as experience, age, background, etc. Scene understanding is one of the computer vision problems that fall into this category. Conventional methods relax this problem by assuming scene images are mutually exclusive; and therefore, focus on developing different approaches to perform the binary classification tasks. In this paper, we show that scene images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC provides a ranking interpretation instead of binary decision. Evaluations in term of qualitative and quantitative using large numbers and challenging public scene datasets have shown the effectiveness of our proposed method in modeling the non-mutually exclusive scene images. version:1
arxiv-1410-3699 | A graph Laplacian regularization for hyperspectral data unmixing | http://arxiv.org/abs/1410.3699 | id:1410.3699 author:Rita Ammanouil, André Ferrari, Cédric Richard category:cs.CV  published:2014-10-14 summary:This paper introduces a graph Laplacian regularization in the hyperspectral unmixing formulation. The proposed regularization relies upon the construction of a graph representation of the hyperspectral image. Each node in the graph represents a pixel's spectrum, and edges connect spectrally and spatially similar pixels. The proposed graph framework promotes smoothness in the estimated abundance maps and collaborative estimation between homogeneous areas of the image. The resulting convex optimization problem is solved using the Alternating Direction Method of Multipliers (ADMM). A special attention is given to the computational complexity of the algorithm, and Graph-cut methods are proposed in order to reduce the computational burden. Finally, simulations conducted on synthetic data illustrate the effectiveness of the graph Laplacian regularization with respect to other classical regularizations for hyperspectral unmixing. version:1
arxiv-1309-4306 | Sparsity Based Poisson Denoising with Dictionary Learning | http://arxiv.org/abs/1309.4306 | id:1309.4306 author:Raja Giryes, Michael Elad category:cs.CV stat.ML  published:2013-09-17 summary:The problem of Poisson denoising appears in various imaging applications, such as low-light photography, medical imaging and microscopy. In cases of high SNR, several transformations exist so as to convert the Poisson noise into an additive i.i.d. Gaussian noise, for which many effective algorithms are available. However, in a low SNR regime, these transformations are significantly less accurate, and a strategy that relies directly on the true noise statistics is required. A recent work by Salmon et al. took this route, proposing a patch-based exponential image representation model based on GMM (Gaussian mixture model), leading to state-of-the-art results. In this paper, we propose to harness sparse-representation modeling to the image patches, adopting the same exponential idea. Our scheme uses a greedy pursuit with boot-strapping based stopping condition and dictionary learning within the denoising process. The reconstruction performance of the proposed scheme is competitive with leading methods in high SNR, and achieving state-of-the-art results in cases of low SNR. version:3
arxiv-1410-3595 | A stochastic behavior analysis of stochastic restricted-gradient descent algorithm in reproducing kernel Hilbert spaces | http://arxiv.org/abs/1410.3595 | id:1410.3595 author:Masa-aki Takizawa, Masahiro Yukawa, Cedric Richard category:cs.LG stat.ML  published:2014-10-14 summary:This paper presents a stochastic behavior analysis of a kernel-based stochastic restricted-gradient descent method. The restricted gradient gives a steepest ascent direction within the so-called dictionary subspace. The analysis provides the transient and steady state performance in the mean squared error criterion. It also includes stability conditions in the mean and mean-square sense. The present study is based on the analysis of the kernel normalized least mean square (KNLMS) algorithm initially proposed by Chen et al. Simulation results validate the analysis. version:1
arxiv-1410-2959 | Direct Processing of Document Images in Compressed Domain | http://arxiv.org/abs/1410.2959 | id:1410.2959 author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV  published:2014-10-11 summary:With the rapid increase in the volume of Big data of this digital era, fax documents, invoices, receipts, etc are traditionally subjected to compression for the efficiency of data storage and transfer. However, in order to process these documents, they need to undergo the stage of decompression which indents additional computing resources. This limitation induces the motivation to research on the possibility of directly processing of compressed images. In this research paper, we summarize the research work carried out to perform different operations straight from run-length compressed documents without going through the stage of decompression. The different operations demonstrated are feature extraction; text-line, word and character segmentation; document block segmentation; and font size detection, all carried out in the compressed version of the document. Feature extraction methods demonstrate how to extract the conventionally defined features such as projection profile, run-histogram and entropy, directly from the compressed document data. Document segmentation involves the extraction of compressed segments of text-lines, words and characters using the vertical and horizontal projection profile features. Further an attempt is made to segment randomly a block of interest from the compressed document and subsequently facilitate absolute and relative characterization of the segmented block which finds real time applications in automatic processing of Bank Cheques, Challans, etc, in compressed domain. Finally an application to detect font size at text line level is also investigated. All the proposed algorithms are validated experimentally with sufficient data set of compressed documents. version:2
arxiv-1402-0555 | Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits | http://arxiv.org/abs/1402.0555 | id:1402.0555 author:Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, Robert E. Schapire category:cs.LG stat.ML  published:2014-02-04 summary:We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of $K$ actions in response to the observed context, and observes the reward only for that chosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only $\tilde{O}(\sqrt{KT/\log N})$ oracle calls across all $T$ rounds, where $N$ is the number of policies in the policy class we compete against. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines. version:2
arxiv-1410-3386 | Testing Poisson Binomial Distributions | http://arxiv.org/abs/1410.3386 | id:1410.3386 author:Jayadev Acharya, Constantinos Daskalakis category:cs.DS cs.IT cs.LG math.IT  published:2014-10-13 summary:A Poisson Binomial distribution over $n$ variables is the distribution of the sum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm for testing whether a distribution $P$ supported on $\{0,...,n\}$ to which we have sample access is a Poisson Binomial distribution, or far from all Poisson Binomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$ to which we provide a matching lower bound. We note that our sample complexity improves quadratically upon that of the naive "learn followed by tolerant-test" approach, while instance optimal identity testing [VV14] is not applicable since we are looking to simultaneously test against a whole family of distributions. version:2
arxiv-1311-6063 | A Short Introduction to NILE | http://arxiv.org/abs/1311.6063 | id:1311.6063 author:Sheng Yu, Tianxi Cai category:cs.CL  published:2013-11-23 summary:In this paper, we briefly introduce the Narrative Information Linear Extraction (NILE) system, a natural language processing library for clinical narratives. NILE is an experiment of our ideas on efficient and effective medical language processing. We introduce the overall design of NILE and its major components, and show the performance of it in real projects. version:4
arxiv-1410-3469 | Enhanced Higgs to $τ^+τ^-$ Searches with Deep Learning | http://arxiv.org/abs/1410.3469 | id:1410.3469 author:Pierre Baldi, Peter Sadowski, Daniel Whiteson category:hep-ph cs.LG hep-ex  published:2014-10-13 summary:The Higgs boson is thought to provide the interaction that imparts mass to the fundamental fermions, but while measurements at the Large Hadron Collider (LHC) are consistent with this hypothesis, current analysis techniques lack the statistical power to cross the traditional 5$\sigma$ significance barrier without more data. \emph{Deep learning} techniques have the potential to increase the statistical power of this analysis by \emph{automatically} learning complex, high-level data representations. In this work, deep neural networks are used to detect the decay of the Higgs to a pair of tau leptons. A Bayesian optimization algorithm is used to tune the network architecture and training algorithm hyperparameters, resulting in a deep network of eight non-linear processing layers that improves upon the performance of shallow classifiers even without the use of features specifically engineered by physicists for this application. The improvement in discovery significance is equivalent to an increase in the accumulated dataset of 25\%. version:1
arxiv-1410-3426 | Computing Topology Preservation of RBF Transformations for Landmark-Based Image Registration | http://arxiv.org/abs/1410.3426 | id:1410.3426 author:R. Cavoretto, A. De Rossi, H. Qiao, B. Quatember, W. Recheis, M. Mayr category:math.NA cs.CV  published:2014-10-13 summary:In image registration, a proper transformation should be topology preserving. Especially for landmark-based image registration, if the displacement of one landmark is larger enough than those of neighbourhood landmarks, topology violation will be occurred. This paper aim to analyse the topology preservation of some Radial Basis Functions (RBFs) which are used to model deformations in image registration. Mat\'{e}rn functions are quite common in the statistic literature (see, e.g. \cite{Matern86,Stein99}). In this paper, we use them to solve the landmark-based image registration problem. We present the topology preservation properties of RBFs in one landmark and four landmarks model respectively. Numerical results of three kinds of Mat\'{e}rn transformations are compared with results of Gaussian, Wendland's, and Wu's functions. version:1
arxiv-1408-2584 | Homotopy equivalence of finite digital images | http://arxiv.org/abs/1408.2584 | id:1408.2584 author:Jason Haarmann, Meg P. Murphy, Casey S. Peters, P. Christopher Staecker category:math.GN cs.CG cs.CV 55P10  68R10 I.4.m  published:2014-08-11 summary:For digital images, there is an established homotopy equivalence relation which parallels that of classical topology. Many classical homotopy equivalence invariants, such as the Euler characteristic and the homology groups, do not remain invariants in the digital setting. This paper develops a numerical digital homotopy invariant and begins to catalog all possible connected digital images on a small number of points, up to homotopy equivalence. version:2
arxiv-1410-3348 | Fast Multilevel Support Vector Machines | http://arxiv.org/abs/1410.3348 | id:1410.3348 author:Talayeh Razzaghi, Ilya Safro category:stat.ML cs.LG  published:2014-10-13 summary:Solving different types of optimization models (including parameters fitting) for support vector machines on large-scale training data is often an expensive computational task. This paper proposes a multilevel algorithmic framework that scales efficiently to very large data sets. Instead of solving the whole training set in one optimization process, the support vectors are obtained and gradually refined at multiple levels of coarseness of the data. The proposed framework includes: (a) construction of hierarchy of large-scale data coarse representations, and (b) a local processing of updating the hyperplane throughout this hierarchy. Our multilevel framework substantially improves the computational time without loosing the quality of classifiers. The algorithms are demonstrated for both regular and weighted support vector machines. Experimental results are presented for balanced and imbalanced classification problems. Quality improvement on several imbalanced data sets has been observed. version:1
arxiv-1410-3463 | Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson | http://arxiv.org/abs/1410.3463 | id:1410.3463 author:Lavanya Sita Tekumalla, Chiranjib Bhattacharyya category:cs.OS cs.LG cs.SY  published:2014-10-13 summary:Existing caching strategies, in the storage domain, though well suited to exploit short range spatio-temporal patterns, are unable to leverage long-range motifs for improving hitrates. Motivated by this, we investigate novel Bayesian non-parametric modeling(BNP) techniques for count vectors, to capture long range correlations for cache preloading, by mining Block I/O traces. Such traces comprise of a sequence of memory accesses that can be aggregated into high-dimensional sparse correlated count vector sequences. While there are several state of the art BNP algorithms for clustering and their temporal extensions for prediction, there has been no work on exploring these for correlated count vectors. Our first contribution addresses this gap by proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its temporal extension(HMM-DP-MMVP) that captures the full covariance structure of multivariate count data. However, modeling full covariance structure for count vectors is computationally expensive, particularly for high dimensional data. Hence, we exploit sparsity in our count vectors, and as our main contribution, introduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP), generalizing our DP-MMVP mixture model, also leading to more efficient inference. We then discuss a temporal extension to our model for cache preloading. We take the first step towards mining historical data, to capture long range patterns in storage traces for cache preloading. Experimentally, we show a dramatic improvement in hitrates on benchmark traces and lay the groundwork for further research in storage domain to reduce latencies using data mining techniques to capture long range motifs. version:1
arxiv-1409-5743 | Neural Hypernetwork Approach for Pulmonary Embolism diagnosis | http://arxiv.org/abs/1409.5743 | id:1409.5743 author:Matteo Rucco, David M. S. Rodrigues, Emanuela Merelli, Jeffrey H. Johnson, Lorenzo Falsetti, Cinzia Nitti, Aldo Salvi category:physics.med-ph cs.LG physics.data-an q-bio.QM stat.ML  published:2014-09-19 summary:This work introduces an integrative approach based on Q-analysis with machine learning. The new approach, called Neural Hypernetwork, has been applied to a case study of pulmonary embolism diagnosis. The objective of the application of neural hyper-network to pulmonary embolism (PE) is to improve diagnose for reducing the number of CT-angiography needed. Hypernetworks, based on topological simplicial complex, generalize the concept of two-relation to many-body relation. Furthermore, Hypernetworks provide a significant generalization of network theory, enabling the integration of relational structure, logic and analytic dynamics. Another important results is that Q-analysis stays close to the data, while other approaches manipulate data, projecting them into metric spaces or applying some filtering functions to highlight the intrinsic relations. A pulmonary embolism (PE) is a blockage of the main artery of the lung or one of its branches, frequently fatal. Our study uses data on 28 diagnostic features of 1,427 people considered to be at risk of PE. The resulting neural hypernetwork correctly recognized 94% of those developing a PE. This is better than previous results that have been obtained with other methods (statistical selection of features, partial least squares regression, topological data analysis in a metric space). version:2
arxiv-1410-3314 | Propagation Kernels | http://arxiv.org/abs/1410.3314 | id:1410.3314 author:Marion Neumann, Roman Garnett, Christian Bauckhage, Kristian Kersting category:stat.ML cs.LG  published:2014-10-13 summary:We introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. Propagation kernels are based on monitoring how information spreads through a set of given graphs. They leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information. This has two benefits. First, off-the-shelf propagation schemes can be used to naturally construct kernels for many graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs. Second, by leveraging existing efficient and informative propagation schemes, propagation kernels can be considerably faster than state-of-the-art approaches without sacrificing predictive performance. We will also show that if the graphs at hand have a regular structure, for instance when modeling image or video data, one can exploit this regularity to scale the kernel computation to large databases of graphs with thousands of nodes. We support our contributions by exhaustive experiments on a number of real-world graphs from a variety of application domains. version:1
arxiv-1410-3462 | Tag Relevance Fusion for Social Image Retrieval | http://arxiv.org/abs/1410.3462 | id:1410.3462 author:Xirong Li category:cs.IR cs.CV H.3.3  published:2014-10-13 summary:Due to the subjective nature of social tagging, measuring the relevance of social tags with respect to the visual content is crucial for retrieving the increasing amounts of social-networked images. Witnessing the limit of a single measurement of tag relevance, we introduce in this paper tag relevance fusion as an extension to methods for tag relevance estimation. We present a systematic study, covering tag relevance fusion in early and late stages, and in supervised and unsupervised settings. Experiments on a large present-day benchmark set show that tag relevance fusion leads to better image retrieval. Moreover, unsupervised tag relevance fusion is found to be practically as effective as supervised tag relevance fusion, but without the need of any training efforts. This finding suggests the potential of tag relevance fusion for real-world deployment. version:1
arxiv-1410-3460 | Sentiment Analysis based on User Tag for Traditional Chinese Medicine in Weibo | http://arxiv.org/abs/1410.3460 | id:1410.3460 author:Junhui Shen, Peiyan Zhu, Rui Fan, Wei Tan category:cs.CL cs.SI  published:2014-10-13 summary:With the acceptance of Western culture and science, Traditional Chinese Medicine (TCM) has become a controversial issue in China. So, it's important to study the public's sentiment and opinion on TCM. The rapid development of online social network, such as twitter, make it convenient and efficient to sample hundreds of millions of people for the aforementioned sentiment study. To the best of our knowledge, the present work is the first attempt that applies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like microblogging service in China). In our work, firstly we collect tweets topic about TCM from Sina Weibo, and label the tweets as supporting TCM and opposing TCM automatically based on user tag. Then, a support vector machine classifier has been built to predict the sentiment of TCM tweets without labels. Finally, we present a method to adjust the classifier result. The performance of F-measure attained with our method is 97%. version:1
arxiv-1410-3234 | Markov Random Fields and Mass Spectra Discrimination | http://arxiv.org/abs/1410.3234 | id:1410.3234 author:Ao Kong, Robert Azencott category:stat.ML stat.AP stat.CO 62P10  68T10  published:2014-10-13 summary:For mass spectra acquired from cancer patients by MALDI or SELDI techniques, automated discrimination between cancer types or stages has often been implemented by machine learnings. These techniques typically generate "black-box" classifiers, which are difficult to interpret biologically. We develop new and efficient signature discovery algorithms leading to interpretable signatures combining the discriminating power of explicitly selected small groups of biomarkers, identified by their m/z ratios. Our approach is based on rigorous stochastic modeling of "homogeneous" datasets of mass spectra by a versatile class of parameterized Markov Random Fields. We present detailed algorithms validated by precise theoretical results. We also outline the successful tests of our approach to generate efficient explicit signatures for six benchmark discrimination tasks, based on mass spectra acquired from colorectal cancer patients, as well as from ovarian cancer patients. version:1
arxiv-1410-1771 | PAC-Bayesian AUC classification and scoring | http://arxiv.org/abs/1410.1771 | id:1410.1771 author:James Ridgway, Pierre Alquier, Nicolas Chopin, Feng Liang category:stat.ML stat.CO 62H30  published:2014-10-07 summary:We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior. version:2
arxiv-1410-3192 | Learning without Concentration for General Loss Functions | http://arxiv.org/abs/1410.3192 | id:1410.3192 author:Shahar Mendelson category:stat.ML K.3.2  published:2014-10-13 summary:We study prediction and estimation problems using empirical risk minimization, relative to a general convex loss function. We obtain sharp error rates even when concentration is false or is very restricted, for example, in heavy-tailed scenarios. Our results show that the error rate depends on two parameters: one captures the intrinsic complexity of the class, and essentially leads to the error rate in a noise-free (or realizable) problem; the other measures interactions between class members the target and the loss, and is dominant when the problem is far from realizable. We also explain how one may deal with outliers by choosing the loss in a way that is calibrated to the intrinsic complexity of the class and to the noise-level of the problem (the latter is measured by the distance between the target and the class). version:1
arxiv-1410-3169 | Multi-Scale Local Shape Analysis and Feature Selection in Machine Learning Applications | http://arxiv.org/abs/1410.3169 | id:1410.3169 author:Paul Bendich, Ellen Gasparovic, John Harer, Rauf Izmailov, Linda Ness category:cs.CG cs.LG math.AT stat.ML  published:2014-10-13 summary:We introduce a method called multi-scale local shape analysis, or MLSA, for extracting features that describe the local structure of points within a dataset. The method uses both geometric and topological features at multiple levels of granularity to capture diverse types of local information for subsequent machine learning algorithms operating on the dataset. Using synthetic and real dataset examples, we demonstrate significant performance improvement of classification algorithms constructed for these datasets with correspondingly augmented features. version:1
arxiv-1401-7623 | Graph matching: relax or not? | http://arxiv.org/abs/1401.7623 | id:1401.7623 author:Yonathan Aflalo, Alex Bronstein, Ron Kimmel category:cs.DS cs.CG cs.CV math.OC  published:2014-01-29 summary:We consider the problem of exact and inexact matching of weighted undirected graphs, in which a bijective correspondence is sought to minimize a quadratic weight disagreement. This computationally challenging problem is often relaxed as a convex quadratic program, in which the space of permutations is replaced by the space of doubly-stochastic matrices. However, the applicability of such a relaxation is poorly understood. We define a broad class of friendly graphs characterized by an easily verifiable spectral property. We prove that for friendly graphs, the convex relaxation is guaranteed to find the exact isomorphism or certify its inexistence. This result is further extended to approximately isomorphic graphs, for which we develop an explicit bound on the amount of weight disagreement under which the relaxation is guaranteed to find the globally optimal approximate isomorphism. We also show that in many cases, the graph matching problem can be further harmlessly relaxed to a convex quadratic program with only n separable linear equality constraints, which is substantially more efficient than the standard relaxation involving 2n equality and n^2 inequality constraints. Finally, we show that our results are still valid for unfriendly graphs if additional information in the form of seeds or attributes is allowed, with the latter satisfying an easy to verify spectral characteristic. version:5
arxiv-1312-3522 | Sparse Matrix-based Random Projection for Classification | http://arxiv.org/abs/1312.3522 | id:1312.3522 author:Weizhi Lu, Weiyu Li, Kidiyo Kpalma, Joseph Ronsin category:cs.LG cs.CV stat.ML  published:2013-12-12 summary:As a typical dimensionality reduction technique, random projection can be simply implemented with linear projection, while maintaining the pairwise distances of high-dimensional data with high probability. Considering this technique is mainly exploited for the task of classification, this paper is developed to study the construction of random matrix from the viewpoint of feature selection, rather than of traditional distance preservation. This yields a somewhat surprising theoretical result, that is, the sparse random matrix with exactly one nonzero element per column, can present better feature selection performance than other more dense matrices, if the projection dimension is sufficiently large (namely, not much smaller than the number of feature elements); otherwise, it will perform comparably to others. For random projection, this theoretical result implies considerable improvement on both complexity and performance, which is widely confirmed with the classification experiments on both synthetic data and real data. version:3
arxiv-1410-3145 | Machine Learning Techniques in Cognitive Radio Networks | http://arxiv.org/abs/1410.3145 | id:1410.3145 author:Peter Hossain, Adaulfo Komisarczuk, Garin Pawetczak, Sarah Van Dijk, Isabella Axelsen category:cs.LG cs.NI  published:2014-10-12 summary:Cognitive radio is an intelligent radio that can be programmed and configured dynamically to fully use the frequency resources that are not used by licensed users. It defines the radio devices that are capable of learning and adapting to their transmission to the external radio environment, which means it has some kind of intelligence for monitoring the radio environment, learning the environment and make smart decisions. In this paper, we are reviewing some examples of the usage of machine learning techniques in cognitive radio networks for implementing the intelligent radio. version:1
arxiv-1410-3111 | Hierarchical models for neural population dynamics in the presence of non-stationarity | http://arxiv.org/abs/1410.3111 | id:1410.3111 author:Mijung Park, Jakob H. Macke category:stat.ML q-bio.NC  published:2014-10-12 summary:Neural population activity often exhibits rich variability and temporal structure. This variability is thought to arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as from modulations of neural firing properties on long time-scales, often referred to as "non-stationarity". To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we need statistical models that are able to capture multiple sources of variability on different time-scales. Here, we introduce a hierarchical statistical model of neural population activity which models both neural population dynamics as well as inter-trial modulations in firing rates. In addition, we extend the model to allow us to capture non-stationarities in the population dynamics itself (i.e., correlations across neurons). We develop variational inference methods for learning model parameters, and demonstrate that the method can recover non-stationarities in both average firing rates and correlation structure. Applied to neural population recordings from anesthetized macaque primary visual cortex, our models provide a better account of the structure of neural firing than stationary dynamics models. version:1
arxiv-1410-3080 | Tree-Structure Bayesian Compressive Sensing for Video | http://arxiv.org/abs/1410.3080 | id:1410.3080 author:Xin Yuan, Patrick Llull, David J. Brady, Lawrence Carin category:cs.CV  published:2014-10-12 summary:A Bayesian compressive sensing framework is developed for video reconstruction based on the color coded aperture compressive temporal imaging (CACTI) system. By exploiting the three dimension (3D) tree structure of the wavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesian compressive sensing inversion algorithm is derived to reconstruct (up to 22) color video frames from a single monochromatic compressive measurement. Both simulated and real datasets are adopted to verify the performance of the proposed algorithm. version:1
arxiv-1409-7489 | Recommending Investors for Crowdfunding Projects | http://arxiv.org/abs/1409.7489 | id:1409.7489 author:Jisun An, Daniele Quercia, Jon Crowcroft category:cs.SI cs.CY cs.HC physics.soc-ph stat.ML  published:2014-09-26 summary:To bring their innovative ideas to market, those embarking in new ventures have to raise money, and, to do so, they have often resorted to banks and venture capitalists. Nowadays, they have an additional option: that of crowdfunding. The name refers to the idea that funds come from a network of people on the Internet who are passionate about supporting others' projects. One of the most popular crowdfunding sites is Kickstarter. In it, creators post descriptions of their projects and advertise them on social media sites (mainly Twitter), while investors look for projects to support. The most common reason for project failure is the inability of founders to connect with a sufficient number of investors, and that is mainly because hitherto there has not been any automatic way of matching creators and investors. We thus set out to propose different ways of recommending investors found on Twitter for specific Kickstarter projects. We do so by conducting hypothesis-driven analyses of pledging behavior and translate the corresponding findings into different recommendation strategies. The best strategy achieves, on average, 84% of accuracy in predicting a list of potential investors' Twitter accounts for any given project. Our findings also produced key insights about the whys and wherefores of investors deciding to support innovative efforts. version:2
arxiv-1410-2954 | Q-learning for Optimal Control of Continuous-time Systems | http://arxiv.org/abs/1410.2954 | id:1410.2954 author:Biao Luo, Derong Liu, Tingwen Huang category:cs.SY stat.ML  published:2014-10-11 summary:In this paper, two Q-learning (QL) methods are proposed and their convergence theories are established for addressing the model-free optimal control problem of general nonlinear continuous-time systems. By introducing the Q-function for continuous-time systems, policy iteration based QL (PIQL) and value iteration based QL (VIQL) algorithms are proposed for learning the optimal control policy from real system data rather than using mathematical system model. It is proved that both PIQL and VIQL methods generate a nonincreasing Q-function sequence, which converges to the optimal Q-function. For implementation of the QL algorithms, the method of weighted residuals is applied to derived the parameters update rule. The developed PIQL and VIQL algorithms are essentially off-policy reinforcement learning approachs, where the system data can be collected arbitrary and thus the exploration ability is increased. With the data collected from the real system, the QL methods learn the optimal control policy offline, and then the convergent control policy will be employed to real system. The effectiveness of the developed QL algorithms are verified through computer simulation. version:1
arxiv-1305-7477 | On model selection consistency of regularized M-estimators | http://arxiv.org/abs/1305.7477 | id:1305.7477 author:Jason D. Lee, Yuekai Sun, Jonathan E. Taylor category:math.ST cs.LG math.OC stat.ME stat.ML stat.TH  published:2013-05-31 summary:Regularized M-estimators are used in diverse areas of science and engineering to fit high-dimensional models with some low-dimensional structure. Usually the low-dimensional structure is encoded by the presence of the (unknown) parameters in some low-dimensional model subspace. In such settings, it is desirable for estimates of the model parameters to be \emph{model selection consistent}: the estimates also fall in the model subspace. We develop a general framework for establishing consistency and model selection consistency of regularized M-estimators and show how it applies to some special cases of interest in statistical learning. Our analysis identifies two key properties of regularized M-estimators, referred to as geometric decomposability and irrepresentability, that ensure the estimators are consistent and model selection consistent. version:8
arxiv-1405-4802 | Use of Computer Vision to Detect Tangles in Tangled Objects | http://arxiv.org/abs/1405.4802 | id:1405.4802 author:Paritosh Parmar category:cs.CV  published:2014-05-19 summary:Untangling of structures like ropes and wires by autonomous robots can be useful in areas such as personal robotics, industries and electrical wiring & repairing by robots. This problem can be tackled by using computer vision system in robot. This paper proposes a computer vision based method for analyzing visual data acquired from camera for perceiving the overlap of wires, ropes, hoses i.e. detecting tangles. Information obtained after processing image according to the proposed method comprises of position of tangles in tangled object and which wire passes over which wire. This information can then be used to guide robot to untangle wire/s. Given an image, preprocessing is done to remove noise. Then edges of wire are detected. After that, the image is divided into smaller blocks and each block is checked for wire overlap/s and finding other relevant information. TANGLED-100 dataset was introduced, which consists of images of tangled linear deformable objects. Method discussed in here was tested on the TANGLED-100 dataset. Accuracy achieved during experiments was found to be 74.9%. Robotic simulations were carried out to demonstrate the use of the proposed method in applications of robot. Proposed method is a general method that can be used by robots working in different situations. version:2
arxiv-1312-6184 | Do Deep Nets Really Need to be Deep? | http://arxiv.org/abs/1312.6184 | id:1312.6184 author:Lei Jimmy Ba, Rich Caruana category:cs.LG cs.NE  published:2013-12-21 summary:Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available. version:7
arxiv-1410-2910 | Riesz Logic | http://arxiv.org/abs/1410.2910 | id:1410.2910 author:Daoud Clarke category:cs.LO cs.CL  published:2014-10-10 summary:We introduce Riesz Logic, whose models are abelian lattice ordered groups, which generalise Riesz spaces (vector lattices), and show soundness and completeness. Our motivation is to provide a logic for distributional semantics of natural language, where words are typically represented as elements of a vector space whose dimensions correspond to contexts in which words may occur. This basis provides a lattice ordering on the space, and this ordering may be interpreted as "distributional entailment". Several axioms of Riesz Logic are familiar from Basic Fuzzy Logic, and we show how the models of these two logics may be related; Riesz Logic may thus be considered a new fuzzy logic. In addition to applications in natural language processing, there is potential for applying the theory to neuro-fuzzy systems. version:1
arxiv-1410-2838 | Approximate False Positive Rate Control in Selection Frequency for Random Forest | http://arxiv.org/abs/1410.2838 | id:1410.2838 author:Ender Konukoglu, Melanie Ganz category:cs.LG stat.ME  published:2014-10-10 summary:Random Forest has become one of the most popular tools for feature selection. Its ability to deal with high-dimensional data makes this algorithm especially useful for studies in neuroimaging and bioinformatics. Despite its popularity and wide use, feature selection in Random Forest still lacks a crucial ingredient: false positive rate control. To date there is no efficient, principled and computationally light-weight solution to this shortcoming. As a result, researchers using Random Forest for feature selection have to resort to using heuristically set thresholds on feature rankings. This article builds an approximate probabilistic model for the feature selection process in random forest training, which allows us to compute an estimated false positive rate for a given threshold on selection frequency. Hence, it presents a principled way to determine thresholds for the selection of relevant features without any additional computational load. Experimental analysis with synthetic data demonstrates that the proposed approach can limit false positive rates on the order of the desired values and keep false negative rates low. Results show that this holds even in the presence of a complex correlation structure between features. Its good statistical properties and light-weight computational needs make this approach widely applicable to feature selection for a wide-range of applications. version:1
arxiv-1410-2786 | New SVD based initialization strategy for Non-negative Matrix Factorization | http://arxiv.org/abs/1410.2786 | id:1410.2786 author:Hanli Qiao category:cs.LG cs.NA  published:2014-10-10 summary:There are two problems need to be dealt with for Non-negative Matrix Factorization (NMF): choose a suitable rank of the factorization and provide a good initialization method for NMF algorithms. This paper aims to solve these two problems using Singular Value Decomposition (SVD). At first we extract the number of main components as the rank, actually this method is inspired from [1, 2]. Second, we use the singular value and its vectors to initialize NMF algorithm. In 2008, Boutsidis and Gollopoulos [3] provided the method titled NNDSVD to enhance initialization of NMF algorithms. They extracted the positive section and respective singular triplet information of the unit matrices {C(j)}k j=1 which were obtained from singular vector pairs. This strategy aims to use positive section to cope with negative elements of the singular vectors, but in experiments we found that even replacing negative elements by their absolute values could get better results than NNDSVD. Hence, we give another method based SVD to fulfil initialization for NMF algorithms (SVD-NMF). Numerical experiments on two face databases ORL and YALE [16, 17] show that our method is better than NNDSVD. version:1
arxiv-1409-5114 | A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-resolution | http://arxiv.org/abs/1409.5114 | id:1409.5114 author:Shuxin Ouyang, Timothy Hospedales, Yi-Zhe Song, Xueming Li category:cs.CV A.1; I.4.9; I.5.4  published:2014-09-17 summary:Heterogeneous face recognition (HFR) refers to matching face imagery across different domains. It has received much interest from the research community as a result of its profound implications in law enforcement. A wide variety of new invariant features, cross-modality matching models and heterogeneous datasets being established in recent years. This survey provides a comprehensive review of established techniques and recent developments in HFR. Moreover, we offer a detailed account of datasets and benchmarks commonly used for evaluation. We finish by assessing the state of the field and discussing promising directions for future research. version:2
arxiv-1410-2724 | Compressed Sensing With Side Information: Geometrical Interpretation and Performance Bounds | http://arxiv.org/abs/1410.2724 | id:1410.2724 author:João F. C. Mota, Nikos Deligiannis, Miguel R. D. Rodrigues category:cs.IT math.IT math.OC stat.ML  published:2014-10-10 summary:We address the problem of Compressed Sensing (CS) with side information. Namely, when reconstructing a target CS signal, we assume access to a similar signal. This additional knowledge, the side information, is integrated into CS via L1-L1 and L1-L2 minimization. We then provide lower bounds on the number of measurements that these problems require for successful reconstruction of the target signal. If the side information has good quality, the number of measurements is significantly reduced via L1-L1 minimization, but not so much via L1-L2 minimization. We provide geometrical interpretations and experimental results illustrating our findings. version:1
arxiv-1410-2663 | Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet marginals | http://arxiv.org/abs/1410.2663 | id:1410.2663 author:Florian Yger category:cs.CV  published:2014-10-10 summary:This short memo aims at explaining our approach for the challenge IEEE-ISBI on Bone Texture Characterization. In this work, we focus on the use of covariance matrices and wavelet marginals in an SVM classifier. version:1
arxiv-1410-2082 | Contrastive Unsupervised Word Alignment with Non-Local Features | http://arxiv.org/abs/1410.2082 | id:1410.2082 author:Yang Liu, Maosong Sun category:cs.CL  published:2014-10-08 summary:Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over state-of-the-art unsupervised word alignment methods. version:2
arxiv-1410-2653 | Distributed Estimation, Information Loss and Exponential Families | http://arxiv.org/abs/1410.2653 | id:1410.2653 author:Qiang Liu, Alexander Ihler category:stat.ML  published:2014-10-09 summary:Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE given the whole dataset. We study this framework's statistical properties, showing that the efficiency loss compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the "full-exponential-family-ness" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of both methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions. version:1
arxiv-1410-2646 | Hybrid approaches for automatic vowelization of Arabic texts | http://arxiv.org/abs/1410.2646 | id:1410.2646 author:Mohamed Bebah, Chennoufi Amine, Mazroui Azzeddine, Lakhouaja Abdelhak category:cs.CL 68T50  published:2014-10-09 summary:Hybrid approaches for automatic vowelization of Arabic texts are presented in this article. The process is made up of two modules. In the first one, a morphological analysis of the text words is performed using the open source morphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out of context, are its different possible vowelizations. The integration of this Analyzer in our vowelization system required the addition of a lexical database containing the most frequent words in Arabic language. Using a statistical approach based on two hidden Markov models (HMM), the second module aims to eliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic words are the observed states and the vowelized words are the hidden states. The observed states of the second HMM are identical to those of the first, but the hidden states are the lists of possible diacritics of the word without its Arabic letters. Our system uses Viterbi algorithm to select the optimal path among the solutions proposed by Al Khalil Morpho Sys. Our approach opens an important way to improve the performance of automatic vowelization of Arabic texts for other uses in automatic natural language processing. version:1
arxiv-1410-2596 | Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares | http://arxiv.org/abs/1410.2596 | id:1410.2596 author:Trevor Hastie, Rahul Mazumder, Jason Lee, Reza Zadeh category:stat.ME stat.ML  published:2014-10-09 summary:The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm-regularized matrix approximation (Candes and Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-margin matrix factorization (Srebro, Rennie and Jaakkola, 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package "softImpute" in R for implementing our approaches, and a distributed version for very large matrices using the "Spark" cluster programming environment. version:1
arxiv-1410-2535 | A unified approach for multi-object triangulation, tracking and camera calibration | http://arxiv.org/abs/1410.2535 | id:1410.2535 author:Jeremie Houssineau, Daniel Clark, Spela Ivekovic, Chee Sing Lee, Jose Franco category:cs.CV stat.ME  published:2014-10-09 summary:Object triangulation, 3-D object tracking, feature correspondence, and camera calibration are key problems for estimation from camera networks. This paper addresses these problems within a unified Bayesian framework for joint multi-object tracking and sensor registration. Given that using standard filtering approaches for state estimation from cameras is problematic, an alternative parametrisation is exploited, called disparity space. The disparity space-based approach for triangulation and object tracking is shown to be more effective than non-linear versions of the Kalman filter and particle filtering for non-rectified cameras. The approach for feature correspondence is based on the Probability Hypothesis Density (PHD) filter, and hence inherits the ability to update without explicit measurement association, to initiate new targets, and to discriminate between target and clutter. The PHD filtering approach then forms the basis of a camera calibration method from static or moving objects. Results are shown on simulated data. version:1
arxiv-1410-2474 | Genetic Stereo Matching Algorithm with Fuzzy Fitness | http://arxiv.org/abs/1410.2474 | id:1410.2474 author:Haythem Ghazouani category:cs.CV  published:2014-10-09 summary:This paper presents a genetic stereo matching algorithm with fuzzy evaluation function. The proposed algorithm presents a new encoding scheme in which a chromosome is represented by a disparity matrix. Evolution is controlled by a fuzzy fitness function able to deal with noise and uncertain camera measurements, and uses classical evolutionary operators. The result of the algorithm is accurate dense disparity maps obtained in a reasonable computational time suitable for real-time applications as shown in experimental results. version:1
arxiv-1410-1035 | Learning Invariant Color Features for Person Re-Identification | http://arxiv.org/abs/1410.1035 | id:1410.1035 author:Rahul Rama Varior, Gang Wang, Jiwen Lu category:cs.CV  published:2014-10-04 summary:Matching people across multiple camera views known as person re-identification, is a challenging problem due to the change in visual appearance caused by varying lighting conditions. The perceived color of the subject appears to be different with respect to illumination. Previous works use color as it is or address these challenges by designing color spaces focusing on a specific cue. In this paper, we propose a data driven approach for learning color patterns from pixels sampled from images across two camera views. The intuition behind this work is that, even though pixel values of same color would be different across views, they should be encoded with the same values. We model color feature generation as a learning problem by jointly learning a linear transformation and a dictionary to encode pixel values. We also analyze different photometric invariant color spaces. Using color as the only cue, we compare our approach with all the photometric invariant color spaces and show superior performance over all of them. Combining with other learned low-level and high-level features, we obtain promising results in ViPER, Person Re-ID 2011 and CAVIAR4REID datasets. version:2
arxiv-1401-6497 | Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination | http://arxiv.org/abs/1401.6497 | id:1401.6497 author:Qibin Zhao, Liqing Zhang, Andrzej Cichocki category:cs.LG cs.CV stat.ML  published:2014-01-25 summary:CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank. In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance. version:2
arxiv-1410-2381 | Recognition of cDNA microarray image Using Feedforward artificial neural network | http://arxiv.org/abs/1410.2381 | id:1410.2381 author:R. M. Farouk, S. Badr, M. Sayed Elahl category:cs.CV cs.NE  published:2014-10-09 summary:The complementary DNA (cDNA) sequence is considered to be the magic biometric technique for personal identification. In this paper, we present a new method for cDNA recognition based on the artificial neural network (ANN). Microarray imaging is used for the concurrent identification of thousands of genes. We have segmented the location of the spots in a cDNA microarray. Thus, a precise localization and segmenting of a spot are essential to obtain a more accurate intensity measurement, leading to a more precise expression measurement of a gene. The segmented cDNA microarray image is resized and it is used as an input for the proposed artificial neural network. For matching and recognition, we have trained the artificial neural network. Recognition results are given for the galleries of cDNA sequences . The numerical results show that, the proposed matching technique is an effective in the cDNA sequences process. We also compare our results with previous results and find out that, the proposed technique is an effective matching performance. version:1
arxiv-1410-0719 | Proceedings of the second "international Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST'14) | http://arxiv.org/abs/1410.0719 | id:1410.0719 author:L. Jacques, C. De Vleeschouwer, Y. Boursier, P. Sudhakar, C. De Mol, A. Pizurica, S. Anthoine, P. Vandergheynst, P. Frossard, C. Bilen, S. Kitic, N. Bertin, R. Gribonval, N. Boumal, B. Mishra, P. -A. Absil, R. Sepulchre, S. Bundervoet, C. Schretter, A. Dooms, P. Schelkens, O. Chabiron, F. Malgouyres, J. -Y. Tourneret, N. Dobigeon, P. Chainais, C. Richard, B. Cornelis, I. Daubechies, D. Dunson, M. Dankova, P. Rajmic, K. Degraux, V. Cambareri, B. Geelen, G. Lafruit, G. Setti, J. -F. Determe, J. Louveaux, F. Horlin, A. Drémeau, P. Heas, C. Herzet, V. Duval, G. Peyré, A. Fawzi, M. Davies, N. Gillis, S. A. Vavasis, C. Soussen, L. Le Magoarou, J. Liang, J. Fadili, A. Liutkus, D. Martina, S. Gigan, L. Daudet, M. Maggioni, S. Minsker, N. Strawn, C. Mory, F. Ngole, J. -L. Starck, I. Loris, S. Vaiter, M. Golbabaee, D. Vukobratovic category:cs.NA cs.CV cs.IT cs.LG math.IT math.OC math.ST stat.TH  published:2014-10-02 summary:The implicit objective of the biennial "international - Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST) is to foster collaboration between international scientific teams by disseminating ideas through both specific oral/poster presentations and free discussions. For its second edition, the iTWIST workshop took place in the medieval and picturesque town of Namur in Belgium, from Wednesday August 27th till Friday August 29th, 2014. The workshop was conveniently located in "The Arsenal" building within walking distance of both hotels and town center. iTWIST'14 has gathered about 70 international participants and has featured 9 invited talks, 10 oral presentations, and 14 posters on the following themes, all related to the theory, application and generalization of the "sparsity paradigm": Sparsity-driven data sensing and processing; Union of low dimensional subspaces; Beyond linear and convex inverse problem; Matrix/manifold/graph sensing/processing; Blind inverse problems and dictionary learning; Sparsity and computational neuroscience; Information theory, geometry and randomness; Complexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?; Sparse machine learning and inference. version:2
arxiv-1412-6018 | Automatic Training Data Synthesis for Handwriting Recognition Using the Structural Crossing-Over Technique | http://arxiv.org/abs/1412.6018 | id:1412.6018 author:Sirisak Visessenee, Sanparith Marukatat, Rachada Kongkachandra category:cs.CV cs.LG  published:2014-10-09 summary:The paper presents a novel technique called "Structural Crossing-Over" to synthesize qualified data for training machine learning-based handwriting recognition. The proposed technique can provide a greater variety of patterns of training data than the existing approaches such as elastic distortion and tangent-based affine transformation. A couple of training characters are chosen, then they are analyzed by their similar and different structures, and finally are crossed over to generate the new characters. The experiments are set to compare the performances of tangent-based affine transformation and the proposed approach in terms of the variety of generated characters and percent of recognition errors. The standard MNIST corpus including 60,000 training characters and 10,000 test characters is employed in the experiments. The proposed technique uses 1,000 characters to synthesize 60,000 characters, and then uses these data to train and test the benchmark handwriting recognition system that exploits Histogram of Gradient (HOG) as features and Support Vector Machine (SVM) as recognizer. The experimental result yields 8.06% of errors. It significantly outperforms the tangent-based affine transformation and the original MNIST training data, which are 11.74% and 16.55%, respectively. version:1
arxiv-1410-3341 | Generalization Analysis for Game-Theoretic Machine Learning | http://arxiv.org/abs/1410.3341 | id:1410.3341 author:Haifang Li, Fei Tian, Wei Chen, Tao Qin, Tie-Yan Liu category:cs.LG cs.GT  published:2014-10-09 summary:For Internet applications like sponsored search, cautions need to be taken when using machine learning to optimize their mechanisms (e.g., auction) since self-interested agents in these applications may change their behaviors (and thus the data distribution) in response to the mechanisms. To tackle this problem, a framework called game-theoretic machine learning (GTML) was recently proposed, which first learns a Markov behavior model to characterize agents' behaviors, and then learns the optimal mechanism by simulating agents' behavior changes in response to the mechanism. While GTML has demonstrated practical success, its generalization analysis is challenging because the behavior data are non-i.i.d. and dependent on the mechanism. To address this challenge, first, we decompose the generalization error for GTML into the behavior learning error and the mechanism learning error; second, for the behavior learning error, we obtain novel non-asymptotic error bounds for both parametric and non-parametric behavior learning methods; third, for the mechanism learning error, we derive a uniform convergence bound based on a new concept called nested covering number of the mechanism space and the generalization analysis techniques developed for mixing sequences. To the best of our knowledge, this is the first work on the generalization analysis of GTML, and we believe it has general implications to the theoretical analysis of other complicated machine learning problems. version:1
arxiv-1410-2265 | A Scalable, Lexicon Based Technique for Sentiment Analysis | http://arxiv.org/abs/1410.2265 | id:1410.2265 author:Chetan Kaushik, Atul Mishra category:cs.IR cs.CL  published:2014-10-08 summary:Rapid increase in the volume of sentiment rich social media on the web has resulted in an increased interest among researchers regarding Sentimental Analysis and opinion mining. However, with so much social media available on the web, sentiment analysis is now considered as a big data task. Hence the conventional sentiment analysis approaches fails to efficiently handle the vast amount of sentiment data available now a days. The main focus of the research was to find such a technique that can efficiently perform sentiment analysis on big data sets. A technique that can categorize the text as positive, negative and neutral in a fast and accurate manner. In the research, sentiment analysis was performed on a large data set of tweets using Hadoop and the performance of the technique was measured in form of speed and accuracy. The experimental results shows that the technique exhibits very good efficiency in handling big sentiment data sets. version:1
