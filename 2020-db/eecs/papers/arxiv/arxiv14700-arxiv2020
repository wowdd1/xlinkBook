arxiv-1602-02665 | The happiness paradox: your friends are happier than you | http://arxiv.org/abs/1602.02665 | id:1602.02665 author:Johan Bollen, Bruno Gonçalves, Ingrid van de Leemput, Guangchen Ruan category:cs.SI cs.CL cs.HC physics.soc-ph  published:2016-02-08 summary:Most individuals in social networks experience a so-called Friendship Paradox: they are less popular than their friends on average. This effect may explain recent findings that widespread social network media use leads to reduced happiness. However the relation between popularity and happiness is poorly understood. A Friendship paradox does not necessarily imply a Happiness paradox where most individuals are less happy than their friends. Here we report the first direct observation of a significant Happiness Paradox in a large-scale online social network of $39,110$ Twitter users. Our results reveal that popular individuals are indeed happier and that a majority of individuals experience a significant Happiness paradox. The magnitude of the latter effect is shaped by complex interactions between individual popularity, happiness, and the fact that users cluster assortatively by level of happiness. Our results indicate that the topology of online social networks and the distribution of happiness in some populations can cause widespread psycho-social effects that affect the well-being of billions of individuals. version:1
arxiv-1602-02666 | A Variational Analysis of Stochastic Gradient Algorithms | http://arxiv.org/abs/1602.02666 | id:1602.02666 author:Stephan Mandt, Matthew D. Hoffman, David M. Blei category:stat.ML cs.LG  published:2016-02-08 summary:Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models. version:1
arxiv-1602-02656 | LSTM Deep Neural Networks Postfiltering for Improving the Quality of Synthetic Voices | http://arxiv.org/abs/1602.02656 | id:1602.02656 author:Marvin Coto-Jiménez, John Goddard-Close category:cs.SD cs.NE  published:2016-02-08 summary:Recent developments in speech synthesis have produced systems capable of outcome intelligible speech, but now researchers strive to create models that more accurately mimic human voices. One such development is the incorporation of multiple linguistic styles in various languages and accents. HMM-based Speech Synthesis is of great interest to many researchers, due to its ability to produce sophisticated features with small footprint. Despite such progress, its quality has not yet reached the level of the predominant unit-selection approaches that choose and concatenate recordings of real speech. Recent efforts have been made in the direction of improving these systems. In this paper we present the application of Long-Short Term Memory Deep Neural Networks as a Postfiltering step of HMM-based speech synthesis, in order to obtain closer spectral characteristics to those of natural speech. The results show how HMM-voices could be improved using this approach. version:1
arxiv-1602-02651 | Automatic Face Reenactment | http://arxiv.org/abs/1602.02651 | id:1602.02651 author:Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen, Patrick Perez, Christian Theobalt category:cs.CV cs.GR  published:2016-02-08 summary:We propose an image-based, facial reenactment system that replaces the face of an actor in an existing target video with the face of a user from a source video, while preserving the original target performance. Our system is fully automatic and does not require a database of source expressions. Instead, it is able to produce convincing reenactment results from a short source video captured with an off-the-shelf camera, such as a webcam, where the user performs arbitrary facial gestures. Our reenactment pipeline is conceived as part image retrieval and part face transfer: The image retrieval is based on temporal clustering of target frames and a novel image matching metric that combines appearance and motion to select candidate frames from the source video, while the face transfer uses a 2D warping strategy that preserves the user's identity. Our system excels in simplicity as it does not rely on a 3D face model, it is robust under head motion and does not require the source and target performance to be similar. We show convincing reenactment results for videos that we recorded ourselves and for low-quality footage taken from the Internet. version:1
arxiv-1602-02616 | Guarantees in Wasserstein Distance for the Langevin Monte Carlo Algorithm | http://arxiv.org/abs/1602.02616 | id:1602.02616 author:Thomas Bonis category:stat.CO math.ST stat.ML stat.TH  published:2016-02-08 summary:We study the problem of sampling from a distribution $\target$ using the Langevin Monte Carlo algorithm and provide rate of convergences for this algorithm in terms of Wasserstein distance of order $2$. Our result holds as long as the continuous diffusion process associated with the algorithm converges exponentially fast to the target distribution along with some technical assumptions. While such an exponential convergence holds for example in the log-concave measure case, it also holds for the more general case of asymptoticaly log-concave measures. Our results thus extends the known rates of convergence in total variation and Wasserstein distances which have only been obtained in the log-concave case. Moreover, using a sharper approximation bound of the continuous process, we obtain better asymptotic rates than traditional results. We also look into variations of the Langevin Monte Carlo algorithm using other discretization schemes. In a first time, we look into the use of the Ozaki's discretization but are unable to obtain any significative improvement in terms of convergence rates compared to the Euler's scheme. We then provide a (sub-optimal) way to study more general schemes, however our approach only holds for the log-concave case. version:1
arxiv-1511-03466 | God(s) Know(s): Developmental and Cross-Cultural Patterns in Children Drawings | http://arxiv.org/abs/1511.03466 | id:1511.03466 author:Ksenia Konyushkova, Nikolaos Arvanitopoulos, Zhargalma Dandarova Robert, Pierre-Yves Brandt, Sabine Süsstrunk category:cs.CV  published:2015-11-11 summary:This paper introduces a novel approach to data analysis designed for the needs of specialists in psychology of religion. We detect developmental and cross-cultural patterns in children's drawings of God(s) and other supernatural agents. We develop methods to objectively evaluate our empirical observations of the drawings with respect to: (1) the gravity center, (2) the average intensities of the colors \emph{green} and \emph{yellow}, (3) the use of different colors (palette) and (4) the visual complexity of the drawings. We find statistically significant differences across ages and countries in the gravity centers and in the average intensities of colors. These findings support the hypotheses of the experts and raise new questions for further investigation. version:2
arxiv-1602-02586 | Tumour ROI Estimation in Ultrasound Images via Radon Barcodes in Patients with Locally Advanced Breast Cancer | http://arxiv.org/abs/1602.02586 | id:1602.02586 author:Hamid R. Tizhoosh, Mehrdad J. Gangeh, Hadi Tadayyon, Gregory J. Czarnota category:cs.CV  published:2016-02-08 summary:Quantitative ultrasound (QUS) methods provide a promising framework that can non-invasively and inexpensively be used to predict or assess the tumour response to cancer treatment. The first step in using the QUS methods is to select a region of interest (ROI) inside the tumour in ultrasound images. Manual segmentation, however, is very time consuming and tedious. In this paper, a semi-automated approach will be proposed to roughly localize an ROI for a tumour in ultrasound images of patients with locally advanced breast cancer (LABC). Content-based barcodes, a recently introduced binary descriptor based on Radon transform, were used in order to find similar cases and estimate a bounding box surrounding the tumour. Experiments with 33 B-scan images resulted in promising results with an accuracy of $81\%$. version:1
arxiv-1602-02543 | Homogeneity of Cluster Ensembles | http://arxiv.org/abs/1602.02543 | id:1602.02543 author:Brijnesh J. Jain category:cs.LG cs.CV  published:2016-02-08 summary:The expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions. version:1
arxiv-1510-07545 | Using Shortlists to Support Decision Making and Improve Recommender System Performance | http://arxiv.org/abs/1510.07545 | id:1510.07545 author:Tobias Schnabel, Paul N. Bennett, Susan T. Dumais, Thorsten Joachims category:cs.HC cs.IR cs.LG  published:2015-10-26 summary:In this paper, we study shortlists as an interface component for recommender systems with the dual goal of supporting the user's decision process, as well as improving implicit feedback elicitation for increased recommendation quality. A shortlist is a temporary list of candidates that the user is currently considering, e.g., a list of a few movies the user is currently considering for viewing. From a cognitive perspective, shortlists serve as digital short-term memory where users can off-load the items under consideration -- thereby decreasing their cognitive load. From a machine learning perspective, adding items to the shortlist generates a new implicit feedback signal as a by-product of exploration and decision making which can improve recommendation quality. Shortlisting therefore provides additional data for training recommendation systems without the increases in cognitive load that requesting explicit feedback would incur. We perform an user study with a movie recommendation setup to compare interfaces that offer shortlist support with those that do not. From the user studies we conclude: (i) users make better decisions with a shortlist; (ii) users prefer an interface with shortlist support; and (iii) the additional implicit feedback from sessions with a shortlist improves the quality of recommendations by nearly a factor of two. version:2
arxiv-1602-02523 | Data-Efficient Reinforcement Learning in Continuous-State POMDPs | http://arxiv.org/abs/1602.02523 | id:1602.02523 author:Rowan McAllister, Carl Edward Rasmussen category:stat.ML cs.LG cs.SY  published:2016-02-08 summary:We present a data-efficient reinforcement learning algorithm resistant to observation noise. Our method extends the highly data-efficient PILCO algorithm (Deisenroth & Rasmussen, 2011) into partially observed Markov decision processes (POMDPs) by considering the filtering process during policy evaluation. PILCO conducts policy search, evaluating each policy by first predicting an analytic distribution of possible system trajectories. We additionally predict trajectories w.r.t. a filtering process, achieving significantly higher performance than combining a filter with a policy optimised by the original (unfiltered) framework. Our test setup is the cartpole swing-up task with sensor noise, which involves nonlinear dynamics and requires nonlinear control. version:1
arxiv-1602-02522 | A Semi-Automated Method for Object Segmentation in Infant's Egocentric Videos to Study Object Perception | http://arxiv.org/abs/1602.02522 | id:1602.02522 author:Qazaleh Mirsharif, Sidharth Sadani, Shishir Shah, Hanako Yoshida, Joseph Burling category:cs.CV  published:2016-02-08 summary:Object segmentation in infant's egocentric videos is a fundamental step in studying how children perceive objects in early stages of development. From the computer vision perspective, object segmentation in such videos pose quite a few challenges because the child's view is unfocused, often with large head movements, effecting in sudden changes in the child's point of view which leads to frequent change in object properties such as size, shape and illumination. In this paper, we develop a semi-automated, domain specific, method to address these concerns and facilitate the object annotation process for cognitive scientists allowing them to select and monitor the object under segmentation. The method starts with an annotation from the user of the desired object and employs graph cut segmentation and optical flow computation to predict the object mask for subsequent video frames automatically. To maintain accuracy, we use domain specific heuristic rules to re-initialize the program with new user input whenever object properties change dramatically. The evaluations demonstrate the high speed and accuracy of the presented method for object segmentation in voluminous egocentric videos. We apply the proposed method to investigate potential patterns in object distribution in child's view at progressive ages. version:1
arxiv-1602-02518 | Multi-view Kernel Completion | http://arxiv.org/abs/1602.02518 | id:1602.02518 author:Sahely Bhadra, Samuel Kaski, Juho Rousu category:cs.LG stat.ML  published:2016-02-08 summary:In this paper, we introduce the first method that (1) can complete kernel matrices with completely missing rows and columns as opposed to individual missing kernel values, (2) does not require any of the kernels to be complete a priori, and (3) can tackle non-linear kernels. These aspects are necessary in practical applications such as integrating legacy data sets, learning under sensor failures and learning when measurements are costly for some of the views. The proposed approach predicts missing rows by modelling both within-view and between-view relationships among kernel values. We show, both on simulated data and real world data, that the proposed method outperforms existing techniques in the restricted settings where they are available, and extends applicability to new settings. version:1
arxiv-1602-02068 | From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification | http://arxiv.org/abs/1602.02068 | id:1602.02068 author:André F. T. Martins, Ramón Fernandez Astudillo category:cs.CL cs.LG stat.ML  published:2016-02-05 summary:We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus. version:2
arxiv-1601-07435 | Co-Occurrence Patterns in the Voynich Manuscript | http://arxiv.org/abs/1601.07435 | id:1601.07435 author:Torsten Timm category:cs.CL cs.CR  published:2016-01-27 summary:The Voynich Manuscript is a medieval book written in an unknown script. This paper studies the distribution of similarly spelled words in the Voynich Manuscript. It shows that the distribution of words within the manuscript is not compatible with natural languages. version:2
arxiv-1602-02485 | Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling | http://arxiv.org/abs/1602.02485 | id:1602.02485 author:Atsushi Shibagaki, Masayuki Karasuyama, Kohei Hatano, Ichiro Takeuchi category:stat.ML  published:2016-02-08 summary:The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of non-active features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A significant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and vice-versa. We first theoretically investigate the synergy effect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples. version:1
arxiv-1510-00556 | Autonomous Perceptron Neural Network Inspired from Quantum computing | http://arxiv.org/abs/1510.00556 | id:1510.00556 author:M. Zidan, A. Sagheer, N. Metwally category:quant-ph cs.NE  published:2015-10-02 summary:This abstract will be modified after correcting the minor error in Eq.(2) version:2
arxiv-1602-02454 | Efficient Algorithms for Adversarial Contextual Learning | http://arxiv.org/abs/1602.02454 | id:1602.02454 author:Vasilis Syrgkanis, Akshay Krishnamurthy, Robert E. Schapire category:cs.LG  published:2016-02-08 summary:We provide the first oracle efficient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently in one of the contexts in the set. Our algorithms fall into the follow the perturbed leader family \cite{Kalai2005} and achieve regret $O(T^{3/4}\sqrt{K\log(N)})$ in the transductive setting and $O(T^{2/3} d^{3/4} K\sqrt{\log(N)})$ in the separator setting, where $K$ is the number of actions, $N$ is the number of baseline policies, and $d$ is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences. version:1
arxiv-1511-06485 | The Effect of Gradient Noise on the Energy Landscape of Deep Networks | http://arxiv.org/abs/1511.06485 | id:1511.06485 author:Pratik Chaudhari, Stefano Soatto category:cs.LG  published:2015-11-20 summary:We analyze the regularization properties of additive gradient noise in the training of deep networks by posing it as finding the ground state of the Hamiltonian of a spherical spin glass in an external magnetic field. We show that depending upon the magnitude of the magnetic field, the Hamiltonian changes dramatically from a highly non-convex energy landscape with exponentially many critical points to a regime with polynomially many critical points and finally, "trivializes"' to exactly one minimum. This phenomenon, known as topology trivialization in the physics literature, can be leveraged to devise annealing schemes for additive noise such that the training starts in the polynomial regime but gradually morphs the energy landscape into the original one as training progresses. We demonstrate through experiments on fully-connected and convolutional neural networks that annealing schemes based on trivialization lead to accelerated training and also improve generalization error. version:4
arxiv-1602-02442 | A Simple Practical Accelerated Method for Finite Sums | http://arxiv.org/abs/1602.02442 | id:1602.02442 author:Aaron Defazio category:stat.ML cs.LG  published:2016-02-08 summary:We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems, matching the conjectured optimal rate. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. version:1
arxiv-1602-02434 | Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization | http://arxiv.org/abs/1602.02434 | id:1602.02434 author:Shervin Minaee, Yao Wang category:cs.CV  published:2016-02-07 summary:Sparse decomposition has been widely used for different applications, such as source separation, image classification, image denoising and more. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition and total variation minimization. The proposed method is designed based on the assumption that the background part of the image is smoothly varying and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics can be modeled with a sparse component overlaid on the smooth background. The background and foreground are separated using a sparse decomposition framework regularized with a few suitable regularization terms which promotes the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to have superior performance over some prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm. version:1
arxiv-1512-05294 | Feature Representation for ICU Mortality | http://arxiv.org/abs/1512.05294 | id:1512.05294 author:Harini Suresh category:cs.AI cs.LG stat.ML  published:2015-12-16 summary:Good predictors of ICU Mortality have the potential to identify high-risk patients earlier, improve ICU resource allocation, or create more accurate population-level risk models. Machine learning practitioners typically make choices about how to represent features in a particular model, but these choices are seldom evaluated quantitatively. This study compares the performance of different representations of clinical event data from MIMIC II in a logistic regression model to predict 36-hour ICU mortality. The most common representations are linear (normalized counts) and binary (yes/no). These, along with a new representation termed "hill", are compared using both L1 and L2 regularization. Results indicate that the introduced "hill" representation outperforms both the binary and linear representations, the hill representation thus has the potential to improve existing models of ICU mortality. version:2
arxiv-1511-06744 | Training CNNs with Low-Rank Filters for Efficient Image Classification | http://arxiv.org/abs/1511.06744 | id:1511.06744 author:Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, Antonio Criminisi category:cs.CV cs.LG cs.NE  published:2015-11-20 summary:We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank representations of convolutional filters. Rather than approximating filters in previously-trained networks with more efficient versions, we learn a set of small basis filters from scratch; during training, the network learns to combine these basis filters into more complex filters that are discriminative for image classification. To train such networks, a novel weight initialization scheme is used. This allows effective initialization of connection weights in convolutional layers composed of groups of differently-shaped filters. We validate our approach by applying it to several existing CNN architectures and training these networks from scratch using the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or higher accuracy than conventional CNNs with much less compute. Applying our method to an improved version of VGG-11 network using global max-pooling, we achieve comparable validation accuracy using 41% less compute and only 24% of the original VGG-11 model parameters; another variant of our method gives a 1 percentage point increase in accuracy over our improved VGG-11 model, giving a top-5 center-crop validation accuracy of 89.7% while reducing computation by 16% relative to the original VGG-11 model. Applying our method to the GoogLeNet architecture for ILSVRC, we achieved comparable accuracy with 26% less compute and 41% fewer model parameters. Applying our method to a near state-of-the-art network for CIFAR, we achieved comparable accuracy with 46% less compute and 55% fewer parameters. version:3
arxiv-1510-02777 | Early Inference in Energy-Based Models Approximates Back-Propagation | http://arxiv.org/abs/1510.02777 | id:1510.02777 author:Yoshua Bengio, Asja Fischer category:cs.LG  published:2015-10-09 summary:We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is back-propagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Back-propagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as back-propagation does. In this theory, the continuous-valued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time. version:2
arxiv-1506-02371 | Interpretable Selection and Visualization of Features and Interactions Using Bayesian Forests | http://arxiv.org/abs/1506.02371 | id:1506.02371 author:Viktoriya Krakovna, Jiong Du, Jun S. Liu category:stat.ML  published:2015-06-08 summary:It is becoming increasingly important for machine learning methods to make predictions that are interpretable as well as accurate. In many practical applications, it is of interest which features and feature interactions are relevant to the prediction task. We present a novel method, Selective Bayesian Forest Classifier, that strikes a balance between predictive power and interpretability by simultaneously performing classification, feature selection, feature interaction detection and visualization. It builds parsimonious yet flexible models using tree-structured Bayesian networks, and samples an ensemble of such models using Markov chain Monte Carlo. We build in feature selection by dividing the trees into two groups according to their relevance to the outcome of interest. Our method performs competitively on classification and feature selection benchmarks in low and high dimensions, and includes a visualization tool that provides insight into relevant features and interactions. version:4
arxiv-1509-01240 | Train faster, generalize better: Stability of stochastic gradient descent | http://arxiv.org/abs/1509.01240 | id:1509.01240 author:Moritz Hardt, Benjamin Recht, Yoram Singer category:cs.LG math.OC stat.ML  published:2015-09-03 summary:We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit. version:2
arxiv-1403-6348 | Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams | http://arxiv.org/abs/1403.6348 | id:1403.6348 author:Blaz Sovdat category:cs.AI cs.LG I.2.6  published:2014-03-25 summary:Despite growing interest in data stream mining the most successful incremental learners, such as VFDT, still use periodic recomputation to update attribute information gains and Gini indices. This note provides simple incremental formulas and algorithms for computing entropy and Gini index from time-changing data streams. version:5
arxiv-1507-00473 | The Optimal Sample Complexity of PAC Learning | http://arxiv.org/abs/1507.00473 | id:1507.00473 author:Steve Hanneke category:cs.LG stat.ML  published:2015-07-02 summary:This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon. version:4
arxiv-1511-04839 | Nonparametric Canonical Correlation Analysis | http://arxiv.org/abs/1511.04839 | id:1511.04839 author:Tomer Michaeli, Weiran Wang, Karen Livescu category:cs.LG stat.ML  published:2015-11-16 summary:Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA. version:4
arxiv-1602-02386 | Network Inference by Learned Node-Specific Degree Prior | http://arxiv.org/abs/1602.02386 | id:1602.02386 author:Qingming Tang, Lifu Tu, Weiran Wang, Jinbo Xu category:stat.ML cs.LG  published:2016-02-07 summary:We propose a novel method for network inference from partially observed edges using a node-specific degree prior. The degree prior is derived from observed edges in the network to be inferred, and its hyper-parameters are determined by cross validation. Then we formulate network inference as a matrix completion problem regularized by our degree prior. Our theoretical analysis indicates that this prior favors a network following the learned degree distribution, and may lead to improved network recovery error bound than previous work. Experimental results on both simulated and real biological networks demonstrate the superior performance of our method in various settings. version:1
arxiv-1602-02383 | Disentangled Representations in Neural Models | http://arxiv.org/abs/1602.02383 | id:1602.02383 author:William Whitney category:cs.LG cs.NE  published:2016-02-07 summary:Representation learning is the foundation for the recent success of neural network models. However, the distributed representations generated by neural networks are far from ideal. Due to their highly entangled nature, they are di cult to reuse and interpret, and they do a poor job of capturing the sparsity which is present in real- world transformations. In this paper, I describe methods for learning disentangled representations in the two domains of graphics and computation. These methods allow neural methods to learn representations which are easy to interpret and reuse, yet they incur little or no penalty to performance. In the Graphics section, I demonstrate the ability of these methods to infer the generating parameters of images and rerender those images under novel conditions. In the Computation section, I describe a model which is able to factorize a multitask learning problem into subtasks and which experiences no catastrophic forgetting. Together these techniques provide the tools to design a wide range of models that learn disentangled representations and better model the factors of variation in the real world. version:1
arxiv-1509-03877 | Learning Contextual Dependencies with Convolutional Hierarchical Recurrent Neural Networks | http://arxiv.org/abs/1509.03877 | id:1509.03877 author:Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingxing Wang, Bing Wang category:cs.CV  published:2015-09-13 summary:Existing deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependencies among different image regions. However, such dependencies are very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information among sequential data, and they only require a limited number of network parameters. General RNNs can hardly be directly applied on non-sequential data. Thus, we proposed the hierarchical RNNs (HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two recurrent neural network models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost; and 2) hierarchical long-short term memory recurrent network (HLSTM), which performs better than HSRN with the price of more computational cost. In this manuscript, we integrate CNNs with HRNNs, and develop end-to-end convolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs not only make use of the representation power of CNNs, but also efficiently encodes spatial and scale dependencies among different image regions. On four of the most challenging object/scene image classification benchmarks, our C-HRNNs achieve state-of-the-art results on Places 205, SUN 397, MIT indoor, and competitive results on ILSVRC 2012. version:2
arxiv-1602-02334 | ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution | http://arxiv.org/abs/1602.02334 | id:1602.02334 author:Zeinab Bahmani, Leopoldo Bertossi, Nikolaos Vasiloglou category:cs.DB cs.AI cs.LG  published:2016-02-07 summary:Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called "matching dependencies" (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating four components of ER: (a) Building a classifier for duplicate/non-duplicate record pairs built using machine learning (ML) techniques; (b) Use of MDs for supporting the blocking phase of ML; (c) Record merging on the basis of the classifier results; and (d) The use of the declarative language "LogiQL" -an extended form of Datalog supported by the "LogicBlox" platform- for all activities related to data processing, and the specification and enforcement of MDs. version:1
arxiv-1602-02332 | Scalable Text Mining with Sparse Generative Models | http://arxiv.org/abs/1602.02332 | id:1602.02332 author:Antti Puurula category:cs.IR cs.AI cs.CL  published:2016-02-07 summary:The information age has brought a deluge of data. Much of this is in text form, insurmountable in scope for humans and incomprehensible in structure for computers. Text mining is an expanding field of research that seeks to utilize the information contained in vast document collections. General data mining methods based on machine learning face challenges with the scale of text data, posing a need for scalable text mining methods. This thesis proposes a solution to scalable text mining: generative models combined with sparse computation. A unifying formalization for generative text models is defined, bringing together research traditions that have used formally equivalent models, but ignored parallel developments. This framework allows the use of methods developed in different processing tasks such as retrieval and classification, yielding effective solutions across different text mining tasks. Sparse computation using inverted indices is proposed for inference on probabilistic models. This reduces the computational complexity of the common text mining operations according to sparsity, yielding probabilistic models with the scalability of modern search engines. The proposed combination provides sparse generative models: a solution for text mining that is general, effective, and scalable. Extensive experimentation on text classification and ranked retrieval datasets are conducted, showing that the proposed solution matches or outperforms the leading task-specific methods in effectiveness, with a order of magnitude decrease in classification times for Wikipedia article categorization with a million classes. The developed methods were further applied in two 2014 Kaggle data mining prize competitions with over a hundred competing teams, earning first and second places. version:1
arxiv-1602-01103 | Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions | http://arxiv.org/abs/1602.01103 | id:1602.01103 author:Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian Lee category:cs.SI cs.CL physics.soc-ph  published:2016-02-02 summary:Changing someone's opinion is arguably one of the most important challenges of social interaction. The underlying process proves difficult to study: it is hard to know how someone's opinions are formed and whether and how someone's views shift. Fortunately, ChangeMyView, an active community on Reddit, provides a platform where users present their own opinions and reasoning, invite others to contest them, and acknowledge when the ensuing discussions change their original views. In this work, we study these interactions to understand the mechanisms behind persuasion. We find that persuasive arguments are characterized by interesting patterns of interaction dynamics, such as participant entry-order and degree of back-and-forth exchange. Furthermore, by comparing similar counterarguments to the same opinion, we show that language factors play an essential role. In particular, the interplay between the language of the opinion holder and that of the counterargument provides highly predictive cues of persuasiveness. Finally, since even in this favorable setting people may not be persuaded, we investigate the problem of determining whether someone's opinion is susceptible to being changed at all. For this more difficult task, we show that stylistic choices in how the opinion is expressed carry predictive power. version:2
arxiv-1309-7821 | MPBART - Multinomial Probit Bayesian Additive Regression Trees | http://arxiv.org/abs/1309.7821 | id:1309.7821 author:Bereket P. Kindo, Hao Wang, Edsel A. Peña category:stat.ML  published:2013-09-30 summary:This article proposes Multinomial Probit Bayesian Additive Regression Trees (MPBART) as a multinomial probit extension of BART - Bayesian Additive Regression Trees (Chipman et al (2010)). MPBART is flexible to allow inclusion of predictors that describe the observed units as well as the available choice alternatives. Through two simulation studies and four real data examples, we show that MPBART exhibits very good predictive performance in comparison to other discrete choice and multiclass classification methods. To implement MPBART, we have developed an R package mpbart available freely from CRAN repositories. version:2
arxiv-1602-02285 | A Deep Learning Approach to Unsupervised Ensemble Learning | http://arxiv.org/abs/1602.02285 | id:1602.02285 author:Uri Shaham, Xiuyuan Cheng, Omer Dror, Ariel Jaffe, Boaz Nadler, Joseph Chang, Yuval Kluger category:stat.ML cs.LG  published:2016-02-06 summary:We show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning. First, we prove that the popular model of Dawid and Skene, which assumes that all classifiers are conditionally independent, is {\em equivalent} to a Restricted Boltzmann Machine (RBM) with a single hidden node. Hence, under this model, the posterior probabilities of the true labels can be instead estimated via a trained RBM. Next, to address the more general case, where classifiers may strongly violate the conditional independence assumption, we propose to apply RBM-based Deep Neural Net (DNN). Experimental results on various simulated and real-world datasets demonstrate that our proposed DNN approach outperforms other state-of-the-art methods, in particular when the data violates the conditional independence assumption. version:1
arxiv-1602-02283 | Importance Sampling for Minibatches | http://arxiv.org/abs/1602.02283 | id:1602.02283 author:Dominik Csiba, Peter Richtárik category:cs.LG math.OC stat.ML  published:2016-02-06 summary:Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling -- a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first {\em importance sampling for minibatches} and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular datasets, and show that the improvement can reach an order of magnitude. version:1
arxiv-1602-02263 | DOLPHIn - Dictionary Learning for Phase Retrieval | http://arxiv.org/abs/1602.02263 | id:1602.02263 author:Andreas M. Tillmann, Yonina C. Eldar, Julien Mairal category:math.OC cs.IT cs.LG math.IT stat.ML  published:2016-02-06 summary:We propose a new algorithm to learn a dictionary for reconstructing and sparsely encoding signals from measurements without phase. Specifically, we consider the task of estimating a two-dimensional image from squared-magnitude measurements of a complex-valued linear transformation of the original image. Several recent phase retrieval algorithms exploit underlying sparsity of the unknown signal in order to improve recovery performance. In this work, we consider such a sparse signal prior in the context of phase retrieval, when the sparsifying dictionary is not known in advance. Our algorithm jointly reconstructs the unknown signal - possibly corrupted by noise - and learns a dictionary such that each patch of the estimated image can be sparsely represented. Numerical experiments demonstrate that our approach can obtain significantly better reconstructions for phase retrieval problems with noise than methods that cannot exploit such "hidden" sparsity. Moreover, on the theoretical side, we provide a convergence result for our method. version:1
arxiv-1602-02262 | Recovery guarantee of weighted low-rank approximation via alternating minimization | http://arxiv.org/abs/1602.02262 | id:1602.02262 author:Yuanzhi Li, Yingyu Liang, Andrej Risteski category:cs.LG cs.DS stat.ML  published:2016-02-06 summary:Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries. In practice, this is typically formulated as weighted low-rank approximation problem and solved using non-convex optimization heuristics such as alternating minimization. Such non-convex techniques have little guarantees. Even worse, weighted low-rank approximation is NP-hard for even the most simple case when the ground truth is a rank-1 matrix. In this paper, we provide provable recovery guarantee in polynomial time for a natural class of matrices and weights. In particular, we bound the spectral norm of the difference between the recovered matrix and the ground truth, by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization. This provides the first theoretical result for weighted low-rank approximation via alternating minimization with non-binary deterministic weights. It is a significant generalization of the results for matrix completion, the special case with binary weights, since our assumptions are similar or weaker than those made in existing works. The key technical challenge is that under non-binary deterministic weights, naive alternating minimization steps will destroy the incoherency and spectral properties of the intermediate solution, which are needed for making progress towards the ground truth. One of our key technical contributions is a whitening step that maintains these properties of the intermediate solution after each round, which may be applied to alternating minimization for other problems and thus is of independent interest. version:1
arxiv-1602-02256 | A Tractable Fully Bayesian Method for the Stochastic Block Model | http://arxiv.org/abs/1602.02256 | id:1602.02256 author:Kohei Hayashi, Takuya Konishi, Tatsuro Kawamoto category:cs.LG stat.ML  published:2016-02-06 summary:The stochastic block model (SBM) is a generative model revealing macroscopic structures in graphs. Bayesian methods are used for (i) cluster assignment inference and (ii) model selection for the number of clusters. In this paper, we study the behavior of Bayesian inference in the SBM in the large sample limit. Combining variational approximation and Laplace's method, a consistent criterion of the fully marginalized log-likelihood is established. Based on that, we derive a tractable algorithm that solves tasks (i) and (ii) concurrently, obviating the need for an outer loop to check all model candidates. Our empirical and theoretical results demonstrate that our method is scalable in computation, accurate in approximation, and concise in model selection. version:1
arxiv-1602-02237 | Reducing training requirements through evolutionary based dimension reduction and subject transfer | http://arxiv.org/abs/1602.02237 | id:1602.02237 author:Adham Atyabi, Martin Luerssena, Sean P. Fitzgibbon, Trent Lewis, David M. W. Powersa category:cs.NE  published:2016-02-06 summary:Training Brain Computer Interface (BCI) systems to understand the intention of a subject through Electroencephalogram (EEG) data currently requires multiple training sessions with a subject in order to develop the necessary expertise to distinguish signals for different tasks. Conventionally the task of training the subject is done by introducing a training and calibration stage during which some feedback is presented to the subject. This training session can take several hours which is not appropriate for on-line EEG-based BCI systems. An alternative approach is to use previous recording sessions of the same person or some other subjects that performed the same tasks (subject transfer) for training the classifiers. The main aim of this study is to generate a methodology that allows the use of data from other subjects while reducing the dimensions of the data. The study investigates several possibilities for reducing the necessary training and calibration period in subjects and the classifiers and addresses the impact of i) evolutionary subject transfer and ii) adapting previously trained methods (retraining) using other subjects data. Our results suggest reduction to 40% of target subject data is sufficient for training the classifier. Our results also indicate the superiority of the approaches that incorporated evolutionary subject transfer and highlights the feasibility of adapting a system trained on other subjects. version:1
arxiv-1510-03519 | Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning | http://arxiv.org/abs/1510.03519 | id:1510.03519 author:Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran category:cs.CL  published:2015-10-13 summary:Recently there has been a lot of interest in learning common representations for multiple views of data. These views could belong to different modalities or languages. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, V1 and V2) but parallel data is available between each of these views and a pivot view (V3). We propose a model for learning a common representation for V1, V2 and V3 using only the parallel data available between V1V3 and V2V3. The proposed model is generic and even works when there are n views of interest and only one pivot view which acts as a bridge between them. There are two specific downstream applications that we focus on (i) Transfer learning between languages L1,L2,...,Ln using a pivot language L and (ii) cross modal access between images and a language L1 using a pivot language L2. We evaluate our model using two datasets : (i) publicly available multilingual TED corpus and (ii) a new multilingual multimodal dataset created and released as a part of this work. On both these datasets, our model outperforms state of the art approaches. version:2
arxiv-1602-02220 | Improved Dropout for Shallow and Deep Learning | http://arxiv.org/abs/1602.02220 | id:1602.02220 author:Zhe Li, Boqing Gong, Tianbao Yang category:cs.LG stat.ML  published:2016-02-06 summary:Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named \textbf{evolutional dropout}) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10\% on the prediction performance and over 50\% on the convergence speed compared to the standard dropout. version:1
arxiv-1602-02219 | Variational Hamiltonian Monte Carlo via Score Matching | http://arxiv.org/abs/1602.02219 | id:1602.02219 author:Cheng Zhang, Babak Shahbaba, Hongkai Zhao category:stat.CO stat.ML  published:2016-02-06 summary:Traditionally, the field of computational Bayesian statistics has been divided into two main subfields: variational methods and Markov chain Monte Carlo (MCMC). In recent years, however, several methods have been proposed based on combining variational Bayesian inference and MCMC simulation in order to improve their overall accuracy and computational efficiency. This marriage of fast evaluation and flexible approximation provides a promising means of designing scalable Bayesian inference methods. In this paper, we explore the possibility of incorporating variational approximation into a state-of-the-art MCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradient computation in the simulation of Hamiltonian flow, which is the bottleneck for many applications of HMC in big data problems. To this end, we use a {\it free-form} approximation induced by a fast and flexible surrogate function based on single-hidden layer feedforward neural networks. The surrogate provides sufficiently accurate approximation while allowing for fast exploration of parameter space, resulting in an efficient approximate inference algorithm. We demonstrate the advantages of our method on both synthetic and real data problems. version:1
arxiv-1602-02215 | Swivel: Improving Embeddings by Noticing What's Missing | http://arxiv.org/abs/1602.02215 | id:1602.02215 author:Noam Shazeer, Ryan Doherty, Colin Evans, Chris Waterson category:cs.CL  published:2016-02-06 summary:We present Submatrix-wise Vector Embedding Learner (Swivel), a method for generating low-dimensional feature embeddings from a feature co-occurrence matrix. Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. While this requires computation proportional to the size of the entire matrix, we make use of vectorized multiplication to process thousands of rows and columns at once to compute millions of predicted values. Furthermore, we partition the matrix into shards in order to parallelize the computation across many nodes. This approach results in more accurate embeddings than can be achieved with methods that consider only observed co-occurrences, and can scale to much larger corpora than can be handled with sampling methods. version:1
arxiv-1602-02210 | Classification Accuracy as a Proxy for Two Sample Testing | http://arxiv.org/abs/1602.02210 | id:1602.02210 author:Aaditya Ramdas, Aarti Singh, Larry Wasserman category:cs.LG cs.AI math.ST stat.ML stat.TH  published:2016-02-06 summary:When data analysts train a classifier and check if its accuracy is significantly different from random guessing, they are implicitly and indirectly performing a hypothesis test (two sample testing) and it is of importance to ask whether this indirect method for testing is statistically optimal or not. Given that hypothesis tests attempt to maximize statistical power subject to a bound on the allowable false positive rate, while prediction attempts to minimize statistical risk on future predictions on unseen data, we wish to study whether a predictive approach for an ultimate aim of testing is prudent. We formalize this problem by considering the two-sample mean-testing setting where one must determine if the means of two Gaussians (with known and equal covariance) are the same or not, but the analyst indirectly does so by checking whether the accuracy achieved by Fisher's LDA classifier is significantly different from chance or not. Unexpectedly, we find that the asymptotic power of LDA's sample-splitting classification accuracy is actually minimax rate-optimal in terms of problem-dependent parameters. Since prediction is commonly thought to be harder than testing, it might come as a surprise to some that solving a harder problem does not create a information-theoretic bottleneck for the easier one. On the flip side, even though the power is rate-optimal, our derivation suggests that it may be worse by a small constant factor; hence practitioners must be wary of using (admittedly flexible) prediction methods on disguised testing problems. version:1
arxiv-1602-02196 | BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits | http://arxiv.org/abs/1602.02196 | id:1602.02196 author:Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML  published:2016-02-06 summary:We present efficient algorithms for the problem of contextual bandits with i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of policies. Our algorithm BISTRO requires d calls to the empirical risk minimization (ERM) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the ERM problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the ERM. The integrality gap of the relaxation only enters in the regret bound rather than the benchmark. Finally, we show that the adversarial version of the contextual bandit problem is learnable (and efficient) whenever the full-information supervised online learning problem has a non-trivial regret guarantee (and efficient). version:1
arxiv-1602-02181 | Active Information Acquisition | http://arxiv.org/abs/1602.02181 | id:1602.02181 author:He He, Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG  published:2016-02-05 summary:We propose a general framework for sequential and dynamic acquisition of useful information in order to solve a particular task. While our goal could in principle be tackled by general reinforcement learning, our particular setting is constrained enough to allow more efficient algorithms. In this paper, we work under the Learning to Search framework and show how to formulate the goal of finding a dynamic information acquisition policy in that framework. We apply our formulation on two tasks, sentiment analysis and image recognition, and show that the learned policies exhibit good statistical performance. As an emergent byproduct, the learned policies show a tendency to focus on the most prominent parts of each instance and give harder instances more attention without explicitly being trained to do so. version:1
arxiv-1602-02172 | On Column Selection in Approximate Kernel Canonical Correlation Analysis | http://arxiv.org/abs/1602.02172 | id:1602.02172 author:Weiran Wang category:cs.LG stat.ML  published:2016-02-05 summary:We study the problem of column selection in large-scale kernel canonical correlation analysis (KCCA) using the Nystr\"om approximation, where one approximates two positive semi-definite kernel matrices using "landmark" points from the training set. When building low-rank kernel approximations in KCCA, previous work mostly samples the landmarks uniformly at random from the training set. We propose novel strategies for sampling the landmarks non-uniformly based on a version of statistical leverage scores recently developed for kernel ridge regression. We study the approximation accuracy of the proposed non-uniform sampling strategy, develop an incremental algorithm that explores the path of approximation ranks and facilitates efficient model selection, and derive the kernel stability of out-of-sample mapping for our method. Experimental results on both synthetic and real-world datasets demonstrate the promise of our method. version:1
arxiv-1602-02164 | A Note on Alternating Minimization Algorithm for the Matrix Completion Problem | http://arxiv.org/abs/1602.02164 | id:1602.02164 author:David Gamarnik, Sidhant Misra category:stat.ML cs.LG cs.NA  published:2016-02-05 summary:We consider the problem of reconstructing a low rank matrix from a subset of its entries and analyze two variants of the so-called Alternating Minimization algorithm, which has been proposed in the past. We establish that when the underlying matrix has rank $r=1$, has positive bounded entries, and the graph $\mathcal{G}$ underlying the revealed entries has bounded degree and diameter which is at most logarithmic in the size of the matrix, both algorithms succeed in reconstructing the matrix approximately in polynomial time starting from an arbitrary initialization. We further provide simulation results which suggest that the second algorithm which is based on the message passing type updates, performs significantly better. version:1
arxiv-1602-02159 | Daleel: Simplifying Cloud Instance Selection Using Machine Learning | http://arxiv.org/abs/1602.02159 | id:1602.02159 author:Faiza Samreen, Yehia Elkhatib, Matthew Rowe, Gordon S. Blair category:cs.DC cs.LG cs.PF  published:2016-02-05 summary:Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules, each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure. version:1
arxiv-1602-02151 | Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters | http://arxiv.org/abs/1602.02151 | id:1602.02151 author:Zeyuan Allen-Zhu, Yang Yuan, Karthik Sridharan category:cs.LG stat.ML  published:2016-02-05 summary:The amount of data available in the world is growing faster and bigger than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the most fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently obtained with just one pass of the data, and propose two algorithms. Our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using the clustering information, and our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of each cluster. Our algorithms outperform their classical counterparts both in theory and practice. version:1
arxiv-1602-02136 | Reducing Runtime by Recycling Samples | http://arxiv.org/abs/1602.02136 | id:1602.02136 author:Jialei Wang, Hai Wang, Nathan Srebro category:cs.LG stat.ML  published:2016-02-05 summary:Contrary to the situation with stochastic gradient descent, we argue that when using stochastic methods with variance reduction, such as SDCA, SAG or SVRG, as well as their variants, it could be beneficial to reuse previously used samples instead of fresh samples, even when fresh samples are available. We demonstrate this empirically for SDCA, SAG and SVRG, studying the optimal sample size one should use, and also uncover be-havior that suggests running SDCA for an integer number of epochs could be wasteful. version:1
arxiv-1602-02133 | Mining Software Quality from Software Reviews: Research Trends and Open Issues | http://arxiv.org/abs/1602.02133 | id:1602.02133 author:Issa Atoum, Ahmed Otoom category:cs.CL cs.IR  published:2016-02-05 summary:Software review text fragments have considerably valuable information about users experience. It includes a huge set of properties including the software quality. Opinion mining or sentiment analysis is concerned with analyzing textual user judgments. The application of sentiment analysis on software reviews can find a quantitative value that represents software quality. Although many software quality methods are proposed they are considered difficult to customize and many of them are limited. This article investigates the application of opinion mining as an approach to extract software quality properties. We found that the major issues of software reviews mining using sentiment analysis are due to software lifecycle and the diverse users and teams. version:1
arxiv-1602-02130 | Sub-cortical brain structure segmentation using F-CNN's | http://arxiv.org/abs/1602.02130 | id:1602.02130 author:Mahsa Shakeri, Stavros Tsogkas, Enzo Ferrante, Sarah Lippe, Samuel Kadoury, Nikos Paragios, Iasonas Kokkinos category:cs.CV  published:2016-02-05 summary:In this paper we propose a deep learning approach for segmenting sub-cortical structures of the human brain in Magnetic Resonance (MR) image data. We draw inspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN) architecture for semantic segmentation of objects in natural images, and adapt it to our task. Unlike previous CNN-based methods that operate on image patches, our model is applied on a full blown 2D image, without any alignment or registration steps at testing time. We further improve segmentation results by interpreting the CNN output as potentials of a Markov Random Field (MRF), whose topology corresponds to a volumetric grid. Alpha-expansion is used to perform approximate inference imposing spatial volumetric homogeneity to the CNN priors. We compare the performance of the proposed pipeline with a similar system using Random Forest-based priors, as well as state-of-art segmentation algorithms, and show promising results on two different brain MRI datasets. version:1
arxiv-1602-02123 | Sequence Classification with Neural Conditional Random Fields | http://arxiv.org/abs/1602.02123 | id:1602.02123 author:Myriam Abramson category:cs.LG  published:2016-02-05 summary:The proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multi-sensor fusion algorithms. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models provide different categorization of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained if we calibrate the class membership estimate of the sequence. We introduce and compare different neural network based linear-chain CRFs and we present experiments on two complex sequence classification and structured prediction tasks to support this claim. version:1
arxiv-1602-02114 | Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities | http://arxiv.org/abs/1602.02114 | id:1602.02114 author:Adrien Todeschini, François Caron category:stat.ME cs.SI physics.soc-ph stat.ML  published:2016-02-05 summary:We propose a novel statistical model for sparse networks with overlapping community structure. The model is based on representing the graph as an exchangeable point process, and naturally generalizes existing probabilistic models with overlapping block-structure to the sparse regime. Our construction builds on vectors of completely random measures, and has interpretable parameters, each node being assigned a vector representing its level of affiliation to some latent communities. We develop methods for simulating this class of random graphs, as well as to perform posterior inference. We show that the proposed approach can recover interpretable structure from two real-world networks and can handle graphs with thousands of nodes and tens of thousands of edges. version:1
arxiv-1511-09120 | Low-cost and Faster Tracking Systems Using Core-sets for Pose-Estimation | http://arxiv.org/abs/1511.09120 | id:1511.09120 author:Soliman Nasser, Ibrahim Jubran, Dan Feldman category:cs.RO cs.CV  published:2015-11-30 summary:How can a \$20 toy quadcopter hover using a weak "Internet of Things" mini-board and a web-cam? In the pose-estimation problem we need to rotate a set of $n$ marker (points) and choose one of their n! permutations, so that the sum of squared corresponding distances to another ordered set of $n$ markers is minimized. A popular heuristic for this problem is ICP. We prove that \emph{every} set has a weighted subset (core-set) of constant size (independent of $n$), such that computing the optimal orientation of the small core-set would yield \emph{exactly} the same result as using the full set of $n$ markers. A deterministic algorithm for computing this core-set in $O(n)$ time is provided, using the Caratheodory Theorem from computational geometry. We developed a system that enables low-cost and real-time tracking by computing this core-set on the cloud in one thread, and uses the last computed core-set locally in a parallel thread. Our experimental results show how these core-sets can boost the tracking time and quality for large and even small sets of both IR and RGB markers on a toy quadcopter. Open source code for the system and algorithm is provided version:2
arxiv-1602-02101 | Variance-Reduced and Projection-Free Stochastic Optimization | http://arxiv.org/abs/1602.02101 | id:1602.02101 author:Elad Hazan, Haipeng Luo category:cs.LG  published:2016-02-05 summary:The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve $1-\epsilon$ accuracy. For example, we improve from $O(\frac{1}{\epsilon})$ to $O(\ln\frac{1}{\epsilon})$ if the objective function is smooth and strongly convex, and from $O(\frac{1}{\epsilon^2})$ to $O(\frac{1}{\epsilon^{1.5}})$ if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application. version:1
arxiv-1602-02089 | Harmonic Grammar in a DisCo Model of Meaning | http://arxiv.org/abs/1602.02089 | id:1602.02089 author:Martha Lewis, Bob Coecke category:cs.AI cs.CL  published:2016-02-05 summary:The model of cognition developed in (Smolensky and Legendre, 2006) seeks to unify two levels of description of the cognitive process: the connectionist and the symbolic. The theory developed brings together these two levels into the Integrated Connectionist/Symbolic Cognitive architecture (ICS). Clark and Pulman (2007) draw a parallel with semantics where meaning may be modelled on both distributional and symbolic levels, developed by Coecke et al, 2010 into the Distributional Compositional (DisCo) model of meaning. In the current work, we revisit Smolensky and Legendre (S&L)'s model. We describe the DisCo framework, summarise the key ideas in S&L's architecture, and describe how their description of harmony as a graded measure of grammaticality may be applied in the DisCo model. version:1
arxiv-1602-02047 | Utilização de Grafos e Matriz de Similaridade na Sumarização Automática de Documentos Baseada em Extração de Frases | http://arxiv.org/abs/1602.02047 | id:1602.02047 author:Elvys Linhares Pontes category:cs.CL cs.IR  published:2016-02-05 summary:The internet increased the amount of information available. However, the reading and understanding of this information are costly tasks. In this scenario, the Natural Language Processing (NLP) applications enable very important solutions, highlighting the Automatic Text Summarization (ATS), which produce a summary from one or more source texts. Automatically summarizing one or more texts, however, is a complex task because of the difficulties inherent to the analysis and generation of this summary. This master's thesis describes the main techniques and methodologies (NLP and heuristics) to generate summaries. We have also addressed and proposed some heuristics based on graphs and similarity matrix to measure the relevance of judgments and to generate summaries by extracting sentences. We used the multiple languages (English, French and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA (French) corpus to evaluate the developped systems. The results obtained were quite interesting. version:1
arxiv-1602-02023 | Efficient Multi-view Performance Capture of Fine-Scale Surface Detail | http://arxiv.org/abs/1602.02023 | id:1602.02023 author:Nadia Robertini, Edilson De Aguiar, Thomas Helten, Christian Theobalt category:cs.CV cs.GR  published:2016-02-05 summary:We present a new effective way for performance capture of deforming meshes with fine-scale time-varying surface detail from multi-view video. Our method builds up on coarse 4D surface reconstructions, as obtained with commonly used template-based methods. As they only capture models of coarse-to-medium scale detail, fine scale deformation detail is often done in a second pass by using stereo constraints, features, or shading-based refinement. In this paper, we propose a new effective and stable solution to this second step. Our framework creates an implicit representation of the deformable mesh using a dense collection of 3D Gaussian functions on the surface, and a set of 2D Gaussians for the images. The fine scale deformation of all mesh vertices that maximizes photo-consistency can be efficiently found by densely optimizing a new model-to-image consistency energy on all vertex positions. A principal advantage is that our problem formulation yields a smooth closed form energy with implicit occlusion handling and analytic derivatives. Error-prone correspondence finding, or discrete sampling of surface displacement values are also not needed. We show several reconstructions of human subjects wearing loose clothing, and we qualitatively and quantitatively show that we robustly capture more detail than related methods. version:1
arxiv-1602-02022 | Preoperative Volume Determination for Pituitary Adenoma | http://arxiv.org/abs/1602.02022 | id:1602.02022 author:Dzenan Zukic, Jan Egger, Miriam H. A. Bauer, Daniela Kuhnt, Barbara Carl, Bernd Freisleben, Andreas Kolb, Christopher Nimsky category:cs.CV cs.CG cs.GR  published:2016-02-05 summary:The most common sellar lesion is the pituitary adenoma, and sellar tumors are approximately 10-15% of all intracranial neoplasms. Manual slice-by-slice segmentation takes quite some time that can be reduced by using the appropriate algorithms. In this contribution, we present a segmentation method for pituitary adenoma. The method is based on an algorithm that we have applied recently to segmenting glioblastoma multiforme. A modification of this scheme is used for adenoma segmentation that is much harder to perform, due to lack of contrast-enhanced boundaries. In our experimental evaluation, neurosurgeons performed manual slice-by-slice segmentation of ten magnetic resonance imaging (MRI) cases. The segmentations were compared to the segmentation results of the proposed method using the Dice Similarity Coefficient (DSC). The average DSC for all datasets was 75.92% +/- 7.24%. A manual segmentation took about four minutes and our algorithm required about one second. version:1
arxiv-1508-07709 | Word Representations, Tree Models and Syntactic Functions | http://arxiv.org/abs/1508.07709 | id:1508.07709 author:Simon Šuster, Gertjan van Noord, Ivan Titov category:cs.CL cs.LG stat.ML  published:2015-08-31 summary:Word representations induced from models with discrete latent variables (e.g.\ HMMs) have been shown to be beneficial in many NLP applications. In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models. Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components. Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications. We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification. We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods. Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident. version:2
arxiv-1602-01940 | Automatic and Quantitative evaluation of attribute discovery methods | http://arxiv.org/abs/1602.01940 | id:1602.01940 author:Liangchen Liu, Arnold Wiliem, Shaokang Chen, Brian C. Lovell category:cs.CV  published:2016-02-05 summary:Many automatic attribute discovery methods have been developed to extract a set of visual attributes from images for various tasks. However, despite good performance in some image classification tasks, it is difficult to evaluate whether these methods discover meaningful attributes and which one is the best to find the attributes for image descriptions. An intuitive way to evaluate this is to manually verify whether consistent identifiable visual concepts exist to distinguish between positive and negative images of an attribute. This manual checking is tedious, labor intensive and expensive and it is very hard to get quantitative comparisons between different methods. In this work, we tackle this problem by proposing an attribute meaningfulness metric, that can perform automatic evaluation on the meaningfulness of attribute sets as well as achieving quantitative comparisons. We apply our proposed metric to recent automatic attribute discovery methods and popular hashing methods on three attribute datasets. A user study is also conducted to validate the effectiveness of the metric. In our evaluation, we gleaned some insights that could be beneficial in developing automatic attribute discovery methods to generate meaningful attributes. To the best of our knowledge, this is the first work to quantitatively measure the semantic content of automatically discovered attributes. version:1
arxiv-1408-2313 | Bags of Affine Subspaces for Robust Object Tracking | http://arxiv.org/abs/1408.2313 | id:1408.2313 author:Sareh Shirazi, Conrad Sanderson, Chris McCool, Mehrtash T. Harandi category:cs.CV cs.MM cs.RO 14M15  54B05  published:2014-08-11 summary:We propose an adaptive tracking algorithm where the object is modelled as a continuously updated bag of affine subspaces, with each subspace constructed from the object's appearance over several consecutive frames. In contrast to linear subspaces, affine subspaces explicitly model the origin of subspaces. Furthermore, instead of using a brittle point-to-subspace distance during the search for the object in a new frame, we propose to use a subspace-to-subspace distance by representing candidate image areas also as affine subspaces. Distances between subspaces are then obtained by exploiting the non-Euclidean geometry of Grassmann manifolds. Experiments on challenging videos (containing object occlusions, deformations, as well as variations in pose and illumination) indicate that the proposed method achieves higher tracking accuracy than several recent discriminative trackers. version:3
arxiv-1602-01827 | Face Attribute Prediction with classification CNN | http://arxiv.org/abs/1602.01827 | id:1602.01827 author:Yang Zhong, Josephine Sullivan, Haibo Li category:cs.CV  published:2016-02-04 summary:Predicting facial attributes from faces in the wild is very challenging due to pose and lighting variations in the real world. The key to this problem is to build proper feature representations to cope with these unfavorable conditions. Given the success of convolutional neural network (CNN) in image classification, the high-level CNN feature as an intuitive and reasonable choice has been widely utilized for this problem. In this paper, however, we consider the mid-level CNN features as an alternative to the high-level ones for attribute prediction. This is based on the observation that face attributes are different: some of them are locally oriented while others are globally defined. Our investigations reveal that the mid-level deep representations outperform the prediction accuracy achieved by the high-level abstractions. We demonstrate that the mid-level representations achieve state-of-the-art prediction performance on CelebA and LFWA datasets. Our investigations also show that by utilizing the mid-level representations one can employ a single deep network to achieve both face recognition and attribute prediction. version:2
arxiv-1602-01929 | Fantastic 4 system for NIST 2015 Language Recognition Evaluation | http://arxiv.org/abs/1602.01929 | id:1602.01929 author:Kong Aik Lee, Ville Hautamäki, Anthony Larcher, Wei Rao, Hanwu Sun, Trung Hieu Nguyen, Guangsen Wang, Aleksandr Sizov, Ivan Kukanov, Amir Poorjam, Trung Ngo Trong, Xiong Xiao, Cheng-Lin Xu, Hai-Hua Xu, Bin Ma, Haizhou Li, Sylvain Meignier category:cs.CL  published:2016-02-05 summary:This article describes the systems jointly submitted by Institute for Infocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\'e du Maine (LIUM), Nanyang Technology University (NTU) and the University of Eastern Finland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The submitted system is a fusion of nine sub-systems based on i-vectors extracted from different types of features. Given the i-vectors, several classifiers are adopted for the language detection task including support vector machines (SVM), multi-class logistic regression (MCLR), Probabilistic Linear Discriminant Analysis (PLDA) and Deep Neural Networks (DNN). version:1
arxiv-1602-01927 | On Feature based Delaunay Triangulation for Palmprint Recognition | http://arxiv.org/abs/1602.01927 | id:1602.01927 author:Zanobya N. Khan, Rashid Jalal Qureshi, Jamil Ahmad category:cs.CV  published:2016-02-05 summary:Authentication of individuals via palmprint based biometric system is becoming very popular due to its reliability as it contains unique and stable features. In this paper, we present a novel approach for palmprint recognition and its representation. To extract the palm lines, local thresholding technique Niblack binarization algorithm is adopted. The endpoints of these lines are determined and a connection is created among them using the Delaunay triangulation thereby generating a distinct topological structure of each palmprint. Next, we extract different geometric as well as quantitative features from the triangles of the Delaunay triangulation that assist in identifying different individuals. To ensure that the proposed approach is invariant to rotation and scaling, features were made relative to topological and geometrical structure of the palmprint. The similarity of the two palmprints is computed using the weighted sum approach and compared with the k-nearest neighbor. The experimental results obtained reflect the effectiveness of the proposed approach to discriminate between different palmprint images and thus achieved a recognition rate of 90% over large databases. version:1
arxiv-1304-2810 | High-dimensional Mixed Graphical Models | http://arxiv.org/abs/1304.2810 | id:1304.2810 author:Jie Cheng, Tianxi Li, Elizaveta Levina, Ji Zhu category:stat.ML stat.ME  published:2013-04-09 summary:While graphical models for continuous data (Gaussian graphical models) and discrete data (Ising models) have been extensively studied, there is little work on graphical models linking both continuous and discrete variables (mixed data), which are common in many scientific applications. We propose a novel graphical model for mixed data, which is simple enough to be suitable for high-dimensional data, yet flexible enough to represent all possible graph structures. We develop a computationally efficient regression-based algorithm for fitting the model by focusing on the conditional log-likelihood of each variable given the rest. The parameters have a natural group structure, and sparsity in the fitted graph is attained by incorporating a group lasso penalty, approximated by a weighted $\ell_1$ penalty for computational efficiency. We demonstrate the effectiveness of our method through an extensive simulation study and apply it to a music annotation data set (CAL500), obtaining a sparse and interpretable graphical model relating the continuous features of the audio signal to categorical variables such as genre, emotions, and usage associated with particular songs. While we focus on binary discrete variables, we also show that the proposed methodology can be easily extended to general discrete variables. version:2
arxiv-1602-01921 | Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model | http://arxiv.org/abs/1602.01921 | id:1602.01921 author:Haanvid Lee, Minju Jung, Jun Tani category:cs.CV cs.AI cs.LG  published:2016-02-05 summary:The current paper proposes a novel dynamic neural network model, multiple spatio-temporal scales recurrent neural network (MSTRNN) used for categorization of complex human action pattern in video image. The MSTRNN has been developed by newly introducing recurrent connectivity to a prior-proposed model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the model can learn to extract latent spatio-temporal structures more effectively by developing adequate recurrent contextual dynamics. The MSTRNN was evaluated by conducting a set of simulation experiments on learning to categorize human action visual patterns. The first experiment on categorizing a set of long-concatenated human movement patterns showed that MSTRNN outperforms MSTNN in the capability of learning to extract long-ranged correlation in video image. The second experiment on categorizing a set of object-directed actions showed that the MSTRNN can learn to extract structural relationship between actions and directed-objects. Our analysis on the characteristics of miscategorization in both cases of object-directed action and pantomime actions indicated that the model network developed the categorical memories by organizing relational structure among them. Development of such relational structure is considered to be beneficial for gaining generalization in categorization. version:1
arxiv-1602-01910 | Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping Clustering | http://arxiv.org/abs/1602.01910 | id:1602.01910 author:Yangyang Hou, Joyce Jiyoung Whang, David F. Gleich, Inderjit S. Dhillon category:cs.LG  published:2016-02-05 summary:Clustering is one of the most fundamental and important tasks in data mining. Traditional clustering algorithms, such as K-means, assign every data point to exactly one cluster. However, in real-world datasets, the clusters may overlap with each other. Furthermore, often, there are outliers that should not belong to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a way to address both issues in an integrated fashion. Optimizing this discrete objective is NP-hard, and even though there is a convex relaxation of the objective, straightforward convex optimization approaches are too expensive for large datasets. A practical alternative is to use a low-rank factorization of the solution matrix in the convex formulation. The resulting optimization problem is non-convex, and we can locally optimize the objective function using an augmented Lagrangian method. In this paper, we consider two fast multiplier methods to accelerate the convergence of an augmented Lagrangian scheme: a proximal method of multipliers and an alternating direction method of multipliers (ADMM). For the proximal augmented Lagrangian or proximal method of multipliers, we show a convergence result for the non-convex case with bound-constrained subproblems. These methods are up to 13 times faster---with no change in quality---compared with a standard augmented Lagrangian method on problems with over 10,000 variables and bring runtimes down from over an hour to around 5 minutes. version:1
arxiv-1501-05352 | Optimizing affinity-based binary hashing using auxiliary coordinates | http://arxiv.org/abs/1501.05352 | id:1501.05352 author:Ramin Raziperchikolaei, Miguel Á. Carreira-Perpiñán category:cs.LG cs.CV math.OC stat.ML  published:2015-01-21 summary:In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions. version:2
arxiv-1602-01895 | Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features | http://arxiv.org/abs/1602.01895 | id:1602.01895 author:Shijian Tang, Song Han category:cs.CV cs.CL cs.LG  published:2016-02-05 summary:Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores. version:1
arxiv-1602-01890 | Search Tracker: Human-derived object tracking in-the-wild through large-scale search and retrieval | http://arxiv.org/abs/1602.01890 | id:1602.01890 author:Archith J. Bency, S. Karthikeyan, Carter De Leo, Santhoshkumar Sunderrajan, B. S. Manjunath category:cs.CV cs.MM  published:2016-02-05 summary:Humans use context and scene knowledge to easily localize moving objects in conditions of complex illumination changes, scene clutter and occlusions. In this paper, we present a method to leverage human knowledge in the form of annotated video libraries in a novel search and retrieval based setting to track objects in unseen video sequences. For every video sequence, a document that represents motion information is generated. Documents of the unseen video are queried against the library at multiple scales to find videos with similar motion characteristics. This provides us with coarse localization of objects in the unseen video. We further adapt these retrieved object locations to the new video using an efficient warping scheme. The proposed method is validated on in-the-wild video surveillance datasets where we outperform state-of-the-art appearance-based trackers. We also introduce a new challenging dataset with complex object appearance changes. version:1
arxiv-1601-04114 | Training Recurrent Neural Networks by Diffusion | http://arxiv.org/abs/1601.04114 | id:1601.04114 author:Hossein Mobahi category:cs.LG  published:2016-01-16 summary:This work presents a new algorithm for training recurrent neural networks (although ideas are applicable to feedforward networks as well). The algorithm is derived from a theory in nonconvex optimization related to the diffusion equation. The contributions made in this work are two fold. First, we show how some seemingly disconnected mechanisms used in deep learning such as smart initialization, annealed learning rate, layerwise pretraining, and noise injection (as done in dropout and SGD) arise naturally and automatically from this framework, without manually crafting them into the algorithms. Second, we present some preliminary results on comparing the proposed method against SGD. It turns out that the new algorithm can achieve similar level of generalization accuracy of SGD in much fewer number of epochs. version:2
arxiv-1509-08535 | Boolean Matrix Factorization and Noisy Completion via Message Passing | http://arxiv.org/abs/1509.08535 | id:1509.08535 author:Siamak Ravanbakhsh, Barnabas Poczos, Russell Greiner category:math.ST cs.AI cs.DM stat.ML stat.TH  published:2015-09-28 summary:Boolean matrix factorization and Boolean matrix completion from noisy observations are desirable unsupervised data-analysis methods due to their interpretability, but hard to perform due to their NP-hardness. We treat these problems as maximum a posteriori inference problems in a graphical model and present a message passing approach that scales linearly with the number of observations and factors. Our empirical study demonstrates that message passing is able to recover low-rank Boolean matrices, in the boundaries of theoretically possible recovery and compares favorably with state-of-the-art in real-world applications, such collaborative filtering with large-scale Boolean data. version:3
arxiv-1602-01818 | Random Feature Maps via a Layered Random Projection (LaRP) Framework for Object Classification | http://arxiv.org/abs/1602.01818 | id:1602.01818 author:A. G. Chung, M. J. Shafiee, A. Wong category:cs.CV cs.LG stat.ML  published:2016-02-04 summary:The approximation of nonlinear kernels via linear feature maps has recently gained interest due to their applications in reducing the training and testing time of kernel-based learning algorithms. Current random projection methods avoid the curse of dimensionality by embedding the nonlinear feature space into a low dimensional Euclidean space to create nonlinear kernels. We introduce a Layered Random Projection (LaRP) framework, where we model the linear kernels and nonlinearity separately for increased training efficiency. The proposed LaRP framework was assessed using the MNIST hand-written digits database and the COIL-100 object database, and showed notable improvement in object classification performance relative to other state-of-the-art random projection methods. version:1
arxiv-1510-07025 | Modeling User Exposure in Recommendation | http://arxiv.org/abs/1510.07025 | id:1510.07025 author:Dawen Liang, Laurent Charlin, James McInerney, David M. Blei category:stat.ML cs.IR cs.LG  published:2015-10-23 summary:Collaborative filtering analyzes user preferences for items (e.g., books, movies, restaurants, academic papers) by exploiting the similarity patterns across users. In implicit feedback settings, all the items, including the ones that a user did not consume, are taken into consideration. But this assumption does not accord with the common sense understanding that users have a limited scope and awareness of items. For example, a user might not have heard of a certain paper, or might live too far away from a restaurant to experience it. In the language of causal analysis, the assignment mechanism (i.e., the items that a user is exposed to) is a latent variable that may change for various user/item combinations. In this paper, we propose a new probabilistic approach that directly incorporates user exposure to items into collaborative filtering. The exposure is modeled as a latent variable and the model infers its value from data. In doing so, we recover one of the most successful state-of-the-art approaches as a special case of our model, and provide a plug-in method for conditioning exposure on various forms of exposure covariates (e.g., topics in text, venue locations). We show that our scalable inference algorithm outperforms existing benchmarks in four different domains both with and without exposure covariates. version:2
arxiv-1506-02025 | Spatial Transformer Networks | http://arxiv.org/abs/1506.02025 | id:1506.02025 author:Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu category:cs.CV  published:2015-06-05 summary:Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations. version:3
arxiv-1602-01729 | Correntropy Maximization via ADMM - Application to Robust Hyperspectral Unmixing | http://arxiv.org/abs/1602.01729 | id:1602.01729 author:Fei Zhu, Abderrahim Halimi, Paul Honeine, Badong Chen, Nanning Zheng category:stat.ML cs.CV cs.NE  published:2016-02-04 summary:In hyperspectral images, some spectral bands suffer from low signal-to-noise ratio due to noisy acquisition and atmospheric effects, thus requiring robust techniques for the unmixing problem. This paper presents a robust supervised spectral unmixing approach for hyperspectral images. The robustness is achieved by writing the unmixing problem as the maximization of the correntropy criterion subject to the most commonly used constraints. Two unmixing problems are derived: the first problem considers the fully-constrained unmixing, with both the non-negativity and sum-to-one constraints, while the second one deals with the non-negativity and the sparsity-promoting of the abundances. The corresponding optimization problems are solved efficiently using an alternating direction method of multipliers (ADMM) approach. Experiments on synthetic and real hyperspectral images validate the performance of the proposed algorithms for different scenarios, demonstrating that the correntropy-based unmixing is robust to outlier bands. version:1
arxiv-1602-01728 | NeRD: a Neural Response Divergence Approach to Visual Salience Detection | http://arxiv.org/abs/1602.01728 | id:1602.01728 author:M. J. Shafiee, P. Siva, C. Scharfenberger, P. Fieguth, A. Wong category:cs.CV  published:2016-02-04 summary:In this paper, a novel approach to visual salience detection via Neural Response Divergence (NeRD) is proposed, where synaptic portions of deep neural networks, previously trained for complex object recognition, are leveraged to compute low level cues that can be used to compute image region distinctiveness. Based on this concept , an efficient visual salience detection framework is proposed using deep convolutional StochasticNets. Experimental results using CSSD and MSRA10k natural image datasets show that the proposed NeRD approach can achieve improved performance when compared to state-of-the-art image saliency approaches, while the attaining low computational complexity necessary for near-real-time computer vision applications. version:1
arxiv-1602-01711 | The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version | http://arxiv.org/abs/1602.01711 | id:1602.01711 author:Anthony Bagnall, Aaron Bostrom, James Large, Jason Lines category:cs.LG  published:2016-02-04 summary:In the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future. version:1
arxiv-1602-04853 | Complex Networks of Words in Fables | http://arxiv.org/abs/1602.04853 | id:1602.04853 author:Yurij Holovatch, Vasyl Palchykov category:physics.soc-ph cs.CL  published:2016-02-04 summary:In this chapter we give an overview of the application of complex network theory to quantify some properties of language. Our study is based on two fables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists of two parts: the analysis of frequency-rank distributions of words and the application of complex-network theory. The first part shows that the text sizes are sufficiently large to observe statistical properties. This supports their selection for the analysis of typical properties of the language networks in the second part of the chapter. In describing language as a complex network, while words are usually associated with nodes, there is more variability in the choice of links and different representations result in different networks. Here, we examine a number of such representations of the language network and perform a comparative analysis of their characteristics. Our results suggest that, irrespective of link representation, the Ukrainian language network used in the selected fables is a strongly correlated, scale-free, small world. We discuss how such empirical approaches may help form a useful basis for a theoretical description of language evolution and how they may be used in analyses of other textual narratives. version:1
arxiv-1602-00955 | Unsupervised High-level Feature Learning by Ensemble Projection for Semi-supervised Image Classification and Image Clustering | http://arxiv.org/abs/1602.00955 | id:1602.00955 author:Dengxin Dai, Luc Van Gool category:cs.CV  published:2016-02-02 summary:This paper investigates the problem of image classification with limited or no annotations, but abundant unlabeled data. The setting exists in many tasks such as semi-supervised image classification, image clustering, and image retrieval. Unlike previous methods, which develop or learn sophisticated regularizers for classifiers, our method learns a new image representation by exploiting the distribution patterns of all available data for the task at hand. Particularly, a rich set of visual prototypes are sampled from all available data, and are taken as surrogate classes to train discriminative classifiers; images are projected via the classifiers; the projected values, similarities to the prototypes, are stacked to build the new feature vector. The training set is noisy. Hence, in the spirit of ensemble learning we create a set of such training sets which are all diverse, leading to diverse classifiers. The method is dubbed Ensemble Projection (EP). EP captures not only the characteristics of individual images, but also the relationships among images. It is conceptually simple and computationally efficient, yet effective and flexible. Experiments on eight standard datasets show that: (1) EP outperforms previous methods for semi-supervised image classification; (2) EP produces promising results for self-taught image classification, where unlabeled samples are a random collection of images rather than being from the same distribution as the labeled ones; and (3) EP improves over the original features for image clustering. The code of the method is available on the project page. version:2
arxiv-1602-01644 | A semi-automatic computer-aided method for surgical template design | http://arxiv.org/abs/1602.01644 | id:1602.01644 author:Xiaojun Chen, Lu Xu, Yue Yang, Jan Egger category:cs.GR cs.CG cs.CV  published:2016-02-04 summary:This paper presents a generalized integrated framework of semi-automatic surgical template design. Several algorithms were implemented including the mesh segmentation, offset surface generation, collision detection, ruled surface generation, etc., and a special software named TemDesigner was developed. With a simple user interface, a customized template can be semi- automatically designed according to the preoperative plan. Firstly, mesh segmentation with signed scalar of vertex is utilized to partition the inner surface from the input surface mesh based on the indicated point loop. Then, the offset surface of the inner surface is obtained through contouring the distance field of the inner surface, and segmented to generate the outer surface. Ruled surface is employed to connect inner and outer surfaces. Finally, drilling tubes are generated according to the preoperative plan through collision detection and merging. It has been applied to the template design for various kinds of surgeries, including oral implantology, cervical pedicle screw insertion, iliosacral screw insertion and osteotomy, demonstrating the efficiency, functionality and generality of our method. version:1
arxiv-1602-01635 | A Generalised Quantifier Theory of Natural Language in Categorical Compositional Distributional Semantics with Bialgebras | http://arxiv.org/abs/1602.01635 | id:1602.01635 author:Jules Hedges, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT I.2.7  published:2016-02-04 summary:Categorical compositional distributional semantics is a model of natural language; it combines the statistical vector space models of words with the compositional models of grammar. We formalise in this model the generalised quantifier theory of natural language, due to Barwise and Cooper. The underlying setting is a compact closed category with bialgebras. We start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it, then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps. We prove the equivalence of the relational instantiation to the truth theoretic semantics of generalized quantifiers. The vector space instantiation formalises the statistical usages of words and enables us to, for the first time, reason about quantified phrases and sentences compositionally in distributional semantics. version:1
arxiv-1602-01625 | Self-Transfer Learning for Fully Weakly Supervised Object Localization | http://arxiv.org/abs/1602.01625 | id:1602.01625 author:Sangheum Hwang, Hyo-Eun Kim category:cs.CV  published:2016-02-04 summary:Recent advances of deep learning have achieved remarkable performances in various challenging computer vision tasks. Especially in object localization, deep convolutional neural networks outperform traditional approaches based on extraction of data/task-driven features instead of hand-crafted features. Although location information of region-of-interests (ROIs) gives good prior for object localization, it requires heavy annotation efforts from human resources. Thus a weakly supervised framework for object localization is introduced. The term "weakly" means that this framework only uses image-level labeled datasets to train a network. With the help of transfer learning which adopts weight parameters of a pre-trained network, the weakly supervised learning framework for object localization performs well because the pre-trained network already has well-trained class-specific features. However, those approaches cannot be used for some applications which do not have pre-trained networks or well-localized large scale images. Medical image analysis is a representative among those applications because it is impossible to obtain such pre-trained networks. In this work, we present a "fully" weakly supervised framework for object localization ("semi"-weakly is the counterpart which uses pre-trained filters for weakly supervised localization) named as self-transfer learning (STL). It jointly optimizes both classification and localization networks simultaneously. By controlling a supervision level of the localization network, STL helps the localization network focus on correct ROIs without any types of priors. We evaluate the proposed STL framework using two medical image datasets, chest X-rays and mammograms, and achieve signiticantly better localization performance compared to previous weakly supervised approaches. version:1
arxiv-1511-00830 | The Variational Fair Autoencoder | http://arxiv.org/abs/1511.00830 | id:1511.00830 author:Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, Richard Zemel category:stat.ML cs.LG  published:2015-11-03 summary:We investigate the problem of learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational autoencoding architecture with priors that encourage independence between sensitive and latent factors of variation. Any subsequent processing, such as classification, can then be performed on this purged latent representation. To remove any remaining dependencies we incorporate an additional penalty term based on the "Maximum Mean Discrepancy" (MMD) measure. We discuss how these architectures can be efficiently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations. version:5
arxiv-1602-01616 | Fpga Based Implementation of Deep Neural Networks Using On-chip Memory Only | http://arxiv.org/abs/1602.01616 | id:1602.01616 author:Jinhwan Park, Wonyong Sung category:cs.AR cs.NE  published:2016-02-04 summary:Deep neural networks (DNNs) demand a very large amount of computation and weight storage, and thus efficient implementation using special purpose hardware is highly desired. In this work, we have developed an FPGA based fixed-point DNN system using only on-chip memory not to access external DRAM. The execution time and energy consumption of the developed system is compared with a GPU based implementation. Since the capacity of memory in FPGA is limited, only 3-bit weights are used for this implementation, and training based fixed-point weight optimization is employed. The implementation using Xilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmark and a phoneme recognition task on TIMIT corpus. The obtained speed is about one quarter of a GPU based implementation and much better than that of a PC based one. The power consumption is less than 5 Watt at the full speed operation resulting in much higher efficiency compared to GPU based systems. version:1
arxiv-1602-01601 | Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors | http://arxiv.org/abs/1602.01601 | id:1602.01601 author:Johanna Carvajal, Chris McCool, Brian Lovell, Conrad Sanderson category:cs.CV  published:2016-02-04 summary:We propose a hierarchical approach to multi-action recognition that performs joint classification and segmentation. A given video (containing several consecutive actions) is processed via a sequence of overlapping temporal windows. Each frame in a temporal window is represented through selective low-level spatio-temporal features which efficiently capture relevant local dynamics. Features from each window are represented as a Fisher vector, which captures first and second order statistics. Instead of directly classifying each Fisher vector, it is converted into a vector of class probabilities. The final classification decision for each frame is then obtained by integrating the class probabilities at the frame level, which exploits the overlapping of the temporal windows. Experiments were performed on two datasets: s-KTH (a stitched version of the KTH dataset to simulate multi-actions), and the challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an accuracy of 85.0%, significantly outperforming two recent approaches based on GMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, the proposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMM approaches which obtained 33.7% and 38.4%, respectively. Furthermore, the proposed system is on average 40 times faster than the GMM based approach. version:1
arxiv-1602-01599 | Comparative Evaluation of Action Recognition Methods via Riemannian Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions | http://arxiv.org/abs/1602.01599 | id:1602.01599 author:Johanna Carvajal, Arnold Wiliem, Chris McCool, Brian Lovell, Conrad Sanderson category:cs.CV I.4; I.5; I.5.4  published:2016-02-04 summary:We present a comparative evaluation of various techniques for action recognition while keeping as many variables as possible controlled. We employ two categories of Riemannian manifolds: symmetric positive definite matrices and linear subspaces. For both categories we use their corresponding nearest neighbour classifiers, kernels, and recent kernelised sparse representations. We compare against traditional action recognition techniques based on Gaussian mixture models and Fisher vectors (FVs). We evaluate these action recognition techniques under ideal conditions, as well as their sensitivity in more challenging conditions (variations in scale and translation). Despite recent advancements for handling manifolds, manifold based techniques obtain the lowest performance and their kernel representations are more unstable in the presence of challenging conditions. The FV approach obtains the highest accuracy under ideal conditions. Moreover, FV best deals with moderate scale and translation changes. version:1
arxiv-1510-05067 | How Important is Weight Symmetry in Backpropagation? | http://arxiv.org/abs/1510.05067 | id:1510.05067 author:Qianli Liao, Joel Z. Leibo, Tomaso Poggio category:cs.LG  published:2015-10-17 summary:Gradient backpropagation (BP) requires symmetric feedforward and feedback connections -- the same weights must be used for forward and backward passes. This "weight transport problem" (Grossberg 1987) is thought to be one of the main reasons to doubt BP's biologically plausibility. Using 15 different classification datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter -- the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a "Batch Manhattan" (BM) update rule. version:4
arxiv-1602-01580 | Long-term Planning by Short-term Prediction | http://arxiv.org/abs/1602.01580 | id:1602.01580 author:Shai Shalev-Shwartz, Nir Ben-Zrihem, Aviad Cohen, Amnon Shashua category:cs.LG  published:2016-02-04 summary:We consider planning problems, that often arise in autonomous driving applications, in which an agent should decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the $Q$ function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment. version:1
arxiv-1602-01576 | A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling | http://arxiv.org/abs/1602.01576 | id:1602.01576 author:Anantharaman Palacod category:cs.CL cs.AI  published:2016-02-04 summary:Statistical language models are central to many applications that use semantics. Recurrent Neural Networks (RNN) are known to produce state of the art results for language modelling, outperforming their traditional n-gram counterparts in many cases. To generate a probability distribution across a vocabulary, these models require a softmax output layer that linearly increases in size with the size of the vocabulary. Large vocabularies need a commensurately large softmax layer and training them on typical laptops/PCs requires significant time and machine resources. In this paper we present a new technique for implementing RNN based large vocabulary language models that substantially speeds up computation while optimally using the limited memory resources. Our technique, while building on the notion of factorizing the output layer by having multiple output layers, improves on the earlier work by substantially optimizing on the individual output layer size and also eliminating the need for a multistep prediction process. version:1
arxiv-1410-2455 | BilBOWA: Fast Bilingual Distributed Representations without Word Alignments | http://arxiv.org/abs/1410.2455 | id:1410.2455 author:Stephan Gouws, Yoshua Bengio, Greg Corrado category:stat.ML cs.CL cs.LG  published:2014-10-09 summary:We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data. version:3
arxiv-1601-07648 | A Grassmannian Graph Approach to Affine Invariant Feature Matching | http://arxiv.org/abs/1601.07648 | id:1601.07648 author:Mark Moyou, John Corring, Adrian Peter, Anand Rangarajan category:cs.CV  published:2016-01-28 summary:In this work, we present a novel and practical approach to address one of the longstanding problems in computer vision: 2D and 3D affine invariant feature matching. Our Grassmannian Graph (GrassGraph) framework employs a two stage procedure that is capable of robustly recovering correspondences between two unorganized, affinely related feature (point) sets. The first stage maps the feature sets to an affine invariant Grassmannian representation, where the features are mapped into the same subspace. It turns out that coordinate representations extracted from the Grassmannian differ by an arbitrary orthonormal matrix. In the second stage, by approximating the Laplace-Beltrami operator (LBO) on these coordinates, this extra orthonormal factor is nullified, providing true affine-invariant coordinates which we then utilize to recover correspondences via simple nearest neighbor relations. The resulting GrassGraph algorithm is empirically shown to work well in non-ideal scenarios with noise, outliers, and occlusions. Our validation benchmarks use an unprecedented 440,000+ experimental trials performed on 2D and 3D datasets, with a variety of parameter settings and competing methods. State-of-the-art performance in the majority of these extensive evaluations confirm the utility of our method. version:2
arxiv-1602-01557 | An ensemble diversity approach to supervised binary hashing | http://arxiv.org/abs/1602.01557 | id:1602.01557 author:Miguel Á. Carreira-Perpiñán, Ramin Raziperchikolaei category:cs.LG cs.CV math.OC stat.ML  published:2016-02-04 summary:Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval. version:1
arxiv-1602-01541 | Fundamental Limits in Multi-image Alignment | http://arxiv.org/abs/1602.01541 | id:1602.01541 author:Cecilia Aguerrebere, Mauricio Delbracio, Alberto Bartesaghi, Guillermo Sapiro category:cs.CV  published:2016-02-04 summary:The performance of multi-image alignment, bringing different images into one coordinate system, is critical in many applications with varied signal-to-noise ratio (SNR) conditions. A great amount of effort is being invested into developing methods to solve this problem. Several important questions thus arise, including: Which are the fundamental limits in multi-image alignment performance? Does having access to more images improve the alignment? Theoretical bounds provide a fundamental benchmark to compare methods and can help establish whether improvements can be made. In this work, we tackle the problem of finding the performance limits in image registration when multiple shifted and noisy observations are available. We derive and analyze the Cram\'er-Rao and Ziv-Zakai lower bounds under different statistical models for the underlying image. The accuracy of the derived bounds is experimentally assessed through a comparison to the maximum likelihood estimator. We show the existence of different behavior zones depending on the difficulty level of the problem, given by the SNR conditions of the input images. We find that increasing the number of images is only useful below a certain SNR threshold, above which the pairwise MLE estimation proves to be optimal. The analysis we present here brings further insight into the fundamental limitations of the multi-image alignment problem. version:1
arxiv-1511-02503 | Bearing fault diagnosis based on spectrum images of vibration signals | http://arxiv.org/abs/1511.02503 | id:1511.02503 author:Wei Li, Mingquan Qiu, Zhencai Zhu, Bo Wu, Gongbo Zhou category:cs.CV cs.SD  published:2015-11-08 summary:Bearing fault diagnosis has been a challenge in the monitoring activities of rotating machinery, and it's receiving more and more attention. The conventional fault diagnosis methods usually extract features from the waveforms or spectrums of vibration signals in order to realize fault classification. In this paper, a novel feature in the form of images is presented, namely the spectrum images of vibration signals. The spectrum images are simply obtained by doing fast Fourier transformation. Such images are processed with two-dimensional principal component analysis (2DPCA) to reduce the dimensions, and then a minimum distance method is applied to classify the faults of bearings. The effectiveness of the proposed method is verified with experimental data. version:5
arxiv-1506-08909 | The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems | http://arxiv.org/abs/1506.08909 | id:1506.08909 author:Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE  published:2015-06-30 summary:This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response. version:3
arxiv-1512-09194 | Exploiting Local Structures with the Kronecker Layer in Convolutional Networks | http://arxiv.org/abs/1512.09194 | id:1512.09194 author:Shuchang Zhou, Jia-Nan Wu, Yuxin Wu, Xinyu Zhou category:cs.CV  published:2015-12-31 summary:In this paper, we propose and study a technique to reduce the number of parameters and computation time in convolutional neural networks. We use Kronecker product to exploit the local structures within convolution and fully-connected layers, by replacing the large weight matrices by combinations of multiple Kronecker products of smaller matrices. Just as the Kronecker product is a generalization of the outer product from vectors to matrices, our method is a generalization of the low rank approximation method for convolution neural networks. We also introduce combinations of different shapes of Kronecker product to increase modeling capacity. Experiments on SVHN, scene text recognition and ImageNet dataset demonstrate that we can achieve $3.3 \times$ speedup or $3.6 \times$ parameter reduction with less than 1\% drop in accuracy, showing the effectiveness and efficiency of our method. Moreover, the computation efficiency of Kronecker layer makes using larger feature map possible, which in turn enables us to outperform the previous state-of-the-art on both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character recognition) datasets. version:2
arxiv-1511-03260 | A Hierarchical Spectral Method for Extreme Classification | http://arxiv.org/abs/1511.03260 | id:1511.03260 author:Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG  published:2015-11-10 summary:Extreme classification problems are multiclass and multilabel classification problems where the number of outputs is so large that straightforward strategies are neither statistically nor computationally viable. One strategy for dealing with the computational burden is via a tree decomposition of the output space. While this typically leads to training and inference that scales sublinearly with the number of outputs, it also results in reduced statistical performance. In this work, we identify two shortcomings of tree decomposition methods, and describe two heuristic mitigations. We compose these with an eigenvalue technique for constructing the tree. The end result is a computationally efficient algorithm that provides good statistical performance on several extreme data sets. version:4
arxiv-1602-01522 | Risk estimation for high-dimensional lasso regression | http://arxiv.org/abs/1602.01522 | id:1602.01522 author:Darren Homrighausen, Daniel J. McDonald category:stat.ME stat.ML  published:2016-02-04 summary:In high-dimensional estimation, analysts are faced with more parameters $p$ than available observations $n$, and asymptotic analysis of performance allows the ratio $p/n\rightarrow \infty$. This situation makes regularization both necessary and desirable in order for estimators to possess theoretical guarantees. However, the amount of regularization, often determined by one or more tuning parameters, is integral to achieving good performance. In practice, choosing the tuning parameter is done through resampling methods (e.g. cross-validation), generalized information criteria, or reformulating the optimization problem (e.g. square-root lasso or scaled sparse regression). Each of these techniques comes with varying levels of theoretical guarantee for the low- or high-dimensional regimes. However, there are some notable deficiencies in the literature. The theory, and sometimes practice, of many methods relies on either the knowledge or estimation of the variance parameter, which is difficult to estimate in high dimensions. In this paper, we provide theoretical intuition suggesting that some previously proposed approaches based on information criteria work poorly in high dimensions. We introduce a suite of new risk estimators leveraging the burgeoning literature on high-dimensional variance estimation. Finally, we compare our proposal to many existing methods for choosing the tuning parameters for lasso regression by providing an extensive simulation to examine their finite sample performance. We find that our new estimators perform quite well, often better than the existing approaches across a wide range of simulation conditions and evaluation criteria. version:1
arxiv-1602-01517 | Towards Better Exploiting Convolutional Neural Networks for Remote Sensing Scene Classification | http://arxiv.org/abs/1602.01517 | id:1602.01517 author:Keiller Nogueira, Otávio A. B. Penatti, Jefersson A. dos Santos category:cs.CV  published:2016-02-04 summary:We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to obtain the best profit from existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used. version:1
arxiv-1602-01510 | Unsupervised Regenerative Learning of Hierarchical Features in Spiking Deep Networks for Object Recognition | http://arxiv.org/abs/1602.01510 | id:1602.01510 author:Priyadarshini Panda, Kaushik Roy category:cs.NE  published:2016-02-03 summary:We present a spike-based unsupervised regenerative learning scheme to train Spiking Deep Networks (SpikeCNN) for object recognition problems using biologically realistic leaky integrate-and-fire neurons. The training methodology is based on the Auto-Encoder learning model wherein the hierarchical network is trained layer wise using the encoder-decoder principle. Regenerative learning uses spike-timing information and inherent latencies to update the weights and learn representative levels for each convolutional layer in an unsupervised manner. The features learnt from the final layer in the hierarchy are then fed to an output layer. The output layer is trained with supervision by showing a fraction of the labeled training dataset and performs the overall classification of the input. Our proposed methodology yields 0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparable with state-of-the-art results. The proposed methodology also introduces sparsity in the hierarchical feature representations on account of event-based coding resulting in computationally efficient learning. version:1
arxiv-1602-02139 | A simple method for estimating the fractal dimension from digital images: The compression dimension | http://arxiv.org/abs/1602.02139 | id:1602.02139 author:P. Chamorro-Posada category:cs.GR cs.CV physics.data-an  published:2016-02-03 summary:The fractal structure of real world objects is often analyzed using digital images. In this context, the compression fractal dimension is put forward. It provides a simple method for the direct estimation of the dimension of fractals stored as digital image files. The computational scheme can be implemented using readily available free software. Its simplicity also makes it very interesting for introductory elaborations of basic concepts of fractal geometry, complexity, and information theory. A test of the computational scheme using limited-quality images of well-defined fractal sets obtained from the Internet and free software has been performed. version:1
arxiv-1202-2194 | Efficient statistical classification of satellite measurements | http://arxiv.org/abs/1202.2194 | id:1202.2194 author:Peter Mills category:physics.ao-ph stat.ML  published:2012-02-10 summary:Supervised statistical classification is a vital tool for satellite image processing. It is useful not only when a discrete result, such as feature extraction or surface type, is required, but also for continuum retrievals by dividing the quantity of interest into discrete ranges. Because of the high resolution of modern satellite instruments and because of the requirement for real-time processing, any algorithm has to be fast to be useful. Here we describe an algorithm based on kernel estimation called Adaptive Gaussian Filtering that incorporates several innovations to produce superior efficiency as compared to three other popular methods: k-nearest-neighbour (KNN), Learning Vector Quantization (LVQ) and Support Vector Machines (SVM). This efficiency is gained with no compromises: accuracy is maintained, while estimates of the conditional probabilities are returned. These are useful not only to gauge the accuracy of an estimate in the absence of its true value, but also to re-calibrate a retrieved image and as a proxy for a discretized continuum variable. The algorithm is demonstrated and compared with the other three on a pair of synthetic test classes and to map the waterways of the Netherlands. Software may be found at: http://libagf.sourceforge.net. version:4
arxiv-1206-2248 | Fast Cross-Validation via Sequential Testing | http://arxiv.org/abs/1206.2248 | id:1206.2248 author:Tammo Krueger, Danny Panknin, Mikio Braun category:cs.LG stat.ML I.2.6; I.2.8  published:2012-06-11 summary:With the increasing size of today's data sets, finding the right parameter configuration in model selection via cross-validation can be an extremely time-consuming task. In this paper we propose an improved cross-validation procedure which uses nonparametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating underperforming candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of the full cross-validation. Theoretical considerations underline the statistical power of our procedure. The experimental evaluation shows that our method reduces the computation time by a factor of up to 120 compared to a full cross-validation with a negligible impact on the accuracy. version:6
arxiv-1512-05004 | Towards Cultural-Scale Models of Full Text | http://arxiv.org/abs/1512.05004 | id:1512.05004 author:Jaimie Murdock, Jiaan Zeng, Colin Allen category:cs.DL cs.CL cs.IR  published:2015-12-15 summary:This technical report consists of two components: an administrative report for the HathiTrust Research Center (HTRC) Advanced Collaborative Support (ACS) program and a research report on the variance of topic models trained over random samples of books in the Hathi Trust. Cultural-scale models of full text documents are prone to over-interpretation by researchers making unintentionally strong socio-linguistic claims without recognizing that even large digital libraries are merely samples of all the books ever produced. In this study, we test the sensitivity of the topic models to the sampling process by taking random samples of books in the Hathi Trust Digital Library within different Library of Congress Classification (LCC) areas. For each classification area, we train several topic models over the entire class with different random seeds, generating a set of spanning models. Then, we train topic models on random samples of books from the classification area, generating a set of sample models. Finally, we align topics from the sample models to the spanning models and measure the alignment distance and topic overlap. We find that sample models with a large sample size typically have an alignment distance that falls in the range of the alignment distance between spanning models. Unsurprisingly, as sample size increases, alignment distance decreases. We also find that the topic overlap increases as sample size increases. However, the decomposition of these measures by sample size differs by field and by number of topics. We speculate that these measures could be used to find classes which have a common "canon" discussed among all books in the area, as shown by high topic overlap and low alignment distance even in small sample sizes. version:2
arxiv-1602-01464 | Latent-Class Hough Forests for 6 DoF Object Pose Estimation | http://arxiv.org/abs/1602.01464 | id:1602.01464 author:Rigas Kouskouridas, Alykhan Tejani, Andreas Doumanoglou, Danhang Tang, Tae-Kyun Kim category:cs.CV  published:2016-02-03 summary:In this paper we present Latent-Class Hough Forests, a method for object detection and 6 DoF pose estimation in heavily cluttered and occluded scenarios. We adapt a state of the art template matching feature into a scale-invariant patch descriptor and integrate it into a regression forest using a novel template-based split function. We train with positive samples only and we treat class distributions at the leaf nodes as latent variables. During testing we infer by iteratively updating these distributions, providing accurate estimation of background clutter and foreground occlusions and, thus, better detection rate. Furthermore, as a by-product, our Latent-Class Hough Forests can provide accurate occlusion aware segmentation masks, even in the multi-instance scenario. In addition to an existing public dataset, which contains only single-instance sequences with large amounts of clutter, we have collected two, more challenging, datasets for multiple-instance detection containing heavy 2D and 3D clutter as well as foreground occlusions. We provide extensive experiments on the various parameters of the framework such as patch size, number of trees and number of iterations to infer class distributions at test time. We also evaluate the Latent-Class Hough Forests on all datasets where we outperform state of the art methods. version:1
arxiv-1602-01428 | "Draw My Topics": Find Desired Topics fast from large scale of Corpus | http://arxiv.org/abs/1602.01428 | id:1602.01428 author:Jason Dou, Ni Sun, Xiaojun Zou category:cs.CL cs.IR  published:2016-02-03 summary:We develop the "Draw My Topics" toolkit, which provides a fast way to incorporate social scientists' interest into standard topic modelling. Instead of using raw corpus with primitive processing as input, an algorithm based on Vector Space Model and Conditional Entropy are used to connect social scientists' willingness and unsupervised topic models' output. Space for users' adjustment on specific corpus of their interest is also accommodated. We demonstrate the toolkit's use on the Diachronic People's Daily Corpus in Chinese. version:1
arxiv-1602-01410 | A General Framework for Fast Image Deconvolution with Incomplete Observations. Applications to Unknown Boundaries, Inpainting, Superresolution, and Demosaicing | http://arxiv.org/abs/1602.01410 | id:1602.01410 author:Miguel Simões, Luis B. Almeida, José Bioucas-Dias, Jocelyn Chanussot category:cs.CV stat.ML  published:2016-02-03 summary:In image deconvolution problems, the diagonalization of the underlying operators by means of the FFT usually yields very large speedups. When there are incomplete observations (e.g., in the case of unknown boundaries), standard deconvolution techniques normally involve non-diagonalizable operators---resulting in rather slow methods---or, otherwise, use inexact convolution models, resulting in the occurrence of artifacts in the enhanced images. In this paper, we propose a new deconvolution framework for images with incomplete observations that allows us to work with diagonalized convolution operators, and therefore is very fast. We iteratively alternate the estimation of the unknown pixels and of the deconvolved image, using, e.g., a FFT-based deconvolution method. In principle, any fast deconvolution method can be used. We give an example in which a published method that assumes periodic boundary conditions is extended, through the use of this framework, to unknown boundary conditions. Furthermore, we propose an implementation of this framework, based on the alternating direction method of multipliers (ADMM). We provide a proof of convergence for the resulting algorithm, which can be seen as a "partial" ADMM, in which not all variables are dualized. We report experimental comparisons with other primal-dual methods, in which the proposed one performed at the level of the state of the art. Four different kinds of applications were tested in the experiments: deconvolution, deconvolution with inpainting, superresolution, and demosaicing, all with unknown boundaries. version:1
arxiv-1511-08495 | Incremental Truncated LSTD | http://arxiv.org/abs/1511.08495 | id:1511.08495 author:Clement Gehring, Yangchen Pan, Martha White category:cs.LG cs.AI  published:2015-11-26 summary:Balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning. Temporal difference (TD) learning algorithms stochastically update the value function, with a linear time complexity in the number of features, whereas least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. In this work, we develop an efficient incremental low-rank LSTD({\lambda}) algorithm that progresses towards the goal of better balancing computation and sample efficiency. The algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of LSTD. We derive a simulation bound on the solution given by truncated low-rank approximation, illustrating a bias- variance trade-off dependent on the choice of rank. We demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain. version:2
arxiv-1511-03753 | Shearlet-Based Detection of Flame Fronts | http://arxiv.org/abs/1511.03753 | id:1511.03753 author:Rafael Reisenhofer, Johannes Kiefer, Emily J. King category:cs.CV  published:2015-11-12 summary:Identifying and characterizing flame fronts is the most common task in the computer-assisted analysis of data obtained from imaging techniques such as planar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), or particle imaging velocimetry (PIV). We present a novel edge and ridge (line) detection algorithm based on complex-valued wavelet-like analyzing functions -- so-called complex shearlets -- displaying several traits useful for the extraction of flame fronts. In addition to providing a unified approach to the detection of edges and ridges, our method inherently yields estimates of local tangent orientations and local curvatures. To examine the applicability for high-frequency recordings of combustion processes, the algorithm is applied to mock images distorted with varying degrees of noise and real-world PLIF images of both OH and CH radicals. Furthermore, we compare the performance of the newly proposed complex shearlet-based measure to well-established edge and ridge detection techniques such as the Canny edge detector, another shearlet-based edge detector, and the phase congruency measure. version:2
arxiv-1512-02337 | Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors | http://arxiv.org/abs/1512.02337 | id:1512.02337 author:Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, David Steurer category:cs.DS cs.CC cs.LG stat.ML  published:2015-12-08 summary:We consider two problems that arise in machine learning applications: the problem of recovering a planted sparse vector in a random linear subspace and the problem of decomposing a random low-rank overcomplete 3-tensor. For both problems, the best known guarantees are based on the sum-of-squares method. We develop new algorithms inspired by analyses of the sum-of-squares method. Our algorithms achieve the same or similar guarantees as sum-of-squares for these problems but the running time is significantly faster. For the planted sparse vector problem, we give an algorithm with running time nearly linear in the input size that approximately recovers a planted sparse vector with up to constant relative sparsity in a random subspace of $\mathbb R^n$ of dimension up to $\tilde \Omega(\sqrt n)$. These recovery guarantees match the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to logarithmic factors. For tensor decomposition, we give an algorithm with running time close to linear in the input size (with exponent $\approx 1.086$) that approximately recovers a component of a random 3-tensor over $\mathbb R^n$ of rank up to $\tilde \Omega(n^{4/3})$. The best previous algorithm for this problem due to Ge and Ma (RANDOM 2015) works up to rank $\tilde \Omega(n^{3/2})$ but requires quasipolynomial time. version:2
arxiv-1511-05077 | Diversity Networks | http://arxiv.org/abs/1511.05077 | id:1511.05077 author:Zelda Mariet, Suvrit Sra category:cs.LG cs.NE  published:2015-11-16 summary:We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches. version:5
arxiv-1602-01345 | A Probabilistic Modeling Approach to Hearing Loss Compensation | http://arxiv.org/abs/1602.01345 | id:1602.01345 author:Thijs van de Laar, Bert de Vries category:stat.ML  published:2016-02-03 summary:Hearing Aid (HA) algorithms need to be tuned ("fitted") to match the impairment of each specific patient. The lack of a fundamental HA fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20% of hearing aid patients (Kochkin, 2014). This paper proposes a probabilistic modeling approach to the design of HA algorithms. The proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding (1) signal processing algorithm, (2) the fitting solution as well as a principled (3) performance evaluation metric. All three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on hearing aid or mobile device hardware. The methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model (Zurek, 2007). version:1
arxiv-1602-01323 | Biclustering Readings and Manuscripts via Non-negative Matrix Factorization, with Application to the Text of Jude | http://arxiv.org/abs/1602.01323 | id:1602.01323 author:Joey McCollum, Stephen Brown category:cs.LG 6U815 I.2.7  published:2016-02-03 summary:The text-critical practice of grouping witnesses into families or texttypes often faces two obstacles: Contamination in the manuscript tradition, and co-dependence in identifying characteristic readings and manuscripts. We introduce non-negative matrix factorization (NMF) as a simple, unsupervised, and efficient way to cluster large numbers of manuscripts and readings simultaneously while summarizing contamination using an easy-to-interpret mixture model. We apply this method to an extensive collation of the New Testament epistle of Jude and show that the resulting clusters correspond to human-identified textual families from existing research. version:1
arxiv-1602-01321 | A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks | http://arxiv.org/abs/1602.01321 | id:1602.01321 author:Luke B. Godfrey, Michael S. Gashler category:cs.NE  published:2016-02-03 summary:We present the soft exponential activation function for artificial neural networks that continuously interpolates between logarithmic, linear, and exponential functions. This activation function is simple, differentiable, and parameterized so that it can be trained as the rest of the network is trained. We hypothesize that soft exponential has the potential to improve neural network learning, as it can exactly calculate many natural operations that typical neural networks can only approximate, including addition, multiplication, inner product, distance, polynomials, and sinusoids. version:1
arxiv-1508-01903 | Diffusion Maximum Correntropy Criterion Algorithms for Robust Distributed Estimation | http://arxiv.org/abs/1508.01903 | id:1508.01903 author:Wentao Ma, Badong Chen, Jiandong Duan, Haiquan Zhao category:stat.ML cs.LG  published:2015-08-08 summary:Robust diffusion adaptive estimation algorithms based on the maximum correntropy criterion (MCC), including adaptation to combination MCC and combination to adaptation MCC, are developed to deal with the distributed estimation over network in impulsive (long-tailed) noise environments. The cost functions used in distributed estimation are in general based on the mean square error (MSE) criterion, which is desirable when the measurement noise is Gaussian. In non-Gaussian situations, such as the impulsive-noise case, MCC based methods may achieve much better performance than the MSE methods as they take into account higher order statistics of error distribution. The proposed methods can also outperform the robust diffusion least mean p-power(DLMP) and diffusion minimum error entropy (DMEE) algorithms. The mean and mean square convergence analysis of the new algorithms are also carried out. version:2
arxiv-1410-7365 | Multiple Output Regression with Latent Noise | http://arxiv.org/abs/1410.7365 | id:1410.7365 author:Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J. Kangas, Pasi Soininen, Mehreen Ali, Aki S. Havulinna, Marjo-Riitta Marjo-Riitta Järvelin, Mika Ala-Korpela, Samuel Kaski category:stat.ML  published:2014-10-27 summary:In high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. Therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. Additionally, (2) assumptions about the correlation structure of the regression weights are needed. We note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. Under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. We introduce a hyperparameter for the \emph{latent signal-to-noise ratio} which turns out to be important for modelling weak signals, and an ordered infinite-dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. Simulations and prediction experiments with metabolite, gene expression, FMRI measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models. version:2
arxiv-1602-01248 | Using Hadoop for Large Scale Analysis on Twitter: A Technical Report | http://arxiv.org/abs/1602.01248 | id:1602.01248 author:Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis, Giannis Tzimas category:cs.DB cs.CL cs.IR H.2.4  published:2016-02-03 summary:Sentiment analysis (or opinion mining) on Twitter data has attracted much attention recently. One of the system's key features, is the immediacy in communication with other users in an easy, user-friendly and fast way. Consequently, people tend to express their feelings freely, which makes Twitter an ideal source for accumulating a vast amount of opinions towards a wide diversity of topics. This amount of information offers huge potential and can be harnessed to receive the sentiment tendency towards these topics. However, since none can invest an infinite amount of time to read through these tweets, an automated decision making approach is necessary. Nevertheless, most existing solutions are limited in centralized environments only. Thus, they can only process at most a few thousand tweets. Such a sample, is not representative to define the sentiment polarity towards a topic due to the massive number of tweets published daily. In this paper, we go one step further and develop a novel method for sentiment learning in the MapReduce framework. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner. Moreover, we utilize Bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm. Through an extensive experimental evaluation, we prove that our solution is efficient, robust and scalable and confirm the quality of our sentiment identification. version:1
arxiv-1602-00904 | Comparative evaluation of state-of-the-art algorithms for SSVEP-based BCIs | http://arxiv.org/abs/1602.00904 | id:1602.00904 author:Vangelis P. Oikonomou, Georgios Liaros, Kostantinos Georgiadis, Elisavet Chatzilari, Katerina Adam, Spiros Nikolopoulos, Ioannis Kompatsiaris category:cs.HC cs.CV stat.ML  published:2016-02-02 summary:Brain-computer interfaces (BCIs) have been gaining momentum in making human-computer interaction more natural, especially for people with neuro-muscular disabilities. Among the existing solutions the systems relying on electroencephalograms (EEG) occupy the most prominent place due to their non-invasiveness. However, the process of translating EEG signals into computer commands is far from trivial, since it requires the optimization of many different parameters that need to be tuned jointly. In this report, we focus on the category of EEG-based BCIs that rely on Steady-State-Visual-Evoked Potentials (SSVEPs) and perform a comparative evaluation of the most promising algorithms existing in the literature. More specifically, we define a set of algorithms for each of the various different parameters composing a BCI system (i.e. filtering, artifact removal, feature extraction, feature selection and classification) and study each parameter independently by keeping all other parameters fixed. The results obtained from this evaluation process are provided together with a dataset consisting of the 256-channel, EEG signals of 11 subjects, as well as a processing toolbox for reproducing the results and supporting further experimentation. In this way, we manage to make available for the community a state-of-the-art baseline for SSVEP-based BCIs that can be used as a basis for introducing novel methods and approaches. version:2
arxiv-1602-01237 | How Far are We from Solving Pedestrian Detection? | http://arxiv.org/abs/1602.01237 | id:1602.01237 author:Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele category:cs.CV  published:2016-02-03 summary:Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the "perfect single frame detector". We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech dataset), and by manually clustering the recurrent errors of a top detector. Our results characterize both localization and background-versus-foreground errors. To address localization errors we study the impact of training annotation noise on the detector performance, and show that we can improve even with a small portion of sanitized training data. To address background/foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance. Other than our in-depth analysis, we report top performance on the Caltech dataset, and provide a new sanitized set of training and test annotations. version:1
arxiv-1505-01582 | Consistency of Spectral Hypergraph Partitioning under Planted Partition Model | http://arxiv.org/abs/1505.01582 | id:1505.01582 author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:stat.ML  published:2015-05-07 summary:Hypergraph partitioning lies at the heart of a number of problems in machine learning and network sciences. Many algorithms for hypergraph partitioning have been proposed that extend standard approaches for graph partitioning to the case of hypergraphs. However, theoretical aspects of such methods have seldom received attention in the literature as compared to the extensive studies on the guarantees of graph partitioning. For instance, consistency results of spectral graph partitioning under the stochastic block model are well known. In this paper, we present a planted partition model for sparse random non-uniform hypergraphs that generalizes the stochastic block model. We derive an error bound for a spectral hypergraph partitioning algorithm under this model using matrix concentration inequalities. To the best of our knowledge, this is the first consistency result related to partitioning non-uniform hypergraphs. version:2
arxiv-1602-01228 | Image and Information | http://arxiv.org/abs/1602.01228 | id:1602.01228 author:Frank Nielsen category:cs.CV  published:2016-02-03 summary:A well-known old adage says that {\em "A picture is worth a thousand words!"} (attributed to the Chinese philosopher Confucius ca 500 years BC). But more precisely, what do we mean by information in images? And how can it be retrieved effectively by machines? We briefly highlight these puzzling questions in this column. But first of all, let us start by defining more precisely what is meant by an "Image." version:1
arxiv-1511-09207 | Incidental Scene Text Understanding: Recent Progresses on ICDAR 2015 Robust Reading Competition Challenge 4 | http://arxiv.org/abs/1511.09207 | id:1511.09207 author:Cong Yao, Jianan Wu, Xinyu Zhou, Chi Zhang, Shuchang Zhou, Zhimin Cao, Qi Yin category:cs.CV  published:2015-11-30 summary:Different from focused texts present in natural images, which are captured with user's intention and intervention, incidental texts usually exhibit much more diversity, variability and complexity, thus posing significant difficulties and challenges for scene text detection and recognition algorithms. The ICDAR 2015 Robust Reading Competition Challenge 4 was launched to assess the performance of existing scene text detection and recognition methods on incidental texts as well as to stimulate novel ideas and solutions. This report is dedicated to briefly introduce our strategies for this challenging problem and compare them with prior arts in this field. version:2
arxiv-1602-01197 | Discriminative Sparse Neighbor Approximation for Imbalanced Learning | http://arxiv.org/abs/1602.01197 | id:1602.01197 author:Chen Huang, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2016-02-03 summary:Data imbalance is common in many vision tasks where one or more classes are rare. Without addressing this issue conventional methods tend to be biased toward the majority class with poor predictive accuracy for the minority class. These methods further deteriorate on small, imbalanced data that has a large degree of class overlap. In this study, we propose a novel discriminative sparse neighbor approximation (DSNA) method to ameliorate the effect of class-imbalance during prediction. Specifically, given a test sample, we first traverse it through a cost-sensitive decision forest to collect a good subset of training examples in its local neighborhood. Then we generate from this subset several class-discriminating but overlapping clusters and model each as an affine subspace. From these subspaces, the proposed DSNA iteratively seeks an optimal approximation of the test sample and outputs an unbiased prediction. We show that our method not only effectively mitigates the imbalance issue, but also allows the prediction to extrapolate to unseen data. The latter capability is crucial for achieving accurate prediction on small dataset with limited samples. The proposed imbalanced learning method can be applied to both classification and regression tasks at a wide range of imbalance levels. It significantly outperforms the state-of-the-art methods that do not possess an imbalance handling mechanism, and is found to perform comparably or even better than recent deep learning methods by using hand-crafted features only. version:1
arxiv-1602-01182 | High-Dimensional Regularized Discriminant Analysis | http://arxiv.org/abs/1602.01182 | id:1602.01182 author:John A. Ramey, Caleb K. Stein, Phil D. Young, Dean M. Young category:stat.ML  published:2016-02-03 summary:Friedman proposed the popular regularized discriminant analysis (RDA) classifier that utilizes a biased covariance-matrix estimator that partially pools the sample covariance matrices from linear and quadratic discriminant analysis and shrinks the resulting estimator towards a scaled identity matrix. The RDA classifier's two tuning parameters are typically estimated via a computationally burdensome cross-validation procedure that uses a grid search. We formulate a new RDA-based classifier for the small-sample, high-dimensional setting and then show that the classification decision rule is equivalent to a classifier in a subspace having a much lower dimension. As a result, the utilization of the dimension-reduction step yields a substantial reduction in computation during model selection. Also, our parameterization offers interpretability that was previously lacking with the RDA classifier. We demonstrate that our proposed classifier is often superior to several recently proposed sparse and regularized classifiers in terms of classification accuracy with three artificial and six real high-dimensional data sets. Finally, we provide an implementation of our proposed classifier in the sparsediscrim R package, which is available on CRAN. version:1
arxiv-1602-01164 | Single-Solution Hypervolume Maximization and its use for Improving Generalization of Neural Networks | http://arxiv.org/abs/1602.01164 | id:1602.01164 author:Conrado S. Miranda, Fernando J. Von Zuben category:cs.LG cs.NE stat.ML  published:2016-02-03 summary:This paper introduces the hypervolume maximization with a single solution as an alternative to the mean loss minimization. The relationship between the two problems is proved through bounds on the cost function when an optimal solution to one of the problems is evaluated on the other, with a hyperparameter to control the similarity between the two problems. This same hyperparameter allows higher weight to be placed on samples with higher loss when computing the hypervolume's gradient, whose normalized version can range from the mean loss to the max loss. An experiment on MNIST with a neural network is used to validate the theory developed, showing that the hypervolume maximization can behave similarly to the mean loss minimization and can also provide better performance, resulting on a 20% reduction of the classification error on the test set. version:1
arxiv-1602-01130 | GraphPrints: Towards a Graph Analytic Method for Network Anomaly Detection | http://arxiv.org/abs/1602.01130 | id:1602.01130 author:Christopher R. Harshaw, Robert A. Bridges, Michael D. Iannacone, Joel W. Reed, John R. Goodall category:cs.CR stat.ML  published:2016-02-02 summary:This paper introduces a novel graph-analytic approach for detecting anomalies in network flow data called GraphPrints. Building on foundational network-mining techniques, our method represents time slices of traffic as a graph, then counts graphlets -- small induced subgraphs that describe local topology. By performing outlier detection on the sequence of graphlet counts, anomalous intervals of traffic are identified, and furthermore, individual IPs experiencing abnormal behavior are singled-out. Initial testing of GraphPrints is performed on real network data with an implanted anomaly. Evaluation shows false positive rates bounded by 2.84% at the time-interval level, and 0.05% at the IP-level with 100% true positive rates at both. version:1
arxiv-1602-01125 | Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and Soft Correspondences | http://arxiv.org/abs/1602.01125 | id:1602.01125 author:Anil Bas, William A. P. Smith, Timo Bolkart, Stefanie Wuhrer category:cs.CV  published:2016-02-02 summary:We propose a fully automatic method for fitting a 3D morphable model to single face images in arbitrary pose and lighting. Our approach relies on geometric features (edges and landmarks) and, inspired by the iterated closest point algorithm, is based on computing hard correspondences between model vertices and edge pixels. We demonstrate that this is superior to previous work that uses soft correspondences to form an edge-derived cost surface that is minimised by nonlinear optimisation. version:1
arxiv-1602-01120 | On the Nyström and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets | http://arxiv.org/abs/1602.01120 | id:1602.01120 author:Darren Homrighausen, Daniel J. McDonald category:stat.ML stat.CO  published:2016-02-02 summary:In this paper we analyze approximate methods for undertaking a principal components analysis (PCA) on large data sets. PCA is a classical dimension reduction method that involves the projection of the data onto the subspace spanned by the leading eigenvectors of the covariance matrix. This projection can be used either for exploratory purposes or as an input for further analysis, e.g. regression. If the data have billions of entries or more, the computational and storage requirements for saving and manipulating the design matrix in fast memory is prohibitive. Recently, the Nystr\"om and column-sampling methods have appeared in the numerical linear algebra community for the randomized approximation of the singular value decomposition of large matrices. However, their utility for statistical applications remains unclear. We compare these approximations theoretically by bounding the distance between the induced subspaces and the desired, but computationally infeasible, PCA subspace. Additionally we show empirically, through simulations and a real data example involving a corpus of emails, the trade-off of approximation accuracy and computational complexity. version:1
arxiv-1602-01107 | Do Cascades Recur? | http://arxiv.org/abs/1602.01107 | id:1602.01107 author:Justin Cheng, Lada A Adamic, Jon Kleinberg, Jure Leskovec category:cs.SI physics.soc-ph stat.ML H.2.8  published:2016-02-02 summary:Cascades of information-sharing are a primary mechanism by which content reaches its audience on social media, and an active line of research has studied how such cascades, which form as content is reshared from person to person, develop and subside. In this paper, we perform a large-scale analysis of cascades on Facebook over significantly longer time scales, and find that a more complex picture emerges, in which many large cascades recur, exhibiting multiple bursts of popularity with periods of quiescence in between. We characterize recurrence by measuring the time elapsed between bursts, their overlap and proximity in the social network, and the diversity in the demographics of individuals participating in each peak. We discover that content virality, as revealed by its initial popularity, is a main driver of recurrence, with the availability of multiple copies of that content helping to spark new bursts. Still, beyond a certain popularity of content, the rate of recurrence drops as cascades start exhausting the population of interested individuals. We reproduce these observed patterns in a simple model of content recurrence simulated on a real social network. Using only characteristics of a cascade's initial burst, we demonstrate strong performance in predicting whether it will recur in the future. version:1
arxiv-1409-3964 | Self-taught Object Localization with Deep Networks | http://arxiv.org/abs/1409.3964 | id:1409.3964 author:Loris Bazzani, Alessandro Bergamo, Dragomir Anguelov, Lorenzo Torresani category:cs.CV  published:2014-09-13 summary:This paper introduces self-taught object localization, a novel approach that leverages deep convolutional networks trained for whole-image recognition to localize objects in images without additional human supervision, i.e., without using any ground-truth bounding boxes for training. The key idea is to analyze the change in the recognition scores when artificially masking out different regions of the image. The masking out of a region that includes the object typically causes a significant drop in recognition score. This idea is embedded into an agglomerative clustering technique that generates self-taught localization hypotheses. Our object localization scheme outperforms existing proposal methods in both precision and recall for small number of subwindow proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that the annotations automatically-generated by our method can be used to train object detectors yielding recognition results remarkably close to those obtained by training on manually-annotated bounding boxes. version:7
arxiv-1602-01449 | Development of an Ideal Observer that Incorporates Nuisance Parameters and Processes List-Mode Data | http://arxiv.org/abs/1602.01449 | id:1602.01449 author:Christopher J. MacGahan, Matthew A. Kupinski, Nathan R. Hilton, Erik M. Brubaker, William C. Johnson category:physics.data-an cs.CV  published:2016-02-02 summary:Observer models were developed to process data in list-mode format in order to perform binary discrimination tasks for use in an arms-control-treaty context. Data used in this study was generated using GEANT4 Monte Carlo simulations for photons using custom models of plutonium inspection objects and a radiation imaging system. Observer model performance was evaluated and presented using the area under the receiver operating characteristic curve. The ideal observer was studied under both signal-known-exactly conditions and in the presence of unknowns such as object orientation and absolute count-rate variability; when these additional sources of randomness were present, their incorporation into the observer yielded superior performance. version:1
arxiv-1602-01042 | Improved Achievability and Converse Bounds for Erdős-Rényi Graph Matching | http://arxiv.org/abs/1602.01042 | id:1602.01042 author:Daniel Cullina, Negar Kiyavash category:cs.IT cs.LG math.IT  published:2016-02-02 summary:We consider the problem of perfectly recovering the vertex correspondence between two correlated Erd\H{o}s-R\'enyi (ER) graphs. For a pair of correlated graphs on the same vertex set, the correspondence between the vertices can be obscured by randomly permuting the vertex labels of one of the graphs. In some cases, the structural information in the graphs allow this correspondence to be recovered. We investigate the information-theoretic threshold for exact recovery, i.e. the conditions under which the entire vertex correspondence can be correctly recovered given unbounded computational resources. Pedarsani and Grossglauser provided an achievability result of this type. Their result establishes the scaling dependence of the threshold on the number of vertices. We improve on their achievability bound. We also provide a converse bound, establishing conditions under which exact recovery is impossible. Together, these establish the scaling dependence of the threshold on the level of correlation between the two graphs. The converse and achievability bounds differ by a factor of two for sparse, significantly correlated graphs. version:1
arxiv-1602-01024 | On Deep Multi-View Representation Learning: Objectives and Optimization | http://arxiv.org/abs/1602.01024 | id:1602.01024 author:Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes category:cs.LG  published:2016-02-02 summary:We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations. version:1
arxiv-1602-01006 | A-expansion for multiple "hedgehog" shapes | http://arxiv.org/abs/1602.01006 | id:1602.01006 author:Hossam Isack, Yuri Boykov, Olga Veksler category:cs.CV  published:2016-02-02 summary:Overlapping colors and cluttered or weak edges are common segmentation problems requiring additional regularization. For example, star-convexity is popular for interactive single object segmentation due to simplicity and amenability to exact graph cut optimization. This paper proposes an approach to multiobject segmentation where objects could be restricted to separate "hedgehog" shapes. We show that a-expansion moves are submodular for our multi-shape constraints. Each "hedgehog" shape has its surface normals constrained by some vector field, e.g. gradients of a distance transform for user scribbles. Tight constraint give an extreme case of a shape prior enforcing skeleton consistency with the scribbles. Wider cones of allowed normals gives more relaxed hedgehog shapes. A single click and +/-90 degrees normal orientation constraints reduce our hedgehog prior to star-convexity. If all hedgehogs come from single clicks then our approach defines multi-star prior. Our general method has significantly more applications than standard one-star segmentation. For example, in medical data we can separate multiple non-star organs with similar appearances and weak or noisy edges. version:1
arxiv-1602-00032 | What Can I Do Around Here? Deep Functional Scene Understanding for Cognitive Robots | http://arxiv.org/abs/1602.00032 | id:1602.00032 author:Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos category:cs.RO cs.CV  published:2016-01-29 summary:For robots that have the capability to interact with the physical environment through their end effectors, understanding the surrounding scenes is not merely a task of image classification or object recognition. To perform actual tasks, it is critical for the robot to have a functional understanding of the visual scene. Here, we address the problem of localizing and recognition of functional areas from an arbitrary indoor scene, formulated as a two-stage deep learning based detection pipeline. A new scene functionality testing-bed, which is complied from two publicly available indoor scene datasets, is used for evaluation. Our method is evaluated quantitatively on the new dataset, demonstrating the ability to perform efficient recognition of functional areas from arbitrary indoor scenes. We also demonstrate that our detection model can be generalized onto novel indoor scenes by cross validating it with the images from two different datasets. version:2
arxiv-1602-00997 | Head Pose Estimation of Occluded Faces using Regularized Regression | http://arxiv.org/abs/1602.00997 | id:1602.00997 author:Amit Kumar, Rishabh Bindal, Soumya Indela, Michael Rotkowitz category:cs.CV  published:2016-02-02 summary:This paper presents regression methods for estimation of head pose from occluded 2-D face images. The process primarily involves reconstructing a face from its occluded image, followed by classification. Typical methods for reconstruction assume that the pixel errors of the occluded regions are independent. However, such an assumption is not true in the case of occlusion, because of its inherent contiguous nature. Hence, we use nuclear norm as a metric that can describe well the structure of the error. We also use LASSO Regression based l1 - regularization to improve reconstruction. Next, we implement Nuclear Norm Regularized Regression (NR), and also our proposed method, for reconstruction and subsequent classification. Finally, we compare the performance of the methods in terms of accuracy of head pose estimation of occluded faces. version:1
arxiv-1602-00970 | Visual descriptors for content-based retrieval of remote sensing images | http://arxiv.org/abs/1602.00970 | id:1602.00970 author:Paolo Napoletano category:cs.CV  published:2016-02-02 summary:In this paper we present an extensive evaluation of visual descriptors for the content-based retrieval of remote sensing images. The evaluation includes global, local, and Convolutional Neural Network (CNNs) features coupled with three different Content-Based Image Retrieval schemas. We conducted all the experiments on two publicly available datasets: the 21-class UC Merced Land Use/Land Cover data set and 19-class High-resolution Satellite Scene dataset. Results demonstrate that features extracted from CNNs are the best performing whatever is the retrieval schema adopted. Local descriptors perform better than CNN-based descriptors only when dealing with images that contain fine-grained textures or objects. version:1
arxiv-1601-04011 | Tightening the Sample Complexity of Empirical Risk Minimization via Preconditioned Stability | http://arxiv.org/abs/1601.04011 | id:1601.04011 author:Alon Gonen, Shai Shalev-Shwartz category:cs.LG  published:2016-01-15 summary:We tighten the sample complexity of empirical risk minimization (ERM) associated with a class of generalized linear models that include linear and logistic regression. In particular, we conclude that ERM attains the optimal sample complexity for linear regression. Our analysis relies on a new notion of stability, called preconditioned stability, which may be of independent interest. version:2
arxiv-1602-00515 | Marvin: Semantic annotation using multiple knowledge sources | http://arxiv.org/abs/1602.00515 | id:1602.00515 author:Nikola Milosevic category:cs.AI cs.CL D.3.2; K.2; H.2.4  published:2016-02-01 summary:People are producing more written material then anytime in the history. The increase is so high that professionals from the various fields are no more able to cope with this amount of publications. Text mining tools can offer tools to help them and one of the tools that can aid information retrieval and information extraction is semantic text annotation. In this report we present Marvin, a text annotator written in Java, which can be used as a command line tool and as a Java library. Marvin is able to annotate text using multiple sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS. version:2
arxiv-1603-08497 | On distances, paths and connections for hyperspectral image segmentation | http://arxiv.org/abs/1603.08497 | id:1603.08497 author:Guillaume Noyel, Jesus Angulo, Dominique Jeulin category:cs.CV math.NA  published:2016-02-02 summary:The present paper introduces the $\eta$ and {\eta} connections in order to add regional information on $\lambda$-flat zones, which only take into account a local information. A top-down approach is considered. First $\lambda$-flat zones are built in a way leading to a sub-segmentation. Then a finer segmentation is obtained by computing $\eta$-bounded regions and $\mu$-geodesic balls inside the $\lambda$-flat zones. The proposed algorithms for the construction of new partitions are based on queues with an ordered selection of seeds using the cumulative distance. $\eta$-bounded regions offers a control on the variations of amplitude in the class from a point, called center, and $\mu$-geodesic balls controls the "size" of the class. These results are applied to hyperspectral images. version:1
arxiv-1502-01400 | Fast unsupervised Bayesian image segmentation with adaptive spatial regularisation | http://arxiv.org/abs/1502.01400 | id:1502.01400 author:Marcelo Pereyra, Steve McLaughlin category:stat.CO cs.CV  published:2015-02-05 summary:This paper presents a new Bayesian estimation technique for hidden Potts-Markov random fields with unknown regularisation parameters, with application to fast unsupervised K-class image segmentation. The technique is derived by first removing the regularisation parameter from the Bayesian model by marginalisation, followed by a small-variance-asymptotic (SVA) analysis in which the spatial regularisation and the integer-constrained terms of the Potts model are decoupled. The evaluation of this SVA Bayesian estimator is then relaxed into a problem that can be computed efficiently by iteratively solving a convex total-variation denoising problem and a least-squares clustering (K-means) problem, both of which can be solved straightforwardly, even in high-dimensions, and with parallel computing techniques. This leads to a fast fully unsupervised Bayesian image segmentation methodology in which the strength of the spatial regularisation is adapted automatically to the observed image during the inference procedure, and that can be easily applied in large 2D and 3D scenarios or in applications requiring low computing times. Experimental results on real images, as well as extensive comparisons with state-of-the-art algorithms, confirm that the proposed methodology offer extremely fast convergence and produces accurate segmentation results, with the important additional advantage of self-adjusting regularisation parameters. version:3
arxiv-1602-00828 | Learning a Deep Model for Human Action Recognition from Novel Viewpoints | http://arxiv.org/abs/1602.00828 | id:1602.00828 author:Hossein Rahmani, Ajmal Mian, Mubarak Shah category:cs.CV  published:2016-02-02 summary:Recognizing human actions from unknown and unseen (novel) views is a challenging problem. We propose a Robust Non-Linear Knowledge Transfer Model (R-NKTM) for human action recognition from novel views. The proposed R-NKTM is a deep fully-connected neural network that transfers knowledge of human actions from any unknown view to a shared high-level virtual view by finding a non-linear virtual path that connects the views. The R-NKTM is learned from dense trajectories of synthetic 3D human models fitted to real motion capture data and generalizes to real videos of human actions. The strength of our technique is that we learn a single R-NKTM for all actions and all viewpoints for knowledge transfer of any real human action video without the need for re-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to incorporate new action classes. R-NKTM is learned with dummy labels and does not require knowledge of the camera viewpoint at any stage. Experiments on three benchmark cross-view human action datasets show that our method outperforms existing state-of-the-art. version:1
arxiv-1602-00812 | The Grail theorem prover: Type theory for syntax and semantics | http://arxiv.org/abs/1602.00812 | id:1602.00812 author:Richard Moot category:cs.CL  published:2016-02-02 summary:As the name suggests, type-logical grammars are a grammar formalism based on logic and type theory. From the prespective of grammar design, type-logical grammars develop the syntactic and semantic aspects of linguistic phenomena hand-in-hand, letting the desired semantics of an expression inform the syntactic type and vice versa. Prototypical examples of the successful application of type-logical grammars to the syntax-semantics interface include coordination, quantifier scope and extraction.This chapter describes the Grail theorem prover, a series of tools for designing and testing grammars in various modern type-logical grammars which functions as a tool . All tools described in this chapter are freely available. version:1
arxiv-1512-05430 | Large Scale Business Discovery from Street Level Imagery | http://arxiv.org/abs/1512.05430 | id:1512.05430 author:Qian Yu, Christian Szegedy, Martin C. Stumpe, Liron Yatziv, Vinay Shet, Julian Ibarz, Sacha Arnoud category:cs.CV  published:2015-12-17 summary:Search with local intent is becoming increasingly useful due to the popularity of the mobile device. The creation and maintenance of accurate listings of local businesses worldwide is time consuming and expensive. In this paper, we propose an approach to automatically discover businesses that are visible on street level imagery. Precise business store front detection enables accurate geo-location of businesses, and further provides input for business categorization, listing generation, etc. The large variety of business categories in different countries makes this a very challenging problem. Moreover, manual annotation is prohibitive due to the scale of this problem. We propose the use of a MultiBox based approach that takes input image pixels and directly outputs store front bounding boxes. This end-to-end learning approach instead preempts the need for hand modeling either the proposal generation phase or the post-processing phase, leveraging large labelled training datasets. We demonstrate our approach outperforms the state of the art detection techniques with a large margin in terms of performance and run-time efficiency. In the evaluation, we show this approach achieves human accuracy in the low-recall settings. We also provide an end-to-end evaluation of business discovery in the real world. version:2
arxiv-1404-4178 | Speeding Up MCMC by Efficient Data Subsampling | http://arxiv.org/abs/1404.4178 | id:1404.4178 author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.ME stat.CO stat.ML  published:2014-04-16 summary:We propose a Markov Chain Monte Carlo (MCMC) framework where the likelihood function for $n$ observations is estimated from a random subset of $m$ observations. Inspired by the survey sampling literature, we introduce a general and highly efficient log-likelihood estimator. The estimator incorporates information about each observation's contribution to the log-likelihood function. The computational complexity of the estimator can be much smaller than for the full log-likelihood, and we document substantial speed-ups in the applications. The likelihood estimate is used within a Pseudo-marginal framework to sample from a perturbed posterior which we prove to be within $O(m^{-1/2})$ of the true posterior. Moreover, the approximation error is demonstrated to be negligible even for a small $m$ in our applications. We propose a simple way to adaptively choose the sample size $m$ during the MCMC to optimize sampling efficiency for a fixed computational budget. We also propose a correlated pseudo marginal approach to subsampling that dramatically improves performance. The method is illustrated on three examples, each one representing a different data structure. In particular, we show that our method outperforms other subsampling MCMC methods proposed in the literature. version:3
arxiv-1602-00763 | Simple Online and Realtime Tracking | http://arxiv.org/abs/1602.00763 | id:1602.00763 author:Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft category:cs.CV  published:2016-02-02 summary:This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers. version:1
arxiv-1602-00753 | Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects | http://arxiv.org/abs/1602.00753 | id:1602.00753 author:Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi category:cs.AI cs.CV  published:2016-02-02 summary:Human vision greatly benefits from the information about sizes of objects. The role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition. However, the impact of the information about sizes of objects is yet to be determined in AI. We postulate that this is mainly attributed to the lack of a comprehensive repository of size information. In this paper, we introduce a method to automatically infer object sizes, leveraging visual and textual information from web. By maximizing the joint likelihood of textual and visual observations, our method learns reliable relative size estimates, with no explicit human supervision. We introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons. version:1
arxiv-1602-00749 | Combining ConvNets with Hand-Crafted Features for Action Recognition Based on an HMM-SVM Classifier | http://arxiv.org/abs/1602.00749 | id:1602.00749 author:Pichao Wang, Zhaoyang Li, Yonghong Hou, Wanqing Li category:cs.CV  published:2016-02-01 summary:This paper proposes a new framework for RGB-D-based action recognition that takes advantages of hand-designed features from skeleton data and deeply learned features from depth maps, and exploits effectively both the local and global temporal information. Specifically, depth and skeleton data are firstly augmented for deep learning and making the recognition insensitive to view variance. Secondly, depth sequences are segmented using the hand-crafted features based on skeleton joints motion histogram to exploit the local temporal information. All training se gments are clustered using an Infinite Gaussian Mixture Model (IGMM) through Bayesian estimation and labelled for training Convolutional Neural Networks (ConvNets) on the depth maps. Thus, a depth sequence can be reliably encoded into a sequence of segment labels. Finally, the sequence of labels is fed into a joint Hidden Markov Model and Support Vector Machine (HMM-SVM) classifier to explore the global temporal information for final recognition. version:1
arxiv-1602-00734 | Learning Data Triage: Linear Decoding Works for Compressive MRI | http://arxiv.org/abs/1602.00734 | id:1602.00734 author:Yen-Huan Li, Volkan Cevher category:cs.IT cs.LG math.IT stat.ML  published:2016-02-01 summary:The standard approach to compressive sampling considers recovering an unknown deterministic signal with certain known structure, and designing the sub-sampling pattern and recovery algorithm based on the known structure. This approach requires looking for a good representation that reveals the signal structure, and solving a non-smooth convex minimization problem (e.g., basis pursuit). In this paper, another approach is considered: We learn a good sub-sampling pattern based on available training signals, without knowing the signal structure in advance, and reconstruct an accordingly sub-sampled signal by computationally much cheaper linear reconstruction. We provide a theoretical guarantee on the recovery error, and show via experiments on real-world MRI data the effectiveness of the proposed compressive MRI scheme. version:1
arxiv-1602-00715 | Algorithm-Induced Prior for Image Restoration | http://arxiv.org/abs/1602.00715 | id:1602.00715 author:Stanley H. Chan category:cs.CV  published:2016-02-01 summary:This paper studies a type of image priors that are constructed implicitly through the alternating direction method of multiplier (ADMM) algorithm, called the algorithm-induced prior. Different from classical image priors which are defined before running the reconstruction algorithm, algorithm-induced priors are defined by the denoising procedure used to replace one of the two modules in the ADMM algorithm. Since such prior is not explicitly defined, analyzing the performance has been difficult in the past. Focusing on the class of symmetric smoothing filters, this paper presents an explicit expression of the prior induced by the ADMM algorithm. The new prior is reminiscent to the conventional graph Laplacian but with stronger reconstruction performance. It can also be shown that the overall reconstruction has an efficient closed-form implementation if the associated symmetric smoothing filter is low rank. The results are validated with experiments on image inpainting. version:1
arxiv-1602-00585 | Improving Vertebra Segmentation through Joint Vertebra-Rib Atlases | http://arxiv.org/abs/1602.00585 | id:1602.00585 author:Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers category:cs.CV  published:2016-02-01 summary:Accurate spine segmentation allows for improved identification and quantitative characterization of abnormalities of the vertebra, such as vertebral fractures. However, in existing automated vertebra segmentation methods on computed tomography (CT) images, leakage into nearby bones such as ribs occurs due to the close proximity of these visibly intense structures in a 3D CT volume. To reduce this error, we propose the use of joint vertebra-rib atlases to improve the segmentation of vertebrae via multi-atlas joint label fusion. Segmentation was performed and evaluated on CTs containing 106 thoracic and lumbar vertebrae from 10 pathological and traumatic spine patients on an individual vertebra level basis. Vertebra atlases produced errors where the segmentation leaked into the ribs. The use of joint vertebra-rib atlases produced a statistically significant increase in the Dice coefficient from 92.5 $\pm$ 3.1% to 93.8 $\pm$ 2.1% for the left and right transverse processes and a decrease in the mean and max surface distance from 0.75 $\pm$ 0.60mm and 8.63 $\pm$ 4.44mm to 0.30 $\pm$ 0.27mm and 3.65 $\pm$ 2.87mm, respectively. version:1
arxiv-1602-00577 | A Deep Learning Based Fast Image Saliency Detection Algorithm | http://arxiv.org/abs/1602.00577 | id:1602.00577 author:Hengyue Pan, Hui Jiang category:cs.CV  published:2016-02-01 summary:In this paper, we propose a fast deep learning method for object saliency detection using convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify the input images based on the pixel-wise gradients to reduce a pre-defined cost function, which is defined to measure the class-specific objectness and clamp the class-irrelevant outputs to maintain image background. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. We further apply SLIC superpixels and LAB color based low level saliency features to smooth and refine the gradients. Our methods are quite computationally efficient, much faster than other deep learning based saliency methods. Experimental results on two benchmark tasks, namely Pascal VOC 2012 and MSRA10k, have shown that our proposed methods can generate high-quality salience maps, at least comparable with many slow and complicated deep learning methods. Comparing with the pure low-level methods, our approach excels in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects. version:1
arxiv-1602-00554 | Graph-based Predictable Feature Analysis | http://arxiv.org/abs/1602.00554 | id:1602.00554 author:Björn Weghenkel, Asja Fischer, Laurenz Wiskott category:cs.LG  published:2016-02-01 summary:We propose a new method for the unsupervised extraction of predictable features from high-dimensional time-series, where high predictability is understood very generically as low variance in the distribution of the next data point given the current one. We show how this objective can be understood in terms of graph embedding as well as how it corresponds to the information-theoretic measure of excess entropy in special cases. Experimentally, we compare the approach to two other algorithms for the extraction of predictable features, namely ForeCA and PFA, and show how it is able to outperform them in certain settings. version:1
arxiv-1503-02291 | TED: A Tolerant Edit Distance for Segmentation Evaluation | http://arxiv.org/abs/1503.02291 | id:1503.02291 author:Jan Funke, Francesc Moreno-Noguer, Albert Cardona, Matthew Cook category:cs.CV  published:2015-03-08 summary:In this paper, we present a novel error measure to compare a segmentation against ground truth. This measure, which we call Tolerant Edit Distance (TED), is motivated by two observations: (1) Some errors, like small boundary shifts, are tolerable in practice. Which errors are tolerable is application dependent and should be a parameter of the measure. (2) Non-tolerable errors have to be corrected manually. The time needed to do so should be reflected by the error measure. Using integer linear programming, the TED finds the minimal weighted sum of split and merge errors exceeding a given tolerance criterion, and thus provides a time-to-fix estimate. In contrast to commonly used measures like Rand index or variation of information, the TED (1) does not count small, but tolerable, differences, (2) provides intuitive numbers, (3) gives a time-to-fix estimate, and (4) can localize and classify the type of errors. By supporting both isotropic and anisotropic volumes and having a flexible tolerance criterion, the TED can be adapted to different requirements. On example segmentations for 3D neuron segmentation, we demonstrate that the TED is capable of counting topological errors, while ignoring small boundary shifts. version:3
arxiv-1602-00522 | PAC-Bayesian Online Clustering | http://arxiv.org/abs/1602.00522 | id:1602.00522 author:Le Li, Benjamin Guedj, Sébastien Loustau category:stat.ML math.ST stat.TH  published:2016-02-01 summary:This paper addresses the online clustering problem. When faced with high frequency streams of data, clustering raises theoretical and algorithmic pitfalls. Working under a sparsity assumption, a new online clustering algorithm is introduced. Our procedure relies on the PAC-Bayesian approach, allowing for a dynamic (i.e., time-dependent) estimation of the number of clusters. Its theoretical merits are supported by sparsity regret bounds, and an RJMCMC-flavored implementation called PACO is proposed along with numerical experiments to assess its potential. version:1
arxiv-1602-00490 | I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video Streaming Title Classification | http://arxiv.org/abs/1602.00490 | id:1602.00490 author:Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar category:cs.MM cs.LG cs.NI  published:2016-02-01 summary:Previous research has shown that information can be extracted from encrypted multimedia streams. This includes video titles classification of non HTTP adaptive streams (non-HAS). This paper presents an algorithm for \emph{encrypted HTTP adaptive video streaming title classification}. We evaluated our algorithm on a new YouTube popular videos dataset that was collected from the internet under real-world network conditions. We provide the dataset and the crawler for future research. Our algorithm's classification accuracy is 98\%. version:1
arxiv-1602-00426 | An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection | http://arxiv.org/abs/1602.00426 | id:1602.00426 author:Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Chia-Hsiang Liu, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG  published:2016-02-01 summary:In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens. version:1
arxiv-1508-03326 | A Survey on Contextual Multi-armed Bandits | http://arxiv.org/abs/1508.03326 | id:1508.03326 author:Li Zhou category:cs.LG  published:2015-08-13 summary:In this survey we cover a few stochastic and adversarial contextual bandit algorithms. We analyze each algorithm's assumption and regret bound. version:2
arxiv-1601-07267 | Evolutionary stability implies asymptotic stability under multiplicative weights | http://arxiv.org/abs/1601.07267 | id:1601.07267 author:Ioannis Avramopoulos category:cs.GT cs.LG math.OC  published:2016-01-27 summary:We show that evolutionarily stable states in general (nonlinear) population games (which can be viewed as continuous vector fields constrained on a polytope) are asymptotically stable under a multiplicative weights dynamic (under appropriate choices of a parameter called the learning rate or step size, which we demonstrate to be crucial to achieve convergence, as otherwise even chaotic behavior is possible to manifest). Our result implies that evolutionary theories based on multiplicative weights are compatible (in principle, more general) with those based on the notion of evolutionary stability. However, our result further establishes multiplicative weights as a nonlinear programming primitive (on par with standard nonlinear programming methods) since various nonlinear optimization problems, such as finding Nash/Wardrop equilibria in nonatomic congestion games, which are well-known to be equipped with a convex potential function, and finding strict local maxima of quadratic programming problems, are special cases of the problem of computing evolutionarily stable states in nonlinear population games. version:2
arxiv-1509-00181 | Differentially Private Online Learning for Cloud-Based Video Recommendation with Multimedia Big Data in Social Networks | http://arxiv.org/abs/1509.00181 | id:1509.00181 author:Pan Zhou, Yingxue Zhou, Dapeng Wu, Hai Jin category:cs.LG  published:2015-09-01 summary:With the rapid growth in multimedia services and the enormous offers of video contents in online social networks, users have difficulty in obtaining their interests. Therefore, various personalized recommendation systems have been proposed. However, they ignore that the accelerated proliferation of social media data has led to the big data era, which has greatly impeded the process of video recommendation. In addition, none of them has considered both the privacy of users' contexts (e,g., social status, ages and hobbies) and video service vendors' repositories, which are extremely sensitive and of significant commercial value. To handle the problems, we propose a cloud-assisted differentially private video recommendation system based on distributed online learning. In our framework, service vendors are modeled as distributed cooperative learners, recommending videos according to user's context, while simultaneously adapting the video-selection strategy based on user-click feedback to maximize total user clicks (reward). Considering the sparsity and heterogeneity of big social media data, we also propose a novel geometric differentially private model, which can greatly reduce the performance (recommendation accuracy) loss. Our simulation shows the proposed algorithms outperform other existing methods and keep a delicate balance between computing accuracy and privacy preserving level. version:7
arxiv-1602-00386 | Scene Invariant Crowd Segmentation and Counting Using Scale-Normalized Histogram of Moving Gradients (HoMG) | http://arxiv.org/abs/1602.00386 | id:1602.00386 author:Parthipan Siva, Mohammad Javad Shafiee, Mike Jamieson, Alexander Wong category:cs.CV  published:2016-02-01 summary:The problem of automated crowd segmentation and counting has garnered significant interest in the field of video surveillance. This paper proposes a novel scene invariant crowd segmentation and counting algorithm designed with high accuracy yet low computational complexity in mind, which is key for widespread industrial adoption. A novel low-complexity, scale-normalized feature called Histogram of Moving Gradients (HoMG) is introduced for highly effective spatiotemporal representation of individuals and crowds within a video. Real-time crowd segmentation is achieved via boosted cascade of weak classifiers based on sliding-window HoMG features, while linear SVM regression of crowd-region HoMG features is employed for real-time crowd counting. Experimental results using multi-camera crowd datasets show that the proposed algorithm significantly outperform state-of-the-art crowd counting algorithms, as well as achieve very promising crowd segmentation results, thus demonstrating the efficacy of the proposed method for highly-accurate, real-time video-driven crowd analysis. version:1
arxiv-1602-00374 | ConfidentCare: A Clinical Decision Support System for Personalized Breast Cancer Screening | http://arxiv.org/abs/1602.00374 | id:1602.00374 author:Ahmed M. Alaa, Kyeong H. Moon, William Hsu, Mihaela van der Schaar category:cs.LG  published:2016-02-01 summary:Breast cancer screening policies attempt to achieve timely diagnosis by the regular screening of apparently healthy women. Various clinical decisions are needed to manage the screening process; those include: selecting the screening tests for a woman to take, interpreting the test outcomes, and deciding whether or not a woman should be referred to a diagnostic test. Such decisions are currently guided by clinical practice guidelines (CPGs), which represent a one-size-fits-all approach that are designed to work well on average for a population, without guaranteeing that it will work well uniformly over that population. Since the risks and benefits of screening are functions of each patients features, personalized screening policies that are tailored to the features of individuals are needed in order to ensure that the right tests are recommended to the right woman. In order to address this issue, we present ConfidentCare: a computer-aided clinical decision support system that learns a personalized screening policy from the electronic health record (EHR) data. ConfidentCare operates by recognizing clusters of similar patients, and learning the best screening policy to adopt for each cluster. A cluster of patients is a set of patients with similar features (e.g. age, breast density, family history, etc.), and the screening policy is a set of guidelines on what actions to recommend for a woman given her features and screening test scores. ConfidentCare algorithm ensures that the policy adopted for every cluster of patients satisfies a predefined accuracy requirement with a high level of confidence. We show that our algorithm outperforms the current CPGs in terms of cost-efficiency and false positive rates. version:1
arxiv-1602-00367 | Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers | http://arxiv.org/abs/1602.00367 | id:1602.00367 author:Yijun Xiao, Kyunghyun Cho category:cs.CL  published:2016-02-01 summary:Document classification tasks were primarily tackled at word level. Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words. We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs. We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models. It achieves comparable performances with much less parameters. version:1
arxiv-1602-00360 | Semi-supervised K-means++ | http://arxiv.org/abs/1602.00360 | id:1602.00360 author:Jordan Yoder, Carey E. Priebe category:stat.ML  published:2016-02-01 summary:Traditionally, practitioners initialize the {\tt k-means} algorithm with centers chosen uniformly at random. Randomized initialization with uneven weights ({\tt k-means++}) has recently been used to improve the performance over this strategy in cost and run-time. We consider the k-means problem with semi-supervised information, where some of the data are pre-labeled, and we seek to label the rest according to the minimum cost solution. By extending the {\tt k-means++} algorithm and analysis to account for the labels, we derive an improved theoretical bound on expected cost and observe improved performance in simulated and real data examples. This analysis provides theoretical justification for a roughly linear semi-supervised clustering algorithm. version:1
arxiv-1602-00357 | DeepCare: A Deep Dynamic Memory Model for Predictive Medicine | http://arxiv.org/abs/1602.00357 | id:1602.00357 author:Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG  published:2016-02-01 summary:Personalized predictive medicine necessitates the modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, recorded in electronic medical records, are episodic and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural network that reads medical records, stores previous illness history, infers current illness states and predicts future medical outcomes. At the data level, DeepCare represents care episodes as vectors in space, models patient health state trajectories through explicit memory of historical records. Built on Long Short-Term Memory (LSTM), DeepCare introduces time parameterizations to handle irregular timed events by moderating the forgetting and consolidation of memory cells. DeepCare also incorporates medical interventions that change the course of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling, intervention recommendation, and future risk prediction. On two important cohorts with heavy social and economic burden -- diabetes and mental health -- the results show improved modeling and risk prediction accuracy. version:1
arxiv-1602-00355 | A Spectral Series Approach to High-Dimensional Nonparametric Regression | http://arxiv.org/abs/1602.00355 | id:1602.00355 author:Ann B. Lee, Rafael Izbicki category:stat.ME stat.ML  published:2016-02-01 summary:A key question in modern statistics is how to make fast and reliable inferences for complex, high-dimensional data. While there has been much interest in sparse techniques, current methods do not generalize well to data with nonlinear structure. In this work, we present an orthogonal series estimator for predictors that are complex aggregate objects, such as natural images, galaxy spectra, trajectories, and movies. Our series approach ties together ideas from kernel machine learning, and Fourier methods. We expand the unknown regression on the data in terms of the eigenfunctions of a kernel-based operator, and we take advantage of orthogonality of the basis with respect to the underlying data distribution, P, to speed up computations and tuning of parameters. If the kernel is appropriately chosen, then the eigenfunctions adapt to the intrinsic geometry and dimension of the data. We provide theoretical guarantees for a radial kernel with varying bandwidth, and we relate smoothness of the regression function with respect to P to sparsity in the eigenbasis. Finally, using simulated and real-world data, we systematically compare the performance of the spectral series approach with classical kernel smoothing, k-nearest neighbors regression, kernel ridge regression, and state-of-the-art manifold and local regression methods. version:1
arxiv-1602-00351 | Adaptive Subgradient Methods for Online AUC Maximization | http://arxiv.org/abs/1602.00351 | id:1602.00351 author:Yi Ding, Peilin Zhao, Steven C. H. Hoi, Yew-Soon Ong category:cs.LG  published:2016-02-01 summary:Learning for maximizing AUC performance is an important research problem in Machine Learning and Artificial Intelligence. Unlike traditional batch learning methods for maximizing AUC which often suffer from poor scalability, recent years have witnessed some emerging studies that attempt to maximize AUC by single-pass online learning approaches. Despite their encouraging results reported, the existing online AUC maximization algorithms often adopt simple online gradient descent approaches that fail to exploit the geometrical knowledge of the data observed during the online learning process, and thus could suffer from relatively larger regret. To address the above limitation, in this work, we explore a novel algorithm of Adaptive Online AUC Maximization (AdaOAM) which employs an adaptive gradient method that exploits the knowledge of historical gradients to perform more informative online learning. The new adaptive updating strategy of the AdaOAM is less sensitive to the parameter settings and maintains the same time complexity as previous non-adaptive counterparts. Additionally, we extend the algorithm to handle high-dimensional sparse data (SAdaOAM) and address sparsity in the solution by performing lazy gradient updating. We analyze the theoretical bounds and evaluate their empirical performance on various types of data sets. The encouraging empirical results obtained clearly highlighted the effectiveness and efficiency of the proposed algorithms. version:1
arxiv-1510-07727 | Statistically efficient thinning of a Markov chain sampler | http://arxiv.org/abs/1510.07727 | id:1510.07727 author:Art B. Owen category:stat.CO cs.LG stat.ML 65C40  62M05  published:2015-10-27 summary:It is common to subsample Markov chain samples to reduce the storage burden of the output. It is also well known that discarding $k-1$ out of every $k$ observations will not improve statistical efficiency. It is less frequently remarked that subsampling a Markov chain allows one to omit some of the computation beyond that needed to simply advance the chain. When this reduced computation is accounted for, thinning the Markov chain by subsampling it can improve statistical efficiency. Given an autocorrelation parameter $\rho$ and a cost ratio $\theta$, this paper shows how to compute the most efficient subsampling frequency $k$. The optimal $k$ grows rapidly as $\rho$ increases towards $1$. The resulting efficiency gain depends primarily on $\theta$, not $\rho$. Taking $k=1$ (no thinning) is optimal when $\rho\le0$. For $\rho>0$ it is optimal if and only if $\theta \le (1-\rho)^2/(2\rho)$. The efficiency gain never exceeds $1+\theta$. The derivations are exact for an AR(1) autocorrelation which is often a good approximation to the autocorrelations one sees in practice. version:4
arxiv-1405-7129 | Marginalization and Conditioning for LWF Chain Graphs | http://arxiv.org/abs/1405.7129 | id:1405.7129 author:Kayvan Sadeghi category:stat.OT math.ST stat.ML stat.TH  published:2014-05-28 summary:In this paper, we deal with the problem of marginalization over and conditioning on two disjoint subsets of the node set of chain graphs (CGs) with the LWF Markov property. For this purpose, we define the class of chain mixed graphs (CMGs) with three types of edges and, for this class, provide a separation criterion under which the class of CMGs is stable under marginalization and conditioning and contains the class of LWF CGs as its subclass. We provide a method for generating such graphs after marginalization and conditioning for a given CMG or a given LWF CG. We then define and study the class of anterial graphs, which is also stable under marginalization and conditioning and contains LWF CGs, but has a simpler structure than CMGs. version:3
arxiv-1602-00328 | Novel Views of Objects from a Single Image | http://arxiv.org/abs/1602.00328 | id:1602.00328 author:Konstantinos Rematas, Chuong Nguyen, Tobias Ritschel, Mario Fritz, Tinne Tuytelaars category:cs.CV cs.GR  published:2016-01-31 summary:Taking an image of an object is at its core a lossy process. The rich information about the three-dimensional structure of the world is flattened to an image plane and decisions such as viewpoint and camera parameters are final and not easily revertible. As a consequence, possibilities of changing viewpoint are limited. Given a single image depicting an object, novel-view synthesis is the task of generating new images that render the object from a different viewpoint than the one given. The main difficulty is to synthesize the parts that are disoccluded; disocclusion occurs when parts of an object are hidden by the object itself under a specific viewpoint. In this work, we show how to improve novel-view synthesis by making use of the correlations observed in 3D models and applying them to new image instances. We propose a technique to use the structural information extracted from a 3D model that matches the image object in terms of viewpoint and shape. For the latter part, we propose an efficient 2D-to-3D alignment method that associates precisely the image appearance with the 3D model geometry with minimal user interaction. Our technique is able to simulate plausible viewpoint changes for a variety of object classes within seconds. Additionally, we show that our synthesized images can be used as additional training data that improves the performance of standard object detectors. version:1
arxiv-1602-00309 | Bandits meet Computer Architecture: Designing a Smartly-allocated Cache | http://arxiv.org/abs/1602.00309 | id:1602.00309 author:Yonatan Glassner, Koby Crammer category:cs.LG  published:2016-01-31 summary:In many embedded systems, such as imaging sys- tems, the system has a single designated purpose, and same threads are executed repeatedly. Profiling thread behavior, allows the system to allocate each thread its resources in a way that improves overall system performance. We study an online resource al- locationproblem,wherearesourcemanagersimulta- neously allocates resources (exploration), learns the impact on the different consumers (learning) and im- proves allocation towards optimal performance (ex- ploitation). We build on the rich framework of multi- armed bandits and present online and offline algo- rithms. Through extensive experiments with both synthetic data and real-world cache allocation to threads we show the merits and properties of our al- gorithms version:1
arxiv-1602-00307 | Bit-Planes: Dense Subpixel Alignment of Binary Descriptors | http://arxiv.org/abs/1602.00307 | id:1602.00307 author:Hatem Alismail, Brett Browning, Simon Lucey category:cs.CV  published:2016-01-31 summary:Binary descriptors have been instrumental in the recent evolution of computationally efficient sparse image alignment algorithms. Increasingly, however, the vision community is interested in dense image alignment methods, which are more suitable for estimating correspondences from high frame rate cameras as they do not rely on exhaustive search. However, classic dense alignment approaches are sensitive to illumination change. In this paper, we propose an easy to implement and low complexity dense binary descriptor, which we refer to as bit-planes, that can be seamlessly integrated within a multi-channel Lucas & Kanade framework. This novel approach combines the robustness of binary descriptors with the speed and accuracy of dense alignment methods. The approach is demonstrated on a template tracking problem achieving state-of-the-art robustness and faster than real-time performance on consumer laptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+ fps on an iPad Air 2). version:1
arxiv-1601-07014 | Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound | http://arxiv.org/abs/1601.07014 | id:1601.07014 author:Fausto Milletari, Seyed-Ahmad Ahmadi, Christine Kroll, Annika Plate, Verena Rozanski, Juliana Maiostre, Johannes Levin, Olaf Dietrich, Birgit Ertl-Wagner, Kai Bötzel, Nassir Navab category:cs.CV  published:2016-01-26 summary:In this work we propose a novel approach to perform segmentation by leveraging the abstraction capabilities of convolutional neural networks (CNNs). Our method is based on Hough voting, a strategy that allows for fully automatic localisation and segmentation of the anatomies of interest. This approach does not only use the CNN classification outcomes, but it also implements voting by exploiting the features produced by the deepest portion of the network. We show that this learning-based segmentation method is robust, multi-region, flexible and can be easily adapted to different modalities. In the attempt to show the capabilities and the behaviour of CNNs when they are applied to medical image analysis, we perform a systematic study of the performances of six different network architectures, conceived according to state-of-the-art criteria, in various situations. We evaluate the impact of both different amount of training data and different data dimensionality (2D, 2.5D and 3D) on the final results. We show results on both MRI and transcranial US volumes depicting respectively 26 regions of the basal ganglia and the midbrain. version:3
arxiv-1602-00293 | WASSUP? LOL : Characterizing Out-of-Vocabulary Words in Twitter | http://arxiv.org/abs/1602.00293 | id:1602.00293 author:Suman Kalyan Maity, Chaitanya Sarda, Anshit Chaudhary, Abhijeet Patil, Shraman Kumar, Akash Mondal, Animesh Mukherjee category:cs.CL cs.SI  published:2016-01-31 summary:Language in social media is mostly driven by new words and spellings that are constantly entering the lexicon thereby polluting it and resulting in high deviation from the formal written version. The primary entities of such language are the out-of-vocabulary (OOV) words. In this paper, we study various sociolinguistic properties of the OOV words and propose a classification model to categorize them into at least six categories. We achieve 81.26% accuracy with high precision and recall. We observe that the content features are the most discriminative ones followed by lexical and context features. version:1
arxiv-1602-00260 | DOLDA - a regularized supervised topic model for high-dimensional multi-class regression | http://arxiv.org/abs/1602.00260 | id:1602.00260 author:Måns Magnusson, Leif Jonsson, Mattias Villani category:stat.ML  published:2016-01-31 summary:We introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), a supervised topic model for multi-class classification that can handle both many classes as well as many covariates. To handle many classes we use the recently proposed Diagonal Orthant (DO) probit model together with an efficient horseshoe prior for variable selection/shrinkage. An important advantage of DOLDA is that learned topics are directly connected to individual classes without the need for a reference class. We propose a computationally efficient parallel Gibbs sampler for the new model. We study the model properties on an IMDb dataset with roughly 8000 documents, and document preliminary results in a bug prediction context where 118 components are predicted using 100 topics from bug reports. version:1
arxiv-1602-00236 | Nonlinearities and Adaptation of Color Vision from Sequential Principal Curves Analysis | http://arxiv.org/abs/1602.00236 | id:1602.00236 author:Valero Laparra, Sandra Jiménez, Gustavo Camps-Valls, Jesús Malo category:stat.ML q-bio.NC  published:2016-01-31 summary:Mechanisms of human color vision are characterized by two phenomenological aspects: the system is nonlinear and adaptive to changing environments. Conventional attempts to derive these features from statistics use separate arguments for each aspect. The few statistical approaches that do consider both phenomena simultaneously follow parametric formulations based on empirical models. Therefore, it may be argued that the behavior does not come directly from the color statistics but from the convenient functional form adopted. In addition, many times the whole statistical analysis is based on simplified databases that disregard relevant physical effects in the input signal, as for instance by assuming flat Lambertian surfaces. Here we address the simultaneous statistical explanation of (i) the nonlinear behavior of achromatic and chromatic mechanisms in a fixed adaptation state, and (ii) the change of such behavior. Both phenomena emerge directly from the samples through a single data-driven method: the Sequential Principal Curves Analysis (SPCA) with local metric. SPCA is a new manifold learning technique to derive a set of sensors adapted to the manifold using different optimality criteria. A new database of colorimetrically calibrated images of natural objects under these illuminants was collected. The results obtained by applying SPCA show that the psychophysical behavior on color discrimination thresholds, discount of the illuminant and corresponding pairs in asymmetric color matching, emerge directly from realistic data regularities assuming no a priori functional form. These results provide stronger evidence for the hypothesis of a statistically driven organization of color sensors. Moreover, the obtained results suggest that color perception at this low abstraction level may be guided by an error minimization strategy rather than by the information maximization principle. version:1
arxiv-1602-00229 | Iterative Gaussianization: from ICA to Random Rotations | http://arxiv.org/abs/1602.00229 | id:1602.00229 author:Valero Laparra, Gustavo Camps-Valls, Jesús Malo category:stat.ML  published:2016-01-31 summary:Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation. version:1
arxiv-1602-00224 | Order-aware Convolutional Pooling for Video Based Action Recognition | http://arxiv.org/abs/1602.00224 | id:1602.00224 author:Peng Wang, Lingqiao Liu, Chunhua Shen, Heng Tao Shen category:cs.CV  published:2016-01-31 summary:Most video based action recognition approaches create the video-level representation by temporally pooling the features extracted at each frame. The pooling methods that they adopt, however, usually completely or partially neglect the dynamic information contained in the temporal domain, which may undermine the discriminative power of the resulting video representation since the video sequence order could unveil the evolution of a specific event or action. To overcome this drawback and explore the importance of incorporating the temporal order information, in this paper we propose a novel temporal pooling approach to aggregate the frame-level features. Inspired by the capacity of Convolutional Neural Networks (CNN) in making use of the internal structure of images for information abstraction, we propose to apply the temporal convolution operation to the frame-level representations to extract the dynamic information. However, directly implementing this idea on the original high-dimensional feature would inevitably result in parameter explosion. To tackle this problem, we view the temporal evolution of the feature value at each feature dimension as a 1D signal and learn a unique convolutional filter bank for each of these 1D signals. We conduct experiments on two challenging video-based action recognition datasets, HMDB51 and UCF101; and demonstrate that the proposed method is superior to the conventional pooling methods. version:1
arxiv-1602-00223 | Variance-Reduced Second-Order Methods | http://arxiv.org/abs/1602.00223 | id:1602.00223 author:Luo Luo, Zihao Chen, Zhihua Zhang, Wu-Jun Li category:cs.LG stat.ML  published:2016-01-31 summary:In this paper, we discuss the problem of minimizing the sum of two convex functions: a smooth function plus a non-smooth function. Further, the smooth part can be expressed by the average of a large number of smooth component functions, and the non-smooth part is equipped with a simple proximal mapping. We propose a proximal stochastic second-order method, which is efficient and scalable. It incorporates the Hessian in the smooth part of the function and exploits multistage scheme to reduce the variance of the stochastic gradient. We prove that our method can achieve linear rate of convergence. version:1
arxiv-1602-00221 | Principal Polynomial Analysis | http://arxiv.org/abs/1602.00221 | id:1602.00221 author:Valero Laparra, Sandra Jiménez, Devis Tuia, Gustau Camps-Valls, Jesús Malo category:stat.ML  published:2016-01-31 summary:This paper presents a new framework for manifold learning based on a sequence of principal polynomials that capture the possibly nonlinear nature of the data. The proposed Principal Polynomial Analysis (PPA) generalizes PCA by modeling the directions of maximal variance by means of curves, instead of straight lines. Contrarily to previous approaches, PPA reduces to performing simple univariate regressions, which makes it computationally feasible and robust. Moreover, PPA shows a number of interesting analytical properties. First, PPA is a volume-preserving map, which in turn guarantees the existence of the inverse. Second, such an inverse can be obtained in closed form. Invertibility is an important advantage over other learning methods, because it permits to understand the identified features in the input domain where the data has physical meaning. Moreover, it allows to evaluate the performance of dimensionality reduction in sensible (input-domain) units. Volume preservation also allows an easy computation of information theoretic quantities, such as the reduction in multi-information after the transform. Third, the analytical nature of PPA leads to a clear geometrical interpretation of the manifold: it allows the computation of Frenet-Serret frames (local features) and of generalized curvatures at any point of the space. And fourth, the analytical Jacobian allows the computation of the metric induced by the data, thus generalizing the Mahalanobis distance. These properties are demonstrated theoretically and illustrated experimentally. The performance of PPA is evaluated in dimensionality and redundancy reduction, in both synthetic and real datasets from the UCI repository. version:1
arxiv-1602-00217 | Image Denoising with Kernels based on Natural Image Relations | http://arxiv.org/abs/1602.00217 | id:1602.00217 author:Valero Laparra, Juan Gutiérrez, Gustavo Camps-Valls, Jesús Malo category:cs.CV stat.ML  published:2016-01-31 summary:A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. However, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefficients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefficients are specific to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The specific signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. Training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions of signal and noise in order to enforce similarity. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefficient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a more flexible (model-free) alternative to the explicit description of wavelet coefficient relations for image denoising. version:1
arxiv-1602-00214 | Dimensionality Reduction via Regression in Hyperspectral Imagery | http://arxiv.org/abs/1602.00214 | id:1602.00214 author:Valero Laparra, Jesus Malo, Gustau Camps-Valls category:stat.ML cs.CV  published:2016-01-31 summary:This paper introduces a new unsupervised method for dimensionality reduction via regression (DRR). The algorithm belongs to the family of invertible transforms that generalize Principal Component Analysis (PCA) by using curvilinear instead of linear features. DRR identifies the nonlinear features through multivariate regression to ensure the reduction in redundancy between he PCA coefficients, the reduction of the variance of the scores, and the reduction in the reconstruction error. More importantly, unlike other nonlinear dimensionality reduction methods, the invertibility, volume-preservation, and straightforward out-of-sample extension, makes DRR interpretable and easy to apply. The properties of DRR enable learning a more broader class of data manifolds than the recently proposed Non-linear Principal Components Analysis (NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performance of the representation in reducing the dimensionality of remote sensing data. In particular, we tackle two common problems: processing very high dimensional spectral information such as in hyperspectral image sounding data, and dealing with spatial-spectral image patches of multispectral images. Both settings pose collinearity and ill-determination problems. Evaluation of the expressive power of the features is assessed in terms of truncation error, estimating atmospheric variables, and surface land cover classification error. Results show that DRR outperforms linear PCA and recently proposed invertible extensions based on neural networks (NLPCA) and univariate regressions (PPA). version:1
arxiv-1602-00206 | Unsupervised Deep Hashing for Large-scale Visual Search | http://arxiv.org/abs/1602.00206 | id:1602.00206 author:Zhaoqiang Xia, Xiaoyi Feng, Jinye Peng, Abdenour Hadid category:cs.CV cs.LG  published:2016-01-31 summary:Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. Extensive experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to state-of-the-art. version:1
arxiv-1602-00203 | Greedy Deep Dictionary Learning | http://arxiv.org/abs/1602.00203 | id:1602.00203 author:Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa category:cs.LG cs.AI stat.ML  published:2016-01-31 summary:In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all. version:1
arxiv-1601-05613 | Partial Sum Minimization of Singular Values Representation on Grassmann Manifolds | http://arxiv.org/abs/1601.05613 | id:1601.05613 author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV  published:2016-01-21 summary:As a significant subspace clustering method, low rank representation (LRR) has attracted great attention in recent years. To further improve the performance of LRR and extend its applications, there are several issues to be resolved. The nuclear norm in LRR does not sufficiently use the prior knowledge of the rank which is known in many practical problems. The LRR is designed for vectorial data from linear spaces, thus not suitable for high dimensional data with intrinsic non-linear manifold structure. This paper proposes an extended LRR model for manifold-valued Grassmann data which incorporates prior knowledge by minimizing partial sum of singular values instead of the nuclear norm, namely Partial Sum minimization of Singular Values Representation (GPSSVR). The new model not only enforces the global structure of data in low rank, but also retains important information by minimizing only smaller singular values. To further maintain the local structures among Grassmann points, we also integrate the Laplacian penalty with GPSSVR. An effective algorithm is proposed to solve the optimization problem based on the GPSSVR model. The proposed model and algorithms are assessed on some widely used human action video datasets and a real scenery dataset. The experimental results show that the proposed methods obviously outperform other state-of-the-art methods. version:2
arxiv-1602-00177 | Tracing liquid level and material boundaries in transparent vessels using the graph cut computer vision approach | http://arxiv.org/abs/1602.00177 | id:1602.00177 author:Sagi Eppel category:cs.CV  published:2016-01-31 summary:Detection of boundaries of materials stored in transparent vessels is essential for identifying properties such as liquid level and phase boundaries, which are vital for controlling numerous processes in the industry and chemistry laboratory. This work presents a computer vision method for identifying the boundary of materials in transparent vessels using the graph-cut algorithm. The method receives an image of a transparent vessel containing a material and the contour of the vessel in the image. The boundary of the material in the vessel is found by the graph cut method. In general the method uses the vessel region of the image to create a graph, where pixels are vertices, and the cost of an edge between two pixels is inversely correlated with their intensity difference. The bottom 10% of the vessel region in the image is assumed to correspond to the material phase and defined as the graph and source. The top 10% of the pixels in the vessels are assumed to correspond to the air phase and defined as the graph sink. The minimal cut that splits the resulting graph between the source and sink (hence, material and air) is traced using the max-flow/min-cut approach. This cut corresponds to the boundary of the material in the image. The method gave high accuracy in boundary recognition for a wide range of liquid, solid, granular and powder materials in various glass vessels from everyday life and the chemistry laboratory, such as bottles, jars, Glasses, Chromotography colums and separatory funnels. version:1
arxiv-1602-00172 | Deep Learning For Smile Recognition | http://arxiv.org/abs/1602.00172 | id:1602.00172 author:Patrick O. Glauner category:cs.CV cs.LG cs.NE  published:2016-01-30 summary:Inspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. A smile recognition test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. The novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. This is feasible because all experiments were run on a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations on a CPU. version:1
arxiv-1602-00163 | A multiple instance learning approach for sequence data with across bag dependencies | http://arxiv.org/abs/1602.00163 | id:1602.00163 author:Manel Zoghlami, Sabeur Aridhi, Haitham Sghaier, Mondher Maddouri, Engelbert Mephu Nguifo category:cs.LG  published:2016-01-30 summary:In Multiple Instance Learning (MIL) problem for sequence data, the learning data consist of a set of bags where each bag contains a set of instances/sequences. In many real world applications such as bioinformatics, web mining, and text mining, comparing a random couple of sequences makes no sense. In fact, each instance of each bag may have structural and/or temporal relation with other instances in other bags. Thus, the classification task should take into account the relation between semantically related instances across bags. In this paper, we present two novel MIL approaches for sequence data classification: (1) ABClass and (2) ABSim. In ABClass, each sequence is represented by one vector of attributes. For each sequence of the unknown bag, a discriminative classifier is applied in order to compute a partial classification result. Then, an aggregation method is applied to these partial results in order to generate the final result. In ABSim, we use a similarity measure between each sequence of the unknown bag and the corresponding sequences in the learning bags. An unknown bag is labeled with the bag that presents more similar sequences. We applied both approaches to the problem of bacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated and discussed the proposed approaches on well known Ionizing Radiation Resistance Bacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented by primary structure of basal DNA repair proteins. The experimental results show that both ABClass and ABSim approaches are efficient. version:1
arxiv-1601-00318 | A Unified Approach for Learning the Parameters of Sum-Product Networks | http://arxiv.org/abs/1601.00318 | id:1601.00318 author:Han Zhao, Pascal Poupart category:cs.LG cs.AI  published:2016-01-03 summary:We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. Both the projected gradient descent (PGD) and the exponentiated gradient (EG) in this setting can be viewed as first order approximations of the signomial program after proper transformation of the objective function. Based on the signomial program formulation, we construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general. Extensive experiments on 20 data sets demonstrate the effectiveness and efficiency of the two proposed approaches for learning SPNs. We also show that the proposed methods can improve the performance of structure learning and yield state-of-the-art results. version:3
arxiv-1602-00110 | DNA-inspired online behavioral modeling and its application to spambot detection | http://arxiv.org/abs/1602.00110 | id:1602.00110 author:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi category:cs.SI cs.CR cs.LG H.2.8.d; I.2.4  published:2016-01-30 summary:We propose a strikingly novel, simple, and effective approach to model online user behavior: we extract and analyze digital DNA sequences from user online actions and we use Twitter as a benchmark to test our proposal. We obtain an incisive and compact DNA-inspired characterization of user actions. Then, we apply standard DNA analysis techniques to discriminate between genuine and spambot accounts on Twitter. An experimental campaign supports our proposal, showing its effectiveness and viability. To the best of our knowledge, we are the first ones to identify and adapt DNA-inspired techniques to online user behavioral modeling. While Twitter spambot detection is a specific use case on a specific social media, our proposed methodology is platform and technology agnostic, hence paving the way for diverse behavioral characterization tasks. version:1
arxiv-1602-00104 | Extracting Keyword for Disambiguating Name Based on the Overlap Principle | http://arxiv.org/abs/1602.00104 | id:1602.00104 author:Mahyuddin K. M. Nasution category:cs.IR cs.CL F.2.2; I.2.7  published:2016-01-30 summary:Name disambiguation has become one of the main themes in the Semantic Web agenda. The semantic web is an extension of the current Web in which information is not only given well-defined meaning, but also has many purposes that contain the ambiguous naturally or a lot of thing came with the overlap, mainly deals with the persons name. Therefore, we develop an approach to extract keywords from web snippet with utilizing the overlap principle, a concept to understand things with ambiguous, whereby features of person are generated for dealing with the variety of web, the web is steadily gaining ground in the semantic research. version:1
arxiv-1602-00078 | Latent common manifold learning with alternating diffusion: analysis and applications | http://arxiv.org/abs/1602.00078 | id:1602.00078 author:Ronen Talmon, Hau-tieng Wu category:physics.data-an cs.DS math.NA stat.ML  published:2016-01-30 summary:The analysis of data sets arising from multiple sensors has drawn significant research attention over the years. Traditional methods, including kernel-based methods, are typically incapable of capturing nonlinear geometric structures. We introduce a latent common manifold model underlying multiple sensor observations for the purpose of multimodal data fusion. A method based on alternating diffusion is presented and analyzed; we provide theoretical analysis of the method under the latent common manifold model. To exemplify the power of the proposed framework, experimental results in several applications are reported. version:1
arxiv-1602-00020 | Deep convolutional networks for automated detection of posterior-element fractures on spine CT | http://arxiv.org/abs/1602.00020 | id:1602.00020 author:Holger R. Roth, Yinong Wang, Jianhua Yao, Le Lu, Joseph E. Burns, Ronald M. Summers category:cs.CV  published:2016-01-29 summary:Injuries of the spine, and its posterior elements in particular, are a common occurrence in trauma patients, with potentially devastating consequences. Computer-aided detection (CADe) could assist in the detection and classification of spine fractures. Furthermore, CAD could help assess the stability and chronicity of fractures, as well as facilitate research into optimization of treatment paradigms. In this work, we apply deep convolutional networks (ConvNets) for the automated detection of posterior element fractures of the spine. First, the vertebra bodies of the spine with its posterior elements are segmented in spine CT using multi-atlas label fusion. Then, edge maps of the posterior elements are computed. These edge maps serve as candidate regions for predicting a set of probabilities for fractures along the image edges using ConvNets in a 2.5D fashion (three orthogonal patches in axial, coronal and sagittal planes). We explore three different methods for training the ConvNet using 2.5D patches along the edge maps of 'positive', i.e. fractured posterior-elements and 'negative', i.e. non-fractured elements. An experienced radiologist retrospectively marked the location of 55 displaced posterior-element fractures in 18 trauma patients. We randomly split the data into training and testing cases. In testing, we achieve an area-under-the-curve of 0.857. This corresponds to 71% or 81% sensitivities at 5 or 10 false-positives per patient, respectively. Analysis of our set of trauma patients demonstrates the feasibility of detecting posterior-element fractures in spine CT images using computer vision techniques such as deep convolutional networks. version:1
arxiv-1509-07497 | High Dimensional Data Modeling Techniques for Detection of Chemical Plumes and Anomalies in Hyperspectral Images and Movies | http://arxiv.org/abs/1509.07497 | id:1509.07497 author:Yi, Wang, Guangliang Chen, Mauro Maggioni category:stat.ML  published:2015-09-24 summary:We briefly review recent progress in techniques for modeling and analyzing hyperspectral images and movies, in particular for detecting plumes of both known and unknown chemicals. For detecting chemicals of known spectrum, we extend the technique of using a single subspace for modeling the background to a "mixture of subspaces" model to tackle more complicated background. Furthermore, we use partial least squares regression on a resampled training set to boost performance. For the detection of unknown chemicals we view the problem as an anomaly detection problem, and use novel estimators with low-sampled complexity for intrinsically low-dimensional data in high-dimensions that enable us to model the "normal" spectra and detect anomalies. We apply these algorithms to benchmark data sets made available by the Automated Target Detection program co-funded by NSF, DTRA and NGA, and compare, when applicable, to current state-of-the-art algorithms, with favorable results. version:2
arxiv-1509-02301 | Probabilistic Bag-Of-Hyperlinks Model for Entity Linking | http://arxiv.org/abs/1509.02301 | id:1509.02301 author:Octavian-Eugen Ganea, Marina Ganea, Aurelien Lucchi, Carsten Eickhoff, Thomas Hofmann category:cs.CL  published:2015-09-08 summary:Many fundamental problems in natural language processing rely on determining what entities appear in a given text. Commonly referenced as entity linking, this step is a fundamental component of many NLP tasks such as text understanding, automatic summarization, semantic search or machine translation. Name ambiguity, word polysemy, context dependencies and a heavy-tailed distribution of entities contribute to the complexity of this problem. We here propose a probabilistic approach that makes use of an effective graphical model to perform collective entity disambiguation. Input mentions (i.e.,~linkable token spans) are disambiguated jointly across an entire document by combining a document-level prior of entity co-occurrences with local information captured from mentions and their surrounding context. The model is based on simple sufficient statistics extracted from data, thus relying on few parameters to be learned. Our method does not require extensive feature engineering, nor an expensive training procedure. We use loopy belief propagation to perform approximate inference. The low complexity of our model makes this step sufficiently fast for real-time usage. We demonstrate the accuracy of our approach on a wide range of benchmark datasets, showing that it matches, and in many cases outperforms, existing state-of-the-art methods. version:3
arxiv-1601-08201 | Spectrally Grouped Total Variation Reconstruction for Scatter Imaging Using ADMM | http://arxiv.org/abs/1601.08201 | id:1601.08201 author:Ikenna Odinaka, Yan Kaganovsky, Joel A. Greenberg, Mehadi Hassan, David G. Politte, Joseph A. O'Sullivan, Lawrence Carin, David J. Brady category:math.NA cs.CV  published:2016-01-29 summary:We consider X-ray coherent scatter imaging, where the goal is to reconstruct momentum transfer profiles (spectral distributions) at each spatial location from multiplexed measurements of scatter. Each material is characterized by a unique momentum transfer profile (MTP) which can be used to discriminate between different materials. We propose an iterative image reconstruction algorithm based on a Poisson noise model that can account for photon-limited measurements as well as various second order statistics of the data. To improve image quality, previous approaches use edge-preserving regularizers to promote piecewise constancy of the image in the spatial domain while treating each spectral bin separately. Instead, we propose spectrally grouped regularization that promotes piecewise constant images along the spatial directions but also ensures that the MTPs of neighboring spatial bins are similar, if they contain the same material. We demonstrate that this group regularization results in improvement of both spectral and spatial image quality. We pursue an optimization transfer approach where convex decompositions are used to lift the problem such that all hyper-voxels can be updated in parallel and in closed-form. The group penalty introduces a challenge since it is not directly amendable to these decompositions. We use the alternating directions method of multipliers (ADMM) to replace the original problem with an equivalent sequence of sub-problems that are amendable to convex decompositions, leading to a highly parallel algorithm. We demonstrate the performance on real data. version:1
arxiv-1601-08188 | Lipreading with Long Short-Term Memory | http://arxiv.org/abs/1601.08188 | id:1601.08188 author:Michael Wand, Jan Koutník, Jürgen Schmidhuber category:cs.CV cs.CL  published:2016-01-29 summary:Lipreading, i.e. speech recognition from visual-only recordings of a speaker's face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feed-forward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-to-end neural network-based solution (11.6% improvement over the best feature-based solution evaluated). version:1
arxiv-1603-06400 | Joint System and Algorithm Design for Computationally Efficient Fan Beam Coded Aperture X-ray Coherent Scatter Imaging | http://arxiv.org/abs/1603.06400 | id:1603.06400 author:Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P. MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani Krishnamurthy, Anuj Kapadia, Lawrence Carin, David J. Brady category:cs.CV stat.ME  published:2016-01-29 summary:In x-ray coherent scatter tomography, tomographic measurements of the forward scatter distribution are used to infer scatter densities within a volume. A radiopaque 2D pattern placed between the object and the detector array enables the disambiguation between different scatter events. The use of a fan beam source illumination to speed up data acquisition relative to a pencil beam presents computational challenges. To facilitate the use of iterative algorithms based on a penalized Poisson log-likelihood function, efficient computational implementation of the forward and backward models are needed. Our proposed implementation exploits physical symmetries and structural properties of the system and suggests a joint system-algorithm design, where the system design choices are influenced by computational considerations, and in turn lead to reduced reconstruction time. Computational-time speedups of approximately 146 and 32 are achieved in the computation of the forward and backward models, respectively. Results validating the forward model and reconstruction algorithm are presented on simulated analytic and Monte Carlo data. version:1
arxiv-1601-08169 | Kernels for sequentially ordered data | http://arxiv.org/abs/1601.08169 | id:1601.08169 author:Franz J Király, Harald Oberhauser category:stat.ML cs.DM cs.LG math.ST stat.ME stat.TH  published:2016-01-29 summary:We present a novel framework for kernel learning with sequential data of any kind, such as time series, sequences of graphs, or strings. Our approach is based on signature features which can be seen as an ordered variant of sample (cross-)moments; it allows to obtain a "sequentialized" version of any static kernel. The sequential kernels are efficiently computable for discrete sequences and are shown to approximate a continuous moment form in a sampling sense. A number of known kernels for sequences arise as "sequentializations" of suitable static kernels: string kernels may be obtained as a special case, and alignment kernels are closely related up to a modification that resolves their open non-definiteness issue. Our experiments indicate that our signature-based sequential kernel framework may be a promising approach to learning with sequential data, such as time series, that allows to avoid extensive manual pre-processing. version:1
arxiv-1601-08165 | Mapping Tractography Across Subjects | http://arxiv.org/abs/1601.08165 | id:1601.08165 author:Thien Bao Nguyen, Emanuele Olivetti, Paolo Avesani category:stat.ML cs.CV q-bio.NC  published:2016-01-29 summary:Diffusion magnetic resonance imaging (dMRI) and tractography provide means to study the anatomical structures within the white matter of the brain. When studying tractography data across subjects, it is usually necessary to align, i.e. to register, tractographies together. This registration step is most often performed by applying the transformation resulting from the registration of other volumetric images (T1, FA). In contrast with registration methods that "transform" tractographies, in this work, we try to find which streamline in one tractography correspond to which streamline in the other tractography, without any transformation. In other words, we try to find a "mapping" between the tractographies. We propose a graph-based solution for the tractography mapping problem and we explain similarities and differences with the related well-known graph matching problem. Specifically, we define a loss function based on the pairwise streamline distance and reformulate the mapping problem as combinatorial optimization of that loss function. We show preliminary promising results where we compare the proposed method, implemented with simulated annealing, against a standard registration techniques in a task of segmentation of the corticospinal tract. version:1
arxiv-1602-00709 | Quantum perceptron over a field and neural network architecture selection in a quantum computer | http://arxiv.org/abs/1602.00709 | id:1602.00709 author:Adenilton J. da Silva, Teresa B. Ludermir, Wilson R. de Oliveira category:quant-ph cs.NE  published:2016-01-29 summary:In this work, we propose a quantum neural network named quantum perceptron over a field (QPF). Quantum computers are not yet a reality and the models and algorithms proposed in this work cannot be simulated in actual (or classical) computers. QPF is a direct generalization of a classical perceptron and solves some drawbacks found in previous models of quantum perceptrons. We also present a learning algorithm named Superposition based Architecture Learning algorithm (SAL) that optimizes the neural network weights and architectures. SAL searches for the best architecture in a finite set of neural network architectures with linear time over the number of patterns in the training set. SAL is the first learning algorithm to determine neural network architectures in polynomial time. This speedup is obtained by the use of quantum parallelism and a non-linear quantum operator. version:1
arxiv-1601-08068 | Online Sparse Gaussian Process Training with Input Noise | http://arxiv.org/abs/1601.08068 | id:1601.08068 author:Hildo Bijl, Thomas B. Schön, Jan-Willem van Wingerden, Michel Verhaegen category:stat.ML cs.LG cs.SY  published:2016-01-29 summary:Gaussian process regression traditionally has three important downsides. (1) It is computationally intensive, (2) it cannot efficiently implement newly obtained measurements online, and (3) it cannot deal with stochastic (noisy) input points. In this paper we present an algorithm tackling all these three issues simultaneously. The resulting Sparse Online Noisy Input GP (SONIG) regression algorithm can incorporate new measurements in constant runtime. A comparison has shown that it is more accurate than similar existing regression algorithms. In addition, the algorithm can be applied to non-linear black-box system modeling, where its performance is competitive with non-linear ARX models. version:1
arxiv-1601-08057 | On the Geometric Ergodicity of Hamiltonian Monte Carlo | http://arxiv.org/abs/1601.08057 | id:1601.08057 author:Samuel Livingstone, Michael Betancourt, Simon Byrne, Mark Girolami category:stat.CO stat.ME stat.ML  published:2016-01-29 summary:We establish general conditions under under which Markov chains produced by the Hamiltonian Monte Carlo method will and will not be $\pi$-irreducible and geometrically ergodic. We consider implementations with both fixed and dynamic integration times. In the fixed case we find that the conditions for geometric ergodicity are essentially a non-vanishing gradient of the log-density which asymptotically points towards the centre of the space and does not grow faster than linearly. In an idealised scenario in which the integration time is allowed to change in different regions of the space, we show that geometric ergodicity can be recovered for a much broader class of target distributions, leading to some guidelines for the choice of this free parameter in practice. version:1
arxiv-1601-06750 | A Robust UCB Scheme for Active Learning in Regression from Strategic Crowds | http://arxiv.org/abs/1601.06750 | id:1601.06750 author:Divya Padmanabhan, Satyanath Bhat, Dinesh Garg, Shirish Shevade, Y. Narahari category:cs.LG stat.ML  published:2016-01-25 summary:We study the problem of training an accurate linear regression model by procuring labels from multiple noisy crowd annotators, under a budget constraint. We propose a Bayesian model for linear regression in crowdsourcing and use variational inference for parameter estimation. To minimize the number of labels crowdsourced from the annotators, we adopt an active learning approach. In this specific context, we prove the equivalence of well-studied criteria of active learning like entropy minimization and expected error reduction. Interestingly, we observe that we can decouple the problems of identifying an optimal unlabeled instance and identifying an annotator to label it. We observe a useful connection between the multi-armed bandit framework and the annotator selection in active learning. Due to the nature of the distribution of the rewards on the arms, we use the Robust Upper Confidence Bound (UCB) scheme with truncated empirical mean estimator to solve the annotator selection problem. This yields provable guarantees on the regret. We further apply our model to the scenario where annotators are strategic and design suitable incentives to induce them to put in their best efforts. version:2
arxiv-1601-08003 | Efficient Robust Mean Value Calculation of 1D Features | http://arxiv.org/abs/1601.08003 | id:1601.08003 author:Erik Jonsson, Michael Felsberg category:cs.CV  published:2016-01-29 summary:A robust mean value is often a good alternative to the standard mean value when dealing with data containing many outliers. An efficient method for samples of one-dimensional features and the truncated quadratic error norm is presented and compared to the method of channel averaging (soft histograms). version:1
arxiv-1406-5311 | Towards A Deeper Geometric, Analytic and Algorithmic Understanding of Margins | http://arxiv.org/abs/1406.5311 | id:1406.5311 author:Aaditya Ramdas, Javier Peña category:math.OC cs.AI cs.LG math.NA stat.ML  published:2014-06-20 summary:Given a matrix $A$, a linear feasibility problem (of which linear classification is a special case) aims to find a solution to a primal problem $w: A^Tw > \textbf{0}$ or a certificate for the dual problem which is a probability distribution $p: Ap = \textbf{0}$. Inspired by the continued importance of "large-margin classifiers" in machine learning, this paper studies a condition measure of $A$ called its \textit{margin} that determines the difficulty of both the above problems. To aid geometrical intuition, we first establish new characterizations of the margin in terms of relevant balls, cones and hulls. Our second contribution is analytical, where we present generalizations of Gordan's theorem, and variants of Hoffman's theorems, both using margins. We end by proving some new results on a classical iterative scheme, the Perceptron, whose convergence rates famously depends on the margin. Our results are relevant for a deeper understanding of margin-based learning and proving convergence rates of iterative schemes, apart from providing a unifying perspective on this vast topic. version:2
arxiv-1601-07977 | Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain Adaptation | http://arxiv.org/abs/1601.07977 | id:1601.07977 author:Guo-Sen Xie, Xu-Yao Zhang, Shuicheng Yan, Cheng-Lin Liu category:cs.CV  published:2016-01-29 summary:Convolutional neural network (CNN) has achieved state-of-the-art performance in many different visual tasks. Learned from a large-scale training dataset, CNN features are much more discriminative and accurate than the hand-crafted features. Moreover, CNN features are also transferable among different domains. On the other hand, traditional dictionarybased features (such as BoW and SPM) contain much more local discriminative and structural information, which is implicitly embedded in the images. To further improve the performance, in this paper, we propose to combine CNN with dictionarybased models for scene recognition and visual domain adaptation. Specifically, based on the well-tuned CNN models (e.g., AlexNet and VGG Net), two dictionary-based representations are further constructed, namely mid-level local representation (MLR) and convolutional Fisher vector representation (CFV). In MLR, an efficient two-stage clustering method, i.e., weighted spatial and feature space spectral clustering on the parts of a single image followed by clustering all representative parts of all images, is used to generate a class-mixture or a classspecific part dictionary. After that, the part dictionary is used to operate with the multi-scale image inputs for generating midlevel representation. In CFV, a multi-scale and scale-proportional GMM training strategy is utilized to generate Fisher vectors based on the last convolutional layer of CNN. By integrating the complementary information of MLR, CFV and the CNN features of the fully connected layer, the state-of-the-art performance can be achieved on scene recognition and domain adaptation problems. An interested finding is that our proposed hybrid representation (from VGG net trained on ImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained on Place205) greatly. version:1
arxiv-1601-07969 | Selection models of language production support informed text partitioning: an intuitive and practical, bag-of-phrases framework for text analysis | http://arxiv.org/abs/1601.07969 | id:1601.07969 author:Jake Ryland Williams, James P. Bagrow, Andrew J. Reagan, Sharon E. Alajajian, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL  published:2016-01-29 summary:The task of text segmentation, or 'chunking,' may occur at many levels in text analysis, depending on whether it is most beneficial to break it down by paragraphs of a book, sentences of a paragraph, etc. Here, we focus on a fine-grained segmentation task, which we refer to as text partitioning, where we apply methodologies to segment sentences or clauses into phrases, or lexical constructions of one or more words. In the past, we have explored (uniform) stochastic text partitioning---a process on the gaps between words whereby each space assumes one from a binary state of fixed (word binding) or broken (word separating) by some probability. In that work, we narrowly explored perhaps the most naive version of this process: random, or, uniform stochastic partitioning, where all word-word gaps are prescribed a uniformly-set breakage probability, q. Under this framework, the breakage probability is a tunable parameter, and was set to be pure-uniform: q = 1/2. In this work, we explore phrase frequency distributions under variation of the parameter q, and define non-uniform, or informed stochastic partitions, where q is a function of surrounding information. Using a crude but effective function for q, we go on to apply informed partitions to over 20,000 English texts from the Project Gutenberg eBooks database. In these analyses, we connect selection models to generate a notion of fit goodness for the 'bag-of-terms' (words or phrases) representations of texts, and find informed (phrase) partitions to be an improvement over the q = 1 (word) and q = 1/2 (phrase) partitions in most cases. This, together with the scalability of the methods proposed, suggests that the bag-of-phrases model should more often than not be implemented in place of the bag-of-words model, setting the stage for a paradigm shift in feature selection, which lies at the foundation of text analysis methodology. version:1
arxiv-1601-07950 | Face Alignment by Local Deep Descriptor Regression | http://arxiv.org/abs/1601.07950 | id:1601.07950 author:Amit Kumar, Rajeev Ranjan, Vishal Patel, Rama Chellappa category:cs.CV  published:2016-01-29 summary:We present an algorithm for extracting key-point descriptors using deep convolutional neural networks (CNN). Unlike many existing deep CNNs, our model computes local features around a given point in an image. We also present a face alignment algorithm based on regression using these local descriptors. The proposed method called Local Deep Descriptor Regression (LDDR) is able to localize face landmarks of varying sizes, poses and occlusions with high accuracy. Deep Descriptors presented in this paper are able to uniquely and efficiently describe every pixel in the image and therefore can potentially replace traditional descriptors such as SIFT and HOG. Extensive evaluations on five publicly available unconstrained face alignment datasets show that our deep descriptor network is able to capture strong local features around a given landmark and performs significantly better than many competitive and state-of-the-art face alignment algorithms. version:1
arxiv-1601-07947 | Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear Subspace Tracking | http://arxiv.org/abs/1601.07947 | id:1601.07947 author:Fateme Sheikholeslami, Dimitris Berberidis, Georgios B. Giannakis category:stat.ML cs.LG  published:2016-01-28 summary:Kernel-based methods enjoy powerful generalization capabilities in handling a variety of learning tasks. When such methods are provided with sufficient training data, broadly-applicable classes of nonlinear functions can be approximated with desired accuracy. Nevertheless, inherent to the nonparametric nature of kernel-based estimators are computational and memory requirements that become prohibitive with large-scale datasets. In response to this formidable challenge, the present work puts forward a low-rank, kernel-based, feature extraction approach that is particularly tailored for online operation, where data streams need not be stored in memory. A novel generative model is introduced to approximate high-dimensional (possibly infinite) features via a low-rank nonlinear subspace, the learning of which leads to a direct kernel function approximation. Offline and online solvers are developed for the subspace learning task, along with affordable versions, in which the number of stored data vectors is confined to a predefined budget. Analytical results provide performance bounds on how well the kernel matrix as well as kernel-based classification and regression tasks can be approximated by leveraging budgeted online subspace learning and feature extraction schemes. Tests on synthetic and real datasets demonstrate and benchmark the efficiency of the proposed method when linear classification and regression is applied to the extracted features. version:1
arxiv-1601-07925 | Automating biomedical data science through tree-based pipeline optimization | http://arxiv.org/abs/1601.07925 | id:1601.07925 author:Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, Jason H. Moore category:cs.LG cs.NE  published:2016-01-28 summary:Over the past decade, data science and machine learning has grown from a mysterious art form to a staple tool across a variety of fields in academia, business, and government. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement a Tree-based Pipeline Optimization Tool (TPOT) and demonstrate its effectiveness on a series of simulated and real-world genetic data sets. In particular, we show that TPOT can build machine learning pipelines that achieve competitive classification accuracy and discover novel pipeline operators---such as synthetic feature constructors---that significantly improve classification accuracy on these data sets. We also highlight the current challenges to pipeline optimization, such as the tendency to produce pipelines that overfit the data, and suggest future research paths to overcome these challenges. As such, this work represents an early step toward fully automating machine learning pipeline design. version:1
arxiv-1505-03566 | COROLA: A Sequential Solution to Moving Object Detection Using Low-rank Approximation | http://arxiv.org/abs/1505.03566 | id:1505.03566 author:Moein Shakeri, Hong Zhang category:cs.CV cs.RO  published:2015-05-13 summary:Extracting moving objects from a video sequence and estimating the background of each individual image are fundamental issues in many practical applications such as visual surveillance, intelligent vehicle navigation, and traffic monitoring. Recently, some methods have been proposed to detect moving objects in a video via low-rank approximation and sparse outliers where the background is modeled with the computed low-rank component of the video and the foreground objects are detected as the sparse outliers in the low-rank approximation. All of these existing methods work in a batch manner, preventing them from being applied in real time and long duration tasks. In this paper, we present an online sequential framework, namely contiguous outliers representation via online low-rank approximation (COROLA), to detect moving objects and learn the background model at the same time. We also show that our model can detect moving objects with a moving camera. Our experimental evaluation uses simulated data and real public datasets and demonstrates the superior performance of COROLA in terms of both accuracy and execution time. version:2
arxiv-1601-07913 | Parameterized Machine Learning for High-Energy Physics | http://arxiv.org/abs/1601.07913 | id:1601.07913 author:Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, Daniel Whiteson category:hep-ex cs.LG hep-ph  published:2016-01-28 summary:We investigate a new structure for machine learning classifiers applied to problems in high-energy physics by expanding the inputs to include not only measured features but also physics parameters. The physics parameters represent a smoothly varying learning task, and the resulting parameterized classifier can smoothly interpolate between them and replace sets of classifiers trained at individual values. This simplifies the training process and gives improved performance at intermediate values, even for complex problems requiring deep learning. Applications include tools parameterized in terms of theoretical model parameters, such as the mass of a particle, which allow for a single network to provide improved discrimination across a range of masses. This concept is simple to implement and allows for optimized interpolatable results. version:1
arxiv-1509-02900 | Statistical Inference, Learning and Models in Big Data | http://arxiv.org/abs/1509.02900 | id:1509.02900 author:Beate Franke, Jean-François Plante, Ribana Roscher, Annie Lee, Cathal Smyth, Armin Hatefi, Fuqi Chen, Einat Gil, Alexander Schwing, Alessandro Selvitella, Michael M. Hoffman, Roger Grosse, Dieter Hendricks, Nancy Reid category:stat.ML cs.LG 62-07  published:2015-09-09 summary:The need for new methods to deal with big data is a common theme in most scientific fields, although its definition tends to vary with the context. Statistical ideas are an essential part of this, and as a partial response, a thematic program on statistical inference, learning, and models in big data was held in 2015 in Canada, under the general direction of the Canadian Statistical Sciences Institute, with major funding from, and most activities located at, the Fields Institute for Research in Mathematical Sciences. This paper gives an overview of the topics covered, describing challenges and strategies that seem common to many different areas of application, and including some examples of applications to make these challenges and strategies more concrete. version:2
arxiv-1601-07884 | Geo-distinctive Visual Element Matching for Location Estimation of Images | http://arxiv.org/abs/1601.07884 | id:1601.07884 author:Xinchao Li, Martha A. Larson, Alan Hanjalic category:cs.MM cs.CV  published:2016-01-28 summary:We propose an image representation and matching approach that substantially improves visual-based location estimation for images. The main novelty of the approach, called distinctive visual element matching (DVEM), is its use of representations that are specific to the query image whose location is being predicted. These representations are based on visual element clouds, which robustly capture the connection between the query and visual evidence from candidate locations. We then maximize the influence of visual elements that are geo-distinctive because they do not occur in images taken at many other locations. We carry out experiments and analysis for both geo-constrained and geo-unconstrained location estimation cases using two large-scale, publicly-available datasets: the San Francisco Landmark dataset with $1.06$ million street-view images and the MediaEval '15 Placing Task dataset with $5.6$ million geo-tagged images from Flickr. We present examples that illustrate the highly-transparent mechanics of the approach, which are based on common sense observations about the visual patterns in image collections. Our results show that the proposed method delivers a considerable performance improvement compared to the state of the art. version:1
arxiv-1601-07883 | Towards the Design of an End-to-End Automated System for Image and Video-based Recognition | http://arxiv.org/abs/1601.07883 | id:1601.07883 author:Rama Chellappa, Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Vishal M. Patel, Carlos D. Castillo category:cs.CV  published:2016-01-28 summary:Over many decades, researchers working in object recognition have longed for an end-to-end automated system that will simply accept 2D or 3D image or videos as inputs and output the labels of objects in the input data. Computer vision methods that use representations derived based on geometric, radiometric and neural considerations and statistical and structural matchers and artificial neural network-based methods where a multi-layer network learns the mapping from inputs to class labels have provided competing approaches for image recognition problems. Over the last four years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements on object detection/recognition challenge problems. This has been made possible due to the availability of large annotated data, a better understanding of the non-linear mapping between image and class labels as well as the affordability of GPUs. In this paper, we present a brief history of developments in computer vision and artificial neural networks over the last forty years for the problem of image-based recognition. We then present the design details of a deep learning system for end-to-end unconstrained face verification/recognition. Some open issues regarding DCNNs for object recognition problems are then discussed. We caution the readers that the views expressed in this paper are from the authors and authors only! version:1
arxiv-1506-00990 | Unsupervised Learning on Neural Network Outputs: with Application in Zero-shot Learning | http://arxiv.org/abs/1506.00990 | id:1506.00990 author:Yao Lu category:cs.LG  published:2015-06-02 summary:The outputs of a trained neural network contain much richer information than just an one-hot classifier. For example, a neural network might give an image of a dog the probability of one in a million of being a cat but it is still much larger than the probability of being a car. To reveal the hidden structure in them, we apply two unsupervised learning algorithms, PCA and ICA, to the outputs of a deep Convolutional Neural Network trained on the ImageNet of 1000 classes. The PCA/ICA embedding of the object classes reveals their visual similarity and the PCA/ICA components can be interpreted as common visual features shared by similar object classes. For an application, we proposed a new zero-shot learning method, in which the visual features learned by PCA/ICA are employed. Our zero-shot learning method achieves the state-of-the-art results on the ImageNet of over 20000 classes. version:10
arxiv-1509-08971 | Conditional Deep Learning for Energy-Efficient and Enhanced Pattern Recognition | http://arxiv.org/abs/1509.08971 | id:1509.08971 author:Priyadarshini Panda, Abhronil Sengupta, Kaushik Roy category:cs.CV  published:2015-09-29 summary:Deep learning neural networks have emerged as one of the most powerful classification tools for vision related applications. However, the computational and energy requirements associated with such deep nets can be quite high, and hence their energy-efficient implementation is of great interest. Although traditionally the entire network is utilized for the recognition of all inputs, we observe that the classification difficulty varies widely across inputs in real-world datasets; only a small fraction of inputs require the full computational effort of a network, while a large majority can be classified correctly with very low effort. In this paper, we propose Conditional Deep Learning (CDL) where the convolutional layer features are used to identify the variability in the difficulty of input instances and conditionally activate the deeper layers of the network. We achieve this by cascading a linear network of output neurons for each convolutional layer and monitoring the output of the linear network to decide whether classification can be terminated at the current stage or not. The proposed methodology thus enables the network to dynamically adjust the computational effort depending upon the difficulty of the input data while maintaining competitive classification accuracy. We evaluate our approach on the MNIST dataset. Our experiments demonstrate that our proposed CDL yields 1.91x reduction in average number of operations per input, which translates to 1.84x improvement in energy. In addition, our results show an improvement in classification accuracy from 97.5% to 98.9% as compared to the original network. version:6
arxiv-1601-07843 | An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning | http://arxiv.org/abs/1601.07843 | id:1601.07843 author:Nabin K. Mishra, M. Emre Celebi category:cs.CV stat.ML I.4; I.5.4; J.3  published:2016-01-28 summary:The incidence of malignant melanoma continues to increase worldwide. This cancer can strike at any age; it is one of the leading causes of loss of life in young persons. Since this cancer is visible on the skin, it is potentially detectable at a very early stage when it is curable. New developments have converged to make fully automatic early melanoma detection a real possibility. First, the advent of dermoscopy has enabled a dramatic boost in clinical diagnostic ability to the point that melanoma can be detected in the clinic at the very earliest stages. The global adoption of this technology has allowed accumulation of large collections of dermoscopy images of melanomas and benign lesions validated by histopathology. The development of advanced technologies in the areas of image processing and machine learning have given us the ability to allow distinction of malignant melanoma from the many benign mimics that require no biopsy. These new technologies should allow not only earlier detection of melanoma, but also reduction of the large number of needless and costly biopsy procedures. Although some of the new systems reported for these technologies have shown promise in preliminary trials, widespread implementation must await further technical progress in accuracy and reproducibility. In this paper, we provide an overview of computerized detection of melanoma in dermoscopy images. First, we discuss the various aspects of lesion segmentation. Then, we provide a brief overview of clinical feature segmentation. Finally, we discuss the classification stage where machine learning algorithms are applied to the attributes generated from the segmented features to predict the existence of melanoma. version:1
arxiv-1601-07804 | Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor Compressive Sensing | http://arxiv.org/abs/1601.07804 | id:1601.07804 author:Xin Ding, Wei Chen, Ian J. Wassell category:cs.LG cs.IT math.IT  published:2016-01-28 summary:Tensor Compressive Sensing (TCS) is a multidimensional framework of Compressive Sensing (CS), and it is advantageous in terms of reducing the amount of storage, easing hardware implementations and preserving multidimensional structures of signals in comparison to a conventional CS system. In a TCS system, instead of using a random sensing matrix and a predefined dictionary, the average-case performance can be further improved by employing an optimized multidimensional sensing matrix and a learned multilinear sparsifying dictionary. In this paper, we propose a joint optimization approach of the sensing matrix and dictionary for a TCS system. For the sensing matrix design in TCS, an extended separable approach with a closed form solution and a novel iterative non-separable method are proposed when the multilinear dictionary is fixed. In addition, a multidimensional dictionary learning method that takes advantages of the multidimensional structure is derived, and the influence of sensing matrices is taken into account in the learning process. A joint optimization is achieved via alternately iterating the optimization of the sensing matrix and dictionary. Numerical experiments using both synthetic data and real images demonstrate the superiority of the proposed approaches. version:1
arxiv-1509-07009 | Is Image Super-resolution Helpful for Other Vision Tasks? | http://arxiv.org/abs/1509.07009 | id:1509.07009 author:Dengxin Dai, Yujian Wang, Yuhua Chen, Luc Van Gool category:cs.CV  published:2015-09-23 summary:Despite the great advances made in the field of image super-resolution (ISR) during the last years, the performance has merely been evaluated perceptually. Thus, it is still unclear whether ISR is helpful for other vision tasks. In this paper, we present the first comprehensive study and analysis of the usefulness of ISR for other vision applications. In particular, six ISR methods are evaluated on four popular vision tasks, namely edge detection, semantic image segmentation, digit recognition, and scene recognition. We show that applying ISR to input images of other vision systems does improve their performance when the input images are of low-resolution. We also study the correlation between four standard perceptual evaluation criteria (namely PSNR, SSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments show that they correlate well with each other in general, but perceptual criteria are still not accurate enough to be used as full proxies for the usefulness. We hope this work will inspire the community to evaluate ISR methods also in real vision applications, and to adopt ISR as a pre-processing step of other vision tasks if the resolution of their input images is low. version:2
arxiv-1601-06581 | Character-Level Incremental Speech Recognition with Recurrent Neural Networks | http://arxiv.org/abs/1601.06581 | id:1601.06581 author:Kyuyeon Hwang, Wonyong Sung category:cs.CL cs.LG cs.NE  published:2016-01-25 summary:In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal classification (CTC), and an RNN-based character-level language model (LM). The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary (OOV) words according to pronunciation. The proposed model achieves the word error rate (WER) of 8.90% on the Wall Street Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284 training set. version:2
arxiv-1601-07721 | Distributed Low Rank Approximation of Implicit Functions of a Matrix | http://arxiv.org/abs/1601.07721 | id:1601.07721 author:David P. Woodruff, Peilin Zhong category:cs.NA cs.LG  published:2016-01-28 summary:We study distributed low rank approximation in which the matrix to be approximated is only implicitly represented across the different servers. For example, each of $s$ servers may have an $n \times d$ matrix $A^t$, and we may be interested in computing a low rank approximation to $A = f(\sum_{t=1}^s A^t)$, where $f$ is a function which is applied entrywise to the matrix $\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to efficiently compute a $d \times d$ rank-$k$ projection matrix $P$ for which $\ A - AP\ _F^2 \leq \ A - [A]_k\ _F^2 + \varepsilon \ A\ _F^2$, where $AP$ denotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the best rank-$k$ approximation to $A$ given by the singular value decomposition. The communication cost of our protocols is $d \cdot (sk/\varepsilon)^{O(1)}$, and they succeed with high probability. Our framework allows us to efficiently compute a low rank approximation to an entry-wise softmax, to a Gaussian kernel expansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low rank approximation). We also show that our additive error approximation is best possible, in the sense that any protocol achieving relative error for these problems requires significantly more communication. Finally, we experimentally validate our algorithms on real datasets. version:1
arxiv-1601-07714 | Log-Normal Matrix Completion for Large Scale Link Prediction | http://arxiv.org/abs/1601.07714 | id:1601.07714 author:Brian Mohtashemi, Thomas Ketseoglou category:cs.SI cs.LG stat.ML  published:2016-01-28 summary:The ubiquitous proliferation of online social networks has led to the widescale emergence of relational graphs expressing unique patterns in link formation and descriptive user node features. Matrix Factorization and Completion have become popular methods for Link Prediction due to the low rank nature of mutual node friendship information, and the availability of parallel computer architectures for rapid matrix processing. Current Link Prediction literature has demonstrated vast performance improvement through the utilization of sparsity in addition to the low rank matrix assumption. However, the majority of research has introduced sparsity through the limited L1 or Frobenius norms, instead of considering the more detailed distributions which led to the graph formation and relationship evolution. In particular, social networks have been found to express either Pareto, or more recently discovered, Log Normal distributions. Employing the convexity-inducing Lovasz Extension, we demonstrate how incorporating specific degree distribution information can lead to large scale improvements in Matrix Completion based Link prediction. We introduce Log-Normal Matrix Completion (LNMC), and solve the complex optimization problem by employing Alternating Direction Method of Multipliers. Using data from three popular social networks, our experiments yield up to 5% AUC increase over top-performing non-structured sparsity based methods. version:1
arxiv-1506-03498 | Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation | http://arxiv.org/abs/1506.03498 | id:1506.03498 author:Alaa Saade, Florent Krzakala, Lenka Zdeborová category:cond-mat.dis-nn cs.LG stat.ML  published:2015-06-10 summary:The completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank $r$ reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries. We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank $r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$. We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches. version:3
arxiv-1312-7219 | Combining persistent homology and invariance groups for shape comparison | http://arxiv.org/abs/1312.7219 | id:1312.7219 author:Patrizio Frosini, Grzegorz Jablonski category:math.AT cs.CG cs.CV I.4.7; I.5.1  published:2013-12-27 summary:In many applications concerning the comparison of data expressed by $\mathbb{R}^m$-valued functions defined on a topological space $X$, the invariance with respect to a given group $G$ of self-homeomorphisms of $X$ is required. While persistent homology is quite efficient in the topological and qualitative comparison of this kind of data when the invariance group $G$ is the group $\mathrm{Homeo}(X)$ of all self-homeomorphisms of $X$, this theory is not tailored to manage the case in which $G$ is a proper subgroup of $\mathrm{Homeo}(X)$, and its invariance appears too general for several tasks. This paper proposes a way to adapt persistent homology in order to get invariance just with respect to a given group of self-homeomorphisms of $X$. The main idea consists in a dual approach, based on considering the set of all $G$-invariant non-expanding operators defined on the space of the admissible filtering functions on $X$. Some theoretical results concerning this approach are proven and two experiments are presented. An experiment illustrates the application of the proposed technique to compare 1D-signals, when the invariance is expressed by the group of affinities, the group of orientation-preserving affinities, the group of isometries, the group of translations and the identity group. Another experiment shows how our technique can be used for image comparison. version:4
arxiv-1512-02109 | Obtaining A Linear Combination of the Principal Components of a Matrix on Quantum Computers | http://arxiv.org/abs/1512.02109 | id:1512.02109 author:Anmer Daskin category:quant-ph cs.LG math.ST stat.TH  published:2015-11-26 summary:Principal component analysis is a multivariate statistical method frequently used in science and engineering to reduce the dimension of a problem or extract the most significant features from a dataset. In this paper, using a similar notion to the quantum counting, we show how to apply the amplitude amplification together with the phase estimation algorithm to an operator in order to procure the eigenvectors of the operator associated to the eigenvalues defined in the range $\left[a, b\right]$, where $a$ and $b$ are real and $0 \leq a \leq b \leq 1$. This makes possible to obtain a combination of the eigenvectors associated to the largest eigenvalues and so can be used to do principal component analysis on quantum computers. version:3
arxiv-1506-08387 | Robustness Analysis of Preconditioned Successive Projection Algorithm for General Form of Separable NMF Problem | http://arxiv.org/abs/1506.08387 | id:1506.08387 author:Tomohiko Mizutani category:stat.ML math.OC  published:2015-06-28 summary:The successive projection algorithm (SPA) has been known to work well for separable nonnegative matrix factorization (NMF) problems arising in applications, such as topic extraction from documents and endmember detection in hyperspectral images. One of the reasons is in that the algorithm is robust to noise. Gillis and Vavasis showed in [SIAM J. Optim., 25(1), pp. 677-698, 2015] that a preconditioner can further enhance its noise robustness. The proof rested on the condition that the dimension $d$ and factorization rank $r$ in the separable NMF problem coincide with each other. However, it may be unrealistic to expect that the condition holds in separable NMF problems appearing in actual applications; in such problems, $d$ is usually greater than $r$. This paper shows, without the condition $d=r$, that the preconditioned SPA is robust to noise. version:2
arxiv-1301-2444 | TEI and LMF crosswalks | http://arxiv.org/abs/1301.2444 | id:1301.2444 author:Laurent Romary category:cs.CL  published:2013-01-11 summary:The present paper explores various arguments in favour of making the Text Encoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO standard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the issues that would have to be resolved in order to reach an appropriate implementation of these ideas, in particular in terms of infor-mational coverage. We show how the customisation facilities offered by the TEI guidelines can provide an adequate background, not only to cover missing components within the current Dictionary chapter of the TEI guidelines, but also to allow specific lexical projects to deal with local constraints. We expect this proposal to be a basis for a future ISO project in the context of the on going revision of LMF. version:3
arxiv-1601-07665 | Non-Gaussian Component Analysis with Log-Density Gradient Estimation | http://arxiv.org/abs/1601.07665 | id:1601.07665 author:Hiroaki Sasaki, Gang Niu, Masashi Sugiyama category:stat.ML  published:2016-01-28 summary:Non-Gaussian component analysis (NGCA) is aimed at identifying a linear subspace such that the projected data follows a non-Gaussian distribution. In this paper, we propose a novel NGCA algorithm based on log-density gradient estimation. Unlike existing methods, the proposed NGCA algorithm identifies the linear subspace by using the eigenvalue decomposition without any iterative procedures, and thus is computationally reasonable. Furthermore, through theoretical analysis, we prove that the identified subspace converges to the true subspace at the optimal parametric rate. Finally, the practical performance of the proposed algorithm is demonstrated on both artificial and benchmark datasets. version:1
arxiv-1508-00964 | MAP Support Detection for Greedy Sparse Signal Recovery Algorithms in Compressive Sensing | http://arxiv.org/abs/1508.00964 | id:1508.00964 author:Namyoon Lee category:cs.IT cs.LG math.IT  published:2015-08-05 summary:A reliable support detection is essential for a greedy algorithm to reconstruct a sparse signal accurately from compressed and noisy measurements. This paper proposes a novel support detection method for greedy algorithms, which is referred to as "\textit{maximum a posteriori (MAP) support detection}". Unlike existing support detection methods that identify support indices with the largest correlation value in magnitude per iteration, the proposed method selects them with the largest likelihood ratios computed under the true and null support hypotheses by simultaneously exploiting the distributions of sensing matrix, sparse signal, and noise. Leveraging this technique, MAP-Matching Pursuit (MAP-MP) is first presented to show the advantages of exploiting the proposed support detection method, and a sufficient condition for perfect signal recovery is derived for the case when the sparse signal is binary. Subsequently, a set of iterative greedy algorithms, called MAP-generalized Orthogonal Matching Pursuit (MAP-gOMP), MAP-Compressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-Subspace Pursuit (MAP-SP) are presented to demonstrate the applicability of the proposed support detection method to existing greedy algorithms. From empirical results, it is shown that the proposed greedy algorithms with highly reliable support detection can be better, faster, and easier to implement than basis pursuit via linear programming. version:2
arxiv-1601-07649 | Discriminative Training of Deep Fully-connected Continuous CRF with Task-specific Loss | http://arxiv.org/abs/1601.07649 | id:1601.07649 author:Fayao Liu, Guosheng Lin, Chunhua Shen category:cs.CV  published:2016-01-28 summary:Recent works on deep conditional random fields (CRF) have set new records on many vision tasks involving structured predictions. Here we propose a fully-connected deep continuous CRF model for both discrete and continuous labelling problems. We exemplify the usefulness of the proposed model on multi-class semantic labelling (discrete) and the robust depth estimation (continuous) problems. In our framework, we model both the unary and the pairwise potential functions as deep convolutional neural networks (CNN), which are jointly learned in an end-to-end fashion. The proposed method possesses the main advantage of continuously-valued CRF, which is a closed-form solution for the Maximum a posteriori (MAP) inference. To better adapt to different tasks, instead of using the commonly employed maximum likelihood CRF parameter learning protocol, we propose task-specific loss functions for learning the CRF parameters. It enables direct optimization of the quality of the MAP estimates during the course of learning. Specifically, we optimize the multi-class classification loss for the semantic labelling task and the Turkey's biweight loss for the robust depth estimation problem. Experimental results on the semantic labelling and robust depth estimation tasks demonstrate that the proposed method compare favorably against both baseline and state-of-the-art methods. In particular, we show that although the proposed deep CRF model is continuously valued, with the equipment of task-specific loss, it achieves impressive results even on discrete labelling tasks. version:1
arxiv-1601-07630 | Automatic Generation of Building Models Using 2D Maps and Street View Images | http://arxiv.org/abs/1601.07630 | id:1601.07630 author:Jiangye Yuan, Anil M. Cheriyadat category:cs.CV  published:2016-01-28 summary:We introduce a new approach for generating simple 3D building models by combining building footprints from 2D maps with street level images. The approach works with crowd sourced maps such as the OpenStreetMap and street level images acquired by a calibrated camera mounted on a moving vehicle. Buildings are modeled as boxes extruded from building footprints with the height information estimated from images. Building footprints are elevated in world coordinates and projected onto images. Building heights are estimated by scoring projected footprints based on their alignment with visible building features in images. However, it is challenging to achieve accurate projections due to camera pose errors inherited from external sensors resulting in incorrect height estimation. We derive a solution to precisely locate cameras on maps using correspondence between image features and building footprints. We tightly couple the camera localization and height estimation steps to produce an effective method for 3D building model generation. Experiments using Google Street View images and publicly available map data show the promise of our method. version:1
arxiv-1512-03883 | Sparse Generalized Principal Component Analysis for Large-scale Applications beyond Gaussianity | http://arxiv.org/abs/1512.03883 | id:1512.03883 author:Qiaoya Zhang, Yiyuan She category:stat.CO stat.ML  published:2015-12-12 summary:Principal Component Analysis (PCA) is a dimension reduction technique. It produces inconsistent estimators when the dimensionality is moderate to high, which is often the problem in modern large-scale applications where algorithm scalability and model interpretability are difficult to achieve, not to mention the prevalence of missing values. While existing sparse PCA methods alleviate inconsistency, they are constrained to the Gaussian assumption of classical PCA and fail to address algorithm scalability issues. We generalize sparse PCA to the broad exponential family distributions under high-dimensional setup, with built-in treatment for missing values. Meanwhile we propose a family of iterative sparse generalized PCA (SG-PCA) algorithms such that despite the non-convexity and non-smoothness of the optimization task, the loss function decreases in every iteration. In terms of ease and intuitive parameter tuning, our sparsity-inducing regularization is far superior to the popular Lasso. Furthermore, to promote overall scalability, accelerated gradient is integrated for fast convergence, while a progressive screening technique gradually squeezes out nuisance dimensions of a large-scale problem for feasible optimization. High-dimensional simulation and real data experiments demonstrate the efficiency and efficacy of SG-PCA. version:2
arxiv-1601-07621 | Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks | http://arxiv.org/abs/1601.07621 | id:1601.07621 author:Evan Racah, Seyoon Ko, Peter Sadowski, Wahid Bhimji, Craig Tull, Sang-Yun Oh, Pierre Baldi, Prabhat category:stat.ML cs.LG physics.data-an  published:2016-01-28 summary:Experiments in particle physics produce enormous quantities of data that must be analyzed and interpreted by teams of physicists. This analysis is often exploratory, where scientists are unable to enumerate the possible types of signal prior to performing the experiment. Thus, tools for summarizing, clustering, visualizing and classifying high-dimensional data are essential. In this work, we show that meaningful physical content can be revealed by transforming the raw data into a learned high-level representation using deep neural networks, with measurements taken at the Daya Bay Neutrino Experiment as a case study. We further show how convolutional deep neural networks can provide an effective classification filter with greater than 97% accuracy across different classes of physics events, significantly better than other machine learning approaches. version:1
arxiv-1601-07596 | Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization | http://arxiv.org/abs/1601.07596 | id:1601.07596 author:Francisco Chicano, Darrell Whitley, Renato Tinos category:cs.AI cs.NE I.2.8  published:2016-01-27 summary:Local search algorithms and iterated local search algorithms are a basic technique. Local search can be a stand along search methods, but it can also be hybridized with evolutionary algorithms. Recently, it has been shown that it is possible to identify improving moves in Hamming neighborhoods for k-bounded pseudo-Boolean optimization problems in constant time. This means that local search does not need to enumerate neighborhoods to find improving moves. It also means that evolutionary algorithms do not need to use random mutation as a operator, except perhaps as a way to escape local optima. In this paper, we show how improving moves can be identified in constant time for multiobjective problems that are expressed as k-bounded pseudo-Boolean functions. In particular, multiobjective forms of NK Landscapes and Mk Landscapes are considered. version:1
arxiv-1601-07576 | Locally-Supervised Deep Hybrid Model for Scene Recognition | http://arxiv.org/abs/1601.07576 | id:1601.07576 author:Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV  published:2016-01-27 summary:Convolutional neural networks (CNN) have recently achieved remarkable successes in various image classification and understanding tasks. The deep features obtained at the top fully-connected layer of the CNN (FC-features) exhibit rich global semantic information and are extremely effective in image classification. On the other hand, the convolutional features in the middle layers of the CNN also contain meaningful local information, but are not fully explored for image representation. In this paper, we propose a novel Locally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and explores the convolutional features for scene recognition. Firstly, we notice that the convolutional features capture local objects and fine structures of scene images, which yield important cues for discriminating ambiguous scenes, whereas these features are significantly eliminated in the highly-compressed FC representation. Secondly, we propose a new Local Convolutional Supervision (LCS) layer to enhance the local structure of the image by directly propagating the label information to the convolutional layers. Thirdly, we propose an efficient Fisher Convolutional Vector (FCV) that successfully rescues the orderless mid-level semantic information (e.g. objects and textures) of scene image. The FCV encodes the large-sized convolutional maps into a fixed-length mid-level representation, and is demonstrated to be strongly complementary to the high-level FC-features. Finally, both the FCV and FC-features are collaboratively employed in the LSDHM representation, which achieves outstanding performance in our experiments. It obtains 83.75% and 67.56% accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397 datasets, advancing the stat-of-the-art substantially. version:1
arxiv-1601-02068 | Minimax Subsampling for Estimation and Prediction in Low-Dimensional Linear Regression | http://arxiv.org/abs/1601.02068 | id:1601.02068 author:Yining Wang, Aarti Singh category:stat.ML cs.LG math.ST stat.TH  published:2016-01-09 summary:Subsampling strategies are derived to sample a small portion of design (data) points in a low-dimensional linear regression model $y=X\beta+\varepsilon$ with near-optimal statistical rates. Our results apply to both problems of estimation of the underlying linear model $\beta$ and predicting the real-valued response $y$ of a new data point $x$. The derived subsampling strategies are minimax optimal under the fixed design setting, up to a small $(1+\epsilon)$ relative factor. We also give interpretable subsampling probabilities for the random design setting and demonstrate explicit gaps in statistial rates between optimal and baseline (e.g., uniform) subsampling methods. version:2
arxiv-1211-6581 | Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs | http://arxiv.org/abs/1211.6581 | id:1211.6581 author:Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas, William Groves, Ioannis Vlahavas category:cs.LG  published:2012-11-28 summary:In many practical applications of supervised learning the task involves the prediction of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are continuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classification domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called Stacked Single-Target and Ensemble of Regressor Chains, by adapting two popular multi-label classification methods of this family. Furthermore, we highlight an inherent problem of these methods - a discrepancy of the values of the additional input variables between training and prediction - and develop extensions that use out-of-sample estimates of the target variables during training in order to tackle this problem. The results of an extensive experimental evaluation carried out on a large and diverse collection of datasets show that, when the discrepancy is appropriately mitigated, the proposed methods attain consistent improvements over the independent regressions baseline. Moreover, two versions of Ensemble of Regression Chains perform significantly better than four state-of-the-art methods including regularization-based multi-task learning methods and a multi-objective random forest approach. version:5
arxiv-1502-01380 | Artificial neural networks in calibration of nonlinear mechanical models | http://arxiv.org/abs/1502.01380 | id:1502.01380 author:Tomáš Mareš, Eliška Janouchová, Anna Kučerová category:cs.NE cs.CE  published:2015-02-04 summary:Rapid development in numerical modelling of materials and the complexity of new models increases quickly together with their computational demands. Despite the growing performance of modern computers and clusters, calibration of such models from noisy experimental data remains a nontrivial and often computationally exhaustive task. The layered neural networks thus represent a robust and efficient technique to overcome the time-consuming simulations of a calibrated model. The potential of neural networks consists in simple implementation and high versatility in approximating nonlinear relationships. Therefore, there were several approaches proposed to accelerate the calibration of nonlinear models by neural networks. This contribution reviews and compares three possible strategies based on approximating (i) model response, (ii) inverse relationship between the model response and its parameters and (iii) error function quantifying how well the model fits the data. The advantages and drawbacks of particular strategies are demonstrated on the calibration of four parameters of the affinity hydration model from simulated data as well as from experimental measurements. This model is highly nonlinear, but computationally cheap thus allowing its calibration without any approximation and better quantification of results obtained by the examined calibration strategies. The paper can be thus viewed as a guide intended for the engineers to help them select an appropriate strategy in their particular calibration problems. version:2
arxiv-1601-07533 | Osteoporotic and Neoplastic Compression Fracture Classification on Longitudinal CT | http://arxiv.org/abs/1601.07533 | id:1601.07533 author:Yinong Wang, Jianhua Yao, Joseph E. Burns, Ronald M. Summers category:cs.CV q-bio.TO  published:2016-01-27 summary:Classification of vertebral compression fractures (VCF) having osteoporotic or neoplastic origin is fundamental to the planning of treatment. We developed a fracture classification system by acquiring quantitative morphologic and bone density determinants of fracture progression through the use of automated measurements from longitudinal studies. A total of 250 CT studies were acquired for the task, each having previously identified VCFs with osteoporosis or neoplasm. Thirty-six features or each identified VCF were computed and classified using a committee of support vector machines. Ten-fold cross validation on 695 identified fractured vertebrae showed classification accuracies of 0.812, 0.665, and 0.820 for the measured, longitudinal, and combined feature sets respectively. version:1
arxiv-1601-07532 | Learning to Extract Motion from Videos in Convolutional Neural Networks | http://arxiv.org/abs/1601.07532 | id:1601.07532 author:Damien Teney, Martial Hebert category:cs.CV  published:2016-01-27 summary:This paper shows how to extract dense optical flow from videos with a convolutional neural network (CNN). The proposed model constitutes a potential building block for deeper architectures to allow using motion without resorting to an external algorithm, \eg for recognition in videos. We derive our network architecture from signal processing principles to provide desired invariances to image contrast, phase and texture. We constrain weights within the network to enforce strict rotation invariance and substantially reduce the number of parameters to learn. We demonstrate end-to-end training on only 8 sequences of the Middlebury dataset, orders of magnitude less than competing CNN-based motion estimation methods, and obtain comparable performance to classical methods on the Middlebury benchmark. Importantly, our method outputs a distributed representation of motion that allows representing multiple, transparent motions, and dynamic textures. Our contributions on network design and rotation invariance offer insights nonspecific to motion estimation. version:1
arxiv-1509-00116 | FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation | http://arxiv.org/abs/1509.00116 | id:1509.00116 author:M. Salman Asif, Ali Ayremlou, Aswin Sankaranarayanan, Ashok Veeraraghavan, Richard Baraniuk category:cs.CV  published:2015-09-01 summary:FlatCam is a thin form-factor lensless camera that consists of a coded mask placed on top of a bare, conventional sensor array. Unlike a traditional, lens-based camera where an image of the scene is directly recorded on the sensor pixels, each pixel in FlatCam records a linear combination of light from multiple scene elements. A computational algorithm is then used to demultiplex the recorded measurements and reconstruct an image of the scene. FlatCam is an instance of a coded aperture imaging system; however, unlike the vast majority of related work, we place the coded mask extremely close to the image sensor that can enable a thin system. We employ a separable mask to ensure that both calibration and image reconstruction are scalable in terms of memory requirements and computational complexity. We demonstrate the potential of the FlatCam design using two prototypes: one at visible wavelengths and one at infrared wavelengths. version:2
arxiv-1502-05197 | Analysis and approximation of some Shape-from-Shading models for non-Lambertian surfaces | http://arxiv.org/abs/1502.05197 | id:1502.05197 author:Silvia Tozza, Maurizio Falcone category:math.NA cs.CV cs.NA math.AP  published:2015-02-18 summary:The reconstruction of a 3D object or a scene is a classical inverse problem in Computer Vision. In the case of a single image this is called the Shape-from-Shading (SfS) problem and it is known to be ill-posed even in a simplified version like the vertical light source case. A huge number of works deals with the orthographic SfS problem based on the Lambertian reflectance model, the most common and simplest model which leads to an eikonal type equation when the light source is on the vertical axis. In this paper we want to study non-Lambertian models since they are more realistic and suitable whenever one has to deal with different kind of surfaces, rough or specular. We will present a unified mathematical formulation of some popular orthographic non-Lambertian models, considering vertical and oblique light directions as well as different viewer positions. These models lead to more complex stationary nonlinear partial differential equations of Hamilton-Jacobi type which can be regarded as the generalization of the classical eikonal equation corresponding to the Lambertian case. However, all the equations corresponding to the models considered here (Oren-Nayar and Phong) have a similar structure so we can look for weak solutions to this class in the viscosity solution framework. Via this unified approach, we are able to develop a semi-Lagrangian approximation scheme for the Oren-Nayar and the Phong model and to prove a general convergence result. Numerical simulations on synthetic and real images will illustrate the effectiveness of this approach and the main features of the scheme, also comparing the results with previous results in the literature. version:2
arxiv-1601-07482 | Unsupervised Learning in Neuromemristive Systems | http://arxiv.org/abs/1601.07482 | id:1601.07482 author:Cory Merkel, Dhireesha Kudithipudi category:cs.ET cs.LG stat.ML  published:2016-01-27 summary:Neuromemristive systems (NMSs) currently represent the most promising platform to achieve energy efficient neuro-inspired computation. However, since the research field is less than a decade old, there are still countless algorithms and design paradigms to be explored within these systems. One particular domain that remains to be fully investigated within NMSs is unsupervised learning. In this work, we explore the design of an NMS for unsupervised clustering, which is a critical element of several machine learning algorithms. Using a simple memristor crossbar architecture and learning rule, we are able to achieve performance which is on par with MATLAB's k-means clustering. version:1
arxiv-1601-07471 | Shape Distributions of Nonlinear Dynamical Systems for Video-based Inference | http://arxiv.org/abs/1601.07471 | id:1601.07471 author:Vinay Venkataraman, Pavan Turaga category:cs.CV  published:2016-01-27 summary:This paper presents a shape-theoretic framework for dynamical analysis of nonlinear dynamical systems which appear frequently in several video-based inference tasks. Traditional approaches to dynamical modeling have included linear and nonlinear methods with their respective drawbacks. A novel approach we propose is the use of descriptors of the shape of the dynamical attractor as a feature representation of nature of dynamics. The proposed framework has two main advantages over traditional approaches: a) representation of the dynamical system is derived directly from the observational data, without any inherent assumptions, and b) the proposed features show stability under different time-series lengths where traditional dynamical invariants fail. We illustrate our idea using nonlinear dynamical models such as Lorenz and Rossler systems, where our feature representations (shape distribution) support our hypothesis that the local shape of the reconstructed phase space can be used as a discriminative feature. Our experimental analyses on these models also indicate that the proposed framework show stability for different time-series lengths, which is useful when the available number of samples are small/variable. The specific applications of interest in this paper are: 1) activity recognition using motion capture and RGBD sensors, 2) activity quality assessment for applications in stroke rehabilitation, and 3) dynamical scene classification. We provide experimental validation through action and gesture recognition experiments on motion capture and Kinect datasets. In all these scenarios, we show experimental evidence of the favorable properties of the proposed representation. version:1
arxiv-1601-07795 | Distributed User Association in Energy Harvesting Small Cell Networks: A Probabilistic Model | http://arxiv.org/abs/1601.07795 | id:1601.07795 author:Setareh Maghsudi, Ekram Hossain category:cs.IT cs.LG math.IT  published:2016-01-27 summary:We consider a distributed downlink user association problem in a small cell network, where small cells obtain the required energy for providing wireless services to users through ambient energy harvesting. Since energy harvesting is opportunistic in nature, the amount of harvested energy is a random variable, without any a priori known characteristics. Moreover, since users arrive in the network randomly and require different wireless services, the energy consumption is a random variable as well. In this paper, we propose a probabilistic framework to mathematically model and analyze the random behavior of energy harvesting and energy consumption in dense small cell networks. Furthermore, as acquiring (even statistical) channel and network knowledge is very costly in a distributed dense network, we develop a bandit-theoretical formulation for distributed user association when no information is available at users version:1
arxiv-1412-5250 | Hierarchical Vector Autoregression | http://arxiv.org/abs/1412.5250 | id:1412.5250 author:William B. Nicholson, Jacob Bien, David S. Matteson category:stat.ME stat.CO stat.ML  published:2014-12-17 summary:Vector autoregression (VAR) is a fundamental tool for modeling the joint dynamics of multivariate time series. However, as the number of component series is increased, the VAR model quickly becomes overparameterized, making reliable estimation difficult and impeding its adoption as a forecasting tool in high dimensional settings. A number of authors have sought to address this issue by incorporating regularized approaches, such as the lasso, that impose sparse or low-rank structures on the estimated coefficient parameters of the VAR. More traditional approaches attempt to address overparameterization by selecting a low lag order, based on the assumption that dynamic dependence among components is short-range. However, these methods typically assume a single, universal lag order that applies across all components, unnecessarily constraining the dynamic relationship between the components and impeding forecast performance. The lasso-based approaches are more flexible but do not incorporate the notion of lag order selection. We propose a new class of regularized VAR models, called hierarchical vector autoregression (HVAR), that embed the notion of lag selection into a convex regularizer. The key convex modeling tool is a group lasso with nested groups which ensure the sparsity pattern of autoregressive lag coefficients honors the ordered structure inherent to VAR. We provide computationally efficient algorithms for solving HVAR problems that can be parallelized across the components. A simulation study shows the improved performance in forecasting and lag order selection over previous approaches, and a macroeconomic application further highlights forecasting improvements as well as the convenient, interpretable output of a HVAR model. version:2
arxiv-1601-07446 | A First Attempt to Cloud-Based User Verification in Distributed System | http://arxiv.org/abs/1601.07446 | id:1601.07446 author:Marcin Wozniak, Dawid Polap, Grzegorz Borowik, Christian Napoli category:cs.NE cs.AI cs.CR cs.DC  published:2016-01-27 summary:In this paper, the idea of client verification in distributed systems is presented. The proposed solution presents a sample system where client verification through cloud resources using input signature is discussed. For different signatures the proposed method has been examined. Research results are presented and discussed to show potential advantages. version:1
arxiv-1601-07358 | Quantum machine learning with glow for episodic tasks and decision games | http://arxiv.org/abs/1601.07358 | id:1601.07358 author:Jens Clausen, Hans J. Briegel category:quant-ph cs.AI cs.LG  published:2016-01-27 summary:We consider a general class of models, where a reinforcement learning (RL) agent learns from cyclic interactions with an external environment via classical signals. Perceptual inputs are encoded as quantum states, which are subsequently transformed by a quantum channel representing the agent's memory, while the outcomes of measurements performed at the channel's output determine the agent's actions. The learning takes place via stepwise modifications of the channel properties. They are described by an update rule that is inspired by the projective simulation (PS) model and equipped with a glow mechanism that allows for a backpropagation of policy changes, analogous to the eligibility traces in RL and edge glow in PS. In this way, the model combines features of PS with the ability for generalization, offered by its physical embodiment as a quantum system. We apply the agent to various setups of an invasion game and a grid world, which serve as elementary model tasks allowing a direct comparison with a basic classical PS agent. version:1
arxiv-1209-1557 | Learning Model-Based Sparsity via Projected Gradient Descent | http://arxiv.org/abs/1209.1557 | id:1209.1557 author:Sohail Bahmani, Petros T. Boufounos, Bhiksha Raj category:stat.ML cs.LG math.OC 62FXX  65KXX  published:2012-09-07 summary:Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper we study the projected gradient descent with non-convex structured-sparse parameter model as the constraint set. Should the cost function have a Stable Model-Restricted Hessian the algorithm produces an approximation for the desired minimizer. As an example we elaborate on application of the main results to estimation in Generalized Linear Model. version:4
arxiv-1501-01186 | Analysing domain shift factors between videos and images for object detection | http://arxiv.org/abs/1501.01186 | id:1501.01186 author:Vicky Kalogeiton, Vittorio Ferrari, Cordelia Schmid category:cs.CV  published:2015-01-06 summary:Object detection is one of the most important challenges in computer vision. Object detectors are usually trained on bounding-boxes from still images. Recently, video has been used as an alternative source of data. Yet, for a given test domain (image or video), the performance of the detector depends on the domain it was trained on. In this paper, we examine the reasons behind this performance gap. We define and evaluate different domain shift factors: spatial location accuracy, appearance diversity, image quality and aspect distribution. We examine the impact of these factors by comparing performance before and after factoring them out. The results show that all four factors affect the performance of the detectors and their combined effect explains nearly the whole performance gap. version:3
arxiv-1506-08438 | Unsupervised Semantic Parsing of Video Collections | http://arxiv.org/abs/1506.08438 | id:1506.08438 author:Ozan Sener, Amir Zamir, Silvio Savarese, Ashutosh Saxena category:cs.CV  published:2015-06-28 summary:Human communication typically has an underlying structure. This is reflected in the fact that in many user generated videos, a starting point, ending, and certain objective steps between these two can be identified. In this paper, we propose a method for parsing a video into such semantic steps in an unsupervised way. The proposed method is capable of providing a semantic "storyline" of the video composed of its objective steps. We accomplish this using both visual and language cues in a joint generative model. The proposed method can also provide a textual description for each of the identified semantic steps and video segments. We evaluate this method on a large number of complex YouTube videos and show results of unprecedented quality for this intricate and impactful problem. version:4
arxiv-1601-07336 | Neighborhood Preserved Sparse Representation for Robust Classification on Symmetric Positive Definite Matrices | http://arxiv.org/abs/1601.07336 | id:1601.07336 author:Ming Yin, Shengli Xie, Yi Guo, Junbin Gao, Yun Zhang category:cs.CV  published:2016-01-27 summary:Due to its promising classification performance, sparse representation based classification(SRC) algorithm has attracted great attention in the past few years. However, the existing SRC type methods apply only to vector data in Euclidean space. As such, there is still no satisfactory approach to conduct classification task for symmetric positive definite (SPD) matrices which is very useful in computer vision. To address this problem, in this paper, a neighborhood preserved kernel SRC method is proposed on SPD manifolds. Specifically, by embedding the SPD matrices into a Reproducing Kernel Hilbert Space (RKHS), the proposed method can perform classification on SPD manifolds through an appropriate Log-Euclidean kernel. Through exploiting the geodesic distance between SPD matrices, our method can effectively characterize the intrinsic local Riemannian geometry within data so as to well unravel the underlying sub-manifold structure. Despite its simplicity, experimental results on several famous database demonstrate that the proposed method achieves better classification results than the state-of-the-art approaches. version:1
arxiv-1504-02763 | Performance measures for classification systems with rejection | http://arxiv.org/abs/1504.02763 | id:1504.02763 author:Filipe Condessa, Jelena Kovacevic, Jose Bioucas-Dias category:cs.CV cs.LG 68-04  published:2015-04-10 summary:Classifiers with rejection are essential in real-world applications where misclassifications and their effects are critical. However, if no problem specific cost function is defined, there are no established measures to assess the performance of such classifiers. We introduce a set of desired properties for performance measures for classifiers with rejection, based on which we propose a set of three performance measures for the evaluation of the performance of classifiers with rejection that satisfy the desired properties. The nonrejected accuracy measures the ability of the classifier to accurately classify nonrejected samples; the classification quality measures the correct decision making of the classifier with rejector; and the rejection quality measures the ability to concentrate all misclassified samples onto the set of rejected samples. From the measures, we derive the concept of relative optimality that allows us to connect the measures to a family of cost functions that take into account the trade-off between rejection and misclassification. We illustrate the use of the proposed performance measures on classifiers with rejection applied to synthetic and real-world data. version:2
arxiv-1504-02089 | The Computational Power of Optimization in Online Learning | http://arxiv.org/abs/1504.02089 | id:1504.02089 author:Elad Hazan, Tomer Koren category:cs.LG cs.GT  published:2015-04-08 summary:We consider the fundamental problem of prediction with expert advice where the experts are "optimizable": there is a black-box optimization oracle that can be used to compute, in constant time, the leading expert in retrospect at any point in time. In this setting, we give a novel online algorithm that attains vanishing regret with respect to $N$ experts in total $\widetilde{O}(\sqrt{N})$ computation time. We also give a lower bound showing that this running time cannot be improved (up to log factors) in the oracle model, thereby exhibiting a quadratic speedup as compared to the standard, oracle-free setting where the required time for vanishing regret is $\widetilde{\Theta}(N)$. These results demonstrate an exponential gap between the power of optimization in online learning and its power in statistical learning: in the latter, an optimization oracle---i.e., an efficient empirical risk minimizer---allows to learn a finite hypothesis class of size $N$ in time $O(\log{N})$. We also study the implications of our results to learning in repeated zero-sum games, in a setting where the players have access to oracles that compute, in constant time, their best-response to any mixed strategy of their opponent. We show that the runtime required for approximating the minimax value of the game in this setting is $\widetilde{\Theta}(\sqrt{N})$, yielding again a quadratic improvement upon the oracle-free setting, where $\widetilde{\Theta}(N)$ is known to be tight. version:4
arxiv-1601-07270 | Comprehensive Feature-based Robust Video Fingerprinting Using Tensor Model | http://arxiv.org/abs/1601.07270 | id:1601.07270 author:Xiushan Nie, Yilong Yin, Jiande Sun category:cs.CV  published:2016-01-27 summary:Content-based near-duplicate video detection (NDVD) is essential for effective search and retrieval, and robust video fingerprinting is a good solution for NDVD. Most existing video fingerprinting methods use a single feature or concatenating different features to generate video fingerprints, and show a good performance under single-mode modifications such as noise addition and blurring. However, when they suffer combined modifications, the performance is degraded to a certain extent because such features cannot characterize the video content completely. By contrast, the assistance and consensus among different features can improve the performance of video fingerprinting. Therefore, in the present study, we mine the assistance and consensus among different features based on tensor model, and present a new comprehensive feature to fully use them in the proposed video fingerprinting framework. We also analyze what the comprehensive feature really is for representing the original video. In this framework, the video is initially set as a high-order tensor that consists of different features, and the video tensor is decomposed via the Tucker model with a solution that determines the number of components. Subsequently, the comprehensive feature is generated by the low-order tensor obtained from tensor decomposition. Finally, the video fingerprint is computed using this feature. A matching strategy used for narrowing the search is also proposed based on the core tensor. The robust video fingerprinting framework is resistant not only to single-mode modifications, but also to the combination of them. version:1
arxiv-1601-07265 | Deep Learning Driven Visual Path Prediction from a Single Image | http://arxiv.org/abs/1601.07265 | id:1601.07265 author:Siyu Huang, Xi Li, Zhongfei Zhang, Zhouzhou He, Fei Wu, Wei Liu, Jinhui Tang, Yueting Zhuang category:cs.CV  published:2016-01-27 summary:Capabilities of inference and prediction are significant components of visual systems. In this paper, we address an important and challenging task of them: visual path prediction. Its goal is to infer the future path for a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and motion patterns underlying video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of the considered models. Motivated by these observations, we propose a deep learning framework which simultaneously performs deep feature learning for visual representation in conjunction with spatio-temporal context modeling. After that, we propose a unified path planning scheme to make accurate future path prediction based on the analytic results of the context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scene and motion pattern, consequently improving the performance of the visual path prediction task. In order to comprehensively evaluate the model's performance on the visual path prediction task, we construct two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the existing approaches and owns a better generalization capability. version:1
arxiv-1601-07258 | Fast Integral Image Estimation at 1% measurement rate | http://arxiv.org/abs/1601.07258 | id:1601.07258 author:Kuldeep Kulkarni, Pavan Turaga category:cs.CV math.OC  published:2016-01-27 summary:We propose a framework called ReFInE to directly obtain integral image estimates from a very small number of spatially multiplexed measurements of the scene without iterative reconstruction of any auxiliary image, and demonstrate their practical utility in visual object tracking. Specifically, we design measurement matrices which are tailored to facilitate extremely fast estimation of the integral image, by using a single-shot linear operation on the measured vector. Leveraging a prior model for the images, we formulate a nuclear norm minimization problem with second order conic constraints to jointly obtain the measurement matrix and the linear operator. Through qualitative and quantitative experiments, we show that high quality integral image estimates can be obtained using our framework at very low measurement rates. Further, on a standard dataset of 50 videos, we present object tracking results which are comparable to the state-of-the-art methods, even at an extremely low measurement rate of 1%. version:1
arxiv-1601-07255 | PersonNet: Person Re-identification with Deep Convolutional Neural Networks | http://arxiv.org/abs/1601.07255 | id:1601.07255 author:Lin Wu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2016-01-27 summary:In this paper, we propose a deep end-to-end neu- ral network to simultaneously learn high-level features and a corresponding similarity metric for person re-identification. The network takes a pair of raw RGB images as input, and outputs a similarity value indicating whether the two input images depict the same person. A layer of computing neighborhood range differences across two input images is employed to capture local relationship between patches. This operation is to seek a robust feature from input images. By increasing the depth to 10 weight layers and using very small (3$\times$3) convolution filters, our architecture achieves a remarkable improvement on the prior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp) gradient decent algorithm is integrated into our architecture, which is beneficial to deep nets. Our method consistently outperforms state-of-the-art on two large datasets (CUHK03 and Market-1501), and a medium-sized data set (CUHK01). version:1
arxiv-1601-07252 | Font Identification in Historical Documents Using Active Learning | http://arxiv.org/abs/1601.07252 | id:1601.07252 author:Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard Furuta, Laura Mandell category:cs.CV cs.AI cs.DL stat.AP stat.ML I.5; I.2  published:2016-01-27 summary:Identifying the type of font (e.g., Roman, Blackletter) used in historical documents can help optical character recognition (OCR) systems produce more accurate text transcriptions. Towards this end, we present an active-learning strategy that can significantly reduce the number of labeled samples needed to train a font classifier. Our approach extracts image-based features that exploit geometric differences between fonts at the word level, and combines them into a bag-of-word representation for each page in a document. We evaluate six sampling strategies based on uncertainty, dissimilarity and diversity criteria, and test them on a database containing over 3,000 historical documents with Blackletter, Roman and Mixed fonts. Our results show that a combination of uncertainty and diversity achieves the highest predictive accuracy (89% of test cases correctly classified) while requiring only a small fraction of the data (17%) to be labeled. We discuss the implications of this result for mass digitization projects of historical documents. version:1
arxiv-1512-06757 | GraphConnect: A Regularization Framework for Neural Networks | http://arxiv.org/abs/1512.06757 | id:1512.06757 author:Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro category:cs.CV cs.LG cs.NE  published:2015-12-21 summary:Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay. version:2
arxiv-1601-07233 | Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers on Subgraphs of Molecules | http://arxiv.org/abs/1601.07233 | id:1601.07233 author:Andrew Schaumberg, Angela Yu, Tatsuhiro Koshi, Xiaochan Zong, Santoshkalyan Rayadhurgam category:stat.ML cs.LG I.2.1; J.3  published:2016-01-27 summary:In this study, we intend to solve a mutual information problem in interacting molecules of any type, such as proteins, nucleic acids, and small molecules. Using machine learning techniques, we accurately predict pairwise interactions, which can be of medical and biological importance. Graphs are are useful in this problem for their generality to all types of molecules, due to the inherent association of atoms through atomic bonds. Subgraphs can represent different molecular domains. These domains can be biologically significant as most molecules only have portions that are of functional significance and can interact with other domains. Thus, we use subgraphs as features in different machine learning algorithms to predict if two drugs interact and predict potential single molecule effects. version:1
arxiv-1601-07227 | A network that learns Strassen multiplication | http://arxiv.org/abs/1601.07227 | id:1601.07227 author:Veit Elser category:math.NA cs.NE  published:2016-01-26 summary:We study neural networks whose only non-linear components are multipliers, to test a new training rule in a context where the precise representation of data is paramount. These networks are challenged to discover the rules of matrix multiplication, given many examples. By limiting the number of multipliers, the network is forced to discover the Strassen multiplication rules. This is the mathematical equivalent of finding low rank decompositions of the $n\times n$ matrix multiplication tensor, $M_n$. We train these networks with the conservative learning rule, which makes minimal changes to the weights so as to give the correct output for each input at the time the input-output pair is received. Conservative learning needs a few thousand examples to find the rank 7 decomposition of $M_2$, and $10^5$ for the rank 23 decomposition of $M_3$ (the lowest known). High precision is critical, especially for $M_3$, to discriminate between true decompositions and "border approximations". version:1
arxiv-1601-07215 | Recurrent Neural Network Postfilters for Statistical Parametric Speech Synthesis | http://arxiv.org/abs/1601.07215 | id:1601.07215 author:Prasanna Kumar Muthukumar, Alan W Black category:cs.CL  published:2016-01-26 summary:In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently. version:1
arxiv-1506-02059 | Sentence Directed Video Object Codetection | http://arxiv.org/abs/1506.02059 | id:1506.02059 author:Haonan Yu, Jeffrey Mark Siskind category:cs.CV  published:2015-06-05 summary:We tackle the problem of video object codetection by leveraging the weak semantic constraint implied by sentences that describe the video content. Unlike most existing work that focuses on codetecting large objects which are usually salient both in size and appearance, we can codetect objects that are small or medium sized. Our method assumes no human pose or depth information such as is required by the most recent state-of-the-art method. We employ weak semantic constraint on the codetection process by pairing the video with sentences. Although the semantic information is usually simple and weak, it can greatly boost the performance of our codetection framework by reducing the search space of the hypothesized object detections. Our experiment demonstrates an average IoU score of 0.423 on a new challenging dataset which contains 15 object classes and 150 videos with 12,509 frames in total, and an average IoU score of 0.373 on a subset of an existing dataset, originally intended for activity recognition, which contains 5 object classes and 75 videos with 8,854 frames in total. version:2
arxiv-1511-02274 | Stacked Attention Networks for Image Question Answering | http://arxiv.org/abs/1511.02274 | id:1511.02274 author:Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola category:cs.LG cs.CL cs.CV cs.NE  published:2015-11-07 summary:This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer. version:2
arxiv-1501-05499 | Globally Optimal Cell Tracking using Integer Programming | http://arxiv.org/abs/1501.05499 | id:1501.05499 author:Engin Türetken, Xinchao Wang, Carlos Becker, Carsten Haubold, Pascal Fua category:cs.CV  published:2015-01-22 summary:We propose a novel approach to automatically tracking cell populations in time-lapse images. To account for cell occlusions and overlaps, we introduce a robust method that generates an over-complete set of competing detection hypotheses. We then perform detection and tracking simultaneously on these hypotheses by solving to optimality an integer program with only one type of flow variables. This eliminates the need for heuristics to handle missed detections due to occlusions and complex morphology. We demonstrate the effectiveness of our approach on a range of challenging sequences consisting of clumped cells and show that it outperforms state-of-the-art techniques. version:2
arxiv-1601-07140 | COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images | http://arxiv.org/abs/1601.07140 | id:1601.07140 author:Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, Serge Belongie category:cs.CV  published:2016-01-26 summary:This paper describes the COCO-Text dataset. In recent years large-scale datasets like SUN and Imagenet drove the advancement of scene understanding and object recognition. The goal of COCO-Text is to advance state-of-the-art in text detection and recognition in natural images. The dataset is based on the MS COCO dataset, which contains images of complex everyday scenes. The images were not collected with text in mind and thus contain a broad variety of text instances. To reflect the diversity of text in natural scenes, we annotate text with (a) location in terms of a bounding box, (b) fine-grained classification into machine printed text and handwritten text, (c) classification into legible and illegible text, (d) script of the text and (e) transcriptions of legible text. The dataset contains over 173k text annotations in over 63k images. We provide a statistical analysis of the accuracy of our annotations. In addition, we present an analysis of three leading state-of-the-art photo Optical Character Recognition (OCR) approaches on our dataset. While scene text detection and recognition enjoys strong advances in recent years, we identify significant shortcomings motivating future work. version:1
arxiv-1511-09426 | A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks | http://arxiv.org/abs/1511.09426 | id:1511.09426 author:Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE  published:2015-11-30 summary:To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits. version:2
arxiv-1601-07124 | LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization | http://arxiv.org/abs/1601.07124 | id:1601.07124 author:Elvys Linhares Pontes, Juan-Manuel Torres-Moreno, Andréa Carneiro Linhares category:cs.CL cs.IR  published:2016-01-26 summary:This paper aims to introduces a new algorithm for automatic speech-to-text summarization based on statistical divergences of probabilities and graphs. The input is a text from speech conversations with noise, and the output a compact text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus are very encouraging version:1
arxiv-1502-04269 | Supersparse Linear Integer Models for Optimized Medical Scoring Systems | http://arxiv.org/abs/1502.04269 | id:1502.04269 author:Berk Ustun, Cynthia Rudin category:stat.ML cs.DM cs.LG stat.AP stat.ME  published:2015-02-15 summary:Scoring systems are linear classification models that only require users to add, subtract and multiply a few small numbers in order to make a prediction. These models are in widespread use by the medical community, but are difficult to learn from data because they need to be accurate and sparse, have coprime integer coefficients, and satisfy multiple operational constraints. We present a new method for creating data-driven scoring systems called a Supersparse Linear Integer Model (SLIM). SLIM scoring systems are built by solving an integer program that directly encodes measures of accuracy (the 0-1 loss) and sparsity (the $\ell_0$-seminorm) while restricting coefficients to coprime integers. SLIM can seamlessly incorporate a wide range of operational constraints related to accuracy and sparsity, and can produce highly tailored models without parameter tuning. We provide bounds on the testing and training accuracy of SLIM scoring systems, and present a new data reduction technique that can improve scalability by eliminating a portion of the training data beforehand. Our paper includes results from a collaboration with the Massachusetts General Hospital Sleep Laboratory, where SLIM was used to create a highly tailored scoring system for sleep apnea screening version:3
arxiv-1512-01320 | What can we learn about CNNs from a large scale controlled object dataset? | http://arxiv.org/abs/1512.01320 | id:1512.01320 author:Ali Borji, Saeed Izadi, Laurent Itti category:cs.CV  published:2015-12-04 summary:Tolerance to image variations (e.g. translation, scale, pose, illumination) is an important desired property of any object recognition system, be it human or machine. Moving towards increasingly bigger datasets has been trending in computer vision specially with the emergence of highly popular deep learning models. While being very useful for learning invariance to object inter- and intra-class shape variability, these large-scale wild datasets are not very useful for learning invariance to other parameters forcing researchers to resort to other tricks for training a model. In this work, we introduce a large-scale synthetic dataset, which is freely and publicly available, and use it to answer several fundamental questions regarding invariance and selectivity properties of convolutional neural networks. Our dataset contains two parts: a) objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on a semicircular arch, 5 lighting conditions, 3 focus levels, variety of backgrounds (23.4 per instance) generating 1320 images per instance (over 20 million images in total), and b) scenes: in which a robot arm takes pictures of objects on a 1:160 scale scene. We study: 1) invariance and selectivity of different CNN layers, 2) knowledge transfer from one object category to another, 3) systematic or random sampling of images to build a train set, 4) domain adaptation from synthetic to natural scenes, and 5) order of knowledge delivery to CNNs. We also explore how our analyses can lead the field to develop more efficient CNNs. version:2
arxiv-1601-07021 | Polyhedron Volume-Ratio-based Classification for Image Recognition | http://arxiv.org/abs/1601.07021 | id:1601.07021 author:Qingxiang Feng, Jeng-Shyang Pan, Jar-Ferr Yang, Yang-Ting Chou category:cs.CV  published:2016-01-26 summary:In this paper, a novel method, called polyhedron volume ratio classification (PVRC) is proposed for image recognition version:1
arxiv-1509-02223 | Diffusion tensor imaging with deterministic error bounds | http://arxiv.org/abs/1509.02223 | id:1509.02223 author:Artur Gorokh, Yury Korolev, Tuomo Valkonen category:cs.CV cs.NA  published:2015-09-07 summary:Errors in the data and the forward operator of an inverse problem can be handily modelled using partial order in Banach lattices. We present some existing results of the theory of regularisation in this novel framework, where errors are represented as bounds by means of the appropriate partial order. We apply the theory to Diffusion Tensor Imaging, where correct noise modelling is challenging: it involves the Rician distribution and the nonlinear Stejskal-Tanner equation. Linearisation of the latter in the statistical framework would complicate the noise model even further. We avoid this using the error bounds approach, which preserves simple error structure under monotone transformations. version:2
arxiv-1601-06950 | Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction | http://arxiv.org/abs/1601.06950 | id:1601.06950 author:Michael Waechter, Mate Beljan, Simon Fuhrmann, Nils Moehrle, Johannes Kopf, Michael Goesele category:cs.CV cs.GR I.3.7  published:2016-01-26 summary:The ultimate goal of many image-based modeling systems is to render photo-realistic novel views of a scene without visible artifacts. Existing evaluation metrics and benchmarks focus mainly on the geometric accuracy of the reconstructed model, which is, however, a poor predictor of visual accuracy. Furthermore, using only geometric accuracy by itself does not allow evaluating systems that either lack a geometric scene representation or utilize coarse proxy geometry. Examples include light field or image-based rendering systems. We propose a unified evaluation approach based on novel view prediction error that is able to analyze the visual quality of any method that can render novel views from input images. One of the key advantages of this approach is that it does not require ground truth geometry. This dramatically simplifies the creation of test datasets and benchmarks. It also allows us to evaluate the quality of an unknown scene during the acquisition and reconstruction process, which is useful for acquisition planning. We evaluate our approach on a range of methods including standard geometry-plus-texture pipelines as well as image-based rendering techniques, compare it to existing geometry-based benchmarks, and demonstrate its utility for a range of use cases. version:1
arxiv-1511-07131 | DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization | http://arxiv.org/abs/1511.07131 | id:1511.07131 author:Jun Zhu, Xianjie Chen, Alan L. Yuille category:cs.CV  published:2015-11-23 summary:In this paper, we propose a deep part-based model (DeePM) for symbiotic object detection and semantic part localization. For this purpose, we annotate semantic parts for all 20 object categories on the PASCAL VOC 2012 dataset, which provides information on object pose, occlusion, viewpoint and functionality. DeePM is a latent graphical model based on the state-of-the-art R-CNN framework, which learns an explicit representation of the object-part configuration with flexible type sharing (e.g., a sideview horse head can be shared by a fully-visible sideview horse and a highly truncated sideview horse with head and neck only). For comparison, we also present an end-to-end Object-Part (OP) R-CNN which learns an implicit feature representation for jointly mapping an image ROI to the object and part bounding boxes. We evaluate the proposed methods for both the object and part detection performance on PASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in detecting objects and parts. In addition, it obtains superior performance to Fast and Faster R-CNNs in object detection. version:3
arxiv-1601-06933 | A Novel Memetic Feature Selection Algorithm | http://arxiv.org/abs/1601.06933 | id:1601.06933 author:Mohadeseh Montazeri, Hamid Reza Naji, Mitra Montazeri, Ahmad Faraahi category:cs.LG  published:2016-01-26 summary:Feature selection is a problem of finding efficient features among all features in which the final feature set can improve accuracy and reduce complexity. In feature selection algorithms search strategies are key aspects. Since feature selection is an NP-Hard problem; therefore heuristic algorithms have been studied to solve this problem. In this paper, we have proposed a method based on memetic algorithm to find an efficient feature subset for a classification problem. It incorporates a filter method in the genetic algorithm to improve classification performance and accelerates the search in identifying core feature subsets. Particularly, the method adds or deletes a feature from a candidate feature subset based on the multivariate feature information. Empirical study on commonly data sets of the university of California, Irvine shows that the proposed method outperforms existing methods. version:1
arxiv-1601-06931 | Fisher Motion Descriptor for Multiview Gait Recognition | http://arxiv.org/abs/1601.06931 | id:1601.06931 author:F. M. Castro, M. J. Marín-Jiménez, N. Guil, R. Muñoz-Salinas category:cs.CV cs.AI  published:2016-01-26 summary:The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on `CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and the recent `AVA Multiview Gait' dataset. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths. version:1
arxiv-1601-06925 | Classification and Verification of Online Handwritten Signatures with Time Causal Information Theory Quantifiers | http://arxiv.org/abs/1601.06925 | id:1601.06925 author:Osvaldo A. Rosso, Raydonal Ospina, Alejandro C. Frery category:cs.IT cs.CV math.IT  published:2016-01-26 summary:We present a new approach for online handwritten signature classification and verification based on descriptors stemming from Information Theory. The proposal uses the Shannon Entropy, the Statistical Complexity, and the Fisher Information evaluated over the Bandt and Pompe symbolization of the horizontal and vertical coordinates of signatures. These six features are easy and fast to compute, and they are the input to an One-Class Support Vector Machine classifier. The results produced surpass state-of-the-art techniques that employ higher-dimensional feature spaces which often require specialized software and hardware. We assess the consistency of our proposal with respect to the size of the training sample, and we also use it to classify the signatures into meaningful groups. version:1
arxiv-1601-06911 | Functional archetype and archetypoid analysis | http://arxiv.org/abs/1601.06911 | id:1601.06911 author:Irene Epifanio category:stat.ME stat.AP stat.ML  published:2016-01-26 summary:Archetype and archetypoid analysis can be extended to functional data. Each function is represented as a mixture of actual observations (functional archetypoids) or functional archetypes, which are a mixture of observations in the data set. Well-known Canadian temperature data are used to illustrate the analysis developed. Computational methods are proposed for performing these analyses, based on the coefficients of a basis. They are compared with other alternatives in a simulation study using a well-known curve discrimination problem, achieving better or similar performance. Unlike a previous attempt to compute functional archetypes, which was only valid for an orthogonal basis, the proposed methodology can be used for any basis. It is computationally less demanding than the simple approach of discretizing the functions. Multivariate functional archetype and archetypoid analysis are also introduced and applied in an interesting problem about the study of human development around the world over the last 50 years. These tools can contribute to the understanding of a functional data set, as in the multivariate case. version:1
arxiv-1506-07902 | Minimax Structured Normal Means Inference | http://arxiv.org/abs/1506.07902 | id:1506.07902 author:Akshay Krishnamurthy category:stat.ML cs.IT math.IT  published:2015-06-25 summary:We provide a unified treatment of a broad class of noisy structure recovery problems, known as structured normal means problems. In this setting, the goal is to identify, from a finite collection of Gaussian distributions with different means, the distribution that produced some observed data. Recent work has studied several special cases including sparse vectors, biclusters, and graph-based structures. We establish nearly matching upper and lower bounds on the minimax probability of error for any structured normal means problem, and we derive an optimality certificate for the maximum likelihood estimator, which can be applied to many instantiations. We also consider an experimental design setting, where we generalize our minimax bounds and derive an algorithm for computing a design strategy with a certain optimality property. We show that our results give tight minimax bounds for many structure recovery problems and consider some consequences for interactive sampling. version:2
arxiv-1601-06823 | Survey on the attention based RNN model and its applications in computer vision | http://arxiv.org/abs/1601.06823 | id:1601.06823 author:Feng Wang, David M. J. Tax category:cs.CV cs.LG  published:2016-01-25 summary:The recurrent neural networks (RNN) can be used to solve the sequence to sequence problem, where both the input and the output have sequential structures. Usually there are some implicit relations between the structures. However, it is hard for the common RNN model to fully explore the relations between the sequences. In this survey, we introduce some attention based RNN models which can focus on different parts of the input for each output item, in order to explore and take advantage of the implicit relations between the input and the output items. The different attention mechanisms are described in detail. We then introduce some applications in computer vision which apply the attention based RNN models. The superiority of the attention based RNN model is shown by the experimental results. At last some future research directions are given. version:1
arxiv-1601-06815 | Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add | http://arxiv.org/abs/1601.06815 | id:1601.06815 author:Tyler Highlander, Andres Rodriguez category:cs.NE cs.LG  published:2016-01-25 summary:Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to $\mathcal{O}(N^{2}n^{2})$ per kernel per layer where the inputs are $N \times N$ arrays and the kernels are $n \times n$ arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of $\mathcal{O}(N^{2}\log_2 N)$ using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when $N\gg n$ as is the case in CNNs. We mitigate this by using the "overlap-and-add" technique reducing the computational complexity to $\mathcal{O}(N^2\log_2 n)$ per kernel. This method increases the algorithm's efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 $\times$ 8 kernel and a 224 $\times$ 224 image. version:1
arxiv-1601-06763 | Emerging Dimension Weights in a Conceptual Spaces Model of Concept Combination | http://arxiv.org/abs/1601.06763 | id:1601.06763 author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA  published:2016-01-25 summary:We investigate the generation of new concepts from combinations of properties as an artificial language develops. To do so, we have developed a new framework for conjunctive concept combination. This framework gives a semantic grounding to the weighted sum approach to concept combination seen in the literature. We implement the framework in a multi-agent simulation of language evolution and show that shared combination weights emerge. The expected value and the variance of these weights across agents may be predicted from the distribution of elements in the conceptual space, as determined by the underlying environment, together with the rate at which agents adopt others' concepts. When this rate is smaller, the agents are able to converge to weights with lower variance. However, the time taken to converge to a steady state distribution of weights is longer. version:1
arxiv-1507-08173 | Fast Robust PCA on Graphs | http://arxiv.org/abs/1507.08173 | id:1507.08173 author:Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy, Pierre Vandergheynst category:cs.CV  published:2015-07-29 summary:Mining useful clusters from high dimensional data has received significant attention of the computer vision and pattern recognition community in the recent years. Linear and non-linear dimensionality reduction has played an important role to overcome the curse of dimensionality. However, often such methods are accompanied with three different problems: high computational complexity (usually associated with the nuclear norm minimization), non-convexity (for matrix factorization methods) and susceptibility to gross corruptions in the data. In this paper we propose a principal component analysis (PCA) based solution that overcomes these three issues and approximates a low-rank recovery method for high dimensional datasets. We target the low-rank recovery by enforcing two types of graph smoothness assumptions, one on the data samples and the other on the features by designing a convex optimization problem. The resulting algorithm is fast, efficient and scalable for huge datasets with O(nlog(n)) computational complexity in the number of data samples. It is also robust to gross corruptions in the dataset as well as to the model parameters. Clustering experiments on 7 benchmark datasets with different types of corruptions and background separation experiments on 3 video datasets show that our proposed model outperforms 10 state-of-the-art dimensionality reduction models. Our theoretical analysis proves that the proposed model is able to recover approximate low-rank representations with a bounded error for clusterable data. version:2
arxiv-1601-06755 | The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels | http://arxiv.org/abs/1601.06755 | id:1601.06755 author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA  published:2016-01-25 summary:We investigate the emergence of shared concepts in a community of language users using a multi-agent simulation. We extend results showing that negated assertions are of use in developing shared categories, to include assertions modified by linguistic hedges. Results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways. Firstly, using contraction hedges like `very' gives better convergence over time. Secondly, using expansion hedges such as `quite' reduces concept overlap. However, both these improvements come at a cost of slower speed of development. version:1
arxiv-1601-04908 | Graded Entailment for Compositional Distributional Semantics | http://arxiv.org/abs/1601.04908 | id:1601.04908 author:Desislava Bankova, Bob Coecke, Martha Lewis, Daniel Marsden category:cs.CL cs.AI cs.LO math.CT quant-ph  published:2016-01-19 summary:The categorical compositional distributional model of natural language provides a conceptually motivated procedure to compute the meaning of sentences, given grammatical structure and the meanings of its words. This approach has outperformed other models in mainstream empirical language processing tasks. However, until recently it has lacked the crucial feature of lexical entailment -- as do other distributional models of meaning. In this paper we solve the problem of entailment for categorical compositional distributional semantics. Taking advantage of the abstract categorical framework allows us to vary our choice of model. This enables the introduction of a notion of entailment, exploiting ideas from the categorical semantics of partial knowledge in quantum computation. The new model of language uses density matrices, on which we introduce a novel robust graded order capturing the entailment strength between concepts. This graded measure emerges from a general framework for approximate entailment, induced by any commutative monoid. Quantum logic embeds in our graded order. Our main theorem shows that entailment strength lifts compositionally to the sentence level, giving a lower bound on sentence entailment. We describe the essential properties of graded entailment such as continuity, and provide a procedure for calculating entailment strength. version:2
arxiv-1512-07851 | Context-Based Prediction of App Usage | http://arxiv.org/abs/1512.07851 | id:1512.07851 author:Joseph Keshet, Adam Kariv, Arnon Dagan, Dvir Volk, Joey Simhon category:cs.LG  published:2015-12-24 summary:There are around a hundred installed apps on an average smartphone. The high number of apps and the limited number of app icons that can be displayed on the device's screen requires a new paradigm to address their visibility to the user. In this paper we propose a new online algorithm for dynamically predicting a set of apps that the user is likely to use. The algorithm runs on the user's device and constantly learns the user's habits at a given time, location, and device state. It is designed to actively help the user to navigate to the desired app as well as to provide a personalized feeling, and hence is aimed at maximizing the AUC. We show both theoretically and empirically that the algorithm maximizes the AUC, and yields good results on a set of 1,000 devices. version:2
arxiv-1601-06738 | A Label Semantics Approach to Linguistic Hedges | http://arxiv.org/abs/1601.06738 | id:1601.06738 author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL  published:2016-01-25 summary:We introduce a model for the linguistic hedges `very' and `quite' within the label semantics framework, and combined with the prototype and conceptual spaces theories of concepts. The proposed model emerges naturally from the representational framework we use and as such, has a clear semantic grounding. We give generalisations of these hedge models and show that they can be composed with themselves and with other functions, going on to examine their behaviour in the limit of composition. version:1
arxiv-1601-06732 | Concept Generation in Language Evolution | http://arxiv.org/abs/1601.06732 | id:1601.06732 author:Martha Lewis, Jonathan Lawry category:cs.AI cs.CL cs.MA  published:2016-01-25 summary:This thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves. We give a method for combining concepts, and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation. version:1
arxiv-1601-06680 | Conditional distribution variability measures for causality detection | http://arxiv.org/abs/1601.06680 | id:1601.06680 author:José A. R. Fonollosa category:stat.ML cs.LG  published:2016-01-25 summary:In this paper we derive variability measures for the conditional probability distributions of a pair of random variables, and we study its application in the inference of causal-effect relationships. We also study the combination of the proposed measures with standard statistical measures in the the framework of the ChaLearn cause-effect pair challenge. The developed model obtains an AUC score of 0.82 on the final test database and ranked second in the challenge. version:1
arxiv-1601-06651 | Testing for Causality in Continuous Time Bayesian Network Models of High-Frequency Data | http://arxiv.org/abs/1601.06651 | id:1601.06651 author:Jonas Hallgren, Timo Koski category:stat.ML q-fin.TR  published:2016-01-25 summary:Continuous time Bayesian networks are investigated with a special focus on their ability to express causality. A framework is presented for doing inference in these networks. The central contributions are a representation of the intensity matrices for the networks and the introduction of a causality measure. A new model for high-frequency financial data is presented. It is calibrated to market data and by the new causality measure it performs better than older models. version:1
arxiv-1601-06650 | Time-Varying Gaussian Process Bandit Optimization | http://arxiv.org/abs/1601.06650 | id:1601.06650 author:Ilija Bogunovic, Jonathan Scarlett, Volkan Cevher category:stat.ML cs.LG  published:2016-01-25 summary:We consider the sequential Bayesian optimization problem with bandit feedback, adopting a formulation that allows for the reward function to vary with time. We model the reward function using a Gaussian process whose evolution obeys a simple Markov model. We introduce two natural extensions of the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB, instead forgets about old data in a smooth fashion. Our main contribution comprises of novel regret bounds for these algorithms, providing an explicit characterization of the trade-off between the time horizon and the rate at which the function varies. We illustrate the performance of the algorithms on both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB to perform favorably compared to the sharp resetting of R-GP-UCB. Moreover, both algorithms significantly outperform classical GP-UCB, since it treats stale and fresh data equally. version:1
