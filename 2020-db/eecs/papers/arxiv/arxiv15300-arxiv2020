arxiv-1603-00944 | PCANet: An energy perspective | http://arxiv.org/abs/1603.00944 | id:1603.00944 author:Jiasong Wu, Shijie Qiu, Youyong Kong, Longyu Jiang, Lotfi Senhadji, Huazhong Shu category:cs.CV  published:2016-03-03 summary:The principal component analysis network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various databases. However, the explanation of the PCANet is lacked. In this paper, we try to explain why PCANet works well from energy perspective point of view based on a set of experiments. The impact of various parameters on the error rate of PCANet is analyzed in depth. It was found that this error rate is correlated with the logarithm of energy of image. The proposed energy explanation approach can be used as a testing method for checking if every step of the constructed networks is necessary. version:1
arxiv-1603-00912 | LiDAR Ground Filtering Algorithm for Urban Areas Using Scan Line Based Segmentation | http://arxiv.org/abs/1603.00912 | id:1603.00912 author:Lei Wang, Yongun Zhang category:cs.CV  published:2016-03-02 summary:This paper addresses the task of separating ground points from airborne LiDAR point cloud data in urban areas. A novel ground filtering method using scan line segmentation is proposed here, which we call SLSGF. It utilizes the scan line information in LiDAR data to segment the LiDAR data. The similarity measurements are designed to make it possible to segment complex roof structures into a single segment as much as possible so the topological relationships between the roof and the ground are simpler, which will benefit the labeling process. In the labeling process, the initial ground segments are detected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011 are used to test the accuracy of SLSGF; and our analytical and experimental results show that this method is computationally-efficient and noise-insensitive, thereby making a denoising process unnecessary before filtering. version:1
arxiv-1603-00892 | Counter-fitting Word Vectors to Linguistic Constraints | http://arxiv.org/abs/1603.00892 | id:1603.00892 author:Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Lina Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG  published:2016-03-02 summary:In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains. version:1
arxiv-1603-00856 | Molecular Graph Convolutions: Moving Beyond Fingerprints | http://arxiv.org/abs/1603.00856 | id:1603.00856 author:Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick Riley category:stat.ML cs.LG  published:2016-03-02 summary:Molecular "fingerprints" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a novel machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph (atoms, bonds, distances, etc.), allowing the model to take greater advantage of information in the graph structure. version:1
arxiv-1603-00845 | Shallow and Deep Convolutional Networks for Saliency Prediction | http://arxiv.org/abs/1603.00845 | id:1603.00845 author:Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor, Xavier Giro-i-Nieto category:cs.CV cs.LG  published:2016-03-02 summary:The prediction of salient areas in images has been traditionally addressed with hand-crafted features based on neuroscience principles. This paper, however, addresses the problem with a completely data-driven approach by training a convolutional neural network (convnet). The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train end-to-end architectures that are both fast and accurate. Two designs are proposed: a shallow convnet trained from scratch, and a another deeper solution whose first three layers are adapted from another network trained for classification. To the authors knowledge, these are the first end-to-end CNNs trained and tested for the purpose of saliency prediction. version:1
arxiv-1603-00841 | Automatic segmentation of lizard spots using an active contour model | http://arxiv.org/abs/1603.00841 | id:1603.00841 author:Jhony Giraldo, Augusto Salazar category:cs.CV  published:2016-03-02 summary:Animal biometrics is a challenging task. In the literature, many algorithms have been used, e.g. penguin chest recognition, elephant ears recognition and leopard stripes pattern recognition, but to use technology to a large extent in this area of research, still a lot of work has to be done. One important target in animal biometrics is to automate the segmentation process, so in this paper we propose a segmentation algorithm for extracting the spots of Diploglossus millepunctatus, an endangered lizard species. The automatic segmentation is achieved with a combination of preprocessing, active contours and morphology. The parameters of each stage of the segmentation algorithm are found using an optimization procedure, which is guided by the ground truth. The results show that automatic segmentation of spots is possible. A 78.37 % of correct segmentation in average is reached. version:1
arxiv-1508-01722 | Unconstrained Face Verification using Deep CNN Features | http://arxiv.org/abs/1508.01722 | id:1508.01722 author:Jun-Cheng Chen, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2015-08-07 summary:In this paper, we present an algorithm for unconstrained face verification based on deep convolutional features and evaluate it on the newly released IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the traditional Labeled Face in the Wild (LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network (DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the IJB-A dataset are provided. version:2
arxiv-1511-06238 | Multimodal sparse representation learning and applications | http://arxiv.org/abs/1511.06238 | id:1511.06238 author:Miriam Cha, Youngjune Gwon, H. T. Kung category:cs.LG cs.CV stat.ML  published:2015-11-19 summary:Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text). version:3
arxiv-1603-00816 | A Nonlinear Weighted Total Variation Image Reconstruction Algorithm for Electrical Capacitance Tomography | http://arxiv.org/abs/1603.00816 | id:1603.00816 author:Kezhi Li, Daniel Holland category:cs.CV  published:2016-03-02 summary:Based on the techniques of iterative soft thresholding on total variation penalty and adaptive reweighted compressive sensing, a new iterative reconstruction algorithm for electrical capacitance tomography (ECT) is proposed. This algorithm encourages sharp changes in the ECT image and overcomes the disadvantage of the $l_1$ minimization by equipping the total variation an adaptive weighted depending on the reconstructed image. Moreover, the nonlinear effect is also partially reduced due to the adoption of the updated accurate sensitivity matrix. Simulation results show that it recovers ECT images more precisely and therefore suitable for the imaging of multiphase systems in industrial or medical applications. version:1
arxiv-1508-07091 | Multi-armed Bandit Problem with Known Trend | http://arxiv.org/abs/1508.07091 | id:1508.07091 author:Djallel Bouneffouf, Raphaël Feraud category:cs.LG I.2  published:2015-08-28 summary:We consider a variant of the multi-armed bandit model, which we call multi-armed bandit problem with known trend, where the gambler knows the shape of the reward function of each arm but not its distribution. This new problem is motivated by different online problems like active learning, music and interface recommendation applications, where when an arm is sampled by the model the received reward change according to a known trend. By adapting the standard multi-armed bandit algorithm UCB1 to take advantage of this setting, we propose the new algorithm named A-UCB that assumes a stochastic model. We provide upper bounds of the regret which compare favourably with the ones of UCB1. We also confirm that experimentally with different simulations version:3
arxiv-1603-00802 | Flies as Ship Captains? Digital Evolution Unravels Selective Pressures to Avoid Collision in Drosophila | http://arxiv.org/abs/1603.00802 | id:1603.00802 author:Ali Tehrani-Saleh, Christoph Adami category:q-bio.PE cs.CV nlin.AO q-bio.NC  published:2016-03-02 summary:Flies that walk in a covered planar arena on straight paths avoid colliding with each other, but which of the two flies stops is not random. High-throughput video observations, coupled with dedicated experiments with controlled robot flies have revealed that flies utilize the type of optic flow on their retina as a determinant of who should stop, a strategy also used by ship captains to determine which of two ships on a collision course should throw engines in reverse. We use digital evolution to test whether this strategy evolves when collision avoidance is the sole penalty. We find that the strategy does indeed evolve in a narrow range of cost/benefit ratios, for experiments in which the "regressive motion" cue is error free. We speculate that these stringent conditions may not be sufficient to evolve the strategy in real flies, pointing perhaps to auxiliary costs and benefits not modeled in our study version:1
arxiv-1603-00788 | Automatic Differentiation Variational Inference | http://arxiv.org/abs/1603.00788 | id:1603.00788 author:Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei category:stat.ML cs.AI cs.LG stat.CO  published:2016-03-02 summary:Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use. version:1
arxiv-1603-00786 | Learning Word Segmentation Representations to Improve Named Entity Recognition for Chinese Social Media | http://arxiv.org/abs/1603.00786 | id:1603.00786 author:Nanyun Peng, Mark Dredze category:cs.CL  published:2016-03-02 summary:Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results. version:1
arxiv-1603-00751 | Equity forecast: Predicting long term stock price movement using machine learning | http://arxiv.org/abs/1603.00751 | id:1603.00751 author:Nikola Milosevic category:cs.LG q-fin.GN  published:2016-03-02 summary:Long term investment is one of the major investment strategies. However, calculating intrinsic value of some company and evaluating shares for long term investment is not easy, since analyst have to care about a large number of financial indicators and evaluate them in a right manner. So far, little help in predicting the direction of the company value over the longer period of time has been provided from the machines. In this paper we present a machine learning aided approach to evaluate the equity's future price over the long time. Our method is able to correctly predict whether some company's value will be 10% higher or not over the period of one year in 76.5% of cases. version:1
arxiv-1603-00748 | Continuous Deep Q-Learning with Model-based Acceleration | http://arxiv.org/abs/1603.00748 | id:1603.00748 author:Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, Sergey Levine category:cs.LG cs.AI cs.RO cs.SY  published:2016-03-02 summary:Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable. version:1
arxiv-1603-00709 | Probabilistic Relational Model Benchmark Generation | http://arxiv.org/abs/1603.00709 | id:1603.00709 author:Mouna Ben Ishak, Rajani Chulyadyo, Philippe Leray category:cs.LG cs.AI  published:2016-03-02 summary:The validation of any database mining methodology goes through an evaluation process where benchmarks availability is essential. In this paper, we aim to randomly generate relational database benchmarks that allow to check probabilistic dependencies among the attributes. We are particularly interested in Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs) to a relational data mining context and enable effective and robust reasoning over relational data. Even though a panoply of works have focused, separately , on the generation of random Bayesian networks and relational databases, no work has been identified for PRMs on that track. This paper provides an algorithmic approach for generating random PRMs from scratch to fill this gap. The proposed method allows to generate PRMs as well as synthetic relational data from a randomly generated relational schema and a random set of probabilistic dependencies. This can be of interest not only for machine learning researchers to evaluate their proposals in a common framework, but also for databases designers to evaluate the effectiveness of the components of a database management system. version:1
arxiv-1507-02268 | Optimal approximate matrix product in terms of stable rank | http://arxiv.org/abs/1507.02268 | id:1507.02268 author:Michael B. Cohen, Jelani Nelson, David P. Woodruff category:cs.DS cs.LG stat.ML  published:2015-07-08 summary:We prove, using the subspace embedding guarantee in a black box way, that one can achieve the spectral norm guarantee for approximate matrix multiplication with a dimensionality-reducing map having $m = O(\tilde{r}/\varepsilon^2)$ rows. Here $\tilde{r}$ is the maximum stable rank, i.e. squared ratio of Frobenius and operator norms, of the two matrices being multiplied. This is a quantitative improvement over previous work of [MZ11, KVZ14], and is also optimal for any oblivious dimensionality-reducing map. Furthermore, due to the black box reliance on the subspace embedding property in our proofs, our theorem can be applied to a much more general class of sketching matrices than what was known before, in addition to achieving better bounds. For example, one can apply our theorem to efficient subspace embeddings such as the Subsampled Randomized Hadamard Transform or sparse subspace embeddings, or even with subspace embedding constructions that may be developed in the future. Our main theorem, via connections with spectral error matrix multiplication shown in prior work, implies quantitative improvements for approximate least squares regression and low rank approximation. Our main result has also already been applied to improve dimensionality reduction guarantees for $k$-means clustering [CEMMP14], and implies new results for nonparametric regression [YPW15]. We also separately point out that the proof of the "BSS" deterministic row-sampling result of [BSS12] can be modified to show that for any matrices $A, B$ of stable rank at most $\tilde{r}$, one can achieve the spectral norm guarantee for approximate matrix multiplication of $A^T B$ by deterministically sampling $O(\tilde{r}/\varepsilon^2)$ rows that can be found in polynomial time. The original result of [BSS12] was for rank instead of stable rank. Our observation leads to a stronger version of a main theorem of [KMST10]. version:3
arxiv-1511-02187 | Streaming regularization parameter selection via stochastic gradient descent | http://arxiv.org/abs/1511.02187 | id:1511.02187 author:Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML  published:2015-11-06 summary:We propose a framework to perform streaming covariance selection. Our approach employs regularization constraints where a time-varying sparsity parameter is iteratively estimated via stochastic gradient descent. This allows for the regularization parameter to be efficiently learnt in an online manner. The proposed framework is developed for linear regression models and extended to graphical models via neighbourhood selection. Under mild assumptions, we are able to obtain convergence results in a non-stochastic setting. The capabilities of such an approach are demonstrated using both synthetic data as well as neuroimaging data. version:2
arxiv-1511-07069 | Auxiliary Image Regularization for Deep CNNs with Noisy Labels | http://arxiv.org/abs/1511.07069 | id:1511.07069 author:Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell category:cs.CV  published:2015-11-22 summary:Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data. version:2
arxiv-1411-2664 | Preserving Statistical Validity in Adaptive Data Analysis | http://arxiv.org/abs/1411.2664 | id:1411.2664 author:Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth category:cs.LG cs.DS  published:2014-11-10 summary:A great deal of effort has been devoted to reducing the risk of spurious scientific discoveries, from the use of sophisticated validation techniques, to deep statistical methods for controlling the false discovery rate in multiple hypothesis testing. However, there is a fundamental disconnect between the theoretical results and the practice of data analysis: the theory of statistical inference assumes a fixed collection of hypotheses to be tested, or learning algorithms to be applied, selected non-adaptively before the data are gathered, whereas in practice data is shared and reused with hypotheses and new analyses being generated on the basis of data exploration and the outcomes of previous analyses. In this work we initiate a principled study of how to guarantee the validity of statistical inference in adaptive data analysis. As an instance of this problem, we propose and investigate the question of estimating the expectations of $m$ adaptively chosen functions on an unknown distribution given $n$ random samples. We show that, surprisingly, there is a way to estimate an exponential in $n$ number of expectations accurately even if the functions are chosen adaptively. This gives an exponential improvement over standard empirical estimators that are limited to a linear number of estimates. Our result follows from a general technique that counter-intuitively involves actively perturbing and coordinating the estimates, using techniques developed for privacy preservation. We give additional applications of this technique to our question. version:3
arxiv-1510-08628 | WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation | http://arxiv.org/abs/1510.08628 | id:1510.08628 author:Jianfei Chen, Kaiwei Li, Jun Zhu, Wenguang Chen category:stat.ML cs.DC cs.IR cs.LG H.3.4; G.3; G.4  published:2015-10-29 summary:Developing efficient and scalable algorithms for Latent Dirichlet Allocation (LDA) is of wide interest for many applications. Previous work has developed an O(1) Metropolis-Hastings sampling method for each token. However, the performance is far from being optimal due to random accesses to the parameter matrices and frequent cache misses. In this paper, we first carefully analyze the memory access efficiency of existing algorithms for LDA by the scope of random access, which is the size of the memory region in which random accesses fall, within a short period of time. We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time complexity per token and the best O(K) scope of random access. Our empirical results in a wide range of testing conditions demonstrate that WarpLDA is consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based LightLDA, and is comparable or faster than the sparsity aware F+LDA. With WarpLDA, users can learn up to one million topics from hundreds of millions of documents in a few hours, at an unprecedentedly throughput of 11G tokens per second. version:2
arxiv-1511-05939 | Metric Learning with Adaptive Density Discrimination | http://arxiv.org/abs/1511.05939 | id:1511.05939 author:Oren Rippel, Manohar Paluri, Piotr Dollar, Lubomir Bourdev category:stat.ML cs.LG  published:2015-11-18 summary:Distance metric learning (DML) approaches learn a transformation to a representation space where distance is in correspondence with a predefined notion of similarity. While such models offer a number of compelling benefits, it has been difficult for these to compete with modern classification algorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address a number of subtle yet important issues which have stymied earlier DML algorithms. It maintains an explicit model of the distributions of the different classes in representation space. It then employs this knowledge to adaptively assess similarity, and achieve local discrimination by penalizing class distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approach achieves state-of-the-art classification results on a number of fine-grained visual recognition datasets, surpassing the standard softmax classifier and outperforming triplet loss by a relative margin of 30-40%. In terms of computational performance, it alleviates training inefficiencies in the traditional triplet loss, reaching the same error in 5-30 times fewer iterations. Beyond classification, we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties, achieving 10-25% relative gains on the softmax classifier and 25-50% on triplet loss in these tasks. version:2
arxiv-1603-00576 | Distributed Estimation of Dynamic Parameters : Regret Analysis | http://arxiv.org/abs/1603.00576 | id:1603.00576 author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI  published:2016-03-02 summary:This paper addresses the estimation of a time- varying parameter in a network. A group of agents sequentially receive noisy signals about the parameter (or moving target), which does not follow any particular dynamics. The parameter is not observable to an individual agent, but it is globally identifiable for the whole network. Viewing the problem with an online optimization lens, we aim to provide the finite-time or non-asymptotic analysis of the problem. To this end, we use a notion of dynamic regret which suits the online, non-stationary nature of the problem. In our setting, dynamic regret can be recognized as a finite-time counterpart of stability in the mean- square sense. We develop a distributed, online algorithm for tracking the moving target. Defining the path-length as the consecutive differences between target locations, we express an upper bound on regret in terms of the path-length of the target and network errors. We further show the consistency of the result with static setting and noiseless observations. version:1
arxiv-1603-00564 | Asymptotic behavior of $\ell_p$-based Laplacian regularization in semi-supervised learning | http://arxiv.org/abs/1603.00564 | id:1603.00564 author:Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan category:cs.LG stat.ML  published:2016-03-02 summary:Given a weighted graph with $N$ vertices, consider a real-valued regression problem in a semi-supervised setting, where one observes $n$ labeled vertices, and the task is to label the remaining ones. We present a theoretical study of $\ell_p$-based Laplacian regularization under a $d$-dimensional geometric random graph model. We provide a variational characterization of the performance of this regularized learner as $N$ grows to infinity while $n$ stays constant, the associated optimality conditions lead to a partial differential equation that must be satisfied by the associated function estimate $\hat{f}$. From this formulation we derive several predictions on the limiting behavior the $d$-dimensional function $\hat{f}$, including (a) a phase transition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoff between smoothness and sensitivity to the underlying unlabeled data distribution $P$. Thus, over the range $p \leq d$, the function estimate $\hat{f}$ is degenerate and "spiky," whereas for $p\geq d+1$, the function estimate $\hat{f}$ is smooth. We show that the effect of the underlying density vanishes monotonically with $p$, such that in the limit $p = \infty$, corresponding to the so-called Absolutely Minimal Lipschitz Extension, the estimate $\hat{f}$ is independent of the distribution $P$. Under the assumption of semi-supervised smoothness, ignoring $P$ can lead to poor statistical performance, in particular, we construct a specific example for $d=1$ to demonstrate that $p=2$ has lower risk than $p=\infty$ due to the former penalty adapting to $P$ and the latter ignoring it. We also provide simulations that verify the accuracy of our predictions for finite sample sizes. Together, these properties show that $p = d+1$ is an optimal choice, yielding a function estimate $\hat{f}$ that is both smooth and non-degenerate, while remaining maximally sensitive to $P$. version:1
arxiv-1506-03486 | Sequential Nonparametric Testing with the Law of the Iterated Logarithm | http://arxiv.org/abs/1506.03486 | id:1506.03486 author:Akshay Balsubramani, Aaditya Ramdas category:stat.ML cs.LG math.ST stat.ME stat.TH  published:2015-06-10 summary:We propose a new algorithmic framework for sequential hypothesis testing with i.i.d. data, which includes A/B testing, nonparametric two-sample testing, and independence testing as special cases. It is novel in several ways: (a) it takes linear time and constant space to compute on the fly, (b) it has the same power guarantee as a non-sequential version of the test with the same computational constraints up to a small factor, and (c) it accesses only as many samples as are required - its stopping time adapts to the unknown difficulty of the problem. All our test statistics are constructed to be zero-mean martingales under the null hypothesis, and the rejection threshold is governed by a uniform non-asymptotic law of the iterated logarithm (LIL). For the case of nonparametric two-sample mean testing, we also provide a finite sample power analysis, and the first non-asymptotic stopping time calculations for this class of problems. We verify our predictions for type I and II errors and stopping times using simulations. version:2
arxiv-1603-00546 | US-Cut: Interactive Algorithm for rapid Detection and Segmentation of Liver Tumors in Ultrasound Acquisitions | http://arxiv.org/abs/1603.00546 | id:1603.00546 author:Jan Egger, Philip Voglreiter, Mark Dokter, Michael Hofmann, Xiaojun Chen, Wolfram G. Zoller, Dieter Schmalstieg, Alexander Hann category:cs.CV cs.CE cs.CG cs.GR  published:2016-03-02 summary:Ultrasound (US) is the most commonly used liver imaging modality worldwide. It plays an important role in follow-up of cancer patients with liver metastases. We present an interactive segmentation approach for liver tumors in US acquisitions. Due to the low image quality and the low contrast between the tumors and the surrounding tissue in US images, the segmentation is very challenging. Thus, the clinical practice still relies on manual measurement and outlining of the tumors in the US images. We target this problem by applying an interactive segmentation algorithm to the US data, allowing the user to get real-time feedback of the segmentation results. The algorithm has been developed and tested hand-in-hand by physicians and computer scientists to make sure a future practical usage in a clinical setting is feasible. To cover typical acquisitions from the clinical routine, the approach has been evaluated with dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic (darker) or isoechoic (similar) in comparison to the surrounding liver tissue. Due to the interactive real-time behavior of the approach, it was possible even in difficult cases to find satisfying segmentations of the tumors within seconds and without parameter settings, and the average tumor deviation was only 1.4mm compared with manual measurements. However, the long term goal is to ease the volumetric acquisition of liver tumors in order to evaluate for treatment response. Additional aim is the registration of intraoperative US images via the interactive segmentations to the patient's pre-interventional CT acquisitions. version:1
arxiv-1602-05149 | Parallel Bayesian Global Optimization of Expensive Functions | http://arxiv.org/abs/1602.05149 | id:1602.05149 author:Jialei Wang, Scott C. Clark, Eric Liu, Peter I. Frazier category:stat.ML math.OC  published:2016-02-16 summary:We consider parallel global optimization of derivative-free expensive-to-evaluate functions, and propose an efficient method based on stochastic approximation for implementing a conceptual Bayesian optimization algorithm proposed by Ginsbourger et al. (2007). To accomplish this, we use infinitessimal perturbation analysis (IPA) to construct a stochastic gradient estimator and show that this estimator is unbiased. We also show that the stochastic gradient ascent algorithm using the constructed gradient estimator converges to a stationary point of the q-EI surface, and therefore, as the number of multiple starts of the gradient ascent algorithm and the number of steps for each start grow large, the one-step Bayes optimal set of points is recovered. We show in numerical experiments that our method for maximizing the q-EI is faster than methods based on closed-form evaluation using high-dimensional integration, when considering many parallel function evaluations, and is comparable in speed when considering few. We also show that the resulting one-step Bayes optimal algorithm for parallel global optimization finds high quality solutions with fewer evaluations that a heuristic based on approximately maximizing the q-EI. A high quality open source implementation of this algorithm is available in the open source Metrics Optimization Engine (MOE). version:2
arxiv-1603-00531 | LOFS: Library of Online Streaming Feature Selection | http://arxiv.org/abs/1603.00531 | id:1603.00531 author:Kui Yu, Wei Ding, Xindong Wu category:cs.LG stat.ML  published:2016-03-02 summary:As an emerging research direction, online streaming feature selection deals with sequentially added dimensions in a feature space while the number of data instances is fixed. Online streaming feature selection provides a new, complementary algorithmic methodology to enrich online feature selection, especially targets to high dimensionality in big data analytics. This paper introduces the first comprehensive open-source library for use in MATLAB that implements the state-of-the-art algorithms of online streaming feature selection. The library is designed to facilitate the development of new algorithms in this exciting research direction and make comparisons between the new methods and existing ones available. version:1
arxiv-1506-05011 | Bayesian representation learning with oracle constraints | http://arxiv.org/abs/1506.05011 | id:1506.05011 author:Theofanis Karaletsos, Serge Belongie, Gunnar Rätsch category:stat.ML cs.CV cs.LG  published:2015-06-16 summary:Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \emph{generative unsupervised feature learning} with a \emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables. version:4
arxiv-1603-00522 | Solving Combinatorial Games using Products, Projections and Lexicographically Optimal Bases | http://arxiv.org/abs/1603.00522 | id:1603.00522 author:Swati Gupta, Michel Goemans, Patrick Jaillet category:cs.LG  published:2016-03-01 summary:In order to find Nash-equilibria for two-player zero-sum games where each player plays combinatorial objects like spanning trees, matchings etc, we consider two online learning algorithms: the online mirror descent (OMD) algorithm and the multiplicative weights update (MWU) algorithm. The OMD algorithm requires the computation of a certain Bregman projection, that has closed form solutions for simple convex sets like the Euclidean ball or the simplex. However, for general polyhedra one often needs to exploit the general machinery of convex optimization. We give a novel primal-style algorithm for computing Bregman projections on the base polytopes of polymatroids. Next, in the case of the MWU algorithm, although it scales logarithmically in the number of pure strategies or experts $N$ in terms of regret, the algorithm takes time polynomial in $N$; this especially becomes a problem when learning combinatorial objects. We give a general recipe to simulate the multiplicative weights update algorithm in time polynomial in their natural dimension. This is useful whenever there exists a polynomial time generalized counting oracle (even if approximate) over these objects. Finally, using the combinatorial structure of symmetric Nash-equilibria (SNE) when both players play bases of matroids, we show that these can be found with a single projection or convex minimization (without using online learning). version:1
arxiv-1511-06018 | Segmental Recurrent Neural Networks | http://arxiv.org/abs/1511.06018 | id:1511.06018 author:Lingpeng Kong, Chris Dyer, Noah A. Smith category:cs.CL cs.LG  published:2015-11-18 summary:We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these "segment embeddings" are used to define compatibility scores with output labels. These local compatibility scores are integrated using a global semi-Markov conditional random field. Both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward. Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies. version:2
arxiv-1511-06085 | Variable Rate Image Compression with Recurrent Neural Networks | http://arxiv.org/abs/1511.06085 | id:1511.06085 author:George Toderici, Sean M. O'Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja, Michele Covell, Rahul Sukthankar category:cs.CV cs.LG cs.NE  published:2015-11-19 summary:A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32$\times$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more. version:5
arxiv-1603-00502 | Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features | http://arxiv.org/abs/1603.00502 | id:1603.00502 author:JT Turner, Kalyan Gupta, Brendan Morris, David W. Aha category:cs.CV  published:2016-03-01 summary:Although recent advances in regional Convolutional Neural Networks (CNNs) enable them to outperform conventional techniques on standard object detection and classification tasks, their response time is still slow for real-time performance. To address this issue, we propose a method for region proposal as an alternative to selective search, which is used in current state-of-the art object detection algorithms. We evaluate our Keypoint Density-based Region Proposal (KDRP) approach and show that it speeds up detection and classification on fine-grained tasks by 100% versus the existing selective search region proposal technique without compromising classification accuracy. KDRP makes the application of CNNs to real-time detection and classification feasible. version:1
arxiv-1511-05614 | Model-based Dashboards for Customer Analytics | http://arxiv.org/abs/1511.05614 | id:1511.05614 author:Ryan Dew, Asim Ansari category:stat.AP stat.ML  published:2015-11-17 summary:Automating the customer analytics process is crucial for companies that manage distinct customer bases. In such data-rich and dynamic environments, visualization plays a key role in understanding events of interest. These ideas have led to the popularity of analytics dashboards, yet academic research has paid scant attention to these managerial needs. We develop a probabilistic, nonparametric framework for understanding and predicting individual-level spending using Gaussian process priors over latent functions that describe customer spending along calendar time, interpurchase time, and customer lifetime dimensions. These curves form a dashboard that provides a visual model-based representation of purchasing dynamics that is easily comprehensible. The model flexibly and automatically captures the form and duration of the impact of events that influence spend propensity, even when such events are unknown a-priori. We illustrate the use of our Gaussian Process Propensity Model (GPPM) on data from two popular mobile games. We show that the GPPM generalizes hazard and buy-till-you-die models by incorporating calendar time dynamics while simultaneously accounting for recency and lifetime effects. It therefore provides insights about spending propensity beyond those available from these models. Finally, we show that the GPPM outperforms these benchmarks both in fitting and forecasting real and simulated spend data. version:3
arxiv-1603-04746 | Fourier ptychographic reconstruction using Poisson maximum likelihood and truncated Wirtinger gradient | http://arxiv.org/abs/1603.04746 | id:1603.04746 author:Liheng Bian, Jinli Suo, Jaebum Chung, Xiaoze Ou, Changhuei Yang, Feng Chen, Qionghai Dai category:cs.CV physics.optics  published:2016-03-01 summary:Fourier ptychographic microscopy (FPM) is a novel computational coherent imaging technique for high space-bandwidth product imaging. Mathematically, Fourier ptychographic (FP) reconstruction can be implemented as a phase retrieval optimization process, in which we only obtain low resolution intensity images corresponding to the sub-bands of the sample's high resolution (HR) spatial spectrum, and aim to retrieve the complex HR spectrum. In real setups, the measurements always suffer from various degenerations such as Gaussian noise, Poisson noise, speckle noise and pupil location error, which would largely degrade the reconstruction. To efficiently address these degenerations, we propose a novel FP reconstruction method under a gradient descent optimization framework in this paper. The technique utilizes Poisson maximum likelihood for better signal modeling, and truncated Wirtinger gradient for error removal. Results on both simulated data and real data captured using our laser FPM setup show that the proposed method outperforms other state-of-the-art algorithms. Also, we have released our source code for non-commercial use. version:1
arxiv-1603-00441 | Crowdsourcing On-street Parking Space Detection | http://arxiv.org/abs/1603.00441 | id:1603.00441 author:Ruizhi Liao, Cristian Roman, Peter Ball, Shumao Ou, Liping Chen category:cs.HC cs.LG  published:2016-03-01 summary:As the number of vehicles continues to grow, parking spaces are at a premium in city streets. Additionally, due to the lack of knowledge about street parking spaces, heuristic circling the blocks not only costs drivers' time and fuel, but also increases city congestion. In the wake of recent trend to build convenient, green and energy-efficient smart cities, we rethink common techniques adopted by high-profile smart parking systems, and present a user-engaged (crowdsourcing) and sonar-based prototype to identify urban on-street parking spaces. The prototype includes an ultrasonic sensor, a GPS receiver and associated Arduino micro-controllers. It is mounted on the passenger side of a car to measure the distance from the vehicle to the nearest roadside obstacle. Multiple road tests are conducted around Wheatley, Oxford to gather results and emulate the crowdsourcing approach. By extracting parked vehicles' features from the collected trace, a supervised learning algorithm is developed to estimate roadside parking occupancy and spot illegal parking vehicles. A quantity estimation model is derived to calculate the required number of sensing units to cover urban streets. The estimation is quantitatively compared to a fixed sensing solution. The results show that the crowdsourcing way would need substantially fewer sensors compared to the fixed sensing system. version:1
arxiv-1603-00438 | Convolutional Patch Representations for Image Retrieval: an Unsupervised Approach | http://arxiv.org/abs/1603.00438 | id:1603.00438 author:Mattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Perronnin, Cordelia Schmid category:cs.CV  published:2016-03-01 summary:Convolutional neural networks (CNNs) have recently received a lot of attention due to their ability to model local stationary structures in natural images in a multi-scale fashion, when learning all model parameters with supervision. While excellent performance was achieved for image classification when large amounts of labeled visual data are available, their success for un-supervised tasks such as image retrieval has been moderate so far. Our paper focuses on this latter setting and explores several methods for learning patch descriptors without supervision with application to matching and instance-level retrieval. To that effect, we propose a new family of convolutional descriptors for patch representation , based on the recently introduced convolutional kernel networks. We show that our descriptor, named Patch-CKN, performs better than SIFT as well as other convolutional networks learned by artificially introducing supervision and is significantly faster to train. To demonstrate its effectiveness, we perform an extensive evaluation on standard benchmarks for patch and image retrieval where we obtain state-of-the-art results. We also introduce a new dataset called RomePatches, which allows to simultaneously study descriptor performance for patch and image retrieval. version:1
arxiv-1603-00437 | Technical Report: Band selection for nonlinear unmixing of hyperspectral images as a maximal click problem | http://arxiv.org/abs/1603.00437 | id:1603.00437 author:Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard category:cs.CV  published:2016-03-01 summary:Kernel-based nonlinear mixing models have been applied to unmix spectral information of hyperspectral images when the type of mixing occurring in the scene is too complex or unknown. Such methods, however, usually require the inversion of matrices of sizes equal to the number of spectral bands. Reducing the computational load of these methods remains a challenge in large scale applications. This paper proposes a centralized method for band selection (BS) in the reproducing kernel Hilbert space (RKHS). It is based upon the coherence criterion, which sets the largest value allowed for correlations between the basis kernel functions characterizing the unmixing model. We show that the proposed BS approach is equivalent to solving a maximum clique problem (MCP), that is, searching for the biggest complete subgraph in a graph. Furthermore, we devise a strategy for selecting the coherence threshold and the Gaussian kernel bandwidth using coherence bounds for linearly independent bases. Simulation results illustrate the efficiency of the proposed method. version:1
arxiv-1603-00427 | A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear Functionals | http://arxiv.org/abs/1603.00427 | id:1603.00427 author:Felipe C. Pinheiro, Cássio G. Lopes category:cs.SY cs.LG  published:2016-03-01 summary:Nonlinear adaptive filtering allows for modeling of some additional aspects of a general system and usually relies on highly complex algorithms, such as those based on the Volterra series. Through the use of the Kronecker product and some basic facts of tensor algebra, we propose a simple model of nonlinearity, one that can be interpreted as a product of the outputs of K FIR linear filters, and compute its cost function together with its gradient, which allows for some analysis of the optimization problem. We use these results it in a stochastic gradient framework, from which we derive an LMS-like algorithm and investigate the problems of multi-modality in the mean-square error surface and the choice of adequate initial conditions. Its computational complexity is calculated. The new algorithm is tested in a system identification setup and is compared with other polynomial algorithms from the literature, presenting favorable convergence and/or computational complexity. version:1
arxiv-1603-00423 | Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs | http://arxiv.org/abs/1603.00423 | id:1603.00423 author:Phong Le, Willem Zuidema category:cs.AI cs.CL cs.NE  published:2016-03-01 summary:Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary. version:1
arxiv-1511-06432 | Delving Deeper into Convolutional Networks for Learning Video Representations | http://arxiv.org/abs/1511.06432 | id:1511.06432 author:Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville category:cs.CV cs.LG cs.NE  published:2015-11-19 summary:We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features. version:4
arxiv-1603-00389 | Multi-Information Source Optimization with General Model Discrepancies | http://arxiv.org/abs/1603.00389 | id:1603.00389 author:Matthias Poloczek, Jialei Wang, Peter I. Frazier category:stat.ML  published:2016-03-01 summary:In the multi-information source optimization problem our goal is to optimize a complex design. However, we only have indirect access to the objective value of any design via information sources that are subject to model discrepancy, i.e. whose internal model inherently deviates from reality. We present a novel algorithm that is based on a rigorous mathematical treatment of the uncertainties arising from the model discrepancies. Its optimization decisions rely on a stringent value of information analysis that trades off the predicted benefit and its cost. We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process. version:1
arxiv-1511-05666 | Super-Resolution with Deep Convolutional Sufficient Statistics | http://arxiv.org/abs/1511.05666 | id:1511.05666 author:Joan Bruna, Pablo Sprechmann, Yann LeCun category:cs.CV  published:2015-11-18 summary:Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension. version:4
arxiv-1403-3320 | Numerical Approaches for Linear Left-invariant Diffusions on SE(2), their Comparison to Exact Solutions, and their Applications in Retinal Imaging | http://arxiv.org/abs/1403.3320 | id:1403.3320 author:Jiong Zhang, Remco Duits, Gonzalo Sanguinetti, Bart M. ter Haar Romeny category:math.NA cs.CV  published:2014-03-13 summary:Left-invariant PDE-evolutions on the roto-translation group $SE(2)$ (and their resolvent equations) have been widely studied in the fields of cortical modeling and image analysis. They include hypo-elliptic diffusion (for contour enhancement) proposed by Citti & Sarti, and Petitot, and they include the direction process (for contour completion) proposed by Mumford. This paper presents a thorough study and comparison of the many numerical approaches, which, remarkably, is missing in the literature. Existing numerical approaches can be classified into 3 categories: Finite difference methods, Fourier based methods (equivalent to $SE(2)$-Fourier methods), and stochastic methods (Monte Carlo simulations). There are also 3 types of exact solutions to the PDE-evolutions that were derived explicitly (in the spatial Fourier domain) in previous works by Duits and van Almsick in 2005. Here we provide an overview of these 3 types of exact solutions and explain how they relate to each of the 3 numerical approaches. We compute relative errors of all numerical approaches to the exact solutions, and the Fourier based methods show us the best performance with smallest relative errors. We also provide an improvement of Mathematica algorithms for evaluating Mathieu-functions, crucial in implementations of the exact solutions. Furthermore, we include an asymptotical analysis of the singularities within the kernels and we propose a probabilistic extension of underlying stochastic processes that overcomes the singular behavior in the origin of time-integrated kernels. Finally, we show retinal imaging applications of combining left-invariant PDE-evolutions with invertible orientation scores. version:7
arxiv-1509-06113 | Deep Spatial Autoencoders for Visuomotor Learning | http://arxiv.org/abs/1509.06113 | id:1509.06113 author:Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel category:cs.LG cs.CV cs.RO  published:2015-09-21 summary:Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm. version:3
arxiv-1507-04760 | Driver Gaze Region Estimation Without Using Eye Movement | http://arxiv.org/abs/1507.04760 | id:1507.04760 author:Lex Fridman, Philipp Langhans, Joonbum Lee, Bryan Reimer category:cs.CV  published:2015-07-16 summary:Automated estimation of the allocation of a driver's visual attention may be a critical component of future Advanced Driver Assistance Systems. In theory, vision-based tracking of the eye can provide a good estimate of gaze location. In practice, eye tracking from video is challenging because of sunglasses, eyeglass reflections, lighting conditions, occlusions, motion blur, and other factors. Estimation of head pose, on the other hand, is robust to many of these effects, but cannot provide as fine-grained of a resolution in localizing the gaze. However, for the purpose of keeping the driver safe, it is sufficient to partition gaze into regions. In this effort, we propose a system that extracts facial features and classifies their spatial configuration into six regions in real-time. Our proposed method achieves an average accuracy of 91.4% at an average decision rate of 11 Hz on a dataset of 50 drivers from an on-road study. version:2
arxiv-1603-00284 | Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition | http://arxiv.org/abs/1603.00284 | id:1603.00284 author:Aleksandr Y. Aravkin, Stephen Becker category:stat.ML cs.CV math.OC  published:2016-03-01 summary:We focus on the robust principal component analysis (RPCA) problem, and review a range of old and new convex formulations for the problem and its variants. We then review dual smoothing and level set techniques in convex optimization, present several novel theoretical results, and apply the techniques on the RPCA problem. In the final sections, we show a range of numerical experiments for simulated and real-world problems. version:1
arxiv-1602-00542 | Cluster-Seeking James-Stein Estimators | http://arxiv.org/abs/1602.00542 | id:1602.00542 author:K. Pavan Srinath, Ramji Venkataramanan category:cs.IT math.IT math.ST stat.ML stat.TH  published:2016-02-01 summary:This paper considers the problem of estimating a high-dimensional vector of parameters $\boldsymbol{\theta} \in \mathbb{R}^n$ from a noisy observation. The noise vector is i.i.d. Gaussian with known variance. For a squared-error loss function, the James-Stein (JS) estimator is known to dominate the simple maximum-likelihood (ML) estimator when the dimension $n$ exceeds two. The JS-estimator shrinks the observed vector towards the origin, and the risk reduction over the ML-estimator is greatest for $\boldsymbol{\theta}$ that lie close to the origin. JS-estimators can be generalized to shrink the data towards any target subspace. Such estimators also dominate the ML-estimator, but the risk reduction is significant only when $\boldsymbol{\theta}$ lies close to the subspace. This leads to the question: in the absence of prior information about $\boldsymbol{\theta}$, how do we design estimators that give significant risk reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$? In this paper, we propose shrinkage estimators that attempt to infer the structure of $\boldsymbol{\theta}$ from the observed data in order to construct a good attracting subspace. In particular, the components of the observed vector are separated into clusters, and the elements in each cluster shrunk towards a common attractor. The number of clusters and the attractor for each cluster are determined from the observed vector. We provide concentration results for the squared-error loss and convergence results for the risk of the proposed estimators. The results show that the estimators give significant risk reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$, particularly for large $n$. Simulation results are provided to support the theoretical claims. version:2
arxiv-1511-05389 | Learning to retrieve out-of-vocabulary words in speech recognition | http://arxiv.org/abs/1511.05389 | id:1511.05389 author:Imran Sheikh, Irina Illina, Dominique Fohr, Georges Linarès category:cs.CL  published:2015-11-17 summary:Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech recognition systems used to process diachronic audio data. To help recovery of the PNs missed by the system, relevant OOV PNs can be retrieved out of the many OOVs by exploiting semantic context of the spoken content. In this paper, we propose two neural network models targeted to retrieve OOV PNs relevant to an audio document: (a) Document level Continuous Bag of Words (D-CBOW), (b) Document level Continuous Bag of Weighted Words (D-CBOW2). Both these models take document words as input and learn with an objective to maximise the retrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new approach in which the input embedding layer is augmented with a context anchor layer. This layer learns to assign importance to input words and has the ability to capture (task specific) key-words in a bag-of-word neural network model. With experiments on French broadcast news videos we show that these two models outperform the baseline methods based on raw embeddings from LDA, Skip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives faster convergence during training. version:4
arxiv-1603-00275 | Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest | http://arxiv.org/abs/1603.00275 | id:1603.00275 author:Korsuk Sirinukunwattana, Josien P. W. Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J. Matuszewski, Elia Bruni, Urko Sanchez, Anton Böhm, Olaf Ronneberger, Bassem Ben Cheikh, Daniel Racoceanu, Philipp Kainz, Michael Pfeiffer, Martin Urschler, David R. J. Snead, Nasir M. Rajpoot category:cs.CV  published:2016-03-01 summary:Colorectal adenocarcinoma originating in intestinal glandular structures is the most common form of colon cancer. In clinical practice, the morphology of intestinal glands, including architectural appearance and glandular formation, is used by pathologists to inform prognosis and plan the treatment of individual patients. However, achieving good inter-observer as well as intra-observer reproducibility of cancer grading is still a major challenge in modern pathology. An automated approach which quantifies the morphology of glands is a solution to the problem. This paper provides an overview to the Gland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at MICCAI'2015. Details of the challenge, including organization, dataset and evaluation criteria, are presented, along with the method descriptions and evaluation results from the top performing methods. version:1
arxiv-1603-00260 | Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search and Analytics | http://arxiv.org/abs/1603.00260 | id:1603.00260 author:Dhruv Gupta category:cs.IR cs.CL  published:2016-03-01 summary:In this article, I present the questions that I seek to answer in my PhD research. I posit to analyze natural language text with the help of semantic annotations and mine important events for navigating large text corpora. Semantic annotations such as named entities, geographic locations, and temporal expressions can help us mine events from the given corpora. These events thus provide us with useful means to discover the locked knowledge in them. I pose three problems that can help unlock this knowledge vault in semantically annotated text corpora: i. identifying important events; ii. semantic search; and iii. event analytics. version:1
arxiv-1502-03273 | Image denoising based on improved data-driven sparse representation | http://arxiv.org/abs/1502.03273 | id:1502.03273 author:Dai-Qiang Chen category:cs.CV 68U10  90C90  65T60  published:2015-02-11 summary:Sparse representation of images under certain transform domain has been playing a fundamental role in image restoration tasks. One such representative method is the widely used wavelet tight frame systems. Instead of adopting fixed filters for constructing a tight frame to sparsely model any input image, a data-driven tight frame was proposed for the sparse representation of images, and shown to be very efficient for image denoising very recently. However, in this method the number of framelet filters used for constructing a tight frame is the same as the length of filters. In fact, through further investigation it is found that part of these filters are unnecessary and even harmful to the recovery effect due to the influence of noise. Therefore, an improved data-driven sparse representation systems constructed with much less number of filters are proposed. Numerical results on denoising experiments demonstrate that the proposed algorithm overall outperforms the original data-driven tight frame construction scheme on both the recovery quality and computational time. version:2
arxiv-1511-06114 | Multi-task Sequence to Sequence Learning | http://arxiv.org/abs/1511.06114 | id:1511.06114 author:Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser category:cs.LG cs.CL stat.ML  published:2015-11-19 summary:Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought. version:4
arxiv-1603-00223 | Segmental Recurrent Neural Networks for End-to-end Speech Recognition | http://arxiv.org/abs/1603.00223 | id:1603.00223 author:Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith, Steve Renals category:cs.CL cs.LG cs.NE  published:2016-03-01 summary:We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model. version:1
arxiv-1509-06664 | Reasoning about Entailment with Neural Attention | http://arxiv.org/abs/1509.06664 | id:1509.06664 author:Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Phil Blunsom category:cs.CL cs.AI cs.LG cs.NE 68T50 I.2.6; I.2.7  published:2015-09-22 summary:While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset. version:4
arxiv-1602-05875 | Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data | http://arxiv.org/abs/1602.05875 | id:1602.05875 author:Gil Keren, Björn Schuller category:stat.ML cs.CL  published:2016-02-18 summary:Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. version:2
arxiv-1602-03014 | Herding as a Learning System with Edge-of-Chaos Dynamics | http://arxiv.org/abs/1602.03014 | id:1602.03014 author:Yutian Chen, Max Welling category:stat.ML cs.LG  published:2016-02-09 summary:Herding defines a deterministic dynamical system at the edge of chaos. It generates a sequence of model states and parameters by alternating parameter perturbations with state maximizations, where the sequence of states can be interpreted as "samples" from an associated MRF model. Herding differs from maximum likelihood estimation in that the sequence of parameters does not converge to a fixed point and differs from an MCMC posterior sampling approach in that the sequence of states is generated deterministically. Herding may be interpreted as a"perturb and map" method where the parameter perturbations are generated using a deterministic nonlinear dynamical system rather than randomly from a Gumbel distribution. This chapter studies the distinct statistical characteristics of the herding algorithm and shows that the fast convergence rate of the controlled moments may be attributed to edge of chaos dynamics. The herding algorithm can also be generalized to models with latent variables and to a discriminative learning setting. The perceptron cycling theorem ensures that the fast moment matching property is preserved in the more general framework. version:2
arxiv-1510-03421 | Towards Meaningful Maps of Polish Case Law | http://arxiv.org/abs/1510.03421 | id:1510.03421 author:Michal Jungiewicz, Michał Łopuszyński category:cs.CL  published:2015-10-12 summary:In this work, we analyze the utility of two dimensional document maps for exploratory analysis of Polish case law. We start by comparing two methods of generating such visualizations. First is based on linear principal component analysis (PCA). Second makes use of the modern nonlinear t-Distributed Stochastic Neighbor Embedding method (t-SNE). We apply both PCA and t-SNE to a corpus of judgments from different courts in Poland. It emerges that t-SNE provides better, more interpretable results than PCA. As a next test, we apply t-SNE to randomly selected sample of common court judgments corresponding to different keywords. We show that t-SNE, in this case, reveals hidden topical structure of the documents related to keyword,,pension". In conclusion, we find that the t-SNE method could be a promising tool to facilitate the exploitative analysis of legal texts, e.g., by complementing search or browse functionality in legal databases. version:2
arxiv-1503-00848 | Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation | http://arxiv.org/abs/1503.00848 | id:1503.00848 author:Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Ferran Marques, Jitendra Malik category:cs.CV  published:2015-03-03 summary:We propose a unified approach for bottom-up hierarchical image segmentation and object proposal generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efficiently their combinatorial space. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in under five second per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals. version:4
arxiv-1511-06361 | Order-Embeddings of Images and Language | http://arxiv.org/abs/1511.06361 | id:1511.06361 author:Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun category:cs.LG cs.CL cs.CV  published:2015-11-19 summary:Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval. version:6
arxiv-1511-04834 | Neural Programmer: Inducing Latent Programs with Gradient Descent | http://arxiv.org/abs/1511.04834 | id:1511.04834 author:Arvind Neelakantan, Quoc V. Le, Ilya Sutskever category:cs.LG cs.CL stat.ML  published:2015-11-16 summary:Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. version:2
arxiv-1602-08127 | Auto-JacoBin: Auto-encoder Jacobian Binary Hashing | http://arxiv.org/abs/1602.08127 | id:1602.08127 author:Xiping Fu, Brendan McCane, Steven Mills, Michael Albert, Lech Szymanski category:cs.CV cs.LG  published:2016-02-25 summary:Binary codes can be used to speed up nearest neighbor search tasks in large scale data sets as they are efficient for both storage and retrieval. In this paper, we propose a robust auto-encoder model that preserves the geometric relationships of high-dimensional data sets in Hamming space. This is done by considering a noise-removing function in a region surrounding the manifold where the training data points lie. This function is defined with the property that it projects the data points near the manifold into the manifold wisely, and we approximate this function by its first order approximation. Experimental results show that the proposed method achieves better than state-of-the-art results on three large scale high dimensional data sets. version:2
arxiv-1603-00150 | GOGMA: Globally-Optimal Gaussian Mixture Alignment | http://arxiv.org/abs/1603.00150 | id:1603.00150 author:Dylan Campbell, Lars Petersson category:cs.CV cs.RO  published:2016-03-01 summary:Gaussian mixture alignment is a family of approaches that are frequently used for robustly solving the point-set registration problem. However, since they use local optimisation, they are susceptible to local minima and can only guarantee local optimality. Consequently, their accuracy is strongly dependent on the quality of the initialisation. This paper presents the first globally-optimal solution to the 3D rigid Gaussian mixture alignment problem under the L2 distance between mixtures. The algorithm, named GOGMA, employs a branch-and-bound approach to search the space of 3D rigid motions SE(3), guaranteeing global optimality regardless of the initialisation. The geometry of SE(3) was used to find novel upper and lower bounds for the objective function and local optimisation was integrated into the scheme to accelerate convergence without voiding the optimality guarantee. The evaluation empirically supported the optimality proof and showed that the method performed much more robustly on two challenging datasets than an existing globally-optimal registration solution. version:1
arxiv-1603-00146 | Storm Detection by Visual Learning Using Satellite Images | http://arxiv.org/abs/1603.00146 | id:1603.00146 author:Yu Zhang, Stephen Wistar, Jia Li, Michael Steinberg, James Z. Wang category:cs.CV  published:2016-03-01 summary:Computers are widely utilized in today's weather forecasting as a powerful tool to leverage an enormous amount of data. Yet, despite the availability of such data, current techniques often fall short of producing reliable detailed storm forecasts. Each year severe thunderstorms cause significant damage and loss of life, some of which could be avoided if better forecasts were available. We propose a computer algorithm that analyzes satellite images from historical archives to locate visual signatures of severe thunderstorms for short-term predictions. While computers are involved in weather forecasts to solve numerical models based on sensory data, they are less competent in forecasting based on visual patterns from satellite images. In our system, we extract and summarize important visual storm evidence from satellite image sequences in the way that meteorologists interpret the images. In particular, the algorithm extracts and fits local cloud motion from image sequences to model the storm-related cloud patches. Image data from the year 2008 have been adopted to train the model, and historical thunderstorm reports in continental US from 2000 through 2013 have been used as the ground-truth and priors in the modeling process. Experiments demonstrate the usefulness and potential of the algorithm for producing more accurate thunderstorm forecasts. version:1
arxiv-1603-00145 | On Tie Strength Augmented Social Correlation for Inferring Preference of Mobile Telco Users | http://arxiv.org/abs/1603.00145 | id:1603.00145 author:Shifeng Liu, Zheng Hu, Sujit Dey, Xin Ke category:cs.SI cs.IR cs.LG  published:2016-03-01 summary:For mobile telecom operators, it is critical to build preference profiles of their customers and connected users, which can help operators make better marketing strategies, and provide more personalized services. With the deployment of deep packet inspection (DPI) in telecom networks, it is possible for the telco operators to obtain user online preference. However, DPI has its limitations and user preference derived only from DPI faces sparsity and cold start problems. To better infer the user preference, social correlation in telco users network derived from Call Detailed Records (CDRs) with regard to online preference is investigated. Though widely verified in several online social networks, social correlation between online preference of users in mobile telco networks, where the CDRs derived relationship are of less social properties and user mobile internet surfing activities are not visible to neighbourhood, has not been explored at a large scale. Based on a real world telecom dataset including CDRs and preference of more than $550K$ users for several months, we verified that correlation does exist between online preference in such \textit{ambiguous} social network. Furthermore, we found that the stronger ties that users build, the more similarity between their preference may have. After defining the preference inferring task as a Top-$K$ recommendation problem, we incorporated Matrix Factorization Collaborative Filtering model with social correlation and tie strength based on call patterns to generate Top-$K$ preferred categories for users. The proposed Tie Strength Augmented Social Recommendation (TSASoRec) model takes data sparsity and cold start user problems into account, considering both the recorded and missing recorded category entries. The experiment on real dataset shows the proposed model can better infer user preference, especially for cold start users. version:1
arxiv-1602-06664 | A Geometric Analysis of Phase Retrieval | http://arxiv.org/abs/1602.06664 | id:1602.06664 author:Ju Sun, Qing Qu, John Wright category:cs.IT math.IT math.OC stat.ML  published:2016-02-22 summary:Can we recover a complex signal from its Fourier magnitudes? More generally, given a set of $m$ measurements, $y_k = \mathbf a_k^* \mathbf x $ for $k = 1, \dots, m$, is it possible to recover $\mathbf x \in \mathbb{C}^n$ (i.e., length-$n$ complex vector)? This **generalized phase retrieval** (GPR) problem is a fundamental task in various disciplines, and has been the subject of much recent investigation. Natural nonconvex heuristics often work remarkably well for GPR in practice, but lack clear theoretical explanations. In this paper, we take a step towards bridging this gap. We prove that when the measurement vectors $\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number of measurements is large enough ($m \ge C n \log^3 n$), with high probability, a natural least-squares formulation for GPR has the following benign geometric structure: (1) there are no spurious local minimizers, and all global minimizers are equal to the target signal $\mathbf x$, up to a global phase; and (2) the objective function has a negative curvature around each saddle point. This structure allows a number of iterative optimization methods to efficiently find a global minimizer, without special initialization. To corroborate the claim, we describe and analyze a second-order trust-region algorithm. version:2
arxiv-1508-07468 | Image Annotation Incorporating Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors | http://arxiv.org/abs/1508.07468 | id:1508.07468 author:Yuqing Hou category:cs.CV  published:2015-08-29 summary:Tag-based image retrieval (TBIR) has drawn much attention in recent years due to the explosive amount of digital images and crowdsourcing tags. However, TBIR is still suffering from the incomplete and inaccurate tags provided by users, posing a great challenge for tag-based image management applications. In this work, we proposed a novel method for image annotation, incorporating several priors: Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors. Highly representative CNN feature vectors are adopt to model the tag-visual correlation and narrow the semantic gap. And we extract word vectors for tags to measure similarity between tags in the semantic level, which is more accurate than traditional frequency-based or graph-based methods. We utilize the accelerated proximal gradient (APG) method to solve our model efficiently. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and robustness of the proposed method. version:2
arxiv-1601-03055 | Image Annotation combining Subspace Clustering , Matrix Completion and Inhomogeneous Errors | http://arxiv.org/abs/1601.03055 | id:1601.03055 author:Yuqing Hou category:cs.CV  published:2016-01-12 summary:Image annotation methods have greatly facilitated image management applications. However, existing methods are still suffering from the degradation of the missing and noisy tags provided by users. In this study, we propose an image annotation method which performs tag completion and refinement sequentially. We assume that images are sampled from a union of subspaces. Images sampled from the same subspace, as well as their corresponding tags, should form a compatible image-tag sub-matrix. Thus we segment the subspaces by the Sparse Subspace Clustering (SSC) method and share tags in each subspace. A novel matrix completion model is designed for tag refinement, taking visual-tag correlation, semantic-tag correlation and the inhomogeneous errors property, which is explored in this field for the first time, into consideration. We exploit CNN features to improve the model. The proposed algorithm outperforms state-of-the-art approaches when handling missing and noisy tags on multiple benchmark datasets. version:2
arxiv-1603-00132 | A Universal Update-pacing Framework For Visual Tracking | http://arxiv.org/abs/1603.00132 | id:1603.00132 author:Zexi Hu, Yuefang Gao, Dong Wang, Xuhong Tian category:cs.CV  published:2016-03-01 summary:This paper proposes a novel framework to alleviate the model drift problem in visual tracking, which is based on paced updates and trajectory selection. Given a base tracker, an ensemble of trackers is generated, in which each tracker's update behavior will be paced and then traces the target object forward and backward to generate a pair of trajectories in an interval. Then, we implicitly perform self-examination based on trajectory pair of each tracker and select the most robust tracker. The proposed framework can effectively leverage temporal context of sequential frames and avoid to learn corrupted information. Extensive experiments on the standard benchmark suggest that the proposed framework achieves superior performance against state-of-the-art trackers. version:1
arxiv-1602-08124 | Virtualizing Deep Neural Networks for Memory-Efficient Neural Network Design | http://arxiv.org/abs/1602.08124 | id:1602.08124 author:Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, Stephen W. Keckler category:cs.DC cs.LG cs.NE  published:2016-02-25 summary:The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average memory usage of AlexNet by 61% and OverFeat by 83%, a significant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA K40 GPU card containing 12 GB of memory, with 22% performance loss compared to a hypothetical GPU with enough memory to hold the entire DNN. version:2
arxiv-1603-00128 | Cascaded Subpatch Networks for Effective CNNs | http://arxiv.org/abs/1603.00128 | id:1603.00128 author:Xiaoheng Jiang, Yanwei Pang, Manli Sun, Xuelong Li category:cs.CV  published:2016-03-01 summary:Conventional Convolutional Neural Networks (CNNs) use either a linear or non-linear filter to extract features from an image patch (region) of spatial size $ H\times W $ (Typically, $ H $ is small and is equal to $ W$, e.g., $ H $ is 5 or 7). Generally, the size of the filter is equal to the size $ H\times W $ of the input patch. We argue that the representation ability of equal-size strategy is not strong enough. To overcome the drawback, we propose to use subpatch filter whose spatial size $ h\times w $ is smaller than $ H\times W $. The proposed subpatch filter consists of two subsequent filters. The first one is a linear filter of spatial size $ h\times w $ and is aimed at extracting features from spatial domain. The second one is of spatial size $ 1\times 1 $ and is used for strengthening the connection between different input feature channels and for reducing the number of parameters. The subpatch filter convolves with the input patch and the resulting network is called a subpatch network. Taking the output of one subpatch network as input, we further repeat constructing subpatch networks until the output contains only one neuron in spatial domain. These subpatch networks form a new network called Cascaded Subpatch Network (CSNet). The feature layer generated by CSNet is called csconv layer. For the whole input image, we construct a deep neural network by stacking a sequence of csconv layers. Experimental results on four benchmark datasets demonstrate the effectiveness and compactness of the proposed CSNet. For example, our CSNet reaches a test error of $ 5.68\% $ on the CIFAR10 dataset without model averaging. To the best of our knowledge, this is the best result ever obtained on the CIFAR10 dataset. version:1
arxiv-1603-00124 | Learning Multilayer Channel Features for Pedestrian Detection | http://arxiv.org/abs/1603.00124 | id:1603.00124 author:Jiale Cao, Yanwei Pang, Xuelong Li category:cs.CV  published:2016-03-01 summary:Pedestrian detection based on the combination of Convolutional Neural Network (i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved great success. Generally, HOG+LUV are used to generate the candidate proposals and then CNN classifies these proposals. Despite its success, there is still room for improvement. For example, CNN classifies these proposals by the full-connected layer features while proposal scores and the features in the inner-layers of CNN are ignored. In this paper, we propose a unifying framework called Multilayer Channel Features (MCF) to overcome the drawback. It firstly integrates HOG+LUV with each layer of CNN into a multi-layer image channels. Based on the multi-layer image channels, a multi-stage cascade AdaBoost is then learned. The weak classifiers in each stage of the multi-stage cascade is learned from the image channels of corresponding layer. With more abundant features, MCF achieves the state-of-the-art on Caltech pedestrian dataset (i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves 7.98% miss rate. As many non-pedestrian detection windows can be quickly rejected by the first few stages, it accelerates detection speed by 1.43 times. By eliminating the highly overlapped detection windows with lower scores after the first stage, it's 4.07 times faster with negligible performance loss. version:1
arxiv-1511-03677 | Learning to Diagnose with LSTM Recurrent Neural Networks | http://arxiv.org/abs/1511.03677 | id:1511.03677 author:Zachary C. Lipton, David C. Kale, Charles Elkan, Randall Wetzell category:cs.LG  published:2015-11-11 summary:Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features. version:6
arxiv-1502-07334 | Sparse Multivariate Factor Regression | http://arxiv.org/abs/1502.07334 | id:1502.07334 author:Milad Kharratzadeh, Mark Coates category:stat.ML  published:2015-02-25 summary:We consider the problem of multivariate regression in a setting where the relevant predictors could be shared among different responses. We propose an algorithm which decomposes the coefficient matrix into the product of a long matrix and a wide matrix, with an elastic net penalty on the former and an $\ell_1$ penalty on the latter. The first matrix linearly transforms the predictors to a set of latent factors, and the second one regresses the responses on these factors. Our algorithm simultaneously performs dimension reduction and coefficient estimation and automatically estimates the number of latent factors from the data. Our formulation results in a non-convex optimization problem, which despite its flexibility to impose effective low-dimensional structure, is difficult, or even impossible, to solve exactly in a reasonable time. We specify an optimization algorithm based on alternating minimization with three different sets of updates to solve this non-convex problem and provide theoretical results on its convergence and optimality. Finally, we demonstrate the effectiveness of our algorithm via experiments on simulated and real data. version:5
arxiv-1511-06455 | Variational Auto-encoded Deep Gaussian Processes | http://arxiv.org/abs/1511.06455 | id:1511.06455 author:Zhenwen Dai, Andreas Damianou, Javier González, Neil Lawrence category:cs.LG stat.ML  published:2015-11-19 summary:We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization. version:2
arxiv-1511-06448 | Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks | http://arxiv.org/abs/1511.06448 | id:1511.06448 author:Pouya Bashivan, Irina Rish, Mohammed Yeasin, Noel Codella category:cs.LG cs.CV  published:2015-11-19 summary:One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to inter- and intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field. version:3
arxiv-1511-06281 | Density Modeling of Images using a Generalized Normalization Transformation | http://arxiv.org/abs/1511.06281 | id:1511.06281 author:Johannes Ballé, Valero Laparra, Eero P. Simoncelli category:cs.LG cs.CV  published:2015-11-19 summary:We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture. version:4
arxiv-1602-09118 | Easy Monotonic Policy Iteration | http://arxiv.org/abs/1602.09118 | id:1602.09118 author:Joshua Achiam category:cs.LG cs.AI stat.ML  published:2016-02-29 summary:A key problem in reinforcement learning for control with general function approximators (such as deep neural networks and other nonlinear functions) is that, for many algorithms employed in practice, updates to the policy or $Q$-function may fail to improve performance---or worse, actually cause the policy performance to degrade. Prior work has addressed this for policy iteration by deriving tight policy improvement bounds; by optimizing the lower bound on policy improvement, a better policy is guaranteed. However, existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which cannot be efficiently estimated or differentiated. In this work, we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework. version:1
arxiv-1509-02971 | Continuous control with deep reinforcement learning | http://arxiv.org/abs/1509.02971 | id:1509.02971 author:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra category:cs.LG stat.ML  published:2015-09-09 summary:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs. version:5
arxiv-1411-7676 | Visual Representations: Definint Properties and Deep Approximations | http://arxiv.org/abs/1411.7676 | id:1411.7676 author:Stefano Soatto, Alessandro Chiuso category:cs.CV  published:2014-11-27 summary:Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization. version:9
arxiv-1511-02793 | Generating Images from Captions with Attention | http://arxiv.org/abs/1511.02793 | id:1511.02793 author:Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov category:cs.LG cs.CV  published:2015-11-09 summary:Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset. version:2
arxiv-1602-08210 | Architectural Complexity Measures of Recurrent Neural Networks | http://arxiv.org/abs/1602.08210 | id:1602.08210 author:Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, Yoshua Bengio category:cs.LG cs.NE  published:2016-02-26 summary:In this paper, we systematically analyse the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems, as we improve the state-of-the-art for sequential MNIST dataset. version:2
arxiv-1602-09046 | On Complex Valued Convolutional Neural Networks | http://arxiv.org/abs/1602.09046 | id:1602.09046 author:Nitzan Guberman category:cs.NE  published:2016-02-29 summary:Convolutional neural networks (CNNs) are the cutting edge model for supervised machine learning in computer vision. In recent years CNNs have outperformed traditional approaches in many computer vision tasks such as object detection, image classification and face recognition. CNNs are vulnerable to overfitting, and a lot of research focuses on finding regularization methods to overcome it. One approach is designing task specific models based on prior knowledge. Several works have shown that properties of natural images can be easily captured using complex numbers. Motivated by these works, we present a variation of the CNN model with complex valued input and weights. We construct the complex model as a generalization of the real model. Lack of order over the complex field raises several difficulties both in the definition and in the training of the network. We address these issues and suggest possible solutions. The resulting model is shown to be a restricted form of a real valued CNN with twice the parameters. It is sensitive to phase structure, and we suggest it serves as a regularized model for problems where such structure is important. This suggestion is verified empirically by comparing the performance of a complex and a real network in the problem of cell detection. The two networks achieve comparable results, and although the complex model is hard to train, it is significantly less vulnerable to overfitting. We also demonstrate that the complex network detects meaningful phase structure in the data. version:1
arxiv-1511-04773 | Large-Scale Approximate Kernel Canonical Correlation Analysis | http://arxiv.org/abs/1511.04773 | id:1511.04773 author:Weiran Wang, Karen Livescu category:cs.LG  published:2015-11-15 summary:Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view representation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an $N\times N$ eigenvalue system where $N$ is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to first transform the original inputs to an $M$-dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality $M$ of the random feature space may need to be very large in order to obtain a sufficiently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a stochastic optimization algorithm, recently proposed for linear CCA and its neural-network extension, to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with $1.4$ million training samples and a random feature space of dimensionality $M=100000$ on a typical workstation. version:4
arxiv-1511-06410 | Better Computer Go Player with Neural Network and Long-term Prediction | http://arxiv.org/abs/1511.06410 | id:1511.06410 author:Yuandong Tian, Yan Zhu category:cs.LG cs.AI  published:2015-11-19 summary:Competing with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go's high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go's evaluation function could change drastically with one stone change. Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for pattern-matching approaches against MCTS-based approaches, even with looser search budgets. Against human players, the newest versions, darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a substantial improvement upon the estimated 4k-5k ranks for DCNN reported in Clark & Storkey (2015) based on games against other machine players. Adding MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000 rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al. (2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament. version:3
arxiv-1601-06759 | Pixel Recurrent Neural Networks | http://arxiv.org/abs/1601.06759 | id:1601.06759 author:Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu category:cs.CV cs.LG cs.NE  published:2016-01-25 summary:Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent. version:2
arxiv-1602-08986 | Even Trolls Are Useful: Efficient Link Classification in Signed Networks | http://arxiv.org/abs/1602.08986 | id:1602.08986 author:Géraud Le Falher, Fabio Vitale category:cs.LG cs.SI physics.soc-ph  published:2016-02-29 summary:We address the problem of classifying the links of signed social networks given their full structural topology. Motivated by a binary user behaviour assumption, which is supported by decades of research in psychology, we develop an efficient and surprisingly simple approach to solve this classification problem. Our methods operate both within the active and batch settings. We demonstrate that the algorithms we developed are extremely fast in both theoretical and practical terms. Within the active setting, we provide a new complexity measure and a rigorous analysis of our methods that hold for arbitrary signed networks. We validate our theoretical claims carrying out a set of experiments on three well known real-world datasets, showing that our methods outperform the competitors while being much faster. version:1
arxiv-1602-08977 | Clustering Based Feature Learning on Variable Stars | http://arxiv.org/abs/1602.08977 | id:1602.08977 author:Cristóbal Mackenzie, Karim Pichara, Pavlos Protopapas category:astro-ph.SR cs.CV  published:2016-02-29 summary:The success of automatic classification of variable stars strongly depends on the lightcurve representation. Usually, lightcurves are represented as a vector of many statistical descriptors designed by astronomers called features. These descriptors commonly demand significant computational power to calculate, require substantial research effort to develop and do not guarantee good performance on the final classification task. Today, lightcurve representation is not entirely automatic; algorithms that extract lightcurve features are designed by humans and must be manually tuned up for every survey. The vast amounts of data that will be generated in future surveys like LSST mean astronomers must develop analysis pipelines that are both scalable and automated. Recently, substantial efforts have been made in the machine learning community to develop methods that prescind from expert-designed and manually tuned features for features that are automatically learned from data. In this work we present what is, to our knowledge, the first unsupervised feature learning algorithm designed for variable stars. Our method first extracts a large number of lightcurve subsequences from a given set of photometric data, which are then clustered to find common local patterns in the time series. Representatives of these patterns, called exemplars, are then used to transform lightcurves of a labeled set into a new representation that can then be used to train an automatic classifier. The proposed algorithm learns the features from both labeled and unlabeled lightcurves, overcoming the bias generated when the learning process is done only with labeled data. We test our method on MACHO and OGLE datasets; the results show that the classification performance we achieve is as good and in some cases better than the performance achieved using traditional features, while the computational cost is significantly lower. version:1
arxiv-1602-08960 | FALDOI: Large Displacement Optical Flow by Astute Initialization | http://arxiv.org/abs/1602.08960 | id:1602.08960 author:Roberto P. Palomares, Gloria Haro, Coloma Ballester, Enric Meinhardt-Llopis category:cs.CV 68U10  49M29  65K10  published:2016-02-29 summary:We propose a large displacement optical flow method that introduces a new strategy to compute a good local minimum of any optical flow energy functional. The method requires a given set of discrete matches, which can be extremely sparse, and an energy functional. The matches are used to guide a structured coordinate-descent of the energy functional around these keypoints. It results in a two-step minimization method at the finest scale which is very robust to the inevitable outliers of the sparse matcher, and it is better than the multi- scale methods, especially when there are small objects with very large displacements, that the multi-scale methods are incapable to find. Indeed, the proposed method recovers the correct motion field of any object which has at least one correct match, regardless of the magnitude of the displacement. We validate our proposal using several optical flow variational models. The results consistently outperform the coarse-to-fine approaches and achieve good qualitative and quantitative performance on the standard optical flow benchmarks. version:1
arxiv-1602-05659 | Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification | http://arxiv.org/abs/1602.05659 | id:1602.05659 author:Fuqiang Liu, Fukun Bi, Yiding Yang, Liang Chen category:cs.CV cs.LG  published:2016-02-18 summary:This paper proposes a universal method, Boost Picking, to train supervised classification models mainly by un-labeled data. Boost Picking only adopts two weak classifiers to estimate and correct the error. It is theoretically proved that Boost Picking could train a supervised model mainly by un-labeled data as effectively as the same model trained by 100% labeled data, only if recalls of the two weak classifiers are all greater than zero and the sum of precisions is greater than one. Based on Boost Picking, we present "Test along with Training (TawT)" to improve the generalization of supervised models. Both Boost Picking and TawT are successfully tested in varied little data sets. version:2
arxiv-1602-08927 | $L_2$Boosting in High-Dimensions: Rate of Convergence | http://arxiv.org/abs/1602.08927 | id:1602.08927 author:Ye Luo, Martin Spindler category:stat.ML cs.LG math.ST stat.ME stat.TH  published:2016-02-29 summary:Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional setting. Moreover, we introduce so-called \textquotedblleft post-Boosting\textquotedblright. This is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $L_2$Boosting. Another variant is orthogonal boosting where after each step an orthogonal projection is conducted. We show that both post-$L_2$Boosting and the orthogonal boosting achieve the same rate of convergence as Lasso in a sparse, high-dimensional setting. The \textquotedblleft classical\textquotedblright $L_2$Boosting achieves a slower convergence rate for prediction, but no assumptions on the design matrix are imposed for this result in contrast to rates e.g.~established with LASSO. We also introduce rules for early stopping which can easily be implemented and will be used in applied work. Moreover, our results also allow a direct comparison between LASSO and boosting that has been missing in the literature. Finally, we present simulation studies to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In the simulation studies post-$L_2$Boosting clearly outperforms LASSO. version:1
arxiv-1511-06279 | Neural Programmer-Interpreters | http://arxiv.org/abs/1511.06279 | id:1511.06279 author:Scott Reed, Nando de Freitas category:cs.LG cs.NE  published:2015-11-19 summary:We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms. version:4
arxiv-1503-07027 | Convergence radius and sample complexity of ITKM algorithms for dictionary learning | http://arxiv.org/abs/1503.07027 | id:1503.07027 author:Karin Schnass category:cs.LG cs.IT math.IT  published:2015-03-24 summary:In this work we show that iterative thresholding and K-means (ITKM) algorithms can recover a generating dictionary with K atoms from noisy $S$ sparse signals up to an error $\tilde \varepsilon$ as long as the initialisation is within a convergence radius, that is up to a $\log K$ factor inversely proportional to the dynamic range of the signals, and the sample size is proportional to $K \log K \tilde \varepsilon^{-2}$. The results are valid for arbitrary target errors if the sparsity level is of the order of the square of the signal dimension $d$ and for target errors down to $K^{-\ell}$ if $S$ scales as $S \leq d/(\ell \log K)$. version:3
arxiv-1602-08886 | Stochastic bandits on a social network: Collaborative learning with local information sharing | http://arxiv.org/abs/1602.08886 | id:1602.08886 author:Ravi Kumar Kolla, Krishna Jagannathan, Aditya Gopalan category:cs.LG stat.ML  published:2016-02-29 summary:We consider a collaborative online learning paradigm, wherein a group of agents connected through a social network are engaged in learning a Multi-Armed Bandit problem. Each time an agent takes an action, the corresponding reward is instantaneously observed by the agent, as well as its neighbours in the social network. We perform a regret analysis of various policies in this collaborative learning setting. A key finding of this paper is that appropriate network extensions of widely-studied single agent learning policies do not perform well in terms of regret. In particular, we identify a class of non-altruistic and individually consistent policies, which could suffer a large regret. We also show that the regret performance can be substantially improved by exploiting the network structure. Specifically, we consider a star network, which is a common motif in hierarchical social networks, and show that the hub agent can be used as an information sink, to aid the learning rates of the entire network. We also present numerical experiments to corroborate our analytical results. version:1
arxiv-1602-08855 | Pandora: Description of a Painting Database for Art Movement Recognition with Baselines and Perspectives | http://arxiv.org/abs/1602.08855 | id:1602.08855 author:Corneliu Florea, Razvan Condorovici, Constantin Vertan, Raluca Boia, Laura Florea, Ruxandra Vranceanu category:cs.CV  published:2016-02-29 summary:To facilitate computer analysis of visual art, in the form of paintings, we introduce Pandora (Paintings Dataset for Recognizing the Art movement) database, a collection of digitized paintings labelled with respect to the artistic movement. Noting that the set of databases available as benchmarks for evaluation is highly reduced and most existing ones are limited in variability and number of images, we propose a novel large scale dataset of digital paintings. The database consists of more than 7700 images from 12 art movements. Each genre is illustrated by a number of images varying from 250 to nearly 1000. We investigate how local and global features and classification systems are able to recognize the art movement. Our experimental results suggest that accurate recognition is achievable by a combination of various categories.To facilitate computer analysis of visual art, in the form of paintings, we introduce Pandora (Paintings Dataset for Recognizing the Art movement) database, a collection of digitized paintings labelled with respect to the artistic movement. Noting that the set of databases available as benchmarks for evaluation is highly reduced and most existing ones are limited in variability and number of images, we propose a novel large scale dataset of digital paintings. The database consists of more than 7700 images from 12 art movements. Each genre is illustrated by a number of images varying from 250 to nearly 1000. We investigate how local and global features and classification systems are able to recognize the art movement. Our experimental results suggest that accurate recognition is achievable by a combination of various categories. version:1
arxiv-1602-08448 | Simple Bayesian Algorithms for Best Arm Identification | http://arxiv.org/abs/1602.08448 | id:1602.08448 author:Daniel Russo category:cs.LG  published:2016-02-26 summary:This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. I propose three simple Bayesian algorithms for adaptively allocating measurement effort. One is Top-Two Probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant a top-two sampling which considers not only the probability a design is optimal, but the expected amount by which it exceeds other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. I prove that these simple algorithms satisfy a strong optimality property. In a frequestist setting where the true quality of the designs is fixed, the posterior is said to be consistent if it correctly identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. I show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation version:2
arxiv-1602-08844 | Bioinformatics and Classical Literary Study | http://arxiv.org/abs/1602.08844 | id:1602.08844 author:Pramit Chaudhuri, Joseph P. Dexter category:cs.CL  published:2016-02-29 summary:This paper describes a collaborative project between classicists, quantitative biologists, and computer scientists to apply ideas and methods drawn from the sciences to the study of literature. A core goal of the project is the use of computational biology, natural language processing, and machine learning techniques to investigate intertextuality, reception, and related phenomena of literary significance. As a case study in our approach, here we describe the use of sequence alignment, a common technique in genomics, to detect intertextuality in Latin literature. Sequence alignment is distinguished by its ability to find inexact verbal parallels, which makes it ideal for identifying phonetic resemblances in large corpora of Latin texts. Although especially suited to Latin, sequence alignment in principle can be extended to many other languages. version:1
arxiv-1602-08465 | Seq-NMS for Video Object Detection | http://arxiv.org/abs/1602.08465 | id:1602.08465 author:Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, Thomas S. Huang category:cs.CV  published:2016-02-26 summary:Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015). version:2
arxiv-1602-08802 | Exploring the coevolution of predator and prey morphology and behavior | http://arxiv.org/abs/1602.08802 | id:1602.08802 author:Randal S. Olson, Arend Hintze, Fred C. Dyer, Jason H. Moore, Christoph Adami category:q-bio.PE cs.NE  published:2016-02-29 summary:A common idiom in biology education states, "Eyes in the front, the animal hunts. Eyes on the side, the animal hides." In this paper, we explore one possible explanation for why predators tend to have forward-facing, high-acuity visual systems. We do so using an agent-based computational model of evolution, where predators and prey interact and adapt their behavior and morphology to one another over successive generations of evolution. In this model, we observe a coevolutionary cycle between prey swarming behavior and the predator's visual system, where the predator and prey continually adapt their visual system and behavior, respectively, over evolutionary time in reaction to one another due to the well-known "predator confusion effect." Furthermore, we provide evidence that the predator visual system is what drives this coevolutionary cycle, and suggest that the cycle could be closed if the predator evolves a hybrid visual system capable of narrow, high-acuity vision for tracking prey as well as broad, coarse vision for prey discovery. Thus, the conflicting demands imposed on a predator's visual system by the predator confusion effect could have led to the evolution of complex eyes in many predators. version:1
arxiv-1602-08800 | Iterative Aggregation Method for Solving Principal Component Analysis Problems | http://arxiv.org/abs/1602.08800 | id:1602.08800 author:Vitaly Bulgakov category:cs.NA cs.IR cs.LG  published:2016-02-29 summary:Motivated by the previously developed multilevel aggregation method for solving structural analysis problems a novel two-level aggregation approach for efficient iterative solution of Principal Component Analysis (PCA) problems is proposed. The course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem by a power iterations method. The method is tested on several data sets consisting of large number of text documents. version:1
arxiv-1602-07800 | Monomial Gamma Monte Carlo Sampling | http://arxiv.org/abs/1602.07800 | id:1602.07800 author:Yizhe Zhang, Xiangyu Wang, Changyou Chen, Kai Fan, Lawrence Carin category:stat.ML stat.ME  published:2016-02-25 summary:We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling by demonstrating their connection under the canonical transformation from Hamiltonian mechanics. This insight enables us to extend HMC and slice sampling to a broader family of samplers, called monomial Gamma samplers (MGS). We analyze theoretically the mixing performance of such samplers by proving that the MGS draws samples from a target distribution with zero-autocorrelation, in the limit of a single parameter. This property potentially allows us to generating decorrelated samples, which is not achievable by existing MCMC algorithms. We further show that this performance gain is obtained at a cost of increasing the complexity of numerical integrators. Our theoretical results are validated with synthetic data and real-world applications. version:3
arxiv-1407-3422 | A Spectral Algorithm for Inference in Hidden Semi-Markov Models | http://arxiv.org/abs/1407.3422 | id:1407.3422 author:Igor Melnyk, Arindam Banerjee category:stat.ML cs.LG  published:2014-07-12 summary:Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large datasets. version:3
arxiv-1511-06051 | SparkNet: Training Deep Networks in Spark | http://arxiv.org/abs/1511.06051 | id:1511.06051 author:Philipp Moritz, Robert Nishihara, Ion Stoica, Michael I. Jordan category:stat.ML cs.DC cs.LG cs.NE math.OC  published:2015-11-19 summary:Training deep networks is a time-consuming process, with networks for object recognition often requiring multiple days to train. For this reason, leveraging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster's communication overhead, and we benchmark our system's performance on the ImageNet dataset. version:4
arxiv-1602-06550 | Semi-Markov Switching Vector Autoregressive Model-based Anomaly Detection in Aviation Systems | http://arxiv.org/abs/1602.06550 | id:1602.06550 author:Igor Melnyk, Arindam Banerjee, Bryan Matthews, Nikunj Oza category:cs.LG stat.AP stat.ML  published:2016-02-21 summary:In this work we consider the problem of anomaly detection in heterogeneous, multivariate, variable-length time series datasets. Our focus is on the aviation safety domain, where data objects are flights and time series are sensor readings and pilot switches. In this context the goal is to detect anomalous flight segments, due to mechanical, environmental, or human factors in order to identifying operationally significant events and provide insights into the flight operations and highlight otherwise unavailable potential safety risks and precursors to accidents. For this purpose, we propose a framework which represents each flight using a semi-Markov switching vector autoregressive (SMS-VAR) model. Detection of anomalies is then based on measuring dissimilarities between the model's prediction and data observation. The framework is scalable, due to the inherent parallel nature of most computations, and can be used to perform online anomaly detection. Extensive experimental results on simulated and real datasets illustrate that the framework can detect various types of anomalies along with the key parameters involved. version:2
arxiv-1602-08780 | Does quantification without adjustments work? | http://arxiv.org/abs/1602.08780 | id:1602.08780 author:Dirk Tasche category:stat.ML cs.LG math.ST stat.TH 62C10  published:2016-02-28 summary:Classification is the task of predicting the class labels of objects based on the observation of their features. In contrast, quantification has been defined as the task of determining the prevalence of the positive class labels in a target dataset. The simplest approach to quantification is Classify & Count where a classifier is optimised for classification on a training set and applied to the target dataset for the prediction of positive class labels. The number of predicted positive labels is then used as an estimate of the positive class prevalence in the target dataset. Since the performance of Classify & Count for quantification is known to be inferior its results typically are subject to adjustments. However, some researchers recently have suggested that Classify & Count might actually work without adjustments if it is based on a classifier that was specifically trained for quantification. We discuss the theoretical foundation for this claim and explore its potential and limitations with a numerical example based on the binormal model with equal variances. version:1
arxiv-1602-06606 | Estimating Structured Vector Autoregressive Model | http://arxiv.org/abs/1602.06606 | id:1602.06606 author:Igor Melnyk, Arindam Banerjee category:math.ST stat.ML stat.TH  published:2016-02-21 summary:While considerable advances have been made in estimating high-dimensional structured models from independent data using Lasso-type models, limited progress has been made for settings when the samples are dependent. We consider estimating structured VAR (vector auto-regressive models), where the structure can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted Lasso, sparse group Lasso, etc. In VAR setting with correlated noise, although there is strong dependence over time and covariates, we establish bounds on the non-asymptotic estimation error of structured VAR parameters. Surprisingly, the estimation error is of the same order as that of the corresponding Lasso-type estimator with independent samples, and the analysis holds for any norm. Our analysis relies on results in generic chaining, sub-exponential martingales, and spectral representation of VAR models. Experimental results on synthetic data with a variety of structures as well as real aviation data are presented, validating theoretical results. version:2
arxiv-1511-07543 | Convergent Learning: Do different neural networks learn the same representations? | http://arxiv.org/abs/1511.07543 | id:1511.07543 author:Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John Hopcroft category:cs.LG cs.NE  published:2015-11-24 summary:Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units. version:3
arxiv-1602-08742 | Optimizing the Learning Order of Chinese Characters Using a Novel Topological Sort Algorithm | http://arxiv.org/abs/1602.08742 | id:1602.08742 author:James C. Loach, Jinzhao Wang category:cs.CL physics.soc-ph  published:2016-02-28 summary:We develop a novel algorithm for optimizing the order in which Chinese characters are learned, one that incorporates the benefits of learning them in order of usage frequency and in order of their hierarchal structural relationships. We show that our work outperforms previously published orderings and algorithms. Our algorithm is applicable to any scheduling task where nodes have intrinsic differences in importance and must be visited in topological order. version:1
arxiv-1602-08741 | Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task? | http://arxiv.org/abs/1602.08741 | id:1602.08741 author:Nikolay N. Vasiliev category:cs.CL  published:2016-02-28 summary:The most studied and most successful language models were developed and evaluated mainly for English and other close European languages, such as French, German, etc. It is important to study applicability of these models to other languages. The use of vector space models for Russian was recently studied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models were evaluated against word semantic similarity task. For our knowledge Twitter was not considered as a corpus for this task, with this work we fill the gap. Results for vectors trained on Twitter corpus are comparable in accuracy with other single-corpus trained models, although the best performance is currently achieved by combination of multiple corpora. version:1
arxiv-1602-08734 | A Structured Variational Auto-encoder for Learning Deep Hierarchies of Sparse Features | http://arxiv.org/abs/1602.08734 | id:1602.08734 author:Tim Salimans category:stat.ML cs.LG stat.CO  published:2016-02-28 summary:In this note we present a generative model of natural images consisting of a deep hierarchy of layers of latent random variables, each of which follows a new type of distribution that we call rectified Gaussian. These rectified Gaussian units allow spike-and-slab type sparsity, while retaining the differentiability necessary for efficient stochastic gradient variational inference. To learn the parameters of the new model, we approximate the posterior of the latent variables with a variational auto-encoder. Rather than making the usual mean-field assumption however, the encoder parameterizes a new type of structured variational approximation that retains the prior dependencies of the generative model. Using this structured posterior approximation, we are able to perform joint training of deep models with many layers of latent random variables, without having to resort to stacking or other layerwise training procedures. version:1
arxiv-1602-08715 | Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus | http://arxiv.org/abs/1602.08715 | id:1602.08715 author:Avi Shmidman, Moshe Koppel, Ely Porat category:cs.CL  published:2016-02-28 summary:We propose a method for efficiently finding all parallel passages in a large corpus, even if the passages are not quite identical due to rephrasing and orthographic variation. The key ideas are the representation of each word in the corpus by its two most infrequent letters, finding matched pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. Using this method, over 4600 parallel pairs of passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of over 1.8 million words, in just over 30 seconds. Empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods. version:1
arxiv-1511-05042 | An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family | http://arxiv.org/abs/1511.05042 | id:1511.05042 author:Alexandre de Brébisson, Pascal Vincent category:cs.NE cs.LG stat.ML  published:2015-11-16 summary:In a multi-class classification problem, it is standard to model the output of a neural network as a categorical distribution conditioned on the inputs. The output must therefore be positive and sum to one, which is traditionally enforced by a softmax. This probabilistic mapping allows to use the maximum likelihood principle, which leads to the well-known log-softmax loss. However the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions. It is thus unclear why the log-softmax loss would perform better than other loss alternatives. In particular Vincent et al. (2015) recently introduced a class of loss functions, called the spherical family, for which there exists an efficient algorithm to compute the updates of the output weights irrespective of the output size. In this paper, we explore several loss functions from this family as possible alternatives to the traditional log-softmax. In particular, we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and the log-Taylor Softmax that we introduce. Although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks, they surprisingly outperform it in our experiments on MNIST and CIFAR-10, suggesting that they might be relevant in a broad range of applications. version:3
arxiv-1602-08712 | On the entropy numbers of the mixed smoothness function classes | http://arxiv.org/abs/1602.08712 | id:1602.08712 author:V. Temlyakov category:math.NA math.FA stat.ML  published:2016-02-28 summary:Behavior of the entropy numbers of classes of multivariate functions with mixed smoothness is studied here. This problem has a long history and some fundamental problems in the area are still open. The main goal of this paper is to develop a new method of proving the upper bounds for the entropy numbers. This method is based on recent developments of nonlinear approximation, in particular, on greedy approximation. This method consists of the following two steps strategy. At the first step we obtain bounds of the best m-term approximations with respect to a dictionary. At the second step we use general inequalities relating the entropy numbers to the best m-term approximations. For the lower bounds we use the volume estimates method, which is a well known powerful method for proving the lower bounds for the entropy numbers. It was used in a number of previous papers. version:1
arxiv-1509-07308 | Bilingual Distributed Word Representations from Document-Aligned Comparable Data | http://arxiv.org/abs/1509.07308 | id:1509.07308 author:Ivan Vulić, Marie-Francine Moens category:cs.CL  published:2015-09-24 summary:We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs. version:2
arxiv-1602-08671 | Lie Access Neural Turing Machine | http://arxiv.org/abs/1602.08671 | id:1602.08671 author:Greg Yang category:cs.NE cs.AI cs.LG  published:2016-02-28 summary:Recently, Neural Turing Machine and Memory Networks have shown that adding an external memory can greatly ameliorate a traditional recurrent neural network's tendency to forget after a long period of time. Here we present a new design of an external memory, wherein memories are stored in an Euclidean key space $\mathbb R^n$. An LSTM controller performs read and write via specialized structures called read and write heads, following the design of Neural Turing Machine. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the "L" and "R" instructions of a traditional Turing Machine is generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. As LANTM is differentiable end-to-end, training was done with RMSProp. We found the right configuration of LANTM to be capable of learning different permutation and arithmetic tasks and extrapolating to at least twice the input size, all with the number of parameters 2 orders of magnitude below that for the LSTM baseline. In particular, we trained LANTM on addition of $k$-digit numbers for $2 \le k \le 16$, but it was able to generalize almost perfectly to $17 \le k \le 32$. version:1
arxiv-1602-08657 | QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source | http://arxiv.org/abs/1602.08657 | id:1602.08657 author:Luc Herren category:cs.CL  published:2016-02-28 summary:The software programs generally used with the TLG (Thesaurus Linguae Graecae) and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well suited for finding quotations and allusions. QuotationFinder uses more sophisticated criteria as it ranks search results based on how closely they match the source text, listing search results with literal quotations first and loose verbal parallels last. version:1
arxiv-1510-00098 | Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping | http://arxiv.org/abs/1510.00098 | id:1510.00098 author:Michael Xie, Neal Jean, Marshall Burke, David Lobell, Stefano Ermon category:cs.CV cs.CY  published:2015-10-01 summary:The lack of reliable data in developing countries is a major obstacle to sustainable development, food security, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming increasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no techniques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian efforts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Networks (CNN). We therefore propose a transfer learning approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are useful for poverty prediction. The model learns filters identifying different terrains and man-made structures, including roads, buildings, and farmlands, without any supervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field. version:2
arxiv-1602-07836 | A Bayesian baseline for belief in uncommon events | http://arxiv.org/abs/1602.07836 | id:1602.07836 author:V. Palonen category:stat.AP stat.ML stat.OT  published:2016-02-25 summary:The plausibility of uncommon events and miracles based on testimony of such an event has been much discussed. When analyzing the probabilities involved, it has mostly been assumed that the common events can be taken as data in the calculations. However, we usually have only testimonies for the common events. While this difference does not have a significant effect on the inductive part of the inference, it has a large influence on how one should view the reliability of testimonies. In this work, a full Bayesian solution is given for the more realistic case, where one has a large number of testimonies for a common event and one testimony for an uncommon event. It is seen that, in order for there to be a large amount of testimonies for a common event, the testimonies will probably be quite reliable. For this reason, because the testimonies are quite reliable based on the testimonies for the common events, the probability for the uncommon event, given a testimony for it, is also higher. Hence, one should be more open-minded when considering the plausibility of uncommon events. version:2
arxiv-1512-07962 | Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization | http://arxiv.org/abs/1512.07962 | id:1512.07962 author:Changyou Chen, David Carlson, Zhe Gan, Chunyuan Li, Lawrence Carin category:stat.ML cs.LG  published:2015-12-25 summary:Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms. version:2
arxiv-1310-1147 | A Primal Dual Active Set Algorithm for a Class of Nonconvex Sparsity Optimization | http://arxiv.org/abs/1310.1147 | id:1310.1147 author:Yuling Jiao, Bangti Jin, Xiliang Lu, Weina Ren category:math.OC stat.ML  published:2013-10-04 summary:In this paper, we consider the problem of recovering a sparse vector from noisy measurement data. Traditionally, it is formulated as a penalized least-squares problem with an $\ell^1$ penalty. Recent studies show that nonconvex penalties, e.g., $\ell^0$ and bridge, allow more effective sparse recovery. We develop an algorithm of primal dual active set type for a class of nonconvex sparsity-promoting penalties, which cover $\ell^0$, bridge, smoothly clipped absolute deviation, capped $\ell^1$ and minimax concavity penalty. First we establish the existence of a global minimizer for the class of optimization problems. Then we derive a novel necessary optimality condition for the global minimizer using the associated thresholding operator. The solutions to the optimality system are coordinate-wise minimizers, and under minor conditions, they are also local minimizers. Upon introducing the dual variable, the active set can be determined from the primal and dual variables. This relation lends itself to an iterative algorithm of active set type which at each step involves updating the primal variable only on the active set and then updating the dual variable explicitly. When combined with a continuation strategy on the regularization parameter, it has a global convergence property under the restricted isometry property. Extensive numerical experiments demonstrate its efficiency and accuracy. version:3
arxiv-1510-06646 | A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya Parameters and Topic Models | http://arxiv.org/abs/1510.06646 | id:1510.06646 author:Osama Khalifa, David Wolfe Corne, Mike Chantler category:cs.LG cs.CL stat.ML  published:2015-10-22 summary:Hyper-parameters play a major role in the learning and inference process of latent Dirichlet allocation (LDA). In order to begin the LDA latent variables learning process, these hyper-parameters values need to be pre-determined. We propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs Newton' (LDA-GN), which places non-informative priors over these hyper-parameters and uses Gibbs sampling to learn appropriate values for them. At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new technique for learning the parameters of multivariate Polya distributions. We report Gibbs-Newton performance results compared with two prominent existing approaches to the latter task: Minka's fixed-point iteration method and the Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with standard LDA in terms of the ability of the resulting topic models to generalize to unseen documents; (ii) by comparing it with standard LDA in its performance on a binary classification task. version:2
arxiv-1602-08581 | Content-based Video Indexing and Retrieval Using Corr-LDA | http://arxiv.org/abs/1602.08581 | id:1602.08581 author:Rahul Radhakrishnan Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush Ramsurat, Bhiksha Raj, Rita Singh category:cs.IR cs.CV  published:2016-02-27 summary:Existing video indexing and retrieval methods on popular web-based multimedia sharing websites are based on user-provided sparse tagging. This paper proposes a very specific way of searching for video clips, based on the content of the video. We present our work on Content-based Video Indexing and Retrieval using the Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilistic framework. This is a model that provides for auto-annotation of videos in a database with textual descriptors, and brings the added benefit of utilizing the semantic relations between the content of the video and text. We use the concept-level matching provided by corr-LDA to build correspondences between text and multimedia, with the objective of retrieving content with increased accuracy. In our experiments, we employ only the audio components of the individual recordings and compare our results with an SVM-based approach. version:1
arxiv-1602-08575 | Single-Image Superresolution Through Directional Representations | http://arxiv.org/abs/1602.08575 | id:1602.08575 author:Wojciech Czaja, James M. Murphy, Daniel Weinberg category:cs.CV  published:2016-02-27 summary:We develop a mathematically-motivated algorithm for image superresolution, based on the discrete shearlet transform. The shearlet transform is strongly directional, and is known to provide near-optimally sparse representations for a broad class of images. This often leads to superior performance in edge detection and image representation, when compared to other isotropic frames. We justify the use of shearlet frames for superresolution mathematically before presenting a superresolution algorithm that combines the shearlet transform with the sparse mixing estimators (SME) approach pioneered by Mallat and Yu. Our algorithm is compared with an isotropic superresolution method, a previous prototype of a shearlet superresolution algorithm, and SME superresolution with a discrete wavelet frame. Our numerical results on a variety of image types show strong performance in terms of PSNR. version:1
arxiv-1602-08574 | Graph clustering, variational image segmentation methods and Hough transform scale detection for object measurement in images | http://arxiv.org/abs/1602.08574 | id:1602.08574 author:Luca Calatroni, Yves van Gennip, Carola-Bibiane Schönlieb, Hannah Rowland, Arjuna Flenner category:math.AP cs.CV  published:2016-02-27 summary:We consider the problem of scale detection in images where a region of interest is present together with a measurement tool (e.g. a ruler). For the segmentation part, we focus on the graph based method by Flenner and Bertozzi which reinterprets classical continuous Ginzburg-Landau minimisation models in a totally discrete framework. To overcome the numerical difficulties due to the large size of the images considered we use matrix completion and splitting techniques. The scale on the measurement tool is detected via a Hough transform based algorithm. The method is then applied to some measurement tasks arising in real-world applications such as zoology, medicine and archaeology. version:1
arxiv-1602-08571 | Towards Neural Knowledge DNA | http://arxiv.org/abs/1602.08571 | id:1602.08571 author:Haoxi Zhang, Cesar Sanin, Edward Szczerbicki category:cs.AI cs.NE  published:2016-02-27 summary:In this paper, we propose the Neural Knowledge DNA, a framework that tailors the ideas underlying the success of neural networks to the scope of knowledge representation. Knowledge representation is a fundamental field that dedicate to representing information about the world in a form that computer systems can utilize to solve complex tasks. The proposed Neural Knowledge DNA is designed to support discovering, storing, reusing, improving, and sharing knowledge among machines and organisation. It is constructed in a similar fashion of how DNA formed: built up by four essential elements. As the DNA produces phenotypes, the Neural Knowledge DNA carries information and knowledge via its four essential elements, namely, Networks, Experiences, States, and Actions. version:1
arxiv-1509-03295 | Liberating language research from dogmas of the 20th century | http://arxiv.org/abs/1509.03295 | id:1509.03295 author:Ramon Ferrer-i-Cancho, Carlos Gómez-Rodríguez category:cs.CL cs.SI physics.soc-ph  published:2015-09-09 summary:A commentary on the article "Large-scale evidence of dependency length minimization in 37 languages" by Futrell, Mahowald & Gibson (PNAS 2015 112 (33) 10336-10341). version:3
arxiv-1602-08557 | Multiplier-less Artificial Neurons Exploiting Error Resiliency for Energy-Efficient Neural Computing | http://arxiv.org/abs/1602.08557 | id:1602.08557 author:Syed Shakib Sarwar, Swagath Venkataramani, Anand Raghunathan, Kaushik Roy category:cs.NE  published:2016-02-27 summary:Large-scale artificial neural networks have shown significant promise in addressing a wide range of classification and recognition applications. However, their large computational requirements stretch the capabilities of computing platforms. The fundamental components of these neural networks are the neurons and its synapses. The core of a digital hardware neuron consists of multiplier, accumulator and activation function. Multipliers consume most of the processing energy in the digital neurons, and thereby in the hardware implementations of artificial neural networks. We propose an approximate multiplier that utilizes the notion of computation sharing and exploits error resilience of neural network applications to achieve improved energy consumption. We also propose Multiplier-less Artificial Neuron (MAN) for even larger improvement in energy consumption and adapt the training process to ensure minimal degradation in accuracy. We evaluated the proposed design on 5 recognition applications. The results show, 35% and 60% reduction in energy consumption, for neuron sizes of 8 bits and 12 bits, respectively, with a maximum of ~2.83% loss in network accuracy, compared to a conventional neuron implementation. We also achieve 37% and 62% reduction in area for a neuron size of 8 bits and 12 bits, respectively, under iso-speed conditions. version:1
arxiv-1602-08556 | Significance Driven Hybrid 8T-6T SRAM for Energy-Efficient Synaptic Storage in Artificial Neural Networks | http://arxiv.org/abs/1602.08556 | id:1602.08556 author:Gopalakrishnan Srinivasan, Parami Wijesinghe, Syed Shakib Sarwar, Akhilesh Jaiswal, Kaushik Roy category:cs.NE  published:2016-02-27 summary:Multilayered artificial neural networks (ANN) have found widespread utility in classification and recognition applications. The scale and complexity of such networks together with the inadequacies of general purpose computing platforms have led to a significant interest in the development of efficient hardware implementations. In this work, we focus on designing energy efficient on-chip storage for the synaptic weights. In order to minimize the power consumption of typical digital CMOS implementations of such large-scale networks, the digital neurons could be operated reliably at scaled voltages by reducing the clock frequency. On the contrary, the on-chip synaptic storage designed using a conventional 6T SRAM is susceptible to bitcell failures at reduced voltages. However, the intrinsic error resiliency of NNs to small synaptic weight perturbations enables us to scale the operating voltage of the 6TSRAM. Our analysis on a widely used digit recognition dataset indicates that the voltage can be scaled by 200mV from the nominal operating voltage (950mV) for practically no loss (less than 0.5%) in accuracy (22nm predictive technology). Scaling beyond that causes substantial performance degradation owing to increased probability of failures in the MSBs of the synaptic weights. We, therefore propose a significance driven hybrid 8T-6T SRAM, wherein the sensitive MSBs are stored in 8T bitcells that are robust at scaled voltages due to decoupled read and write paths. In an effort to further minimize the area penalty, we present a synaptic-sensitivity driven hybrid memory architecture consisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulation framework shows that the proposed synaptic-sensitivity driven architecture provides a 30.91% reduction in the memory access power with a 10.41% area overhead, for less than 1% loss in the classification accuracy. version:1
arxiv-1511-05440 | Deep multi-scale video prediction beyond mean square error | http://arxiv.org/abs/1511.05440 | id:1511.05440 author:Michael Mathieu, Camille Couprie, Yann LeCun category:cs.LG cs.CV stat.ML  published:2015-11-17 summary:Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset version:6
arxiv-1602-08510 | Patch-Ordering as a Regularization for Inverse Problems in Image Processing | http://arxiv.org/abs/1602.08510 | id:1602.08510 author:Gregory Vaksman, Michael Zibulevsky, Michael Elad category:cs.CV 62H35  68U10  94A08  published:2016-02-26 summary:Recent work in image processing suggests that operating on (overlapping) patches in an image may lead to state-of-the-art results. This has been demonstrated for a variety of problems including denoising, inpainting, deblurring, and super-resolution. The work reported in [1,2] takes an extra step forward by showing that ordering these patches to form an approximate shortest path can be leveraged for better processing. The core idea is to apply a simple filter on the resulting 1D smoothed signal obtained after the patch-permutation. This idea has been also explored in combination with a wavelet pyramid, leading eventually to a sophisticated and highly effective regularizer for inverse problems in imaging. In this work we further study the patch-permutation concept, and harness it to propose a new simple yet effective regularization for image restoration problems. Our approach builds on the classic Maximum A'posteriori probability (MAP), with a penalty function consisting of a regular log-likelihood term and a novel permutation-based regularization term. Using a plain 1D Laplacian, the proposed regularization forces robust smoothness (L1) on the permuted pixels. Since the permutation originates from patch-ordering, we propose to accumulate the smoothness terms over all the patches' pixels. Furthermore, we take into account the found distances between adjacent patches in the ordering, by weighting the Laplacian outcome. We demonstrate the proposed scheme on a diverse set of problems: (i) severe Poisson image denoising, (ii) Gaussian image denoising, (iii) image deblurring, and (iv) single image super-resolution. In all these cases, we use recent methods that handle these problems as initialization to our scheme. This is followed by an L-BFGS optimization of the above-described penalty function, leading to state-of-the-art results, and especially so for highly ill-posed cases. version:1
arxiv-1506-02632 | Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control | http://arxiv.org/abs/1506.02632 | id:1506.02632 author:Prashanth L. A., Cheng Jie, Michael Fu, Steve Marcus, Csaba Szepesvári category:cs.LG math.OC  published:2015-06-08 summary:Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The RL setting presents two particular challenges when CPT is applied: estimating the CPT objective requires estimations of the entire distribution of the value function and finding a randomized optimal policy. The estimation scheme that we propose uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of a CPT-value optimization procedure that is based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA). We provide theoretical convergence guarantees for all the proposed algorithms and also illustrate the usefulness of CPT-based criteria in a traffic signal control application. version:3
arxiv-1511-08130 | A Roadmap towards Machine Intelligence | http://arxiv.org/abs/1511.08130 | id:1511.08130 author:Tomas Mikolov, Armand Joulin, Marco Baroni category:cs.AI cs.CL  published:2015-11-25 summary:The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment. version:2
arxiv-1506-03662 | Variance Reduced Stochastic Gradient Descent with Neighbors | http://arxiv.org/abs/1506.03662 | id:1506.03662 author:Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, Brian McWilliams category:cs.LG math.OC stat.ML 90C06  90C25  68T05 G.1.6; I.2.6  published:2015-06-11 summary:Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase. As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory. version:4
arxiv-1511-06426 | Reasoning in Vector Space: An Exploratory Study of Question Answering | http://arxiv.org/abs/1511.06426 | id:1511.06426 author:Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, Li Deng, Paul Smolensky category:cs.CL  published:2015-11-19 summary:Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms. version:4
arxiv-1602-08425 | Shape-aware Surface Reconstruction from Sparse Data | http://arxiv.org/abs/1602.08425 | id:1602.08425 author:Florian Bernard, Luis Salamanca, Johan Thunberg, Alexander Tack, Dennis Jentsch, Hans Lamecker, Stefan Zachow, Frank Hertel, Jorge Goncalves, Peter Gemmar category:cs.CV stat.ML  published:2016-02-26 summary:The reconstruction of an object's shape or surface from a set of 3D points is a common topic in materials and life sciences, computationally handled in computer graphics. Such points usually stem from optical or tactile 3D coordinate measuring equipment. Surface reconstruction also appears in medical image analysis, e.g. in anatomy reconstruction from tomographic measurements or the alignment of intra-operative navigation and preoperative planning data. In contrast to mere 3D point clouds, medical imaging yields contextual information on the 3D point data that can be used to adopt prior information on the shape that is to be reconstructed from the measurements. In this work we propose to use a statistical shape model (SSM) as a prior for surface reconstruction. The prior knowledge is represented by a point distribution model (PDM) that is associated with a surface mesh. Using the shape distribution that is modelled by the PDM, we reformulate the problem of surface reconstruction from a probabilistic perspective based on a Gaussian Mixture Model (GMM). In order to do so, the given measurements are interpreted as samples of the GMM. By using mixture components with anisotropic covariances that are oriented according to the surface normals at the PDM points, a surface-based fitting is accomplished. By estimating the parameters of the GMM in a maximum a posteriori manner, the reconstruction of the surface from the given measurements is achieved. Extensive experiments suggest that our proposed approach leads to superior surface reconstructions compared to Iterative Closest Point (ICP) methods. version:1
arxiv-1602-08418 | Multivariate Hawkes Processes for Large-scale Inference | http://arxiv.org/abs/1602.08418 | id:1602.08418 author:Rémi Lemonnier, Kevin Scaman, Argyris Kalogeratos category:stat.ML  published:2016-02-26 summary:In this paper, we present a framework for fitting multivariate Hawkes processes for large-scale problems both in the number of events in the observed history $n$ and the number of event types $d$ (i.e. dimensions). The proposed Low-Rank Hawkes Process (LRHP) framework introduces a low-rank approximation of the kernel matrix that allows to perform the nonparametric learning of the $d^2$ triggering kernels using at most $O(ndr^2)$ operations, where $r$ is the rank of the approximation ($r \ll d,n$). This comes as a major improvement to the existing state-of-the-art inference algorithms that are in $O(nd^2)$. Furthermore, the low-rank approximation allows LRHP to learn representative patterns of interaction between event types, which may be valuable for the analysis of such complex processes in real world datasets. The efficiency and scalability of our approach is illustrated with numerical experiments on simulated as well as real datasets. version:1
arxiv-1602-07783 | Top-N Recommendation with Novel Rank Approximation | http://arxiv.org/abs/1602.07783 | id:1602.07783 author:Zhao Kang, Qiang Cheng category:cs.IR cs.AI stat.ML  published:2016-02-25 summary:The importance of accurate recommender systems has been widely recognized by academia and industry. However, the recommendation quality is still rather low. Recently, a linear sparse and low-rank representation of the user-item matrix has been applied to produce Top-N recommendations. This approach uses the nuclear norm as a convex relaxation for the rank function and has achieved better recommendation accuracy than the state-of-the-art methods. In the past several years, solving rank minimization problems by leveraging nonconvex relaxations has received increasing attention. Some empirical results demonstrate that it can provide a better approximation to original problems than convex relaxation. In this paper, we propose a novel rank approximation to enhance the performance of Top-N recommendation systems, where the approximation error is controllable. Experimental results on real data show that the proposed rank approximation improves the Top-$N$ recommendation accuracy substantially. version:2
arxiv-1411-0560 | Multivariate response and parsimony for Gaussian cluster-weighted models | http://arxiv.org/abs/1411.0560 | id:1411.0560 author:Utkarsh J. Dang, Antonio Punzo, Paul D. McNicholas, Salvatore Ingrassia, Ryan P. Browne category:stat.CO stat.ME stat.ML  published:2014-11-03 summary:A family of parsimonious Gaussian cluster-weighted models is presented. This family concerns a multivariate extension to cluster-weighted modelling that can account for correlations between multivariate responses. Parsimony is attained by constraining parts of an eigen-decomposition imposed on the component covariance matrices. A sufficient condition for identifiability is provided and an expectation-maximization algorithm is presented for parameter estimation. Model performance is investigated on both synthetic and classical real data sets and compared with some popular approaches. Finally, accounting for linear dependencies in the presence of a linear regression structure is shown to offer better performance, vis-\`{a}-vis clustering, over existing methodologies. version:2
arxiv-1602-08350 | Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets | http://arxiv.org/abs/1602.08350 | id:1602.08350 author:Patrick O. Glauner, Andre Boechat, Lautaro Dolberg, Radu State, Franck Bettinger, Yves Rangoni, Diogo Duarte category:cs.LG cs.AI  published:2016-02-26 summary:Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets. version:1
arxiv-1602-08325 | Victory Sign Biometric for Terrorists Identification | http://arxiv.org/abs/1602.08325 | id:1602.08325 author:Ahmad B. A. Hassanat, Mahmoud B. Alhasanat, Mohammad Ali Abbadi, Eman Btoush, Mouhammd Al-Awadi, Ahmad S. Tarawneh category:cs.CV  published:2016-02-26 summary:Covering the face and all body parts, sometimes the only evidence to identify a person is their hand geometry, and not the whole hand- only two fingers (the index and the middle fingers) while showing the victory sign, as seen in many terrorists videos. This paper investigates for the first time a new way to identify persons, particularly (terrorists) from their victory sign. We have created a new database in this regard using a mobile phone camera, imaging the victory signs of 50 different persons over two sessions. Simple measurements for the fingers, in addition to the Hu Moments for the areas of the fingers were used to extract the geometric features of the shown part of the hand shown after segmentation. The experimental results using the KNN classifier were encouraging for most of the recorded persons; with about 40% to 93% total identification accuracy, depending on the features, distance metric and K used. version:1
arxiv-1602-08323 | Deep Spiking Networks | http://arxiv.org/abs/1602.08323 | id:1602.08323 author:Peter O'Connor, Max Welling category:cs.NE  published:2016-02-26 summary:We introduce the Spiking Multi-Layer Perceptron (SMLP). The SMLP is a spiking version of a conventional Multi-Layer Perceptron with rectified-linear units. Our architecture is event-based, meaning that neurons in the network communicate by sending "events" to downstream neurons, and that the state of each neuron is only updated when it receives an event. We show that the SMLP behaves identically, during both prediction and training, to a conventional deep network of rectified-linear units in the limiting case where we run the spiking network for a long time. We apply this architecture to a conventional classification problem (MNIST) and achieve performance very close to that of a conventional MLP with the same architecture. Our network is a natural architecture for learning based on streaming event-based data, and has potential applications in robotic systems systems, which require low power and low response latency. version:1
arxiv-1602-08313 | Enhancing Genetic Algorithms using Multi Mutations | http://arxiv.org/abs/1602.08313 | id:1602.08313 author:Ahmad B. A. Hassanat, Esra'a Alkafaween, Nedal A. Al-Nawaiseh, Mohammad A. Abbadi, Mouhammd Alkasassbeh, Mahmoud B. Alhasanat category:cs.AI cs.NE  published:2016-02-26 summary:Mutation is one of the most important stages of the genetic algorithm because of its impact on the exploration of global optima, and to overcome premature convergence. There are many types of mutation, and the problem lies in selection of the appropriate type, where the decision becomes more difficult and needs more trial and error. This paper investigates the use of more than one mutation operator to enhance the performance of genetic algorithms. Novel mutation operators are proposed, in addition to two selection strategies for the mutation operators, one of which is based on selecting the best mutation operator and the other randomly selects any operator. Several experiments on some Travelling Salesman Problems (TSP) were conducted to evaluate the proposed methods, and these were compared to the well-known exchange mutation and rearrangement mutation. The results show the importance of some of the proposed methods, in addition to the significant enhancement of the genetic algorithm's performance, particularly when using more than one mutation operator. version:1
arxiv-1602-08254 | Theoretical Analysis of the $k$-Means Algorithm - A Survey | http://arxiv.org/abs/1602.08254 | id:1602.08254 author:Johannes Blömer, Christiane Lammersen, Melanie Schmidt, Christian Sohler category:cs.DS cs.LG  published:2016-02-26 summary:The $k$-means algorithm is one of the most widely used clustering heuristics. Despite its simplicity, analyzing its running time and quality of approximation is surprisingly difficult and can lead to deep insights that can be used to improve the algorithm. In this paper we survey the recent results in this direction as well as several extension of the basic $k$-means method. version:1
arxiv-1602-08225 | Multimodal Emotion Recognition Using Multimodal Deep Learning | http://arxiv.org/abs/1602.08225 | id:1602.08225 author:Wei Liu, Wei-Long Zheng, Bao-Liang Lu category:cs.HC cs.CV cs.LG  published:2016-02-26 summary:To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-the-art approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa. version:1
arxiv-1602-08207 | Learning and Free Energy in Expectation Consistent Approximate Inference | http://arxiv.org/abs/1602.08207 | id:1602.08207 author:Alyson K. Fletcher category:cs.IT math.IT stat.ML  published:2016-02-26 summary:Approximations of loopy belief propagation are commonly combined with expectation-maximization (EM) for probabilistic inference problems when the densities have unknown parameters. This work considers an approximate EM learning method combined with Opper and Winther's Expectation Consistent Approximate Inference method. The combined algorithm is called EM-EC and is shown to have a simple variational free energy interpretation. In addition, the algorithm can provide a computationally efficient and general approach to a number of learning problems with hidden states including empirical Bayesian forms of regression, classification, compressed sensing, and sparse Bayesian learning. Systems with linear dynamics interconnected with non-Gaussian or nonlinear components can also be easily considered. version:1
arxiv-1510-03009 | Neural Networks with Few Multiplications | http://arxiv.org/abs/1510.03009 | id:1510.03009 author:Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, Yoshua Bengio category:cs.LG cs.NE  published:2015-10-11 summary:For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks. version:3
arxiv-1602-08194 | Scalable and Sustainable Deep Learning via Randomized Hashing | http://arxiv.org/abs/1602.08194 | id:1602.08194 author:Ryan Spring, Anshumali Shrivastava category:stat.ML cs.LG cs.NE  published:2016-02-26 summary:Current deep learning architectures are growing larger in order to learn from enormous datasets.These architectures require giant matrix multiplication operations to train millions or billions of parameters during forward and back propagation steps. These operations are very expensive from a computational and energy standpoint. We present a novel technique to reduce the amount of computation needed to train and test deep net-works drastically. Our approach combines recent ideas from adaptive dropouts and randomized hashing for maximum inner product search to select only the nodes with the highest activation efficiently. Our new algorithm for training deep networks reduces the overall computational cost,of both feed-forward pass and backpropagation,by operating on significantly fewer nodes. As a consequence, our algorithm only requires 5% of computations (multiplications) compared to traditional algorithms, without any loss in the accuracy. Furthermore, due to very sparse gradient updates, our algorithm is ideally suited for asynchronous training leading to near linear speedup with increasing parallelism. We demonstrate the scalability and sustainability (energy efficiency) of our proposed algorithm via rigorous experimental evaluations. version:1
arxiv-1601-04738 | Sub-Sampled Newton Methods II: Local Convergence Rates | http://arxiv.org/abs/1601.04738 | id:1601.04738 author:Farbod Roosta-Khorasani, Michael W. Mahoney category:math.OC cs.LG stat.ML  published:2016-01-18 summary:Many data-fitting applications require the solution of an optimization problem involving a sum of large number of functions of high dimensional parameter. Here, we consider the problem of minimizing a sum of $n$ functions over a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both $n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$ can offer great amount of computational efficiency. Within the context of second order methods, we first give quantitative local convergence results for variants of Newton's method where the Hessian is uniformly sub-sampled. Using random matrix concentration inequalities, one can sub-sample in a way that the curvature information is preserved. Using such sub-sampling strategy, we establish locally Q-linear and Q-superlinear convergence rates. We also give additional convergence results for when the sub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type regularization. Finally, in addition to Hessian sub-sampling, we consider sub-sampling the gradient as way to further reduce the computational complexity per iteration. We use approximate matrix multiplication results from randomized numerical linear algebra (RandNLA) to obtain the proper sampling strategy and we establish locally R-linear convergence rates. In such a setting, we also show that a very aggressive sample size increase results in a R-superlinearly convergent algorithm. While the sample size depends on the condition number of the problem, our convergence rates are problem-independent, i.e., they do not depend on the quantities related to the problem. Hence, our analysis here can be used to complement the results of our basic framework from the companion paper, [38], by exploring algorithmic trade-offs that are important in practice. version:3
arxiv-1601-04737 | Sub-Sampled Newton Methods I: Globally Convergent Algorithms | http://arxiv.org/abs/1601.04737 | id:1601.04737 author:Farbod Roosta-Khorasani, Michael W. Mahoney category:math.OC cs.LG stat.ML  published:2016-01-18 summary:Large scale optimization problems are ubiquitous in machine learning and data analysis and there is a plethora of algorithms for solving such problems. Many of these algorithms employ sub-sampling, as a way to either speed up the computations and/or to implicitly implement a form of statistical regularization. In this paper, we consider second-order iterative optimization algorithms and we provide bounds on the convergence of the variants of Newton's method that incorporate uniform sub-sampling as a means to estimate the gradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our algorithms are global and are guaranteed to converge from any initial iterate. Using random matrix concentration inequalities, one can sub-sample the Hessian to preserve the curvature information. Our first algorithm incorporates Hessian sub-sampling while using the full gradient. We also give additional convergence results for when the sub-sampled Hessian is regularized by modifying its spectrum or ridge-type regularization. Next, in addition to Hessian sub-sampling, we also consider sub-sampling the gradient as a way to further reduce the computational complexity per iteration. We use approximate matrix multiplication results from randomized numerical linear algebra to obtain the proper sampling strategy. In all these algorithms, computing the update boils down to solving a large scale linear system, which can be computationally expensive. As a remedy, for all of our algorithms, we also give global convergence results for the case of inexact updates where such linear system is solved only approximately. This paper has a more advanced companion paper, [42], in which we demonstrate that, by doing a finer-grained analysis, we can get problem-independent bounds for local convergence of these algorithms and explore trade-offs to improve upon the basic results of the present paper. version:3
arxiv-1602-08186 | Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn | http://arxiv.org/abs/1602.08186 | id:1602.08186 author:Viet Ha-Thuc, Ye Xu, Satya Pradeep Kanduri, Xianren Wu, Vijay Dialani, Yan Yan, Abhishek Gupta, Shakti Sinha category:cs.IR cs.LG  published:2016-02-26 summary:One key challenge in talent search is how to translate complex criteria of a hiring position into a search query. This typically requires deep knowledge on which skills are typically needed for the position, what are their alternatives, which companies are likely to have such candidates, etc. However, listing examples of suitable candidates for a given position is a relatively easy job. Therefore, in order to help searchers overcome this challenge, we design a next generation of talent search paradigm at LinkedIn: Search by Ideal Candidates. This new system only needs the searcher to input one or several examples of suitable candidates for the position. The system will generate a query based on the input candidates and then retrieve and rank results based on the query as well as the input candidates. The query is also shown to the searcher to make the system transparent and to allow the searcher to interact with it. As the searcher modifies the initial query and makes it deviate from the ideal candidates, the search ranking function dynamically adjusts an refreshes the ranking results balancing between the roles of query and ideal candidates. As of writing this paper, the new system is being launched to our customers. version:1
arxiv-1511-03643 | Unifying distillation and privileged information | http://arxiv.org/abs/1511.03643 | id:1511.03643 author:David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, Vladimir Vapnik category:stat.ML cs.LG  published:2015-11-11 summary:Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data. version:3
arxiv-1510-00452 | Optimal Binary Classifier Aggregation for General Losses | http://arxiv.org/abs/1510.00452 | id:1510.00452 author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML  published:2015-10-01 summary:We address the problem of aggregating an ensemble of binary classifiers in a semi-supervised setting. Recently, this problem was solved optimally using a game-theoretic approach, but that analysis was specific to the 0-1 loss. In this paper, we generalize the minimax optimal algorithm of the previous work to a very general, novel class of loss functions, including but not limited to all convex surrogates, while extending its performance and efficiency guarantees. The result is a family of parameter-free ensemble aggregation algorithms which use labeled and unla- beled data; these are as efficient as linear learning and prediction for convex risk minimization, but work without any relaxations on many non-convex loss functions. The prediction algorithms take a form familiar in decision theory, applying sigmoid functions to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning. version:4
arxiv-1602-08159 | Harnessing disordered quantum dynamics for machine learning | http://arxiv.org/abs/1602.08159 | id:1602.08159 author:Keisuke Fujii, Kohei Nakajima category:quant-ph cs.AI cs.LG cs.NE nlin.CD  published:2016-02-26 summary:Quantum computer has an amazing potential of fast information processing. However, realisation of a digital quantum computer is still a challenging problem requiring highly accurate controls and key application strategies. Here we propose a novel platform, quantum reservoir computing, to solve these issues successfully by exploiting natural quantum dynamics, which is ubiquitous in laboratories nowadays, for machine learning. In this framework, nonlinear dynamics including classical chaos can be universally emulated in quantum systems. A number of numerical experiments show that quantum systems consisting of at most seven qubits possess computational capabilities comparable to conventional recurrent neural networks of 500 nodes. This discovery opens up a new paradigm for information processing with artificial intelligence powered by quantum physics. version:1
arxiv-1505-04732 | Layered Adaptive Importance Sampling | http://arxiv.org/abs/1505.04732 | id:1505.04732 author:L. Martino, V. Elvira, D. Luengo, J. Corander category:stat.CO cs.LG stat.ML  published:2015-05-18 summary:Monte Carlo methods represent the "de facto" standard for approximating complicated integrals involving multidimensional target distributions. In order to generate random realizations from the target distribution, Monte Carlo techniques use simpler proposal probability densities to draw candidate samples. The performance of any such method is strictly related to the specification of the proposal distribution, such that unfortunate choices easily wreak havoc on the resulting estimators. In this work, we introduce a layered (i.e., hierarchical) procedure to generate samples employed within a Monte Carlo scheme. This approach ensures that an appropriate equivalent proposal density is always obtained automatically (thus eliminating the risk of a catastrophic performance), although at the expense of a moderate increase in the complexity. Furthermore, we provide a general unified importance sampling (IS) framework, where multiple proposal densities are employed and several IS schemes are introduced by applying the so-called deterministic mixture approach. Finally, given these schemes, we also propose a novel class of adaptive importance samplers using a population of proposals, where the adaptation is driven by independent parallel or interacting Markov Chain Monte Carlo (MCMC) chains. The resulting algorithms efficiently combine the benefits of both IS and MCMC methods. version:3
arxiv-1602-08151 | Learning to Abstain from Binary Prediction | http://arxiv.org/abs/1602.08151 | id:1602.08151 author:Akshay Balsubramani category:cs.LG stat.ML  published:2016-02-25 summary:We address how to learn a binary classifier capable of abstaining from making a label prediction. Such a classifier hopes to abstain where it would be most inaccurate if forced to predict, so it has two goals in tension with each other: minimizing errors, and avoiding abstaining unnecessarily often. In this work, we exactly characterize the best achievable tradeoff between these two goals in a general semi-supervised setting, given an ensemble of classifiers of varying competence as well as unlabeled data on which we wish to predict or abstain. We give an algorithm for learning a classifier which trades off its errors with abstentions in a minimax optimal manner. This algorithm is as efficient as linear learning and prediction, and comes with strong and robust theoretical guarantees. Our analysis extends to a large class of loss functions and other scenarios, including ensembles comprised of "specialist" classifiers that can themselves abstain. version:1
arxiv-1602-08141 | Autonomous navigation for low-altitude UAVs in urban areas | http://arxiv.org/abs/1602.08141 | id:1602.08141 author:Thomas Castelli, Aidean Sharghi, Don Harper, Alain Tremeau, Mubarak Shah category:cs.RO cs.CV  published:2016-02-25 summary:In recent years, consumer Unmanned Aerial Vehicles have become very popular, everyone can buy and fly a drone without previous experience, which raises concern in regards to regulations and public safety. In this paper, we present a novel approach towards enabling safe operation of such vehicles in urban areas. Our method uses geodetically accurate dataset images with Geographical Information System (GIS) data of road networks and buildings provided by Google Maps, to compute a weighted A* shortest path from start to end locations of a mission. Weights represent the potential risk of injuries for individuals in all categories of land-use, i.e. flying over buildings is considered safer than above roads. We enable safe UAV operation in regards to 1- land-use by computing a static global path dependent on environmental structures, and 2- avoiding flying over moving objects such as cars and pedestrians by dynamically optimizing the path locally during the flight. As all input sources are first geo-registered, pixels and GPS coordinates are equivalent, it therefore allows us to generate an automated and user-friendly mission with GPS waypoints readable by consumer drones' autopilots. We simulated 54 missions and show significant improvement in maximizing UAV's standoff distance to moving objects with a quantified safety parameter over 40 times better than the naive straight line navigation. version:1
arxiv-1602-08132 | Adaptive Frequency Cepstral Coefficients for Word Mispronunciation Detection | http://arxiv.org/abs/1602.08132 | id:1602.08132 author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.CV  published:2016-02-25 summary:Systems based on automatic speech recognition (ASR) technology can provide important functionality in computer assisted language learning applications. This is a young but growing area of research motivated by the large number of students studying foreign languages. Here we propose a Hidden Markov Model (HMM)-based method to detect mispronunciations. Exploiting the specific dialog scripting employed in language learning software, HMMs are trained for different pronunciations. New adaptive features have been developed and obtained through an adaptive warping of the frequency scale prior to computing the cepstral coefficients. The optimization criterion used for the warping function is to maximize separation of two major groups of pronunciations (native and non-native) in terms of classification rate. Experimental results show that the adaptive frequency scale yields a better coefficient representation leading to higher classification rates in comparison with conventional HMMs using Mel-frequency cepstral coefficients. version:1
arxiv-1602-08128 | PCA Method for Automated Detection of Mispronounced Words | http://arxiv.org/abs/1602.08128 | id:1602.08128 author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.CL cs.LG  published:2016-02-25 summary:This paper presents a method for detecting mispronunciations with the aim of improving Computer Assisted Language Learning (CALL) tools used by foreign language learners. The algorithm is based on Principle Component Analysis (PCA). It is hierarchical with each successive step refining the estimate to classify the test word as being either mispronounced or correct. Preprocessing before detection, like normalization and time-scale modification, is implemented to guarantee uniformity of the feature vectors input to the detection system. The performance using various features including spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated. Best results were obtained using MFCCs, achieving up to 99% accuracy in word verification and 93% in native/non-native classification. Compared with Hidden Markov Models (HMMs) which are used pervasively in recognition application, this particular approach is computational efficient and effective when training data is limited. version:1
arxiv-1602-08118 | Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network | http://arxiv.org/abs/1602.08118 | id:1602.08118 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2016-02-25 summary:Recurrent neural networks (RNN) are capable of learning to encode and exploit activation history over an arbitrary timescale. However, in practice, state of the art gradient descent based training methods are known to suffer from difficulties in learning long term dependencies. Here, we describe a novel training method that involves concurrent parallel cloned networks, each sharing the same weights, each trained at different stimulus phase and each maintaining independent activation histories. Training proceeds by recursively performing batch-updates over the parallel clones as activation history is progressively increased. This allows conflicts to propagate hierarchically from short-term contexts towards longer-term contexts until they are resolved. We illustrate the parallel clones method and hierarchical conflict propagation with a character-level deep RNN tasked with memorizing a paragraph of Moby Dick (by Herman Melville). version:1
arxiv-1511-05176 | MuProp: Unbiased Backpropagation for Stochastic Neural Networks | http://arxiv.org/abs/1511.05176 | id:1511.05176 author:Shixiang Gu, Sergey Levine, Ilya Sutskever, Andriy Mnih category:cs.LG  published:2015-11-16 summary:Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks. version:3
arxiv-1602-08045 | PCA/LDA Approach for Text-Independent Speaker Recognition | http://arxiv.org/abs/1602.08045 | id:1602.08045 author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.LG  published:2016-02-25 summary:Various algorithms for text-independent speaker recognition have been developed through the decades, aiming to improve both accuracy and e?ciency. This paper presents a novel PCA/LDA-based approach that is faster than traditional statistical model-based methods and achieves competitive results. First, the performance based on only PCA and only LDA is measured; then a mixed model, taking advantages of both methods, is introduced. A subset of the TIMIT corpus composed of 200 male speakers, is used for enrollment, validation and testing. The best results achieve 100%; 96% and 95% classi?cation rate at population level 50; 100 and 200, using 39-dimensional MFCC features with delta and double delta. These results are based on 12-second text-independent speech for training and 4-second data for test. These are comparable to the conventional MFCC-GMM methods, but require signi?cantly less time to train and operate. version:1
arxiv-1511-05634 | Local entropy as a measure for sampling solutions in Constraint Satisfaction Problems | http://arxiv.org/abs/1511.05634 | id:1511.05634 author:Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:cond-mat.dis-nn stat.ML G.1.6; I.2.M  published:2015-11-18 summary:We introduce a novel Entropy-driven Monte Carlo (EdMC) strategy to efficiently sample solutions of random Constraint Satisfaction Problems (CSPs). First, we extend a recent result that, using a large-deviation analysis, shows that the geometry of the space of solutions of the Binary Perceptron Learning Problem (a prototypical CSP), contains regions of very high-density of solutions. Despite being sub-dominant, these regions can be found by optimizing a local entropy measure. Building on these results, we construct a fast solver that relies exclusively on a local entropy estimate, and can be applied to general CSPs. We describe its performance not only for the Perceptron Learning Problem but also for the random $K$-Satisfiabilty Problem (another prototypical CSP with a radically different structure), and show numerically that a simple zero-temperature Metropolis search in the smooth local entropy landscape can reach sub-dominant clusters of optimal solutions in a small number of steps, while standard Simulated Annealing either requires extremely long cooling procedures or just fails. We also discuss how the EdMC can heuristically be made even more efficient for the cases we studied. version:2
arxiv-1602-08017 | Meta-learning within Projective Simulation | http://arxiv.org/abs/1602.08017 | id:1602.08017 author:Adi Makmal, Alexey A. Melnikov, Vedran Dunjko, Hans J. Briegel category:cs.AI cs.LG stat.ML  published:2016-02-25 summary:Learning models of artificial intelligence can nowadays perform very well on a large variety of tasks. However, in practice different task environments are best handled by different learning models, rather than a single, universal, approach. Most non-trivial models thus require the adjustment of several to many learning parameters, which is often done on a case-by-case basis by an external party. Meta-learning refers to the ability of an agent to autonomously and dynamically adjust its own learning parameters, or meta-parameters. In this work we show how projective simulation, a recently developed model of artificial intelligence, can naturally be extended to account for meta-learning in reinforcement learning settings. The projective simulation approach is based on a random walk process over a network of clips. The suggested meta-learning scheme builds upon the same design and employs clip networks to monitor the agent's performance and to adjust its meta-parameters "on the fly". We distinguish between "reflexive adaptation" and "adaptation through learning", and show the utility of both approaches. In addition, a trade-off between flexibility and learning-time is addressed. The extended model is examined on three different kinds of reinforcement learning tasks, in which the agent has different optimal values of the meta-parameters, and is shown to perform well, reaching near-optimal to optimal success rates in all of them, without ever needing to manually adjust any meta-parameter. version:1
arxiv-1511-05952 | Prioritized Experience Replay | http://arxiv.org/abs/1511.05952 | id:1511.05952 author:Tom Schaul, John Quan, Ioannis Antonoglou, David Silver category:cs.LG  published:2015-11-18 summary:Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games. version:4
arxiv-1602-08007 | Practical Riemannian Neural Networks | http://arxiv.org/abs/1602.08007 | id:1602.08007 author:Gaétan Marceau-Caron, Yann Ollivier category:cs.NE cs.LG stat.ML  published:2016-02-25 summary:We provide the first experimental results on non-synthetic datasets for the quasi-diagonal Riemannian gradient descents for neural networks introduced in [Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a previously unpublished electroencephalogram dataset. The quasi-diagonal Riemannian algorithms consistently beat simple stochastic gradient gradient descents by a varying margin. The computational overhead with respect to simple backpropagation is around a factor $2$. Perhaps more interestingly, these methods also reach their final performance quickly, thus requiring fewer training epochs and a smaller total computation time. We also present an implementation guide to these Riemannian gradient descents for neural networks, showing how the quasi-diagonal versions can be implemented with minimal effort on top of existing routines which compute gradients. version:1
arxiv-1602-07337 | Sparse Estimation of Multivariate Poisson Log-Normal Model and Inverse Covariance for Count Data | http://arxiv.org/abs/1602.07337 | id:1602.07337 author:Hao Wu, Xinwei Deng, Naren Ramakrishnan category:stat.ME cs.LG  published:2016-02-22 summary:Modeling data with multivariate count responses is a challenge problem due to the discrete nature of the responses. Such data are commonly observed in many applications such as biology, epidemiology and social studies. The existing methods for univariate count response cannot be easily extended to the multivariate situation since the dependency among multiple responses needs to be properly accommodated. In this paper, we propose a multivariate Poisson Log-Normal regression model for multivariate data with count responses. An efficient Monte Carlo EM algorithm is also developed to facilitate the model estimation. By simultaneously estimating the regression coefficients and inverse covariance matrix over the latent variables, the proposed regression model takes advantages of association among multiple count responses to improve the model prediction performance. Simulation studies are conducted to systematically evaluate the performance of the proposed method in comparison with conventional methods. The proposed method is further used to analyze a real influenza-like illness data, showing accurate prediction and forecasting with meaningful interpretation. version:2
arxiv-1602-07985 | How effective can simple ordinal peer grading be? | http://arxiv.org/abs/1602.07985 | id:1602.07985 author:Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris category:cs.AI cs.DS cs.LG  published:2016-02-25 summary:Ordinal peer grading has been proposed as a simple and scalable solution for computing reliable information about student performance in massive open online courses. The idea is to outsource the grading task to the students themselves as follows. After the end of an exam, each student is asked to rank ---in terms of quality--- a bundle of exam papers by fellow students. An aggregation rule will then combine the individual rankings into a global one that contains all students. We define a broad class of simple aggregation rules and present a theoretical framework for assessing their effectiveness. When statistical information about the grading behaviour of students is available, the framework can be used to compute the optimal rule from this class with respect to a series of performance objectives. For example, a natural rule known as Borda is proved to be optimal when students grade correctly. In addition, we present extensive simulations and a field experiment that validate our theory and prove it to be extremely accurate in predicting the performance of aggregation rules even when only rough information about grading behaviour is available. version:1
arxiv-1511-06306 | Robust Convolutional Neural Networks under Adversarial Noise | http://arxiv.org/abs/1511.06306 | id:1511.06306 author:Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello category:cs.LG cs.CV  published:2015-11-19 summary:Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called "adversarial examples". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise. version:2
arxiv-1508-06163 | Accurate Urban Road Centerline Extraction from VHR Imagery via Multiscale Segmentation and Tensor Voting | http://arxiv.org/abs/1508.06163 | id:1508.06163 author:Guangliang Cheng, Feiyun Zhu, Shiming Xiang, Chunhong Pan category:cs.CV  published:2015-08-25 summary:It is very useful and increasingly popular to extract accurate road centerlines from very-high-resolution (VHR) re- mote sensing imagery for various applications, such as road map generation and updating etc. There are three shortcomings of current methods: (a) Due to the noise and occlusions (owing to vehicles and trees), most road extraction methods bring in heterogeneous classification results; (b) Morphological thinning algorithm is widely used to extract road centerlines, while it pro- duces small spurs around the centerlines; (c) Many methods are ineffective to extract centerlines around the road intersections. To address the above three issues, we propose a novel method to ex- tract smooth and complete road centerlines via three techniques: the multiscale joint collaborative representation (MJCR) & graph cuts (GC), tensor voting (TV) & non-maximum suppression (NMS) and fitting based connection algorithm. Specifically, a MJCR-GC based road area segmentation method is proposed by incorporating mutiscale features and spatial information. In this way, a homogenous road segmentation result is achieved. Then, to obtain a smooth and correct road centerline network, a TV-NMS based centerline extraction method is introduced. This method not only extracts smooth road centerlines, but also connects the discontinuous road centerlines. Finally, to overcome the ineffectiveness of current methods in the road intersection, a fitting based road centerline connection algorithm is proposed. As a result, we can get a complete road centerline network. Extensive experiments on two datasets demonstrate that our method achieves higher quantitative results, as well as more satisfactory visual performances by comparing with state-of-the- art methods. version:2
arxiv-1509-04186 | Expanded Parts Model for Semantic Description of Humans in Still Images | http://arxiv.org/abs/1509.04186 | id:1509.04186 author:Gaurav Sharma, Frederic Jurie, Cordelia Schmid category:cs.CV  published:2015-09-14 summary:We introduce an Expanded Parts Model (EPM) for recognizing human attributes (e.g. young, short hair, wearing suit) and actions (e.g. running, jumping) in still images. An EPM is a collection of part templates which are learnt discriminatively to explain specific scale-space regions in the images (in human centric coordinates). This is in contrast to current models which consist of a relatively few (i.e. a mixture of) 'average' templates. EPM uses only a subset of the parts to score an image and scores the image sparsely in space, i.e. it ignores redundant and random background in an image. To learn our model, we propose an algorithm which automatically mines parts and learns corresponding discriminative templates together with their respective locations from a large number of candidate parts. We validate our method on three recent challenging datasets of human attributes and actions. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets. version:2
arxiv-1602-07884 | Firefly Algorithm for optimization problems with non-continuous variables: A Review and Analysis | http://arxiv.org/abs/1602.07884 | id:1602.07884 author:Surafel Luleseged Tilahun, Jean Medard T Ngnotchouye category:cs.NE  published:2016-02-25 summary:Firefly algorithm is a swarm based metaheuristic algorithm inspired by the flashing behavior of fireflies. It is an effective and an easy to implement algorithm. It has been tested on different problems from different disciplines and found to be effective. Even though the algorithm is proposed for optimization problems with continuous variables, it has been modified and used for problems with non-continuous variables, including binary and integer valued problems. In this paper a detailed review of this modifications of firefly algorithm for problems with non-continuous variables will be discussed. The strength and weakness of the modifications along with possible future works will be presented. version:1
arxiv-1602-07873 | CNN for License Plate Motion Deblurring | http://arxiv.org/abs/1602.07873 | id:1602.07873 author:Pavel Svoboda, Michal Hradis, Lukas Marsik, Pavel Zemcik category:cs.CV  published:2016-02-25 summary:In this work we explore the previously proposed approach of direct blind deconvolution and denoising with convolutional neural networks in a situation where the blur kernels are partially constrained. We focus on blurred images from a real-life traffic surveillance system, on which we, for the first time, demonstrate that neural networks trained on artificial data provide superior reconstruction quality on real images compared to traditional blind deconvolution methods. The training data is easy to obtain by blurring sharp photos from a target system with a very rough approximation of the expected blur kernels, thereby allowing custom CNNs to be trained for a specific application (image content and blur range). Additionally, we evaluate the behavior and limits of the CNNs with respect to blur direction range and length. version:1
arxiv-1602-07865 | Projected Estimators for Robust Semi-supervised Classification | http://arxiv.org/abs/1602.07865 | id:1602.07865 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2016-02-25 summary:For semi-supervised techniques to be applied safely in practice we at least want methods to outperform their supervised counterparts. We study this question for classification using the well-known quadratic surrogate loss function. Using a projection of the supervised estimate onto a set of constraints imposed by the unlabeled data, we find we can safely improve over the supervised solution in terms of this quadratic loss. Unlike other approaches to semi-supervised learning, the procedure does not rely on assumptions that are not intrinsic to the classifier at hand. It is theoretically demonstrated that, measured on the labeled and unlabeled training data, this semi-supervised procedure never gives a lower quadratic loss than the supervised alternative. To our knowledge this is the first approach that offers such strong, albeit conservative, guarantees for improvement over the supervised solution. The characteristics of our approach are explicated using benchmark datasets to further understand the similarities and differences between the quadratic loss criterion used in the theoretical results and the classification accuracy often considered in practice. version:1
arxiv-1602-07863 | Learning Gaussian Graphical Models With Fractional Marginal Pseudo-likelihood | http://arxiv.org/abs/1602.07863 | id:1602.07863 author:Janne Leppä-aho, Johan Pensar, Teemu Roos, Jukka Corander category:stat.ML cs.LG  published:2016-02-25 summary:We propose a Bayesian approximate inference method for learning the dependence structure of a Gaussian graphical model. Using pseudo-likelihood, we derive an analytical expression to approximate the marginal likelihood for an arbitrary graph structure without invoking any assumptions about decomposability. The majority of the existing methods for learning Gaussian graphical models are either restricted to decomposable graphs or require specification of a tuning parameter that may have a substantial impact on learned structures. By combining a simple sparsity inducing prior for the graph structures with a default reference prior for the model parameters, we obtain a fast and easily applicable scoring function that works well for even high-dimensional data. We demonstrate the favourable performance of our approach by large-scale comparisons against the leading methods for learning non-decomposable Gaussian graphical models. A theoretical justification for our method is provided by showing that it yields a consistent estimator of the graph structure. version:1
arxiv-1602-07860 | Probably Approximately Correct Greedy Maximization | http://arxiv.org/abs/1602.07860 | id:1602.07860 author:Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek category:cs.AI cs.LG stat.ML  published:2016-02-25 summary:Submodular function maximization finds application in a variety of real-world decision-making problems. However, most existing methods, based on greedy maximization, assume it is computationally feasible to evaluate F, the function being maximized. Unfortunately, in many realistic settings F is too expensive to evaluate exactly even once. We present probably approximately correct greedy maximization, which requires access only to cheap anytime confidence bounds on F and uses them to prune elements. We show that, with high probability, our method returns an approximately optimal set. We propose novel, cheap confidence bounds for conditional entropy, which appears in many common choices of F and for which it is difficult to find unbiased or bounded estimates. Finally, results on a real-world dataset from a multi-camera tracking system in a shopping mall demonstrate that our approach performs comparably to existing methods, but at a fraction of the computational cost. version:1
arxiv-1405-5505 | Kernel Mean Shrinkage Estimators | http://arxiv.org/abs/1405.5505 | id:1405.5505 author:Krikamol Muandet, Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf category:stat.ML cs.LG  published:2014-05-21 summary:A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a "large d, small n" paradigm. version:3
arxiv-1602-07844 | Fast Nonsmooth Regularized Risk Minimization with Continuation | http://arxiv.org/abs/1602.07844 | id:1602.07844 author:Shuai Zheng, Ruiliang Zhang, James T. Kwok category:cs.LG  published:2016-02-25 summary:In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art. version:1
arxiv-1602-07803 | Automated Word Prediction in Bangla Language Using Stochastic Language Models | http://arxiv.org/abs/1602.07803 | id:1602.07803 author:Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman category:cs.CL  published:2016-02-25 summary:Word completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. They can have profound impact on the typing of disable people. Our work is based on word prediction on Bangla sentence by using stochastic, i.e. N-gram language model such as unigram, bigram, trigram, deleted Interpolation and backoff models for auto completing a sentence by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. We use large data corpus of Bangla language of different word types to predict correct word with the accuracy as much as possible. We have found promising results. We hope that our work will impact on the baseline for automated Bangla typing. version:1
arxiv-1602-07795 | Expectation Consistent Approximate Inference: Generalizations and Convergence | http://arxiv.org/abs/1602.07795 | id:1602.07795 author:Alyson Fletcher, Mojtaba Sahraee-Ardakan, Sundeep Rangan, Philip Schniter category:cs.IT math.IT stat.ML  published:2016-02-25 summary:Approximations of loopy belief propagation, including expectation propagation and approximate message passing, have attracted considerable attention for probabilistic inference problems. This paper proposes and analyzes a generalization of Opper and Winther's expectation consistent (EC) approximate inference method. The proposed method, called Generalized Expectation Consistency (GEC), can be applied to both maximum a posteriori (MAP) and minimum mean squared error (MMSE) estimation. Here we characterize its fixed points, convergence, and performance relative to the replica prediction of optimality. version:1
arxiv-1602-06064 | On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation | http://arxiv.org/abs/1602.06064 | id:1602.06064 author:Tianxing He, Yu Zhang, Jasha Droppo, Kai Yu category:cs.CL  published:2016-02-19 summary:We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE). Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still(regretfully), it did not out-perform the baseline uni-directional NNLM. version:3
arxiv-1602-07754 | A Compressed Sensing Based Decomposition of Electro-Dermal Activity Signals | http://arxiv.org/abs/1602.07754 | id:1602.07754 author:Swayambhoo Jain, Urvashi Oswal, Kevin S. Xu, Brian Eriksson, Jarvis Haupt category:stat.ML  published:2016-02-24 summary:The measurement and analysis of Electro-Dermal Activity (EDA) offers applications in diverse areas ranging from market research, to seizure detection, to human stress analysis. Unfortunately, the analysis of EDA signals is made difficult by the superposition of numerous components which can obscure the signal information related to a user's response to a stimulus. We show how simple pre-processing followed by a novel compressed sensing based decomposition can mitigate the effects of these noise components and help reveal the underlying physiological signal. The proposed framework allows for fast decomposition of EDA signals with provable bounds on the recovery of user responses. We test our procedure on both synthetic and real-world EDA signals from wearable sensors, and demonstrate that our approach allows for more accurate recovery of user responses as compared to the existing techniques. version:1
arxiv-1602-07749 | Toward Mention Detection Robustness with Recurrent Neural Networks | http://arxiv.org/abs/1602.07749 | id:1602.07749 author:Thien Huu Nguyen, Avirup Sil, Georgiana Dinu, Radu Florian category:cs.CL  published:2016-02-24 summary:One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9\% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22\% relative error reduction). version:1
arxiv-1602-07714 | Learning functions across many orders of magnitudes | http://arxiv.org/abs/1602.07714 | id:1602.07714 author:Hado van Hasselt, Arthur Guez, Matteo Hessel, David Silver category:cs.LG cs.AI cs.NE stat.ML  published:2016-02-24 summary:Learning non-linear functions can be hard when the magnitude of the target function is unknown beforehand, as most learning algorithms are not scale invariant. We propose an algorithm to adaptively normalize these targets. This is complementary to recent advances in input normalization. Importantly, the proposed method preserves the unnormalized outputs whenever the normalization is updated to avoid instability caused by non-stationarity. It can be combined with any learning algorithm and any non-linear function approximation, including the important special case of deep learning. We empirically validate the method in supervised learning and reinforcement learning and apply it to learning how to play Atari 2600 games. Previous work on applying deep learning to this domain relied on clipping the rewards to make learning in different games more homogeneous, but this uses the domain-specific knowledge that in these games counting rewards is often almost as informative as summing these. Using our adaptive normalization we can remove this heuristic without diminishing overall performance, and even improve performance on some games, such as Ms. Pac-Man and Centipede, on which previous methods did not perform well. version:1
arxiv-1511-06644 | Recurrent Gaussian Processes | http://arxiv.org/abs/1511.06644 | id:1511.06644 author:César Lincoln C. Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A. Barreto, Neil D. Lawrence category:cs.LG stat.ML  published:2015-11-20 summary:We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available. version:6
arxiv-1511-05263 | The Use of Machine Learning Algorithms in Recommender Systems: A Systematic Review | http://arxiv.org/abs/1511.05263 | id:1511.05263 author:Ivens Portugal, Paulo Alencar, Donald Cowan category:cs.SE cs.IR cs.LG  published:2015-11-17 summary:Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of a recommender system using a machine learning algorithm often has problems and open questions that must be evaluated, so software engineers know where to focus research efforts. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies research opportunities for software engineering research. The study concludes that Bayesian and decision tree algorithms are widely used in recommender systems because of their relative simplicity, and that requirement and design phases of recommender system development appear to offer opportunities for further research. version:4
arxiv-1602-07630 | Online Dual Coordinate Ascent Learning | http://arxiv.org/abs/1602.07630 | id:1602.07630 author:Bicheng Ying, Kun Yuan, Ali H. Sayed category:math.OC cs.LG stat.ML  published:2016-02-24 summary:The stochastic dual coordinate-ascent (S-DCA) technique is a useful alternative to the traditional stochastic gradient-descent algorithm for solving large-scale optimization problems due to its scalability to large data sets and strong theoretical guarantees. However, the available S-DCA formulation is limited to finite sample sizes and relies on performing multiple passes over the same data. This formulation is not well-suited for online implementations where data keep streaming in. In this work, we develop an {\em online} dual coordinate-ascent (O-DCA) algorithm that is able to respond to streaming data and does not need to revisit the past data. This feature embeds the resulting construction with continuous adaptation, learning, and tracking abilities, which are particularly attractive for online learning scenarios. version:1
arxiv-1602-07616 | Noisy population recovery in polynomial time | http://arxiv.org/abs/1602.07616 | id:1602.07616 author:Anindya De, Michael Saks, Sijian Tang category:cs.CC cs.DS cs.LG  published:2016-02-24 summary:In the noisy population recovery problem of Dvir et al., the goal is to learn an unknown distribution $f$ on binary strings of length $n$ from noisy samples. For some parameter $\mu \in [0,1]$, a noisy sample is generated by flipping each coordinate of a sample from $f$ independently with probability $(1-\mu)/2$. We assume an upper bound $k$ on the size of the support of the distribution, and the goal is to estimate the probability of any string to within some given error $\varepsilon$. It is known that the algorithmic complexity and sample complexity of this problem are polynomially related to each other. We show that for $\mu > 0$, the sample complexity (and hence the algorithmic complexity) is bounded by a polynomial in $k$, $n$ and $1/\varepsilon$ improving upon the previous best result of $\mathsf{poly}(k^{\log\log k},n,1/\varepsilon)$ due to Lovett and Zhang. Our proof combines ideas from Lovett and Zhang with a \emph{noise attenuated} version of M\"{o}bius inversion. In turn, the latter crucially uses the construction of \emph{robust local inverse} due to Moitra and Saks. version:1
arxiv-1507-07880 | Optimally Confident UCB: Improved Regret for Finite-Armed Bandits | http://arxiv.org/abs/1507.07880 | id:1507.07880 author:Tor Lattimore category:cs.LG math.OC  published:2015-07-28 summary:I present the first algorithm for stochastic finite-armed bandits that simultaneously enjoys order-optimal problem-dependent regret and worst-case regret. Besides the theoretical results, the new algorithm is simple, efficient and empirically superb. The approach is based on UCB, but with a carefully chosen confidence parameter that optimally balances the risk of failing confidence intervals against the cost of excessive optimism. version:3
arxiv-1410-7140 | A data-driven method for syndrome type identification and classification in traditional Chinese medicine | http://arxiv.org/abs/1410.7140 | id:1410.7140 author:Nevin L. Zhang, Chen Fu, Teng Fei Liu, Bao Xin Chen, Kin Man Poon, Pei Xian Chen, Yun Ling Zhang category:cs.LG stat.AP  published:2014-10-27 summary:Objective: The efficacy of traditional Chinese medicine (TCM) treatments for Western medicine (WM) diseases relies heavily on the proper classification of patients into TCM syndrome types. We develop a data-driven method for solving the classification problem, where syndrome types are identified and quantified based on patterns detected in unlabeled symptom survey data. Method: Latent class analysis (LCA) has been applied in WM research to solve a similar problem, i.e., to identify subtypes of a patient population in the absence of a gold standard. A widely known weakness of LCA is that it makes an unrealistically strong independence assumption. We relax the assumption by first detecting symptom co-occurrence patterns from survey data and use those patterns instead of the symptoms as features for LCA. Results: The result of the investigation is a six-step method: Data collection, symptom co-occurrence pattern discovery, pattern interpretation, syndrome identification, syndrome type identification, and syndrome type classification. A software package called Lantern is developed to support the application of the method. The method is illustrated using a data set on Vascular Mild Cognitive Impairment (VMCI). Conclusions: A data-driven method for TCM syndrome identification and classification is presented. The method can be used to answer the following questions about a Western medicine disease: What TCM syndrome types are there among the patients with the disease? What is the prevalence of each syndrome type? What are the statistical characteristics of each syndrome type in terms of occurrence of symptoms? How can we determine the syndrome type(s) of a patient? version:5
arxiv-1507-04777 | Sparse Estimation in a Correlated Probit Model | http://arxiv.org/abs/1507.04777 | id:1507.04777 author:Stephan Mandt, Florian Wenzel, Shinichi Nakajima, John P. Cunningham, Christoph Lippert, Marius Kloft category:stat.ML cs.LG  published:2015-07-16 summary:Among the goals of statistical genetics is to find associations between genetic data and binary phenotypes, such as heritable diseases. Often, the data are obfuscated by confounders such as age, ethnicity, or population structure. Linear mixed models are linear regression models that correct for confounding by means of correlated label noise; they are widely appreciated in the field of statistical genetics. We generalize this modeling paradigm to binary classification, where we face the problem that marginalizing over the noise leads to an intractable, high-dimensional integral. We present a scalable, approximate inference algorithm that lets us fit the model to high-dimensional data sets. The algorithm selects features based on an $\ell_1$ norm regularizer which are up to 40% less confounded compared to the outcomes of uncorrected feature selection, as we show. The proposed method also outperforms Gaussian process classification and uncorrelated probit regression in terms of prediction performance. version:2
arxiv-1602-07570 | Bayesian Exploration: Incentivizing Exploration in Bayesian Games | http://arxiv.org/abs/1602.07570 | id:1602.07570 author:Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, Zhiwei Steven Wu category:cs.GT cs.DS cs.LG  published:2016-02-24 summary:We consider a ubiquitous scenario in the Internet economy when individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents' incentives and minimizes a suitable notion of regret. We extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game, receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment. Our main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (where the constant depends on the prior distribution, but not on the time horizon), and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions, the actions which some incentive-compatible policy can recommend with non-zero probability. We show how the principal can identify (and explore) all explorable actions, and use the revealed information to perform optimally. version:1
arxiv-1511-05879 | Particular object retrieval with integral max-pooling of CNN activations | http://arxiv.org/abs/1511.05879 | id:1511.05879 author:Giorgos Tolias, Ronan Sicre, Hervé Jégou category:cs.CV  published:2015-11-18 summary:Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets. version:2
arxiv-1602-07542 | On the Accuracy of Point Localisation in a Circular Camera-Array | http://arxiv.org/abs/1602.07542 | id:1602.07542 author:Alireza Ghasemi, Adam Scholefield, Martin Vetterli category:cs.CV  published:2016-02-24 summary:Although many advances have been made in light-field and camera-array image processing, there is still a lack of thorough analysis of the localisation accuracy of different multi-camera systems. By considering the problem from a frame-quantisation perspective, we are able to quantify the point localisation error of circular camera configurations. Specifically, we obtain closed form expressions bounding the localisation error in terms of the parameters describing the acquisition setup. These theoretical results are independent of the localisation algorithm and thus provide fundamental limits on performance. Furthermore, the new frame-quantisation perspective is general enough to be extended to more complex camera configurations. version:1
arxiv-1602-07535 | SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay | http://arxiv.org/abs/1602.07535 | id:1602.07535 author:Alireza Ghasemi, Adam Scholefield, Martin Vetterli category:cs.CV  published:2016-02-24 summary:We propose a novel camera pose estimation or perspective-n-point (PnP) algorithm, based on the idea of consistency regions and half-space intersections. Our algorithm has linear time-complexity and a squared reconstruction error that decreases at least quadratically, as the number of feature point correspondences increase. Inspired by ideas from triangulation and frame quantisation theory, we define consistent reconstruction and then present SHAPE, our proposed consistent pose estimation algorithm. We compare this algorithm with state-of-the-art pose estimation techniques in terms of accuracy and error decay rate. The experimental results verify our hypothesis on the optimal worst-case quadratic decay and demonstrate its promising performance compared to other approaches. version:1
arxiv-1602-07507 | A Bayesian Approach to the Data Description Problem | http://arxiv.org/abs/1602.07507 | id:1602.07507 author:Alireza Ghasemi, Hamid R. Rabiee, Mohammad T. Manzuri, M. H. Rohban category:cs.LG  published:2016-02-24 summary:In this paper, we address the problem of data description using a Bayesian framework. The goal of data description is to draw a boundary around objects of a certain class of interest to discriminate that class from the rest of the feature space. Data description is also known as one-class learning and has a wide range of applications. The proposed approach uses a Bayesian framework to precisely compute the class boundary and therefore can utilize domain information in form of prior knowledge in the framework. It can also operate in the kernel space and therefore recognize arbitrary boundary shapes. Moreover, the proposed method can utilize unlabeled data in order to improve accuracy of discrimination. We evaluate our method using various real-world datasets and compare it with other state of the art approaches of data description. Experiments show promising results and improved performance over other data description and one-class learning algorithms. version:1
arxiv-1602-07495 | Active Learning from Positive and Unlabeled Data | http://arxiv.org/abs/1602.07495 | id:1602.07495 author:Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri, Mohammad H. Rohban category:cs.LG  published:2016-02-24 summary:During recent years, active learning has evolved into a popular paradigm for utilizing user's feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available. Such problems arise in many real-world situations and are known as the problem of learning from positive and unlabeled data. In this paper we propose an active learning algorithm that can work when only samples of one class as well as a set of unlabelled data are available. Our method works by separately estimating probability desnity of positive and unlabeled points and then computing expected value of informativeness to get rid of a hyper-parameter and have a better measure of informativeness./ Experiments and empirical analysis show promising results compared to other similar methods. version:1
arxiv-1602-07480 | Boosting patch-based scene text script identification with ensembles of conjoined networks | http://arxiv.org/abs/1602.07480 | id:1602.07480 author:Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas category:cs.CV  published:2016-02-24 summary:This paper focuses on the problem of script identification in scene text images. Facing this problem with state of the art CNN classifiers is not straightforward, as they fail to address a key characteristic of scene text instances: their extremely variable aspect ratio. Instead of resizing input images to a fixed aspect ratio as in the typical use of holistic CNN classifiers, we propose here a patch-based classification framework in order to preserve discriminative parts of the image that are characteristic of its class. We describe a novel method based on the use of ensembles of conjoined networks to jointly learn discriminative stroke-parts representations and their relative importance in a patch-based classification scheme. Our experiments with this learning procedure demonstrate state-of-the-art results in two public script identification datasets. In addition, we propose a new public benchmark dataset for the evaluation of multi-lingual scene text end-to-end reading systems. Experiments done in this dataset demonstrate the key role of script identification in a complete end-to-end system that combines our script identification method with a previously published text detector and an off-the-shelf OCR engine. version:1
arxiv-1602-07475 | A fine-grained approach to scene text script identification | http://arxiv.org/abs/1602.07475 | id:1602.07475 author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV  published:2016-02-24 summary:This paper focuses on the problem of script identification in unconstrained scenarios. Script identification is an important prerequisite to recognition, and an indispensable condition for automatic text understanding systems designed for multi-language environments. Although widely studied for document images and handwritten documents, it remains an almost unexplored territory for scene text images. We detail a novel method for script identification in natural images that combines convolutional features and the Naive-Bayes Nearest Neighbor classifier. The proposed framework efficiently exploits the discriminative power of small stroke-parts, in a fine-grained classification framework. In addition, we propose a new public benchmark dataset for the evaluation of joint text detection and script identification in natural scenes. Experiments done in this new dataset demonstrate that the proposed method yields state of the art results, while it generalizes well to different datasets and variable number of scripts. The evidence provided shows that multi-lingual scene text recognition in the wild is a viable proposition. Source code of the proposed method is made available online. version:1
arxiv-1511-06530 | Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications | http://arxiv.org/abs/1511.06530 | id:1511.06530 author:Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, Dongjun Shin category:cs.CV cs.LG  published:2015-11-20 summary:Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme. version:2
arxiv-1602-07466 | Asymptotic consistency and order specification for logistic classifier chains in multi-label learning | http://arxiv.org/abs/1602.07466 | id:1602.07466 author:Paweł Teisseyre category:cs.LG stat.ML  published:2016-02-24 summary:Classifier chains are popular and effective method to tackle a multi-label classification problem. The aim of this paper is to study the asymptotic properties of the chain model in which the conditional probabilities are of the logistic form. In particular we find conditions on the number of labels and the distribution of feature vector under which the estimated mode of the joint distribution of labels converges to the true mode. Best of our knowledge, this important issue has not yet been studied in the context of multi-label learning. We also investigate how the order of model building in a chain influences the estimation of the joint distribution of labels. We establish the link between the problem of incorrect ordering in the chain and incorrect model specification. We propose a procedure of determining the optimal ordering of labels in the chain, which is based on using measures of correct specification and allows to find the ordering such that the consecutive logistic models are best possibly specified. The other important question raised in this paper is how accurately can we estimate the joint posterior probability when the ordering of labels is wrong or the logistic models in the chain are incorrectly specified. The numerical experiments illustrate the theoretical results. version:1
arxiv-1602-05179 | Towards a Biologically Plausible Backprop | http://arxiv.org/abs/1602.05179 | id:1602.05179 author:Benjamin Scellier, Yoshua Bengio category:cs.LG  published:2016-02-16 summary:This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00%. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains. version:2
arxiv-1602-07464 | Feature ranking for multi-label classification using Markov Networks | http://arxiv.org/abs/1602.07464 | id:1602.07464 author:Paweł Teisseyre category:cs.LG stat.ML  published:2016-02-24 summary:We propose a simple and efficient method for ranking features in multi-label classification. The method produces a ranking of features showing their relevance in predicting labels, which in turn allows to choose a final subset of features. The procedure is based on Markov Networks and allows to model the dependencies between labels and features in a direct way. In the first step we build a simple network using only labels and then we test how much adding a single feature affects the initial network. More specifically, in the first step we use the Ising model whereas the second step is based on the score statistic, which allows to test a significance of added features very quickly. The proposed approach does not require transformation of label space, gives interpretable results and allows for attractive visualization of dependency structure. We give a theoretical justification of the procedure by discussing some theoretical properties of the Ising model and the score statistic. We also discuss feature ranking procedure based on fitting Ising model using $l_1$ regularized logistic regressions. Numerical experiments show that the proposed methods outperform the conventional approaches on the considered artificial and real datasets. version:1
arxiv-1511-03814 | Hand-Object Interaction and Precise Localization in Transitive Action Recognition | http://arxiv.org/abs/1511.03814 | id:1511.03814 author:Amir Rosenfeld, Shimon Ullman category:cs.CV  published:2015-11-12 summary:Action recognition in still images has seen major improvement in recent years due to advances in human pose estimation, object recognition and stronger feature representations produced by deep neural networks. However, there are still many cases in which performance remains far from that of humans. A major difficulty arises in distinguishing between transitive actions in which the overall actor pose is similar, and recognition therefore depends on details of the grasp and the object, which may be largely occluded. In this paper we demonstrate how recognition is improved by obtaining precise localization of the action-object and consequently extracting details of the object shape together with the actor-object interaction. To obtain exact localization of the action object and its interaction with the actor, we employ a coarse-to-fine approach which combines semantic segmentation and contextual features, in successive stages. We focus on (but are not limited) to face-related actions, a set of actions that includes several currently challenging categories. We present an average relative improvement of 35% over state-of-the art and validate through experimentation the effectiveness of our approach. version:2
arxiv-1602-07428 | Max-Margin Nonparametric Latent Feature Models for Link Prediction | http://arxiv.org/abs/1602.07428 | id:1602.07428 author:Jun Zhu, Jiaming Song, Bei Chen category:cs.LG cs.SI stat.ME stat.ML  published:2016-02-24 summary:Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference. version:1
arxiv-1602-05419 | Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression | http://arxiv.org/abs/1602.05419 | id:1602.05419 author:Aymeric Dieuleveut, Nicolas Flammarion, Francis Bach category:math.OC cs.LG stat.ML  published:2016-02-17 summary:We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting of initial conditions in O(1/n 2), and in terms of dependence on the noise and dimension d of the problem, as O(d/n). Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small while the " optimal " terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance. version:2
arxiv-1602-07394 | Improved Accent Classification Combining Phonetic Vowels with Acoustic Features | http://arxiv.org/abs/1602.07394 | id:1602.07394 author:Zhenhao Ge category:cs.SD cs.CL  published:2016-02-24 summary:Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field. version:1
arxiv-1602-07393 | Domain Specific Author Attribution Based on Feedforward Neural Network Language Models | http://arxiv.org/abs/1602.07393 | id:1602.07393 author:Zhenhao Ge, Yufang Sun category:cs.CL cs.LG cs.NE  published:2016-02-24 summary:Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at https://github.com/zge/authorship-attribution/. version:1
arxiv-1602-07388 | The Myopia of Crowds: A Study of Collective Evaluation on Stack Exchange | http://arxiv.org/abs/1602.07388 | id:1602.07388 author:Keith Burghardt, Emanuel F. Alsina, Michelle Girvan, William Rand, Kristina Lerman category:cs.HC cs.CY cs.LG physics.soc-ph  published:2016-02-24 summary:Crowds can often make better decisions than individuals or small groups of experts by leveraging their ability to aggregate diverse information. Question answering sites, such as Stack Exchange, rely on the "wisdom of crowds" effect to identify the best answers to questions asked by users. We analyze data from 250 communities on the Stack Exchange network to pinpoint factors affecting which answers are chosen as the best answers. Our results suggest that, rather than evaluate all available answers to a question, users rely on simple cognitive heuristics to choose an answer to vote for or accept. These cognitive heuristics are linked to an answer's salience, such as the order in which it is listed and how much screen space it occupies. While askers appear to depend more on heuristics, compared to voting users, when choosing an answer to accept as the most helpful one, voters use acceptance itself as a heuristic: they are more likely to choose the answer after it is accepted than before that very same answer was accepted. These heuristics become more important in explaining and predicting behavior as the number of available answers increases. Our findings suggest that crowd judgments may become less reliable as the number of answers grow. version:1
arxiv-1602-07383 | Automatic Moth Detection from Trap Images for Pest Management | http://arxiv.org/abs/1602.07383 | id:1602.07383 author:Weiguang Ding, Graham Taylor category:cs.CV cs.LG cs.NE  published:2016-02-24 summary:Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required. version:1
arxiv-1604-08095 | Accent Classification with Phonetic Vowel Representation | http://arxiv.org/abs/1604.08095 | id:1604.08095 author:Zhenhao Ge, Yingyi Tan, Aravind Ganapathiraju category:cs.SD cs.CL  published:2016-02-24 summary:Previous accent classification research focused mainly on detecting accents with pure acoustic information without recognizing accented speech. This work combines phonetic knowledge such as vowels with acoustic information to build Guassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP) features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With input about 20-second accented speech, this system achieves classification rate of 51% on a 7-way classification system focusing on the major types of accents in English, which is competitive to the state-of-the-art results in this field. version:1
arxiv-1602-07373 | On Study of the Binarized Deep Neural Network for Image Classification | http://arxiv.org/abs/1602.07373 | id:1602.07373 author:Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi category:cs.NE cs.CV cs.LG  published:2016-02-24 summary:Recently, the deep neural network (derived from the artificial neural network) has attracted many researchers' attention by its outstanding performance. However, since this network requires high-performance GPUs and large storage, it is very hard to use it on individual devices. In order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. Unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. This network is a pure binary system, in which all the values and calculations are binarized. As a result, our network can save a lot of computational resource and storage. Therefore, it is possible to use it on various devices. Moreover, the experimental results proved the feasibility of the proposed network. version:1
arxiv-1502-03520 | RAND-WALK: A Latent Variable Model Approach to Word Embeddings | http://arxiv.org/abs/1502.03520 | id:1502.03520 author:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski category:cs.LG cs.CL stat.ML  published:2015-02-12 summary:Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods such as Latent Semantic Analysis (LSA), generative text models such as topic models, matrix factorization, neural nets, and energy-based models. Many methods use nonlinear operations ---such as Pairwise Mutual Information or PMI--- on co-occurrence statistics, and have hand-tuned hyperparameters and reweightings. Often a {\em generative model} can help provide theoretical insight into such modeling choices, but there appears to be no such model to "explain" the above nonlinear models. For example, we know of no generative model for which the correct solution is the usual (dimension-restricted) PMI model. This paper gives a new generative model, a dynamic version of the loglinear topic model of \citet{mnih2007three}. The methodological novelty is to use the prior to compute {\em closed form} expressions for word statistics. These provide an explanation for nonlinear models like PMI, {\bf word2vec}, and GloVe, as well as some hyperparameter choices. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed ("isotropic") in space. The model also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by~\citet{mikolov2013efficient} and many subsequent papers. version:6
arxiv-1511-01169 | adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs | http://arxiv.org/abs/1511.01169 | id:1511.01169 author:Nitish Shirish Keskar, Albert S. Berahas category:cs.LG math.OC stat.ML  published:2015-11-04 summary:Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional performance on several pattern recognition problems. However, the training of RNNs is a computationally difficult task owing to the well-known "vanishing/exploding" gradient problem. Algorithms proposed for training RNNs either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. The former set includes diagonally-scaled first-order methods such as ADAGRAD and ADAM, while the latter consists of second-order algorithms like Hessian-Free Newton and K-FAC. In this paper, we present adaQN, a stochastic quasi-Newton algorithm for training RNNs. Our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. We present numerical experiments on two language modeling tasks and show that adaQN is competitive with popular RNN training algorithms. version:5
arxiv-1602-07349 | Parsimonious modeling with Information Filtering Networks | http://arxiv.org/abs/1602.07349 | id:1602.07349 author:Wolfram Barfuss, Guido Previde Massara, T. Di Matteo, Tomaso Aste category:cs.IT math.IT stat.ML  published:2016-02-23 summary:We introduce a methodology to construct sparse models from data by using information filtering networks as inference structure. This method is computationally very efficient and statistically robust because it is based {on} local, low-dimensional, inversions of the covariance matrix to generate a global sparse inverse. Compared with state-of-the-art methodologies such as lasso, our method is computationally more efficient producing in a fraction of computation time models that have equivalent or better performances but with a sparser and more meaningful inference structure. The local nature of this approach allow{s} dynamical partial updating when the properties of some variables change without the need of recomputing the whole model. We discuss performances with financial data and financial applications to prediction, stress testing and risk allocation. version:1
arxiv-1511-06391 | Order Matters: Sequence to sequence for sets | http://arxiv.org/abs/1511.06391 | id:1511.06391 author:Oriol Vinyals, Samy Bengio, Manjunath Kudlur category:stat.ML cs.CL cs.LG  published:2015-11-19 summary:Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models. version:4
arxiv-1602-07332 | Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations | http://arxiv.org/abs/1602.07332 | id:1602.07332 author:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li category:cs.CV cs.AI  published:2016-02-23 summary:Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers. version:1
arxiv-1602-07320 | Stuck in a What? Adventures in Weight Space | http://arxiv.org/abs/1602.07320 | id:1602.07320 author:Zachary C. Lipton category:cs.LG  published:2016-02-23 summary:Deep learning researchers commonly suggest that converged models are stuck in local minima. More recently, some researchers observed that under reasonable assumptions, the vast majority of critical points are saddle points, not true minima. Both descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it's possible that neither interpretation is accurate. As neural networks are typically over-complete, it's easy to show the existence of vast continuous regions through weight space with equal loss. In this paper, we build on recent work empirically characterizing the error surfaces of neural networks. We analyze training paths through weight space, presenting evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it's trivial to show that neural network error surfaces are globally non-convex, we show that error surfaces are also locally non-convex, even after breaking symmetry with a random initialization and also after partial training. version:1
arxiv-1510-05830 | Unsupervised Ensemble Learning with Dependent Classifiers | http://arxiv.org/abs/1510.05830 | id:1510.05830 author:Ariel Jaffe, Ethan Fetaya, Boaz Nadler, Tingting Jiang, Yuval Kluger category:cs.LG stat.ML  published:2015-10-20 summary:In unsupervised ensemble learning, one obtains predictions from multiple sources or classifiers, yet without knowing the reliability and expertise of each source, and with no labeled data to assess it. The task is to combine these possibly conflicting predictions into an accurate meta-learner. Most works to date assumed perfect diversity between the different sources, a property known as conditional independence. In realistic scenarios, however, this assumption is often violated, and ensemble learners based on it can be severely sub-optimal. The key challenges we address in this paper are:\ (i) how to detect, in an unsupervised manner, strong violations of conditional independence; and (ii) construct a suitable meta-learner. To this end we introduce a statistical model that allows for dependencies between classifiers. Our main contributions are the development of novel unsupervised methods to detect strongly dependent classifiers, better estimate their accuracies, and construct an improved meta-learner. Using both artificial and real datasets, we showcase the importance of taking classifier dependencies into account and the competitive performance of our approach. version:2
arxiv-1602-07291 | The IBM 2016 Speaker Recognition System | http://arxiv.org/abs/1602.07291 | id:1602.07291 author:Seyed Omid Sadjadi, Sriram Ganapathy, Jason W. Pelecanos category:cs.SD cs.CL stat.ML  published:2016-02-23 summary:In this paper we describe the recent advancements made in the IBM i-vector speaker recognition system for conversational speech. In particular, we identify key techniques that contribute to significant improvements in performance of our system, and quantify their contributions. The techniques include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is formulated to alleviate some of the limitations associated with the conventional linear discriminant analysis (LDA) that assumes Gaussian class-conditional distributions, 2) the application of speaker- and channel-adapted features, which are derived from an automatic speech recognition (ASR) system, for speaker recognition, and 3) the use of a deep neural network (DNN) acoustic model with a large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 speaker recognition evaluation (SRE) extended core conditions involving telephone and microphone trials. Experimental results indicate that: 1) the NDA is more effective (up to 35% relative improvement in terms of EER) than the traditional parametric LDA for speaker recognition, 2) when compared to raw acoustic features (e.g., MFCCs), the ASR speaker-adapted features provide gains in speaker recognition performance, and 3) increasing the number of output units in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k) provides consistent improvements in performance (for example from 37% to 57% relative EER gains over our baseline GMM i-vector system). To our knowledge, results reported in this paper represent the best performances published to date on the NIST SRE 2010 extended core tasks. version:1
arxiv-1602-07277 | A Simple Approach to Sparse Clustering | http://arxiv.org/abs/1602.07277 | id:1602.07277 author:Ery Arias-Castro, Xiao Pu category:stat.ML  published:2016-02-23 summary:We consider the problem of sparse clustering, where it is assumed that only a subset of the features are useful for clustering purposes. In the framework of the COSA method of Friedman and Meulman (2004), subsequently improved in the form of the Sparse K-means method of Witten and Tibshirani (2010), we propose a very natural and simpler hill-climbing approach that is competitive with these two methods. version:1
arxiv-1511-00394 | Submodular Functions: from Discrete to Continous Domains | http://arxiv.org/abs/1511.00394 | id:1511.00394 author:Francis Bach category:cs.LG math.OC  published:2015-11-02 summary:Submodular set-functions have many applications in combinatorial optimization, as they can be minimized and approximately maximized in polynomial time. A key element in many of the algorithms and analyses is the possibility of extending the submodular set-function to a convex function, which opens up tools from convex optimization. Submodularity goes beyond set-functions and has naturally been considered for problems with multiple labels or for functions defined on continuous domains, where it corresponds essentially to cross second-derivatives being nonpositive. In this paper, we show that most results relating submodularity and convexity for set-functions can be extended to all submodular functions. In particular, (a) we naturally define a continuous extension in a set of probability measures, (b) show that the extension is convex if and only if the original function is submodular, (c) prove that the problem of minimizing a submodular function is equivalent to a typically non-smooth convex optimization problem, and (d) propose another convex optimization problem with better computational properties (e.g., a smooth dual problem). Most of these extensions from the set-function situation are obtained by drawing links with the theory of multi-marginal optimal transport, which provides also a new interpretation of existing results for set-functions. We then provide practical algorithms to minimize generic submodular functions on discrete domains, with associated convergence rates. version:2
arxiv-1602-06561 | Deep Learning in Finance | http://arxiv.org/abs/1602.06561 | id:1602.06561 author:J. B. Heaton, N. G. Polson, J. H. Witte category:cs.LG  published:2016-02-21 summary:We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems -- such as those presented in designing and pricing securities, constructing portfolios, and risk management -- often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory. version:2
arxiv-1602-07265 | Search Improves Label for Active Learning | http://arxiv.org/abs/1602.07265 | id:1602.07265 author:Alina Beygelzimer, Daniel Hsu, John Langford, Chicheng Zhang category:cs.LG stat.ML  published:2016-02-23 summary:We investigate active learning with access to two distinct oracles: Label (which is standard) and Search (which is not). The Search oracle models the situation where a human searches a database to seed or counterexample an existing solution. Search is stronger than Label while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over Label alone. version:1
arxiv-1601-01750 | Learning to Remove Multipath Distortions in Time-of-Flight Range Images for a Robotic Arm Setup | http://arxiv.org/abs/1601.01750 | id:1601.01750 author:Kilho Son, Ming-Yu Liu, Yuichi Taguchi category:cs.CV cs.RO  published:2016-01-08 summary:Range images captured by Time-of-Flight (ToF) cameras are corrupted with multipath distortions due to interaction between modulated light signals and scenes. The interaction is often complicated, which makes a model-based solution elusive. We propose a learning-based approach for removing the multipath distortions for a ToF camera in a robotic arm setup. Our approach is based on deep learning. We use the robotic arm to automatically collect a large amount of ToF range images containing various multipath distortions. The training images are automatically labeled by leveraging a high precision structured light sensor available only in the training time. In the test time, we apply the learned model to remove the multipath distortions. This allows our robotic arm setup to enjoy the speed and compact form of the ToF camera without compromising with its range measurement errors. We conduct extensive experimental validations and compare the proposed method to several baseline algorithms. The experiment results show that our method achieves 55% error reduction in range estimation and largely outperforms the baseline algorithms. version:3
arxiv-1602-07261 | Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning | http://arxiv.org/abs/1602.07261 | id:1602.07261 author:Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke category:cs.CV  published:2016-02-23 summary:Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge version:1
arxiv-1602-07236 | Petrarch 2 : Petrarcher | http://arxiv.org/abs/1602.07236 | id:1602.07236 author:Clayton Norris category:cs.CL  published:2016-02-23 summary:PETRARCH 2 is the fourth generation of a series of Event-Data coders stemming from research by Phillip Schrodt. Each iteration has brought new functionality and usability, and this is no exception.Petrarch 2 takes much of the power of the original Petrarch's dictionaries and redirects it into a faster and smarter core logic. Earlier iterations handled sentences largely as a list of words, incorporating some syntactic information here and there. Petrarch 2 now views the sentence entirely on the syntactic level. It receives the syntactic parse of a sentence from the Stanford CoreNLP software, and stores this data as a tree structure of linked nodes, where each node is a Phrase object. Prepositional, noun, and verb phrases each have their own version of this Phrase class, which deals with the logic particular to those kinds of phrases. Since this is an event coder, the core of the logic focuses around the verbs: who is acting, who is being acted on, and what is happening. The theory behind this new structure and its logic is founded in Generative Grammar, Information Theory, and Lambda-Calculus Semantics. version:1
arxiv-1602-07194 | Lens depth function and k-relative neighborhood graph: versatile tools for ordinal data analysis | http://arxiv.org/abs/1602.07194 | id:1602.07194 author:Matthäus Kleindessner, Ulrike von Luxburg category:stat.ML cs.DS cs.LG  published:2016-02-23 summary:In recent years it has become popular to study machine learning problems based on ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as d(A,B) < d(C,D). For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification and clustering when given only ordinal distance comparisons. They are based on estimating the lens depth function and the k-relative neighborhood graph on a data set. Our algorithms are simple, can easily be parallelized, avoid many of the drawbacks of an ordinal embedding approach and are much faster. version:1
arxiv-1602-07182 | Explore First, Exploit Next: The True Shape of Regret in Bandit Problems | http://arxiv.org/abs/1602.07182 | id:1602.07182 author:Aurélien Garivier, Pierre Ménard, Gilles Stoltz category:math.ST cs.LG stat.TH  published:2016-02-23 summary:We revisit lower bounds on the regret in the case of multi-armed bandit problems. We obtain non-asymptotic bounds and provide straightforward proofs based only on well-known properties of Kullback-Leibler divergences. These bounds show that in an initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase. The proof techniques come to the essence of the arguments used and they are deprived of all unnecessary complications. version:1
arxiv-1507-06266 | Particle detection and tracking by a-contrario approach: application to fluorescence time-lapse imaging | http://arxiv.org/abs/1507.06266 | id:1507.06266 author:Mariella Dimiccoli, Jean-Pascal Jacob, Lionel Moisan category:cs.CV  published:2015-07-22 summary:In this work, we propose a probabilistic approach for the detection and the tracking of particles on biological images. In presence of very noised and poor quality data, particles and trajectories can be characterized by an a-contrario model, that estimates the probability of observing the structures of interest in random data. This approach, first introduced in the modeling of human visual perception and then successfully applied in many image processing tasks, leads to algorithms that do not require a previous learning stage, nor a tedious parameter tuning and are very robust to noise. Comparative evaluations against a well established baseline show that the proposed approach outperforms the state of the art. version:4
arxiv-1505-02910 | Permutational Rademacher Complexity: a New Complexity Measure for Transductive Learning | http://arxiv.org/abs/1505.02910 | id:1505.02910 author:Ilya Tolstikhin, Nikita Zhivotovskiy, Gilles Blanchard category:stat.ML cs.LG  published:2015-05-12 summary:Transductive learning considers situations when a learner observes $m$ labelled training points and $u$ unlabelled test points with the final goal of giving correct answers for the test points. This paper introduces a new complexity measure for transductive learning called Permutational Rademacher Complexity (PRC) and studies its properties. A novel symmetrization inequality is proved, which shows that PRC provides a tighter control over expected suprema of empirical processes compared to what happens in the standard i.i.d. setting. A number of comparison results are also provided, which show the relation between PRC and other popular complexity measures used in statistical learning theory, including Rademacher complexity and Transductive Rademacher Complexity (TRC). We argue that PRC is a more suitable complexity measure for transductive learning. Finally, these results are combined with a standard concentration argument to provide novel data-dependent risk bounds for transductive learning. version:2
arxiv-1602-07120 | Submodular Learning and Covering with Response-Dependent Costs | http://arxiv.org/abs/1602.07120 | id:1602.07120 author:Sivan Sabato category:cs.LG stat.ML  published:2016-02-23 summary:We consider interactive learning and covering problems, in a setting where actions may incur different costs, depending on the outcomes of the action. For instance, in a clinical trial, selecting a patient for treatment might result in improved health or adverse effects for this patients, and these two outcomes have different costs. We consider a setting where these costs can be inferred from observable responses to the actions. We generalize previous analyses of interactive learning and covering to \emph{consistency aware} submodular objectives, and propose a natural greedy algorithm for the setting of response-dependent costs. We bound the approximation factor of this greedy algorithm, for general submodular functions, as well as specifically for \emph{learning objectives}, and show that a different property of the cost function controls the approximation factor in each of these scenarios. We further show that in both settings, the approximation factor of this greedy algorithm is near-optimal in the class of greedy algorithms. Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting. version:1
arxiv-1602-07119 | The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection | http://arxiv.org/abs/1602.07119 | id:1602.07119 author:Pascal Mettes, Dennis C. Koelma, Cees G. M. Snoek category:cs.CV  published:2016-02-23 summary:This paper strives for video event detection using a representation learned from deep convolutional neural networks. Different from the leading approaches, who all learn from the 1,000 classes defined in the ImageNet Large Scale Visual Recognition Challenge, we investigate how to leverage the complete ImageNet hierarchy for pre-training deep networks. To deal with the problems of over-specific classes and classes with few images, we introduce a bottom-up and top-down approach for reorganization of the ImageNet hierarchy based on all its 21,814 classes and more than 14 million images. Experiments on the TRECVID Multimedia Event Detection 2013 and 2015 datasets show that video representations derived from the layers of a deep neural network pre-trained with our reorganized hierarchy i) improves over standard pre-training, ii) is complementary among different reorganizations, iii) maintains the benefits of fusion with other modalities, and iv) leads to state-of-the-art event detection results. The reorganized hierarchies and their derived Caffe models are publicly available at http://tinyurl.com/imagenetshuffle. version:1
arxiv-1602-03218 | Learning Efficient Algorithms with Hierarchical Attentive Memory | http://arxiv.org/abs/1602.03218 | id:1602.03218 author:Marcin Andrychowicz, Karol Kurach category:cs.LG  published:2016-02-09 summary:In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue. version:2
arxiv-1602-07107 | A Streaming Algorithm for Crowdsourced Data Classification | http://arxiv.org/abs/1602.07107 | id:1602.07107 author:Thomas Bonald, Richard Combes category:stat.ML cs.LG  published:2016-02-23 summary:We propose a streaming algorithm for the binary classification of data based on crowdsourcing. The algorithm learns the competence of each labeller by comparing her labels to those of other labellers on the same tasks and uses this information to minimize the prediction error rate on each task. We provide performance guarantees of our algorithm for a fixed population of independent labellers. In particular, we show that our algorithm is optimal in the sense that the cumulative regret compared to the optimal decision with known labeller error probabilities is finite, independently of the number of tasks to label. The complexity of the algorithm is linear in the number of labellers and the number of tasks, up to some logarithmic factors. Numerical experiments illustrate the performance of our algorithm compared to existing algorithms, including simple majority voting and expectation-maximization algorithms, on both synthetic and real datasets. version:1
arxiv-1602-07613 | Object Learning and Convex Cardinal Shape Composition | http://arxiv.org/abs/1602.07613 | id:1602.07613 author:Alireza Aghasi, Justin Romberg category:cs.CV math.OC  published:2016-02-23 summary:This work mainly focuses on a novel segmentation and partitioning scheme, based on learning the principal elements of the optimal partitioner in the image. The problem of interest is characterizing the objects present in an image as a composition of matching elements from a dictionary of prototype shapes. The composition model allows set union and difference among the selected elements, while regularizing the problem by restricting their count to a fixed level. This is a combinatorial problem addressing which is not in general computationally tractable. Convex cardinal shape composition (CSC) is a recent relaxation scheme presented as a proxy to the original problem. From a theoretical standpoint, this paper improves the results presented in the original work, by deriving the general sufficient conditions under which CSC identifies a target composition. We also provide qualitative results on who well the CSC outcome approximates the combinatorial solution. From a computational standpoint, two convex solvers, one supporting distributed processing for large-scale problems, and one cast as a linear program are presented. Applications such as multi-resolution segmentation and recovery of the principal shape components are presented as the experiments supporting the proposed ideas. version:1
arxiv-1602-03061 | Minimum Conditional Description Length Estimation for Markov Random Fields | http://arxiv.org/abs/1602.03061 | id:1602.03061 author:Matthew G. Reyes, David L. Neuhoff category:cs.IT cs.LG math.IT math.ST stat.TH  published:2016-02-09 summary:In this paper we discuss a method, which we call Minimum Conditional Description Length (MCDL), for estimating the parameters of a subset of sites within a Markov random field. We assume that the edges are known for the entire graph $G=(V,E)$. Then, for a subset $U\subset V$, we estimate the parameters for nodes and edges in $U$ as well as for edges incident to a node in $U$, by finding the exponential parameter for that subset that yields the best compression conditioned on the values on the boundary $\partial U$. Our estimate is derived from a temporally stationary sequence of observations on the set $U$. We discuss how this method can also be applied to estimate a spatially invariant parameter from a single configuration, and in so doing, derive the Maximum Pseudo-Likelihood (MPL) estimate. version:2
arxiv-1506-02761 | WordRank: Learning Word Embeddings via Robust Ranking | http://arxiv.org/abs/1506.02761 | id:1506.02761 author:Shihao Ji, Hyokun Yun, Pinar Yanardag, Shin Matsushima, S. V. N. Vishwanathan category:cs.CL cs.LG stat.ML  published:2015-06-09 summary:Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the-arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-machine distributed implementation of WordRank is open sourced for general usage. version:3
arxiv-1602-07046 | An Improved Gap-Dependency Analysis of the Noisy Power Method | http://arxiv.org/abs/1602.07046 | id:1602.07046 author:Maria Florina Balcan, Simon S. Du, Yining Wang, Adams Wei Yu category:stat.ML cs.LG  published:2016-02-23 summary:We consider the noisy power method algorithm, which has wide applications in machine learning and statistics, especially those related to principal component analysis (PCA) under resource (communication, memory or privacy) constraints. Existing analysis of the noisy power method shows an unsatisfactory dependency over the "consecutive" spectral gap $(\sigma_k-\sigma_{k+1})$ of an input data matrix, which could be very small and hence limits the algorithm's applicability. In this paper, we present a new analysis of the noisy power method that achieves improved gap dependency for both sample complexity and noise tolerance bounds. More specifically, we improve the dependency over $(\sigma_k-\sigma_{k+1})$ to dependency over $(\sigma_k-\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and could be much larger than the target rank $k$. Our proofs are built upon a novel characterization of proximity between two subspaces that differ from canonical angle characterizations analyzed in previous works. Finally, we apply our improved bounds to distributed private PCA and memory-efficient streaming PCA and obtain bounds that are superior to existing results in the literature. version:1
arxiv-1602-07043 | Auditing Black-box Models by Obscuring Features | http://arxiv.org/abs/1602.07043 | id:1602.07043 author:Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, Suresh Venkatasubramanian category:stat.ML cs.LG  published:2016-02-23 summary:Data-trained predictive models are widely used to assist in decision making. But they are used as black boxes that output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior: and in particular how different attributes influence the model prediction. This is very important when trying to interpret the behavior of complex models, or ensure that certain problematic attributes (like race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models: we can study the extent to which existing models take advantage of particular features in the dataset without knowing how the models work. We show how a class of techniques originally developed for the detection and repair of disparate impact in classification models can be used to study the sensitivity of any model with respect to any feature subsets. Our approach does not require the black-box model to be retrained. This is important if (for example) the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence like feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available datasets and models. We also validate our procedure using techniques from interpretable learning and feature selection. version:1
arxiv-1602-07038 | Computer Aided Restoration of Handwritten Character Strokes | http://arxiv.org/abs/1602.07038 | id:1602.07038 author:Barak Sober, David Levin category:cs.GR cs.CV math.NA  published:2016-02-23 summary:This work suggests a new variational approach to the task of computer aided restoration of incomplete characters, residing in a highly noisy document. We model character strokes as the movement of a pen with a varying radius. Following this model, a cubic spline representation is being utilized to perform gradient descent steps, while maintaining interpolation at some initial (manually sampled) points. The proposed algorithm was utilized in the process of restoring approximately 1000 ancient Hebrew characters (dating to ca. 8th-7th century BCE), some of which are presented herein and show that the algorithm yields plausible results when applied on deteriorated documents. version:1
arxiv-1602-07031 | Mobile Big Data Analytics Using Deep Learning and Apache Spark | http://arxiv.org/abs/1602.07031 | id:1602.07031 author:Mohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, Zhu Han category:cs.DC cs.LG cs.NE  published:2016-02-23 summary:The proliferation of mobile devices, such as smartphones and Internet of Things (IoT) gadgets, results in the recent mobile big data (MBD) era. Collecting MBD is unprofitable unless suitable analytics and learning methods are utilized for extracting meaningful information and hidden patterns from data. This article presents an overview and brief tutorial of deep learning in MBD analytics and discusses a scalable learning framework over Apache Spark. Specifically, a distributed deep learning is executed as an iterative MapReduce computing on many Spark workers. Each Spark worker learns a partial deep model on a partition of the overall MBD, and a master deep model is then built by averaging the parameters of all partial models. This Spark-based framework speeds up the learning of deep models consisting of many hidden layers and millions of parameters. We use a context-aware activity recognition application with a real-world dataset containing millions of samples to validate our framework and assess its speedup effectiveness. version:1
arxiv-1602-07029 | Latent Skill Embedding for Personalized Lesson Sequence Recommendation | http://arxiv.org/abs/1602.07029 | id:1602.07029 author:Siddharth Reddy, Igor Labutov, Thorsten Joachims category:cs.LG cs.AI cs.CY  published:2016-02-23 summary:Students in online courses generate large amounts of data that can be used to personalize the learning process and improve quality of education. In this paper, we present the Latent Skill Embedding (LSE), a probabilistic model of students and educational content that can be used to recommend personalized sequences of lessons with the goal of helping students prepare for specific assessments. Akin to collaborative filtering for recommender systems, the algorithm does not require students or content to be described by features, but it learns a representation using access traces. We formulate this problem as a regularized maximum-likelihood embedding of students, lessons, and assessments from historical student-content interactions. An empirical evaluation on large-scale data from Knewton, an adaptive learning technology company, shows that this approach predicts assessment results competitively with benchmark models and is able to discriminate between lesson sequences that lead to mastery and failure. version:1
arxiv-1406-0597 | Maximum margin classifier working in a set of strings | http://arxiv.org/abs/1406.0597 | id:1406.0597 author:Hitoshi Koyano, Morihiro Hayashida, Tatsuya Akutsu category:stat.ML 62G20  68Q32  86W32  published:2014-06-03 summary:Numbers and numerical vectors account for a large portion of data. However, recently the amount of string data generated has increased dramatically. Consequently, classifying string data is a common problem in many fields. The most widely used approach to this problem is to convert strings into numerical vectors using string kernels and subsequently apply a support vector machine that works in a numerical vector space. However, this non-one-to-one conversion involves a loss of information and makes it impossible to evaluate, using probability theory, the generalization error of a learning machine, considering that the given data to train and test the machine are strings generated according to probability laws. In this study, we approach this classification problem by constructing a classifier that works in a set of strings. To evaluate the generalization error of such a classifier theoretically, probability theory for strings is required. Therefore, we first extend a limit theorem on the asymptotic behavior of a consensus sequence of strings, which is the counterpart of the mean of numerical vectors, as demonstrated in the probability theory on a metric space of strings developed by one of the authors and his colleague in a previous study [18]. Using the obtained result, we then demonstrate that our learning machine classifies strings in an asymptotically optimal manner. Furthermore, we demonstrate the usefulness of our machine in practical data analysis by applying it to predicting protein--protein interactions using amino acid sequences. version:3
arxiv-1602-07019 | Sentence Similarity Learning by Lexical Decomposition and Composition | http://arxiv.org/abs/1602.07019 | id:1602.07019 author:Zhiguo Wang, Haitao Mi, Abraham Ittycheriah category:cs.CL  published:2016-02-23 summary:Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task. version:1
arxiv-1602-07017 | A survey of sparse representation: algorithms and applications | http://arxiv.org/abs/1602.07017 | id:1602.07017 author:Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, David Zhang category:cs.CV cs.LG  published:2016-02-23 summary:Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with $l_0$-norm minimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization, sparse representation with $l_1$-norm minimization and sparse representation with $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithm-based optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at: http://www.yongxu.org/lunwen.html. version:1
arxiv-1602-06989 | Recovering the number of clusters in data sets with noise features using feature rescaling factors | http://arxiv.org/abs/1602.06989 | id:1602.06989 author:Renato Cordeiro de Amorim, Christian Hennig category:stat.ML cs.LG  published:2016-02-22 summary:In this paper we introduce three methods for re-scaling data sets aiming at improving the likelihood of clustering validity indexes to return the true number of spherical Gaussian clusters with additional noise features. Our method obtains feature re-scaling factors taking into account the structure of a given data set and the intuitive idea that different features may have different degrees of relevance at different clusters. We experiment with the Silhouette (using squared Euclidean, Manhattan, and the p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz and Hartigan indexes on data sets with spherical Gaussian clusters with and without noise features. We conclude that our methods indeed increase the chances of estimating the true number of clusters in a data set. version:1
arxiv-1602-06979 | Empath: Understanding Topic Signals in Large-Scale Text | http://arxiv.org/abs/1602.06979 | id:1602.06979 author:Ethan Fast, Binbin Chen, Michael Bernstein category:cs.CL cs.AI  published:2016-02-22 summary:Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like "bleed" and "punch" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC. version:1
arxiv-1602-06967 | Blind score normalization method for PLDA based speaker recognition | http://arxiv.org/abs/1602.06967 | id:1602.06967 author:Danila Doroshin, Nikolay Lubimov, Marina Nastasenko, Mikhail Kotov category:cs.CL cs.LG cs.SD I.5.1; I.5.2; G.3  published:2016-02-22 summary:Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art method for modeling $i$-vector space in speaker recognition task. However the performance degradation is observed if enrollment data size differs from one speaker to another. This paper presents a solution to such problem by introducing new PLDA scoring normalization technique. Normalization parameters are derived in a blind way, so that, unlike traditional \textit{ZT-norm}, no extra development data is required. Moreover, proposed method has shown to be optimal in terms of detection cost function. The experiments conducted on NIST SRE 2014 database demonstrate an improved accuracy in a mixed enrollment number condition. version:1
arxiv-1503-00949 | Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning | http://arxiv.org/abs/1503.00949 | id:1503.00949 author:Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid category:cs.CV  published:2015-03-03 summary:Object category localization is a challenging problem in computer vision. Standard supervised training requires bounding box annotations of object instances. This time-consuming annotation process is sidestepped in weakly supervised learning. In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations. We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images. Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations. This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features. We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior. We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach. version:3
arxiv-1511-06342 | Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning | http://arxiv.org/abs/1511.06342 | id:1511.06342 author:Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov category:cs.LG  published:2015-11-19 summary:The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods. version:4
arxiv-1602-06916 | Sparse Linear Regression via Generalized Orthogonal Least-Squares | http://arxiv.org/abs/1602.06916 | id:1602.06916 author:Abolfazl Hashemi, Haris Vikalo category:stat.ML cs.IT cs.LG math.IT  published:2016-02-22 summary:Sparse linear regression, which entails finding a sparse solution to an underdetermined system of linear equations, can formally be expressed as an $l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS) algorithm sequentially selects the features (i.e., columns of the coefficient matrix) to greedily find an approximate sparse solution. In this paper, a generalization of Orthogonal Least-Squares which relies on a recursive relation between the components of the optimal solution to select L features at each step and solve the resulting overdetermined system of equations is proposed. Simulation results demonstrate that the generalized OLS algorithm is computationally efficient and achieves performance superior to that of existing greedy algorithms broadly used in the literature. version:1
arxiv-1511-01574 | Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model | http://arxiv.org/abs/1511.01574 | id:1511.01574 author:Ciprian Chelba, Fernando Pereira category:cs.CL  published:2015-11-05 summary:We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data. Being able to train on held-out data is important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark, we are able to slightly improve on our previous results which use a different loss function, and employ leave-one-out training on a subset of the main training set. Surprisingly, an adjustment model with meta-features that discard all lexical information can perform as well as lexicalized meta-features. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources that are imbalanced in size, and of different degrees of relevance to the held-out and test data, taking into account the data source for a given skip-/n-gram feature and combining them for best performance on held-out/test data improves over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT setup, or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to a mismatched held-out set is probably the most attractive feature of the new estimation method for SNM LM. version:2
arxiv-1507-04540 | Learning to classify with possible sensor failures | http://arxiv.org/abs/1507.04540 | id:1507.04540 author:Tianpei Xie, Nasser M. Nasrabadi, Alfred O. Hero category:cs.LG cs.IT math.IT stat.ML  published:2015-07-16 summary:In this paper, we propose a general framework to learn a robust large-margin binary classifier when corrupt measurements, called anomalies, caused by sensor failure might be present in the training set. The goal is to minimize the generalization error of the classifier on non-corrupted measurements while controlling the false alarm rate associated with anomalous samples. By incorporating a non-parametric regularizer based on an empirical entropy estimator, we propose a Geometric-Entropy-Minimization regularized Maximum Entropy Discrimination (GEM-MED) method to learn to classify and detect anomalies in a joint manner. We demonstrate using simulated data and a real multimodal data set. Our GEM-MED method can yield improved performance over previous robust classification methods in terms of both classification accuracy and anomaly detection rate. version:3
arxiv-1602-03468 | 3D Pictorial Structures on RGB-D Data for Articulated Human Detection in Operating Rooms | http://arxiv.org/abs/1602.03468 | id:1602.03468 author:Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin, Nicolas Padoy category:cs.CV  published:2016-02-10 summary:Reliable human pose estimation (HPE) is essential to many clinical applications, such as surgical workflow analysis, radiation safety monitoring and human-robot cooperation. Proposed methods for the operating room (OR) rely either on foreground estimation using a multi-camera system, which is a challenge in real ORs due to color similarities and frequent illumination changes, or on wearable sensors or markers, which are invasive and therefore difficult to introduce in the room. Instead, we propose a novel approach based on Pictorial Structures (PS) and on RGB-D data, which can be easily deployed in real ORs. We extend the PS framework in two ways. First, we build robust and discriminative part detectors using both color and depth images. We also present a novel descriptor for depth images, called histogram of depth differences (HDD). Second, we extend PS to 3D by proposing 3D pairwise constraints and a new method for exact and tractable inference. Our approach is evaluated for pose estimation and clinician detection on a challenging RGB-D dataset recorded in a busy operating room during live surgeries. We conduct series of experiments to study the different part detectors in conjunction with the various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3D PS with RGB-D part detectors significantly improves the results in a visually challenging operating environment. version:2
arxiv-1602-07275 | Temporal Network Analysis of Literary Texts | http://arxiv.org/abs/1602.07275 | id:1602.07275 author:Sandra D. Prado, Silvio R. Dahmen, Ana L. C. Bazzan, Padraig Mac Carron, Ralph Kenna category:physics.soc-ph cs.CL  published:2016-02-22 summary:We study temporal networks of characters in literature focusing on "Alice's Adventures in Wonderland" (1865) by Lewis Carroll and the anonymous "La Chanson de Roland" (around 1100). The former, one of the most influential pieces of nonsense literature ever written, describes the adventures of Alice in a fantasy world with logic plays interspersed along the narrative. The latter, a song of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. during Charlemagne's campaign on the Iberian Peninsula. We apply methods recently developed by Taylor and coworkers \cite{Taylor+2015} to find time-averaged eigenvector centralities, Freeman indices and vitalities of characters. We show that temporal networks are more appropriate than static ones for studying stories, as they capture features that the time-independent approaches fail to yield. version:1
arxiv-1602-06872 | Principal Component Projection Without Principal Component Analysis | http://arxiv.org/abs/1602.06872 | id:1602.06872 author:Roy Frostig, Cameron Musco, Christopher Musco, Aaron Sidford category:cs.DS cs.LG stat.ML  published:2016-02-22 summary:We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a "smooth projection" onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision. version:1
arxiv-1511-06394 | Geodesics of learned representations | http://arxiv.org/abs/1511.06394 | id:1511.06394 author:Olivier J. Hénaff, Eero P. Simoncelli category:cs.CV cs.LG  published:2015-11-19 summary:We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a "representational geodesic"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations. version:4
arxiv-1602-06863 | Higher-Order Low-Rank Regression | http://arxiv.org/abs/1602.06863 | id:1602.06863 author:Guillaume Rabusseau, Hachem Kadri category:cs.LG  published:2016-02-22 summary:This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR outperforms multivariate and multilinear regression methods and is considerably faster than existing tensor methods. version:1
arxiv-1507-06145 | Dynamic Filtering of Time-Varying Sparse Signals via l1 Minimization | http://arxiv.org/abs/1507.06145 | id:1507.06145 author:Adam Charles, Aurele Balavoine, Christopher Rozell category:math.ST stat.ML stat.TH  published:2015-07-22 summary:Despite the importance of sparsity signal models and the increasing prevalence of high-dimensional streaming data, there are relatively few algorithms for dynamic filtering of time-varying sparse signals. Of the existing algorithms, fewer still provide strong performance guarantees. This paper examines two algorithms for dynamic filtering of sparse signals that are based on efficient l1 optimization methods. We first present an analysis for one simple algorithm (BPDN-DF) that works well when the system dynamics are known exactly. We then introduce a novel second algorithm (RWL1-DF) that is more computationally complex than BPDN-DF but performs better in practice, especially in the case where the system dynamics model is inaccurate. Robustness to model inaccuracy is achieved by using a hierarchical probabilistic data model and propagating higher-order statistics from the previous estimate (akin to Kalman filtering) in the sparse inference process. We demonstrate the properties of these algorithms on both simulated data as well as natural video sequences. Taken together, the algorithms presented in this paper represent the first strong performance analysis of dynamic filtering algorithms for time-varying sparse signals as well as state-of-the-art performance in this emerging application. version:2
arxiv-1602-07618 | From quantum foundations via natural language meaning to a theory of everything | http://arxiv.org/abs/1602.07618 | id:1602.07618 author:Bob Coecke category:cs.CL quant-ph  published:2016-02-22 summary:In this paper we argue for a paradigmatic shift from `reductionism' to `togetherness'. In particular, we show how interaction between systems in quantum theory naturally carries over to modelling how word meanings interact in natural language. Since meaning in natural language, depending on the subject domain, encompasses discussions within any scientific discipline, we obtain a template for theories such as social interaction, animal behaviour, and many others. version:1
arxiv-1602-06822 | Understanding Visual Concepts with Continuation Learning | http://arxiv.org/abs/1602.06822 | id:1602.06822 author:William F. Whitney, Michael Chang, Tejas Kulkarni, Joshua B. Tenenbaum category:cs.LG  published:2016-02-22 summary:We introduce a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units (corresponding to symbolic representations). We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games. version:1
arxiv-1512-03397 | The p-filter: multi-layer FDR control for grouped hypotheses | http://arxiv.org/abs/1512.03397 | id:1512.03397 author:Rina Foygel Barber, Aaditya Ramdas category:stat.ME stat.ML  published:2015-12-10 summary:In many practical applications of multiple hypothesis testing using the False Discovery Rate (FDR), the given hypotheses can be naturally partitioned into groups, and one may not only want to control the number of false discoveries (wrongly rejected null hypotheses), but also the number of falsely discovered groups of hypotheses (we say a group is falsely discovered if at least one hypothesis within that group is rejected, when in reality the group contains only nulls). In this paper, we introduce the p-filter, a procedure which unifies and generalizes the standard FDR procedure by Benjamini and Hochberg and global null testing procedure by Simes. We first prove that our proposed method can simultaneously control the overall FDR at the finest level (individual hypotheses treated separately) and the group FDR at coarser levels (when such groups are user-specified). We then generalize the p-filter procedure even further to handle multiple partitions of hypotheses, since that might be natural in many applications. For example, in neuroscience experiments, we may have a hypothesis for every (discretized) location in the brain, and at every (discretized) timepoint: does the stimulus correlate with activity in location x at time t after the stimulus was presented? In this setting, one might want to group hypotheses by location and by time. Importantly, our procedure can handle multiple partitions which are nonhierarchical (i.e. one partition may arrange p-values by voxel, and another partition arranges them by time point; neither one is nested inside the other). We prove that our procedure controls FDR simultaneously across these multiple lay- ers, under assumptions that are standard in the literature: we do not need the hypotheses to be independent, but require a nonnegative dependence condition known as PRDS. version:2
arxiv-1602-06797 | Semi-supervised Clustering for Short Text via Deep Representation Learning | http://arxiv.org/abs/1602.06797 | id:1602.06797 author:Zhiguo Wang, Haitao Mi, Abraham Ittycheriah category:cs.CL  published:2016-02-22 summary:In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the k-means clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed. Experimental results on four datasets show that our method works significantly better than several other text clustering methods. version:1
arxiv-1602-07280 | A Statistical Model for Stroke Outcome Prediction and Treatment Planning | http://arxiv.org/abs/1602.07280 | id:1602.07280 author:Abhishek Sengupta, Vaibhav Rajan, Sakyajit Bhattacharya, G R K Sarma category:stat.AP cs.LG  published:2016-02-22 summary:Stroke is a major cause of mortality and long--term disability in the world. Predictive outcome models in stroke are valuable for personalized treatment, rehabilitation planning and in controlled clinical trials. In this paper we design a new model to predict outcome in the short-term, the putative therapeutic window for several treatments. Our regression-based model has a parametric form that is designed to address many challenges common in medical datasets like highly correlated variables and class imbalance. Empirically our model outperforms the best--known previous models in predicting short--term outcomes and in inferring the most effective treatments that improve outcome. version:1
arxiv-1602-06746 | Convexification of Learning from Constraints | http://arxiv.org/abs/1602.06746 | id:1602.06746 author:Iaroslav Shcherbatyi, Bjoern Andres category:cs.LG math.OC stat.ML  published:2016-02-22 summary:Regularized empirical risk minimization with constrained labels (in contrast to fixed labels) is a remarkably general abstraction of learning. For common loss and regularization functions, this optimization problem assumes the form of a mixed integer program (MIP) whose objective function is non-convex. In this form, the problem is resistant to standard optimization techniques. We construct MIPs with the same solutions whose objective functions are convex. Specifically, we characterize the tightest convex extension of the objective function, given by the Legendre-Fenchel biconjugate. Computing values of this tightest convex extension is NP-hard. However, by applying our characterization to every function in an additive decomposition of the objective function, we obtain a class of looser convex extensions that can be computed efficiently. For some decompositions, common loss and regularization functions, we derive a closed form. version:1
arxiv-1602-06709 | Distributed Deep Learning Using Synchronous Stochastic Gradient Descent | http://arxiv.org/abs/1602.06709 | id:1602.06709 author:Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj Kalamkar, Bharat Kaul, Pradeep Dubey category:cs.DC cs.LG  published:2016-02-22 summary:We design and implement a distributed multinode synchronous SGD algorithm, without altering hyper parameters, or compressing data, or altering algorithmic behavior. We perform a detailed analysis of scaling, and identify optimal design points for different networks. We demonstrate scaling of CNNs on 100s of nodes, and present what we believe to be record training throughputs. A 512 minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch VGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64 node cluster. We also demonstrate the generality of our approach via best-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt to democratize deep-learning by training on an Ethernet based AWS cluster and show ~14X scaling on 16 nodes. version:1
arxiv-1602-06701 | Inference Networks for Sequential Monte Carlo in Graphical Models | http://arxiv.org/abs/1602.06701 | id:1602.06701 author:Brooks Paige, Frank Wood category:stat.ML  published:2016-02-22 summary:We introduce a new approach for amortizing inference in directed graphical models by learning heuristic approximations to stochastic inverses, designed specifically for use as proposal distributions in sequential Monte Carlo methods. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation to the distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output of these networks can be used as automatically-learned high-quality proposal distributions to accelerate sequential Monte Carlo across a diverse range of problem settings. version:1
arxiv-1509-04438 | Regular expressions for decoding of neural network outputs | http://arxiv.org/abs/1509.04438 | id:1509.04438 author:Tobias Strauß, Gundram Leifert, Tobias Grüning, Roger Labahn category:cs.NE 49L20  90C39  82C32  published:2015-09-15 summary:This article proposes a convenient tool for decoding the output of neural networks trained by Connectionist Temporal Classification (CTC) for handwritten text recognition. We use regular expressions to describe the complex structures expected in the writing. The corresponding finite automata are employed to build a decoder. We analyze theoretically which calculations are relevant and which can be avoided. A great speed-up results from an approximation. We conclude that the approximation most likely fails if the regular expression does not match the ground truth which is not harmful for many applications since the low probability will be even underestimated. The proposed decoder is very efficient compared to other decoding methods. The variety of applications reaches from information retrieval to full text recognition. We refer to applications where we integrated the proposed decoder successfully. version:2
arxiv-1602-06697 | Correlation Hashing Network for Efficient Cross-Modal Retrieval | http://arxiv.org/abs/1602.06697 | id:1602.06697 author:Yue Cao, Mingsheng Long, Jianmin Wang category:cs.CV  published:2016-02-22 summary:Due to the storage and retrieval efficiency, hashing has been widely deployed to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which improves the quality of hash coding by exploiting the semantic correlation across different modalities, has received increasing attention recently. For most existing cross-modal hashing methods, an object is first represented as of vector of hand-crafted or machine-learned features, followed by another separate quantization step that generates binary codes. However, suboptimal hash coding may be produced, because the quantization error is not statistically minimized and the feature representation is not optimally compatible with the binary coding. In this paper, we propose a novel Correlation Hashing Network (CHN) architecture for cross-modal hashing, in which we jointly learn good data representation tailored to hash coding and formally control the quantization error. The CHN model is a hybrid deep architecture constituting four key components: (1) an image network with multiple convolution-pooling layers to extract good image representations, and a text network with several fully-connected layers to extract good text representations; (2) a fully-connected hashing layer to generate modality-specific compact hash codes; (3) a squared cosine loss layer for capturing both cross-modal correlation and within-modal correlation; and (4) a new cosine quantization loss for controlling the quality of the binarized hash codes. Extensive experiments on standard cross-modal retrieval datasets show the proposed CHN model yields substantial boosts over latest state-of-the-art hashing methods. version:1
arxiv-1508-05550 | MultiView Diffusion Maps | http://arxiv.org/abs/1508.05550 | id:1508.05550 author:Ofir Lindenbaum, Arie Yeredor, Moshe Salhov, Amir Averbuch category:cs.LG stat.ML  published:2015-08-23 summary:In this study we consider learning a reduced dimensionality representation from datasets obtained under multiple views. Such multiple views of datasets can be obtained, for example, when the same underlying process is observed using several different modalities, or measured with different instrumentation. Our goal is to effectively exploit the availability of such multiple views for various purposes, such as non-linear embedding, manifold learning, spectral clustering, anomaly detection and non-linear system identification. Our proposed method exploits the intrinsic relation within each view, as well as the mutual relations between views. We do this by defining a cross-view model, in which an implied Random Walk process between objects is restrained to hop between the different views. Our method is robust to scaling of each dataset, and is insensitive to small structural changes in the data. Within this framework, we define new diffusion distances and analyze the spectra of the implied kernels. We demonstrate the applicability of the proposed approach on both artificial and real data sets. version:4
arxiv-1602-06687 | An Effective and Efficient Approach for Clusterability Evaluation | http://arxiv.org/abs/1602.06687 | id:1602.06687 author:Margareta Ackerman, Andreas Adolfsson, Naomi Brownstein category:cs.LG stat.ML  published:2016-02-22 summary:Clustering is an essential data mining tool that aims to discover inherent cluster structure in data. As such, the study of clusterability, which evaluates whether data possesses such structure, is an integral part of cluster analysis. Yet, despite their central role in the theory and application of clustering, current notions of clusterability fall short in two crucial aspects that render them impractical; most are computationally infeasible and others fail to classify the structure of real datasets. In this paper, we propose a novel approach to clusterability evaluation that is both computationally efficient and successfully captures the structure of real data. Our method applies multimodality tests to the (one-dimensional) set of pairwise distances based on the original, potentially high-dimensional data. We present extensive analyses of our approach for both the Dip and Silverman multimodality tests on real data as well as 17,000 simulations, demonstrating the success of our approach as the first practical notion of clusterability. version:1
arxiv-1511-02014 | Population size predicts lexical diversity, but so does the mean sea level - why it is important to correctly account for the structure of temporal data | http://arxiv.org/abs/1511.02014 | id:1511.02014 author:Alexander Koplenig, Carolin Mueller-Spitzer category:cs.CL  published:2015-11-06 summary:In order to demonstrate why it is important to correctly account for the (serial dependent) structure of temporal data, we document an apparently spectacular relationship between population size and lexical diversity: for five out of seven investigated languages, there is a strong relationship between population size and lexical diversity of the primary language in this country. We show that this relationship is the result of a misspecified model that does not consider the temporal aspect of the data by presenting a similar but nonsensical relationship between the global annual mean sea level and lexical diversity. Given the fact that in the recent past, several studies were published that present surprising links between different economic, cultural, political and (socio-)demographical variables on the one hand and cultural or linguistic characteristics on the other hand, but seem to suffer from exactly this problem, we explain the cause of the misspecification and show that it has profound consequences. We demonstrate how simple transformation of the time series can often solve problems of this type and argue that the evaluation of the plausibility of a relationship is important in this context. We hope that our paper will help both researchers and reviewers to understand why it is important to use special models for the analysis of data with a natural temporal ordering. version:2
arxiv-1511-07289 | Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) | http://arxiv.org/abs/1511.07289 | id:1511.07289 author:Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter category:cs.LG  published:2015-11-23 summary:We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network. version:5
arxiv-1602-06662 | Orthogonal RNNs and Long-Memory Tasks | http://arxiv.org/abs/1602.06662 | id:1602.06662 author:Mikael Henaff, Arthur Szlam, Yann LeCun category:cs.NE cs.AI cs.LG stat.ML  published:2016-02-22 summary:Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices. version:1
arxiv-1602-02343 | Eye-CU: Sleep Pose Classification for Healthcare using Multimodal Multiview Data | http://arxiv.org/abs/1602.02343 | id:1602.02343 author:Carlos Torres, Victor Fragoso, Scott D. Hammond, Jeffrey C. Fried, B. S. Manjunath category:cs.CV  published:2016-02-07 summary:Manual analysis of body poses of bed-ridden patients requires staff to continuously track and record patient poses. Two limitations in the dissemination of pose-related therapies are scarce human resources and unreliable automated systems. This work addresses these issues by introducing a new method and a new system for robust automated classification of sleep poses in an Intensive Care Unit (ICU) environment. The new method, coupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM) data and finds the set of modality trust values that minimizes the difference between expected and estimated labels. The new system, Eye-CU, is an affordable multi-sensor modular system for unobtrusive data collection and analysis in healthcare. Experimental results indicate that the performance of cc-LS matches the performance of existing methods in ideal scenarios. This method outperforms the latest techniques in challenging scenarios by 13% for those with poor illumination and by 70% for those with both poor illumination and occlusions. Results also show that a reduced Eye-CU configuration can classify poses without pressure information with only a slight drop in its performance. version:2
arxiv-1602-06654 | Structured Learning of Binary Codes with Column Generation | http://arxiv.org/abs/1602.06654 | id:1602.06654 author:Guosheng Lin, Fayao Liu, Chunhua Shen, Jianxin Wu, Heng Tao Shen category:cs.LG  published:2016-02-22 summary:Hashing methods aim to learn a set of hash functions which map the original features to compact binary codes with similarity preserving in the Hamming space. Hashing has proven a valuable tool for large-scale information retrieval. We propose a column generation based binary code learning framework for data-dependent hash function learning. Given a set of triplets that encode the pairwise similarity comparison information, our column generation based method learns hash functions that preserve the relative comparison relations within the large-margin learning framework. Our method iteratively learns the best hash functions during the column generation procedure. Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest---multivariate performance measures such as the AUC and NDCG. Our column generation based method can be further generalized from the triplet loss to a general structured learning based framework that allows one to directly optimize multivariate performance measures. For optimizing general ranking measures, the resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. We use a combination of column generation and cutting-plane techniques to solve the optimization problem. To speed-up the training we further explore stage-wise training and propose to use a simplified NDCG loss for efficient inference. We demonstrate the generality of our method by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods. version:1
arxiv-1602-06647 | Planogram Compliance Checking Based on Detection of Recurring Patterns | http://arxiv.org/abs/1602.06647 | id:1602.06647 author:Song Liu, Wanqing Li, Stephen Davis, Christian Ritz, Hongda Tian category:cs.CV  published:2016-02-22 summary:In this paper, a novel method for automatic planogram compliance checking in retail chains is proposed without requiring product template images for training. Product layout is extracted from an input image by means of unsupervised recurring pattern detection and matched via graph matching with the expected product layout specified by a planogram to measure the level of compliance. A divide and conquer strategy is employed to improve the speed. Specifically, the input image is divided into several regions based on the planogram. Recurring patterns are detected in each region respectively and then merged together to estimate the product layout. Experimental results on real data have verified the efficacy of the proposed method. Compared with a template-based method, higher accuracies are achieved by the proposed method over a wide range of products. version:1
arxiv-1602-06645 | Creating Simplified 3D Models with High Quality Textures | http://arxiv.org/abs/1602.06645 | id:1602.06645 author:Song Liu, Wanqing Li, Philip Ogunbona, Yang-Wai Chow category:cs.GR cs.CV  published:2016-02-22 summary:This paper presents an extension to the KinectFusion algorithm which allows creating simplified 3D models with high quality RGB textures. This is achieved through (i) creating model textures using images from an HD RGB camera that is calibrated with Kinect depth camera, (ii) using a modified scheme to update model textures in an asymmetrical colour volume that contains a higher number of voxels than that of the geometry volume, (iii) simplifying dense polygon mesh model using quadric-based mesh decimation algorithm, and (iv) creating and mapping 2D textures to every polygon in the output 3D model. The proposed method is implemented in real-time by means of GPU parallel processing. Visualization via ray casting of both geometry and colour volumes provides users with a real-time feedback of the currently scanned 3D model. Experimental results show that the proposed method is capable of keeping the model texture quality even for a heavily decimated model and that, when reconstructing small objects, photorealistic RGB textures can still be reconstructed. version:1
arxiv-1601-04650 | Statistical Mechanics of High-Dimensional Inference | http://arxiv.org/abs/1601.04650 | id:1601.04650 author:Madhu Advani, Surya Ganguli category:stat.ML cond-mat.dis-nn cond-mat.stat-mech math.ST q-bio.QM stat.TH  published:2016-01-18 summary:To model modern large-scale datasets, we need efficient algorithms to infer a set of $P$ unknown model parameters from $N$ noisy measurements. What are fundamental limits on the accuracy of parameter inference, given finite signal-to-noise ratios, limited measurements, prior information, and computational tractability requirements? How can we combine prior information with measurements to achieve these limits? Classical statistics gives incisive answers to these questions as the measurement density $\alpha = \frac{N}{P}\rightarrow \infty$. However, these classical results are not relevant to modern high-dimensional inference problems, which instead occur at finite $\alpha$. We formulate and analyze high-dimensional inference as a problem in the statistical physics of quenched disorder. Our analysis uncovers fundamental limits on the accuracy of inference in high dimensions, and reveals that widely cherished inference algorithms like maximum likelihood (ML) and maximum-a posteriori (MAP) inference cannot achieve these limits. We further find optimal, computationally tractable algorithms that can achieve these limits. Intriguingly, in high dimensions, these optimal algorithms become computationally simpler than MAP and ML, while still outperforming them. For example, such optimal algorithms can lead to as much as a 20% reduction in the amount of data to achieve the same performance relative to MAP. Moreover, our analysis reveals simple relations between optimal high dimensional inference and low dimensional scalar Bayesian inference, insights into the nature of generalization and predictive power in high dimensions, information theoretic limits on compressed sensing, phase transitions in quadratic inference, and connections to central mathematical objects in convex optimization theory and random matrix theory. version:2
arxiv-1511-04383 | Handling Class Imbalance in Link Prediction using Learning to Rank Techniques | http://arxiv.org/abs/1511.04383 | id:1511.04383 author:Bopeng Li, Sougata Chaudhuri, Ambuj Tewari category:stat.ML cs.LG cs.SI  published:2015-11-13 summary:We consider the link prediction problem in a partially observed network, where the objective is to make predictions in the unobserved portion of the network. Many existing methods reduce link prediction to binary classification problem. However, the dominance of absent links in real world networks makes misclassification error a poor performance metric. Instead, researchers have argued for using ranking performance measures, like AUC, AP and NDCG, for evaluation. Our main contribution is to recast the link prediction problem as a learning to rank problem and use effective learning to rank techniques directly during training. This is in contrast to existing work that uses ranking measures only during evaluation. Our approach is able to deal with the class imbalance problem by using effective, scalable learning to rank techniques during training. Furthermore, our approach allows us to combine network topology and node features. As a demonstration of our general approach, we develop a link prediction method by optimizing the cross-entropy surrogate, originally used in the popular ListNet ranking algorithm. We conduct extensive experiments on publicly available co-authorship, citation and metabolic networks to demonstrate the merits of our method. version:2
arxiv-1511-03225 | On the geometry of output-code multi-class learning | http://arxiv.org/abs/1511.03225 | id:1511.03225 author:Maria Florina Balcan, Travis Dick, Yishay Mansour category:cs.LG  published:2015-11-10 summary:We provide a new perspective on the popular multi-class algorithmic techniques one-vs-all and (error correcting) output-codes. We show that is that in cases where they are successful (at learning from labeled data), these techniques implicitly assume structure on how the classes are related. We show that by making that structure explicit, we can design algorithms to recover the classes based on limited labeled data. We provide results for commonly studied cases where the codewords of the classes are well separated: learning a linear one-vs-all classifier for data on the unit ball and learning a linear error correcting output code when the Hamming distance between the codewords is large (at least $d+1$ in a $d$-dimensional problem). We additionally consider the more challenging case where the codewords are not well separated, but satisfy a boundary features condition. version:2
arxiv-1511-03962 | Document Context Language Models | http://arxiv.org/abs/1511.03962 | id:1511.03962 author:Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, Jacob Eisenstein category:cs.CL cs.LG stat.ML  published:2015-11-12 summary:Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence. version:4
arxiv-1602-06577 | 2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search | http://arxiv.org/abs/1602.06577 | id:1602.06577 author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:stat.ML cs.DS cs.LG  published:2016-02-21 summary:The method of random projections has become a standard tool for machine learning, data mining, and search with massive data at Web scale. The effective use of random projections requires efficient coding schemes for quantizing (real-valued) projected data into integers. In this paper, we focus on a simple 2-bit coding scheme. In particular, we develop accurate nonlinear estimators of data similarity based on the 2-bit strategy. This work will have important practical applications. For example, in the task of near neighbor search, a crucial step (often called re-ranking) is to compute or estimate data similarities once a set of candidate data points have been identified by hash table techniques. This re-ranking step can take advantage of the proposed coding scheme and estimator. As a related task, in this paper, we also study a simple uniform quantization scheme for the purpose of building hash tables with projected data. Our analysis shows that typically only a small number of bits are needed. For example, when the target similarity level is high, 2 or 3 bits might be sufficient. When the target similarity level is not so high, it is preferable to use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good choice for the task of sublinear time approximate near neighbor search via hash tables. Combining these results, we conclude that 2-bit random projections should be recommended for approximate near neighbor search and similarity estimation. Extensive experimental results are provided. version:1
arxiv-1602-06566 | Interactive Storytelling over Document Collections | http://arxiv.org/abs/1602.06566 | id:1602.06566 author:Dipayan Maiti, Mohammad Raihanul Islam, Scotland Leman, Naren Ramakrishnan category:cs.AI cs.LG stat.ML  published:2016-02-21 summary:Storytelling algorithms aim to 'connect the dots' between disparate documents by linking starting and ending documents through a series of intermediate documents. Existing storytelling algorithms are based on notions of coherence and connectivity, and thus the primary way by which users can steer the story construction is via design of suitable similarity functions. We present an alternative approach to storytelling wherein the user can interactively and iteratively provide 'must use' constraints to preferentially support the construction of some stories over others. The three innovations in our approach are distance measures based on (inferred) topic distributions, the use of constraints to define sets of linear inequalities over paths, and the introduction of slack and surplus variables to condition the topic distribution to preferentially emphasize desired terms over others. We describe experimental results to illustrate the effectiveness of our interactive storytelling approach over multiple text datasets. version:1
arxiv-1602-06564 | Automatic Building Extraction in Aerial Scenes Using Convolutional Networks | http://arxiv.org/abs/1602.06564 | id:1602.06564 author:Jiangye Yuan category:cs.CV  published:2016-02-21 summary:Automatic building extraction from aerial and satellite imagery is highly challenging due to extremely large variations of building appearances. To attack this problem, we design a convolutional network with a final stage that integrates activations from multiple preceding stages for pixel-wise prediction, and introduce the signed distance function of building boundaries as the output representation, which has an enhanced representation power. We leverage abundant building footprint data available from geographic information systems (GIS) to compile training data. The trained network achieves superior performance on datasets that are significantly larger and more complex than those used in prior work, demonstrating that the proposed method provides a promising and scalable solution for automating this labor-intensive task. version:1
arxiv-1409-4573 | Non-linear Causal Inference using Gaussianity Measures | http://arxiv.org/abs/1409.4573 | id:1409.4573 author:Daniel Hernández-Lobato, Pablo Morales-Mombiela, David Lopez-Paz, Alberto Suárez category:stat.ML  published:2014-09-16 summary:We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference. version:3
arxiv-1511-05467 | Predictive Entropy Search for Multi-objective Bayesian Optimization | http://arxiv.org/abs/1511.05467 | id:1511.05467 author:Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Amar Shah, Ryan P. Adams category:stat.ML  published:2015-11-17 summary:We present PESMO, a Bayesian method for identifying the Pareto set of multi-objective optimization problems, when the functions are expensive to evaluate. The central idea of PESMO is to choose evaluation points so as to maximally reduce the entropy of the posterior distribution over the Pareto set. Critically, the PESMO multi-objective acquisition function can be decomposed as a sum of objective-specific acquisition functions, which enables the algorithm to be used in \emph{decoupled} scenarios in which the objectives can be evaluated separately and perhaps with different costs. This decoupling capability also makes it possible to identify difficult objectives that require more evaluations. PESMO also offers gains in efficiency, as its cost scales linearly with the number of objectives, in comparison to the exponential cost of other methods. We compare PESMO with other related methods for multi-objective Bayesian optimization on synthetic and real-world problems. The results show that PESMO produces better recommendations with a smaller number of evaluations of the objectives, and that a decoupled evaluation can lead to improvements in performance, particularly when the number of objectives is large. version:3
arxiv-1602-06539 | Determining the best attributes for surveillance video keywords generation | http://arxiv.org/abs/1602.06539 | id:1602.06539 author:Liangchen Liu, Arnold Wiliem, Shaokang Chen, Kun Zhao, Brian C. Lovell category:cs.LG cs.AI  published:2016-02-21 summary:Automatic video keyword generation is one of the key ingredients in reducing the burden of security officers in analyzing surveillance videos. Keywords or attributes are generally chosen manually based on expert knowledge of surveillance. Most existing works primarily aim at either supervised learning approaches relying on extensive manual labelling or hierarchical probabilistic models that assume the features are extracted using the bag-of-words approach; thus limiting the utilization of the other features. To address this, we turn our attention to automatic attribute discovery approaches. However, it is not clear which automatic discovery approach can discover the most meaningful attributes. Furthermore, little research has been done on how to compare and choose the best automatic attribute discovery methods. In this paper, we propose a novel approach, based on the shared structure exhibited amongst meaningful attributes, that enables us to compare between different automatic attribute discovery approaches.We then validate our approach by comparing various attribute discovery methods such as PiCoDeS on two attribute datasets. The evaluation shows that our approach is able to select the automatic discovery approach that discovers the most meaningful attributes. We then employ the best discovery approach to generate keywords for videos recorded from a surveillance system. This work shows it is possible to massively reduce the amount of manual work in generating video keywords without limiting ourselves to a particular video feature descriptor. version:1
arxiv-1602-06531 | Multi-task and Lifelong Learning of Kernels | http://arxiv.org/abs/1602.06531 | id:1602.06531 author:Anastasia Pentina, Shai Ben-David category:stat.ML cs.LG  published:2016-02-21 summary:We consider a problem of learning kernels for use in SVM classification in the multi-task and lifelong scenarios and provide generalization bounds on the error of a large margin classifier. Our results show that, under mild conditions on the family of kernels used for learning, solving several related tasks simultaneously is beneficial over single task learning. In particular, as the number of observed tasks grows, assuming that in the considered family of kernels there exists one that yields low approximation error on all tasks, the overhead associated with learning such a kernel vanishes and the complexity converges to that of learning when this good kernel is given to the learner. version:1
arxiv-1602-06522 | Machine learning meets network science: dimensionality reduction for fast and efficient embedding of networks in the hyperbolic space | http://arxiv.org/abs/1602.06522 | id:1602.06522 author:Josephine Maria Thomas, Alessandro Muscoloni, Sara Ciucci, Ginestra Bianconi, Carlo Vittorio Cannistraci category:cond-mat.dis-nn cs.AI cs.LG  published:2016-02-21 summary:Complex network topologies and hyperbolic geometry seem specularly connected, and one of the most fascinating and challenging problems of recent complex network theory is to map a given network to its hyperbolic space. The Popularity Similarity Optimization (PSO) model represents - at the moment - the climax of this theory. It suggests that the trade-off between node popularity and similarity is a mechanism to explain how complex network topologies emerge - as discrete samples - from the continuous world of hyperbolic geometry. The hyperbolic space seems appropriate to represent real complex networks. In fact, it preserves many of their fundamental topological properties, and can be exploited for real applications such as, among others, link prediction and community detection. Here, we observe for the first time that a topological-based machine learning class of algorithms - for nonlinear unsupervised dimensionality reduction - can directly approximate the network's node angular coordinates of the hyperbolic model into a two-dimensional space, according to a similar topological organization that we named angular coalescence. On the basis of this phenomenon, we propose a new class of algorithms that offers fast and accurate coalescent embedding of networks in the hyperbolic space even for graphs with thousands of nodes. version:1
arxiv-1603-00050 | Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of a Batted Ball | http://arxiv.org/abs/1603.00050 | id:1603.00050 author:Glenn Healey category:stat.AP cs.LG I.2.6  published:2016-02-21 summary:We present an algorithm for learning the intrinsic value of a batted ball in baseball. This work addresses the fundamental problem of separating the value of a batted ball at contact from factors such as the defense, weather, and ballpark that can affect its observed outcome. The algorithm uses a Bayesian model to construct a continuous mapping from a vector of batted ball parameters to an intrinsic measure defined as the expected value of a linear weights representation for run value. A kernel method is used to build nonparametric estimates for the component probability density functions in Bayes theorem from a set of over one hundred thousand batted ball measurements recorded by the HITf/x system during the 2014 major league baseball (MLB) season. Cross-validation is used to determine the optimal vector of smoothing parameters for the density estimates. Properties of the mapping are visualized by considering reduced-dimension subsets of the batted ball parameter space. We use the mapping to derive statistics for intrinsic quality of contact for batters and pitchers which have the potential to improve the accuracy of player models and forecasting systems. We also show that the new approach leads to a simple automated measure of contact-adjusted defense and provides insight into the impact of environmental variables on batted balls. version:1
arxiv-1602-03990 | Efficient functional ANOVA through wavelet-domain Markov groves | http://arxiv.org/abs/1602.03990 | id:1602.03990 author:Li Ma, Jacopo Soriano category:stat.ME stat.CO stat.ML  published:2016-02-12 summary:We introduce a wavelet-domain functional analysis of variance (fANOVA) method based on a Bayesian hierarchical model. The factor effects are modeled through a spike-and-slab mixture at each location-scale combination along with a normal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. A graphical model called the Markov grove (MG) is designed to jointly model the spike-and-slab statuses at all location-scale combinations, which incorporates the clustering of each factor effect in the wavelet-domain thereby allowing borrowing of strength across location and scale. The posterior of this NIG-MG model is analytically available through a pyramid algorithm of the same computational complexity as Mallat's pyramid algorithm for discrete wavelet transform, i.e., linear in both the number of observations and the number of locations. Posterior probabilities of factor contributions can also be computed through pyramid recursion, and exact samples from the posterior can be drawn without MCMC. We investigate the performance of our method through extensive simulation and show that it outperforms existing wavelet-domain fANOVA methods in a variety of common settings. We apply the method to analyzing the orthosis data. version:2
arxiv-1602-06489 | Distributed Private Online Learning for Social Big Data Computing over Data Center Networks | http://arxiv.org/abs/1602.06489 | id:1602.06489 author:Chencheng Li, Pan Zhou, Yingxue Zhou, Kaigui Bian, Tao Jiang, Susanto Rahardja category:cs.DC cs.LG cs.SI  published:2016-02-21 summary:With the rapid growth of Internet technologies, cloud computing and social networks have become ubiquitous. An increasing number of people participate in social networks and massive online social data are obtained. In order to exploit knowledge from copious amounts of data obtained and predict social behavior of users, we urge to realize data mining in social networks. Almost all online websites use cloud services to effectively process the large scale of social data, which are gathered from distributed data centers. These data are so large-scale, high-dimension and widely distributed that we propose a distributed sparse online algorithm to handle them. Additionally, privacy-protection is an important point in social networks. We should not compromise the privacy of individuals in networks, while these social data are being learned for data mining. Thus we also consider the privacy problem in this article. Our simulations shows that the appropriate sparsity of data would enhance the performance of our algorithm and the privacy-preserving method does not significantly hurt the performance of the proposed algorithm. version:1
arxiv-1509-03789 | Bio-Inspired Human Action Recognition using Hybrid Max-Product Neuro-Fuzzy Classifier and Quantum-Behaved PSO | http://arxiv.org/abs/1509.03789 | id:1509.03789 author:Bardia Yousefi, Chu Kiong Loo category:cs.AI cs.CV  published:2015-09-13 summary:Studies on computational neuroscience through functional magnetic resonance imaging (fMRI) and following biological inspired system stated that human action recognition in the brain of mammalian leads two distinct pathways in the model, which are specialized for analysis of motion (optic flow) and form information. Principally, we have defined a novel and robust form features applying active basis model as form extractor in form pathway in the biological inspired model. An unbalanced synergetic neural net-work classifies shapes and structures of human objects along with tuning its attention parameter by quantum particle swarm optimization (QPSO) via initiation of Centroidal Voronoi Tessellations. These tools utilized and justified as strong tools for following biological system model in form pathway. But the final decision has done by combination of ultimate outcomes of both pathways via fuzzy inference which increases novality of proposed model. Combination of these two brain pathways is done by considering each feature sets in Gaussian membership functions with fuzzy product inference method. Two configurations have been proposed for form pathway: applying multi-prototype human action templates using two time synergetic neural network for obtaining uniform template regarding each actions, and second scenario that it uses abstracting human action in four key-frames. Experimental results showed promising accuracy performance on different datasets (KTH and Weizmann). version:2
arxiv-1509-01277 | A Dataset for Improved RGBD-based Object Detection and Pose Estimation for Warehouse Pick-and-Place | http://arxiv.org/abs/1509.01277 | id:1509.01277 author:Colin Rennie, Rahul Shome, Kostas E. Bekris, Alberto F. De Souza category:cs.CV cs.RO 68T40  68T45  68T01  published:2015-09-03 summary:An important logistics application of robotics involves manipulators that pick-and-place objects placed in warehouse shelves. A critical aspect of this task corre- sponds to detecting the pose of a known object in the shelf using visual data. Solving this problem can be assisted by the use of an RGB-D sensor, which also provides depth information beyond visual data. Nevertheless, it remains a challenging problem since multiple issues need to be addressed, such as low illumination inside shelves, clutter, texture-less and reflective objects as well as the limitations of depth sensors. This paper provides a new rich data set for advancing the state-of-the-art in RGBD- based 3D object pose estimation, which is focused on the challenges that arise when solving warehouse pick- and-place tasks. The publicly available data set includes thousands of images and corresponding ground truth data for the objects used during the first Amazon Picking Challenge at different poses and clutter conditions. Each image is accompanied with ground truth information to assist in the evaluation of algorithms for object detection. To show the utility of the data set, a recent algorithm for RGBD-based pose estimation is evaluated in this paper. Based on the measured performance of the algorithm on the data set, various modifications and improvements are applied to increase the accuracy of detection. These steps can be easily applied to a variety of different methodologies for object pose detection and improve performance in the domain of warehouse pick-and-place. version:2
arxiv-1602-06439 | Context-guided diffusion for label propagation on graphs | http://arxiv.org/abs/1602.06439 | id:1602.06439 author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV  published:2016-02-20 summary:Existing approaches for diffusion on graphs, e.g., for label propagation, are mainly focused on isotropic diffusion, which is induced by the commonly-used graph Laplacian regularizer. Inspired by the success of diffusivity tensors for anisotropic diffusion in image processing, we presents anisotropic diffusion on graphs and the corresponding label propagation algorithm. We develop positive definite diffusivity operators on the vector bundles of Riemannian manifolds, and discretize them to diffusivity operators on graphs. This enables us to easily define new robust diffusivity operators which significantly improve semi-supervised learning performance over existing diffusion algorithms. version:1
arxiv-1602-06431 | Burstiness Scale: a highly parsimonious model for characterizing random series of events | http://arxiv.org/abs/1602.06431 | id:1602.06431 author:Rodrigo A S Alves, Renato Assunção, Pedro O S Vaz de Melo category:stat.ML cs.SI H.2.8; G.3  published:2016-02-20 summary:The problem to accurately and parsimoniously characterize random series of events (RSEs) present in the Web, such as e-mail conversations or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events and periods of inactivities. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a highly parsimonious way to characterize general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstyness scale, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications. version:1
arxiv-1602-02338 | Stratified Bayesian Optimization | http://arxiv.org/abs/1602.02338 | id:1602.02338 author:Saul Toscano-Palmerin, Peter I. Frazier category:cs.LG math.OC stat.ML  published:2016-02-07 summary:We consider derivative-free black-box global optimization of expensive noisy functions, when most of the randomness in the objective is produced by a few influential scalar random inputs. We present a new Bayesian global optimization algorithm, called Stratified Bayesian Optimization (SBO), which uses this strong dependence to improve performance. Our algorithm is similar in spirit to stratification, a technique from simulation, which uses strong dependence on a categorical representation of the random input to reduce variance. We demonstrate in numerical experiments that SBO outperforms state-of-the-art Bayesian optimization benchmarks that do not leverage this dependence. version:2
arxiv-1503-06866 | Study of all the periods of a Neuronal Recurrence Equation | http://arxiv.org/abs/1503.06866 | id:1503.06866 author:Serge Alain Ebélé, Renè Ndoundam category:cs.NE 92B20 F.1.1  published:2015-03-23 summary:We characterize the structure of the periods of a neuronal recurrence equation. Firstly, we give a characterization of k-chains in 0-1 periodic sequences. Secondly, we characterize the periods of all cycles of some neuronal recurrence equation. Thirdly, we explain how these results can be used to deduce the existence of the generalized period-halving bifurcation. version:4
arxiv-1602-06359 | Text Matching as Image Recognition | http://arxiv.org/abs/1602.06359 | id:1602.06359 author:Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, Xueqi Cheng category:cs.CL cs.AI  published:2016-02-20 summary:Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines. version:1
