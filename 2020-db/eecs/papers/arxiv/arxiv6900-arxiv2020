arxiv-1306-3392 | Unsupervised deconvolution of dynamic imaging reveals intratumor vascular heterogeneity | http://arxiv.org/abs/1306.3392 | id:1306.3392 author:Li Chen, Peter L. Choyke, Niya Wang, Robert Clarke, Zaver M. Bhujwalla, Elizabeth M. C. Hillman, Yue Wang category:q-bio.QM stat.ML  published:2013-06-14 summary:Intratumor heterogeneity is often manifested by vascular compartments with distinct pharmacokinetics that cannot be resolved directly by in vivo dynamic imaging. We developed tissue-specific compartment modeling (TSCM), an unsupervised computational method of deconvolving dynamic imaging series from heterogeneous tumors that can improve vascular phenotyping in many biological contexts. Applying TSCM to dynamic contrast-enhanced MRI of breast cancers revealed characteristic intratumor vascular heterogeneity and therapeutic responses that were otherwise undetectable. version:3
arxiv-1402-4512 | Classification with Sparse Overlapping Groups | http://arxiv.org/abs/1402.4512 | id:1402.4512 author:Nikhil Rao, Robert Nowak, Christopher Cox, Timothy Rogers category:cs.LG stat.ML  published:2014-02-18 summary:Classification with a sparsity constraint on the solution plays a central role in many high dimensional machine learning applications. In some cases, the features can be grouped together so that entire subsets of features can be selected or not selected. In many applications, however, this can be too restrictive. In this paper, we are interested in a less restrictive form of structured sparse feature selection: we assume that while features can be grouped according to some notion of similarity, not all features in a group need be selected for the task at hand. When the groups are comprised of disjoint sets of features, this is sometimes referred to as the "sparse group" lasso, and it allows for working with a richer class of models than traditional group lasso methods. Our framework generalizes conventional sparse group lasso further by allowing for overlapping groups, an additional flexiblity needed in many applications and one that presents further challenges. The main contribution of this paper is a new procedure called Sparse Overlapping Group (SOG) lasso, a convex optimization program that automatically selects similar features for classification in high dimensions. We establish model selection error bounds for SOGlasso classification problems under a fairly general setting. In particular, the error bounds are the first such results for classification using the sparse group lasso. Furthermore, the general SOGlasso bound specializes to results for the lasso and the group lasso, some known and some new. The SOGlasso is motivated by multi-subject fMRI studies in which functional activity is classified using brain voxels as features, source localization problems in Magnetoencephalography (MEG), and analyzing gene activation patterns in microarray data analysis. Experiments with real and synthetic data demonstrate the advantages of SOGlasso compared to the lasso and group lasso. version:2
arxiv-1211-3038 | Gradient density estimation in arbitrary finite dimensions using the method of stationary phase | http://arxiv.org/abs/1211.3038 | id:1211.3038 author:Karthik S. Gurumoorthy, Anand Rangarajan, John Corring category:stat.ML 41A60  62G07  42B10  published:2012-11-13 summary:We prove that the density function of the gradient of a sufficiently smooth function $S : \Omega \subset \mathbb{R}^d \rightarrow \mathbb{R}$, obtained via a random variable transformation of a uniformly distributed random variable, is increasingly closely approximated by the normalized power spectrum of $\phi=\exp\left(\frac{iS}{\tau}\right)$ as the free parameter $\tau \rightarrow 0$. The result is shown using the stationary phase approximation and standard integration techniques and requires proper ordering of limits. We highlight a relationship with the well-known characteristic function approach to density estimation, and detail why our result is distinct from this approach. version:4
arxiv-1409-1403 | Nonlinear tensor product approximation of functions | http://arxiv.org/abs/1409.1403 | id:1409.1403 author:D. Bazarkhanov, V. Temlyakov category:stat.ML math.NA 41A65  published:2014-09-04 summary:We are interested in approximation of a multivariate function $f(x_1,\dots,x_d)$ by linear combinations of products $u^1(x_1)\cdots u^d(x_d)$ of univariate functions $u^i(x_i)$, $i=1,\dots,d$. In the case $d=2$ it is a classical problem of bilinear approximation. In the case of approximation in the $L_2$ space the bilinear approximation problem is closely related to the problem of singular value decomposition (also called Schmidt expansion) of the corresponding integral operator with the kernel $f(x_1,x_2)$. There are known results on the rate of decay of errors of best bilinear approximation in $L_p$ under different smoothness assumptions on $f$. The problem of multilinear approximation (nonlinear tensor product approximation) in the case $d\ge 3$ is more difficult and much less studied than the bilinear approximation problem. We will present results on best multilinear approximation in $L_p$ under mixed smoothness assumption on $f$. version:1
arxiv-1406-3676 | Question Answering with Subgraph Embeddings | http://arxiv.org/abs/1406.3676 | id:1406.3676 author:Antoine Bordes, Sumit Chopra, Jason Weston category:cs.CL  published:2014-06-14 summary:This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a competitive benchmark of the literature. version:3
arxiv-1409-0031 | Tracking Dynamic Point Processes on Networks | http://arxiv.org/abs/1409.0031 | id:1409.0031 author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.IT cs.SI math.IT  published:2014-08-29 summary:Cascading chains of events are a salient feature of many real-world social, biological, and financial networks. In social networks, social reciprocity accounts for retaliations in gang interactions, proxy wars in nation-state conflicts, or Internet memes shared via social media. Neuron spikes stimulate or inhibit spike activity in other neurons. Stock market shocks can trigger a contagion of volatility throughout a financial network. In these and other examples, only individual events associated with network nodes are observed, usually without knowledge of the underlying dynamic relationships between nodes. This paper addresses the challenge of tracking how events within such networks stimulate or influence future events. The proposed approach is an online learning framework well-suited to streaming data, using a multivariate Hawkes point process model to encapsulate autoregressive features of observed events within the social network. Recent work on online learning in dynamic environments is leveraged not only to exploit the dynamics within the underlying network, but also to track that network structure as it evolves. Regret bounds and experimental results demonstrate that the proposed method performs nearly as well as an oracle or batch algorithm. version:2
arxiv-1409-1200 | Domain Transfer Structured Output Learning | http://arxiv.org/abs/1409.1200 | id:1409.1200 author:Jim Jing-Yan Wang category:cs.LG  published:2014-09-03 summary:In this paper, we propose the problem of domain transfer structured output learn- ing and the first solution to solve it. The problem is defined on two different data domains sharing the same input and output spaces, named as source domain and target domain. The outputs are structured, and for the data samples of the source domain, the corresponding outputs are available, while for most data samples of the target domain, the corresponding outputs are missing. The input distributions of the two domains are significantly different. The problem is to learn a predictor for the target domain to predict the structured outputs from the input. Due to the limited number of outputs available for the samples form the target domain, it is difficult to directly learn the predictor from the target domain, thus it is necessary to use the output information available in source domain. We propose to learn the target domain predictor by adapting a auxiliary predictor trained by using source domain data to the target domain. The adaptation is implemented by adding a delta function on the basis of the auxiliary predictor. An algorithm is developed to learn the parameter of the delta function to minimize loss functions associat- ed with the predicted outputs against the true outputs of the data samples with available outputs of the target domain. version:1
arxiv-1409-1199 | Focused Proofreading: Efficiently Extracting Connectomes from Segmented EM Images | http://arxiv.org/abs/1409.1199 | id:1409.1199 author:Stephen M. Plaza category:q-bio.QM cs.CV  published:2014-09-03 summary:Identifying complex neural circuitry from electron microscopic (EM) images may help unlock the mysteries of the brain. However, identifying this circuitry requires time-consuming, manual tracing (proofreading) due to the size and intricacy of these image datasets, thus limiting state-of-the-art analysis to very small brain regions. Potential avenues to improve scalability include automatic image segmentation and crowd sourcing, but current efforts have had limited success. In this paper, we propose a new strategy, focused proofreading, that works with automatic segmentation and aims to limit proofreading to the regions of a dataset that are most impactful to the resulting circuit. We then introduce a novel workflow, which exploits biological information such as synapses, and apply it to a large dataset in the fly optic lobe. With our techniques, we achieve significant tracing speedups of 3-5x without sacrificing the quality of the resulting circuit. Furthermore, our methodology makes the task of proofreading much more accessible and hence potentially enhances the effectiveness of crowd sourcing. version:1
arxiv-1409-1143 | Tunably Rugged Landscapes with Known Maximum and Minimum | http://arxiv.org/abs/1409.1143 | id:1409.1143 author:Narine Manukyan, Margaret J. Eppstein, Jeffrey S. Buzas category:cs.NE  published:2014-09-03 summary:We propose NM landscapes as a new class of tunably rugged benchmark problems. NM landscapes are well-defined on alphabets of any arity, including both discrete and real-valued alphabets, include epistasis in a natural and transparent manner, are proven to have known value and location of the global maximum and, with some additional constraints, are proven to also have a known global minimum. Empirical studies are used to illustrate that, when coefficients are selected from a recommended distribution, the ruggedness of NM landscapes is smoothly tunable and correlates with several measures of search difficulty. We discuss why these properties make NM landscapes preferable to both NK landscapes and Walsh polynomials as benchmark landscape models with tunable epistasis. version:1
arxiv-1305-5777 | Compressive Sensing of Sparse Tensors | http://arxiv.org/abs/1305.5777 | id:1305.5777 author:Shmuel Friedland, Qun Li, Dan Schonfeld category:cs.CV cs.IT math.IT 15A69  65D18  68U05  published:2013-05-24 summary:Compressive sensing (CS) has triggered enormous research activity since its first appearance. CS exploits the signal's sparsity or compressibility in a particular domain and integrates data compression and acquisition, thus allowing exact reconstruction through relatively few non-adaptive linear measurements. While conventional CS theory relies on data representation in the form of vectors, many data types in various applications such as color imaging, video sequences, and multi-sensor networks, are intrinsically represented by higher-order tensors. Application of CS to higher-order data representation is typically performed by conversion of the data to very long vectors that must be measured using very large sampling matrices, thus imposing a huge computational and memory burden. In this paper, we propose Generalized Tensor Compressive Sensing (GTCS)--a unified framework for compressive sensing of higher-order tensors which preserves the intrinsic structure of tensor data with reduced computational complexity at reconstruction. GTCS offers an efficient means for representation of multidimensional data by providing simultaneous acquisition and compression from all tensor modes. In addition, we propound two reconstruction procedures, a serial method (GTCS-S) and a parallelizable method (GTCS-P). We then compare the performance of the proposed method with Kronecker compressive sensing (KCS) and multi way compressive sensing (MWCS). We demonstrate experimentally that GTCS outperforms KCS and MWCS in terms of both reconstruction accuracy (within a range of compression ratios) and processing speed. The major disadvantage of our methods (and of MWCS as well), is that the compression ratios may be worse than that offered by KCS. version:4
arxiv-1408-3300 | Gradient Distribution Priors for Biomedical Image Processing | http://arxiv.org/abs/1408.3300 | id:1408.3300 author:Yuanhao Gong, Ivo F. Sbalzarini category:cs.CV  published:2014-08-13 summary:Ill-posed inverse problems are commonplace in biomedical image processing. Their solution typically requires imposing prior knowledge about the latent ground truth. While this regularizes the problem to an extent where it can be solved, it also biases the result toward the expected. With inappropriate priors harming more than they use, it remains unclear what prior to use for a given practical problem. Priors are hence mostly chosen in an {\em ad hoc} or empirical fashion. We argue here that the gradient distribution of natural-scene images may provide a versatile and well-founded prior for biomedical images. We provide motivation for this choice from different points of view, and we fully validate the resulting prior for use on biomedical images by showing its stability and correlation with image quality. We then provide a set of simple parametric models for the resulting prior, leading to straightforward (quasi-)convex optimization problems for which we provide efficient solver algorithms. We illustrate the use of the present models and solvers in a variety of common image-processing tasks, including contrast enhancement, noise level estimation, denoising, blind deconvolution, zooming/up-sampling, and dehazing. In all cases we show that the present method leads to results that are comparable to or better than the state of the art; always using the same, simple prior. We conclude by discussing the limitations and possible interpretations of the prior. version:2
arxiv-1409-1073 | Performance Analysis on Evolutionary Algorithms for the Minimum Label Spanning Tree Problem | http://arxiv.org/abs/1409.1073 | id:1409.1073 author:Xinsheng Lai, Yuren Zhou, Jun He, Jun Zhang category:cs.NE cs.DS  published:2014-09-03 summary:Some experimental investigations have shown that evolutionary algorithms (EAs) are efficient for the minimum label spanning tree (MLST) problem. However, we know little about that in theory. As one step towards this issue, we theoretically analyze the performances of the (1+1) EA, a simple version of EAs, and a multi-objective evolutionary algorithm called GSEMO on the MLST problem. We reveal that for the MLST$_{b}$ problem the (1+1) EA and GSEMO achieve a $\frac{b+1}{2}$-approximation ratio in expected polynomial times of $n$ the number of nodes and $k$ the number of labels. We also show that GSEMO achieves a $(2ln(n))$-approximation ratio for the MLST problem in expected polynomial time of $n$ and $k$. At the same time, we show that the (1+1) EA and GSEMO outperform local search algorithms on three instances of the MLST problem. We also construct an instance on which GSEMO outperforms the (1+1) EA. version:1
arxiv-1409-1062 | Structured Low-Rank Matrix Factorization with Missing and Grossly Corrupted Observations | http://arxiv.org/abs/1409.1062 | id:1409.1062 author:Fanhua Shang, Yuanyuan Liu, Hanghang Tong, James Cheng, Hong Cheng category:cs.LG cs.CV stat.ML  published:2014-09-03 summary:Recovering low-rank and sparse matrices from incomplete or corrupted observations is an important problem in machine learning, statistics, bioinformatics, computer vision, as well as signal and image processing. In theory, this problem can be solved by the natural convex joint/mixed relaxations (i.e., l_{1}-norm and trace norm) under certain conditions. However, all current provable algorithms suffer from superlinear per-iteration cost, which severely limits their applicability to large-scale problems. In this paper, we propose a scalable, provable structured low-rank matrix factorization method to recover low-rank and sparse matrices from missing and grossly corrupted data, i.e., robust matrix completion (RMC) problems, or incomplete and grossly corrupted measurements, i.e., compressive principal component pursuit (CPCP) problems. Specifically, we first present two small-scale matrix trace norm regularized bilinear structured factorization models for RMC and CPCP problems, in which repetitively calculating SVD of a large-scale matrix is replaced by updating two much smaller factor matrices. Then, we apply the alternating direction method of multipliers (ADMM) to efficiently solve the RMC problems. Finally, we provide the convergence analysis of our algorithm, and extend it to address general CPCP problems. Experimental results verified both the efficiency and effectiveness of our method compared with the state-of-the-art methods. version:1
arxiv-1409-1057 | Augmented Neural Networks for Modelling Consumer Indebtness | http://arxiv.org/abs/1409.1057 | id:1409.1057 author:Alexandros Ladas, Jonathan M. Garibaldi, Rodrigo Scarpel, Uwe Aickelin category:cs.CE cs.LG cs.NE  published:2014-09-03 summary:Consumer Debt has risen to be an important problem of modern societies, generating a lot of research in order to understand the nature of consumer indebtness, which so far its modelling has been carried out by statistical models. In this work we show that Computational Intelligence can offer a more holistic approach that is more suitable for the complex relationships an indebtness dataset has and Linear Regression cannot uncover. In particular, as our results show, Neural Networks achieve the best performance in modelling consumer indebtness, especially when they manage to incorporate the significant and experimentally verified results of the Data Mining process in the model, exploiting the flexibility Neural Networks offer in designing their topology. This novel method forms an elaborate framework to model Consumer indebtness that can be extended to any other real world application. version:1
arxiv-1409-1053 | Tuning a Multiple Classifier System for Side Effect Discovery using Genetic Algorithms | http://arxiv.org/abs/1409.1053 | id:1409.1053 author:Jenna M. Reps, Uwe Aickelin, Jonathan M. Garibaldi category:cs.LG cs.CE  published:2014-09-03 summary:In previous work, a novel supervised framework implementing a binary classifier was presented that obtained excellent results for side effect discovery. Interestingly, unique side effects were identified when different binary classifiers were used within the framework, prompting the investigation of applying a multiple classifier system. In this paper we investigate tuning a side effect multiple classifying system using genetic algorithms. The results of this research show that the novel framework implementing a multiple classifying system trained using genetic algorithms can obtain a higher partial area under the receiver operating characteristic curve than implementing a single classifier. Furthermore, the framework is able to detect side effects efficiently and obtains a low false positive rate. version:1
arxiv-1409-1043 | Variability of Behaviour in Electricity Load Profile Clustering; Who Does Things at the Same Time Each Day? | http://arxiv.org/abs/1409.1043 | id:1409.1043 author:Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden category:cs.LG cs.CE  published:2014-09-03 summary:UK electricity market changes provide opportunities to alter households' electricity usage patterns for the benefit of the overall electricity network. Work on clustering similar households has concentrated on daily load profiles and the variability in regular household behaviours has not been considered. Those households with most variability in regular activities may be the most receptive to incentives to change timing. Whether using the variability of regular behaviour allows the creation of more consistent groupings of households is investigated and compared with daily load profile clustering. 204 UK households are analysed to find repeating patterns (motifs). Variability in the time of the motif is used as the basis for clustering households. Different clustering algorithms are assessed by the consistency of the results. Findings show that variability of behaviour, using motifs, provides more consistent groupings of households across different clustering algorithms and allows for more efficient targeting of behaviour change interventions. version:1
arxiv-1310-0522 | EVOC: A Computer Model of the Evolution of Culture | http://arxiv.org/abs/1310.0522 | id:1310.0522 author:Liane Gabora category:cs.MA cs.NE  published:2013-10-01 summary:EVOC is a computer model of the EVOlution of Culture. It consists of neural network based agents that invent ideas for actions, and imitate neighbors' actions. EVOC replicates using a different fitness function the results obtained with an earlier model (MAV), including (1) an increase in mean fitness of actions, and (2) an increase and then decrease in the diversity of actions. Diversity of actions is positively correlated with number of needs, population size and density, and with the erosion of borders between populations. Slowly eroding borders maximize diversity, fostering specialization followed by sharing of fit actions. Square (as opposed to toroidal) worlds also exhibit higher diversity. Introducing a leader that broadcasts its actions throughout the population increases the fitness of actions but reduces diversity; these effects diminish the more leaders there are. Low density populations have less fit ideas but broadcasting diminishes this effect. version:2
arxiv-1407-4023 | Aggregate channel features for multi-view face detection | http://arxiv.org/abs/1407.4023 | id:1407.4023 author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV  published:2014-07-15 summary:Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones. While many subsequences have improved the work with more powerful learning algorithms, the feature representation used for face detection still can't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild. To solve this bottleneck, we borrow the concept of channel features to the face detection domain, which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form. We adopt a novel variant called aggregate channel features, make a full exploration of feature design, and discover a multi-scale version of features with better performance. To deal with poses of faces in the wild, we propose a multi-view detection approach featuring score re-ranking and detection adjustment. Following the learning pipelines in Viola-Jones framework, the multi-view face detector using aggregate channel features shows competitive performance against state-of-the-art algorithms on AFW and FDDB testsets, while runs at 42 FPS on VGA images. version:2
arxiv-1211-3589 | A Truncated EM Approach for Spike-and-Slab Sparse Coding | http://arxiv.org/abs/1211.3589 | id:1211.3589 author:Abdul-Saboor Sheikh, Jacquelyn A. Shelton, Jörg Lücke category:stat.ML  published:2012-11-15 summary:We study inference and learning based on a sparse coding model with `spike-and-slab' prior. As in standard sparse coding, the model used assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse prior such as a Laplace distribution, we study the application of a more flexible `spike-and-slab' distribution which models the absence or presence of a source's contribution independently of its strength if it contributes. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: a novel truncated EM approach and, for comparison, an approach based on standard factored variational distributions. The truncated approach can be regarded as a variational approach with truncated posteriors as variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and-slab' priors for the corresponding data domains. Furthermore, we find that the truncated EM approach improves on the standard factored approach in source separation tasks$-$which hints to biases introduced by assuming posterior independence in the factored variational approach. Likewise, on a standard benchmark for image denoising, we find that the truncated EM approach improves on the factored variational approach. While the performance of the factored approach saturates with increasing numbers of hidden dimensions, the performance of the truncated approach improves the state-of-the-art for higher noise levels. version:3
arxiv-1409-5774 | Attributes for Causal Inference in Longitudinal Observational Databases | http://arxiv.org/abs/1409.5774 | id:1409.5774 author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.CE cs.LG  published:2014-09-03 summary:The pharmaceutical industry is plagued by the problem of side effects that can occur anytime a prescribed medication is ingested. There has been a recent interest in using the vast quantities of medical data available in longitudinal observational databases to identify causal relationships between drugs and medical events. Unfortunately the majority of existing post marketing surveillance algorithms measure how dependant or associated an event is on the presence of a drug rather than measuring causality. In this paper we investigate potential attributes that can be used in causal inference to identify side effects based on the Bradford-Hill causality criteria. Potential attributes are developed by considering five of the causality criteria and feature selection is applied to identify the most suitable of these attributes for detecting side effects. We found that attributes based on the specificity criterion may improve side effect signalling algorithms but the experiment and dosage criteria attributes investigated in this paper did not offer sufficient additional information. version:1
arxiv-1409-0964 | Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive Features | http://arxiv.org/abs/1409.0964 | id:1409.0964 author:Liansheng Zhuang, Shenghua Gao, Jinhui Tang, Jingjing Wang, Zhouchen Lin, Yi Ma category:cs.CV cs.LG  published:2014-09-03 summary:This paper aims at constructing a good graph for discovering intrinsic data structures in a semi-supervised learning setting. Firstly, we propose to build a non-negative low-rank and sparse (referred to as NNLRS) graph for the given data representation. Specifically, the weights of edges in the graph are obtained by seeking a nonnegative low-rank and sparse matrix that represents each data sample as a linear combination of others. The so-obtained NNLRS-graph can capture both the global mixture of subspaces structure (by the low rankness) and the locally linear structure (by the sparseness) of the data, hence is both generative and discriminative. Secondly, as good features are extremely important for constructing a good graph, we propose to learn the data embedding matrix and construct the graph jointly within one framework, which is termed as NNLRS with embedded features (referred to as NNLRS-EF). Extensive experiments on three publicly available datasets demonstrate that the proposed method outperforms the state-of-the-art graph construction method by a large margin for both semi-supervised classification and discriminative analysis, which verifies the effectiveness of our proposed method. version:1
arxiv-1408-5882 | Convolutional Neural Networks for Sentence Classification | http://arxiv.org/abs/1408.5882 | id:1408.5882 author:Yoon Kim category:cs.CL cs.NE  published:2014-08-25 summary:We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification. version:2
arxiv-1409-0934 | Breakdown Point of Robust Support Vector Machine | http://arxiv.org/abs/1409.0934 | id:1409.0934 author:Takafumi Kanamori, Shuhei Fujiwara, Akiko Takeda category:stat.ML cs.LG  published:2014-09-03 summary:The support vector machine (SVM) is one of the most successful learning methods for solving classification problems. Despite its popularity, SVM has a serious drawback, that is sensitivity to outliers in training samples. The penalty on misclassification is defined by a convex loss called the hinge loss, and the unboundedness of the convex loss causes the sensitivity to outliers. To deal with outliers, robust variants of SVM have been proposed, such as the robust outlier detection algorithm and an SVM with a bounded loss called the ramp loss. In this paper, we propose a robust variant of SVM and investigate its robustness in terms of the breakdown point. The breakdown point is a robustness measure that is the largest amount of contamination such that the estimated classifier still gives information about the non-contaminated data. The main contribution of this paper is to show an exact evaluation of the breakdown point for the robust SVM. For learning parameters such as the regularization parameter in our algorithm, we derive a simple formula that guarantees the robustness of the classifier. When the learning parameters are determined with a grid search using cross validation, our formula works to reduce the number of candidate search points. The robustness of the proposed method is confirmed in numerical experiments. We show that the statistical properties of the robust SVM are well explained by a theoretical analysis of the breakdown point. version:1
arxiv-1406-1078 | Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation | http://arxiv.org/abs/1406.1078 | id:1406.1078 author:Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML  published:2014-06-03 summary:In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. version:3
arxiv-1409-1411 | Visual Speech Recognition | http://arxiv.org/abs/1409.1411 | id:1409.1411 author:Ahmad B. A. Hassanat category:cs.CV  published:2014-09-03 summary:Lip reading is used to understand or interpret speech without hearing it, a technique especially mastered by people with hearing difficulties. The ability to lip read enables a person with a hearing impairment to communicate with others and to engage in social activities, which otherwise would be difficult. Recent advances in the fields of computer vision, pattern recognition, and signal processing has led to a growing interest in automating this challenging task of lip reading. Indeed, automating the human ability to lip read, a process referred to as visual speech recognition (VSR) (or sometimes speech reading), could open the door for other novel related applications. VSR has received a great deal of attention in the last decade for its potential use in applications such as human-computer interaction (HCI), audio-visual speech recognition (AVSR), speaker recognition, talking heads, sign language recognition and video surveillance. Its main aim is to recognise spoken word(s) by using only the visual signal that is produced during speech. Hence, VSR deals with the visual domain of speech and involves image processing, artificial intelligence, object detection, pattern recognition, statistical modelling, etc. version:1
arxiv-1409-0925 | Bypassing Captcha By Machine A Proof For Passing The Turing Test | http://arxiv.org/abs/1409.0925 | id:1409.0925 author:Ahmad B. A. Hassanat category:cs.CV cs.AI cs.HC  published:2014-09-03 summary:For the last ten years, CAPTCHAs have been widely used by websites to prevent their data being automatically updated by machines. By supposedly allowing only humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT), knowing that humans are more intelligent than machines. Generally, CAPTCHAs have defeated machines, but things are changing rapidly as technology improves. Hence, advanced research into optical character recognition (OCR) is overtaking attempts to strengthen CAPTCHAs against machine-based attacks. This paper investigates the immunity of CAPTCHA, which was built on the failure of the TT. We show that some CAPTCHAs are easily broken using a simple OCR machine built for the purpose of this study. By reviewing other techniques, we show that even more difficult CAPTCHAs can be broken using advanced OCR machines. Current advances in OCR should enable machines to pass the TT in the image recognition domain, which is exactly where machines are seeking to overcome CAPTCHAs. We enhance traditional CAPTCHAs by employing not only characters, but also natural language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs might be able to hold out against machines, at least until the advent of a machine that passes the TT completely. version:1
arxiv-1409-0924 | Visual Passwords Using Automatic Lip Reading | http://arxiv.org/abs/1409.0924 | id:1409.0924 author:Ahmad Basheer Hassanat category:cs.CV cs.CR  published:2014-09-02 summary:This paper presents a visual passwords system to increase security. The system depends mainly on recognizing the speaker using the visual speech signal alone. The proposed scheme works in two stages: setting the visual password stage and the verification stage. At the setting stage the visual passwords system request the user to utter a selected password, a video recording of the user face is captured, and processed by a special words-based VSR system which extracts a sequence of feature vectors. In the verification stage, the same procedure is executed, the features will be sent to be compared with the stored visual password. The proposed scheme has been evaluated using a video database of 20 different speakers (10 females and 10 males), and 15 more males in another video database with different experiment sets. The evaluation has proved the system feasibility, with average error rate in the range of 7.63% to 20.51% at the worst tested scenario, and therefore, has potential to be a practical approach with the support of other conventional authentication methods such as the use of usernames and passwords. version:1
arxiv-1409-0923 | Dimensionality Invariant Similarity Measure | http://arxiv.org/abs/1409.0923 | id:1409.0923 author:Ahmad Basheer Hassanat category:cs.LG  published:2014-09-02 summary:This paper presents a new similarity measure to be used for general tasks including supervised learning, which is represented by the K-nearest neighbor classifier (KNN). The proposed similarity measure is invariant to large differences in some dimensions in the feature space. The proposed metric is proved mathematically to be a metric. To test its viability for different applications, the KNN used the proposed metric for classifying test examples chosen from a number of real datasets. Compared to some other well known metrics, the experimental results show that the proposed metric is a promising distance measure for the KNN classifier with strong potential for a wide range of applications. version:1
arxiv-1409-0919 | Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach | http://arxiv.org/abs/1409.0919 | id:1409.0919 author:Ahmad Basheer Hassanat, Mohammad Ali Abbadi, Ghada Awad Altarawneh, Ahmad Ali Alhasanat category:cs.LG  published:2014-09-02 summary:This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications. version:1
arxiv-1405-7718 | Deformation corrected compressed sensing (DC-CS): a novel framework for accelerated dynamic MRI | http://arxiv.org/abs/1405.7718 | id:1405.7718 author:Sajan Goud Lingala, Edward DiBella, Mathews Jacob category:cs.CV  published:2014-05-29 summary:We propose a novel deformation corrected compressed sensing (DC-CS) framework to recover dynamic magnetic resonance images from undersampled measurements. We introduce a generalized formulation that is capable of handling a wide class of sparsity/compactness priors on the deformation corrected dynamic signal. In this work, we consider example compactness priors such as sparsity in temporal Fourier domain, sparsity in temporal finite difference domain, and nuclear norm penalty to exploit low rank structure. Using variable splitting, we decouple the complex optimization problem to simpler and well understood sub problems; the resulting algorithm alternates between simple steps of shrinkage based denoising, deformable registration, and a quadratic optimization step. Additionally, we employ efficient continuation strategies to minimize the risk of convergence to local minima. The proposed formulation contrasts with existing DC-CS schemes that are customized for free breathing cardiac cine applications, and other schemes that rely on fully sampled reference frames or navigator signals to estimate the deformation parameters. The efficient decoupling enabled by the proposed scheme allows its application to a wide range of applications including contrast enhanced dynamic MRI. Through experiments on numerical phantom and in vivo myocardial perfusion MRI datasets, we demonstrate the utility of the proposed DC-CS scheme in providing robust reconstructions with reduced motion artifacts over classical compressed sensing schemes that utilize the compact priors on the original deformation un-corrected signal. version:2
arxiv-1409-0915 | An Approach for Text Steganography Based on Markov Chains | http://arxiv.org/abs/1409.0915 | id:1409.0915 author:H. Hernan Moraldo category:cs.MM cs.CL 68P25  94A60 D.4.6  published:2014-09-02 summary:A text steganography method based on Markov chains is introduced, together with a reference implementation. This method allows for information hiding in texts that are automatically generated following a given Markov model. Other Markov - based systems of this kind rely on big simplifications of the language model to work, which produces less natural looking and more easily detectable texts. The method described here is designed to generate texts within a good approximation of the original language model provided. version:1
arxiv-1409-0908 | Action Recognition in the Frequency Domain | http://arxiv.org/abs/1409.0908 | id:1409.0908 author:Anh Tran, Jinyan Guan, Thanima Pilantanakitti, Paul Cohen category:cs.CV  published:2014-09-02 summary:In this paper, we describe a simple strategy for mitigating variability in temporal data series by shifting focus onto long-term, frequency domain features that are less susceptible to variability. We apply this method to the human action recognition task and demonstrate how working in the frequency domain can yield good recognition features for commonly used optical flow and articulated pose features, which are highly sensitive to small differences in motion, viewpoint, dynamic backgrounds, occlusion and other sources of variability. We show how these frequency-based features can be used in combination with a simple forest classifier to achieve good and robust results on the popular KTH Actions dataset. version:1
arxiv-1409-0314 | Empirical Evaluation of Tree distances for Parser Evaluation | http://arxiv.org/abs/1409.0314 | id:1409.0314 author:Taraka Rama category:cs.CL  published:2014-09-01 summary:In this empirical study, I compare various tree distance measures -- originally developed in computational biology for the purpose of tree comparison -- for the purpose of parser evaluation. I will control for the parser setting by comparing the automatically generated parse trees from the state-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The article describes two different tree distance measures (RF and QD) along with its variants (GRF and GQD) for the purpose of parser evaluation. The article will argue that RF measure captures similar information as the standard EvalB metric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha, 1989) applied by Tsarfaty et al. (2011). Finally, the article also provides empirical evidence by reporting high correlations between the different tree distances and EvalB metric's scores. version:2
arxiv-1409-0814 | CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein Tertiary Structure Retrieval | http://arxiv.org/abs/1409.0814 | id:1409.0814 author:Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman, Md. Abul Kashem Mia, Farhana Zaman, Salman Rakin category:cs.CV cs.CE cs.IR  published:2014-09-02 summary:Due to the advancements in technology number of entries in the structural database of proteins are increasing day by day. Methods for retrieving protein tertiary structures from this large database is the key to comparative analysis of structures which plays an important role to understand proteins and their function. In this paper, we present fast and accurate methods for the retrieval of proteins from a large database with tertiary structures similar to a query protein. Our proposed methods borrow ideas from the field of computer vision. The speed and accuracy of our methods comes from the two newly introduced features, the co-occurrence matrix of the oriented gradient and pyramid histogram of oriented gradient and from the use of Euclidean distance as the distance measure. Experimental results clearly indicate the superiority of our approach in both running time and accuracy. Our method is readily available for use from this website: http://research.buet.ac.bd:8080/Comograd/. version:1
arxiv-1301-7724 | Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks | http://arxiv.org/abs/1301.7724 | id:1301.7724 author:Gunnar Carlsson, Facundo Mémoli, Alejandro Ribeiro, Santiago Segarra category:cs.LG cs.SI stat.ML  published:2013-01-31 summary:This paper considers networks where relationships between nodes are represented by directed dissimilarities. The goal is to study methods for the determination of hierarchical clusters, i.e., a family of nested partitions indexed by a connectivity parameter, induced by the given dissimilarity structures. Our construction of hierarchical clustering methods is based on defining admissible methods to be those methods that abide by the axioms of value - nodes in a network with two nodes are clustered together at the maximum of the two dissimilarities between them - and transformation - when dissimilarities are reduced, the network may become more clustered but not less. Several admissible methods are constructed and two particular methods, termed reciprocal and nonreciprocal clustering, are shown to provide upper and lower bounds in the space of admissible methods. Alternative clustering methodologies and axioms are further considered. Allowing the outcome of hierarchical clustering to be asymmetric, so that it matches the asymmetry of the original data, leads to the inception of quasi-clustering methods. The existence of a unique quasi-clustering method is shown. Allowing clustering in a two-node network to proceed at the minimum of the two dissimilarities generates an alternative axiomatic construction. There is a unique clustering method in this case too. The paper also develops algorithms for the computation of hierarchical clusters using matrix powers on a min-max dioid algebra and studies the stability of the methods proposed. We proved that most of the methods introduced in this paper are such that similar networks yield similar hierarchical clustering results. Algorithms are exemplified through their application to networks describing internal migration within states of the United States (U.S.) and the interrelation between sectors of the U.S. economy. version:2
arxiv-1409-0797 | Feature Engineering for Map Matching of Low-Sampling-Rate GPS Trajectories in Road Network | http://arxiv.org/abs/1409.0797 | id:1409.0797 author:Jian Yang, Liqiu Meng category:stat.ML cs.LG  published:2014-09-02 summary:Map matching of GPS trajectories from a sequence of noisy observations serves the purpose of recovering the original routes in a road network. In this work in progress, we attempt to share our experience of feature construction in a spatial database by reporting our ongoing experiment of feature extrac-tion in Conditional Random Fields (CRFs) for map matching. Our preliminary results are obtained from real-world taxi GPS trajectories. version:1
arxiv-1409-0791 | Feature Selection in Conditional Random Fields for Map Matching of GPS Trajectories | http://arxiv.org/abs/1409.0791 | id:1409.0791 author:Jian Yang, Liqiu Meng category:stat.ML cs.AI cs.LG  published:2014-09-02 summary:Map matching of the GPS trajectory serves the purpose of recovering the original route on a road network from a sequence of noisy GPS observations. It is a fundamental technique to many Location Based Services. However, map matching of a low sampling rate on urban road network is still a challenging task. In this paper, the characteristics of Conditional Random Fields with regard to inducing many contextual features and feature selection are explored for the map matching of the GPS trajectories at a low sampling rate. Experiments on a taxi trajectory dataset show that our method may achieve competitive results along with the success of reducing model complexity for computation-limited applications. version:1
arxiv-1409-0788 | Ensemble Learning of Colorectal Cancer Survival Rates | http://arxiv.org/abs/1409.0788 | id:1409.0788 author:Chris Roadknight, Uwe Aickelin, John Scholefield, Lindy Durrant category:cs.LG cs.CE  published:2014-09-02 summary:In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. We build on existing research on clustering and machine learning facets of this data to demonstrate a role for an ensemble approach to highlighting patients with clearer prognosis parameters. Results for survival prediction using 3 different approaches are shown for a subset of the data which is most difficult to model. The performance of each model individually is compared with subsets of the data where some agreement is reached for multiple models. Significant improvements in model accuracy on an unseen test set can be achieved for patients where agreement between models is achieved. version:1
arxiv-1409-0775 | Feature selection in detection of adverse drug reactions from the Health Improvement Network (THIN) database | http://arxiv.org/abs/1409.0775 | id:1409.0775 author:Yihui Liu, Uwe Aickelin category:cs.LG cs.CE  published:2014-09-02 summary:Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs are one of most common causes to withdraw some drugs from market. Prescription event monitoring (PEM) is an important approach to detect the adverse drug reactions. The main problem to deal with this method is how to automatically extract the medical events or side effects from high-throughput medical events, which are collected from day to day clinical practice. In this study we propose a novel concept of feature matrix to detect the ADRs. Feature matrix, which is extracted from big medical data from The Health Improvement Network (THIN) database, is created to characterize the medical events for the patients who take drugs. Feature matrix builds the foundation for the irregular and big medical data. Then feature selection methods are performed on feature matrix to detect the significant features. Finally the ADRs can be located based on the significant features. The experiments are carried out on three drugs: Atorvastatin, Alendronate, and Metoclopramide. Major side effects for each drug are detected and better performance is achieved compared to other computerized methods. The detected ADRs are based on computerized methods, further investigation is needed. version:1
arxiv-1409-0772 | Signalling Paediatric Side Effects using an Ensemble of Simple Study Designs | http://arxiv.org/abs/1409.0772 | id:1409.0772 author:Jenna M. Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE  published:2014-09-02 summary:Background: Children are frequently prescribed medication off-label, meaning there has not been sufficient testing of the medication to determine its safety or effectiveness. The main reason this safety knowledge is lacking is due to ethical restrictions that prevent children from being included in the majority of clinical trials. Objective: The objective of this paper is to investigate whether an ensemble of simple study designs can be implemented to signal acutely occurring side effects effectively within the paediatric population by using historical longitudinal data. The majority of pharmacovigilance techniques are unsupervised, but this research presents a supervised framework. Methods: Multiple measures of association are calculated for each drug and medical event pair and these are used as features that are fed into a classiffier to determine the likelihood of the drug and medical event pair corresponding to an adverse drug reaction. The classiffier is trained using known adverse drug reactions or known non-adverse drug reaction relationships. Results: The novel ensemble framework obtained a false positive rate of 0:149, a sensitivity of 0:547 and a specificity of 0:851 when implemented on a reference set of drug and medical event pairs. The novel framework consistently outperformed each individual simple study design. Conclusion: This research shows that it is possible to exploit the mechanism of causality and presents a framework for signalling adverse drug reactions effectively. version:1
arxiv-1409-0768 | A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect Discovery | http://arxiv.org/abs/1409.0768 | id:1409.0768 author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE  published:2014-09-02 summary:Drugs are frequently prescribed to patients with the aim of improving each patient's medical state, but an unfortunate consequence of most prescription drugs is the occurrence of undesirable side effects. Side effects that occur in more than one in a thousand patients are likely to be signalled efficiently by current drug surveillance methods, however, these same methods may take decades before generating signals for rarer side effects, risking medical morbidity or mortality in patients prescribed the drug while the rare side effect is undiscovered. In this paper we propose a novel computational meta-analysis framework for signalling rare side effects that integrates existing methods, knowledge from the web, metric learning and semi-supervised clustering. The novel framework was able to signal many known rare and serious side effects for the selection of drugs investigated, such as tendon rupture when prescribed Ciprofloxacin or Levofloxacin, renal failure with Naproxen and depression associated with Rimonabant. Furthermore, for the majority of the drug investigated it generated signals for rare side effects at a more stringent signalling threshold than existing methods and shows the potential to become a fundamental part of post marketing surveillance to detect rare side effects. version:1
arxiv-1409-0763 | Data classification using the Dempster-Shafer method | http://arxiv.org/abs/1409.0763 | id:1409.0763 author:Qi Chen, Amanda Whitbrook, Uwe Aickelin, Chris Roadknight category:cs.LG  published:2014-09-02 summary:In this paper, the Dempster-Shafer method is employed as the theoretical basis for creating data classification systems. Testing is carried out using three popular (multiple attribute) benchmark datasets that have two, three and four classes. In each case, a subset of the available data is used for training to establish thresholds, limits or likelihoods of class membership for each attribute, and hence create mass functions that establish probability of class membership for each attribute of the test data. Classification of each data item is achieved by combination of these probabilities via Dempster's Rule of Combination. Results for the first two datasets show extremely high classification accuracy that is competitive with other popular methods. The third dataset is non-numerical and difficult to classify, but good results can be achieved provided the system and mass functions are designed carefully and the right attributes are chosen for combination. In all cases the Dempster-Shafer method provides comparable performance to other more popular algorithms, but the overhead of generating accurate mass functions increases the complexity with the addition of new attributes. Overall, the results suggest that the D-S approach provides a suitable framework for the design of classification systems and that automating the mass function design and calculation would increase the viability of the algorithm for complex classification problems. version:1
arxiv-1409-0749 | Image Retrieval And Classification Using Local Feature Vectors | http://arxiv.org/abs/1409.0749 | id:1409.0749 author:Vikas Verma category:cs.IR cs.CV cs.MM  published:2014-09-02 summary:Content Based Image Retrieval(CBIR) is one of the important subfield in the field of Information Retrieval. The goal of a CBIR algorithm is to retrieve semantically similar images in response to a query image submitted by the end user. CBIR is a hard problem because of the phenomenon known as $\textit {semantic gap}$. In this thesis, we aim at analyzing the performance of a CBIR system build using local feature vectors and Intermediate Matching Kernel. We also propose a Two-Step Matching process for reducing the response time of the CBIR systems. Further, we develop a Meta-Learning framework for improving the retrieval performance of these systems. Our results show that the Two-Step Matching process significantly reduces response time and the Meta-Learning Framework improves the retrieval performance by more than two fold. We also analyze the performance of various image classification systems that use different image representations constructed from the local feature vectors. version:1
arxiv-1409-0748 | Comparison of algorithms that detect drug side effects using electronic healthcare databases | http://arxiv.org/abs/1409.0748 | id:1409.0748 author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack Gibson, Richard Hubbard category:cs.LG cs.CE  published:2014-09-02 summary:The electronic healthcare databases are starting to become more readily available and are thought to have excellent potential for generating adverse drug reaction signals. The Health Improvement Network (THIN) database is an electronic healthcare database containing medical information on over 11 million patients that has excellent potential for detecting ADRs. In this paper we apply four existing electronic healthcare database signal detecting algorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on the THIN database for a selection of drugs from six chosen drug families. This is the first comparison of ADR signalling algorithms that includes MUTARA and HUNT and enabled us to set a benchmark for the adverse drug reaction signalling ability of the THIN database. The drugs were selectively chosen to enable a comparison with previous work and for variety. It was found that no algorithm was generally superior and the algorithms' natural thresholds act at variable stringencies. Furthermore, none of the algorithms perform well at detecting rare ADRs. version:1
arxiv-1409-0745 | A natural framework for sparse hierarchical clustering | http://arxiv.org/abs/1409.0745 | id:1409.0745 author:Hongyang Zhang, Ruben H. Zamar category:stat.ML cs.LG 62H30  published:2014-09-02 summary:There has been a surge in the number of large and flat data sets - data sets containing a large number of features and a relatively small number of observations - due to the growing ability to collect and store information in medical research and other fi?elds. Hierarchical clustering is a widely used clustering tool. In hierarchical clustering, large and flat data sets may allow for a better coverage of clustering features (features that help explain the true underlying clusters) but, such data sets usually include a large fraction of noise features (non-clustering features) that may hide the underlying clusters. Witten and Tibshirani (2010) proposed a sparse hierarchical clustering framework to cluster the observations using an adaptively chosen subset of the features, however, we show that this framework has some limitations when the data sets contain clustering features with complex structure. In this paper, another sparse hierarchical clustering (SHC) framework is proposed. We show that, using simulation studies and real data examples, the proposed framework produces superior feature selection and clustering performance comparing to the classical (of-the-shelf) hierarchical clustering and the existing sparse hierarchical clustering framework. version:1
arxiv-1409-0602 | Transferring Landmark Annotations for Cross-Dataset Face Alignment | http://arxiv.org/abs/1409.0602 | id:1409.0602 author:Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2014-09-02 summary:Dataset bias is a well known problem in object recognition domain. This issue, nonetheless, is rarely explored in face alignment research. In this study, we show that dataset plays an integral part of face alignment performance. Specifically, owing to face alignment dataset bias, training on one database and testing on another or unseen domain would lead to poor performance. Creating an unbiased dataset through combining various existing databases, however, is non-trivial as one has to exhaustively re-label the landmarks for standardisation. In this work, we propose a simple and yet effective method to bridge the disparate annotation spaces between databases, making datasets fusion possible. We show extensive results on combining various popular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and unseen data alignment. version:1
arxiv-1409-0585 | On the Equivalence Between Deep NADE and Generative Stochastic Networks | http://arxiv.org/abs/1409.0585 | id:1409.0585 author:Li Yao, Sherjil Ozair, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG  published:2014-09-02 summary:Neural Autoregressive Distribution Estimators (NADEs) have recently been shown as successful alternatives for modeling high dimensional multimodal distributions. One issue associated with NADEs is that they rely on a particular order of factorization for $P(\mathbf{x})$. This issue has been recently addressed by a variant of NADE called Orderless NADEs and its deeper version, Deep Orderless NADE. Orderless NADEs are trained based on a criterion that stochastically maximizes $P(\mathbf{x})$ with all possible orders of factorizations. Unfortunately, ancestral sampling from deep NADE is very expensive, corresponding to running through a neural net separately predicting each of the visible variables given some others. This work makes a connection between this criterion and the training criterion for Generative Stochastic Networks (GSNs). It shows that training NADEs in this way also trains a GSN, which defines a Markov chain associated with the NADE model. Based on this connection, we show an alternative way to sample from a trained Orderless NADE that allows to trade-off computing time and quality of the samples: a 3 to 10-fold speedup (taking into account the waste due to correlations between consecutive samples of the chain) can be obtained without noticeably reducing the quality of the samples. This is achieved using a novel sampling procedure for GSNs called annealed GSN sampling, similar to tempering methods that combines fast mixing (obtained thanks to steps at high noise levels) with accurate samples (obtained thanks to steps at low noise levels). version:1
arxiv-1311-1780 | Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks | http://arxiv.org/abs/1311.1780 | id:1311.1780 author:Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu, Yoshua Bengio category:cs.NE cs.LG stat.ML  published:2013-11-07 summary:In this paper we propose and investigate a novel nonlinear unit, called $L_p$ unit, for deep neural networks. The proposed $L_p$ unit receives signals from several projections of a subset of units in the layer below and computes a normalized $L_p$ norm. We notice two interesting interpretations of the $L_p$ unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013) which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the $L_p$ unit is more efficient at representing complex, nonlinear separating boundaries. Each $L_p$ unit defines a superelliptic boundary, with its exact shape defined by the order $p$. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few $L_p$ units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed $L_p$ units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$ units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep recurrent neural networks (RNN). version:7
arxiv-1409-0272 | Multi-task Sparse Structure Learning | http://arxiv.org/abs/1409.0272 | id:1409.0272 author:Andre R. Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar, Fernando J. Von Zuben, Arindam Banerjee category:cs.LG stat.ML I.5.1  J.2  published:2014-09-01 summary:Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of task relationships. In particular, we consider a joint estimation problem of the task relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship structure learning component builds on recent advances in structure learning of Gaussian graphical models based on sparse estimators of the precision (inverse covariance) matrix. We illustrate the effectiveness of the proposed model on a variety of synthetic and benchmark datasets for regression and classification. We also consider the problem of combining climate model outputs for better projections of future climate, with focus on temperature in South America, and show that the proposed model outperforms several existing methods for the problem. version:2
arxiv-1211-2459 | Measures of Entropy from Data Using Infinitely Divisible Kernels | http://arxiv.org/abs/1211.2459 | id:1211.2459 author:Luis G. Sanchez Giraldo, Murali Rao, Jose C. Principe category:cs.LG cs.IT math.IT stat.ML  published:2012-11-11 summary:Information theory provides principled ways to analyze different inference and learning problems such as hypothesis testing, clustering, dimensionality reduction, classification, among others. However, the use of information theoretic quantities as test statistics, that is, as quantities obtained from empirical data, poses a challenging estimation problem that often leads to strong simplifications such as Gaussian models, or the use of plug in density estimators that are restricted to certain representation of the data. In this paper, a framework to non-parametrically obtain measures of entropy directly from data using operators in reproducing kernel Hilbert spaces defined by infinitely divisible kernels is presented. The entropy functionals, which bear resemblance with quantum entropies, are defined on positive definite matrices and satisfy similar axioms to those of Renyi's definition of entropy. Convergence of the proposed estimators follows from concentration results on the difference between the ordered spectrum of the Gram matrices and the integral operators associated to the population quantities. In this way, capitalizing on both the axiomatic definition of entropy and on the representation power of positive definite kernels, the proposed measure of entropy avoids the estimation of the probability distribution underlying the data. Moreover, estimators of kernel-based conditional entropy and mutual information are also defined. Numerical experiments on independence tests compare favourably with state of the art. version:3
arxiv-1409-0470 | Neural coordination can be enhanced by occasional interruption of normal firing patterns: A self-optimizing spiking neural network model | http://arxiv.org/abs/1409.0470 | id:1409.0470 author:Alexander Woodward, Tom Froese, Takashi Ikegami category:nlin.AO cs.NE q-bio.NC 92B20  published:2014-09-01 summary:The state space of a conventional Hopfield network typically exhibits many different attractors of which only a small subset satisfy constraints between neurons in a globally optimal fashion. It has recently been demonstrated that combining Hebbian learning with occasional alterations of normal neural states avoids this problem by means of self-organized enlargement of the best basins of attraction. However, so far it is not clear to what extent this process of self-optimization is also operative in real brains. Here we demonstrate that it can be transferred to more biologically plausible neural networks by implementing a self-optimizing spiking neural network model. In addition, by using this spiking neural network to emulate a Hopfield network with Hebbian learning, we attempt to make a connection between rate-based and temporal coding based neural systems. Although further work is required to make this model more realistic, it already suggests that the efficacy of the self-optimizing process is independent from the simplifying assumptions of a conventional Hopfield network. We also discuss natural and cultural processes that could be responsible for occasional alteration of neural firing patterns in actual brains version:1
arxiv-1306-3874 | Classifying and Visualizing Motion Capture Sequences using Deep Neural Networks | http://arxiv.org/abs/1306.3874 | id:1306.3874 author:Kyunghyun Cho, Xi Chen category:cs.CV  published:2013-06-17 summary:The gesture recognition using motion capture data and depth sensors has recently drawn more attention in vision recognition. Currently most systems only classify dataset with a couple of dozens different actions. Moreover, feature extraction from the data is often computational complex. In this paper, we propose a novel system to recognize the actions from skeleton data with simple, but effective, features using deep neural networks. Features are extracted for each frame based on the relative positions of joints (PO), temporal differences (TD), and normalized trajectories of motion (NT). Given these features a hybrid multi-layer perceptron is trained, which simultaneously classifies and reconstructs input data. We use deep autoencoder to visualize learnt features, and the experiments show that deep neural networks can capture more discriminative information than, for instance, principal component analysis can. We test our system on a public database with 65 classes and more than 2,000 motion sequences. We obtain an accuracy above 95% which is, to our knowledge, the state of the art result for such a large dataset. version:2
arxiv-1403-6290 | Spectral Sparse Representation for Clustering: Evolved from PCA, K-means, Laplacian Eigenmap, and Ratio Cut | http://arxiv.org/abs/1403.6290 | id:1403.6290 author:Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu category:cs.CV  published:2014-03-25 summary:Dimensionality reduction methods, e.g. PCA and Laplacian eigenmap (LE), and cluster analysis methods, e.g. K-means and ratio cut (Rcut), are two kinds of widely-used unsupervised data analysis tools. The former concerns high representation fidelity, while the latter semantics. Some preliminary relations between these methods have been established in the literature, but they are not yet integrated into a unified framework. In this paper, we show that under an ideal condition, the four methods: PCA, K-means, LE, and Rcut, are unified together; and when the ideal condition is relaxed, the unification evolves to a new sparse representation method, called spectral sparse representation (SSR). It achieves the same representation fidelity as PCA/LE, and is able to reveal the cluster structure of data as K-means/Rcut. SSR is inherently related to cluster analysis, and the sparse codes can be directly used to do clustering. An efficient algorithm NSCrt is developed to solve the sparse codes of SSR. It is observed that NSCrt is able to effectively recover the underlying solutions. As a direct application of SSR, a new clustering algorithm Scut is devised. It reaches the start-of-the-art performance among spectral clustering methods. Compared with K-means based clustering methods, Scut does not depend on initialization and avoids getting trapped in local minima; and the solutions are comparable to the optimal ones of K-means run many times. Extensive experiments using data sets of different nature demonstrate the properties and strengths of SSR, NSCrt, and Scut. version:2
arxiv-1409-0347 | Multi-tensor Completion for Estimating Missing Values in Video Data | http://arxiv.org/abs/1409.0347 | id:1409.0347 author:Chao Li, Lili Guo, Andrzej Cichocki category:cs.CV  published:2014-09-01 summary:Many tensor-based data completion methods aim to solve image and video in-painting problems. But, all methods were only developed for a single dataset. In most of real applications, we can usually obtain more than one dataset to reflect one phenomenon, and all the datasets are mutually related in some sense. Thus one question raised whether such the relationship can improve the performance of data completion or not? In the paper, we proposed a novel and efficient method by exploiting the relationship among datasets for multi-video data completion. Numerical results show that the proposed method significantly improve the performance of video in-painting, particularly in the case of very high missing percentage. version:1
arxiv-1409-0334 | Storing sequences in binary tournament-based neural networks | http://arxiv.org/abs/1409.0334 | id:1409.0334 author:Xiaoran Jiang, Vincent Gripon, Claude Berrou, Michael Rabbat category:cs.NE  published:2014-09-01 summary:An extension to a recently introduced architecture of clique-based neural networks is presented. This extension makes it possible to store sequences with high efficiency. To obtain this property, network connections are provided with orientation and with flexible redundancy carried by both spatial and temporal redundancy, a mechanism of anticipation being introduced in the model. In addition to the sequence storage with high efficiency, this new scheme also offers biological plausibility. In order to achieve accurate sequence retrieval, a double layered structure combining hetero-association and auto-association is also proposed. version:1
arxiv-1112-4164 | A Geometric Approach For Fully Automatic Chromosome Segmentation | http://arxiv.org/abs/1112.4164 | id:1112.4164 author:Shervin Minaee, Mehran Fotouhi, Babak Hossein Khalaj category:cs.CV  published:2011-12-18 summary:A fundamental task in human chromosome analysis is chromosome segmentation. Segmentation plays an important role in chromosome karyotyping. The first step in segmentation is to remove intrusive objects such as stain debris and other noises. The next step is detection of touching and overlapping chromosomes, and the final step is separation of such chromosomes. Common methods for separation between touching chromosomes are interactive and require human intervention for correct separation between touching and overlapping chromosomes. In this paper, a geometric-based method is used for automatic detection of touching and overlapping chromosomes and separating them. The proposed scheme performs segmentation in two phases. In the first phase, chromosome clusters are detected using three geometric criteria, and in the second phase, chromosome clusters are separated using a cut-line. Most of earlier methods did not work properly in case of chromosome clusters that contained more than two chromosomes. Our method, on the other hand, is quite efficient in separation of such chromosome clusters. At each step, one separation will be performed and this algorithm is repeated until all individual chromosomes are separated. Another important point about the proposed method is that it uses the geometric features of chromosomes which are independent of the type of images and it can easily be applied to any type of images such as binary images and does not require multispectral images as well. We have applied our method to a database containing 62 touching and partially overlapping chromosomes and a success rate of 91.9% is achieved. version:5
arxiv-1409-0280 | Towards a Calculus of Echo State Networks | http://arxiv.org/abs/1409.0280 | id:1409.0280 author:Alireza Goudarzi, Darko Stefanovic category:cs.NE  published:2014-09-01 summary:Reservoir computing is a recent trend in neural networks which uses the dynamical perturbations on the phase space of a system to compute a desired target function. We present how one can formulate an expectation of system performance in a simple class of reservoir computing called echo state networks. In contrast with previous theoretical frameworks, which only reveal an upper bound on the total memory in the system, we analytically calculate the entire memory curve as a function of the structure of the system and the properties of the input and the target function. We demonstrate the precision of our framework by validating its result for a wide range of system sizes and spectral radii. Our analytical calculation agrees with numerical simulations. To the best of our knowledge this work presents the first exact analytical characterization of the memory curve in echo state networks. version:1
arxiv-1409-0203 | Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix Completion Algorithm and Theoretical Guarantees | http://arxiv.org/abs/1409.0203 | id:1409.0203 author:Mohammad J. Taghizadeh, Reza Parhizkar, Philip N. Garner, Herve Bourlard, Afsaneh Asaei category:cs.SD cs.LG  published:2014-08-31 summary:This paper addresses the problem of ad hoc microphone array calibration where only partial information about the distances between microphones is available. We construct a matrix consisting of the pairwise distances and propose to estimate the missing entries based on a novel Euclidean distance matrix completion algorithm by alternative low-rank matrix completion and projection onto the Euclidean distance space. This approach confines the recovered matrix to the EDM cone at each iteration of the matrix completion algorithm. The theoretical guarantees of the calibration performance are obtained considering the random and locally structured missing entries as well as the measurement noise on the known distances. This study elucidates the links between the calibration error and the number of microphones along with the noise level and the ratio of missing distances. Thorough experiments on real data recordings and simulated setups are conducted to demonstrate these theoretical insights. A significant improvement is achieved by the proposed Euclidean distance matrix completion algorithm over the state-of-the-art techniques for ad hoc microphone array calibration. version:1
arxiv-1207-3538 | Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models | http://arxiv.org/abs/1207.3538 | id:1207.3538 author:Quan Wang category:cs.CV  published:2012-07-15 summary:Principal component analysis (PCA) is a popular tool for linear dimensionality reduction and feature extraction. Kernel PCA is the nonlinear form of PCA, which better exploits the complicated spatial structure of high-dimensional features. In this paper, we first review the basic ideas of PCA and kernel PCA. Then we focus on the reconstruction of pre-images for kernel PCA. We also give an introduction on how PCA is used in active shape models (ASMs), and discuss how kernel PCA can be applied to improve traditional ASMs. Then we show some experimental results to compare the performance of kernel PCA and standard PCA for classification problems. We also implement the kernel PCA-based ASMs, and use it to construct human face models. version:3
arxiv-1409-0107 | A Plug&Play P300 BCI Using Information Geometry | http://arxiv.org/abs/1409.0107 | id:1409.0107 author:Alexandre Barachant, Marco Congedo category:cs.LG cs.HC stat.ML  published:2014-08-30 summary:This paper presents a new classification methods for Event Related Potentials (ERP) based on an Information geometry framework. Through a new estimation of covariance matrices, this work extend the use of Riemannian geometry, which was previously limited to SMR-based BCI, to the problem of classification of ERPs. As compared to the state-of-the-art, this new method increases performance, reduces the number of data needed for the calibration and features good generalisation across sessions and subjects. This method is illustrated on data recorded with the P300-based game brain invaders. Finally, an online and adaptive implementation is described, where the BCI is initialized with generic parameters derived from a database and continuously adapt to the individual, allowing the user to play the game without any calibration while keeping a high accuracy. version:1
arxiv-1409-0084 | Kernel Coding: General Formulation and Special Cases | http://arxiv.org/abs/1409.0084 | id:1409.0084 author:Mehrtash Harandi, Mathieu Salzmann category:cs.CV  published:2014-08-30 summary:Representing images by compact codes has proven beneficial for many visual recognition tasks. Most existing techniques, however, perform this coding step directly in image feature space, where the distributions of the different classes are typically entangled. In contrast, here, we study the problem of performing coding in a high-dimensional Hilbert space, where the classes are expected to be more easily separable. To this end, we introduce a general coding formulation that englobes the most popular techniques, such as bag of words, sparse coding and locality-based coding, and show how this formulation and its special cases can be kernelized. Importantly, we address several aspects of learning in our general formulation, such as kernel learning, dictionary learning and supervised kernel coding. Our experimental evaluation on several visual recognition tasks demonstrates the benefits of performing coding in Hilbert space, and in particular of jointly learning the kernel, the dictionary and the classifier. version:1
arxiv-1409-0083 | Sparse Coding on Symmetric Positive Definite Manifolds using Bregman Divergences | http://arxiv.org/abs/1409.0083 | id:1409.0083 author:Mehrtash Harandi, Richard Hartley, Brian Lovell, Conrad Sanderson category:cs.CV  published:2014-08-30 summary:This paper introduces sparse coding and dictionary learning for Symmetric Positive Definite (SPD) matrices, which are often used in machine learning, computer vision and related areas. Unlike traditional sparse coding schemes that work in vector spaces, in this paper we discuss how SPD matrices can be described by sparse combination of dictionary atoms, where the atoms are also SPD matrices. We propose to seek sparse coding by embedding the space of SPD matrices into Hilbert spaces through two types of Bregman matrix divergences. This not only leads to an efficient way of performing sparse coding, but also an online and iterative scheme for dictionary learning. We apply the proposed methods to several computer vision tasks where images are represented by region covariance matrices. Our proposed algorithms outperform state-of-the-art methods on a wide range of classification tasks, including face recognition, action recognition, material classification and texture categorization. version:1
arxiv-1107-2379 | Data Stability in Clustering: A Closer Look | http://arxiv.org/abs/1107.2379 | id:1107.2379 author:Shalev Ben-David, Lev Reyzin category:cs.LG cs.DS  published:2011-07-12 summary:We consider the model introduced by Bilu and Linial (2010), who study problems for which the optimal clustering does not change when distances are perturbed. They show that even when a problem is NP-hard, it is sometimes possible to obtain efficient algorithms for instances resilient to certain multiplicative perturbations, e.g. on the order of $O(\sqrt{n})$ for max-cut clustering. Awasthi et al. (2010) consider center-based objectives, and Balcan and Liang (2011) analyze the $k$-median and min-sum objectives, giving efficient algorithms for instances resilient to certain constant multiplicative perturbations. Here, we are motivated by the question of to what extent these assumptions can be relaxed while allowing for efficient algorithms. We show there is little room to improve these results by giving NP-hardness lower bounds for both the $k$-median and min-sum objectives. On the other hand, we show that constant multiplicative resilience parameters can be so strong as to make the clustering problem trivial, leaving only a narrow range of resilience parameters for which clustering is interesting. We also consider a model of additive perturbations and give a correspondence between additive and multiplicative notions of stability. Our results provide a close examination of the consequences of assuming stability in data. version:5
arxiv-1408-7071 | Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for Action Recognition | http://arxiv.org/abs/1408.7071 | id:1408.7071 author:Zhenzhong Lan, Xuanchong Li, Alexandar G. Hauptmann category:cs.CV  published:2014-08-29 summary:Historically, researchers in the field have spent a great deal of effort to create image representations that have scale invariance and retain spatial location information. This paper proposes to encode equivalent temporal characteristics in video representations for action recognition. To achieve temporal scale invariance, we develop a method called temporal scale pyramid (TSP). To encode temporal information, we present and compare two methods called temporal extension descriptor (TED) and temporal division pyramid (TDP) . Our purpose is to suggest solutions for matching complex actions that have large variation in velocity and appearance, which is missing from most current action representations. The experimental results on four benchmark datasets, UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach and significantly outperform state-of-the-art methods. Most noticeably, we achieve 65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51 and Hollywood2 datasets which constitutes an absolute improvement over the state-of-the-art by 7.8% and 3.9%, respectively. version:1
arxiv-1408-6988 | An Information Retrieval Approach to Short Text Conversation | http://arxiv.org/abs/1408.6988 | id:1408.6988 author:Zongcheng Ji, Zhengdong Lu, Hang Li category:cs.IR cs.CL  published:2014-08-29 summary:Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather "intelligently", when combined with a huge repository of conversation data from social media. version:1
arxiv-1408-6980 | Augmentation Schemes for Particle MCMC | http://arxiv.org/abs/1408.6980 | id:1408.6980 author:Paul Fearnhead, Loukia Meligkotsidou category:stat.CO stat.ML  published:2014-08-29 summary:Particle MCMC involves using a particle filter within an MCMC algorithm. For inference of a model which involves an unobserved stochastic process, the standard implementation uses the particle filter to propose new values for the stochastic process, and MCMC moves to propose new values for the parameters. We show how particle MCMC can be generalised beyond this. Our key idea is to introduce new latent variables. We then use the MCMC moves to update the latent variables, and the particle filter to propose new values for the parameters and stochastic process given the latent variables. A generic way of defining these latent variables is to model them as pseudo-observations of the parameters or of the stochastic process. By choosing the amount of information these latent variables have about the parameters and the stochastic process we can often improve the mixing of the particle MCMC algorithm by trading off the Monte Carlo error of the particle filter and the mixing of the MCMC moves. We show that using pseudo-observations within particle MCMC can improve its efficiency in certain scenarios: dealing with initialisation problems of the particle filter; speeding up the mixing of particle Gibbs when there is strong dependence between the parameters and the stochastic process; and enabling further MCMC steps to be used within the particle filter. version:1
arxiv-1408-6974 | Fast Disk Conformal Parameterization of Simply-connected Open Surfaces | http://arxiv.org/abs/1408.6974 | id:1408.6974 author:Pui Tung Choi, Lok Ming Lui category:cs.CG cs.CV cs.GR cs.MM math.DG  published:2014-08-29 summary:Surface parameterizations have been widely used in computer graphics and geometry processing. In particular, as simply-connected open surfaces are conformally equivalent to the unit disk, it is desirable to compute the disk conformal parameterizations of the surfaces. In this paper, we propose a novel algorithm for the conformal parameterization of a simply-connected open surface onto the unit disk, which significantly speeds up the computation, enhances the conformality and stability, and guarantees the bijectivity. The conformality distortions at the inner region and on the boundary are corrected by two steps, with the aid of an iterative scheme using quasi-conformal theories. Experimental results demonstrate the effectiveness of our proposed method. version:1
arxiv-1408-6963 | Comment on "Ensemble Projection for Semi-supervised Image Classification" | http://arxiv.org/abs/1408.6963 | id:1408.6963 author:Xavier Boix, Gemma Roig, Luc Van Gool category:cs.CV  published:2014-08-29 summary:In a series of papers by Dai and colleagues [1,2], a feature map (or kernel) was introduced for semi- and unsupervised learning. This feature map is build from the output of an ensemble of classifiers trained without using the ground-truth class labels. In this critique, we analyze the latest version of this series of papers, which is called Ensemble Projections [2]. We show that the results reported in [2] were not well conducted, and that Ensemble Projections performs poorly for semi-supervised learning. version:1
arxiv-1408-6788 | Strongly Incremental Repair Detection | http://arxiv.org/abs/1408.6788 | id:1408.6788 author:Julian Hough, Matthew Purver category:cs.CL  published:2014-08-28 summary:We present STIR (STrongly Incremental Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency. STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs. Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards. version:2
arxiv-1312-1903 | A sequential reduction method for inference in generalized linear mixed models | http://arxiv.org/abs/1312.1903 | id:1312.1903 author:Helen Ogden category:stat.CO stat.ME stat.ML  published:2013-12-06 summary:The likelihood for the parameters of a generalized linear mixed model involves an integral which may be of very high dimension. Because of this intractability, many approximations to the likelihood have been proposed, but all can fail when the model is sparse, in that there is only a small amount of information available on each random effect. The sequential reduction method described in this paper exploits the dependence structure of the posterior distribution of the random effects to reduce substantially the cost of finding an accurate approximation to the likelihood in models with sparse structure. version:2
arxiv-1408-6915 | Binary matrices of optimal autocorrelations as alignment marks | http://arxiv.org/abs/1408.6915 | id:1408.6915 author:Scott A. Skirlo, Ling Lu, Marin Soljačić category:cs.CV cs.IT math.IT  published:2014-08-29 summary:We define a new class of binary matrices by maximizing the peak-sidelobe distances in the aperiodic autocorrelations. These matrices can be used as robust position marks for in-plane spatial alignment. The optimal square matrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matrices of 8 by 8 and 9 by 9 were found by exhaustive searches. version:1
arxiv-1408-6911 | Text Line Identification in Tagore's Manuscript | http://arxiv.org/abs/1408.6911 | id:1408.6911 author:Chandranath Adak, Bidyut B. Chaudhuri category:cs.CV  published:2014-08-29 summary:In this paper, a text line identification method is proposed. The text lines of printed document are easy to segment due to uniform straightness of the lines and sufficient gap between the lines. But in handwritten documents, the line is non-uniform and interline gaps are variable. We take Rabindranath Tagore's manuscript as it is one of the most difficult manuscripts that contain doodles. Our method consists of a pre-processing stage to clean the document image. Then we separate doodles from the manuscript to get the textual region. After that we identify the text lines on the manuscript. For text line identification, we use window examination, black run-length smearing, horizontal histogram and connected component analysis. version:1
arxiv-1408-6762 | Chatbot for admissions | http://arxiv.org/abs/1408.6762 | id:1408.6762 author:Nikolaos Polatidis category:cs.CY cs.CL  published:2014-08-28 summary:The communication of potential students with a university department is performed manually and it is a very time consuming procedure. The opportunity to communicate with on a one-to-one basis is highly valued. However with many hundreds of applications each year, one-to-one conversations are not feasible in most cases. The communication will require a member of academic staff to expend several hours to find suitable answers and contact each student. It would be useful to reduce his costs and time. The project aims to reduce the burden on the head of admissions, and potentially other users, by developing a convincing chatbot. A suitable algorithm must be devised to search through the set of data and find a potential answer. The program then replies to the user and provides a relevant web link if the user is not satisfied by the answer. Furthermore a web interface is provided for both users and an administrator. The achievements of the project can be summarised as follows. To prepare the background of the project a literature review was undertaken, together with an investigation of existing tools, and consultation with the head of admissions. The requirements of the system were established and a range of algorithms and tools were investigated, including keyword and template matching. An algorithm that combines keyword matching with string similarity has been developed. A usable system using the proposed algorithm has been implemented. The system was evaluated by keeping logs of questions and answers and by feedback received by potential students that used it. version:1
arxiv-1404-3606 | PCANet: A Simple Deep Learning Baseline for Image Classification? | http://arxiv.org/abs/1404.3606 | id:1404.3606 author:Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, Yi Ma category:cs.CV cs.LG cs.NE  published:2014-04-14 summary:In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition. version:2
arxiv-1408-6741 | Memcomputing and Swarm Intelligence | http://arxiv.org/abs/1408.6741 | id:1408.6741 author:Y. V. Pershin, M. Di Ventra category:cs.NE cond-mat.mes-hall cs.ET  published:2014-08-28 summary:We explore the relation between memcomputing, namely computing with and in memory, and swarm intelligence algorithms. In particular, we show that one can design memristive networks to solve short-path optimization problems that can also be solved by ant-colony algorithms. By employing appropriate memristive elements one can demonstrate an almost one-to-one correspondence between memcomputing and ant colony optimization approaches. However, the memristive network has the capability of finding the solution in one deterministic step, compared to the stochastic multi-step ant colony optimization. This result paves the way for nanoscale hardware implementations of several swarm intelligence algorithms that are presently explored, from scheduling problems to robotics. version:1
arxiv-1408-6693 | A study of the fixed points and spurious solutions of the FastICA algorithm | http://arxiv.org/abs/1408.6693 | id:1408.6693 author:Tianwen Wei category:stat.ML  published:2014-08-28 summary:The FastICA algorithm is one of the most popular iterative algorithms in the domain of linear independent component analysis. Despite its success, it is observed that FastICA occasionally yields outcomes that do not correspond to any true solutions (known as demixing vectors) of the ICA problem. These outcomes are commonly referred to as spurious solutions. Although FastICA is among the most extensively studied ICA algorithms, the occurrence of spurious solutions are not yet completely understood by the community. In this contribution, we aim at addressing this issue. In the first part of this work, we are interested in the relationship between demixing vectors, local optimizers of the contrast function and (attractive or unattractive) fixed points of FastICA algorithm. Characterizations of these sets are given, and an inclusion relationship is discovered. In the second part, we investigate the possible scenarios where spurious solutions occur. We show that when certain bimodal Gaussian mixtures distributions are involved, there may exist spurious solutions that are attractive fixed points of FastICA. In this case, popular nonlinearities such as "gauss" or "tanh" tend to yield spurious solutions, whereas only "kurtosis" may give reliable results. Some advices are given for the practical choice of nonlinearity function. version:1
arxiv-1212-5156 | Nonparametric ridge estimation | http://arxiv.org/abs/1212.5156 | id:1212.5156 author:Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH  published:2012-12-20 summary:We study the problem of estimating the ridges of a density function. Ridge estimation is an extension of mode finding and is useful for understanding the structure of a density. It can also be used to find hidden structure in point cloud data. We show that, under mild regularity conditions, the ridges of the kernel density estimator consistently estimate the ridges of the true density. When the data are noisy measurements of a manifold, we show that the ridges are close and topologically similar to the hidden manifold. To find the estimated ridges in practice, we adapt the modified mean-shift algorithm proposed by Ozertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical experiments verify that the algorithm is accurate. version:3
arxiv-1408-6618 | Falsifiable implies Learnable | http://arxiv.org/abs/1408.6618 | id:1408.6618 author:David Balduzzi category:cs.LG math.ST stat.ML stat.TH  published:2014-08-28 summary:The paper demonstrates that falsifiability is fundamental to learning. We prove the following theorem for statistical learning and sequential prediction: If a theory is falsifiable then it is learnable -- i.e. admits a strategy that predicts optimally. An analogous result is shown for universal induction. version:1
arxiv-1408-6617 | Task-group Relatedness and Generalization Bounds for Regularized Multi-task Learning | http://arxiv.org/abs/1408.6617 | id:1408.6617 author:Chao Zhang, Dacheng Tao, Tao Hu, Xiang Li category:cs.LG  published:2014-08-28 summary:In this paper, we study the generalization performance of regularized multi-task learning (RMTL) in a vector-valued framework, where MTL is considered as a learning process for vector-valued functions. We are mainly concerned with two theoretical questions: 1) under what conditions does RMTL perform better with a smaller task sample size than STL? 2) under what conditions is RMTL generalizable and can guarantee the consistency of each task during simultaneous learning? In particular, we investigate two types of task-group relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM), both of which detect the dependence between two groups of multiple related tasks (MRTs). We then introduce the Cartesian product-based uniform entropy number (CPUEN) to measure the complexities of vector-valued function classes. By applying the specific deviation and the symmetrization inequalities to the vector-valued framework, we obtain the generalization bound for RMTL, which is the upper bound of the joint probability of the event that there is at least one task with a large empirical discrepancy between the expected and empirical risks. Finally, we present a sufficient condition to guarantee the consistency of each task in the simultaneous learning process, and we discuss how task relatedness affects the generalization performance of RMTL. Our theoretical findings answer the aforementioned two questions. version:1
arxiv-1408-5544 | To lie or not to lie in a subspace | http://arxiv.org/abs/1408.5544 | id:1408.5544 author:Daniel L. Pimentel-Alarcón category:stat.ML cs.LG  published:2014-08-24 summary:Give deterministic necessary and sufficient conditions to guarantee that if a subspace fits certain partially observed data from a union of subspaces, it is because such data really lies in a subspace. Furthermore, Give deterministic necessary and sufficient conditions to guarantee that if a subspace fits certain partially observed data, such subspace is unique. Do this by characterizing when and only when a set of incomplete vectors behaves as a single but complete one. version:2
arxiv-1408-6335 | Compression, Restoration, Re-sampling, Compressive Sensing: Fast Transforms in Digital Imaging | http://arxiv.org/abs/1408.6335 | id:1408.6335 author:Leonid Yaroslavsky category:cs.CV physics.optics  published:2014-08-27 summary:Transform image processing methods are methods that work in domains of image transforms, such as Discrete Fourier, Discrete Cosine, Wavelet and alike. They are the basic tool in image compression, in image restoration, in image re-sampling and geometrical transformations and can be traced back to early 1970-ths. The paper presents a review of these methods with emphasis on their comparison and relationships, from the very first steps of transform image compression methods to adaptive and local adaptive transform domain filters for image restoration, to methods of precise image re-sampling and image reconstruction from sparse samples and up to "compressive sensing" approach that has gained popularity in last few years. The review has a tutorial character and purpose. version:1
arxiv-1408-2003 | LARSEN-ELM: Selective Ensemble of Extreme Learning Machines using LARS for Blended Data | http://arxiv.org/abs/1408.2003 | id:1408.2003 author:Bo Han, Bo He, Rui Nian, Mengmeng Ma, Shujing Zhang, Minghui Li, Amaury Lendasse category:cs.LG stat.ML  published:2014-08-09 summary:Extreme learning machine (ELM) as a neural network algorithm has shown its good performance, such as fast speed, simple structure etc, but also, weak robustness is an unavoidable defect in original ELM for blended data. We present a new machine learning framework called LARSEN-ELM for overcoming this problem. In our paper, we would like to show two key steps in LARSEN-ELM. In the first step, preprocessing, we select the input variables highly related to the output using least angle regression (LARS). In the second step, training, we employ Genetic Algorithm (GA) based selective ensemble and original ELM. In the experiments, we apply a sum of two sines and four datasets from UCI repository to verify the robustness of our approach. The experimental results show that compared with original ELM and other methods such as OP-ELM, GASEN-ELM and LSBoost, LARSEN-ELM significantly improve robustness performance while keeping a relatively high speed. version:2
arxiv-1410-0969 | A Model of Plant Identification System Using GLCM, Lacunarity And Shen Features | http://arxiv.org/abs/1410.0969 | id:1410.0969 author:Abdul Kadir category:cs.CV  published:2014-08-27 summary:Recently, many approaches have been introduced by several researchers to identify plants. Now, applications of texture, shape, color and vein features are common practices. However, there are many possibilities of methods can be developed to improve the performance of such identification systems. Therefore, several experiments had been conducted in this research. As a result, a new novel approach by using combination of Gray-Level Co-occurrence Matrix, lacunarity and Shen features and a Bayesian classifier gives a better result compared to other plant identification systems. For comparison, this research used two kinds of several datasets that were usually used for testing the performance of each plant identification system. The results show that the system gives an accuracy rate of 97.19% when using the Flavia dataset and 95.00% when using the Foliage dataset and outperforms other approaches. version:1
arxiv-1408-6218 | Adaptive Multinomial Matrix Completion | http://arxiv.org/abs/1408.6218 | id:1408.6218 author:Olga Klopp, Jean Lafond, Eric Moulines, Joseph Salmon category:math.ST stat.ML stat.TH  published:2014-08-26 summary:The task of estimating a matrix given a sample of observed entries is known as the \emph{matrix completion problem}. Most works on matrix completion have focused on recovering an unknown real-valued low-rank matrix from a random sample of its entries. Here, we investigate the case of highly quantized observations when the measurements can take only a small number of values. These quantized outputs are generated according to a probability distribution parametrized by the unknown matrix of interest. This model corresponds, for example, to ratings in recommender systems or labels in multi-class classification. We consider a general, non-uniform, sampling scheme and give theoretical guarantees on the performance of a constrained, nuclear norm penalized maximum likelihood estimator. One important advantage of this estimator is that it does not require knowledge of the rank or an upper bound on the nuclear norm of the unknown matrix and, thus, it is adaptive. We provide lower bounds showing that our estimator is minimax optimal. An efficient algorithm based on lifted coordinate gradient descent is proposed to compute the estimator. A limited Monte-Carlo experiment, using both simulated and real data is provided to support our claims. version:1
arxiv-1408-6214 | A Methodology for the Diagnostic of Aircraft Engine Based on Indicators Aggregation | http://arxiv.org/abs/1408.6214 | id:1408.6214 author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG  published:2014-08-26 summary:Aircraft engine manufacturers collect large amount of engine related data during flights. These data are used to detect anomalies in the engines in order to help companies optimize their maintenance costs. This article introduces and studies a generic methodology that allows one to build automatic early signs of anomaly detection in a way that is understandable by human operators who make the final maintenance decision. The main idea of the method is to generate a very large number of binary indicators based on parametric anomaly scores designed by experts, complemented by simple aggregations of those scores. The best indicators are selected via a classical forward scheme, leading to a much reduced number of indicators that are tuned to a data set. We illustrate the interest of the method on simulated data which contain realistic early signs of anomalies. version:1
arxiv-1408-6181 | Resolving Lexical Ambiguity in Tensor Regression Models of Meaning | http://arxiv.org/abs/1408.6181 | id:1408.6181 author:Dimitri Kartsaklis, Nal Kalchbrenner, Mehrnoosh Sadrzadeh category:cs.CL  published:2014-08-26 summary:This paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition. In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression. The results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is model-independent. version:1
arxiv-1408-6179 | Evaluating Neural Word Representations in Tensor-Based Compositional Settings | http://arxiv.org/abs/1408.6179 | id:1408.6179 author:Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Matthew Purver category:cs.CL  published:2014-08-26 summary:We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the larger-scale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. version:1
arxiv-1307-5996 | Bayesian Fusion of Multi-Band Images | http://arxiv.org/abs/1307.5996 | id:1307.5996 author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV physics.data-an stat.ME  published:2013-07-23 summary:In this paper, a Bayesian fusion technique for remotely sensed multi-band images is presented. The observed images are related to the high spectral and high spatial resolution image to be recovered through physical degradations, e.g., spatial and spectral blurring and/or subsampling defined by the sensor characteristics. The fusion problem is formulated within a Bayesian estimation framework. An appropriate prior distribution exploiting geometrical consideration is introduced. To compute the Bayesian estimator of the scene of interest from its posterior distribution, a Markov chain Monte Carlo algorithm is designed to generate samples asymptotically distributed according to the target distribution. To efficiently sample from this high-dimension distribution, a Hamiltonian Monte Carlo step is introduced in the Gibbs sampling strategy. The efficiency of the proposed fusion method is evaluated with respect to several state-of-the-art fusion techniques. In particular, low spatial resolution hyperspectral and multispectral images are fused to produce a high spatial resolution hyperspectral image. version:2
arxiv-1408-6032 | Inference of Cancer Progression Models with Biological Noise | http://arxiv.org/abs/1408.6032 | id:1408.6032 author:Ilya Korsunsky, Daniele Ramazzotti, Giulio Caravagna, Bud Mishra category:stat.ML cs.LG q-bio.QM  published:2014-08-26 summary:Many applications in translational medicine require the understanding of how diseases progress through the accumulation of persistent events. Specialized Bayesian networks called monotonic progression networks offer a statistical framework for modeling this sort of phenomenon. Current machine learning tools to reconstruct Bayesian networks from data are powerful but not suited to progression models. We combine the technological advances in machine learning with a rigorous philosophical theory of causation to produce Polaris, a scalable algorithm for learning progression networks that accounts for causal or biological noise as well as logical relations among genetic events, making the resulting models easy to interpret qualitatively. We tested Polaris on synthetically generated data and showed that it outperforms a widely used machine learning algorithm and approaches the performance of the competing special-purpose, albeit clairvoyant algorithm that is given a priori information about the model parameters. We also prove that under certain rather mild conditions, Polaris is guaranteed to converge for sufficiently large sample sizes. Finally, we applied Polaris to point mutation and copy number variation data in Prostate cancer from The Cancer Genome Atlas (TCGA) and found that there are likely three distinct progressions, one major androgen driven progression, one major non-androgen driven progression, and one novel minor androgen driven progression. version:1
arxiv-1408-5601 | Learn Convolutional Neural Network for Face Anti-Spoofing | http://arxiv.org/abs/1408.5601 | id:1408.5601 author:Jianwei Yang, Zhen Lei, Stan Z. Li category:cs.CV  published:2014-08-24 summary:Though having achieved some progresses, the hand-crafted texture features, e.g., LBP [23], LBP-TOP [11] are still unable to capture the most discriminative cues between genuine and fake faces. In this paper, instead of designing feature by ourselves, we rely on the deep convolutional neural network (CNN) to learn features of high discriminative ability in a supervised manner. Combined with some data pre-processing, the face anti-spoofing performance improves drastically. In the experiments, over 70% relative decrease of Half Total Error Rate (HTER) is achieved on two challenging datasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art. Meanwhile, the experimental results from inter-tests between two datasets indicates CNN can obtain features with better generalization ability. Moreover, the nets trained using combined data from two datasets have less biases between two datasets. version:2
arxiv-1408-4576 | Introduction to Clustering Algorithms and Applications | http://arxiv.org/abs/1408.4576 | id:1408.4576 author:Sibei Yang, Liangde Tao, Bingchen Gong category:cs.LG cs.CV  published:2014-08-20 summary:Data clustering is the process of identifying natural groupings or clusters within multidimensional data based on some similarity measure. Clustering is a fundamental process in many different disciplines. Hence, researchers from different fields are actively working on the clustering problem. This paper provides an overview of the different representative clustering methods. In addition, application of clustering in different field is briefly introduced. version:2
arxiv-1407-2721 | ARTOS -- Adaptive Real-Time Object Detection System | http://arxiv.org/abs/1407.2721 | id:1407.2721 author:Björn Barz, Erik Rodner, Joachim Denzler category:cs.CV  published:2014-07-10 summary:ARTOS is all about creating, tuning, and applying object detection models with just a few clicks. In particular, ARTOS facilitates learning of models for visual object detection by eliminating the burden of having to collect and annotate a large set of positive and negative samples manually and in addition it implements a fast learning technique to reduce the time needed for the learning step. A clean and friendly GUI guides the user through the process of model creation, adaptation of learned models to different domains using in-situ images, and object detection on both offline images and images from a video stream. A library written in C++ provides the main functionality of ARTOS with a C-style procedural interface, so that it can be easily integrated with any other project. version:2
arxiv-1409-2413 | Image processing | http://arxiv.org/abs/1409.2413 | id:1409.2413 author:Franco Rino category:cs.CV  published:2014-08-25 summary:Gabor filters can extract multi-orientation and multiscale features from face images. Researchers have designed different ways to use the magnitude of the filtered results for face recognition: Gabor Fisher classifier exploited only the magnitude information of Gabor magnitude pictures (GMPs); Local Gabor Binary Pattern uses only the gradient information. In this paper, we regard GMPs as smooth surfaces. By completely describing the shape of GMPs, we get a face representation method called Gabor Surface Feature (GSF). First, we compute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and transform them into decimal values. Finally we construct joint histograms and use subspace methods for classification. Experiments on FERET, ORL and FRGC 1.0.4 database show the effectiveness of GSF. version:1
arxiv-1408-5634 | An application of topological graph clustering to protein function prediction | http://arxiv.org/abs/1408.5634 | id:1408.5634 author:R. Sean Bowman, Douglas Heisterkamp, Jesse Johnson, Danielle O'Donnol category:cs.CE cs.LG q-bio.QM stat.ML  published:2014-08-24 summary:We use a semisupervised learning algorithm based on a topological data analysis approach to assign functional categories to yeast proteins using similarity graphs. This new approach to analyzing biological networks yields results that are as good as or better than state of the art existing approaches. version:1
arxiv-1408-5552 | Fuzzy and entropy facial recognition | http://arxiv.org/abs/1408.5552 | id:1408.5552 author:Jaejun Lee, Taeseon Yun category:cs.CV 68T10  published:2014-08-24 summary:This paper suggests an effective method for facial recognition using fuzzy theory and Shannon entropy. Combination of fuzzy theory and Shannon entropy eliminates the complication of other methods. Shannon entropy calculates the ratio of an element between faces, and fuzzy theory calculates the member ship of the entropy with 1. More details will be mentioned in Section 3. The learning performance is better than others as it is very simple, and only need two data per learning. By using factors that don't usually change during the life, the method will have a high accuracy. version:1
arxiv-1408-5516 | Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation | http://arxiv.org/abs/1408.5516 | id:1408.5516 author:Sanja Fidler, Marko Boben, Ales Leonardis category:cs.CV  published:2014-08-23 summary:Hierarchies allow feature sharing between objects at multiple levels of representation, can code exponential variability in a very compact way and enable fast inference. This makes them potentially suitable for learning and recognizing a higher number of object classes. However, the success of the hierarchical approaches so far has been hindered by the use of hand-crafted features or predetermined grouping rules. This paper presents a novel framework for learning a hierarchical compositional shape vocabulary for representing multiple object classes. The approach takes simple contour fragments and learns their frequent spatial configurations. These are recursively combined into increasingly more complex and class-specific shape compositions, each exerting a high degree of shape variability. At the top-level of the vocabulary, the compositions are sufficiently large and complex to represent the whole shapes of the objects. We learn the vocabulary layer after layer, by gradually increasing the size of the window of analysis and reducing the spatial resolution at which the shape configurations are learned. The lower layers are learned jointly on images of all classes, whereas the higher layers of the vocabulary are learned incrementally, by presenting the algorithm with one object class after another. The experimental results show that the learned multi-class object representation scales favorably with the number of object classes and achieves a state-of-the-art detection performance at both, faster inference as well as shorter training times. version:1
arxiv-1408-5456 | Interpreting Tree Ensembles with inTrees | http://arxiv.org/abs/1408.5456 | id:1408.5456 author:Houtao Deng category:cs.LG stat.ML  published:2014-08-23 summary:Tree ensembles such as random forests and boosted trees are accurate but difficult to understand, debug and deploy. In this work, we provide the inTrees (interpretable trees) framework that extracts, measures, prunes and selects rules from a tree ensemble, and calculates frequent variable interactions. An rule-based learner, referred to as the simplified tree ensemble learner (STEL), can also be formed and used for future prediction. The inTrees framework can applied to both classification and regression problems, and is applicable to many types of tree ensembles, e.g., random forests, regularized random forests, and boosted trees. We implemented the inTrees algorithms in the "inTrees" R package. version:1
arxiv-1408-5404 | A Wild Bootstrap for Degenerate Kernel Tests | http://arxiv.org/abs/1408.5404 | id:1408.5404 author:Kacper Chwialkowski, Dino Sejdinovic, Arthur Gretton category:stat.ML 62G10  published:2014-08-23 summary:A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. version:1
arxiv-1408-5449 | Stretchy Polynomial Regression | http://arxiv.org/abs/1408.5449 | id:1408.5449 author:Kar-Ann Toh category:cs.LG stat.ML  published:2014-08-23 summary:This article proposes a novel solution for stretchy polynomial regression learning. The solution comes in primal and dual closed-forms similar to that of ridge regression. Essentially, the proposed solution stretches the covariance computation via a power term thereby compresses or amplifies the estimation. Our experiments on both synthetic data and real-world data show effectiveness of the proposed method for compressive learning. version:1
arxiv-1408-5400 | Hierarchical Adaptive Structural SVM for Domain Adaptation | http://arxiv.org/abs/1408.5400 | id:1408.5400 author:Jiaolong Xu, Sebastian Ramos, David Vazquez, Antonio M. Lopez category:cs.CV cs.LG  published:2014-08-22 summary:A key topic in classification is the accuracy loss produced when the data distribution in the training (source) domain differs from that in the testing (target) domain. This is being recognized as a very relevant problem for many computer vision tasks such as image classification, object detection, and object category recognition. In this paper, we present a novel domain adaptation method that leverages multiple target domains (or sub-domains) in a hierarchical adaptation tree. The core idea is to exploit the commonalities and differences of the jointly considered target domains. Given the relevance of structural SVM (SSVM) classifiers, we apply our idea to the adaptive SSVM (A-SSVM), which only requires the target domain samples together with the existing source-domain classifier for performing the desired adaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM). As proof of concept we use HA-SSVM for pedestrian detection and object category recognition. In the former we apply HA-SSVM to the deformable part-based model (DPM) while in the latter HA-SSVM is applied to multi-category classifiers. In both cases, we show how HA-SSVM is effective in increasing the detection/recognition accuracy with respect to adaptation strategies that ignore the structure of the target data. Since, the sub-domains of the target data are not always known a priori, we shown how HA-SSVM can incorporate sub-domain structure discovery for object category recognition. version:1
arxiv-1408-5389 | Computing Multi-Relational Sufficient Statistics for Large Databases | http://arxiv.org/abs/1408.5389 | id:1408.5389 author:Zhensong Qian, Oliver Schulte, Yan Sun category:cs.LG cs.DB H.2.8; H.2.4  published:2014-08-22 summary:Databases contain information about which relationships do and do not hold among entities. To make this information accessible for statistical analysis requires computing sufficient statistics that combine information from different database tables. Such statistics may involve any number of {\em positive and negative} relationships. With a naive enumeration approach, computing sufficient statistics for negative relationships is feasible only for small databases. We solve this problem with a new dynamic programming algorithm that performs a virtual join, where the requisite counts are computed without materializing join tables. Contingency table algebra is a new extension of relational algebra, that facilitates the efficient implementation of this M\"obius virtual join operation. The M\"obius Join scales to large datasets (over 1M tuples) with complex schemas. Empirical evaluation with seven benchmark datasets showed that information about the presence and absence of links can be exploited in feature selection, association rule mining, and Bayesian network learning. version:1
arxiv-1408-5369 | Statistical and computational trade-offs in estimation of sparse principal components | http://arxiv.org/abs/1408.5369 | id:1408.5369 author:Tengyao Wang, Quentin Berthet, Richard J. Samworth category:math.ST stat.ML stat.TH 62H25  68Q17  published:2014-08-22 summary:In recent years, Sparse Principal Component Analysis has emerged as an extremely popular dimension reduction technique for high-dimensional data. The theoretical challenge, in the simplest case, is to estimate the leading eigenvector of a population covariance matrix under the assumption that this eigenvector is sparse. An impressive range of estimators have been proposed; some of these are fast to compute, while others are known to achieve the minimax optimal rate over certain Gaussian or subgaussian classes. In this paper we show that, under a widely-believed assumption from computational complexity theory, there is a fundamental trade-off between statistical and computational performance in this problem. More precisely, working with new, larger classes satisfying a Restricted Covariance Concentration condition, we show that no randomised polynomial time algorithm can achieve the minimax optimal rate. On the other hand, we also study a (polynomial time) variant of the well-known semidefinite relaxation estimator, and show that it attains essentially the optimal rate among all randomised polynomial time algorithms. version:1
arxiv-1408-5352 | Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in Polynomial Time | http://arxiv.org/abs/1408.5352 | id:1408.5352 author:Zhaoran Wang, Huanran Lu, Han Liu category:stat.ML cs.LG  published:2014-08-22 summary:Sparse principal component analysis (PCA) involves nonconvex optimization for which the global solution is hard to obtain. To address this issue, one popular approach is convex relaxation. However, such an approach may produce suboptimal estimators due to the relaxation effect. To optimally estimate sparse principal subspaces, we propose a two-stage computational framework named "tighten after relax": Within the 'relax' stage, we approximately solve a convex relaxation of sparse PCA with early stopping to obtain a desired initial estimator; For the 'tighten' stage, we propose a novel algorithm called sparse orthogonal iteration pursuit (SOAP), which iteratively refines the initial estimator by directly solving the underlying nonconvex problem. A key concept of this two-stage framework is the basin of attraction. It represents a local region within which the `tighten' stage has desired computational and statistical guarantees. We prove that, the initial estimator obtained from the 'relax' stage falls into such a region, and hence SOAP geometrically converges to a principal subspace estimator which is minimax-optimal within a certain model class. Unlike most existing sparse PCA estimators, our approach applies to the non-spiked covariance models, and adapts to non-Gaussianity as well as dependent data settings. Moreover, through analyzing the computational complexity of the two stages, we illustrate an interesting phenomenon that larger sample size can reduce the total iteration complexity. Our framework motivates a general paradigm for solving many complex statistical problems which involve nonconvex optimization with provable guarantees. version:1
arxiv-1408-5350 | Structural bias in population-based algorithms | http://arxiv.org/abs/1408.5350 | id:1408.5350 author:Anna V. Kononova, David W. Corne, Philippe De Wilde, Vsevolod Shneer, Fabio Caraffini category:cs.NE  published:2014-08-22 summary:Challenging optimisation problems are abundant in all areas of science. Since the 1950s, scientists have developed ever-diversifying families of black box optimisation algorithms designed to address any optimisation problem, requiring only that quality of a candidate solution is calculated via a fitness function specific to the problem. For such algorithms to be successful, at least three properties are required: an effective informed sampling strategy, that guides generation of new candidates on the basis of fitnesses and locations of previously visited candidates; mechanisms to ensure efficiency, so that same candidates are not repeatedly visited; absence of structural bias, which, if present, would predispose the algorithm towards limiting its search to some regions of solution space. The first two of these properties have been extensively investigated, however the third is little understood. In this article we provide theoretical and empirical analyses that contribute to the understanding of structural bias. We prove a theorem concerning dynamics of population variance in the case of real-valued search spaces. This reveals how structural bias can manifest as non-uniform clustering of population over time. Theory predicts that structural bias is exacerbated with increasing population size and problem difficulty. These predictions reveal two previously unrecognised aspects of structural bias. Respectively, increasing population size, though ostensibly promoting diversity, will magnify any inherent structural bias, and effects of structural bias are more apparent when faced with difficult problems. Our theoretical result also suggests that two commonly used approaches to enhancing exploration, increasing population size and increasing disruptiveness of search operators, have quite distinct implications in terms of structural bias. version:1
arxiv-1408-5348 | Bat Algorithm is Better Than Intermittent Search Strategy | http://arxiv.org/abs/1408.5348 | id:1408.5348 author:Xin-She Yang, Suash Deb, Simon Fong category:math.OC cs.NE  published:2014-08-22 summary:The efficiency of any metaheuristic algorithm largely depends on the way of balancing local intensive exploitation and global diverse exploration. Studies show that bat algorithm can provide a good balance between these two key components with superior efficiency. In this paper, we first review some commonly used metaheuristic algorithms, and then compare the performance of bat algorithm with the so-called intermittent search strategy. From simulations, we found that bat algorithm is better than the optimal intermittent search strategy. We also analyse the comparison results and their implications for higher dimensional optimization problems. In addition, we also apply bat algorithm in solving business optimization and engineering design problems. version:1
arxiv-1408-5343 | Cuckoo Search: A Brief Literature Review | http://arxiv.org/abs/1408.5343 | id:1408.5343 author:I. Fister Jr., X. S. Yang, D. Fister, I. Fister category:math.OC cs.NE nlin.AO 90C26  published:2014-08-22 summary:Cuckoo search (CS) was introduced in 2009, and it has attracted great attention due to its promising efficiency in solving many optimization problems and real-world applications. In the last few years, many papers have been published regarding cuckoo search, and the relevant literature has expanded significantly. This chapter summarizes briefly the majority of the literature about cuckoo search in peer-reviewed journals and conferences found so far. These references can be systematically classified into appropriate categories, which can be used as a basis for further research. version:1
arxiv-1408-5332 | Flower Pollination Algorithm: A Novel Approach for Multiobjective Optimization | http://arxiv.org/abs/1408.5332 | id:1408.5332 author:Xin-She Yang, M. Karamanoglu, X. S. He category:math.OC cs.NE nlin.AO 90C26  published:2014-08-22 summary:Multiobjective design optimization problems require multiobjective optimization techniques to solve, and it is often very challenging to obtain high-quality Pareto fronts accurately. In this paper, the recently developed flower pollination algorithm (FPA) is extended to solve multiobjective optimization problems. The proposed method is used to solve a set of multobjective test functions and two bi-objective design benchmarks, and a comparison of the proposed algorithm with other algorithms has been made, which shows that FPA is efficient with a good convergence rate. Finally, the importance for further parametric studies and theoretical analysis are highlighted and discussed. version:1
arxiv-1408-5320 | Applications and Analysis of Bio-Inspired Eagle Strategy for Engineering Optimization | http://arxiv.org/abs/1408.5320 | id:1408.5320 author:Xin-She Yang, M. Karamanoglu, T. O. Ting, Y. X. Zhao category:math.OC cs.NE nlin.AO 90C26  published:2014-08-22 summary:All swarm-intelligence-based optimization algorithms use some stochastic components to increase the diversity of solutions during the search process. Such randomization is often represented in terms of random walks. However, it is not yet clear why some randomization techniques (and thus why some algorithms) may perform better than others for a given set of problems. In this work, we analyze these randomization methods in the context of nature-inspired algorithms. We also use eagle strategy to provide basic observations and relate step sizes and search efficiency using Markov theory. Then, we apply our analysis and observations to solve four design benchmarks, including the designs of a pressure vessel, a speed reducer, a PID controller and a heat exchanger. Our results demonstrate that eagle strategy with L\'evy flights can perform extremely well in reducing the overall computational efforts. version:1
arxiv-1408-5316 | Cuckoo Search: Recent Advances and Applications | http://arxiv.org/abs/1408.5316 | id:1408.5316 author:Xin-She Yang, Suash Deb category:math.OC cs.NE nlin.AO 90-XX  published:2014-08-22 summary:Cuckoo search (CS) is a relatively new algorithm, developed by Yang and Deb in 2009, and CS is efficient in solving global optimization problems. In this paper, we review the fundamental ideas of cuckoo search and the latest developments as well as its applications. We analyze the algorithm and gain insight into its search mechanisms and find out why it is efficient. We also discuss the essence of algorithms and its link to self-organizing systems, and finally we propose some important topics for further research. version:1
arxiv-1304-5793 | Continuum armed bandit problem of few variables in high dimensions | http://arxiv.org/abs/1304.5793 | id:1304.5793 author:Hemant Tyagi, Bernd Gärtner category:cs.LG  published:2013-04-21 summary:We consider the stochastic and adversarial settings of continuum armed bandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d -> R are assumed to intrinsically depend on at most k coordinate variables implying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknown i_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R with exponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed across time, we propose a simple modification of the CAB1 algorithm where we construct the discrete set of sampling points to obtain a bound of O(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on the regret, with C(k,d) depending at most polynomially in k and sub-logarithmically in d. The construction is based on creating partitions of {1,..,d} into k disjoint subsets and is probabilistic, hence our result holds with high probability. Secondly we extend our results to also handle the more general case where (i_1,...,i_k) can change over time and derive regret bounds for the same. version:4
arxiv-1408-4660 | Joint Hierarchical Gaussian Process Model with Application to Forecast in Medical Monitoring | http://arxiv.org/abs/1408.4660 | id:1408.4660 author:Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak category:stat.ME stat.ML  published:2014-08-20 summary:A novel extrapolation method is proposed for longitudinal forecasting. A hierarchical Gaussian process model is used to combine nonlinear population change and individual memory of the past to make prediction. The prediction error is minimized through the hierarchical design. The method is further extended to joint modeling of continuous measurements and survival events. The baseline hazard, covariate and joint effects are conveniently modeled in this hierarchical structure. The estimation and inference are implemented in fully Bayesian framework using the objective and shrinkage priors. In simulation studies, this model shows robustness in latent estimation, correlation detection and high accuracy in forecasting. The model is illustrated with medical monitoring data from cystic fibrosis (CF) patients. Estimation and forecasts are obtained in the measurement of lung function and records of acute respiratory events. Keyword: Extrapolation, Joint Model, Longitudinal Model, Hierarchical Gaussian Process, Cystic Fibrosis, Medical Monitoring version:2
arxiv-1406-1953 | Automatic Extraction of Protein Interaction in Literature | http://arxiv.org/abs/1406.1953 | id:1406.1953 author:Peilei Liu, Ting Wang category:cs.CL cs.CE H.2.8; H.3.5  published:2014-06-08 summary:Protein-protein interaction extraction is the key precondition of the construction of protein knowledge network, and it is very important for the research in the biomedicine. This paper extracted directional protein-protein interaction from the biological text, using the SVM-based method. Experiments were evaluated on the LLL05 corpus with good results. The results show that dependency features are import for the protein-protein interaction extraction and features related to the interaction word are effective for the interaction direction judgment. At last, we analyzed the effects of different features and planed for the next step. version:2
arxiv-1408-5403 | Neural Mechanism of Language | http://arxiv.org/abs/1408.5403 | id:1408.5403 author:Peilei Liu, Ting Wang category:cs.NE cs.CL q-bio.NC  published:2014-08-22 summary:This paper is based on our previous work on neural coding. It is a self-organized model supported by existing evidences. Firstly, we briefly introduce this model in this paper, and then we explain the neural mechanism of language and reasoning with it. Moreover, we find that the position of an area determines its importance. Specifically, language relevant areas are in the capital position of the cortical kingdom. Therefore they are closely related with autonomous consciousness and working memories. In essence, language is a miniature of the real world. Briefly, this paper would like to bridge the gap between molecule mechanism of neurons and advanced functions such as language and reasoning. version:1
arxiv-1408-5275 | Unsupervised Spike Sorting Based on Discriminative Subspace Learning | http://arxiv.org/abs/1408.5275 | id:1408.5275 author:Mohammad Reza Keshtkaran, Zhi Yang category:cs.CV physics.med-ph  published:2014-08-22 summary:Spike sorting is a fundamental preprocessing step for many neuroscience studies which rely on the analysis of spike trains. In this paper, we present two unsupervised spike sorting algorithms based on discriminative subspace learning. The first algorithm simultaneously learns the discriminative feature subspace and performs clustering. It uses histogram of features in the most discriminative projection to detect the number of neurons. The second algorithm performs hierarchical divisive clustering that learns a discriminative 1-dimensional subspace for clustering in each level of the hierarchy until achieving almost unimodal distribution in the subspace. The algorithms are tested on synthetic and in-vivo data, and are compared against two widely used spike sorting methods. The comparative results demonstrate that our spike sorting methods can achieve substantially higher accuracy in lower dimensional feature space, and they are highly robust to noise. Moreover, they provide significantly better cluster separability in the learned subspace than in the subspace obtained by principal component analysis or wavelet transform. version:1
arxiv-1408-5246 | Improving the Interpretability of Support Vector Machines-based Fuzzy Rules | http://arxiv.org/abs/1408.5246 | id:1408.5246 author:Duc-Hien Nguyen, Manh-Thanh Le category:cs.LG cs.AI 68U35 I.2.3  published:2014-08-22 summary:Support vector machines (SVMs) and fuzzy rule systems are functionally equivalent under some conditions. Therefore, the learning algorithms developed in the field of support vector machines can be used to adapt the parameters of fuzzy systems. Extracting fuzzy models from support vector machines has the inherent advantage that the model does not need to determine the number of rules in advance. However, after the support vector machine learning, the complexity is usually high, and interpretability is also impaired. This paper not only proposes a complete framework for extracting interpretable SVM-based fuzzy modeling, but also provides optimization issues of the models. Simulations examples are given to embody the idea of this paper. version:1
arxiv-1408-5241 | A two-stage architecture for stock price forecasting by combining SOM and fuzzy-SVM | http://arxiv.org/abs/1408.5241 | id:1408.5241 author:Duc-Hien Nguyen, Manh-Thanh Le category:cs.AI cs.LG 68U35 I.2.3  published:2014-08-22 summary:This paper proposed a model to predict the stock price based on combining Self-Organizing Map (SOM) and fuzzy-Support Vector Machines (f-SVM). Extraction of fuzzy rules from raw data based on the combining of statistical machine learning models is base of this proposed approach. In the proposed model, SOM is used as a clustering algorithm to partition the whole input space into the several disjoint regions. For each partition, a set of fuzzy rules is extracted based on a f-SVM combining model. Then fuzzy rules sets are used to predict the test data using fuzzy inference algorithms. The performance of the proposed approach is compared with other models using four data sets version:1
arxiv-1408-2359 | Gap-weighted subsequences for automatic cognate identification and phylogenetic inference | http://arxiv.org/abs/1408.2359 | id:1408.2359 author:Taraka Rama category:cs.CL  published:2014-08-11 summary:In this paper, we describe the problem of cognate identification and its relation to phylogenetic inference. We introduce subsequence based features for discriminating cognates from non-cognates. We show that subsequence based features perform better than the state-of-the-art string similarity measures for the purpose of cognate identification. We use the cognate judgments for the purpose of phylogenetic inference and observe that these classifiers infer a tree which is close to the gold standard tree. The contribution of this paper is the use of subsequence features for cognate identification and to employ the cognate judgments for phylogenetic inference. version:2
arxiv-1408-5099 | Uniform Sampling for Matrix Approximation | http://arxiv.org/abs/1408.5099 | id:1408.5099 author:Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, Aaron Sidford category:cs.DS cs.LG stat.ML  published:2014-08-21 summary:Random sampling has become a critical tool in solving massive matrix problems. For linear regression, a small, manageable set of data rows can be randomly selected to approximate a tall, skinny data matrix, improving processing time significantly. For theoretical performance guarantees, each row must be sampled with probability proportional to its statistical leverage score. Unfortunately, leverage scores are difficult to compute. A simple alternative is to sample rows uniformly at random. While this often works, uniform sampling will eliminate critical row information for many natural instances. We take a fresh look at uniform sampling by examining what information it does preserve. Specifically, we show that uniform sampling yields a matrix that, in some sense, well approximates a large fraction of the original. While this weak form of approximation is not enough for solving linear regression directly, it is enough to compute a better approximation. This observation leads to simple iterative row sampling algorithms for matrix approximation that run in input-sparsity time and preserve row structure and sparsity at all intermediate steps. In addition to an improved understanding of uniform sampling, our main proof introduces a structural result of independent interest: we show that every matrix can be made to have low coherence by reweighting a small subset of its rows. version:1
arxiv-1301-3592 | Deep Learning for Detecting Robotic Grasps | http://arxiv.org/abs/1301.3592 | id:1301.3592 author:Ian Lenz, Honglak Lee, Ashutosh Saxena category:cs.LG cs.CV cs.RO  published:2013-01-16 summary:We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms. version:6
arxiv-1408-5427 | A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets | http://arxiv.org/abs/1408.5427 | id:1408.5427 author:Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek category:stat.ML cs.CL cs.IR cs.LG  published:2014-08-21 summary:Cluster analysis is a field of data analysis that extracts underlying patterns in data. One application of cluster analysis is in text-mining, the analysis of large collections of text to find similarities between documents. We used a collection of about 30,000 tweets extracted from Twitter just before the World Cup started. A common problem with real world text data is the presence of linguistic noise. In our case it would be extraneous tweets that are unrelated to dominant themes. To combat this problem, we created an algorithm that combined the DBSCAN algorithm and a consensus matrix. This way we are left with the tweets that are related to those dominant themes. We then used cluster analysis to find those topics that the tweets describe. We clustered the tweets using k-means, a commonly used clustering algorithm, and Non-Negative Matrix Factorization (NMF) and compared the results. The two algorithms gave similar results, but NMF proved to be faster and provided more easily interpreted results. We explored our results using two visualization tools, Gephi and Wordle. version:1
arxiv-1408-5032 | On the Sample Complexity of Subspace Learning | http://arxiv.org/abs/1408.5032 | id:1408.5032 author:Alessandro Rudi, Guille D. Canas, Lorenzo Rosasco category:stat.ML  published:2014-08-21 summary:A large number of algorithms in machine learning, from principal component analysis (PCA), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples. In this paper we introduce a general formulation of this problem and derive novel learning error estimates. Our results rely on natural assumptions on the spectral properties of the covariance operator associated to the data distribu- tion, and hold for a wide class of metrics between subspaces. As special cases, we discuss sharp error estimates for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods. version:1
arxiv-1403-6566 | Image Retargeting by Content-Aware Synthesis | http://arxiv.org/abs/1403.6566 | id:1403.6566 author:Weiming Dong, Fuzhang Wu, Yan Kong, Xing Mei, Tong-Yee Lee, Xiaopeng Zhang category:cs.GR cs.CV  published:2014-03-26 summary:Real-world images usually contain vivid contents and rich textural details, which will complicate the manipulation on them. In this paper, we design a new framework based on content-aware synthesis to enhance content-aware image retargeting. By detecting the textural regions in an image, the textural image content can be synthesized rather than simply distorted or cropped. This method enables the manipulation of textural & non-textural regions with different strategy since they have different natures. We propose to retarget the textural regions by content-aware synthesis and non-textural regions by fast multi-operators. To achieve practical retargeting applications for general images, we develop an automatic and fast texture detection method that can detect multiple disjoint textural regions. We adjust the saliency of the image according to the features of the textural regions. To validate the proposed method, comparisons with state-of-the-art image targeting techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method. version:2
arxiv-1402-2676 | Ranking via Robust Binary Classification and Parallel Parameter Estimation in Large-Scale Data | http://arxiv.org/abs/1402.2676 | id:1402.2676 author:Hyokun Yun, Parameswaran Raman, S. V. N. Vishwanathan category:stat.ML cs.DC cs.LG stat.CO  published:2014-02-11 summary:We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature. On the other hand, in large scale problems where explicit feature vectors and scores are not given, our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation. version:4
arxiv-1408-4849 | Swarm Intelligence Based Multi-phase OPF For Peak Power Loss Reduction In A Smart Grid | http://arxiv.org/abs/1408.4849 | id:1408.4849 author:Adnan Anwar, A. N. Mahmood category:cs.CE cs.NE  published:2014-08-21 summary:Recently there has been increasing interest in improving smart grids efficiency using computational intelligence. A key challenge in future smart grid is designing Optimal Power Flow tool to solve important planning problems including optimal DG capacities. Although, a number of OPF tools exists for balanced networks there is a lack of research for unbalanced multi-phase distribution networks. In this paper, a new OPF technique has been proposed for the DG capacity planning of a smart grid. During the formulation of the proposed algorithm, multi-phase power distribution system is considered which has unbalanced loadings, voltage control and reactive power compensation devices. The proposed algorithm is built upon a co-simulation framework that optimizes the objective by adapting a constriction factor Particle Swarm optimization. The proposed multi-phase OPF technique is validated using IEEE 8500-node benchmark distribution system. version:1
arxiv-1408-4792 | Enhanced Estimation of Autoregressive Wind Power Prediction Model Using Constriction Factor Particle Swarm Optimization | http://arxiv.org/abs/1408.4792 | id:1408.4792 author:Adnan Anwar, Abdun Naser Mahmood category:cs.CE cs.NE  published:2014-08-21 summary:Accurate forecasting is important for cost-effective and efficient monitoring and control of the renewable energy based power generation. Wind based power is one of the most difficult energy to predict accurately, due to the widely varying and unpredictable nature of wind energy. Although Autoregressive (AR) techniques have been widely used to create wind power models, they have shown limited accuracy in forecasting, as well as difficulty in determining the correct parameters for an optimized AR model. In this paper, Constriction Factor Particle Swarm Optimization (CF-PSO) is employed to optimally determine the parameters of an Autoregressive (AR) model for accurate prediction of the wind power output behaviour. Appropriate lag order of the proposed model is selected based on Akaike information criterion. The performance of the proposed PSO based AR model is compared with four well-established approaches; Forward-backward approach, Geometric lattice approach, Least-squares approach and Yule-Walker approach, that are widely used for error minimization of the AR model. To validate the proposed approach, real-life wind power data of \textit{Capital Wind Farm} was obtained from Australian Energy Market Operator. Experimental evaluation based on a number of different datasets demonstrate that the performance of the AR model is significantly improved compared with benchmark methods. version:1
arxiv-1405-6159 | A Bi-clustering Framework for Consensus Problems | http://arxiv.org/abs/1405.6159 | id:1405.6159 author:Mariano Tepper, Guillermo Sapiro category:cs.CV cs.LG stat.ML  published:2014-04-30 summary:We consider grouping as a general characterization for problems such as clustering, community detection in networks, and multiple parametric model estimation. We are interested in merging solutions from different grouping algorithms, distilling all their good qualities into a consensus solution. In this paper, we propose a bi-clustering framework and perspective for reaching consensus in such grouping problems. In particular, this is the first time that the task of finding/fitting multiple parametric models to a dataset is formally posed as a consensus problem. We highlight the equivalence of these tasks and establish the connection with the computational Gestalt program, that seeks to provide a psychologically-inspired detection theory for visual events. We also present a simple but powerful bi-clustering algorithm, specially tuned to the nature of the problem we address, though general enough to handle many different instances inscribed within our characterization. The presentation is accompanied with diverse and extensive experimental results in clustering, community detection, and multiple parametric model estimation in image processing applications. version:3
arxiv-1408-4753 | Be Careful When Assuming the Obvious: Commentary on "The placement of the head that minimizes online memory: a complex systems approach" | http://arxiv.org/abs/1408.4753 | id:1408.4753 author:Phillip M. Alday category:cs.CL  published:2014-08-20 summary:Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic and diachronic nature of word order based on the assumption that memory costs are a never decreasing function of distance and a few very general linguistic assumptions. However, even these minimal and seemingly obvious assumptions are not as safe as they appear in light of recent typological and psycholinguistic evidence. The interaction of word order and memory has further depths to be explored. version:1
arxiv-1312-4659 | DeepPose: Human Pose Estimation via Deep Neural Networks | http://arxiv.org/abs/1312.4659 | id:1312.4659 author:Alexander Toshev, Christian Szegedy category:cs.CV  published:2013-12-17 summary:We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images. version:3
arxiv-1408-4721 | Code Generation for High-Level Synthesis of Multiresolution Applications on FPGAs | http://arxiv.org/abs/1408.4721 | id:1408.4721 author:Moritz Schmid, Oliver Reiche, Christian Schmitt, Frank Hannig, Jürgen Teich category:cs.CV cs.DC cs.PL  published:2014-08-20 summary:Multiresolution Analysis (MRA) is a mathematical method that is based on working on a problem at different scales. One of its applications is medical imaging where processing at multiple scales, based on the concept of Gaussian and Laplacian image pyramids, is a well-known technique. It is often applied to reduce noise while preserving image detail on different levels of granularity without modifying the filter kernel. In scientific computing, multigrid methods are a popular choice, as they are asymptotically optimal solvers for elliptic Partial Differential Equations (PDEs). As such algorithms have a very high computational complexity that would overwhelm CPUs in the presence of real-time constraints, application-specific processors come into consideration for implementation. Despite of huge advancements in leveraging productivity in the respective fields, designers are still required to have detailed knowledge about coding techniques and the targeted architecture to achieve efficient solutions. Recently, the HIPAcc framework was proposed as a means for automatic code generation of image processing algorithms, based on a Domain-Specific Language (DSL). From the same code base, it is possible to generate code for efficient implementations on several accelerator technologies including different types of Graphics Processing Units (GPUs) as well as reconfigurable logic (FPGAs). In this work, we demonstrate the ability of HIPAcc to generate code for the implementation of multiresolution applications on FPGAs and embedded GPUs. version:1
arxiv-1408-4714 | Conic Multi-Task Classification | http://arxiv.org/abs/1408.4714 | id:1408.4714 author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2014-08-20 summary:Traditionally, Multi-task Learning (MTL) models optimize the average of task-related objective functions, which is an intuitive approach and which we will be referring to as Average MTL. However, a more general framework, referred to as Conic MTL, can be formulated by considering conic combinations of the objective functions instead; in this framework, Average MTL arises as a special case, when all combination coefficients equal 1. Although the advantage of Conic MTL over Average MTL has been shown experimentally in previous works, no theoretical justification has been provided to date. In this paper, we derive a generalization bound for the Conic MTL method, and demonstrate that the tightest bound is not necessarily achieved, when all combination coefficients equal 1; hence, Average MTL may not always be the optimal choice, and it is important to consider Conic MTL. As a byproduct of the generalization bound, it also theoretically explains the good experimental results of previous relevant works. Finally, we propose a new Conic MTL model, whose conic combination coefficients minimize the generalization bound, instead of choosing them heuristically as has been done in previous methods. The rationale and advantage of our model is demonstrated and verified via a series of experiments by comparing with several other methods. version:1
arxiv-1408-4703 | GIMP and Wavelets for Medical Image Processing: Enhancing Images of the Fundus of the Eye | http://arxiv.org/abs/1408.4703 | id:1408.4703 author:Amelia Carolina Sparavigna category:cs.CV  published:2014-08-20 summary:The visual analysis of retina and of its vascular characteristics is important in the diagnosis and monitoring of diseases of visual perception. In the related medical diagnoses, the digital processing of the fundus images is used to obtain the segmentation of retinal vessels. However, an image segmentation is often requiring methods based on peculiar or complex algorithms: in this paper we will show some alternative approaches obtained by applying freely available tools to enhance, without a specific segmentation, the images of the fundus of the eye. We will see in particular, that combining the use of GIMP, the GNU Image Manipulation Program, with the wavelet filter of Iris, a program well-known for processing astronomical images, the result is giving images which can be alternative of those obtained from segmentation. version:1
arxiv-1408-4692 | Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods | http://arxiv.org/abs/1408.4692 | id:1408.4692 author:Alexander Freytag, Johannes Rühle, Paul Bodesheim, Erik Rodner, Joachim Denzler category:cs.CV  published:2014-08-20 summary:Vector-quantized local features frequently used in bag-of-visual-words approaches are the backbone of popular visual recognition systems due to both their simplicity and their performance. Despite their success, bag-of-words-histograms basically contain low-level image statistics (e.g., number of edges of different orientations). The question remains how much visual information is "lost in quantization" when mapping visual features to code words? To answer this question, we present an in-depth analysis of the effect of local feature quantization on human recognition performance. Our analysis is based on recovering the visual information by inverting quantized local features and presenting these visualizations with different codebook sizes to human observers. Although feature inversion techniques are around for quite a while, to the best of our knowledge, our technique is the first visualizing especially the effect of feature quantization. Thereby, we are now able to compare single steps in common image classification pipelines to human counterparts. version:1
arxiv-1408-4673 | Horn functions and the AFP Algorithm | http://arxiv.org/abs/1408.4673 | id:1408.4673 author:Rooholah Majdodin category:cs.LG 68Q32  68T27  published:2014-08-20 summary:It is described why multiple refinements with each negative counterexample does not improve the complexity of the AFP Algorithm. Also Canonical normal formulas for Horn functions are discussed. version:1
arxiv-1408-4622 | A new integral loss function for Bayesian optimization | http://arxiv.org/abs/1408.4622 | id:1408.4622 author:Emmanuel Vazquez, Julien Bect category:stat.CO cs.LG math.OC stat.ML  published:2014-08-20 summary:We consider the problem of maximizing a real-valued continuous function $f$ using a Bayesian approach. Since the early work of Jonas Mockus and Antanas \v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by considering the loss function $\max f - M_n$ (where $M_n$ denotes the best function value observed after $n$ evaluations of $f$). This loss function puts emphasis on the value of the maximum, at the expense of the location of the maximizer. In the special case of a one-step Bayes-optimal strategy, it leads to the classical Expected Improvement (EI) sampling criterion. This is a special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk associated to a certain uncertainty measure (here, the expected loss) on the quantity of interest is minimized at each step of the algorithm. In this article, assuming that $f$ is defined over a measure space $(\mathbb{X}, \lambda)$, we propose to consider instead the integral loss function $\int_{\mathbb{X}} (f - M_n)_{+}\, d\lambda$, and we show that this leads, in the case of a Gaussian process prior, to a new numerically tractable sampling criterion that we call $\rm EI^2$ (for Expected Integrated Expected Improvement). A numerical experiment illustrates that a SUR strategy based on this new sampling criterion reduces the error on both the value and the location of the maximizer faster than the EI-based strategy. version:1
arxiv-1408-4587 | EURETILE D7.3 - Dynamic DAL benchmark coding, measurements on MPI version of DPSNN-STDP (distributed plastic spiking neural net) and improvements to other DAL codes | http://arxiv.org/abs/1408.4587 | id:1408.4587 author:Pier Stanislao Paolucci, Iuliana Bacivarov, Devendra Rai, Lars Schor, Lothar Thiele, Hoeseok Yang, Elena Pastorelli, Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Francesca Lo Cicero, Alessandro Lonardo, Francesco Simula, Laura Tosoratto, Piero Vicini category:cs.DC cs.CE cs.MS cs.NE q-bio.NC  published:2014-08-20 summary:The EURETILE project required the selection and coding of a set of dedicated benchmarks. The project is about the software and hardware architecture of future many-tile distributed fault-tolerant systems. We focus on dynamic workloads characterised by heavy numerical processing requirements. The ambition is to identify common techniques that could be applied to both the Embedded Systems and HPC domains. This document is the first public deliverable of Work Package 7: Challenging Tiled Applications. version:1
arxiv-1312-2139 | Optimal rates for zero-order convex optimization: the power of two function evaluations | http://arxiv.org/abs/1312.2139 | id:1312.2139 author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright, Andre Wibisono category:math.OC cs.IT math.IT stat.ML  published:2013-12-07 summary:We consider derivative-free algorithms for stochastic and non-stochastic convex optimization problems that use only function values rather than gradients. Focusing on non-asymptotic bounds on convergence rates, we show that if pairs of function values are available, algorithms for $d$-dimensional optimization that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{d}$ in convergence rate over traditional stochastic gradient methods. We establish such results for both smooth and non-smooth cases, sharpening previous analyses that suggested a worse dimension dependence, and extend our results to the case of multiple ($m \ge 2$) evaluations. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, establishing the sharpness of our achievable results up to constant (sometimes logarithmic) factors. version:2
arxiv-1408-0765 | Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet Bayesian Network | http://arxiv.org/abs/1408.0765 | id:1408.0765 author:Yu Liu, Osvaldo Simeone, Alexander M. Haimovich, Wei Su category:cs.IT cs.CV math.IT  published:2014-08-04 summary:A novel Bayesian modulation classification scheme is proposed for a single-antenna system over frequency-selective fading channels. The method is based on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN). The use of the proposed latent Dirichlet BN provides a systematic solution to the convergence problem encountered by the conventional Gibbs sampling approach for modulation classification. The method generalizes, and is shown to improve upon, the state of the art. version:2
arxiv-1408-4504 | Unsupervised Parallel Extraction based Texture for Efficient Image Representation | http://arxiv.org/abs/1408.4504 | id:1408.4504 author:Mohammed M. Abdelsamea category:cs.CV  published:2014-08-20 summary:SOM is a type of unsupervised learning where the goal is to discover some underlying structure of the data. In this paper, a new extraction method based on the main idea of Concurrent Self-Organizing Maps (CSOM), representing a winner-takes-all collection of small SOM networks is proposed. Each SOM of the system is trained individually to provide best results for one class only. The experiments confirm that the proposed features based CSOM is capable to represent image content better than extracted features based on a single big SOM and these proposed features improve the final decision of the CAD. Experiments held on Mammographic Image Analysis Society (MIAS) dataset. version:1
arxiv-1408-4487 | On Optimal Decision-Making in Ant Colonies | http://arxiv.org/abs/1408.4487 | id:1408.4487 author:Mahnush Movahedi, Mahdi Zamani category:cs.DC cs.NE  published:2014-08-19 summary:Colonies of ants can collectively choose the best of several nests, even when many of the active ants who organize the move visit only one site. Understanding such a behavior can help us design efficient distributed decision making algorithms. Marshall et al. propose a model for house-hunting in colonies of ant Temnothorax albipennis. Unfortunately, their model does not achieve optimal decision-making while laboratory experiments show that, in fact, colonies usually achieve optimality during the house-hunting process. In this paper, we argue that the model of Marshall et al. can achieve optimality by including nest size information in their mathematical model. We use lab results of Pratt et al. to re-define the differential equations of Marshall et al. Finally, we sketch our strategy for testing the optimality of the new model. version:1
arxiv-1407-5656 | PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical Data Problems | http://arxiv.org/abs/1407.5656 | id:1407.5656 author:Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John A. Miller, William S. York category:cs.AI cs.LG  published:2014-07-21 summary:In the big data era, scalability has become a crucial requirement for any useful computational model. Probabilistic graphical models are very useful for mining and discovering data insights, but they are not scalable enough to be suitable for big data problems. Bayesian Networks particularly demonstrate this limitation when their data is represented using few random variables while each random variable has a massive set of values. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian networks become infeasible for representing the probability distributions for the following reasons: i) Each level represents a single random variable with hundreds of thousands of values, ii) The number of levels is usually small, so there are also few random variables, and iii) The structure of the network is predefined since the dependency is modeled top-down from each parent to each of its child nodes, so the network would contain a single linear path for the random variables from each parent to each child node. In this paper we present a scalable probabilistic graphical model to overcome these limitations for massive hierarchical data. We believe the proposed model will lead to an easily-scalable, more readable, and expressive implementation for problems that require probabilistic-based solutions for massive amounts of hierarchical data. We successfully applied this model to solve two different challenging probabilistic-based problems on massive hierarchical data sets for different domains, namely, bioinformatics and latent semantic discovery over search logs. version:2
arxiv-1305-4537 | Object Detection with Pixel Intensity Comparisons Organized in Decision Trees | http://arxiv.org/abs/1305.4537 | id:1305.4537 author:Nenad Markuš, Miroslav Frljak, Igor S. Pandžić, Jörgen Ahlberg, Robert Forchheimer category:cs.CV  published:2013-05-20 summary:We describe a method for visual object detection based on an ensemble of optimized decision trees organized in a cascade of rejectors. The trees use pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast. Experimental analysis is provided through a face detection problem. The obtained results are encouraging and demonstrate that the method has practical value. Additionally, we analyse its sensitivity to noise and show how to perform fast rotation invariant object detection. Complete source code is provided at https://github.com/nenadmarkus/pico. version:5
arxiv-1408-4363 | Object Segmentation in Images using EEG Signals | http://arxiv.org/abs/1408.4363 | id:1408.4363 author:Eva Mohedano, Graham Healy, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor, Alan F. Smeaton category:cs.CV cs.MM H.1.2; I.4.6; C.3  published:2014-08-19 summary:This paper explores the potential of brain-computer interfaces in segmenting objects from images. Our approach is centered around designing an effective method for displaying the image parts to the users such that they generate measurable brain reactions. When an image region, specifically a block of pixels, is displayed we estimate the probability of the block containing the object of interest using a score based on EEG activity. After several such blocks are displayed, the resulting probability map is binarized and combined with the GrabCut algorithm to segment the image into object and background regions. This study shows that BCI and simple EEG analysis are useful in locating object boundaries in images. version:1
arxiv-1211-4246 | What Regularized Auto-Encoders Learn from the Data Generating Distribution | http://arxiv.org/abs/1211.4246 | id:1211.4246 author:Guillaume Alain, Yoshua Bengio category:cs.LG stat.ML  published:2012-11-18 summary:What do auto-encoders learn about the underlying data generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parametrization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments. version:5
arxiv-1211-4116 | The Algebraic Combinatorial Approach for Low-Rank Matrix Completion | http://arxiv.org/abs/1211.4116 | id:1211.4116 author:Franz J. Király, Louis Theran, Ryota Tomioka category:cs.LG cs.NA math.AG math.CO stat.ML  published:2012-11-17 summary:We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability-one algorithms to decide whether a particular entry of the matrix can be completed. We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition. version:4
arxiv-1408-4325 | What makes an Image Iconic? A Fine-Grained Case Study | http://arxiv.org/abs/1408.4325 | id:1408.4325 author:Yangmuzi Zhang, Diane Larlus, Florent Perronnin category:cs.CV  published:2014-08-19 summary:A natural approach to teaching a visual concept, e.g. a bird species, is to show relevant images. However, not all relevant images represent a concept equally well. In other words, they are not necessarily iconic. This observation raises three questions. Is iconicity a subjective property? If not, can we predict iconicity? And what exactly makes an image iconic? We provide answers to these questions through an extensive experimental study on a challenging fine-grained dataset of birds. We first show that iconicity ratings are consistent across individuals, even when they are not domain experts, thus demonstrating that iconicity is not purely subjective. We then consider an exhaustive list of properties that are intuitively related to iconicity and measure their correlation with these iconicity ratings. We combine them to predict iconicity of new unseen images. We also propose a direct iconicity predictor that is discriminatively trained with iconicity ratings. By combining both systems, we get an iconicity prediction that approaches human performance. version:1
arxiv-1310-7441 | Hierarchical Clustering of Hyperspectral Images using Rank-Two Nonnegative Matrix Factorization | http://arxiv.org/abs/1310.7441 | id:1310.7441 author:Nicolas Gillis, Da Kuang, Haesun Park category:cs.CV cs.IR math.OC  published:2013-09-14 summary:In this paper, we design a hierarchical clustering algorithm for high-resolution hyperspectral images. At the core of the algorithm, a new rank-two nonnegative matrix factorizations (NMF) algorithm is used to split the clusters, which is motivated by convex geometry concepts. The method starts with a single cluster containing all pixels, and, at each step, (i) selects a cluster in such a way that the error at the next step is minimized, and (ii) splits the selected cluster into two disjoint clusters using rank-two NMF in such a way that the clusters are well balanced and stable. The proposed method can also be used as an endmember extraction algorithm in the presence of pure pixels. The effectiveness of this approach is illustrated on several synthetic and real-world hyperspectral images, and shown to outperform standard clustering techniques such as k-means, spherical k-means and standard NMF. version:4
arxiv-1408-4245 | Towards crowdsourcing and cooperation in linguistic resources | http://arxiv.org/abs/1408.4245 | id:1408.4245 author:Dmitry Ustalov category:cs.SI cs.CL K.4.3  published:2014-08-19 summary:Linguistic resources can be populated with data through the use of such approaches as crowdsourcing and gamification when motivated people are involved. However, current crowdsourcing genre taxonomies lack the concept of cooperation, which is the principal element of modern video games and may potentially drive the annotators' interest. This survey on crowdsourcing taxonomies and cooperation in linguistic resources provides recommendations on using cooperation in existent genres of crowdsourcing and an evidence of the efficiency of cooperation using a popular Russian linguistic resource created through crowdsourcing as an example. version:1
arxiv-1408-4222 | Can Artificial Neural Networks be Applied in Seismic Predicition? Preliminary Analysis Applying Radial Topology. Case: Mexico | http://arxiv.org/abs/1408.4222 | id:1408.4222 author:Cinthya Mota-Hernandez, Luis Esquivel-Rodriguez, Rafael Alvarado-Corona category:cs.NE physics.geo-ph  published:2014-08-19 summary:Tectonic earthquakes of high magnitude can cause considerable losses in terms of human lives, economic and infrastructure, among others. According to an evaluation published by the U.S. Geological Survey, 30 is the number of earthquakes which have greatly impacted Mexico from the end of the XIX century to this one. Based upon data from the National Seismological Service, on the period between January 1, 2006 and May 1, 2013 there have occurred 5,826 earthquakes which magnitude has been greater than 4.0 degrees on the Richter magnitude scale (25.54% of the total of earthquakes registered on the national territory), being the Pacific Plate and the Cocos Plate the most important ones. This document describes the development of an Artificial Neural Network (ANN) based on the radial topology which seeks to generate a prediction with an error margin lower than 20% which can inform about the probability of a future earthquake one of the main questions is: can artificial neural networks be applied in seismic forecasting? It can be argued that research has the potential to bring in the forecast seismic, more research is needed to consolidate data and help mitigate the impact caused by such events linked with society. Keywords--- Analysis, Mexico, Neural Artificial Networks, Seismicity. version:1
arxiv-1408-4140 | BET: Bayesian Ensemble Trees for Clustering and Prediction in Heterogeneous Data | http://arxiv.org/abs/1408.4140 | id:1408.4140 author:Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak category:stat.ML stat.CO  published:2014-08-18 summary:We propose a novel "tree-averaging" model that utilizes the ensemble of classification and regression trees (CART). Each constituent tree is estimated with a subset of similar data. We treat this grouping of subsets as Bayesian ensemble trees (BET) and model them as an infinite mixture Dirichlet process. We show that BET adapts to data heterogeneity and accurately estimates each component. Compared with the bootstrap-aggregating approach, BET shows improved prediction performance with fewer trees. We develop an efficient estimating procedure with improved sampling strategies in both CART and mixture models. We demonstrate these advantages of BET with simulations, classification of breast cancer and regression of lung function measurement of cystic fibrosis patients. Keywords: Bayesian CART; Dirichlet Process; Ensemble Approach; Heterogeneity; Mixture of Trees. version:1
arxiv-1408-4077 | Brain: Biological noise-based logic | http://arxiv.org/abs/1408.4077 | id:1408.4077 author:Laszlo B. Kish, Claes-Goran Granqvist, Sergey M. Bezrukov, Tamas Horvath category:cs.NE cs.ET  published:2014-08-18 summary:Neural spikes in the brain form stochastic sequences, i.e., belong to the class of pulse noises. This stochasticity is a counterintuitive feature because extracting information - such as the commonly supposed neural information of mean spike frequency - requires long times for reasonably low error probability. The mystery could be solved by noise-based logic, wherein randomness has an important function and allows large speed enhancements for special-purpose tasks, and the same mechanism is at work for the brain logic version of this concept. version:1
arxiv-1408-3985 | Offline Signature-Based Fuzzy Vault (OSFV: Review and New Results | http://arxiv.org/abs/1408.3985 | id:1408.3985 author:George S. Eskander, Robert Sabourin, Eric Granger category:cs.CV cs.CR  published:2014-08-18 summary:An offline signature-based fuzzy vault (OSFV) is a bio-cryptographic implementation that uses handwritten signature images as biometrics instead of traditional passwords to secure private cryptographic keys. Having a reliable OSFV implementation is the first step towards automating financial and legal authentication processes, as it provides greater security of confidential documents by means of the embedded handwritten signatures. The authors have recently proposed the first OSFV implementation which is reviewed in this paper. In this system, a machine learning approach based on the dissimilarity representation concept is employed to select a reliable feature representation adapted for the fuzzy vault scheme. Some variants of this system are proposed for enhanced accuracy and security. In particular, a new method that adapts user key size is presented. Performance of proposed methods are compared using the Brazilian PUCPR and GPDS signature databases and results indicate that the key-size adaptation method achieves a good compromise between security and accuracy. While average system entropy is increased from 45-bits to about 51-bits, the AER (average error rate) is decreased by about 21%. version:1
arxiv-1408-3934 | On Detecting Messaging Abuse in Short Text Messages using Linguistic and Behavioral patterns | http://arxiv.org/abs/1408.3934 | id:1408.3934 author:Alejandro Mosquera, Lamine Aouad, Slawomir Grzonkowski, Dylan Morss category:cs.CL cs.AI cs.SI  published:2014-08-18 summary:The use of short text messages in social media and instant messaging has become a popular communication channel during the last years. This rising popularity has caused an increment in messaging threats such as spam, phishing or malware as well as other threats. The processing of these short text message threats could pose additional challenges such as the presence of lexical variants, SMS-like contractions or advanced obfuscations which can degrade the performance of traditional filtering solutions. By using a real-world SMS data set from a large telecommunications operator from the US and a social media corpus, in this paper we analyze the effectiveness of machine learning filters based on linguistic and behavioral patterns in order to detect short text spam and abusive users in the network. We have also explored different ways to deal with short text message challenges such as tokenization and entity detection by using text normalization and substring clustering techniques. The obtained results show the validity of the proposed solution by enhancing baseline approaches. version:1
arxiv-1404-3012 | Bayesian image segmentations by Potts prior and loopy belief propagation | http://arxiv.org/abs/1404.3012 | id:1404.3012 author:Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi, Chiou-Ting Hsu category:cs.CV cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML  published:2014-04-11 summary:This paper presents a Bayesian image segmentation model based on Potts prior and loopy belief propagation. The proposed Bayesian model involves several terms, including the pairwise interactions of Potts models, and the average vectors and covariant matrices of Gauss distributions in color image modeling. These terms are often referred to as hyperparameters in statistical machine learning theory. In order to determine these hyperparameters, we propose a new scheme for hyperparameter estimation based on conditional maximization of entropy in the Potts prior. The algorithm is given based on loopy belief propagation. In addition, we compare our conditional maximum entropy framework with the conventional maximum likelihood framework, and also clarify how the first order phase transitions in LBP's for Potts models influence our hyperparameter estimation procedures. version:5
arxiv-1408-3829 | Opinion mining of movie reviews at document level | http://arxiv.org/abs/1408.3829 | id:1408.3829 author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.IR cs.CL  published:2014-08-17 summary:The whole world is changed rapidly and using the current technologies Internet becomes an essential need for everyone. Web is used in every field. Most of the people use web for a common purpose like online shopping, chatting etc. During an online shopping large number of reviews/opinions are given by the users that reflect whether the product is good or bad. These reviews need to be explored, analyse and organized for better decision making. Opinion Mining is a natural language processing task that deals with finding orientation of opinion in a piece of text with respect to a topic. In this paper a document based opinion mining system is proposed that classify the documents as positive, negative and neutral. Negation is also handled in the proposed system. Experimental results using reviews of movies show the effectiveness of the system. version:1
arxiv-1408-3818 | Unsupervised learning segmentation for dynamic speckle activity images | http://arxiv.org/abs/1408.3818 | id:1408.3818 author:Lucia I. Passoni, Ana I. Dai Pra, Gustavo J. Meschino, MArcelo Guzman, Chistian Weber, Héctor Rabal, Marcelo Trivi category:physics.optics cs.CV  published:2014-08-17 summary:This paper proposes the design of decision models based on Computational Intelligence techniques applied to image sequences of dynamic laser speckle. These models aim to identify image regions of biological specimens illuminated by a coherent beam coming from a laser. The field image is pseudo colored using a Self Organizing Map projection. This process is carried out using a set of descriptors applied to the intensity variations along time in every pixel of an image sequence. The models use descriptors selected to improve effectiveness, depending on the specific application. We present two examples of the application of the proposed techniques to assess biological tissues. The results obtained are encouraging and significantly improve those obtained using a single descriptor. version:1
arxiv-1408-3814 | Robust Statistical Approach for Extraction of Moving Human Silhouettes from Videos | http://arxiv.org/abs/1408.3814 | id:1408.3814 author:Oinam Binarani Devi, Nissi S. Paul, Y. Jayanta Singh category:cs.CV  published:2014-08-17 summary:Human pose estimation is one of the key problems in computer vision that has been studied in the recent years. The significance of human pose estimation is in the higher level tasks of understanding human actions applications such as recognition of anomalous actions present in videos and many other related applications. The human poses can be estimated by extracting silhouettes of humans as silhouettes are robust to variations and it gives the shape information of the human body. Some common challenges include illumination changes, variation in environments, and variation in human appearances. Thus there is a need for a robust method for human pose estimation. This paper presents a study and analysis of approaches existing for silhouette extraction and proposes a robust technique for extracting human silhouettes in video sequences. Gaussian Mixture Model (GMM) A statistical approach is combined with HSV (Hue, Saturation and Value) color space model for a robust background model that is used for background subtraction to produce foreground blobs, called human silhouettes. Morphological operations are then performed on foreground blobs from background subtraction. The silhouettes obtained from this work can be used in further tasks associated with human action interpretation and activity processes like human action classification, human pose estimation and action recognition or action interpretation. version:1
arxiv-1408-3807 | On solving Ordinary Differential Equations using Gaussian Processes | http://arxiv.org/abs/1408.3807 | id:1408.3807 author:David Barber category:stat.ME cs.NA math.NA stat.CO stat.ML  published:2014-08-17 summary:We describe a set of Gaussian Process based approaches that can be used to solve non-linear Ordinary Differential Equations. We suggest an explicit probabilistic solver and two implicit methods, one analogous to Picard iteration and the other to gradient matching. All methods have greater accuracy than previously suggested Gaussian Process approaches. We also suggest a general approach that can yield error estimates from any standard ODE solver. version:1
arxiv-1408-3750 | Real-time emotion recognition for gaming using deep convolutional network features | http://arxiv.org/abs/1408.3750 | id:1408.3750 author:Sébastien Ouellet category:cs.CV cs.LG cs.NE  published:2014-08-16 summary:The goal of the present study is to explore the application of deep convolutional network features to emotion recognition. Results indicate that they perform similarly to other published models at a best recognition rate of 94.4%, and do so with a single still image rather than a video stream. An implementation of an affective feedback game is also described, where a classifier using these features tracks the facial expressions of a player in real-time. version:1
arxiv-1408-3740 | A fast patch-dictionary method for whole image recovery | http://arxiv.org/abs/1408.3740 | id:1408.3740 author:Yangyang Xu, Wotao Yin category:cs.CV math.OC 94A08  94A12  published:2014-08-16 summary:Various algorithms have been proposed for dictionary learning. Among those for image processing, many use image patches to form dictionaries. This paper focuses on whole-image recovery from corrupted linear measurements. We address the open issue of representing an image by overlapping patches: the overlapping leads to an excessive number of dictionary coefficients to determine. With very few exceptions, this issue has limited the applications of image-patch methods to the local kind of tasks such as denoising, inpainting, cartoon-texture decomposition, super-resolution, and image deblurring, for which one can process a few patches at a time. Our focus is global imaging tasks such as compressive sensing and medical image recovery, where the whole image is encoded together, making it either impossible or very ineffective to update a few patches at a time. Our strategy is to divide the sparse recovery into multiple subproblems, each of which handles a subset of non-overlapping patches, and then the results of the subproblems are averaged to yield the final recovery. This simple strategy is surprisingly effective in terms of both quality and speed. In addition, we accelerate computation of the learned dictionary by applying a recent block proximal-gradient method, which not only has a lower per-iteration complexity but also takes fewer iterations to converge, compared to the current state-of-the-art. We also establish that our algorithm globally converges to a stationary point. Numerical results on synthetic data demonstrate that our algorithm can recover a more faithful dictionary than two state-of-the-art methods. Combining our whole-image recovery and dictionary-learning methods, we numerically simulate image inpainting, compressive sensing recovery, and deblurring. Our recovery is more faithful than those of a total variation method and a method based on overlapping patches. version:1
arxiv-1408-3735 | Analysis of a chaotic spiking neural model: The NDS neuron | http://arxiv.org/abs/1408.3735 | id:1408.3735 author:Mohammad Alhawarat, Waleed Nazih, Mohammad Eldesouki category:cs.NE nlin.CD  published:2014-08-16 summary:Further analysis and experimentation is carried out in this paper for a chaotic dynamic model, viz. the Nonlinear Dynamic State neuron (NDS). The analysis and experimentations are performed to further understand the underlying dynamics of the model and enhance it as well. Chaos provides many interesting properties that can be exploited to achieve computational tasks. Such properties are sensitivity to initial conditions, space filling, control and synchronization.Chaos might play an important role in information processing tasks in human brain as suggested by biologists. If artificial neural networks (ANNs) is equipped with chaos then it will enrich the dynamic behaviours of such networks. The NDS model has some limitations and can be overcome in different ways. In this paper different approaches are followed to push the boundaries of the NDS model in order to enhance it. One way is to study the effects of scaling the parameters of the chaotic equations of the NDS model and study the resulted dynamics. Another way is to study the method that is used in discretization of the original R\"{o}ssler that the NDS model is based on. These approaches have revealed some facts about the NDS attractor and suggest why such a model can be stabilized to large number of unstable periodic orbits (UPOs) which might correspond to memories in phase space. version:1
arxiv-1408-3733 | Multi-Sensor Event Detection using Shape Histograms | http://arxiv.org/abs/1408.3733 | id:1408.3733 author:Ehtesham Hassan, Gautam Shroff, Puneet Agarwal category:cs.LG  published:2014-08-16 summary:Vehicular sensor data consists of multiple time-series arising from a number of sensors. Using such multi-sensor data we would like to detect occurrences of specific events that vehicles encounter, e.g., corresponding to particular maneuvers that a vehicle makes or conditions that it encounters. Events are characterized by similar waveform patterns re-appearing within one or more sensors. Further such patterns can be of variable duration. In this work, we propose a method for detecting such events in time-series data using a novel feature descriptor motivated by similar ideas in image processing. We define the shape histogram: a constant dimension descriptor that nevertheless captures patterns of variable duration. We demonstrate the efficacy of using shape histograms as features to detect events in an SVM-based, multi-sensor, supervised learning scenario, i.e., multiple time-series are used to detect an event. We present results on real-life vehicular sensor data and show that our technique performs better than available pattern detection implementations on our data, and that it can also be used to combine features from multiple sensors resulting in better accuracy than using any single sensor. Since previous work on pattern detection in time-series has been in the single series context, we also present results using our technique on multiple standard time-series datasets and show that it is the most versatile in terms of how it ranks compared to other published results. version:1
arxiv-1408-3709 | Robust 3D face recognition in presence of pose and partial occlusions or missing parts | http://arxiv.org/abs/1408.3709 | id:1408.3709 author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri category:cs.CV  published:2014-08-16 summary:In this paper, we propose a robust 3D face recognition system which can handle pose as well as occlusions in real world. The system at first takes as input, a 3D range image, simultaneously registers it using ICP(Iterative Closest Point) algorithm. ICP used in this work, registers facial surfaces to a common model by minimizing distances between a probe model and a gallery model. However the performance of ICP relies heavily on the initial conditions. Hence, it is necessary to provide an initial registration, which will be improved iteratively and finally converge to the best alignment possible. Once the faces are registered, the occlusions are automatically extracted by thresholding the depth map values of the 3D image. After the occluded regions are detected, restoration is done by Principal Component Analysis (PCA). The restored images, after the removal of occlusions, are then fed to the recognition system for classification purpose. Features are extracted from the reconstructed non-occluded face images in the form of face normals. The experimental results which were obtained on the occluded facial images from the Bosphorus 3D face database, illustrate that our occlusion compensation scheme has attained a recognition accuracy of 91.30%. version:1
arxiv-1408-3573 | Turkish Presidential Elections TRT Publicity Speech Facial Expression Analysis | http://arxiv.org/abs/1408.3573 | id:1408.3573 author:H. Emrah Tasli, Paul Ivan category:cs.CV  published:2014-08-15 summary:In this paper, facial expressions of the three Turkish presidential candidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzed during the publicity speeches featured at TRT (Turkish Radio and Television) on 03.08.2014. FaceReader is used for the analysis where 3D modeling of the face is achieved using the active appearance models (AAM). Over 500 landmark points are tracked and analyzed for obtaining the facial expressions during the whole speech. All source videos and the data are publicly available for research purposes. version:1
arxiv-1408-3153 | Detection is the central problem in real-word spelling correction | http://arxiv.org/abs/1408.3153 | id:1408.3153 author:L. Amber Wilcox-O'Hearn category:cs.CL  published:2014-08-13 summary:Real-word spelling correction differs from non-word spelling correction in its aims and its challenges. Here we show that the central problem in real-word spelling correction is detection. Methods from non-word spelling correction, which focus instead on selection among candidate corrections, do not address detection adequately, because detection is either assumed in advance or heavily constrained. As we demonstrate in this paper, merely discriminating between the intended word and a random close variation of it within the context of a sentence is a task that can be performed with high accuracy using straightforward models. Trigram models are sufficient in almost all cases. The difficulty comes when every word in the sentence is a potential error, with a large set of possible candidate corrections. Despite their strengths, trigram models cannot reliably find true errors without introducing many more, at least not when used in the obvious sequential way without added structure. The detection task exposes weakness not visible in the selection task. version:2
arxiv-1408-4072 | Indexing Cost Sensitive Prediction | http://arxiv.org/abs/1408.4072 | id:1408.4072 author:Leilani Battle, Edward Benson, Aditya Parameswaran, Eugene Wu category:cs.LG cs.DB cs.DS  published:2014-08-15 summary:Predictive models are often used for real-time decision making. However, typical machine learning techniques ignore feature evaluation cost, and focus solely on the accuracy of the machine learning models obtained utilizing all the features available. We develop algorithms and indexes to support cost-sensitive prediction, i.e., making decisions using machine learning models taking feature evaluation cost into account. Given an item and a online computation cost (i.e., time) budget, we present two approaches to return an appropriately chosen machine learning model that will run within the specified time on the given item. The first approach returns the optimal machine learning model, i.e., one with the highest accuracy, that runs within the specified time, but requires significant up-front precomputation time. The second approach returns a possibly sub- optimal machine learning model, but requires little up-front precomputation time. We study these two algorithms in detail and characterize the scenarios (using real and synthetic data) in which each performs well. Unlike prior work that focuses on a narrow domain or a specific algorithm, our techniques are very general: they apply to any cost-sensitive prediction scenario on any machine learning algorithm. version:1
arxiv-1408-3467 | Robust Statistical Ranking: Theory and Algorithms | http://arxiv.org/abs/1408.3467 | id:1408.3467 author:Qianqian Xu, Jiechao Xiong, Qingming Huang, Yuan Yao category:stat.ME cs.LG stat.ML  published:2014-08-15 summary:Deeply rooted in classical social choice and voting theory, statistical ranking with paired comparison data experienced its renaissance with the wide spread of crowdsourcing technique. As the data quality might be significantly damaged in an uncontrolled crowdsourcing environment, outlier detection and robust ranking have become a hot topic in such data analysis. In this paper, we propose a robust ranking framework based on the principle of Huber's robust statistics, which formulates outlier detection as a LASSO problem to find sparse approximations of the cyclic ranking projection in Hodge decomposition. Moreover, simple yet scalable algorithms are developed based on Linearized Bregman Iteration to achieve an even less biased estimator than LASSO. Statistical consistency of outlier detection is established in both cases which states that when the outliers are strong enough and in Erdos-Renyi random graph sampling settings, outliers can be faithfully detected. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a promising tool for robust ranking with large scale crowdsourcing data arising from computer vision, multimedia, machine learning, sociology, etc. version:1
arxiv-1407-2572 | Classifiers fusion method to recognize handwritten persian numerals | http://arxiv.org/abs/1407.2572 | id:1407.2572 author:Reza Azad, Babak Azad, Iraj Mogharreb, Shahram Jamali category:cs.CV  published:2014-07-09 summary:Recognition of Persian handwritten characters has been considered as a significant field of research for the last few years under pattern analysing technique. In this paper, a new approach for robust handwritten Persian numerals recognition using strong feature set and a classifier fusion method is scrutinized to increase the recognition percentage. For implementing the classifier fusion technique, we have considered k nearest neighbour (KNN), linear classifier (LC) and support vector machine (SVM) classifiers. The innovation of this tactic is to attain better precision with few features using classifier fusion method. For evaluation of the proposed method we considered a Persian numerals database with 20,000 handwritten samples. Spending 15,000 samples for training stage, we verified our technique on other 5,000 samples, and the correct recognition ratio achieved approximately 99.90%. Additional, we got 99.97% exactness using four-fold cross validation procedure on 20,000 databases. version:2
arxiv-1408-3456 | SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation | http://arxiv.org/abs/1408.3456 | id:1408.3456 author:Felix Hill, Roi Reichart, Anna Korhonen category:cs.CL  published:2014-08-15 summary:We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness, so that pairs of entities that are associated but not actually similar [Freud, psychology] have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures. version:1
arxiv-1307-5381 | A convex pseudo-likelihood framework for high dimensional partial correlation estimation with convergence guarantees | http://arxiv.org/abs/1307.5381 | id:1307.5381 author:Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam category:stat.ME stat.CO stat.ML  published:2013-07-20 summary:Sparse high dimensional graphical model selection is a topic of much interest in modern day statistics. A popular approach is to apply l1-penalties to either (1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods, with the latter having the distinct advantage that they do not explicitly assume Gaussianity. As none of the popular methods proposed for solving pseudo-likelihood based objective functions have provable convergence guarantees, it is not clear if corresponding estimators exist or are even computable, or if they actually yield correct partial correlation graphs. This paper proposes a new pseudo-likelihood based graphical model selection method that aims to overcome some of the shortcomings of current methods, but at the same time retain all their respective strengths. In particular, we introduce a novel framework that leads to a convex formulation of the partial covariance regression graph problem, resulting in an objective function comprised of quadratic forms. The objective is then optimized via a coordinate-wise approach. The specific functional form of the objective function facilitates rigorous convergence analysis leading to convergence guarantees; an important property that cannot be established using standard results, when the dimension is larger than the sample size, as is often the case in high dimensional applications. These convergence guarantees ensure that estimators are well-defined under very general conditions, and are always computable. In addition, the approach yields estimators that have good large sample properties and also respect symmetry. Furthermore, application to simulated/real data, timing comparisons and numerical convergence is demonstrated. We also present a novel unifying framework that places all graphical pseudo-likelihood methods as special cases of a more general formulation, leading to important insights. version:3
arxiv-1305-1986 | An Adaptive Statistical Non-uniform Quantizer for Detail Wavelet Components in Lossy JPEG2000 Image Compression | http://arxiv.org/abs/1305.1986 | id:1305.1986 author:Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi category:cs.MM cs.CV  published:2013-05-09 summary:The paper presents a non-uniform quantization method for the Detail components in the JPEG2000 standard. Incorporating the fact that the coefficients lying towards the ends of the histogram plot of each Detail component represent the structural information of an image, the quantization step sizes become smaller at they approach the ends of the histogram plot. The variable quantization step sizes are determined by the actual statistics of the wavelet coefficients. Mean and standard deviation are the two statistical parameters used iteratively to obtain the variable step sizes. Moreover, the mean of the coefficients lying within the step size is chosen as the quantized value, contrary to the deadzone uniform quantizer which selects the midpoint of the quantization step size as the quantized value. The experimental results of the deadzone uniform quantizer and the proposed non-uniform quantizer are objectively compared by using Mean-Squared Error (MSE) and Mean Structural Similarity Index Measure (MSSIM), to evaluate the quantization error and reconstructed image quality, respectively. Subjective analysis of the reconstructed images is also carried out. Through the objective and subjective assessments, it is shown that the non-uniform quantizer performs better than the deadzone uniform quantizer in the perceptual quality of the reconstructed image, especially at low bitrates. More importantly, unlike the deadzone uniform quantizer, the non-uniform quantizer accomplishes better visual quality with a few quantized values. version:3
arxiv-1408-3382 | Likely to stop? Predicting Stopout in Massive Open Online Courses | http://arxiv.org/abs/1408.3382 | id:1408.3382 author:Colin Taylor, Kalyan Veeramachaneni, Una-May O'Reilly category:cs.CY cs.LG  published:2014-08-14 summary:Understanding why students stopout will help in understanding how students learn in MOOCs. In this report, part of a 3 unit compendium, we describe how we build accurate predictive models of MOOC student stopout. We document a scalable, stopout prediction methodology, end to end, from raw source data to model analysis. We attempted to predict stopout for the Fall 2012 offering of 6.002x. This involved the meticulous and crowd-sourced engineering of over 25 predictive features extracted for thousands of students, the creation of temporal and non-temporal data representations for use in predictive modeling, the derivation of over 10 thousand models with a variety of state-of-the-art machine learning techniques and the analysis of feature importance by examining over 70000 models. We found that stop out prediction is a tractable problem. Our models achieved an AUC (receiver operating characteristic area-under-the-curve) as high as 0.95 (and generally 0.88) when predicting one week in advance. Even with more difficult prediction problems, such as predicting stop out at the end of the course with only one weeks' data, the models attained AUCs of 0.7. version:1
arxiv-1408-3337 | 2D View Aggregation for Lymph Node Detection Using a Shallow Hierarchy of Linear Classifiers | http://arxiv.org/abs/1408.3337 | id:1408.3337 author:Ari Seff, Le Lu, Kevin M. Cherry, Holger Roth, Jiamin Liu, Shijun Wang, Joanne Hoffman, Evrim B. Turkbey, Ronald M. Summers category:cs.CV cs.LG  published:2014-08-14 summary:Enlarged lymph nodes (LNs) can provide important information for cancer diagnosis, staging, and measuring treatment reactions, making automated detection a highly sought goal. In this paper, we propose a new algorithm representation of decomposing the LN detection problem into a set of 2D object detection subtasks on sampled CT slices, largely alleviating the curse of dimensionality issue. Our 2D detection can be effectively formulated as linear classification on a single image feature type of Histogram of Oriented Gradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We exploit both simple pooling and sparse linear fusion schemes to aggregate these 2D detection scores for the final 3D LN detection. In this manner, detection is more tractable and does not need to perform perfectly at instance level (as weak hypotheses) since our aggregation process will robustly harness collective information for LN detection. Two datasets (90 patients with 389 mediastinal LNs and 86 patients with 595 abdominal LNs) are used for validation. Cross-validation demonstrates 78.0% sensitivity at 6 false positives/volume (FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10 FP/vol.), for the mediastinal and abdominal datasets respectively. Our results compare favorably to previous state-of-the-art methods. version:1
arxiv-1408-3332 | Exact and empirical estimation of misclassification probability | http://arxiv.org/abs/1408.3332 | id:1408.3332 author:Victor Nedelko category:stat.ML cs.LG  published:2014-08-14 summary:We discuss the problem of risk estimation in the classification problem, with specific focus on finding distributions that maximize the confidence intervals of risk estimation. We derived simple analytic approximations for the maximum bias of empirical risk for histogram classifier. We carry out a detailed study on using these analytic estimates for empirical estimation of risk. version:1
arxiv-1407-4867 | Analysis of Gait Pattern to Recognize the Human Activities | http://arxiv.org/abs/1407.4867 | id:1407.4867 author:Jay Prakash Gupta, Pushkar Dixit, Nishant Singh, Vijay Bhaskar Semwal category:cs.CV  published:2014-07-18 summary:Human activity recognition based on the computer vision is the process of labelling image sequences with action labels. Accurate systems for this problem are applied in areas such as visual surveillance, human computer interaction and video retrieval. version:2
arxiv-1407-6492 | Recognition of Handwritten Persian/Arabic Numerals Based on Robust Feature Set and K-NN Classifier | http://arxiv.org/abs/1407.6492 | id:1407.6492 author:Reza Azad, Fatemeh Davami, Hamid Reza Shayegh category:cs.CV  published:2014-07-24 summary:This paper has been withdrawn by the author due to a crucial sign error in equation 2 and some mistake in Table 1 information. please let me for changing this information and updating this paper. version:2
arxiv-1402-6863 | Addendum on the scoring of Gaussian directed acyclic graphical models | http://arxiv.org/abs/1402.6863 | id:1402.6863 author:Jack Kuipers, Giusi Moffa, David Heckerman category:stat.ML  published:2014-02-27 summary:We provide a correction to the expression for scoring Gaussian directed acyclic graphical models derived in Geiger and Heckerman [Ann. Statist. 30 (2002) 1414-1440] and discuss how to evaluate the score efficiently. version:3
arxiv-1408-3218 | Toward Automated Discovery of Artistic Influence | http://arxiv.org/abs/1408.3218 | id:1408.3218 author:Babak Saleh, Kanako Abe, Ravneet Singh Arora, Ahmed Elgammal category:cs.CV cs.LG  published:2014-08-14 summary:Considering the huge amount of art pieces that exist, there is valuable information to be discovered. Examining a painting, an expert can determine its style, genre, and the time period that the painting belongs. One important task for art historians is to find influences and connections between artists. Is influence a task that a computer can measure? The contribution of this paper is in exploring the problem of computer-automated suggestion of influences between artists, a problem that was not addressed before in a general setting. We first present a comparative study of different classification methodologies for the task of fine-art style classification. A two-level comparative study is performed for this classification problem. The first level reviews the performance of discriminative vs. generative models, while the second level touches the features aspect of the paintings and compares semantic-level features vs. low-level and intermediate-level features present in the painting. Then, we investigate the question "Who influenced this artist?" by looking at his masterpieces and comparing them to others. We pose this interesting question as a knowledge discovery problem. For this purpose, we investigated several painting-similarity and artist-similarity measures. As a result, we provide a visualization of artists (Map of Artists) based on the similarity between their works version:1
arxiv-1408-3215 | Cortical Processing with Thermodynamic-RAM | http://arxiv.org/abs/1408.3215 | id:1408.3215 author:M. Alexander Nugent, Timothy W. Molter category:cs.NE cs.ET  published:2014-08-14 summary:AHaH computing forms a theoretical framework from which a biologically-inspired type of computing architecture can be built where, unlike von Neumann systems, memory and processor are physically combined. In this paper we report on an incremental step beyond the theoretical framework of AHaH computing toward the development of a memristor-based physical neural processing unit (NPU), which we call Thermodynamic-RAM (kT-RAM). While the power consumption and speed dominance of such an NPU over von Neumann architectures for machine learning applications is well appreciated, Thermodynamic-RAM offers several advantages over other hardware approaches to adaptation and learning. Benefits include general-purpose use, a simple yet flexible instruction set and easy integration into existing digital platforms. We present a high level design of kT-RAM and a formal definition of its instruction set. We report the completion of a kT-RAM emulator and the successful port of all previous machine learning benchmark applications including unsupervised clustering, supervised and unsupervised classification, complex signal prediction, unsupervised robotic actuation and combinatorial optimization. Lastly, we extend a previous MNIST hand written digits benchmark application, to show that an extra step of reading the synaptic states of AHaH nodes during the train phase (healing) alone results in plasticity that improves the classifier's performance, bumping our best F1 score up to 99.5%. version:1
arxiv-1408-1664 | A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian Networks | http://arxiv.org/abs/1408.1664 | id:1408.1664 author:Yetian Chen, Jin Tian, Olga Nikolova, Srinivas Aluru category:cs.AI cs.DC cs.LG  published:2014-08-07 summary:Exact Bayesian structure discovery in Bayesian networks requires exponential time and space. Using dynamic programming (DP), the fastest known serial algorithm computes the exact posterior probabilities of structural features in $O(n2^n)$ time and space, if the number of parents per node or indegree is bounded by a constant $d$. Here we present a parallel algorithm capable of computing the exact posterior probabilities for all $n(n-1)$ edges with optimal parallel time and space efficiency. That is, if $p=2^k$ processors are used, the run-time and space usage reduce to $O(n2^{n-k}+k(n-k)^d)$ and $O(n2^{n-k})$, respectively. Our algorithm is based the observation that the original DP steps constitute a $n$-$D$ hypercube. In our algorithm, we take a delicate way to coordinate the computations of correlated DP procedures such that large amount of data exchange is suppressed. Further, we develop parallel techniques for two variants of the well-known zeta transform, which have applications outside the context of Bayesian networks. We demonstrate the capability of our algorithm on datasets with up to 33 variables and its scalability on up to 2048 processors. version:2
arxiv-1408-3169 | Indefinitely Oscillating Martingales | http://arxiv.org/abs/1408.3169 | id:1408.3169 author:Jan Leike, Marcus Hutter category:cs.LG math.PR math.ST stat.TH  published:2014-08-14 summary:We construct a class of nonnegative martingale processes that oscillate indefinitely with high probability. For these processes, we state a uniform rate of the number of oscillations and show that this rate is asymptotically close to the theoretical upper bound. These bounds on probability and expectation of the number of upcrossings are compared to classical bounds from the martingale literature. We discuss two applications. First, our results imply that the limit of the minimum description length operator may not exist. Second, we give bounds on how often one can change one's belief in a given hypothesis when observing a stream of data. version:1
arxiv-1408-3092 | Convergence rate of Bayesian tensor estimator: Optimal rate without restricted strong convexity | http://arxiv.org/abs/1408.3092 | id:1408.3092 author:Taiji Suzuki category:stat.ML cs.LG  published:2014-08-13 summary:In this paper, we investigate the statistical convergence rate of a Bayesian low-rank tensor estimator. Our problem setting is the regression problem where a tensor structure underlying the data is estimated. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a near optimal rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori. version:1
arxiv-1408-3060 | Fastfood: Approximate Kernel Expansions in Loglinear Time | http://arxiv.org/abs/1408.3060 | id:1408.3060 author:Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola category:cs.LG stat.ML  published:2014-08-13 summary:Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that storing and computing the decision function is typically expensive, especially at prediction time. In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks proposed by Rahimi and Recht (2009) and thereby speeding up the computation for a large range of kernel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) computation and storage, without sacrificing accuracy. Our method applies to any translation invariant and any dot-product kernel, such as the popular RBF kernels and polynomial kernels. We prove that the approximation is unbiased and has low variance. Experiments show that we achieve similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster and using 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applications that have large training sets and/or require real-time prediction. version:1
arxiv-1408-2803 | Learning a hyperplane classifier by minimizing an exact bound on the VC dimension | http://arxiv.org/abs/1408.2803 | id:1408.2803 author:Jayadeva category:cs.LG I.5.1; I.5.2  published:2014-08-12 summary:The VC dimension measures the capacity of a learning machine, and a low VC dimension leads to good generalization. While SVMs produce state-of-the-art learning performance, it is well known that the VC dimension of a SVM can be unbounded; despite good results in practice, there is no guarantee of good generalization. In this paper, we show how to learn a hyperplane classifier by minimizing an exact, or \boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed as the Minimal Complexity Machine (MCM), involves solving a simple linear programming problem. Experimental results show, that on a number of benchmark datasets, the proposed approach learns classifiers with error rates much less than conventional SVMs, while often using fewer support vectors. On many benchmark datasets, the number of support vectors is less than one-tenth the number used by SVMs, indicating that the MCM does indeed learn simpler representations. version:2
arxiv-1303-4756 | Marginal Likelihoods for Distributed Parameter Estimation of Gaussian Graphical Models | http://arxiv.org/abs/1303.4756 | id:1303.4756 author:Zhaoshi Meng, Dennis Wei, Ami Wiesel, Alfred O. Hero III category:stat.ML cs.LG  published:2013-03-19 summary:We consider distributed estimation of the inverse covariance matrix, also called the concentration or precision matrix, in Gaussian graphical models. Traditional centralized estimation often requires global inference of the covariance matrix, which can be computationally intensive in large dimensions. Approximate inference based on message-passing algorithms, on the other hand, can lead to unstable and biased estimation in loopy graphical models. In this paper, we propose a general framework for distributed estimation based on a maximum marginal likelihood (MML) approach. This approach computes local parameter estimates by maximizing marginal likelihoods defined with respect to data collected from local neighborhoods. Due to the non-convexity of the MML problem, we introduce and solve a convex relaxation. The local estimates are then combined into a global estimate without the need for iterative message-passing between neighborhoods. The proposed algorithm is naturally parallelizable and computationally efficient, thereby making it suitable for high-dimensional problems. In the classical regime where the number of variables $p$ is fixed and the number of samples $T$ increases to infinity, the proposed estimator is shown to be asymptotically consistent and to improve monotonically as the local neighborhood size increases. In the high-dimensional scaling regime where both $p$ and $T$ increase to infinity, the convergence rate to the true parameters is derived and is seen to be comparable to centralized maximum likelihood estimation. Extensive numerical experiments demonstrate the improved performance of the two-hop version of the proposed estimator, which suffices to almost close the gap to the centralized maximum likelihood estimator at a reduced computational cost. version:6
arxiv-1408-2997 | An Improved Approach for Contrast Enhancement of Spinal Cord Images based on Multiscale Retinex Algorithm | http://arxiv.org/abs/1408.2997 | id:1408.2997 author:Sreenivasa Setty, N. K Srinath, M. C Hanumantharaju category:cs.CV  published:2014-08-13 summary:This paper presents a new approach for contrast enhancement of spinal cord medical images based on multirate scheme incorporated into multiscale retinex algorithm. The proposed work here uses HSV color space, since HSV color space separates color details from intensity. The enhancement of medical image is achieved by down sampling the original image into five versions, namely, tiny, small, medium, fine, and normal scale. This is due to the fact that the each versions of the image when independently enhanced and reconstructed results in enormous improvement in the visual quality. Further, the contrast stretching and MultiScale Retinex (MSR) techniques are exploited in order to enhance each of the scaled version of the image. Finally, the enhanced image is obtained by combining each of these scales in an efficient way to obtain the composite enhanced image. The efficiency of the proposed algorithm is validated by using a wavelet energy metric in the wavelet domain. Reconstructed image using proposed method highlights the details (edges and tissues), reduces image noise (Gaussian and Speckle) and improves the overall contrast. The proposed algorithm also enhances sharp edges of the tissue surrounding the spinal cord regions which is useful for diagnosis of spinal cord lesions. Elaborated experiments are conducted on several medical images and results presented show that the enhanced medical pictures are of good quality and is found to be better compared with other researcher methods. version:1
arxiv-1408-2938 | Learning Multi-Scale Representations for Material Classification | http://arxiv.org/abs/1408.2938 | id:1408.2938 author:Wenbin Li, Mario Fritz category:cs.CV cs.LG cs.NE  published:2014-08-13 summary:The recent progress in sparse coding and deep learning has made unsupervised feature learning methods a strong competitor to hand-crafted descriptors. In computer vision, success stories of learned features have been predominantly reported for object recognition tasks. In this paper, we investigate if and how feature learning can be used for material recognition. We propose two strategies to incorporate scale information into the learning procedure resulting in a novel multi-scale coding procedure. Our results show that our learned features for material recognition outperform hand-crafted descriptors on the FMD and the KTH-TIPS2 material classification benchmarks. version:1
arxiv-1408-2927 | Hashing for Similarity Search: A Survey | http://arxiv.org/abs/1408.2927 | id:1408.2927 author:Jingdong Wang, Heng Tao Shen, Jingkuan Song, Jianqiu Ji category:cs.DS cs.CV cs.DB  published:2014-08-13 summary:Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space. version:1
arxiv-1408-3359 | Linear Contour Learning: A Method for Supervised Dimension Reduction | http://arxiv.org/abs/1408.3359 | id:1408.3359 author:Bing Li, Hongyuan Zha, Francesca Chiaromonte category:cs.LG  published:2014-08-13 summary:We propose a novel approach to sufficient dimension reduction in regression, based on estimating contour directions of negligible variation for the response surface. These directions span the orthogonal complement of the minimal space relevant for the regression, and can be extracted according to a measure of the variation in the response, leading to General Contour Regression(GCR). In comparison to exiisting sufficient dimension reduction techniques, this sontour-based mothology guarantees exhaustive estimation of the central space under ellipticity of the predictoor distribution and very mild additional assumptions, while maintaining vn-consisytency and somputational ease. Moreover, it proves to be robust to departures from ellipticity. We also establish some useful population properties for GCR. Simulations to compare performance with that of standard techniques such as ordinary least squares, sliced inverse regression, principal hessian directions, and sliced average variance estimation confirm the advntages anticipated by theoretical analyses. We also demonstrate the use of contour-based methods on a data set concerning grades of students from Massachusetts colleges. version:1
arxiv-1408-2890 | Robust OS-ELM with a novel selective ensemble based on particle swarm optimization | http://arxiv.org/abs/1408.2890 | id:1408.2890 author:Yang Liu, Bo He, Diya Dong, Yue Shen, Tianhong Yan, Rui Nian, Amaury Lendase category:cs.LG  published:2014-08-13 summary:In this paper, a robust online sequential extreme learning machine (ROS-ELM) is proposed. It is based on the original OS-ELM with an adaptive selective ensemble framework. Two novel insights are proposed in this paper. First, a novel selective ensemble algorithm referred to as particle swarm optimization selective ensemble (PSOSEN) is proposed. Noting that PSOSEN is a general selective ensemble method which is applicable to any learning algorithms, including batch learning and online learning. Second, an adaptive selective ensemble framework for online learning is designed to balance the robustness and complexity of the algorithm. Experiments for both regression and classification problems with UCI data sets are carried out. Comparisons between OS-ELM, simple ensemble OS-ELM (EOS-ELM) and the proposed ROS-ELM empirically show that ROS-ELM significantly improves the robustness and stability. version:1
arxiv-1408-2889 | A Classifier-free Ensemble Selection Method based on Data Diversity in Random Subspaces | http://arxiv.org/abs/1408.2889 | id:1408.2889 author:Albert H. R. Ko, Robert Sabourin, Alceu S. Britto Jr, Luiz E. S. Oliveira category:cs.LG cs.NE I.5.2; I.5.3  published:2014-08-13 summary:The Ensemble of Classifiers (EoC) has been shown to be effective in improving the performance of single classifiers by combining their outputs, and one of the most important properties involved in the selection of the best EoC from a pool of classifiers is considered to be classifier diversity. In general, classifier diversity does not occur randomly, but is generated systematically by various ensemble creation methods. By using diverse data subsets to train classifiers, these methods can create diverse classifiers for the EoC. In this work, we propose a scheme to measure data diversity directly from random subspaces, and explore the possibility of using it to select the best data subsets for the construction of the EoC. Our scheme is the first ensemble selection method to be presented in the literature based on the concept of data diversity. Its main advantage over the traditional framework (ensemble creation then selection) is that it obviates the need for classifier training prior to ensemble selection. A single Genetic Algorithm (GA) and a Multi-Objective Genetic Algorithm (MOGA) were evaluated to search for the best solutions for the classifier-free ensemble selection. In both cases, objective functions based on different clustering diversity measures were implemented and tested. All the results obtained with the proposed classifier-free ensemble selection method were compared with the traditional classifier-based ensemble selection using Mean Classifier Error (ME) and Majority Voting Error (MVE). The applicability of the method is tested on UCI machine learning problems and NIST SD19 handwritten numerals. version:1
arxiv-1408-2869 | Cluster based RBF Kernel for Support Vector Machines | http://arxiv.org/abs/1408.2869 | id:1408.2869 author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG stat.ML  published:2014-08-12 summary:In the classical Gaussian SVM classification we use the feature space projection transforming points to normal distributions with fixed covariance matrices (identity in the standard RBF and the covariance of the whole dataset in Mahalanobis RBF). In this paper we add additional information to Gaussian SVM by considering local geometry-dependent feature space projection. We emphasize that our approach is in fact an algorithm for a construction of the new Gaussian-type kernel. We show that better (compared to standard RBF and Mahalanobis RBF) classification results are obtained in the simple case when the space is preliminary divided by k-means into two sets and points are represented as normal distributions with a covariances calculated according to the dataset partitioning. We call the constructed method C$_k$RBF, where $k$ stands for the amount of clusters used in k-means. We show empirically on nine datasets from UCI repository that C$_2$RBF increases the stability of the grid search (measured as the probability of finding good parameters). version:1
arxiv-1408-2810 | Spectral Unmixing of Hyperspectral Imagery using Multilayer NMF | http://arxiv.org/abs/1408.2810 | id:1408.2810 author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV  published:2014-08-12 summary:Hyperspectral images contain mixed pixels due to low spatial resolution of hyperspectral sensors. Spectral unmixing problem refers to decomposing mixed pixels into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fractions, nonnegative matrix factorization (NMF) methods have been widely used for solving spectral unmixing problem. In this letter we proposed using multilayer NMF (MLNMF) for the purpose of hyperspectral unmixing. In this approach, spectral signature matrix can be modeled as a product of sparse matrices. In fact MLNMF decomposes the observation matrix iteratively in a number of layers. In each layer, we applied sparseness constraint on spectral signature matrix as well as on abundance fractions matrix. In this way signatures matrix can be sparsely decomposed despite the fact that it is not generally a sparse matrix. The proposed algorithm is applied on synthetic and real datasets. Synthetic data is generated based on endmembers from USGS spectral library. AVIRIS Cuprite dataset has been used as a real dataset for evaluation of proposed method. Results of experiments are quantified based on SAD and AAD measures. Results in comparison with previously proposed methods show that the multilayer approach can unmix data more effectively. version:1
arxiv-1006-1138 | Online Learning via Sequential Complexities | http://arxiv.org/abs/1006.1138 | id:1006.1138 author:Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari category:cs.LG stat.ML  published:2010-06-06 summary:We consider the problem of sequential prediction and provide tools to study the minimax value of the associated game. Classical statistical learning theory provides several useful complexity measures to study learning with i.i.d. data. Our proposed sequential complexities can be seen as extensions of these measures to the sequential setting. The developed theory is shown to yield precise learning guarantees for the problem of sequential prediction. In particular, we show necessary and sufficient conditions for online learnability in the setting of supervised learning. Several examples show the utility of our framework: we can establish learnability without having to exhibit an explicit online learning algorithm. version:3
arxiv-1406-3166 | Generalization and Robustness of Batched Weighted Average Algorithm with V-geometrically Ergodic Markov Data | http://arxiv.org/abs/1406.3166 | id:1406.3166 author:Nguyen Viet Cuong, Lam Si Tung Ho, Vu Dinh category:stat.ML  published:2014-06-12 summary:We analyze the generalization and robustness of the batched weighted average algorithm for V-geometrically ergodic Markov data. This algorithm is a good alternative to the empirical risk minimization algorithm when the latter suffers from overfitting or when optimizing the empirical risk is hard. For the generalization of the algorithm, we prove a PAC-style bound on the training sample size for the expected $L_1$-loss to converge to the optimal loss when training data are V-geometrically ergodic Markov chains. For the robustness, we show that if the training target variable's values contain bounded noise, then the generalization bound of the algorithm deviates at most by the range of the noise. Our results can be applied to the regression problem, the classification problem, and the case where there exists an unknown deterministic target hypothesis. version:2
arxiv-1408-2699 | Internal and external dynamics in language: Evidence from verb regularity in a historical corpus of English | http://arxiv.org/abs/1408.2699 | id:1408.2699 author:Christine F. Cuskley, Martina Pugliese, Claudio Castellano, Francesca Colaiori, Vittorio Loreto, Francesca Tria category:physics.soc-ph cs.CL  published:2014-08-12 summary:Human languages are rule governed, but almost invariably these rules have exceptions in the form of irregularities. Since rules in language are efficient and productive, the persistence of irregularity is an anomaly. How does irregularity linger in the face of internal (endogenous) and external (exogenous) pressures to conform to a rule? Here we address this problem by taking a detailed look at simple past tense verbs in the Corpus of Historical American English. The data show that the language is open, with many new verbs entering. At the same time, existing verbs might tend to regularize or irregularize as a consequence of internal dynamics, but overall, the amount of irregularity sustained by the language stays roughly constant over time. Despite continuous vocabulary growth, and presumably, an attendant increase in expressive power, there is no corresponding growth in irregularity. We analyze the set of irregulars, showing they may adhere to a set of minority rules, allowing for increased stability of irregularity over time. These findings contribute to the debate on how language systems become rule governed, and how and why they sustain exceptions to rules, providing insight into the interplay between the emergence and maintenance of rules and exceptions in language. version:1
arxiv-1309-5643 | Multiple Instance Learning with Bag Dissimilarities | http://arxiv.org/abs/1309.5643 | id:1309.5643 author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.LG  published:2013-09-22 summary:Multiple instance learning (MIL) is concerned with learning from sets (bags) of objects (instances), where the individual instance labels are ambiguous. In this setting, supervised learning cannot be applied directly. Often, specialized MIL methods learn by making additional assumptions about the relationship of the bag labels and instance labels. Such assumptions may fit a particular dataset, but do not generalize to the whole range of MIL problems. Other MIL methods shift the focus of assumptions from the labels to the overall (dis)similarity of bags, and therefore learn from bags directly. We propose to represent each bag by a vector of its dissimilarities to other bags in the training set, and treat these dissimilarities as a feature representation. We show several alternatives to define a dissimilarity between bags and discuss which definitions are more suitable for particular MIL problems. The experimental results show that the proposed approach is computationally inexpensive, yet very competitive with state-of-the-art algorithms on a wide range of MIL datasets. version:3
arxiv-1310-4249 | Mapping the stereotyped behaviour of freely-moving fruit flies | http://arxiv.org/abs/1310.4249 | id:1310.4249 author:Gordon J. Berman, Daniel M. Choi, William Bialek, Joshua W. Shaevitz category:q-bio.QM cs.CV physics.bio-ph stat.ML  published:2013-10-16 summary:Most animals possess the ability to actuate a vast diversity of movements, ostensibly constrained only by morphology and physics. In practice, however, a frequent assumption in behavioral science is that most of an animal's activities can be described in terms of a small set of stereotyped motifs. Here we introduce a method for mapping the behavioral space of organisms, relying only upon the underlying structure of postural movement data to organize and classify behaviors. We find that six different drosophilid species each perform a mix of non-stereotyped actions and over one hundred hierarchically-organized, stereotyped behaviors. Moreover, we use this approach to compare these species' behavioral spaces, systematically identifying subtle behavioral differences between closely-related species. version:2
arxiv-1408-2552 | Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction of Tumors | http://arxiv.org/abs/1408.2552 | id:1408.2552 author:Amit G. Deshwar, Shankar Vembu, Quaid Morris category:q-bio.PE cs.LG stat.ML  published:2014-08-11 summary:Statistical machine learning methods, especially nonparametric Bayesian methods, have become increasingly popular to infer clonal population structure of tumors. Here we describe the treeCRP, an extension of the Chinese restaurant process (CRP), a popular construction used in nonparametric mixture models, to infer the phylogeny and genotype of major subclonal lineages represented in the population of cancer cells. We also propose new split-merge updates tailored to the subclonal reconstruction problem that improve the mixing time of Markov chains. In comparisons with the tree-structured stick breaking prior used in PhyloSub, we demonstrate superior mixing and running time using the treeCRP with our new split-merge procedures. We also show that given the same number of samples, TSSB and treeCRP have similar ability to recover the subclonal structure of a tumor. version:1
arxiv-1408-2504 | Compressed Sensing with Very Sparse Gaussian Random Projections | http://arxiv.org/abs/1408.2504 | id:1408.2504 author:Ping Li, Cun-Hui Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT  published:2014-08-11 summary:We study the use of very sparse random projections for compressed sensing (sparse signal recovery) when the signal entries can be either positive or negative. In our setting, the entries of a Gaussian design matrix are randomly sparsified so that only a very small fraction of the entries are nonzero. Our proposed decoding algorithm is simple and efficient in that the major cost is one linear scan of the coordinates. We have developed two estimators: (i) the {\em tie estimator}, and (ii) the {\em absolute minimum estimator}. Using only the tie estimator, we are able to recover a $K$-sparse signal of length $N$ using $1.551 eK \log K/\delta$ measurements (where $\delta\leq 0.05$ is the confidence). Using only the absolute minimum estimator, we can detect the support of the signal using $eK\log N/\delta$ measurements. For a particular coordinate, the absolute minimum estimator requires fewer measurements (i.e., with a constant $e$ instead of $1.551e$). Thus, the two estimators can be combined to form an even more practical decoding framework. Prior studies have shown that existing one-scan (or roughly one-scan) recovery algorithms using sparse matrices would require substantially more (e.g., one order of magnitude) measurements than L1 decoding by linear programming, when the nonzero entries of signals can be either negative or positive. In this paper, following a known experimental setup, we show that, at the same number of measurements, the recovery accuracies of our proposed method are (at least) similar to the standard L1 decoding. version:1
arxiv-1202-4044 | Robust computation of linear models by convex relaxation | http://arxiv.org/abs/1202.4044 | id:1202.4044 author:Gilad Lerman, Michael McCoy, Joel A. Tropp, Teng Zhang category:cs.IT math.IT stat.CO stat.ML 62H25  65K05  90C22  published:2012-02-18 summary:Consider a dataset of vector-valued observations that consists of noisy inliers, which are explained well by a low-dimensional subspace, along with some number of outliers. This work describes a convex optimization problem, called REAPER, that can reliably fit a low-dimensional model to this type of data. This approach parameterizes linear subspaces using orthogonal projectors, and it uses a relaxation of the set of orthogonal projectors to reach the convex formulation. The paper provides an efficient algorithm for solving the REAPER problem, and it documents numerical experiments which confirm that REAPER can dependably find linear structure in synthetic and natural data. In addition, when the inliers lie near a low-dimensional subspace, there is a rigorous theory that describes when REAPER can approximate this subspace. version:2
arxiv-1408-2478 | Learning to see like children: proof of concept | http://arxiv.org/abs/1408.2478 | id:1408.2478 author:Marco Gori, Marco Lippi, Marco Maggini, Stefano Melacci category:cs.CV  published:2014-08-11 summary:In the last few years we have seen a growing interest in machine learning approaches to computer vision and, especially, to semantic labeling. Nowadays state of the art systems use deep learning on millions of labeled images with very successful results on benchmarks, though it is unlikely to expect similar results in unrestricted visual environments. Most learning schemes essentially ignore the inherent sequential structure of videos: this might be a critical issue, since any visual recognition process is remarkably more complex when shuffling video frames. Based on this remark, we propose a re-foundation of the communication protocol between visual agents and the environment, which is referred to as learning to see like children. Like for human interaction, visual concepts are acquired by the agents solely by processing their own visual stream along with human supervisions on selected pixels. We give a proof of concept that remarkable semantic labeling can emerge within this protocol by using only a few supervised examples. This is made possible by exploiting a constraint of motion coherent labeling that virtually offers tons of supervisions. Additional visual constraints, including those associated with object supervisions, are used within the context of learning from constraints. The framework is extended in the direction of lifelong learning, so as our visual agents live in their own visual environment without distinguishing learning and test set. Learning takes place in deep architectures under a progressive developmental scheme. In order to evaluate our Developmental Visual Agents (DVAs), in addition to classic benchmarks, we open the doors of our lab, allowing people to evaluate DVAs by crowd-sourcing. Such assessment mechanism might result in a paradigm shift in methodologies and algorithms for computer vision, encouraging truly novel solutions within the proposed framework. version:1
arxiv-1408-2430 | Optimizing Component Combination in a Multi-Indexing Paragraph Retrieval System | http://arxiv.org/abs/1408.2430 | id:1408.2430 author:Boris Iolis, Gianluca Bontempi category:cs.IR cs.CL 68T50  published:2014-08-11 summary:We demonstrate a method to optimize the combination of distinct components in a paragraph retrieval system. Our system makes use of several indices, query generators and filters, each of them potentially contributing to the quality of the returned list of results. The components are combined with a weighed sum, and we optimize the weights using a heuristic optimization algorithm. This allows us to maximize the quality of our results, but also to determine which components are most valuable in our system. We evaluate our approach on the paragraph selection task of a Question Answering dataset. version:1
arxiv-1408-2380 | Video Face Editing Using Temporal-Spatial-Smooth Warping | http://arxiv.org/abs/1408.2380 | id:1408.2380 author:Xiaoyan Li, Dacheng Tao category:cs.CV cs.AI cs.MM  published:2014-08-11 summary:Editing faces in videos is a popular yet challenging aspect of computer vision and graphics, which encompasses several applications including facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation. Simply applying image-based warping algorithms to video-based face editing produces temporal incoherence in the synthesized videos because it is impossible to consistently localize facial features in two frames representing two different faces in two different videos (or even two consecutive frames representing the same face in one video). Therefore, high performance face editing usually requires significant manual manipulation. In this paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithm to effectively exploit the temporal information in two consecutive frames, as well as the spatial smoothness within each frame. TSSW precisely estimates two control lattices in the horizontal and vertical directions respectively from the corresponding control lattices in the previous frame, by minimizing a novel energy function that unifies a data-driven term, a smoothness term, and feature point constraints. Corresponding warping surfaces then precisely map source frames to the target frames. Experimental testing on facial attractiveness enhancement, makeup transfer, face replacement, and expression manipulation demonstrates that the proposed approaches can effectively preserve spatial smoothness and temporal coherence in editing facial geometry, skin detail, identity, and expression, which outperform the existing face editing methods. In particular, TSSW is robust to subtly inaccurate localization of feature points and is a vast improvement over image-based warping methods. version:1
arxiv-1408-2368 | On the Complexity of Bandit Linear Optimization | http://arxiv.org/abs/1408.2368 | id:1408.2368 author:Ohad Shamir category:cs.LG  published:2014-08-11 summary:We study the attainable regret for online linear optimization problems with bandit feedback, where unlike the full-information setting, the player can only observe its own loss rather than the full loss vector. We show that the price of bandit information in this setting can be as large as $d$, disproving the well-known conjecture that the regret for bandit linear optimization is at most $\sqrt{d}$ times the full-information regret. Surprisingly, this is shown using "trivial" modifications of standard domains, which have no effect in the full-information setting. This and other results we present highlight some interesting differences between full-information and bandit learning, which were not considered in previous literature. version:1
arxiv-1408-2289 | Physical Computing With No Clock to Implement the Gaussian Pyramid of SIFT Algorithm | http://arxiv.org/abs/1408.2289 | id:1408.2289 author:Yi Li, Qi Wei, Fei Qiao, Huazhong Yang category:cs.CV  published:2014-08-11 summary:Physical computing is a technology utilizing the nature of electronic devices and circuit topology to cope with computing tasks. In this paper, we propose an active circuit network to implement multi-scale Gaussian filter, which is also called Gaussian Pyramid in image preprocessing. Various kinds of methods have been tried to accelerate the key stage in image feature extracting algorithm these years. Compared with existing technologies, GPU parallel computing and FPGA accelerating technology, physical computing has great advantage on processing speed as well as power consumption. We have verified that processing time to implement the Gaussian pyramid of the SIFT algorithm stands on nanosecond level through the physical computing technology, while other existing methods all need at least hundreds of millisecond. With an estimate on the stray capacitance of the circuit, the power consumption is around 670pJ to filter a 256x256 image. To the best of our knowledge, this is the most fast processing technology to accelerate the SIFT algorithm, and it is also a rather energy-efficient method, thanks to the proposed physical computing technology. version:1
arxiv-1408-2288 | Genetic Programming for Smart Phone Personalisation | http://arxiv.org/abs/1408.2288 | id:1408.2288 author:Philip Valencia, Aiden Haak, Alban Cotillon, Raja Jurdak category:cs.NE cs.CY  published:2014-08-11 summary:Personalisation in smart phones requires adaptability to dynamic context based on user mobility, application usage and sensor inputs. Current personalisation approaches, which rely on static logic that is developed a priori, do not provide sufficient adaptability to dynamic and unexpected context. This paper proposes genetic programming (GP), which can evolve program logic in realtime, as an online learning method to deal with the highly dynamic context in smart phone personalisation. We introduce the concept of collaborative smart phone personalisation through the GP Island Model, in order to exploit shared context among co-located phone users and reduce convergence time. We implement these concepts on real smartphones to demonstrate the capability of personalisation through GP and to explore the benefits of the Island Model. Our empirical evaluations on two example applications confirm that the Island Model can reduce convergence time by up to two-thirds over standalone GP personalisation. version:1
arxiv-1408-2196 | Exponentiated Gradient Exploration for Active Learning | http://arxiv.org/abs/1408.2196 | id:1408.2196 author:Djallel Bouneffouf category:cs.LG cs.AI I.2  published:2014-08-10 summary:Active learning strategies respond to the costly labelling task in a supervised classification by selecting the most useful unlabelled examples in training a predictive model. Many conventional active learning algorithms focus on refining the decision boundary, rather than exploring new regions that can be more informative. In this setting, we propose a sequential algorithm named EG-Active that can improve any Active learning algorithm by an optimal random exploration. Experimental results show a statistically significant and appreciable improvement in the performance of our new approach over the existing active feedback methods. version:1
arxiv-1408-2195 | R-UCB: a Contextual Bandit Algorithm for Risk-Aware Recommender Systems | http://arxiv.org/abs/1408.2195 | id:1408.2195 author:Djallel Bouneffouf category:cs.IR cs.LG I.2  published:2014-08-10 summary:Mobile Context-Aware Recommender Systems can be naturally modelled as an exploration/exploitation trade-off (exr/exp) problem, where the system has to choose between maximizing its expected rewards dealing with its current knowledge (exploitation) and learning more about the unknown user's preferences to improve its knowledge (exploration). This problem has been addressed by the reinforcement learning community but they do not consider the risk level of the current user's situation, where it may be dangerous to recommend items the user may not desire in her current situation if the risk level is high. We introduce in this paper an algorithm named R-UCB that considers the risk level of the user's situation to adaptively balance between exr and exp. The detailed analysis of the experimental results reveals several important discoveries in the exr/exp behaviour. version:1
arxiv-1408-2156 | Statistical guarantees for the EM algorithm: From population to sample-based analysis | http://arxiv.org/abs/1408.2156 | id:1408.2156 author:Sivaraman Balakrishnan, Martin J. Wainwright, Bin Yu category:math.ST cs.LG stat.ML stat.TH  published:2014-08-09 summary:We develop a general framework for proving rigorous guarantees on the performance of the EM algorithm and a variant known as gradient EM. Our analysis is divided into two parts: a treatment of these algorithms at the population level (in the limit of infinite data), followed by results that apply to updates based on a finite set of samples. First, we characterize the domain of attraction of any global maximizer of the population likelihood. This characterization is based on a novel view of the EM updates as a perturbed form of likelihood ascent, or in parallel, of the gradient EM updates as a perturbed form of standard gradient ascent. Leveraging this characterization, we then provide non-asymptotic guarantees on the EM and gradient EM algorithms when applied to a finite set of samples. We develop consequences of our general theory for three canonical examples of incomplete-data problems: mixture of Gaussians, mixture of regressions, and linear regression with covariates missing completely at random. In each case, our theory guarantees that with a suitable initialization, a relatively small number of EM (or gradient EM) steps will yield (with high probability) an estimate that is within statistical error of the MLE. We provide simulations to confirm this theoretically predicted behavior. version:1
arxiv-1402-7349 | Learning Graphical Models With Hubs | http://arxiv.org/abs/1402.7349 | id:1402.7349 author:Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel, Daniela Witten category:stat.ML stat.CO stat.ME  published:2014-02-28 summary:We consider the problem of learning a high-dimensional graphical model in which certain hub nodes are highly-connected to many other nodes. Many authors have studied the use of an l1 penalty in order to learn a sparse graph in high-dimensional setting. However, the l1 penalty implicitly assumes that each edge is equally likely and independent of all other edges. We propose a general framework to accommodate more realistic networks with hub nodes, using a convex formulation that involves a row-column overlap norm penalty. We apply this general framework to three widely-used probabilistic graphical models: the Gaussian graphical model, the covariance graph model, and the binary Ising model. An alternating direction method of multipliers algorithm is used to solve the corresponding convex optimization problems. On synthetic data, we demonstrate that our proposed framework outperforms competitors that do not explicitly model hub nodes. We illustrate our proposal on a webpage data set and a gene expression data set. version:2
arxiv-1408-2067 | Probabilistic inverse reinforcement learning in unknown environments | http://arxiv.org/abs/1408.2067 | id:1408.2067 author:Aristide Tossou, Christos Dimitrakakis category:cs.LG stat.ML  published:2014-08-09 summary:We consider the problem of learning by demonstration from agents acting in unknown stochastic Markov environments or games. Our aim is to estimate agent preferences in order to construct improved policies for the same task that the agents are trying to solve. To do so, we extend previous probabilistic approaches for inverse reinforcement learning in known MDPs to the case of unknown dynamics or opponents. We do this by deriving two simplified probabilistic models of the demonstrator's policy and utility. For tractability, we use maximum a posteriori estimation rather than full Bayesian inference. Under a flat prior, this results in a convex optimisation problem. We find that the resulting algorithms are highly competitive against a variety of other methods for inverse reinforcement learning that do have knowledge of the dynamics. version:1
arxiv-1408-2066 | Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear Multivariate Regression and Granger Causality | http://arxiv.org/abs/1408.2066 | id:1408.2066 author:Vikas Sindhwani, Ha Quang Minh, Aurelie Lozano category:cs.LG stat.ML  published:2014-08-09 summary:We propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems. This framework allows a broad class of mixed norm regularizers, including those that induce sparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel Hilbert Spaces. We develop a highly scalable and eigendecomposition-free algorithm that orchestrates two inexact solvers for simultaneously learning both the input and output components of separable matrix-valued kernels. As a key application enabled by our framework, we show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems, leading to novel nonlinear extensions of a class of Graphical Granger Causality techniques. Our algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds. version:1
arxiv-1408-2065 | Normalized Online Learning | http://arxiv.org/abs/1408.2065 | id:1408.2065 author:Stephane Ross, Paul Mineiro, John Langford category:cs.LG stat.ML  published:2014-08-09 summary:We introduce online learning algorithms which are independent of feature scales, proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale. This has several useful effects: there is no need to pre-normalize data, the test-time and test-space complexity are reduced, and the algorithms are more robust. version:1
arxiv-1408-2064 | One-Class Support Measure Machines for Group Anomaly Detection | http://arxiv.org/abs/1408.2064 | id:1408.2064 author:Krikamol Muandet, Bernhard Schoelkopf category:cs.LG stat.ML  published:2014-08-09 summary:We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications. version:1
arxiv-1408-2062 | The Lovasz-Bregman Divergence and connections to rank aggregation, clustering, and web ranking | http://arxiv.org/abs/1408.2062 | id:1408.2062 author:Rishabh Iyer, Jeff A. Bilmes category:cs.LG stat.ML  published:2014-08-09 summary:We extend the recently introduced theory of Lovasz-Bregman (LB) divergences (Iyer & Bilmes 2012) in several ways. We show that they represent a distortion between a "score" and an "ordering", thus providing a new view of rank aggregation and order based clustering with interesting connections to web ranking. We show how the LB divergences have a number of properties akin to many permutation based metrics, and in fact have as special cases forms very similar to the Kendall-tau metric. We also show how the LB divergences subsume a number of commonly used ranking measures in information retrieval, like NDCG and AUC. Unlike the traditional permutation based metrics, however, the LB divergence naturally captures a notion of "confidence" in the orderings, thus providing a new representation to applications involving aggregating scores as opposed to just orderings. We show how a number of recently used web ranking models are forms of Lovasz-Bregman rank aggregation and also observe that a natural form of Mallow's model using the LB divergence has been used as conditional ranking models for the "Learning to Rank" problem. version:1
arxiv-1408-2061 | Warped Mixtures for Nonparametric Cluster Shapes | http://arxiv.org/abs/1408.2061 | id:1408.2061 author:Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani category:cs.LG stat.ML  published:2014-08-09 summary:A mixture of Gaussians fit to a single curved or heavy-tailed cluster will report that the data contains many clusters. To produce more appropriate clusterings, we introduce a model which warps a latent mixture of Gaussians to produce nonparametric cluster shapes. The possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters (or density manifolds) describing the data. The number of manifolds, as well as the shape and dimension of each manifold is automatically inferred. We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function. We show that our model is effective for density estimation, performs better than infinite Gaussian mixture models at recovering the true number of clusters, and produces interpretable summaries of high-dimensional datasets. version:1
arxiv-1408-2060 | Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations | http://arxiv.org/abs/1408.2060 | id:1408.2060 author:Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet category:cs.LG cs.DC stat.ML  published:2014-08-09 summary:Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP. version:1
arxiv-1408-2055 | Guess Who Rated This Movie: Identifying Users Through Subspace Clustering | http://arxiv.org/abs/1408.2055 | id:1408.2055 author:Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari category:cs.LG cs.IR stat.ML  published:2014-08-09 summary:It is often the case that, within an online recommender system, multiple users share a common account. Can such shared accounts be identified solely on the basis of the userprovided ratings? Once a shared account is identified, can the different users sharing it be identified as well? Whenever such user identification is feasible, it opens the way to possible improvements in personalized recommendations, but also raises privacy concerns. We develop a model for composite accounts based on unions of linear subspaces, and use subspace clustering for carrying out the identification task. We show that a significant fraction of such accounts is identifiable in a reliable manner, and illustrate potential uses for personalized recommendation. version:1
arxiv-1408-2054 | Non-Convex Rank Minimization via an Empirical Bayesian Approach | http://arxiv.org/abs/1408.2054 | id:1408.2054 author:David Wipf category:cs.LG cs.NA stat.ML  published:2014-08-09 summary:In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candes et al., 2011) can be viewed as a special case. version:1
arxiv-1408-2051 | Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications | http://arxiv.org/abs/1408.2051 | id:1408.2051 author:Rishabh Iyer, Jeff A. Bilmes category:cs.LG stat.ML  published:2014-08-09 summary:We extend the work of Narasimhan and Bilmes [30] for minimizing set functions representable as a dierence between submodular functions. Similar to [30], our new algorithms are guaranteed to monotonically reduce the objective function at every step. We empirically and theoretically show that the per-iteration cost of our algorithms is much less than [30], and our algorithms can be used to efficiently minimize a dierence between submodular functions under various combinatorial constraints, a problem not previously addressed. We provide computational bounds and a hardness result on the multiplicative inapproximability of minimizing the dierence between submodular functions. We show, however, that it is possible to give worst-case additive bounds by providing a polynomial time computable lower-bound on the minima. Finally we show how a number of machine learning problems can be modeled as minimizing the dierence between submodular functions. We experimentally show the validity of our algorithms by testing them on the problem of feature selection with submodular cost features. version:1
arxiv-1408-2049 | Optimally-Weighted Herding is Bayesian Quadrature | http://arxiv.org/abs/1408.2049 | id:1408.2049 author:Ferenc Huszar, David Duvenaud category:cs.LG stat.ML  published:2014-08-09 summary:Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate. version:1
arxiv-1408-2047 | Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior | http://arxiv.org/abs/1408.2047 | id:1408.2047 author:Yutian Chen, Max Welling category:cs.LG stat.ML  published:2014-08-09 summary:In recent years a number of methods have been developed for automatically learning the (sparse) connectivity structure of Markov Random Fields. These methods are mostly based on L1-regularized optimization which has a number of disadvantages such as the inability to assess model uncertainty and expensive crossvalidation to find the optimal regularization parameter. Moreover, the model's predictive performance may degrade dramatically with a suboptimal value of the regularization parameter (which is sometimes desirable to induce sparseness). We propose a fully Bayesian approach based on a "spike and slab" prior (similar to L0 regularization) that does not suffer from these shortcomings. We develop an approximate MCMC method combining Langevin dynamics and reversible jump MCMC to conduct inference in this model. Experiments show that the proposed model learns a good combination of the structure and parameter values without the need for separate hyper-parameter tuning. Moreover, the model's predictive performance is much more robust than L1-based methods with hyper-parameter settings that induce highly sparse model structures. version:1
arxiv-1408-6418 | Video In Sentences Out | http://arxiv.org/abs/1408.6418 | id:1408.6418 author:Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.CL cs.IR  published:2014-08-09 summary:We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the trackto-role assignments, and changing body posture. version:1
arxiv-1408-2045 | Efficient Clustering with Limited Distance Information | http://arxiv.org/abs/1408.2045 | id:1408.2045 author:Konstantin Voevodski, Maria-Florina Balcan, Heiko Roglin, Shang-Hua Teng, Yu Xia category:cs.LG cs.AI  published:2014-08-09 summary:Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s 2 S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. We use our algorithm to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire dataset. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification. version:1
arxiv-1408-2044 | Matrix Coherence and the Nystrom Method | http://arxiv.org/abs/1408.2044 | id:1408.2044 author:Ameet Talwalkar, Afshin Rostamizadeh category:cs.LG stat.ML  published:2014-08-09 summary:The Nystrom method is an efficient technique used to speed up large-scale learning applications by generating low-rank approximations. Crucial to the performance of this technique is the assumption that a matrix can be well approximated by working exclusively with a subset of its columns. In this work we relate this assumption to the concept of matrix coherence, connecting coherence to the performance of the Nystrom method. Making use of related work in the compressed sensing and the matrix completion literature, we derive novel coherence-based bounds for the Nystrom method in the low-rank setting. We then present empirical results that corroborate these theoretical bounds. Finally, we present more general empirical results for the full-rank setting that convincingly demonstrate the ability of matrix coherence to measure the degree to which information can be extracted from a subset of columns. version:1
arxiv-1408-2042 | Gaussian Process Structural Equation Models with Latent Variables | http://arxiv.org/abs/1408.2042 | id:1408.2042 author:Ricardo Silva, Robert B. Gramacy category:cs.LG stat.ML  published:2014-08-09 summary:In a variety of disciplines such as social sciences, psychology, medicine and economics, the recorded data are considered to be noisy measurements of latent variables connected by some causal structure. This corresponds to a family of graphical models known as the structural equation model with latent variables. While linear non-Gaussian variants have been well-studied, inference in nonparametric structural equation models is still underdeveloped. We introduce a sparse Gaussian process parameterization that defines a non-linear structure connecting latent variables, unlike common formulations of Gaussian process latent variable models. The sparse parameterization is given a full Bayesian treatment without compromising Markov chain Monte Carlo efficiency. We compare the stability of the sampling procedure and the predictive ability of the model against the current practice. version:1
arxiv-1408-2041 | GraphLab: A New Framework For Parallel Machine Learning | http://arxiv.org/abs/1408.2041 | id:1408.2041 author:Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos E. Guestrin, Joseph Hellerstein category:cs.LG cs.DC  published:2014-08-09 summary:Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems. version:1
arxiv-1408-2040 | Prediction with Advice of Unknown Number of Experts | http://arxiv.org/abs/1408.2040 | id:1408.2040 author:Alexey Chernov, Vladimir Vovk category:cs.LG stat.ML  published:2014-08-09 summary:In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the Normal- Hedge bound, which mainly depends on the effective number of experts but also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales. version:1
arxiv-1408-2039 | Incorporating Side Information in Probabilistic Matrix Factorization with Gaussian Processes | http://arxiv.org/abs/1408.2039 | id:1408.2039 author:Ryan Prescott Adams, George E. Dahl, Iain Murray category:cs.LG stat.ML  published:2014-08-09 summary:Probabilistic matrix factorization (PMF) is a powerful method for modeling data associ- ated with pairwise relationships, Finding use in collaborative Filtering, computational bi- ology, and document analysis, among other areas. In many domains, there are additional covariates that can assist in prediction. For example, when modeling movie ratings, we might know when the rating occurred, where the user lives, or what actors appear in the movie. It is difficult, however, to incorporate this side information into the PMF model. We propose a framework for incorporating side information by coupling together multi- ple PMF problems via Gaussian process priors. We replace scalar latent features with func- tions that vary over the covariate space. The GP priors on these functions require them to vary smoothly and share information. We apply this new method to predict the scores of professional basketball games, where side information about the venue and date of the game are relevant for the outcome. version:1
arxiv-1408-2038 | A direct method for estimating a causal ordering in a linear non-Gaussian acyclic model | http://arxiv.org/abs/1408.2038 | id:1408.2038 author:Shohei Shimizu, Aapo Hyvarinen, Yoshinobu Kawahara category:cs.LG stat.ML  published:2014-08-09 summary:Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the datagenerating process of variables. Recently, it was shown that use of non-Gaussianity identifies a causal ordering of variables in a linear acyclic model without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model. version:1
arxiv-1408-2037 | Quantum Annealing for Variational Bayes Inference | http://arxiv.org/abs/1408.2037 | id:1408.2037 author:Issei Sato, Kenichi Kurihara, Shu Tanaka, Hiroshi Nakagawa, Seiji Miyashita category:cs.LG stat.ML  published:2014-08-09 summary:This paper presents studies on a deterministic annealing algorithm based on quantum annealing for variational Bayes (QAVB) inference, which can be seen as an extension of the simulated annealing for variational Bayes (SAVB) inference. QAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a better local optimum than SAVB in terms of the variational free energy in latent Dirichlet allocation (LDA). version:1
arxiv-1408-2035 | Quantum Annealing for Clustering | http://arxiv.org/abs/1408.2035 | id:1408.2035 author:Kenichi Kurihara, Shu Tanaka, Seiji Miyashita category:cs.AI cs.LG  published:2014-08-09 summary:This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA). We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice. Experiments show the proposed QA algorithm finds better clustering assignments than SA. Furthermore, QA is as easy as SA to implement. version:1
arxiv-1408-2033 | Robust Graphical Modeling with t-Distributions | http://arxiv.org/abs/1408.2033 | id:1408.2033 author:Michael A. Finegold, Mathias Drton category:cs.LG stat.ML  published:2014-08-09 summary:Graphical Gaussian models have proven to be useful tools for exploring network structures based on multivariate data. Applications to studies of gene expression have generated substantial interest in these models, and resulting recent progress includes the development of fitting methodology involving penalization of the likelihood function. In this paper we advocate the use of the multivariate t and related distributions for more robust inference of graphs. In particular, we demonstrate that penalized likelihood inference combined with an application of the EM algorithm provides a simple and computationally efficient approach to model selection in the t-distribution case. version:1
arxiv-1408-2032 | Bayesian Multitask Learning with Latent Hierarchies | http://arxiv.org/abs/1408.2032 | id:1408.2032 author:Hal Daume III category:cs.LG stat.ML  published:2014-08-09 summary:We learn multiple hypotheses for related tasks under a latent hierarchical relationship between tasks. We exploit the intuition that for domain adaptation, we wish to share classifier structure, but for multitask learning, we wish to share covariance structure. Our hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real-world data sets. version:1
arxiv-1408-2031 | Conditional Probability Tree Estimation Analysis and Algorithms | http://arxiv.org/abs/1408.2031 | id:1408.2031 author:Alina Beygelzimer, John Langford, Yuri Lifshits, Gregory Sorkin, Alexander L. Strehl category:cs.LG stat.ML  published:2014-08-09 summary:We consider the problem of estimating the conditional probability of a label in time O(log n), where n is the number of possible labels. We analyze a natural reduction of this problem to a set of binary regression problems organized in a tree structure, proving a regret bound that scales with the depth of the tree. Motivated by this analysis, we propose the first online algorithm which provably constructs a logarithmic depth tree on the set of labels to solve this problem. We test the algorithm empirically, showing that it works succesfully on a dataset with roughly 106 labels. version:1
arxiv-1408-2025 | Blind Construction of Optimal Nonlinear Recursive Predictors for Discrete Sequences | http://arxiv.org/abs/1408.2025 | id:1408.2025 author:Cosma Shalizi, Kristina Lisa Klinkner category:cs.LG stat.ML  published:2014-08-09 summary:We present a new method for nonlinear prediction of discrete random sequences under minimal structural assumptions. We give a mathematical construction for optimal predictors of such processes, in the form of hidden Markov models. We then describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which approximates the ideal predictor from data. We discuss the reliability of CSSR, its data requirements, and its performance in simulations. Finally, we compare our approach to existing methods using variablelength Markov models and cross-validated hidden Markov models, and show theoretically and experimentally that our method delivers results superior to the former and at least comparable to the latter. version:1
arxiv-1408-2015 | Automatic Removal of Marginal Annotations in Printed Text Document | http://arxiv.org/abs/1408.2015 | id:1408.2015 author:Abdessamad Elboushaki, Rachida Hannane, P. Nagabhushan, Mohammed Javed category:cs.CV  published:2014-08-09 summary:Recovering the original printed texts from a document with added handwritten annotations in the marginal area is one of the challenging problems, especially when the original document is not available. Therefore, this paper aims at salvaging automatically the original document from the annotated document by detecting and removing any handwritten annotations that appear in the marginal area of the document without any loss of information. Here a two stage algorithm is proposed, where in the first stage due to approximate marginal boundary detection with horizontal and vertical projection profiles, all of the marginal annotations along with some part of the original printed text that may appear very close to the marginal boundary are removed. Therefore as a second stage, using the connected components, a strategy is applied to bring back the printed text components cropped during the first stage. The proposed method is validated using a dataset of 50 documents having complex handwritten annotations, which gives an overall accuracy of 89.01% in removing the marginal annotations and 97.74% in case of retrieving the original printed text document. version:1
arxiv-1408-1986 | Gabor-like Image Filtering using a Neural Microcircuit | http://arxiv.org/abs/1408.1986 | id:1408.1986 author:C. Mayr, A. Heittmann, R. Schüffny category:cs.CV cs.ET q-bio.NC  published:2014-08-08 summary:In this letter, we present an implementation of a neural microcircuit for image processing employing Hebbian-adaptive learning. The neuronal circuit utilizes only excitatory synapses to correlate action potentials, extracting the uncorrelated ones, which contain significant image information. This circuit is capable of approximating Gabor-like image filtering and other image processing functions version:1
arxiv-1408-1985 | A model of grassroots changes in linguistic systems | http://arxiv.org/abs/1408.1985 | id:1408.1985 author:Janet B. Pierrehumbert, Forrest Stonedahl, Robert Daland category:cs.CL nlin.AO physics.soc-ph  published:2014-08-08 summary:Linguistic norms emerge in human communities because people imitate each other. A shared linguistic system provides people with the benefits of shared knowledge and coordinated planning. Once norms are in place, why would they ever change? This question, echoing broad questions in the theory of social dynamics, has particular force in relation to language. By definition, an innovator is in the minority when the innovation first occurs. In some areas of social dynamics, important minorities can strongly influence the majority through their power, fame, or use of broadcast media. But most linguistic changes are grassroots developments that originate with ordinary people. Here, we develop a novel model of communicative behavior in communities, and identify a mechanism for arbitrary innovations by ordinary people to have a good chance of being widely adopted. To imitate each other, people must form a mental representation of what other people do. Each time they speak, they must also decide which form to produce themselves. We introduce a new decision function that enables us to smoothly explore the space between two types of behavior: probability matching (matching the probabilities of incoming experience) and regularization (producing some forms disproportionately often). Using Monte Carlo methods, we explore the interactions amongst the degree of regularization, the distribution of biases in a network, and the network position of the innovator. We identify two regimes for the widespread adoption of arbritrary innovations, viewed as informational cascades in the network. With moderate regularization of experienced input, average people (not well-connected people) are the most likely source of successful innovations. Our results shed light on a major outstanding puzzle in the theory of language change. The framework also holds promise for understanding the dynamics of other social norms. version:1
arxiv-1408-1984 | Neighborhood Rank Order Coding for Robust Texture Analysis and Feature Extraction | http://arxiv.org/abs/1408.1984 | id:1408.1984 author:C. Mayr, R. Schüffny category:cs.CV  published:2014-08-08 summary:Research into the visual cortex and general neural information processing has led to various attempts to integrate pulse computation schemes in image analysis systems. Of interest is especially the robustness of representing an analogue signal in the phase or duration of a pulsed, quasi-digital signal, as well as the possibility of direct digital interaction, i.e. computation, among these signals. Such a computation can also achieve information compaction for subsequent processing stages. By using a pulse order encoding scheme motivated by dendritic pulse interaction, we will show that a powerful low-level feature and texture extraction operator, called Pulsed Local Orientation Coding (PLOC), can be implemented. Feature extraction results are being presented, and a possible VLSI implementation is detailed. version:1
arxiv-1408-1928 | Microtask crowdsourcing for disease mention annotation in PubMed abstracts | http://arxiv.org/abs/1408.1928 | id:1408.1928 author:Benjamin M Good, Max Nanis, Andrew I. Su category:cs.CL 9208 H.5.3; I.2.7  published:2014-08-08 summary:Identifying concepts and relationships in biomedical text enables knowledge to be applied in computational analyses. Many biological natural language process (BioNLP) projects attempt to address this challenge, but the state of the art in BioNLP still leaves much room for improvement. Progress in BioNLP research depends on large, annotated corpora for evaluating information extraction systems and training machine learning models. Traditionally, such corpora are created by small numbers of expert annotators often working over extended periods of time. Recent studies have shown that workers on microtask crowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in aggregate, generate high-quality annotations of biomedical text. Here, we investigated the use of the AMT in capturing disease mentions in PubMed abstracts. We used the NCBI Disease corpus as a gold standard for refining and benchmarking our crowdsourcing protocol. After several iterations, we arrived at a protocol that reproduced the annotations of the 593 documents in the training set of this gold standard with an overall F measure of 0.872 (precision 0.862, recall 0.883). The output can also be tuned to optimize for precision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when precision = 0.436). Each document was examined by 15 workers, and their annotations were merged based on a simple voting method. In total 145 workers combined to complete all 593 documents in the span of 1 week at a cost of $.06 per abstract per worker. The quality of the annotations, as judged with the F measure, increases with the number of workers assigned to each task such that the system can be tuned to balance cost against quality. These results demonstrate that microtask crowdsourcing can be a valuable tool for generating well-annotated corpora in BioNLP. version:1
arxiv-1408-1913 | Using Learned Predictions as Feedback to Improve Control and Communication with an Artificial Limb: Preliminary Findings | http://arxiv.org/abs/1408.1913 | id:1408.1913 author:Adam S. R. Parker, Ann L. Edwards, Patrick M. Pilarski category:cs.AI cs.HC cs.LG cs.RO  published:2014-08-08 summary:Many people suffer from the loss of a limb. Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fulfil the needs of individuals with amputations. One promising solution is to provide greater communication between a prosthesis and its user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system's predictions were then communicated to the device's user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm's contact with its workspace. Our trials showed that communicable predictions could be learned quickly during human control of the robot arm. Using these predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use. version:1
arxiv-1408-1906 | Exploring the evolution of a trade-off between vigilance and foraging in group-living organisms | http://arxiv.org/abs/1408.1906 | id:1408.1906 author:Randal S. Olson, Patrick B. Haley, Fred C. Dyer, Christoph Adami category:q-bio.PE cs.GT cs.NE  published:2014-08-08 summary:Despite the fact that grouping behavior has been actively studied for over a century, the relative importance of the numerous proposed fitness benefits of grouping remain unclear. We use a digital model of evolving prey under simulated predation to directly explore the evolution of gregarious foraging behavior according to one such benefit, the "many eyes" hypothesis. According to this hypothesis, collective vigilance allows prey in large groups to detect predators more efficiently by making alarm signals or behavioral cues to each other, thereby allowing individuals within the group to spend more time foraging. Here, we find that collective vigilance is sufficient to select for gregarious foraging behavior as long there is not a direct cost for grouping (e.g., competition for limited food resources), even when controlling for confounding factors such as the dilution effect. Further, we explore the role of the genetic relatedness and reproductive strategy of the prey, and find that highly related groups of prey with a semelparous reproductive strategy are the most likely to evolve gregarious foraging behavior mediated by the benefit of vigilance. These findings, combined with earlier studies with evolving digital organisms, further sharpen our understanding of the factors favoring grouping behavior. version:1
arxiv-1403-1412 | Rate Prediction and Selection in LTE systems using Modified Source Encoding Techniques | http://arxiv.org/abs/1403.1412 | id:1403.1412 author:K. P. Saishankar, Sheetal Kalyani, K. Narendran category:stat.AP cs.IT cs.LG math.IT  published:2014-03-06 summary:In current wireless systems, the base-Station (eNodeB) tries to serve its user-equipment (UE) at the highest possible rate that the UE can reliably decode. The eNodeB obtains this rate information as a quantized feedback from the UE at time n and uses this, for rate selection till the next feedback is received at time n + {\delta}. The feedback received at n can become outdated before n + {\delta}, because of a) Doppler fading, and b) Change in the set of active interferers for a UE. Therefore rate prediction becomes essential. Since, the rates belong to a discrete set, we propose a discrete sequence prediction approach, wherein, frequency trees for the discrete sequences are built using source encoding algorithms like Prediction by Partial Match (PPM). Finding the optimal depth of the frequency tree used for prediction is cast as a model order selection problem. The rate sequence complexity is analysed to provide an upper bound on model order. Information-theoretic criteria are then used to solve the model order problem. Finally, two prediction algorithms are proposed, using the PPM with optimal model order and system level simulations demonstrate the improvement in packet loss and throughput due to these algorithms. version:5
arxiv-1408-1784 | Origin of the computational hardness for learning with binary synapses | http://arxiv.org/abs/1408.1784 | id:1408.1784 author:Haiping Huang, Yoshiyuki Kabashima category:cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC  published:2014-08-08 summary:Supervised learning in a binary perceptron is able to classify an extensive number of random patterns by a proper assignment of binary synaptic weights. However, to find such assignments in practice, is quite a nontrivial task. The relation between the weight space structure and the algorithmic hardness has not yet been fully understood. To this end, we analytically derive the Franz-Parisi potential for the binary preceptron problem, by starting from an equilibrium solution of weights and exploring the weight space structure around it. Our result reveals the geometrical organization of the weight space\textemdash the weight space is composed of isolated solutions, rather than clusters of exponentially many close-by solutions. The point-like clusters far apart from each other in the weight space explain the previously observed glassy behavior of stochastic local search heuristics. version:1
arxiv-1312-0788 | A compact formula for the derivative of a 3-D rotation in exponential coordinates | http://arxiv.org/abs/1312.0788 | id:1312.0788 author:Guillermo Gallego, Anthony Yezzi category:cs.CV math.OC  published:2013-12-03 summary:We present a compact formula for the derivative of a 3-D rotation matrix with respect to its exponential coordinates. A geometric interpretation of the resulting expression is provided, as well as its agreement with other less-compact but better-known formulas. To the best of our knowledge, this simpler formula does not appear anywhere in the literature. We hope by providing this more compact expression to alleviate the common pressure to reluctantly resort to alternative representations in various computational applications simply as a means to avoid the complexity of differential analysis in exponential coordinates. version:2
arxiv-1408-1774 | Beyond description. Comment on "Approaching human language with complex networks" by Cong & Liu | http://arxiv.org/abs/1408.1774 | id:1408.1774 author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph  published:2014-08-08 summary:Comment on "Approaching human language with complex networks" by Cong & Liu version:1
arxiv-1408-1759 | Real-Time and Robust Method for Hand Gesture Recognition System Based on Cross-Correlation Coefficient | http://arxiv.org/abs/1408.1759 | id:1408.1759 author:Reza Azad, Babak Azad, Iman Tavakoli Kazerooni category:cs.CV  published:2014-08-08 summary:Hand gesture recognition possesses extensive applications in virtual reality, sign language recognition, and computer games. The direct interface of hand gestures provides us a new way for communicating with the virtual environment. In this paper a novel and real-time approach for hand gesture recognition system is presented. In the suggested method, first, the hand gesture is extracted from the main image by the image segmentation and morphological operation and then is sent to feature extraction stage. In feature extraction stage the Cross-correlation coefficient is applied on the gesture to recognize it. In the result part, the proposed approach is applied on American Sign Language (ASL) database and the accuracy rate obtained 98.34%. version:1
arxiv-1112-0826 | Clustering under Perturbation Resilience | http://arxiv.org/abs/1112.0826 | id:1112.0826 author:Maria Florina Balcan, Yingyu Liang category:cs.LG cs.DS  published:2011-12-05 summary:Motivated by the fact that distances between data points in many real-world clustering instances are often based on heuristic measures, Bilu and Linial~\cite{BL} proposed analyzing objective based clustering problems under the assumption that the optimum clustering to the objective is preserved under small multiplicative perturbations to distances between points. The hope is that by exploiting the structure in such instances, one can overcome worst case hardness results. In this paper, we provide several strong results within this framework. For center-based objectives, we present an algorithm that can optimally cluster instances resilient to perturbations of factor $(1 + \sqrt{2})$, solving an open problem of Awasthi et al.~\cite{ABS10}. For $k$-median, a center-based objective of special interest, we additionally give algorithms for a more relaxed assumption in which we allow the optimal solution to change in a small $\epsilon$ fraction of the points after perturbation. We give the first bounds known for $k$-median under this more realistic and more general assumption. We also provide positive results for min-sum clustering which is a generally much harder objective than center-based objectives. Our algorithms are based on new linkage criteria that may be of independent interest. Additionally, we give sublinear-time algorithms, showing algorithms that can return an implicit clustering from only access to a small random sample. version:4
arxiv-1408-1688 | Low-rank SIFT: An Affine Invariant Feature for Place Recognition | http://arxiv.org/abs/1408.1688 | id:1408.1688 author:Chao Yang, Shengnan Caih, Jingdong Wang, Long Quan category:cs.CV  published:2014-08-07 summary:In this paper, we present a novel affine-invariant feature based on SIFT, leveraging the regular appearance of man-made objects. The feature achieves full affine invariance without needing to simulate over affine parameter space. Low-rank SIFT, as we name the feature, is based on our observation that local tilt, which are caused by changes of camera axis orientation, could be normalized by converting local patches to standard low-rank forms. Rotation, translation and scaling invariance could be achieved in ways similar to SIFT. As an extension of SIFT, our method seeks to add prior to solve the ill-posed affine parameter estimation problem and normalizes them directly, and is applicable to objects with regular structures. Furthermore, owing to recent breakthrough in convex optimization, such parameter could be computed efficiently. We will demonstrate its effectiveness in place recognition as our major application. As extra contributions, we also describe our pipeline of constructing geotagged building database from the ground up, as well as an efficient scheme for automatic feature selection. version:1
arxiv-1407-2044 | Tracking Individual Targets in High Density Crowd Scenes Analysis of a Video Recording in Hajj 2009 | http://arxiv.org/abs/1407.2044 | id:1407.2044 author:Mohamed H. Dridi category:cs.CV physics.soc-ph  published:2014-07-08 summary:In this paper we present a number of methods (manual, semi-automatic and automatic) for tracking individual targets in high density crowd scenes where thousand of people are gathered. The necessary data about the motion of individuals and a lot of other physical information can be extracted from consecutive image sequences in different ways, including optical flow and block motion estimation. One of the famous methods for tracking moving objects is the block matching method. This way to estimate subject motion requires the specification of a comparison window which determines the scale of the estimate. In this work we present a real-time method for pedestrian recognition and tracking in sequences of high resolution images obtained by a stationary (high definition) camera located in different places on the Haram mosque in Mecca. The objective is to estimate pedestrian velocities as a function of the local density.The resulting data of tracking moving pedestrians based on video sequences are presented in the following section. Through the evaluated system the spatio-temporal coordinates of each pedestrian during the Tawaf ritual are established. The pilgrim velocities as function of the local densities in the Mataf area (Haram Mosque Mecca) are illustrated and very precisely documented. version:2
arxiv-1408-1549 | Real-Time Human-Computer Interaction Based on Face and Hand Gesture Recognition | http://arxiv.org/abs/1408.1549 | id:1408.1549 author:Reza Azad, Babak Azad, Nabil Belhaj Khalifa, Shahram Jamali category:cs.CV  published:2014-08-07 summary:At the present time, hand gestures recognition system could be used as a more expected and useable approach for human computer interaction. Automatic hand gesture recognition system provides us a new tactic for interactive with the virtual environment. In this paper, a face and hand gesture recognition system which is able to control computer media player is offered. Hand gesture and human face are the key element to interact with the smart system. We used the face recognition scheme for viewer verification and the hand gesture recognition in mechanism of computer media player, for instance, volume down/up, next music and etc. In the proposed technique, first, the hand gesture and face location is extracted from the main image by combination of skin and cascade detector and then is sent to recognition stage. In recognition stage, first, the threshold condition is inspected then the extracted face and gesture will be recognized. In the result stage, the proposed technique is applied on the video dataset and the high precision ratio acquired. Additional the recommended hand gesture recognition method is applied on static American Sign Language (ASL) database and the correctness rate achieved nearby 99.40%. also the planned method could be used in gesture based computer games and virtual reality. version:1
arxiv-1410-2173 | Face Detection Using Radial Basis Functions Neural Networks With Fixed Spread | http://arxiv.org/abs/1410.2173 | id:1410.2173 author:K. A. A. Aziz, S. S. Abdullah category:cs.CV  published:2014-08-07 summary:This paper presented a face detection system using Radial Basis Function Neural Networks With Fixed Spread Value. Face detection is the first step in face recognition system. The purpose is to localize and extract the face region from the background that will be fed into the face recognition system for identification. General preprocessing approach was used for normalizing the image and Radial Basis Function (RBF) Neural Network was used to distinguish between face and non-face. RBF Neural Networks offer several advantages compared to other neural network architecture such as they can be trained using fast two stages training algorithm and the network possesses the property of best approximation. The output of the network can be optimized by setting suitable value of center and spread of the RBF. In this paper, fixed spread value will be used. The Radial Basis Function Neural Network (RBFNN) used to distinguish faces and non-faces and the evaluation of the system will be the performance of detection, False Acceptance Rate (FAR), False Rejection Rate (FRR) and the discriminative properties. version:1
arxiv-1406-2507 | WebAL-1: Workshop on Artificial Life and the Web 2014 Proceedings | http://arxiv.org/abs/1406.2507 | id:1406.2507 author:Tim Taylor category:cs.NE cs.MA  published:2014-06-10 summary:Proceedings of WebAL-1: Workshop on Artificial Life and the Web 2014, held at the 14th International Conference on the Synthesis and Simulation of Living Systems (ALIFE 14), New York, NY, 31 July 2014. version:4
arxiv-1407-4596 | Sparse and Low-Rank Covariance Matrices Estimation | http://arxiv.org/abs/1407.4596 | id:1407.4596 author:Shenglong Zhou, Naihua Xiu, Ziyan Luo, Lingchen Kong category:math.ST math.OC stat.ML stat.TH  published:2014-07-17 summary:This paper aims at achieving a simultaneously sparse and low-rank estimator from the semidefinite population covariance matrices. We first benefit from a convex optimization which develops $l_1$-norm penalty to encourage the sparsity and nuclear norm to favor the low-rank property. For the proposed estimator, we then prove that with large probability, the Frobenious norm of the estimation rate can be of order $O(\sqrt{s(\log{r})/n})$ under a mild case, where $s$ and $r$ denote the number of sparse entries and the rank of the population covariance respectively, $n$ notes the sample capacity. Finally an efficient alternating direction method of multipliers with global convergence is proposed to tackle this problem, and meantime merits of the approach are also illustrated by practicing numerical simulations. version:2
arxiv-1408-1655 | Preventing False Discovery in Interactive Data Analysis is Hard | http://arxiv.org/abs/1408.1655 | id:1408.1655 author:Moritz Hardt, Jonathan Ullman category:cs.LG cs.CC cs.DS  published:2014-08-06 summary:We show that, under a standard hardness assumption, there is no computationally efficient algorithm that given $n$ samples from an unknown distribution can give valid answers to $n^{3+o(1)}$ adaptively chosen statistical queries. A statistical query asks for the expectation of a predicate over the underlying distribution, and an answer to a statistical query is valid if it is "close" to the correct expectation over the distribution. Our result stands in stark contrast to the well known fact that exponentially many statistical queries can be answered validly and efficiently if the queries are chosen non-adaptively (no query may depend on the answers to previous queries). Moreover, a recent work by Dwork et al. shows how to accurately answer exponentially many adaptively chosen statistical queries via a computationally inefficient algorithm; and how to answer a quadratic number of adaptive queries via a computationally efficient algorithm. The latter result implies that our result is tight up to a linear factor in $n.$ Conceptually, our result demonstrates that achieving statistical validity alone can be a source of computational intractability in adaptive settings. For example, in the modern large collaborative research environment, data analysts typically choose a particular approach based on previous findings. False discovery occurs if a research finding is supported by the data but not by the underlying distribution. While the study of preventing false discovery in Statistics is decades old, to the best of our knowledge our result is the first to demonstrate a computational barrier. In particular, our result suggests that the perceived difficulty of preventing false discovery in today's collaborative research environment may be inherent. version:1
arxiv-1408-1319 | When does Active Learning Work? | http://arxiv.org/abs/1408.1319 | id:1408.1319 author:Lewis Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG  published:2014-08-06 summary:Active Learning (AL) methods seek to improve classifier performance when labels are expensive or scarce. We consider two central questions: Where does AL work? How much does it help? To address these questions, a comprehensive experimental simulation study of Active Learning is presented. We consider a variety of tasks, classifiers and other AL factors, to present a broad exploration of AL performance in various settings. A precise way to quantify performance is needed in order to know when AL works. Thus we also present a detailed methodology for tackling the complexities of assessing AL performance in the context of this experimental study. version:1
arxiv-1408-1297 | New crossover operators for multiple subset selection tasks | http://arxiv.org/abs/1408.1297 | id:1408.1297 author:Arnab Roy, J. David Schaffer, Craig B. Laramee category:cs.NE 68 I.5.2  I.5.4  published:2014-08-06 summary:We have introduced two crossover operators, MMX-BLXexploit and MMX-BLXexplore, for simultaneously solving multiple feature/subset selection problems where the features may have numeric attributes and the subset sizes are not predefined. These operators differ on the level of exploration and exploitation they perform; one is designed to produce convergence controlled mutation and the other exhibits a quasi-constant mutation rate. We illustrate the characteristic of these operators by evolving pattern detectors to distinguish alcoholics from controls using their visually evoked response potentials (VERPs). This task encapsulates two groups of subset selection problems; choosing a subset of EEG leads along with the lead-weights (features with attributes) and the other that defines the temporal pattern that characterizes the alcoholic VERPs. We observed better generalization performance from MMX-BLXexplore. Perhaps, MMX-BLXexploit was handicapped by not having a restart mechanism. These operators are novel and appears to hold promise for solving simultaneous feature selection problems. version:1
arxiv-1404-1682 | Pseudo-Zernike Based Multi-Pass Automatic Target Recognition From Multi-Channel SAR | http://arxiv.org/abs/1404.1682 | id:1404.1682 author:Carmine Clemente, Luca Pallotta, Ian Proudler, Antonio De Maio, John J. Soraghan, Alfonso Farina category:cs.CV  published:2014-04-07 summary:The capability to exploit multiple sources of information is of fundamental importance in a battlefield scenario. Information obtained from different sources, and separated in space and time, provide the opportunity to exploit diversities in order to mitigate uncertainty. For the specific challenge of Automatic Target Recognition (ATR) from radar platforms, both channel (e.g. polarization) and spatial diversity can provide useful information for such a specific and critical task. In this paper the use of pseudo-Zernike moments applied to multi-channel multi-pass data is presented exploiting diversities and invariant properties leading to high confidence ATR, small computational complexity and data transfer requirements. The effectiveness of the proposed approach, in different configurations and data source availability is demonstrated using real data. version:3
arxiv-1408-1187 | The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions | http://arxiv.org/abs/1408.1187 | id:1408.1187 author:Mattia Ciollaro, Christopher Genovese, Jing Lei, Larry Wasserman category:stat.ME stat.AP stat.ML  published:2014-08-06 summary:We introduce the functional mean-shift algorithm, an iterative algorithm for estimating the local modes of a surrogate density from functional data. We show that the algorithm can be used for cluster analysis of functional data. We propose a test based on the bootstrap for the significance of the estimated local modes of the surrogate density. We present two applications of our methodology. In the first application, we demonstrate how the functional mean-shift algorithm can be used to perform spike sorting, i.e. cluster neural activity curves. In the second application, we use the functional mean-shift algorithm to distinguish between original and fake signatures. version:1
arxiv-1408-0889 | Computing With Contextual Numbers | http://arxiv.org/abs/1408.0889 | id:1408.0889 author:Vahid Moosavi category:cs.CE cs.CV cs.NE  published:2014-08-05 summary:Self Organizing Map (SOM) has been applied into several classical modeling tasks including clustering, classification, function approximation and visualization of high dimensional spaces. The final products of a trained SOM are a set of ordered (low dimensional) indices and their associated high dimensional weight vectors. While in the above-mentioned applications, the final high dimensional weight vectors play the primary role in the computational steps, from a certain perspective, one can interpret SOM as a nonparametric encoder, in which the final low dimensional indices of the trained SOM are pointer to the high dimensional space. We showed how using a one-dimensional SOM, which is not common in usual applications of SOM, one can develop a nonparametric mapping from a high dimensional space to a continuous one-dimensional numerical field. These numerical values, called contextual numbers, are ordered in a way that in a given context, similar numbers refer to similar high dimensional states. Further, as these numbers can be treated similarly to usual continuous numbers, they can be replaced with their corresponding high dimensional states within any data driven modeling problem. As a potential application, we showed how using contextual numbers could be used for the problem of high dimensional spatiotemporal dynamics. version:2
arxiv-1408-1167 | Boosted Markov Networks for Activity Recognition | http://arxiv.org/abs/1408.1167 | id:1408.1167 author:Truyen Tran, Hung Bui, Svetha Venkatesh category:cs.LG cs.CV stat.ML  published:2014-08-06 summary:We explore a framework called boosted Markov networks to combine the learning capacity of boosting and the rich modeling semantics of Markov networks and applying the framework for video-based activity recognition. Importantly, we extend the framework to incorporate hidden variables. We show how the framework can be applied for both model learning and feature selection. We demonstrate that boosted Markov networks with hidden variables perform comparably with the standard maximum likelihood estimation. However, our framework is able to learn sparse models, and therefore can provide computational savings when the learned models are used for classification. version:1
arxiv-1408-3081 | Human Activity Learning and Segmentation using Partially Hidden Discriminative Models | http://arxiv.org/abs/1408.3081 | id:1408.3081 author:Truyen Tran, Hung Bui, Svetha Venkatesh category:cs.LG cs.CV stat.ML  published:2014-08-06 summary:Learning and understanding the typical patterns in the daily activities and routines of people from low-level sensory data is an important problem in many application domains such as building smart environments, or providing intelligent assistance. Traditional approaches to this problem typically rely on supervised learning and generative models such as the hidden Markov models and its extensions. While activity data can be readily acquired from pervasive sensors, e.g. in smart environments, providing manual labels to support supervised training is often extremely expensive. In this paper, we propose a new approach based on semi-supervised training of partially hidden discriminative models such as the conditional random field (CRF) and the maximum entropy Markov model (MEMM). We show that these models allow us to incorporate both labeled and unlabeled data for learning, and at the same time, provide us with the flexibility and accuracy of the discriminative framework. Our experimental results in the video surveillance domain illustrate that these models can perform better than their generative counterpart, the partially hidden Markov model, even when a substantial amount of labels are unavailable. version:1
arxiv-1408-1162 | MCMC for Hierarchical Semi-Markov Conditional Random Fields | http://arxiv.org/abs/1408.1162 | id:1408.1162 author:Truyen Tran, Dinh Phung, Svetha Venkatesh, Hung H. Bui category:stat.ML cs.LG stat.ME  published:2014-08-06 summary:Deep architecture such as hierarchical semi-Markov models is an important class of models for nested sequential data. Current exact inference schemes either cost cubic time in sequence length, or exponential time in model depth. These costs are prohibitive for large-scale problems with arbitrary length and depth. In this contribution, we propose a new approximation technique that may have the potential to achieve sub-cubic time complexity in length and linear time depth, at the cost of some loss of quality. The idea is based on two well-known methods: Gibbs sampling and Rao-Blackwellisation. We provide some simulation-based evaluation of the quality of the RGBS with respect to run time and sequence length. version:1
arxiv-1408-1160 | Mixed-Variate Restricted Boltzmann Machines | http://arxiv.org/abs/1408.1160 | id:1408.1160 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG stat.ME  published:2014-08-06 summary:Modern datasets are becoming heterogeneous. To this end, we present in this paper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling variables of multiple types and modalities, including binary and continuous responses, categorical options, multicategorical choices, ordinal assessment and category-ranked preferences. Dependency among variables is modeled using latent binary variables, each of which can be interpreted as a particular hidden aspect of the data. The proposed model, similar to the standard RBMs, allows fast evaluation of the posterior for the latent variables. Hence, it is naturally suitable for many common tasks including, but not limited to, (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors, thereby offering a dimensionality reduction capacity, (b) as a classifier supporting binary, multiclass, multilabel, and label-ranking outputs, or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. We evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization, data completion and prediction. version:1
arxiv-1408-1135 | It is hard to see a needle in a haystack: Modeling contrast masking effect in a numerical observer | http://arxiv.org/abs/1408.1135 | id:1408.1135 author:Ali R. N. Avanaki, Kathryn S. Espig, Albert Xthona, Tom R. L. Kimpe, Predrag R. Bakic, Andrew D. A. Maidment category:cs.CV  published:2014-08-05 summary:Within the framework of a virtual clinical trial for breast imaging, we aim to develop numerical observers that follow the same detection performance trends as those of a typical human observer. In our prior work, we showed that by including spatiotemporal contrast sensitivity function (stCSF) of human visual system (HVS) in a multi-slice channelized Hotelling observer (msCHO), we can correctly predict trends of a typical human observer performance with the viewing parameters of browsing speed, viewing distance and contrast. In this work we further improve our numerical observer by modeling contrast masking. After stCSF, contrast masking is the second most prominent property of HVS and it refers to the fact that the presence of one signal affects the visibility threshold for another signal. Our results indicate that the improved numerical observer better predicts changes in detection performance with background complexity. version:1
arxiv-1408-0998 | The Case for a Mixed-Initiative Collaborative Neuroevolution Approach | http://arxiv.org/abs/1408.0998 | id:1408.0998 author:Sebastian Risi, Jinhong Zhang, Rasmus Taarnby, Peter Greve, Jan Piskur, Antonios Liapis, Julian Togelius category:cs.NE  published:2014-08-05 summary:It is clear that the current attempts at using algorithms to create artificial neural networks have had mixed success at best when it comes to creating large networks and/or complex behavior. This should not be unexpected, as creating an artificial brain is essentially a design problem. Human design ingenuity still surpasses computational design for most tasks in most domains, including architecture, game design, and authoring literary fiction. This leads us to ask which the best way is to combine human and machine design capacities when it comes to designing artificial brains. Both of them have their strengths and weaknesses; for example, humans are much too slow to manually specify thousands of neurons, let alone the billions of neurons that go into a human brain, but on the other hand they can rely on a vast repository of common-sense understanding and design heuristics that can help them perform a much better guided search in design space than an algorithm. Therefore, in this paper we argue for a mixed-initiative approach for collaborative online brain building and present first results towards this goal. version:1
arxiv-1308-3201 | Confidence Sets Based on Thresholding Estimators in High-Dimensional Gaussian Regression Models | http://arxiv.org/abs/1308.3201 | id:1308.3201 author:Ulrike Schneider category:math.ST stat.ME stat.ML stat.TH  published:2013-08-14 summary:We study confidence intervals based on hard-thresholding, soft-thresholding, and adaptive soft-thresholding in a linear regression model where the number of regressors $k$ may depend on and diverge with sample size $n$. In addition to the case of known error variance, we define and study versions of the estimators when the error variance is unknown. In the known variance case, we provide an exact analysis of the coverage properties of such intervals in finite samples. We show that these intervals are always larger than the standard interval based on the least-squares estimator. Asymptotically, the intervals based on the thresholding estimators are larger even by an order of magnitude when the estimators are tuned to perform consistent variable selection. For the unknown-variance case, we provide non-trivial lower bounds for the coverage probabilities in finite samples and conduct an asymptotic analysis where the results from the known-variance case can be shown to carry over asymptotically if the number of degrees of freedom $n-k$ tends to infinity fast enough in relation to the thresholding parameter. version:2
arxiv-1408-0985 | Speech earthquakes: scaling and universality in human voice | http://arxiv.org/abs/1408.0985 | id:1408.0985 author:Jordi Luque, Bartolo Luque, Lucas Lacasa category:physics.soc-ph cs.CL q-bio.NC  published:2014-08-05 summary:Speech is a distinctive complex feature of human capabilities. In order to understand the physics underlying speech production, in this work we empirically analyse the statistics of large human speech datasets ranging several languages. We first show that during speech the energy is unevenly released and power-law distributed, reporting a universal robust Gutenberg-Richter-like law in speech. We further show that such earthquakes in speech show temporal correlations, as the interevent statistics are again power-law distributed. Since this feature takes place in the intra-phoneme range, we conjecture that the responsible for this complex phenomenon is not cognitive, but it resides on the physiological speech production mechanism. Moreover, we show that these waiting time distributions are scale invariant under a renormalisation group transformation, suggesting that the process of speech generation is indeed operating close to a critical point. These results are put in contrast with current paradigms in speech processing, which point towards low dimensional deterministic chaos as the origin of nonlinear traits in speech fluctuations. As these latter fluctuations are indeed the aspects that humanize synthetic speech, these findings may have an impact in future speech synthesis technologies. Results are robust and independent of the communication language or the number of speakers, pointing towards an universal pattern and yet another hint of complexity in human speech. version:1
arxiv-1408-0972 | A Flexible Iterative Framework for Consensus Clustering | http://arxiv.org/abs/1408.0972 | id:1408.0972 author:Shaina Race, Carl Meyer category:stat.ML cs.CV cs.LG  published:2014-08-05 summary:A novel framework for consensus clustering is presented which has the ability to determine both the number of clusters and a final solution using multiple algorithms. A consensus similarity matrix is formed from an ensemble using multiple algorithms and several values for k. A variety of dimension reduction techniques and clustering algorithms are considered for analysis. For noisy or high-dimensional data, an iterative technique is presented to refine this consensus matrix in way that encourages algorithms to agree upon a common solution. We utilize the theory of nearly uncoupled Markov chains to determine the number, k , of clusters in a dataset by considering a random walk on the graph defined by the consensus matrix. The eigenvalues of the associated transition probability matrix are used to determine the number of clusters. This method succeeds at determining the number of clusters in many datasets where previous methods fail. On every considered dataset, our consensus method provides a final result with accuracy well above the average of the individual algorithms. version:1
arxiv-1408-0967 | Determining the Number of Clusters via Iterative Consensus Clustering | http://arxiv.org/abs/1408.0967 | id:1408.0967 author:Shaina Race, Carl Meyer, Kevin Valakuzhy category:stat.ML cs.CV cs.LG  published:2014-08-05 summary:We use a cluster ensemble to determine the number of clusters, k, in a group of data. A consensus similarity matrix is formed from the ensemble using multiple algorithms and several values for k. A random walk is induced on the graph defined by the consensus matrix and the eigenvalues of the associated transition probability matrix are used to determine the number of clusters. For noisy or high-dimensional data, an iterative technique is presented to refine this consensus matrix in way that encourages a block-diagonal form. It is shown that the resulting consensus matrix is generally superior to existing similarity matrices for this type of spectral analysis. version:1
arxiv-1407-8289 | DuSK: A Dual Structure-preserving Kernel for Supervised Tensor Learning with Applications to Neuroimages | http://arxiv.org/abs/1407.8289 | id:1407.8289 author:Lifang He, Xiangnan Kong, Philip S. Yu, Ann B. Ragin, Zhifeng Hao, Xiaowei Yang category:cs.LG  published:2014-07-31 summary:With advances in data collection technologies, tensor data is assuming increasing prominence in many applications and the problem of supervised tensor learning has emerged as a topic of critical significance in the data mining and machine learning community. Conventional methods for supervised tensor learning mainly focus on learning kernels by flattening the tensor into vectors or matrices, however structural information within the tensors will be lost. In this paper, we introduce a new scheme to design structure-preserving kernels for supervised tensor learning. Specifically, we demonstrate how to leverage the naturally available structure within the tensorial representation to encode prior knowledge in the kernel. We proposed a tensor kernel that can preserve tensor structures based upon dual-tensorial mapping. The dual-tensorial mapping function can map each tensor instance in the input space to another tensor in the feature space while preserving the tensorial structure. Theoretically, our approach is an extension of the conventional kernels in the vector space to tensor space. We applied our novel kernel in conjunction with SVM to real-world tensor classification problems including brain fMRI classification for three different diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV). Extensive empirical studies demonstrate that our proposed approach can effectively boost tensor classification performances, particularly with small sample sizes. version:2
arxiv-1408-0689 | Real-Time Traffic Signal Control for Modern Roundabouts by Using Particle Swarm Optimization-Based Fuzzy Controller | http://arxiv.org/abs/1408.0689 | id:1408.0689 author:Yue-Jiao Gong, Jun Zhang category:cs.NE  published:2014-08-04 summary:Due to that the existing traffic facilities can hardly be extended, developing traffic signal control methods is the most important way to improve the traffic efficiency of modern roundabouts. This paper proposes a novel traffic signal controller with two fuzzy layers for signalizing the roundabout. The outer layer of the controller computes urgency degrees of all the phase subsets and then activates the most urgent subset. This mechanism helps to instantly respond to the current traffic condition of the roundabout so as to improve real-timeness. The inner layer of the controller computes extension time of the current phase. If the extension value is larger than a threshold value, the current phase is maintained; otherwise the next phase in the running phase subset (selected by the outer layer) is activated. The inner layer adopts well-designed phase sequences, which helps to smooth the traffic flows and to avoid traffic jam. In general, the proposed traffic signal controller is capable of improving real-timeness as well as reducing traffic congestion. Moreover, an offline particle swarm optimization (PSO) algorithm is developed to optimize the membership functions adopted in the proposed controller. By using optimal membership functions, the performance of the controller can be further improved. Simulation results demonstrate that the proposed controller outperforms previous traffic signal controllers in terms of improving the traffic efficiency of modern roundabouts. version:2
arxiv-1408-0838 | Estimating Maximally Probable Constrained Relations by Mathematical Programming | http://arxiv.org/abs/1408.0838 | id:1408.0838 author:Lizhen Qu, Bjoern Andres category:cs.LG cs.NA math.OC stat.ML  published:2014-08-04 summary:Estimating a constrained relation is a fundamental problem in machine learning. Special cases are classification (the problem of estimating a map from a set of to-be-classified elements to a set of labels), clustering (the problem of estimating an equivalence relation on a set) and ranking (the problem of estimating a linear order on a set). We contribute a family of probability measures on the set of all relations between two finite, non-empty sets, which offers a joint abstraction of multi-label classification, correlation clustering and ranking by linear ordering. Estimating (learning) a maximally probable measure, given (a training set of) related and unrelated pairs, is a convex optimization problem. Estimating (inferring) a maximally probable relation, given a measure, is a 01-linear program. It is solved in linear time for maps. It is NP-hard for equivalence relations and linear orders. Practical solutions for all three cases are shown in experiments with real data. Finally, estimating a maximally probable measure and relation jointly is posed as a mixed-integer nonlinear program. This formulation suggests a mathematical programming approach to semi-supervised learning. version:1
arxiv-1408-0814 | Object Detection Through Exploration With A Foveated Visual Field | http://arxiv.org/abs/1408.0814 | id:1408.0814 author:Emre Akbas, Miguel P. Eckstein category:cs.CV  published:2014-08-04 summary:We present a foveated object detector (FOD) as a biologically-inspired alternative to the sliding window (SW) approach which is the dominant method of search in computer vision object detection. Similar to the human visual system, the FOD has higher resolution at the fovea and lower resolution at the visual periphery. Consequently, more computational resources are allocated at the fovea and relatively fewer at the periphery. The FOD processes the entire scene, uses retino-specific object detection classifiers to guide eye movements, aligns its fovea with regions of interest in the input image and integrates observations across multiple fixations. Our approach combines modern object detectors from computer vision with a recent model of peripheral pooling regions found at the V1 layer of the human visual system. We assessed various eye movement strategies on the PASCAL VOC 2007 dataset and show that the FOD performs on par with the SW detector while bringing significant computational cost savings. version:1
arxiv-1408-0782 | Targetable Named Entity Recognition in Social Media | http://arxiv.org/abs/1408.0782 | id:1408.0782 author:Sandeep Ashwini, Jinho D. Choi category:cs.CL  published:2014-08-04 summary:We present a novel approach for recognizing what we call targetable named entities; that is, named entities in a targeted set (e.g, movies, books, TV shows). Unlike many other NER systems that need to retrain their statistical models as new entities arrive, our approach does not require such retraining, which makes it more adaptable for types of entities that are frequently updated. For this preliminary study, we focus on one entity type, movie title, using data collected from Twitter. Our system is tested on two evaluation sets, one including only entities corresponding to movies in our training set, and the other excluding any of those entities. Our final model shows F1-scores of 76.19% and 78.70% on these evaluation sets, which gives strong evidence that our approach is completely unbiased to any par- ticular set of entities found during training. version:1
arxiv-1408-1054 | Multithreshold Entropy Linear Classifier | http://arxiv.org/abs/1408.1054 | id:1408.1054 author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG stat.ML  published:2014-08-04 summary:Linear classifiers separate the data with a hyperplane. In this paper we focus on the novel method of construction of multithreshold linear classifier, which separates the data with multiple parallel hyperplanes. Proposed model is based on the information theory concepts -- namely Renyi's quadratic entropy and Cauchy-Schwarz divergence. We begin with some general properties, including data scale invariance. Then we prove that our method is a multithreshold large margin classifier, which shows the analogy to the SVM, while in the same time works with much broader class of hypotheses. What is also interesting, proposed method is aimed at the maximization of the balanced quality measure (such as Matthew's Correlation Coefficient) as opposed to very common maximization of the accuracy. This feature comes directly from the optimization problem statement and is further confirmed by the experiments on the UCI datasets. It appears, that our Multithreshold Entropy Linear Classifier (MELC) obtaines similar or higher scores than the ones given by SVM on both synthetic and real data. We show how proposed approach can be benefitial for the cheminformatics in the task of ligands activity prediction, where despite better classification results, MELC gives some additional insight into the data structure (classes of underrepresented chemical compunds). version:1
arxiv-1408-0680 | A Pattern Recognition System for Detecting Use of Mobile Phones While Driving | http://arxiv.org/abs/1408.0680 | id:1408.0680 author:Rafael A. Berri, Alexandre G. Silva, Rafael S. Parpinelli, Elaine Girardi, Rangel Arthur category:cs.CV  published:2014-08-04 summary:It is estimated that 80% of crashes and 65% of near collisions involved drivers inattentive to traffic for three seconds before the event. This paper develops an algorithm for extracting characteristics allowing the cell phones identification used during driving a vehicle. Experiments were performed on sets of images with 100 positive images (with phone) and the other 100 negative images (no phone), containing frontal images of the driver. Support Vector Machine (SVM) with Polynomial kernel is the most advantageous classification system to the features provided by the algorithm, obtaining a success rate of 91.57% for the vision system. Tests done on videos show that it is possible to use the image datasets for training classifiers in real situations. Periods of 3 seconds were correctly classified at 87.43% of cases. version:1
arxiv-1408-0614 | A Multi-Stage Supply Chain Network Optimization Using Genetic Algorithms | http://arxiv.org/abs/1408.0614 | id:1408.0614 author:Nelson Christopher Dzupire, Yaw Nkansah-Gyekye category:math.OC cs.NE  published:2014-08-04 summary:In today's global business market place, individual firms no longer compete as independent entities with unique brand names but as integral part of supply chain links. Key to success of any business is satisfying customer's demands on time which may result in cost reductions and increase in service level. In supply chain networks decisions are made with uncertainty about product's demands, costs, prices, lead times, quality in a competitive and collaborative environment. If poor decisions are made, they may lead to excess inventories that are costly or to insufficient inventory that cannot meet customer's demands. In this work we developed a bi-objective model that minimizes system wide costs of the supply chain and delays on delivery of products to distribution centers for a three echelon supply chain. Picking a set of Pareto front for multi-objective optimization problems require robust and efficient methods that can search an entire space. We used evolutionary algorithms to find the set of Pareto fronts which have proved to be effective in finding the entire set of Pareto fronts. version:1
arxiv-1405-4930 | Adapted Approach for Fruit Disease Identification using Images | http://arxiv.org/abs/1405.4930 | id:1405.4930 author:Shiv Ram Dubey, Anand Singh Jalal category:cs.CV  published:2014-05-20 summary:Diseases in fruit cause devastating problem in economic losses and production in agricultural industry worldwide. In this paper, an adaptive approach for the identification of fruit diseases is proposed and experimentally validated. The image processing based proposed approach is composed of the following main steps; in the first step K-Means clustering technique is used for the defect segmentation, in the second step some state of the art features are extracted from the segmented image, and finally images are classified into one of the classes by using a Multi-class Support Vector Machine. We have considered diseases of apple as a test case and evaluated our approach for three types of apple diseases namely apple scab, apple blotch and apple rot. Our experimental results express that the proposed solution can significantly support accurate detection and automatic identification of fruit diseases. The classification accuracy for the proposed solution is achieved up to 93%. version:5
arxiv-1309-5676 | Implementation of a language driven Backpropagation algorithm | http://arxiv.org/abs/1309.5676 | id:1309.5676 author:I. V. Grossu, C. I. Ciuluvica category:cs.NE  published:2013-09-23 summary:Inspired by the importance of both communication and feedback on errors in human learning, our main goal was to implement a similar mechanism in supervised learning of artificial neural networks. The starting point in our study was the observation that words should accompany the input vectors included in the training set, thus extending the ANN input space. This had as consequence the necessity to take into consideration a modified sigmoid activation function for neurons in the first hidden layer (in agreement with a specific MLP apartment structure), and also a modified version of the Backpropagation algorithm, which allows using of unspecified (null) desired output components. Following the belief that basic concepts should be tested on simple examples, the previous mentioned mechanism was applied on both the XOR problem and a didactic color case study. In this context, we noticed the interesting fact that the ANN was capable to categorize all desired input vectors in the absence of their corresponding words, even though the training set included only word accompanied inputs, in both positive and negative examples. Further analysis along applying this approach to more complex scenarios is currently in progress, as we consider the proposed language-driven algorithm might contribute to a better understanding of learning in humans, opening as well the possibility to create a specific category of artificial neural networks, with abstraction capabilities. version:3
arxiv-1405-6664 | On the Computational Intractability of Exact and Approximate Dictionary Learning | http://arxiv.org/abs/1405.6664 | id:1405.6664 author:Andreas M. Tillmann category:cs.IT cs.LG math.IT  published:2014-05-26 summary:The efficient sparse coding and reconstruction of signal vectors via linear observations has received a tremendous amount of attention over the last decade. In this context, the automated learning of a suitable basis or overcomplete dictionary from training data sets of certain signal classes for use in sparse representations has turned out to be of particular importance regarding practical signal processing applications. Most popular dictionary learning algorithms involve NP-hard sparse recovery problems in each iteration, which may give some indication about the complexity of dictionary learning but does not constitute an actual proof of computational intractability. In this technical note, we show that learning a dictionary with which a given set of training signals can be represented as sparsely as possible is indeed NP-hard. Moreover, we also establish hardness of approximating the solution to within large factors of the optimal sparsity level. Furthermore, we give NP-hardness and non-approximability results for a recent dictionary learning variation called the sensor permutation problem. Along the way, we also obtain a new non-approximability result for the classical sparse recovery problem from compressed sensing. version:2
arxiv-1408-0453 | Adaptive Wavelet Based Identification and Extraction of PQRST Combination in Randomly Stretching ECG Sequence | http://arxiv.org/abs/1408.0453 | id:1408.0453 author:T. R. Gopalakrishnan Nair, A. P. Geetha, M. Asharani category:cs.CV  published:2014-08-03 summary:Cardiovascular system study using ECG signals have evolved tremendously in the domain of electronics and signal processing. However, there are certain floating challenges unresolved in the analysis and detection of abnormal performances of cardiovascular system. As the medical field is moving towards more automated and intelligent systems, wrong detection or wrong interpretations of ECG waveform of abnormal conditions can be quite fatal. Since the PQRST signals vary their positions randomly, the process of locating, identifying and classifying each feature can be cumbersome and it is prone to errors. Here we present an automated scheme using adaptive wavelet to detect prominent R-peak with extreme accuracy and algorithmically tag and mark the coexisting peaks P, Q, S, and T with almost same accuracy. The adaptive wavelet approach used in this scheme is capable of detecting R-peak in ECG with 99.99% accuracy along with the rest of the waveforms. version:1
arxiv-1408-0452 | Methodology For Detection of QRS Pattern Using Secondary Wavelets | http://arxiv.org/abs/1408.0452 | id:1408.0452 author:T. R. Gopalakrishnan Nair, A. P. Geetha, Asharani category:cs.CV  published:2014-08-03 summary:Applications of wavelet transform to the field of health care signals have paved the way for implementing revolutionary approaches in detecting the presence of certain abnormalities in human health patterns. There were extensive studies carried out using primary wavelets in various signals like Electrocardiogram (ECG), sonogram etc. with a certain amount of success. On the other hand analysis using secondary wavelets which inherits the characteristics of a set of variations available in signals like ECG can be a promise to detect diseases with ease. Here a method to create a generalized adapted wavelet is presented which contains the information of QRS pattern collected from an anomaly sample space. The method has been tested and found to be successful in locating the position of R peak in noise embedded ECG signal. version:1
arxiv-1408-0337 | A Bayesian estimation approach to analyze non-Gaussian data-generating processes with latent classes | http://arxiv.org/abs/1408.0337 | id:1408.0337 author:Naoki Tanaka, Shohei Shimizu, Takashi Washio category:stat.ML  published:2014-08-02 summary:A large amount of observational data has been accumulated in various fields in recent times, and there is a growing need to estimate the generating processes of these data. A linear non-Gaussian acyclic model (LiNGAM) based on the non-Gaussianity of external influences has been proposed to estimate the data-generating processes of variables. However, the results of the estimation can be biased if there are latent classes. In this paper, we first review LiNGAM, its extended model, as well as the estimation procedure for LiNGAM in a Bayesian framework. We then propose a new Bayesian estimation procedure that solves the problem. version:1
arxiv-1408-0325 | Matrix Factorization with Explicit Trust and Distrust Relationships | http://arxiv.org/abs/1408.0325 | id:1408.0325 author:Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, Mohamed Sarwat category:cs.SI cs.IR cs.LG  published:2014-08-02 summary:With the advent of online social networks, recommender systems have became crucial for the success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general recommender systems suffer from the data sparsity and the cold-start problems. To alleviate these issues, in recent years there has been an upsurge of interest in exploiting social information such as trust relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in recommendation process stems from the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, the distrust relations also exist between users. For instance, in Epinions the concepts of personal "web of trust" and personal "block list" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. In this paper, we propose a matrix factorization based model for recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and the cold-start users issues. Through experiments on the Epinions data set, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information can have on recommender systems. version:1
arxiv-1012-0866 | Generalized Species Sampling Priors with Latent Beta reinforcements | http://arxiv.org/abs/1012.0866 | id:1012.0866 author:Edoardo M. Airoldi, Thiago Costa, Federico Bassetti, Fabrizio Leisen, Michele Guindani category:math.ST cs.LG stat.ME stat.TH  published:2010-12-04 summary:Many popular Bayesian nonparametric priors can be characterized in terms of exchangeable species sampling sequences. However, in some applications, exchangeability may not be appropriate. We introduce a {novel and probabilistically coherent family of non-exchangeable species sampling sequences characterized by a tractable predictive probability function with weights driven by a sequence of independent Beta random variables. We compare their theoretical clustering properties with those of the Dirichlet Process and the two parameters Poisson-Dirichlet process. The proposed construction provides a complete characterization of the joint process, differently from existing work. We then propose the use of such process as prior distribution in a hierarchical Bayes modeling framework, and we describe a Markov Chain Monte Carlo sampler for posterior inference. We evaluate the performance of the prior and the robustness of the resulting inference in a simulation study, providing a comparison with popular Dirichlet Processes mixtures and Hidden Markov Models. Finally, we develop an application to the detection of chromosomal aberrations in breast cancer by leveraging array CGH data. version:4
arxiv-1408-0204 | Functional Principal Component Analysis and Randomized Sparse Clustering Algorithm for Medical Image Analysis | http://arxiv.org/abs/1408.0204 | id:1408.0204 author:Nan Lin, Junhai Jiang, Shicheng Guo, Momiao Xiong category:stat.ML cs.AI cs.CV cs.LG  published:2014-08-01 summary:Due to advances in sensors, growing large and complex medical image data have the ability to visualize the pathological change in the cellular or even the molecular level or anatomical changes in tissues and organs. As a consequence, the medical images have the potential to enhance diagnosis of disease, prediction of clinical outcomes, characterization of disease progression, management of health care and development of treatments, but also pose great methodological and computational challenges for representation and selection of features in image cluster analysis. To address these challenges, we first extend one dimensional functional principal component analysis to the two dimensional functional principle component analyses (2DFPCA) to fully capture space variation of image signals. Image signals contain a large number of redundant and irrelevant features which provide no additional or no useful information for cluster analysis. Widely used methods for removing redundant and irrelevant features are sparse clustering algorithms using a lasso-type penalty to select the features. However, the accuracy of clustering using a lasso-type penalty depends on how to select penalty parameters and a threshold for selecting features. In practice, they are difficult to determine. Recently, randomized algorithms have received a great deal of attention in big data analysis. This paper presents a randomized algorithm for accurate feature selection in image cluster analysis. The proposed method is applied to ovarian and kidney cancer histology image data from the TCGA database. The results demonstrate that the randomized feature selection method coupled with functional principal component analysis substantially outperforms the current sparse clustering algorithms in image cluster analysis. version:1
arxiv-1408-0193 | A RobustICA Based Algorithm for Blind Separation of Convolutive Mixtures | http://arxiv.org/abs/1408.0193 | id:1408.0193 author:Zaid Albataineh, Fathi M. Salem category:cs.LG cs.SD  published:2014-08-01 summary:We propose a frequency domain method based on robust independent component analysis (RICA) to address the multichannel Blind Source Separation (BSS) problem of convolutive speech mixtures in highly reverberant environments. We impose regularization processes to tackle the ill-conditioning problem of the covariance matrix and to mitigate the performance degradation in the frequency domain. We apply an algorithm to separate the source signals in adverse conditions, i.e. high reverberation conditions when short observation signals are available. Furthermore, we study the impact of several parameters on the performance of separation, e.g. overlapping ratio and window type of the frequency domain method. We also compare different techniques to solve the frequency-domain permutation ambiguity. Through simulations and real world experiments, we verify the superiority of the presented convolutive algorithm among other BSS algorithms, including recursive regularized ICA (RR ICA), independent vector analysis (IVA). version:1
arxiv-1407-7722 | OpenML: networked science in machine learning | http://arxiv.org/abs/1407.7722 | id:1407.7722 author:Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, Luis Torgo category:cs.LG cs.CY  published:2014-07-29 summary:Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners. version:2
arxiv-1408-0102 | Randomized Memetic Artificial Bee Colony Algorithm | http://arxiv.org/abs/1408.0102 | id:1408.0102 author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE  published:2014-08-01 summary:Artificial Bee Colony (ABC) optimization algorithm is one of the recent population based probabilistic approach developed for global optimization. ABC is simple and has been showed significant improvement over other Nature Inspired Algorithms (NIAs) when tested over some standard benchmark functions and for some complex real world optimization problems. Memetic Algorithms also become one of the key methodologies to solve the very large and complex real-world optimization problems. The solution search equation of Memetic ABC is based on Golden Section Search and an arbitrary value which tries to balance exploration and exploitation of search space. But still there are some chances to skip the exact solution due to its step size. In order to balance between diversification and intensification capability of the Memetic ABC, it is randomized the step size in Memetic ABC. The proposed algorithm is named as Randomized Memetic ABC (RMABC). In RMABC, new solutions are generated nearby the best so far solution and it helps to increase the exploitation capability of Memetic ABC. The experiments on some test problems of different complexities and one well known engineering optimization application show that the proposed algorithm outperforms over Memetic ABC (MeABC) and some other variant of ABC algorithm(like Gbest guided ABC (GABC),Hooke Jeeves ABC (HJABC), Best-So-Far ABC (BSFABC) and Modified ABC (MABC) in case of almost all the problems. version:1
arxiv-1408-0101 | Memetic Search in Differential Evolution Algorithm | http://arxiv.org/abs/1408.0101 | id:1408.0101 author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE  published:2014-08-01 summary:Differential Evolution (DE) is a renowned optimization stratagem that can easily solve nonlinear and comprehensive problems. DE is a well known and uncomplicated population based probabilistic approach for comprehensive optimization. It has apparently outperformed a number of Evolutionary Algorithms and further search heuristics in the vein of Particle Swarm Optimization at what time of testing over both yardstick and actual world problems. Nevertheless, DE, like other probabilistic optimization algorithms, from time to time exhibits precipitate convergence and stagnates at suboptimal position. In order to stay away from stagnation behavior while maintaining an excellent convergence speed, an innovative search strategy is introduced, named memetic search in DE. In the planned strategy, positions update equation customized as per a memetic search stratagem. In this strategy a better solution participates more times in the position modernize procedure. The position update equation is inspired from the memetic search in artificial bee colony algorithm. The proposed strategy is named as Memetic Search in Differential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it is tested over 8 benchmark optimization problems and three real world optimization problems. A comparative analysis has also been carried out among proposed MSDE and original DE. Results show that the anticipated algorithm go one better than the basic DE and its recent deviations in a good number of the experiments. version:1
arxiv-1408-0096 | Conditional Restricted Boltzmann Machines for Cold Start Recommendations | http://arxiv.org/abs/1408.0096 | id:1408.0096 author:Jiankou Li, Wei Zhang category:cs.IR cs.LG stat.ML  published:2014-08-01 summary:Restricted Boltzman Machines (RBMs) have been successfully used in recommender systems. However, as with most of other collaborative filtering techniques, it cannot solve cold start problems for there is no rating for a new item. In this paper, we first apply conditional RBM (CRBM) which could take extra information into account and show that CRBM could solve cold start problem very well, especially for rating prediction task. CRBM naturally combine the content and collaborative data under a single framework which could be fitted effectively. Experiments show that CRBM can be compared favourably with matrix factorization models, while hidden features learned from the former models are more easy to be interpreted. version:1
arxiv-1211-5037 | Bayesian nonparametric Plackett-Luce models for the analysis of preferences for college degree programmes | http://arxiv.org/abs/1211.5037 | id:1211.5037 author:François Caron, Yee Whye Teh, Thomas Brendan Murphy category:stat.ML cs.LG stat.ME  published:2012-11-21 summary:In this paper we propose a Bayesian nonparametric model for clustering partial ranking data. We start by developing a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a completely random measure. We characterise the posterior distribution given data, and derive a simple and effective Gibbs sampler for posterior simulation. We then develop a Dirichlet process mixture extension of our model and apply it to investigate the clustering of preferences for college degree programmes amongst Irish secondary school graduates. The existence of clusters of applicants who have similar preferences for degree programmes is established and we determine that subject matter and geographical location of the third level institution characterise these clusters. version:3
arxiv-1408-0058 | A Framework for learning multi-agent dynamic formation strategy in real-time applications | http://arxiv.org/abs/1408.0058 | id:1408.0058 author:Mehrab Norouzitallab, Valiallah Monajjemi, Saeed Shiry Ghidary, Mohammad Bagher Menhaj category:cs.RO cs.LG cs.MA  published:2014-08-01 summary:Formation strategy is one of the most important parts of many multi-agent systems with many applications in real world problems. In this paper, a framework for learning this task in a limited domain (restricted environment) is proposed. In this framework, agents learn either directly by observing an expert behavior or indirectly by observing other agents or objects behavior. First, a group of algorithms for learning formation strategy based on limited features will be presented. Due to distributed and complex nature of many multi-agent systems, it is impossible to include all features directly in the learning process; thus, a modular scheme is proposed in order to reduce the number of features. In this method, some important features have indirect influence in learning instead of directly involving them as input features. This framework has the ability to dynamically assign a group of positions to a group of agents to improve system performance. In addition, it can change the formation strategy when the context changes. Finally, this framework is able to automatically produce many complex and flexible formation strategy algorithms without directly involving an expert to present and implement such complex algorithms. version:1
arxiv-1408-0055 | Thurstonian Boltzmann Machines: Learning from Multiple Inequalities | http://arxiv.org/abs/1408.0055 | id:1408.0055 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG stat.ME  published:2014-08-01 summary:We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis. version:1
arxiv-1408-0047 | Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data Analysis | http://arxiv.org/abs/1408.0047 | id:1408.0047 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG stat.AP stat.ME  published:2014-07-31 summary:Ordinal data is omnipresent in almost all multiuser-generated feedback - questionnaires, preferences etc. This paper investigates modelling of ordinal data with Gaussian restricted Boltzmann machines (RBMs). In particular, we present the model architecture, learning and inference procedures for both vector-variate and matrix-variate ordinal data. We show that our model is able to capture latent opinion profile of citizens around the world, and is competitive against state-of-art collaborative filtering techniques on large-scale public datasets. The model thus has the potential to extend application of RBMs to diverse domains such as recommendation systems, product reviews and expert assessments. version:1
arxiv-1408-0043 | Learning From Ordered Sets and Applications in Collaborative Ranking | http://arxiv.org/abs/1408.0043 | id:1408.0043 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.LG cs.IR stat.ML  published:2014-07-31 summary:Ranking over sets arise when users choose between groups of items. For example, a group may be of those movies deemed $5$ stars to them, or a customized tour package. It turns out, to model this data type properly, we need to investigate the general combinatorics problem of partitioning a set and ordering the subsets. Here we construct a probabilistic log-linear model over a set of ordered subsets. Inference in this combinatorial space is highly challenging: The space size approaches $(N!/2)6.93145^{N+1}$ as $N$ approaches infinity. We propose a \texttt{split-and-merge} Metropolis-Hastings procedure that can explore the state-space efficiently. For discovering hidden aspects in the data, we enrich the model with latent binary variables so that the posteriors can be efficiently evaluated. Finally, we evaluate the proposed model on large-scale collaborative filtering tasks and demonstrate that it is competitive against state-of-the-art methods. version:1
arxiv-1408-0017 | Learning Nash Equilibria in Congestion Games | http://arxiv.org/abs/1408.0017 | id:1408.0017 author:Walid Krichene, Benjamin Drighès, Alexandre M. Bayen category:cs.LG cs.GT  published:2014-07-31 summary:We study the repeated congestion game, in which multiple populations of players share resources, and make, at each iteration, a decentralized decision on which resources to utilize. We investigate the following question: given a model of how individual players update their strategies, does the resulting dynamics of strategy profiles converge to the set of Nash equilibria of the one-shot game? We consider in particular a model in which players update their strategies using algorithms with sublinear discounted regret. We show that the resulting sequence of strategy profiles converges to the set of Nash equilibria in the sense of Ces\`aro means. However, strong convergence is not guaranteed in general. We show that strong convergence can be guaranteed for a class of algorithms with a vanishing upper bound on discounted regret, and which satisfy an additional condition. We call such algorithms AREP algorithms, for Approximate REPlicator, as they can be interpreted as a discrete-time approximation of the replicator equation, which models the continuous-time evolution of population strategies, and which is known to converge for the class of congestion games. In particular, we show that the discounted Hedge algorithm belongs to the AREP class, which guarantees its strong convergence. version:1
arxiv-1407-8497 | A Bottom-Up Approach for Automatic Pancreas Segmentation in Abdominal CT Scans | http://arxiv.org/abs/1407.8497 | id:1407.8497 author:Amal Farag, Le Lu, Evrim Turkbey, Jiamin Liu, Ronald M. Summers category:cs.CV  published:2014-07-31 summary:Organ segmentation is a prerequisite for a computer-aided diagnosis (CAD) system to detect pathologies and perform quantitative analysis. For anatomically high-variability abdominal organs such as the pancreas, previous segmentation works report low accuracies when comparing to organs like the heart or liver. In this paper, a fully-automated bottom-up method is presented for pancreas segmentation, using abdominal computed tomography (CT) scans. The method is based on a hierarchical two-tiered information propagation by classifying image patches. It labels superpixels as pancreas or not via pooling patch-level confidences on 2D CT slices over-segmented by the Simple Linear Iterative Clustering approach. A supervised random forest (RF) classifier is trained on the patch level and a two-level cascade of RFs is applied at the superpixel level, coupled with multi-channel feature extraction, respectively. On six-fold cross-validation using 80 patient CT volumes, we achieved 68.8% Dice coefficient and 57.2% Jaccard Index, comparable to or slightly better than published state-of-the-art methods. version:1
