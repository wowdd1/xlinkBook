arxiv-1608-02052 | Multi-Model Hypothesize-and-Verify Approach for Incremental Loop Closure Verification | http://arxiv.org/abs/1608.02052 | id:1608.02052 author:Kanji Tanaka category:cs.CV  published:2016-08-06 summary:Loop closure detection, which is the task of identifying locations revisited by a robot in a sequence of odometry and perceptual observations, is typically formulated as a visual place recognition (VPR) task. However, even state-of-the-art VPR techniques generate a considerable number of false positives as a result of confusing visual features and perceptual aliasing. In this paper, we propose a robust incremental framework for loop closure detection, termed incremental loop closure verification. Our approach reformulates the problem of loop closure detection as an instance of a multi-model hypothesize-and-verify framework, in which multiple loop closure hypotheses are generated and verified in terms of the consistency between loop closure hypotheses and VPR constraints at multiple viewpoints along the robot's trajectory. Furthermore, we consider the general incremental setting of loop closure detection, in which the system must update both the set of VPR constraints and that of loop closure hypotheses when new constraints or hypotheses arrive during robot navigation. Experimental results using a stereo SLAM system and DCNN features and visual odometry validate effectiveness of the proposed approach. version:1
arxiv-1608-02051 | Compressive Change Retrieval for Moving Object Detection | http://arxiv.org/abs/1608.02051 | id:1608.02051 author:Tomoya Murase, Kanji Tanaka category:cs.CV  published:2016-08-06 summary:Change detection, or anomaly detection, from street-view images acquired by an autonomous robot at multiple different times, is a major problem in robotic mapping and autonomous driving. Formulation as an image comparison task, which operates on a given pair of query and reference images is common to many existing approaches to this problem. Unfortunately, providing relevant reference images is not straightforward. In this paper, we propose a novel formulation for change detection, termed compressive change retrieval, which can operate on a query image and similar reference images retrieved from the web. Compared to previous formulations, there are two sources of difficulty. First, the retrieved reference images may frequently contain non-relevant reference images, because even state-of-the-art place-recognition techniques suffer from retrieval noise. Second, image comparison needs to be conducted in a compressed domain to minimize the storage cost of large collections of street-view images. To address the above issues, we also present a practical change detection algorithm that uses compressed bag-of-words (BoW) image representation as a scalable solution. The results of experiments conducted on a practical change detection task, "moving object detection (MOD)," using the publicly available Malaga dataset validate the effectiveness of the proposed approach. version:1
arxiv-1608-02026 | Photometric Bundle Adjustment for Vision-Based SLAM | http://arxiv.org/abs/1608.02026 | id:1608.02026 author:Hatem Alismail, Brett Browning, Simon Lucey category:cs.CV cs.RO  published:2016-08-05 summary:We propose a novel algorithm for the joint refinement of structure and motion parameters from image data directly without relying on fixed and known correspondences. In contrast to traditional bundle adjustment (BA) where the optimal parameters are determined by minimizing the reprojection error using tracked features, the proposed algorithm relies on maximizing the photometric consistency and estimates the correspondences implicitly. Since the proposed algorithm does not require correspondences, its application is not limited to corner-like structure; any pixel with nonvanishing gradient could be used in the estimation process. Furthermore, we demonstrate the feasibility of refining the motion and structure parameters simultaneously using the photometric in unconstrained scenes and without requiring restrictive assumptions such as planarity. The proposed algorithm is evaluated on range of challenging outdoor datasets, and it is shown to improve upon the accuracy of the state-of-the-art VSLAM methods obtained using the minimization of the reprojection error using traditional BA as well as loop closure. version:1
arxiv-1608-02025 | Boundary-based MWE segmentation with text partitioning | http://arxiv.org/abs/1608.02025 | id:1608.02025 author:Jake Ryland Williams category:cs.CL  published:2016-08-05 summary:In this article we present a novel algorithm for the task of comprehensively segmenting texts into MWEs. With the basis for this algorithm (referred to as text partitioning) being recently developed, these results constitute its first performance-evaluated application to a natural language processing task. A differentiating feature of this single-parameter model is its focus on gap (i.e., punctuation) crossings as features for MWE identification, which uses substantially more information in training than is present in dictionaries. We show that this algorithm is capable of achieving high levels of precision and recall, using only type-level information, and then extend it to include part-of-speech tags to increase its performance to state-of-the-art levels, despite a simple decision criterion and general feature space (which makes the method directly applicable to other languages). Since the existence of comprehensive MWE annotations are what drive this segmentation algorithm, these results support their continued production. In addition, we have updated and extended the strength-averaging evaluation scheme, allowing for a more accurate and fine-grained understanding of model performance, and leading us to affirm the differences in nature and identifiability between weakly- and strongly-linked MWEs, quantitatively. version:1
arxiv-1608-02023 | Identifying Designs from Incomplete, Fragmented Cultural Heritage Objects by Curve-Pattern Matching | http://arxiv.org/abs/1608.02023 | id:1608.02023 author:Jun Zhou, Haozhou Yu, Karen Smith, Colin Wilder, Hongkai Yu, Song Wang category:cs.CV  published:2016-08-05 summary:Study of cultural-heritage objects with embellished realistic and abstract designs made up of connected and intertwined curves crosscuts a number of related disciplines, including archaeology, art history, and heritage management. However, many objects, such as pottery sherds found in the archaeological record, are fragmentary, making the underlying complete designs unknowable at the scale of the sherd fragment. The challenge to reconstruct and study complete designs is stymied because 1) most fragmentary cultural-heritage objects contain only a small portion of the underlying full design, 2) in the case of a stamping application, the same design may be applied multiple times with spatial overlap on one object, and 3) curve patterns detected on an object are usually incomplete and noisy. As a result, classical curve-pattern matching algorithms, such as Chamfer matching, may perform poorly in identifying the underlying design. In this paper, we develop a new partial-to-global curve matching algorithm to address these challenges and better identify the full design from a fragmented cultural heritage object. Specifically, we develop the algorithm to identify the designs of the carved wooden paddles of the Southeastern Woodlands from unearthed pottery sherds. A set of pottery sherds from the Snow Collection, curated at Georgia Southern University, are used to test the proposed algorithm, with promising results. version:1
arxiv-1608-03465 | The Future of Data Analysis in the Neurosciences | http://arxiv.org/abs/1608.03465 | id:1608.03465 author:Danilo Bzdok, B. T. Thomas Yeo category:q-bio.NC stat.ML  published:2016-08-05 summary:Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative big-sample datasets on microanatomy, synaptic connections, optogenetic brain-behavior assays, and high-level cognition. While growing data availability and information granularity have been amply discussed, we direct attention to a routinely neglected question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more central to distill neurobiological knowledge from healthy and pathological brain recordings. We believe that large-scale data analysis will use more models that are non-parametric, generative, mixing frequentist and Bayesian aspects, and grounded in different statistical inferences. version:1
arxiv-1608-02010 | Communication-Efficient Parallel Block Minimization for Kernel Machines | http://arxiv.org/abs/1608.02010 | id:1608.02010 author:Cho-Jui Hsieh, Si Si, Inderjit S. Dhillon category:cs.LG  published:2016-08-05 summary:Kernel machines often yield superior predictive performance on various tasks; however, they suffer from severe computational challenges. In this paper, we show how to overcome the important challenge of speeding up kernel machines. In particular, we develop a parallel block minimization framework for solving kernel machines, including kernel SVM and kernel logistic regression. Our framework proceeds by dividing the problem into smaller subproblems by forming a block-diagonal approximation of the Hessian matrix. The subproblems are then solved approximately in parallel. After that, a communication efficient line search procedure is developed to ensure sufficient reduction of the objective function value at each iteration. We prove global linear convergence rate of the proposed method with a wide class of subproblem solvers, and our analysis covers strongly convex and some non-strongly convex functions. We apply our algorithm to solve large-scale kernel SVM problems on distributed systems, and show a significant improvement over existing parallel solvers. As an example, on the covtype dataset with half-a-million samples, our algorithm can obtain an approximate solution with 96% accuracy in 20 seconds using 32 machines, while all the other parallel kernel SVM solvers require more than 2000 seconds to achieve a solution with 95% accuracy. Moreover, our algorithm can scale to very large data sets, such as the kdd algebra dataset with 8 million samples and 20 million features. version:1
arxiv-1608-01976 | Kernel Ridge Regression via Partitioning | http://arxiv.org/abs/1608.01976 | id:1608.01976 author:Rashish Tandon, Si Si, Pradeep Ravikumar, Inderjit Dhillon category:stat.ML cs.LG  published:2016-08-05 summary:In this paper, we investigate a divide and conquer approach to Kernel Ridge Regression (KRR). Given n samples, the division step involves separating the points based on some underlying disjoint partition of the input space (possibly via clustering), and then computing a KRR estimate for each partition. The conquering step is simple: for each partition, we only consider its own local estimate for prediction. We establish conditions under which we can give generalization bounds for this estimator, as well as achieve optimal minimax rates. We also show that the approximation error component of the generalization error is lesser than when a single KRR estimate is fit on the data: thus providing both statistical and computational advantages over a single KRR estimate over the entire data (or an averaging over random partitions as in other recent work, [30]). Lastly, we provide experimental validation for our proposed estimator and our assumptions. version:1
arxiv-1608-01972 | Bridging the Gap: a Semantic Similarity Measure between Queries and Documents | http://arxiv.org/abs/1608.01972 | id:1608.01972 author:Sun Kim, W. John Wilbur, Zhiyong Lu category:cs.CL cs.IR  published:2016-08-05 summary:The main approach of traditional information retrieval (IR) is to examine how many words from a query appear in a document. A drawback of this approach, however, is that it may fail to detect relevant documents where no or only few words from a query are found. The semantic analysis methods such as LSA (latent semantic analysis) and LDA (latent Dirichlet allocation) have been proposed to address the issue, but their performance is not superior compared to common IR approaches. Here we present a query-document similarity measure motivated by the Word Mover's Distance. Unlike other similarity measures, the proposed method relies on neural word embeddings to calculate the distance between words. Our method is efficient and straightforward to implement. The experimental results on TREC and PubMed show that our approach provides significantly better performance than BM25. We also discuss the pros and cons of our approach and show that there is a synergy effect when the word embedding measure is combined with the BM25 function. version:1
arxiv-1608-01966 | OpenCL-accelerated object classification in video streams using Spatial Pooler of Hierarchical Temporal Memory | http://arxiv.org/abs/1608.01966 | id:1608.01966 author:Maciej Wielgosz, Marcin Pietroń category:cs.CV  published:2016-08-05 summary:We present a method to classify objects in video streams using a brain-inspired Hierarchical Temporal Memory (HTM) algorithm. Object classification is a challenging task where humans still significantly outperform machine learning algorithms due to their unique capabilities. We have implemented a system which achieves very promising performance in terms of recognition accuracy. Unfortunately, conducting more advanced experiments is very computationally demanding; some of the trials run on a standard CPU may take as long as several days for 960x540 video streams frames. Therefore we have decided to accelerate selected parts of the system using OpenCL. In particular, we seek to determine to what extent porting selected and computationally demanding parts of a core may speed up calculations. The classification accuracy of the system was examined through a series of experiments and the performance was given in terms of F1 score as a function of the number of columns, synapses, $min\_overlap$ and $winners\_set\_size$. The system achieves the highest F1 score of 0.95 and 0.91 for $min\_overlap=4$ and 256 synapses, respectively. We have also conduced a series of experiments with different hardware setups and measured CPU/GPU acceleration. The best kernel speed-up of 632x and 207x was reached for 256 synapses and 1024 columns. However, overall acceleration including transfer time was significantly lower and amounted to 6.5x and 3.2x for the same setup. version:1
arxiv-1608-01993 | Enhanced Directional Smoothing Algorithm for Edge-Preserving Smoothing of Synthetic-Aperture Radar Images | http://arxiv.org/abs/1608.01993 | id:1608.01993 author:Mario Mastriani, Alberto E. Giraldez category:cs.CV  published:2016-08-05 summary:Synthetic aperture radar (SAR) images are subject to prominent speckle noise, which is generally considered a purely multiplicative noise process. In theory, this multiplicative noise is that the ratio of the standard deviation to the signal value, the "coefficient of variation," is theoretically constant at every point in a SAR image. Most of the filters for speckle reduction are based on this property. Such property is irrelevant for the new filter structure, which is based on directional smoothing (DS) theory, the enhanced directional smoothing (EDS) that removes speckle noise from SAR images without blurring edges. We demonstrate the effectiveness of this new filtering method by comparing it to established speckle noise removal techniques on SAR images. version:1
arxiv-1608-01961 | De-Conflated Semantic Representations | http://arxiv.org/abs/1608.01961 | id:1608.01961 author:Mohammad Taher Pilehvar, Nigel Collier category:cs.CL cs.AI  published:2016-08-05 summary:One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them. version:1
arxiv-1608-01910 | Resolving Out-of-Vocabulary Words with Bilingual Embeddings in Machine Translation | http://arxiv.org/abs/1608.01910 | id:1608.01910 author:Pranava Swaroop Madhyastha, Cristina España-Bonet category:cs.CL  published:2016-08-05 summary:Out-of-vocabulary words account for a large proportion of errors in machine translation systems, especially when the system is used on a different domain than the one where it was trained. In order to alleviate the problem, we propose to use a log-bilinear softmax-based model for vocabulary expansion, such that given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language. Our model uses only word embeddings trained on significantly large unlabelled monolingual corpora and trains over a fairly small, word-to-word bilingual dictionary. We input this probabilistic list into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English-Spanish language pair. Especially, we get an improvement of 3.9 BLEU points when tested over an out-of-domain test set. version:1
arxiv-1608-01896 | Blind Deconvolution of PET Images using Anatomical Priors | http://arxiv.org/abs/1608.01896 | id:1608.01896 author:Stéphanie Guérit, Adriana González, Anne Bol, John A. Lee, Laurent Jacques category:cs.CV math.OC  published:2016-08-05 summary:Images from positron emission tomography (PET) provide metabolic information about the human body. They present, however, a spatial resolution that is limited by physical and instrumental factors often modeled by a blurring function. Since this function is typically unknown, blind deconvolution (BD) techniques are needed in order to produce a useful restored PET image. In this work, we propose a general BD technique that restores a low resolution blurry image using information from data acquired with a high resolution modality (e.g., CT-based delineation of regions with uniform activity in PET images). The proposed BD method is validated on synthetic and actual phantoms. version:1
arxiv-1608-01884 | Winograd Schemas and Machine Translation | http://arxiv.org/abs/1608.01884 | id:1608.01884 author:Ernest Davis category:cs.AI cs.CL  published:2016-08-05 summary:A Winograd schema is a pair of sentences that differ in a single word and that contain an ambiguous pronoun whose referent is different in the two sentences and requires the use of commonsense knowledge or world knowledge to disambiguate. This paper discusses how Winograd schemas and other sentence pairs could be used as challenges for machine translation using distinctions between pronouns, such as gender, that appear in the target language but not in the source. version:1
arxiv-1608-01874 | Forward Stagewise Additive Model for Collaborative Multiview Boosting | http://arxiv.org/abs/1608.01874 | id:1608.01874 author:Avisek Lahiri, Biswajit Paria, Prabir Kumar Biswas category:cs.LG  published:2016-08-05 summary:Multiview assisted learning has gained significant attention in recent years in supervised learning genre. Availability of high performance computing devices enables learning algorithms to search simultaneously over multiple views or feature spaces to obtain an optimum classification performance. The paper is a pioneering attempt of formulating a mathematical foundation for realizing a multiview aided collaborative boosting architecture for multiclass classification. Most of the present algorithms apply multiview learning heuristically without exploring the fundamental mathematical changes imposed on traditional boosting. Also, most of the algorithms are restricted to two class or view setting. Our proposed mathematical framework enables collaborative boosting across any finite dimensional view spaces for multiclass learning. The boosting framework is based on forward stagewise additive model which minimizes a novel exponential loss function. We show that the exponential loss function essentially captures difficulty of a training sample space instead of the traditional `1/0' loss. The new algorithm restricts a weak view from over learning and thereby preventing overfitting. The model is inspired by our earlier attempt on collaborative boosting which was devoid of mathematical justification. The proposed algorithm is shown to converge much nearer to global minimum in the exponential loss space and thus supersedes our previous algorithm. The paper also presents analytical and numerical analysis of convergence and margin bounds for multiview boosting algorithms and we show that our proposed ensemble learning manifests lower error bound and higher margin compared to our previous model. Also, the proposed model is compared with traditional boosting and recent multiview boosting algorithms. version:1
arxiv-1608-01866 | Fusing Deep Convolutional Networks for Large Scale Visual Concept Classification | http://arxiv.org/abs/1608.01866 | id:1608.01866 author:Hilal Ergun, Mustafa Sert category:cs.CV  published:2016-08-05 summary:Deep learning architectures are showing great promise in various computer vision domains including image classification, object detection, event detection and action recognition. In this study, we investigate various aspects of convolutional neural networks (CNNs) from the big data perspective. We analyze recent studies and different network architectures both in terms of running time and accuracy. We present extensive empirical information along with best practices for big data practitioners. Using these best practices we propose efficient fusion mechanisms both for single and multiple network models. We present state-of-the art results on benchmark datasets while keeping computational costs at a lower level. Another contribution of our paper is that these state-of-the-art results can be reached without using extensive data augmentation techniques. version:1
arxiv-1608-01825 | Compartmental analysis of dynamic nuclear medicine data: regularization procedure and application to physiology | http://arxiv.org/abs/1608.01825 | id:1608.01825 author:Delbary Fabrice, Garbarino Sara category:physics.med-ph cs.CV math.NA  published:2016-08-05 summary:Compartmental models based on tracer mass balance are extensively used in clinical and pre-clinical nuclear medicine in order to obtain quantitative information on tracer metabolism in the biological tissue. This paper is the second of a series of two that deal with the problem of tracer coefficient estimation via compartmental modelling in an inverse problem framework. While the previous work was devoted to the discussion of identifiability issues for 2, 3 and n-dimension compartmental systems, here we discuss the problem of numerically determining the tracer coefficients by means of a general regularized Multivariate Gauss Newton scheme. In this paper, applications concerning cerebral, hepatic and renal functions are considered, involving experimental measurements on FDG-PET data on different set of murine models. version:1
arxiv-1608-01818 | The BioDynaMo Project: a platform for computer simulations of biological dynamics | http://arxiv.org/abs/1608.01818 | id:1608.01818 author:Leonard Johard, Lukas Breitwieser, Alberto Di Meglio, Marco Manca, Manuel Mazzara, Max Talanov category:cs.NE  published:2016-08-05 summary:This paper is a brief update on developments in the BioDynaMo project, a new platform for computer simulations for biological research. We will discuss the new capabilities of the simulator, important new concepts simulation methodology as well as its numerous applications to the computational biology and nanoscience communities. version:1
arxiv-1608-01807 | SIFT Meets CNN: A Decade Survey of Instance Retrieval | http://arxiv.org/abs/1608.01807 | id:1608.01807 author:Liang Zheng, Yi Yang, Qi Tian category:cs.CV  published:2016-08-05 summary:The Bag-of-Words (BoW) model has been predominantly viewed as the state of the art in Content-Based Image Retrieval (CBIR) systems since 2003. The past 13 years has seen its advance based on the SIFT descriptor due to its advantages in dealing with image transformations. In recent years, image representation based on the Convolutional Neural Network (CNN) has attracted more attention in image retrieval, and demonstrates impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of image retrieval methods over the past decade. In particular, according to the feature extraction and quantization schemes, we classify current methods into three types, i.e., SIFT-based, one-pass CNN-based, and multi-pass CNN-based. This survey reviews milestones in BoW image retrieval, compares previous works that fall into different BoW steps, and shows that SIFT and CNN share common characteristics that can be incorporated in the BoW model. After presenting and analyzing the retrieval accuracy on several benchmark datasets, we highlight promising directions in image retrieval that demonstrate how the CNN-based BoW model can learn from the SIFT feature. version:1
arxiv-1608-01793 | Sparse Subspace Clustering via Diffusion Process | http://arxiv.org/abs/1608.01793 | id:1608.01793 author:Qilin Li, Ling Li, Wanquan Liu category:cs.CV  published:2016-08-05 summary:Subspace clustering refers to the problem of clustering high-dimensional data that lie in a union of low-dimensional subspaces. State-of-the-art subspace clustering methods are based on the idea of expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with L1, L2 or nuclear norms for a sparse solution. L1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be fully connected. L2 and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed L1, L2 and nuclear norm regularization could offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper focuses on using L1 norm and alleviating the corresponding connectivity problem by a simple yet efficient diffusion process on subspace affinity graphs. Without adding any tuning parameter , our method can achieve state-of-the-art clustering performance on Hopkins 155 and Extended Yale B data sets. version:1
arxiv-1608-01783 | The Evolutionary Process of Image Transition in Conjunction with Box and Strip Mutation | http://arxiv.org/abs/1608.01783 | id:1608.01783 author:Aneta Neumann, Bradley Alexander, Frank Neumann category:cs.NE  published:2016-08-05 summary:Evolutionary algorithms have been used in many ways to generate digital art. We study how evolutionary processes are used for evolutionary art and present a new approach to the transition of images. Our main idea is to define evolutionary processes for digital image transition, combining different variants of mutation and evolutionary mechanisms. We introduce box and strip mutation operators which are specifically designed for image transition. Our experimental results show that the process of an evolutionary algorithm in combination with these mutation operators can be used as a valuable way to produce unique generative art. version:1
arxiv-1608-01771 | Community Detection in Political Twitter Networks using Nonnegative Matrix Factorization Methods | http://arxiv.org/abs/1608.01771 | id:1608.01771 author:Mert Ozer, Nyunsu Kim, Hasan Davulcu category:cs.SI physics.soc-ph stat.ML  published:2016-08-05 summary:Community detection is a fundamental task in social network analysis. In this paper, first we develop an endorsement filtered user connectivity network by utilizing Heider's structural balance theory and certain Twitter triad patterns. Next, we develop three Nonnegative Matrix Factorization frameworks to investigate the contributions of different types of user connectivity and content information in community detection. We show that user content and endorsement filtered connectivity information are complementary to each other in clustering politically motivated users into pure political communities. Word usage is the strongest indicator of users' political orientation among all content categories. Incorporating user-word matrix and word similarity regularizer provides the missing link in connectivity only methods which suffer from detection of artificially large number of clusters for Twitter networks. version:1
arxiv-1608-01769 | Deep Learning the City : Quantifying Urban Perception At A Global Scale | http://arxiv.org/abs/1608.01769 | id:1608.01769 author:Abhimanyu Dubey, Nikhil Naik, Devi Parikh, Ramesh Raskar, César A. Hidalgo category:cs.CV  published:2016-08-05 summary:Computer vision methods that quantify the perception of urban environment are increasingly being used to study the relationship between a city's physical appearance and the behavior and health of its residents. Yet, the throughput of current methods is too limited to quantify the perception of cities across the world. To tackle this challenge, we introduce a new crowdsourced dataset containing 110,988 images from 56 cities, and 1,170,000 pairwise comparisons provided by 81,630 online volunteers along six perceptual attributes: safe, lively, boring, wealthy, depressing, and beautiful. Using this data, we train a Siamese-like convolutional neural architecture, which learns from a joint classification and ranking loss, to predict human judgments of pairwise image comparisons. Our results show that crowdsourcing combined with neural networks can produce urban perception data at the global scale. version:1
arxiv-1608-00060 | Double Machine Learning for Treatment and Causal Parameters | http://arxiv.org/abs/1608.00060 | id:1608.00060 author:Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, anad Whitney Newey category:stat.ML 62G  published:2016-07-30 summary:Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods. version:2
arxiv-1608-01747 | A Distance for HMMs based on Aggregated Wasserstein Metric and State Registration | http://arxiv.org/abs/1608.01747 | id:1608.01747 author:Yukun Chen, Jianbo Ye, Jia Li category:cs.LG stat.ML  published:2016-08-05 summary:We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian. For such HMMs, the marginal distribution at any time spot follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs. We refer to such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions. Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions. The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples. It is invariant to relabeling or permutation of the states. This distance quantifies the dissimilarity of GMM-HMMs by measuring both the difference between the two marginal GMMs and the difference between the two transition matrices. Our new distance is tested on the tasks of retrieval and classification of time series. Experiments on both synthetic data and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence. version:1
arxiv-1608-01686 | Optimal Filtered Backprojection for Fast and Accurate Tomography Reconstruction | http://arxiv.org/abs/1608.01686 | id:1608.01686 author:Chen Mu, Chiwoo Park category:cs.CV I.4.3  I.4.5  published:2016-08-04 summary:Tomographic reconstruction is a method of reconstructing a high dimensional image with a series of its low dimensional projections, and the filtered backprojection is one of very popular analytical techniques for the reconstruction due to its computational efficiency and easy of implementation. The accuracy of the filtered backprojection method deteriorates when input data are noisy or input data are available for only a limited number of projection angles. For the case, some algebraic approaches perform better, but they are based on computationally slow iterations. We propose an improvement of the filtered backprojection method which is as fast as the existing filtered backprojection and is as accurate as the algebraic approaches under heavy observation noises and limited availability of projection data. The new approach optimizes the filter of the backprojection operator to minimize a regularized reconstruction error. We compare the new approach with the state-of-the-art in the filtered backprojection and algebraic approaches using four simulated datasets to show its competitive accuracy and computing speed. version:1
arxiv-1608-01658 | Identifying Metastases in Sentinel Lymph Nodes with Deep Convolutional Neural Networks | http://arxiv.org/abs/1608.01658 | id:1608.01658 author:Richard Chen, Yating Jing, Hunter Jackson category:cs.CV  published:2016-08-04 summary:Metastatic presence in lymph nodes is one of the most important prognostic variables of breast cancer. The current diagnostic procedure for manually reviewing sentinel lymph nodes, however, is very time-consuming and subjective. Pathologists have to manually scan an entire digital whole-slide image (WSI) for regions of metastasis that are sometimes only detectable under high resolution or entirely hidden from the human visual cortex. From October 2015 to April 2016, the International Symposium on Biomedical Imaging (ISBI) held the Camelyon Grand Challenge 2016 to crowd-source ideas and algorithms for automatic detection of lymph node metastasis. Using a generalizable stain normalization technique and the Proscia Pathology Cloud computing platform, we trained a deep convolutional neural network on millions of tissue and tumor image tiles to perform slide-based evaluation on our testing set of whole-slide images images, with a sensitivity of 0.96, specificity of 0.89, and AUC score of 0.90. Our results indicate that our platform can automatically scan any WSI for metastatic regions without institutional calibration to respective stain profiles. version:1
arxiv-1608-01647 | A Recursive Framework for Expression Recognition: From Web Images to Deep Models to Game Dataset | http://arxiv.org/abs/1608.01647 | id:1608.01647 author:Wei Li, Christina Tsangouri, Farnaz Abtahi, Zhigang Zhu category:cs.CV  published:2016-08-04 summary:In this paper, we propose a recursive framework to recognize facial expressions from images in real scenes. Unlike traditional approaches that typically focus on developing and refining algorithms for improving recognition performance on an existing dataset, we integrate three important components in a recursive manner: facial dataset generation, facial expression recognition model building, and interactive interfaces for testing and new data collection. To start with, we first create a candid-images-for-facial-expression (CIFE) dataset. We then apply a convolutional neural network (CNN) to CIFE and build a CNN model for web image expression classification. In order to increase the expression recognition accuracy, we also fine-tune the CNN model and thus obtain a better CNN facial expression recognition model. Based on the fine-tuned CNN model, we design a facial expression game engine and collect a new and more balanced dataset, GaMo. The images of this dataset are collected from the different expressions our game users make when playing the game. Finally, we evaluate the GaMo and CIFE datasets and show that our recursive framework can help build a better facial expression model for dealing with real scene facial expression tasks. version:1
arxiv-1608-01561 | UsingWord Embeddings for Query Translation for Hindi to English Cross Language Information Retrieval | http://arxiv.org/abs/1608.01561 | id:1608.01561 author:Paheli Bhattacharya, Pawan Goyal, Sudeshna Sarkar category:cs.CL  published:2016-08-04 summary:Cross-Language Information Retrieval (CLIR) has become an important problem to solve in the recent years due to the growth of content in multiple languages in the Web. One of the standard methods is to use query translation from source to target language. In this paper, we propose an approach based on word embeddings, a method that captures contextual clues for a particular word in the source language and gives those words as translations that occur in a similar context in the target language. Once we obtain the word embeddings of the source and target language pairs, we learn a projection from source to target word embeddings, making use of a dictionary with word translation pairs.We then propose various methods of query translation and aggregation. The advantage of this approach is that it does not require the corpora to be aligned (which is difficult to obtain for resource-scarce languages), a dictionary with word translation pairs is enough to train the word vectors for translation. We experiment with Forum for Information Retrieval and Evaluation (FIRE) 2008 and 2012 datasets for Hindi to English CLIR. The proposed word embedding based approach outperforms the basic dictionary based approach by 70% and when the word embeddings are combined with the dictionary, the hybrid approach beats the baseline dictionary based method by 77%. It outperforms the English monolingual baseline by 15%, when combined with the translations obtained from Google Translate and Dictionary. version:1
arxiv-1608-01536 | Saliency Integration: An Arbitrator Model | http://arxiv.org/abs/1608.01536 | id:1608.01536 author:Yingyue Xu, Xiaopeng Hong, Guoying Zhao category:cs.CV  published:2016-08-04 summary:Saliency integration approaches have aroused general concern on unifying saliency maps from multiple saliency models. In fact, saliency integration is a weighted aggregation of multiple saliency maps, such that measuring the weights of saliency models is essential. In this paper, we propose an unsupervised model for saliency integration, namely the arbitrator model (AM), based on the Bayes' probability theory. The proposed AM incorporates saliency models of varying expertise and a prior map based on the consistency of the evidence from multiple saliency models and a reference saliency map from generally accepted knowledge. Also, we suggest two methods to learn the expertise of the saliency models without ground truth. The experimental results are from various combinations of twenty-two state-of-the-art saliency models on five datasets. The evaluation results show that the AM model improves the performance substantially compared to the existing state-of-the-art approaches, regardless of the chosen candidate saliency models. version:1
arxiv-1608-01529 | Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos | http://arxiv.org/abs/1608.01529 | id:1608.01529 author:Suman Saha, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, Fabio Cuzzolin category:cs.CV  published:2016-08-04 summary:In this work, we propose an approach to the spatiotemporal localisation (detection) and classification of multiple concurrent actions within temporally untrimmed videos. Our framework is composed of three stages. In stage 1, appearance and motion detection networks are employed to localise and score actions from colour images and optical flow. In stage 2, the appearance network detections are boosted by combining them with the motion detection scores, in proportion to their respective spatial overlap. In stage 3, sequences of detection boxes most likely to be associated with a single action instance, called action tubes, are constructed by solving two energy maximisation problems via dynamic programming. While in the first pass, action paths spanning the whole video are built by linking detection boxes over time using their class-specific scores and their spatial overlap, in the second pass, temporal trimming is performed by ensuring label consistency for all constituting detection boxes. We demonstrate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new state-of-the-art results across the board and significantly increasing detection speed at test time. We achieve a huge leap forward in action detection performance and report a 20% and 11% gain in mAP (mean average precision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the state-of-the-art. version:1
arxiv-1608-01505 | Recoding Color Transfer as a Color Homography | http://arxiv.org/abs/1608.01505 | id:1608.01505 author:Han Gong, Graham D. Finlayson, Robert B. Fisher category:cs.CV  published:2016-08-04 summary:Color transfer is an image editing process that adjusts the colors of a picture to match a target picture's color theme. A natural color transfer not only matches the color styles but also prevents after-transfer artifacts due to image compression, noise, and gradient smoothness change. The recently discovered color homography theorem proves that colors across a change in photometric viewing condition are related by a homography. In this paper, we propose a color-homography-based color transfer decomposition which encodes color transfer as a combination of chromaticity shift and shading adjustment. A powerful form of shading adjustment is shown to be a global shading curve by which the same shading homography can be applied elsewhere. Our experiments show that the proposed color transfer decomposition provides a very close approximation to many popular color transfer methods. The advantage of our approach is that the learned color transfer can be applied to many other images (e.g. other frames in a video), instead of a frame-to-frame basis. We demonstrate two applications for color transfer enhancement and video color grading re-application. This simple model of color transfer is also important for future color transfer algorithm design. version:1
arxiv-1608-01471 | UnitBox: An Advanced Object Detection Network | http://arxiv.org/abs/1608.01471 | id:1608.01471 author:Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, Thomas Huang category:cs.CV 68U01 I.4.0  published:2016-08-04 summary:In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the $\ell_2$ loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union ($IoU$) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of $IoU$ loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark. version:1
arxiv-1608-01448 | Word Segmentation on Micro-blog Texts with External Lexicon and Heterogeneous Data | http://arxiv.org/abs/1608.01448 | id:1608.01448 author:Qingrong Xia, Zhenghua Li, Jiayuan Chao, Min Zhang category:cs.CL  published:2016-08-04 summary:This paper describes our system designed for the NLPCC 2016 shared task on word segmentation on micro-blog texts. version:1
arxiv-1608-01441 | Improving Multi-label Learning with Missing Labels by Structured Semantic Correlations | http://arxiv.org/abs/1608.01441 | id:1608.01441 author:Hao Yang, Joey Tianyi Zhou, Jianfei Cai category:cs.CV  published:2016-08-04 summary:Multi-label learning has attracted significant interests in computer vision recently, finding applications in many vision tasks such as multiple object recognition and automatic image annotation. Associating multiple labels to a complex image is very difficult, not only due to the intricacy of describing the image, but also because of the incompleteness nature of the observed labels. Existing works on the problem either ignore the label-label and instance-instance correlations or just assume these correlations are linear and unstructured. Considering that semantic correlations between images are actually structured, in this paper we propose to incorporate structured semantic correlations to solve the missing label problem of multi-label learning. Specifically, we project images to the semantic space with an effective semantic descriptor. A semantic graph is then constructed on these images to capture the structured correlations between them. We utilize the semantic graph Laplacian as a smooth term in the multi-label learning formulation to incorporate the structured semantic correlations. Experimental results demonstrate the effectiveness of the proposed semantic descriptor and the usefulness of incorporating the structured semantic correlations. We achieve better results than state-of-the-art multi-label learning methods on four benchmark datasets. version:1
arxiv-1608-01413 | Solving General Arithmetic Word Problems | http://arxiv.org/abs/1608.01413 | id:1608.01413 author:Subhro Roy, Dan Roth category:cs.CL  published:2016-08-04 summary:This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of {\em quantity schemas} that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems. version:1
arxiv-1608-01410 | Bayesian Kernel and Mutual $k$-Nearest Neighbor Regression | http://arxiv.org/abs/1608.01410 | id:1608.01410 author:Hyun-Chul Kim category:cs.LG stat.ML  published:2016-08-04 summary:We propose Bayesian extensions of two nonparametric regression methods which are kernel and mutual $k$-nearest neighbor regression methods. Derived based on Gaussian process models for regression, the extensions provide distributions for target value estimates and the framework to select the hyperparameters. It is shown that both the proposed methods asymptotically converge to kernel and mutual $k$-nearest neighbor regression methods, respectively. The simulation results show that the proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set. version:1
arxiv-1608-01409 | Holistic SparseCNN: Forging the Trident of Accuracy, Speed, and Size | http://arxiv.org/abs/1608.01409 | id:1608.01409 author:Jongsoo Park, Sheng R. Li, Wei Wen, Hai Li, Yiran Chen, Pradeep Dubey category:cs.CV  published:2016-08-04 summary:We present Holistic SparseCNN, a sparse convolutional neural network design that simultaneously optimizes convolution layers (for classification speed) and fully connected layers (for model size), while maintaining the accuracy. We directly apply convolutions to tensors without bandwidth-wasting lowering step, which is critical for sparse convolution that is more prone to be bandwidth bound than its dense counterpart. Our cross-layer training method balances sparsity among multiple layers to optimize the trade-off between accuracy, speed, and model size, and it is guided by the characteristics of underlying computing platforms. We demonstrate overall classification throughputs significantly higher than the best published numbers on Intel Xeon and Atom processors, which represent datacenter servers and resource-constrained mobile platforms, respectively. version:1
arxiv-1608-01406 | Quantum Algorithms for Compositional Natural Language Processing | http://arxiv.org/abs/1608.01406 | id:1608.01406 author:William Zeng, Bob Coecke category:cs.CL quant-ph  published:2016-08-04 summary:We propose a new application of quantum computing to the field of natural language processing. Ongoing work in this field attempts to incorporate grammatical structure into algorithms that compute meaning. In (Coecke, Sadrzadeh and Clark, 2010), the authors introduce such a model (the CSC model) based on tensor product composition. While this algorithm has many advantages, its implementation is hampered by the large classical computational resources that it requires. In this work we show how computational shortcomings of the CSC approach could be resolved using quantum computation (possibly in addition to existing techniques for dimension reduction). We address the value of quantum RAM (Giovannetti,2008) for this model and extend an algorithm from Wiebe, Braun and Lloyd (2012) into a quantum algorithm to categorize sentences in CSC. Our new algorithm demonstrates a quadratic speedup over classical methods under certain conditions. version:1
arxiv-1608-01405 | Entailment Relations on Distributions | http://arxiv.org/abs/1608.01405 | id:1608.01405 author:John van de Wetering category:cs.CL  published:2016-08-04 summary:In this paper we give an overview of partial orders on the space of probability distributions that carry a notion of information content and serve as a generalisation of the Bayesian order given in (Coecke and Martin, 2011). We investigate what constraints are necessary in order to get a unique notion of information content. These partial orders can be used to give an ordering on words in vector space models of natural language meaning relating to the contexts in which words are used, which is useful for a notion of entailment and word disambiguation. The construction used also points towards a way to create orderings on the space of density operators which allow a more fine-grained study of entailment. The partial orders in this paper are directed complete and form domains in the sense of domain theory. version:1
arxiv-1608-01404 | Quantifier Scope in Categorical Compositional Distributional Semantics | http://arxiv.org/abs/1608.01404 | id:1608.01404 author:Mehrnoosh Sadrzadeh category:cs.CL cs.AI cs.LO  published:2016-08-04 summary:In previous work with J. Hedges, we formalised a generalised quantifiers theory of natural language in categorical compositional distributional semantics with the help of bialgebras. In this paper, we show how quantifier scope ambiguity can be represented in that setting and how this representation can be generalised to branching quantifiers. version:1
arxiv-1608-01403 | Words, Concepts, and the Geometry of Analogy | http://arxiv.org/abs/1608.01403 | id:1608.01403 author:Stephen McGregor, Matthew Purver, Geraint Wiggins category:cs.CL  published:2016-08-04 summary:This paper presents a geometric approach to the problem of modelling the relationship between words and concepts, focusing in particular on analogical phenomena in language and cognition. Grounded in recent theories regarding geometric conceptual spaces, we begin with an analysis of existing static distributional semantic models and move on to an exploration of a dynamic approach to using high dimensional spaces of word meaning to project subspaces where analogies can potentially be solved in an online, contextualised way. The crucial element of this analysis is the positioning of statistics in a geometric environment replete with opportunities for interpretation. version:1
arxiv-1608-01402 | Interacting Conceptual Spaces | http://arxiv.org/abs/1608.01402 | id:1608.01402 author:Josef Bolt, Bob Coecke, Fabrizio Genovese, Martha Lewis, Daniel Marsden, Robin Piedeleu category:cs.AI cs.CL cs.LO  published:2016-08-04 summary:We propose applying the categorical compositional scheme of [6] to conceptual space models of cognition. In order to do this we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We show how conceptual spaces for composite types such as adjectives and verbs can be constructed. We illustrate this new model on detailed examples. version:1
arxiv-1608-01401 | Dual Density Operators and Natural Language Meaning | http://arxiv.org/abs/1608.01401 | id:1608.01401 author:Daniela Ashoush, Bob Coecke category:cs.CL cs.LO quant-ph  published:2016-08-04 summary:Density operators allow for representing ambiguity about a vector representation, both in quantum theory and in distributional natural language meaning. Formally equivalently, they allow for discarding part of the description of a composite system, where we consider the discarded part to be the context. We introduce dual density operators, which allow for two independent notions of context. We demonstrate the use of dual density operators within a grammatical-compositional distributional framework for natural language meaning. We show that dual density operators can be used to simultaneously represent: (i) ambiguity about word meanings (e.g. queen as a person vs. queen as a band), and (ii) lexical entailment (e.g. tiger -> mammal). We provide a proof-of-concept example. version:1
arxiv-1608-01398 | Iterative Hard Thresholding for Model Selection in Genome-Wide Association Studies | http://arxiv.org/abs/1608.01398 | id:1608.01398 author:Kevin L. Keys, Gary K. Chen, Kenneth Lange category:stat.ML  published:2016-08-04 summary:A genome-wide association study (GWAS) correlates marker variation with trait variation in a sample of individuals. Each study subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here we assume that subjects are unrelated and collected at random and that trait values are normally distributed or transformed to normality. Over the past decade, researchers have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with LASSO or MCP penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. This paper introduces the iterative hard thresholding (IHT) algorithm to the GWAS analysis of continuous traits. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage commodity desktop computers in GWAS analysis and to avoid supercomputing. We evaluate IHT performance on both simulated and real GWAS data and conclude that it reduces false positive and false negative rates while remaining competitive in computational time with penalized regression. Source code is freely available at https://github.com/klkeys/IHT.jl. version:1
arxiv-1608-01391 | Language free character recognition using character sketch and center of gravity shifting | http://arxiv.org/abs/1608.01391 | id:1608.01391 author:Masoud Nosrati, Fakhereh Rahimi, Ronak Karimi category:cs.CV  published:2016-08-03 summary:In this research, we present a heuristic method for character recognition. For this purpose, a sketch is constructed from the image that contains the character to be recognized. This sketch contains the most important pixels of image that are representatives of original image. These points are the most probable points in pixel-by-pixel matching of image that adapt to target image. Furthermore, a technique called gravity shifting is utilized for taking over the problem of elongation of characters. The consequence of combining sketch and gravity techniques leaded to a language free character recognition method. This method can be implemented independently for real-time uses or in combination of other classifiers as a feature extraction algorithm. Low complexity and acceptable performance are the most impressive features of this method that let it to be simply implemented in mobile and battery-limited computing devices. Results show that in the best case 86% of accuracy is obtained and in the worst case 28% of recognized characters are accurate. version:1
arxiv-1608-01372 | Permutation NMF | http://arxiv.org/abs/1608.01372 | id:1608.01372 author:Giovanni Barbarino category:cs.CV cs.NA  published:2016-08-03 summary:Nonnegative Matrix Factorization(NMF) is a common used technique in machine learning to extract features out of data such as text documents and images thanks to its natural clustering properties. In particular, it is popular in image processing since it can decompose several pictures and recognize common parts if they're located in the same position over the photos. This paper's aim is to present a way to add the translation invariance to the classical NMF, that is, the algorithms presented are able to detect common features, even when they're shifted, in different original images. version:1
arxiv-1608-01339 | Retinal Vessel Segmentation Using A New Topological Method | http://arxiv.org/abs/1608.01339 | id:1608.01339 author:Martin Brooks category:cs.CG cs.CV physics.med-ph  published:2016-08-03 summary:A novel topological segmentation of retinal images represents blood vessels as connected regions in the continuous image plane, having shape-related analytic and geometric properties. This paper presents topological segmentation results from the DRIVE retinal image database. version:1
arxiv-1608-01298 | A Physical Metaphor to Study Semantic Drift | http://arxiv.org/abs/1608.01298 | id:1608.01298 author:Sándor Darányi, Peter Wittek, Konstantinos Konstantinidis, Symeon Papadopoulos, Efstratios Kontopoulos category:cs.CL cs.NE stat.ML  published:2016-08-03 summary:In accessibility tests for digital preservation, over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field. This articulates the need to detect, measure, interpret and model outcomes of knowledge dynamics. To this end we employ a high-performance machine learning algorithm for the training of extremely large emergent self-organizing maps for exploratory data analysis. The working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of Newtonian mechanics called social mechanics. By using term distances as a measure of semantic relatedness vs. their PageRank values indicating social importance and applied as variable `term mass', gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation. From `term gravitation' over time, one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs. social importance, thereby updating Osgood's semantic differential. The dataset examined is the public catalog metadata of Tate Galleries, London. version:1
arxiv-1608-01281 | Learning Online Alignments with Continuous Rewards Policy Gradient | http://arxiv.org/abs/1608.01281 | id:1608.01281 author:Yuping Luo, Chung-Cheng Chiu, Navdeep Jaitly, Ilya Sutskever category:cs.LG cs.CL  published:2016-08-03 summary:Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering. Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition. To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments. The online alignments model is able to start producing outputs without the need to first process the entire input sequence. A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method. In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal (WSJ) speech recognition datasets. version:1
arxiv-1608-01276 | Fuzzy-based Propagation of Prior Knowledge to Improve Large-Scale Image Analysis Pipelines | http://arxiv.org/abs/1608.01276 | id:1608.01276 author:Johannes Stegmaier, Ralf Mikut category:cs.CV q-bio.QM  published:2016-08-03 summary:Many automatically analyzable scientific questions are well-posed and offer a variety of information about the expected outcome a priori. Although often being neglected, this prior knowledge can be systematically exploited to make automated analysis operations sensitive to a desired phenomenon or to evaluate extracted content with respect to this prior knowledge. For instance, the performance of processing operators can be greatly enhanced by a more focused detection strategy and the direct information about the ambiguity inherent in the extracted data. We present a new concept for the estimation and propagation of uncertainty involved in image analysis operators. This allows using simple processing operators that are suitable for analyzing large-scale 3D+t microscopy images without compromising the result quality. On the foundation of fuzzy set theory, we transform available prior knowledge into a mathematical representation and extensively use it enhance the result quality of various processing operators. All presented concepts are illustrated on a typical bioimage analysis pipeline comprised of seed point detection, segmentation, multiview fusion and tracking. Furthermore, the functionality of the proposed approach is validated on a comprehensive simulated 3D+t benchmark data set that mimics embryonic development and on large-scale light-sheet microscopy data of a zebrafish embryo. The general concept introduced in this contribution represents a new approach to efficiently exploit prior knowledge to improve the result quality of image analysis pipelines. Especially, the automated analysis of terabyte-scale microscopy data will benefit from sophisticated and efficient algorithms that enable a quantitative and fast readout. The generality of the concept, however, makes it also applicable to practically any other field with processing strategies that are arranged as linear pipelines. version:1
arxiv-1608-01264 | Fast and Simple Optimization for Poisson Likelihood Models | http://arxiv.org/abs/1608.01264 | id:1608.01264 author:Niao He, Zaid Harchaoui, Yichen Wang, Le Song category:cs.LG math.OC stat.ML  published:2016-08-03 summary:Poisson likelihood models have been prevalently used in imaging, social networks, and time series analysis. We propose fast, simple, theoretically-grounded, and versatile, optimization algorithms for Poisson likelihood modeling. The Poisson log-likelihood is concave but not Lipschitz-continuous. Since almost all gradient-based optimization algorithms rely on Lipschitz-continuity, optimizing Poisson likelihood models with a guarantee of convergence can be challenging, especially for large-scale problems. We present a new perspective allowing to efficiently optimize a wide range of penalized Poisson likelihood objectives. We show that an appropriate saddle point reformulation enjoys a favorable geometry and a smooth structure. Therefore, we can design a new gradient-based optimization algorithm with $O(1/t)$ convergence rate, in contrast to the usual $O(1/\sqrt{t})$ rate of non-smooth minimization alternatives. Furthermore, in order to tackle problems with large samples, we also develop a randomized block-decomposition variant that enjoys the same convergence rate yet more efficient iteration cost. Experimental results on several point process applications including social network estimation and temporal recommendation show that the proposed algorithm and its randomized block variant outperform existing methods both on synthetic and real-world datasets. version:1
arxiv-1608-00641 | Interactive Image Segmentation Using Constrained Dominant Sets | http://arxiv.org/abs/1608.00641 | id:1608.00641 author:Eyasu Zemene, Marcello Pelillo category:cs.CV  published:2016-08-01 summary:We propose a new approach to interactive image segmentation based on some properties of a family of quadratic optimization problems related to dominant sets, a well-known graph-theoretic notion of a cluster which generalizes the concept of a maximal clique to edge-weighted graphs. In particular, we show that by properly controlling a regularization parameter which determines the structure and the scale of the underlying problem, we are in a position to extract groups of dominant-set clusters which are constrained to contain user-selected elements. The resulting algorithm can deal naturally with any type of input modality, including scribbles, sloppy contours, and bounding boxes, and is able to robustly handle noisy annotations on the part of the user. Experiments on standard benchmark datasets show the effectiveness of our approach as compared to state-of-the-art algorithms on a variety of natural images under several input conditions. version:2
arxiv-1608-01250 | Detailed Garment Recovery from a Single-View Image | http://arxiv.org/abs/1608.01250 | id:1608.01250 author:Shan Yang, Tanya Ambert, Zherong Pan, Ke Wang, Licheng Yu, Tamara Berg, Ming C. Lin category:cs.CV  published:2016-08-03 summary:Most recent garment capturing techniques rely on acquiring multiple views of clothing, which may not always be readily available, especially in the case of pre-existing photographs from the web. As an alternative, we pro- pose a method that is able to compute a rich and realistic 3D model of a human body and its outfits from a single photograph with little human in- teraction. Our algorithm is not only able to capture the global shape and geometry of the clothing, it can also extract small but important details of cloth, such as occluded wrinkles and folds. Unlike previous methods using full 3D information (i.e. depth, multi-view images, or sampled 3D geom- etry), our approach achieves detailed garment recovery from a single-view image by using statistical, geometric, and physical priors and a combina- tion of parameter estimation, semantic parsing, shape recovery, and physics- based cloth simulation. We demonstrate the effectiveness of our algorithm by re-purposing the reconstructed garments for virtual try-on and garment transfer applications, as well as cloth animation for digital characters. version:1
arxiv-1608-01247 | Query Clustering using Segment Specific Context Embeddings | http://arxiv.org/abs/1608.01247 | id:1608.01247 author:S. K Kolluru, Prasenjit Mukherjee category:cs.IR cs.CL  published:2016-08-03 summary:This paper presents a novel query clustering approach to capture the broad interest areas of users querying search engines. We make use of recent advances in NLP - word2vec and extend it to get query2vec, vector representations of queries, based on query contexts, obtained from the top search results for the query and use a highly scalable Divide & Merge clustering algorithm on top of the query vectors, to get the clusters. We have tried this approach on a variety of segments, including Retail, Travel, Health, Phones and found the clusters to be effective in discovering user's interest areas which have high monetization potential. version:1
arxiv-1608-01238 | Improving Quality of Hierarchical Clustering for Large Data Series | http://arxiv.org/abs/1608.01238 | id:1608.01238 author:Manuel R. Ciosici category:cs.CL cs.LG  published:2016-08-03 summary:Brown clustering is a hard, hierarchical, bottom-up clustering of words in a vocabulary. Words are assigned to clusters based on their usage pattern in a given corpus. The resulting clusters and hierarchical structure can be used in constructing class-based language models and for generating features to be used in NLP tasks. Because of its high computational cost, the most-used version of Brown clustering is a greedy algorithm that uses a window to restrict its search space. Like other clustering algorithms, Brown clustering finds a sub-optimal, but nonetheless effective, mapping of words to clusters. Because of its ability to produce high-quality, human-understandable cluster, Brown clustering has seen high uptake the NLP research community where it is used in the preprocessing and feature generation steps. Little research has been done towards improving the quality of Brown clusters, despite the greedy and heuristic nature of the algorithm. The approaches tried so far have focused on: studying the effect of the initialisation in a similar algorithm; tuning the parameters used to define the desired number of clusters and the behaviour of the algorithm; and including a separate parameter to differentiate the window from the desired number of clusters. However, some of these approaches have not yielded significant improvements in cluster quality. In this thesis, a close analysis of the Brown algorithm is provided, revealing important under-specifications and weaknesses in the original algorithm. These have serious effects on cluster quality and reproducibility of research using Brown clustering. In the second part of the thesis, two modifications are proposed. Finally, a thorough evaluation is performed, considering both the optimization criterion of Brown clustering and the performance of the resulting class-based language models. version:1
arxiv-1608-01234 | Fast Algorithms for Demixing Sparse Signals from Nonlinear Observations | http://arxiv.org/abs/1608.01234 | id:1608.01234 author:Mohammadreza Soltani, Chinmay Hegde category:stat.ML  published:2016-08-03 summary:We study the problem of demixing a pair of sparse signals from noisy, nonlinear observations of their superposition. Mathematically, we consider a nonlinear signal observation model, $y_i = g(a_i^Tx) + e_i, \ i=1,\ldots,m$, where $x = \Phi w+\Psi z$ denotes the superposition signal, $\Phi$ and $\Psi$ are orthonormal bases in $\mathbb{R}^n$, and $w, z\in\mathbb{R}^n$ are sparse coefficient vectors of the constituent signals, and $e_i$ represents the noise. Moreover, $g$ represents a nonlinear link function, and $a_i\in\mathbb{R}^n$ is the $i$-th row of the measurement matrix, $A\in\mathbb{R}^{m\times n}$. Problems of this nature arise in several applications ranging from astronomy, computer vision, and machine learning. In this paper, we make some concrete algorithmic progress for the above demixing problem. Specifically, we consider two scenarios: (i) the case when the demixing procedure has no knowledge of the link function, and (ii) the case when the demixing algorithm has perfect knowledge of the link function. In both cases, we provide fast algorithms for recovery of the constituents $w$ and $z$ from the observations. Moreover, we support these algorithms with a rigorous theoretical analysis, and derive (nearly) tight upper bounds on the sample complexity of the proposed algorithms for achieving stable recovery of the component signals. We also provide a range of numerical simulations to illustrate the performance of the proposed algorithms on both real and synthetic signals and images. version:1
arxiv-1608-01230 | Learning a Driving Simulator | http://arxiv.org/abs/1608.01230 | id:1608.01230 author:Eder Santana, George Hotz category:cs.LG stat.ML  published:2016-08-03 summary:Comma.ai's approach to Artificial Intelligence for self-driving cars is based on an agent that learns to clone driver behaviors and plans maneuvers by simulating future events in the road. This paper illustrates one of our research approaches for driving simulation. One where we learn to simulate. Here we investigate variational autoencoders with classical and learned cost functions using generative adversarial networks for embedding road frames. Afterwards, we learn a transition model in the embedded space using action conditioned Recurrent Neural Networks. We show that our approach can keep predicting realistic looking video for several frames despite the transition model being optimized without a cost function in the pixel space. version:1
arxiv-1608-00686 | Clinical Tagging with Joint Probabilistic Models | http://arxiv.org/abs/1608.00686 | id:1608.00686 author:Yoni Halpern, Steven Horng, David Sontag category:stat.ML cs.LG  published:2016-08-02 summary:We describe a method for parameter estimation in bipartite probabilistic graphical models for joint prediction of clinical conditions from the electronic medical record. The method does not rely on the availability of gold-standard labels, but rather uses noisy labels, called anchors, for learning. We provide a likelihood-based objective and a moments-based initialization that are effective at learning the model parameters. The learned model is evaluated in a task of assigning a heldout clinical condition to patients based on retrospective analysis of the records, and outperforms baselines which do not account for the noisiness in the labels or do not model the conditions jointly. version:2
arxiv-1608-00874 | Modelling and computation using NCoRM mixtures for density regression | http://arxiv.org/abs/1608.00874 | id:1608.00874 author:Jim Griffin, Fabrizio Leisen category:stat.ME stat.AP stat.CO stat.ML  published:2016-08-02 summary:Normalized compound random measures are flexible nonparametric priors for related distributions. We consider building general nonparametric regression models using normalized compound random measure mixture models. We develop a general approach to the unbiased estimation of Laplace functionals of compound random measure (which includes completely random measures as a special case). This allows us to propose a novel pseudo-marginal Metropolis-Hastings sampler for normalized compound random measure mixture models. The approach is illustrated on problems of density regression. version:2
arxiv-1608-01127 | Autonomous Grounding of Visual Field Experience through Sensorimotor Prediction | http://arxiv.org/abs/1608.01127 | id:1608.01127 author:Alban Laflaquière category:cs.RO cs.AI cs.CV cs.LG  published:2016-08-03 summary:In a developmental framework, autonomous robots need to explore the world and learn how to interact with it. Without an a priori model of the system, this opens the challenging problem of having robots master their interface with the world: how to perceive their environment using their sensors, and how to act in it using their motors. The sensorimotor approach of perception claims that a naive agent can learn to master this interface by capturing regularities in the way its actions transform its sensory inputs. In this paper, we apply such an approach to the discovery and mastery of the visual field associated with a visual sensor. A computational model is formalized and applied to a simulated system to illustrate the approach. version:1
arxiv-1608-01118 | A supermartingale approach to Gaussian process based sequential design of experiments | http://arxiv.org/abs/1608.01118 | id:1608.01118 author:Julien Bect, François Bachoc, David Ginsbourger category:stat.ML math.PR math.ST stat.TH  published:2016-08-03 summary:Gaussian process (GP) models have become a well-established framework for the adap-tive design of costly experiments, and in particular, but not only, of computer experiments. GP-based sequential designs have been proposed for various objectives, such as global optimization (estimating the global maximum or maximizer(s) of a function), reliability analysis (estimating a probability of failure) or the estimation of level sets and excursion sets. In this paper, we tackle the convergence properties of an important class of such sequential design strategies, known as stepwise uncertainty reduction (SUR) strategies. Our approach relies on the key observation that the sequence of residual uncertainty measures, in a SUR strategy, is in general a supermartingale with respect to the filtration generated by the observations. We also provide some general results about GP-based sequential design, which are of independent interest. Our main application is a proof of almost sure convergence for one of the SUR strategies proposed by Bect, Ginsbourger, Li, Picheny and Vazquez (Stat. Comp., 2012). To the best of our knowledge, this is the first convergence proof for a GP-based sequential design algorithm dedicated to the estimation of probabilities of excursion and excursions sets. We also establish, using the same approach, a new proof of almost sure convergence for the expected improvement algorithm, which is the first proof for this algorithm that applies to any continuous GP. version:1
arxiv-1608-01084 | To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation | http://arxiv.org/abs/1608.01084 | id:1608.01084 author:Christian Hadiwinoto, Yang Liu, Hwee Tou Ng category:cs.CL  published:2016-08-03 summary:Reordering poses a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present a novel reordering approach utilizing sparse features based on dependency word pairs. Each instance of these features captures whether two words, which are related by a dependency link in the source sentence dependency parse tree, follow the same order or are swapped in the translation output. Experiments on Chinese-to-English translation show a statistically significant improvement of 1.21 BLEU point using our approach, compared to a state-of-the-art statistical MT system that incorporates prior reordering approaches. version:1
arxiv-1608-01082 | Learning Common and Specific Features for RGB-D Semantic Segmentation with Deconvolutional Networks | http://arxiv.org/abs/1608.01082 | id:1608.01082 author:Jinghua Wang, Zhenhua Wang, Dacheng Tao, Simon See, Gang Wang category:cs.CV  published:2016-08-03 summary:In this paper, we tackle the problem of RGB-D semantic segmentation of indoor images. We take advantage of deconvolutional networks which can predict pixel-wise class labels, and develop a new structure for deconvolution of multiple modalities. We propose a novel feature transformation network to bridge the convolutional networks and deconvolutional networks. In the feature transformation network, we correlate the two modalities by discovering common features between them, as well as characterize each modality by discovering modality specific features. With the common features, we not only closely correlate the two modalities, but also allow them to borrow features from each other to enhance the representation of shared information. With specific features, we capture the visual patterns that are only visible in one modality. The proposed network achieves competitive segmentation accuracy on NYU depth dataset V1 and V2. version:1
arxiv-1608-01079 | Challenges in video based object detection in maritime scenario using computer vision | http://arxiv.org/abs/1608.01079 | id:1608.01079 author:D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Rajabaly, C. Quek category:cs.CV  published:2016-08-03 summary:This paper discusses the technical challenges in maritime image processing and machine vision problems for video streams generated by cameras. Even well documented problems of horizon detection and registration of frames in a video are very challenging in maritime scenarios. More advanced problems of background subtraction and object detection in video streams are very challenging. Challenges arising from the dynamic nature of the background, unavailability of static cues, presence of small objects at distant backgrounds, illumination effects, all contribute to the challenges as discussed here. version:1
arxiv-1608-01074 | FPGA system for real-time computational extended depth of field imaging using phase aperture coding | http://arxiv.org/abs/1608.01074 | id:1608.01074 author:Tal Remez, Or Litany, Shachar Yoseff, Harel Haim, Alex Bronstein category:cs.CV  published:2016-08-03 summary:We present a proof-of-concept end-to-end system for computational extended depth of field (EDOF) imaging. The acquisition is performed through a phase-coded aperture implemented by placing a thin wavelength-dependent optical mask inside the pupil of a conventional camera lens, as a result of which, each color channel is focused at a different depth. The reconstruction process receives the raw Bayer image as the input, and performs blind estimation of the output color image in focus at an extended range of depths using a patch-wise sparse prior. We present a fast non-iterative reconstruction algorithm operating with constant latency in fixed-point arithmetics and achieving real-time performance in a prototype FPGA implementation. The output of the system, on simulated and real-life scenes, is qualitatively and quantitatively better than the result of clear-aperture imaging followed by state-of-the-art blind deblurring. version:1
arxiv-1608-01072 | Fuzzy c-Shape: A new algorithm for clustering finite time series waveforms | http://arxiv.org/abs/1608.01072 | id:1608.01072 author:Fateme Fahiman, Jame C. Bezdek, Sarah M. Erfani, Christopher Leckie, Marimuthu Palaniswami category:cs.LG  published:2016-08-03 summary:The existence of large volumes of time series data in many applications has motivated data miners to investigate specialized methods for mining time series data. Clustering is a popular data mining method due to its powerful exploratory nature and its usefulness as a preprocessing step for other data mining techniques. This article develops two novel clustering algorithms for time series data that are extensions of a crisp c-shapes algorithm. The two new algorithms are heuristic derivatives of fuzzy c-means (FCM). Fuzzy c-Shapes plus (FCS+) replaces the inner product norm in the FCM model with a shape-based distance function. Fuzzy c-Shapes double plus (FCS++) uses the shape-based distance, and also replaces the FCM cluster centers with shape-extracted prototypes. Numerical experiments on 48 real time series data sets show that the two new algorithms outperform state-of-the-art shape-based clustering algorithms in terms of accuracy and efficiency. Four external cluster validity indices (the Rand index, Adjusted Rand Index, Variation of Information, and Normalized Mutual Information) are used to match candidate partitions generated by each of the studied algorithms. All four indices agree that for these finite waveform data sets, FCS++ gives a small improvement over FCS+, and in turn, FCS+ is better than the original crisp c-shapes method. Finally, we apply two tests of statistical significance to the three algorithms. The Wilcoxon and Friedman statistics both rank the three algorithms in exactly the same way as the four cluster validity indices. version:1
arxiv-1608-00329 | Keyphrase Extraction using Sequential Labeling | http://arxiv.org/abs/1608.00329 | id:1608.00329 author:Sujatha Das Gollapalli, Xiao-li Li category:cs.CL cs.AI cs.IR  published:2016-08-01 summary:Keyphrases efficiently summarize a document's content and are used in various document processing and retrieval tasks. Several unsupervised techniques and classifiers exist for extracting keyphrases from text documents. Most of these methods operate at a phrase-level and rely on part-of-speech (POS) filters for candidate phrase generation. In addition, they do not directly handle keyphrases of varying lengths. We overcome these modeling shortcomings by addressing keyphrase extraction as a sequential labeling task in this paper. We explore a basic set of features commonly used in NLP tasks as well as predictions from various unsupervised methods to train our taggers. In addition to a more natural modeling for the keyphrase extraction problem, we show that tagging models yield significant performance benefits over existing state-of-the-art extraction methods. version:2
arxiv-1608-01059 | Analyzing Linear Dynamical Systems: From Modeling to Coding and Learning | http://arxiv.org/abs/1608.01059 | id:1608.01059 author:Wenbing Huang, Fuchun Sun, Lele Cao, Mehrtash Harandi category:cs.CV  published:2016-08-03 summary:Encoding time-series with Linear Dynamical Systems (LDSs) leads to rich models with applications ranging from dynamical texture recognition to video segmentation to name a few. In this paper, we propose to represent LDSs with infinite-dimensional subspaces and derive an analytic solution to obtain stable LDSs. We then devise efficient algorithms to perform sparse coding and dictionary learning on the space of infinite-dimensional subspaces. In particular, two solutions are developed to sparsely encode an LDS. In the first method, we map the subspaces into a Reproducing Kernel Hilbert Space (RKHS) and achieve our goal through kernel sparse coding. As for the second solution, we propose to embed the infinite-dimensional subspaces into the space of symmetric matrices and formulate the sparse coding accordingly in the induced space. For dictionary learning, we encode time-series by introducing a novel concept, namely the two-fold LDSs. We then make use of the two-fold LDSs to derive an analytical form for updating atoms of an LDS dictionary, i.e., each atom is an LDS itself. Compared to several baselines and state-of-the-art methods, the proposed methods yield higher accuracies in various classification tasks including video classification and tactile recognition. version:1
arxiv-1608-01056 | Morphological Priors for Probabilistic Neural Word Embeddings | http://arxiv.org/abs/1608.01056 | id:1608.01056 author:Parminder Bhatia, Robert Guthrie, Jacob Eisenstein category:cs.CL  published:2016-08-03 summary:Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging. version:1
arxiv-1608-01041 | Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution | http://arxiv.org/abs/1608.01041 | id:1608.01041 author:Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, Zhengyou Zhang category:cs.CV  published:2016-08-03 summary:Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community. version:1
arxiv-1608-01026 | One-Class Slab Support Vector Machine | http://arxiv.org/abs/1608.01026 | id:1608.01026 author:Victor Fragoso, Walter Scheirer, Joao Hespanha, Matthew Turk category:cs.CV  published:2016-08-02 summary:This work introduces the one-class slab SVM (OCSSVM), a one-class classifier that aims at improving the performance of the one-class SVM. The proposed strategy reduces the false positive rate and increases the accuracy of detecting instances from novel classes. To this end, it uses two parallel hyperplanes to learn the normal region of the decision scores of the target class. OCSSVM extends one-class SVM since it can scale and learn non-linear decision functions via kernel methods. The experiments on two publicly available datasets show that OCSSVM can consistently outperform the one-class SVM and perform comparable to or better than other state-of-the-art one-class classifiers. version:1
arxiv-1608-01024 | Incremental Real-Time Multibody VSLAM with Trajectory Optimization Using Stereo Camera | http://arxiv.org/abs/1608.01024 | id:1608.01024 author:N Dinesh Reddy, Iman Abbasnejad, Sheetal Reddy, Amit Kumar Mondal, Vindhya Devalla category:cs.CV  published:2016-08-02 summary:Real time outdoor navigation in highly dynamic environments is an crucial problem. The recent literature on real time static SLAM don't scale up to dynamic outdoor environments. Most of these methods assume moving objects as outliers or discard the information provided by them. We propose an algorithm to jointly infer the camera trajectory and the moving object trajectory simultaneously. In this paper, we perform a sparse scene flow based motion segmentation using a stereo camera. The segmented objects motion models are used for accurate localization of the camera trajectory as well as the moving objects. We exploit the relationship between moving objects for improving the accuracy of the poses. We formulate the poses as a factor graph incorporating all the constraints. We achieve exact incremental solution by solving a full nonlinear optimization problem in real time. The evaluation is performed on the challenging KITTI dataset with multiple moving cars.Our method outperforms the previous baselines in outdoor navigation. version:1
arxiv-1608-00255 | Continuation semantics for multi-quantifier sentences: operation-based approaches | http://arxiv.org/abs/1608.00255 | id:1608.00255 author:Justyna Grudzinska, Marek Zawadowski category:math.LO cs.CL cs.LO 03B65  91F20  published:2016-07-31 summary:Classical scope-assignment strategies for multi-quantifier sentences involve quantifier phrase (QP)-movement. More recent continuation-based approaches provide a compelling alternative, for they interpret QP's in situ - without resorting to Logical Forms or any structures beyond the overt syntax. The continuation-based strategies can be divided into two groups: those that locate the source of scope-ambiguity in the rules of semantic composition and those that attribute it to the lexical entries for the quantifier words. In this paper, we focus on the former operation-based approaches and the nature of the semantic operations involved. More specifically, we discuss three such possible operation-based strategies for multi-quantifier sentences, together with their relative merits and costs. version:2
arxiv-1608-01018 | Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science | http://arxiv.org/abs/1608.01018 | id:1608.01018 author:Dimitrios Kartsaklis, Martha Lewis, Laura Rimell category:cs.CL cs.AI math.CT quant-ph  published:2016-08-02 summary:This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which was held on the 11th of June at the University of Strathclyde, Glasgow, and was co-located with Quantum Physics and Logic (QPL 2016). Exploiting the common ground provided by the concept of a vector space, the workshop brought together researchers working at the intersection of Natural Language Processing (NLP), cognitive science, and physics, offering them an appropriate forum for presenting their uniquely motivated work and ideas. The interplay between these three disciplines inspired theoretically motivated approaches to the understanding of how word meanings interact with each other in sentences and discourse, how diagrammatic reasoning depicts and simplifies this interaction, how language models are determined by input from the world, and how word and sentence meanings interact logically. This first edition of the workshop consisted of three invited talks from distinguished speakers (Hans Briegel, Peter G\"ardenfors, Dominic Widdows) and eight presentations of selected contributed papers. Each submission was refereed by at least three members of the Programme Committee, who delivered detailed and insightful comments and suggestions. version:1
arxiv-1608-01017 | Automated X-ray Image Analysis for Cargo Security: Critical Review and Future Promise | http://arxiv.org/abs/1608.01017 | id:1608.01017 author:Thomas W. Rogers, Nicolas Jaccard, Edward J. Morton, Lewis D. Griffin category:cs.CV  published:2016-08-02 summary:We review the relatively immature field of automated image analysis for X-ray cargo imagery. There is increasing demand for automated analysis methods that can assist in the inspection and selection of containers, due to the ever-growing volumes of traded cargo and the increasing concerns that customs- and security-related threats are being smuggled across borders by organised crime and terrorist networks. We split the field into the classical pipeline of image preprocessing and image understanding. Preprocessing includes: image manipulation; quality improvement; Threat Image Projection (TIP); and material discrimination and segmentation. Image understanding includes: Automated Threat Detection (ATD); and Automated Contents Verification (ACV). We identify several gaps in the literature that need to be addressed and propose ideas for future research. Where the current literature is sparse we borrow from the single-view, multi-view, and CT X-ray baggage domains, which have some characteristics in common with X-ray cargo. version:1
arxiv-1608-01008 | Markov Chain Sampling in Discrete Probabilistic Models with Constraints | http://arxiv.org/abs/1608.01008 | id:1608.01008 author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:stat.ML  published:2016-08-02 summary:We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where often limited resources, prior knowledge, or other pragmatic considerations can impose hard constraints (e.g., cardinality constraints). For a variety of such probabilistic models, we present theoretical results on mixing times of Markov chains, and show sufficient conditions under which the associated chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors that govern our theoretical bounds. version:1
arxiv-1608-00945 | Blocking Collapsed Gibbs Sampler for Latent Dirichlet Allocation Models | http://arxiv.org/abs/1608.00945 | id:1608.00945 author:Xin Zhang, Scott A. Sisson category:stat.CO stat.ML  published:2016-08-02 summary:The latent Dirichlet allocation (LDA) model is a widely-used latent variable model in machine learning for text analysis. Inference for this model typically involves a single-site collapsed Gibbs sampling step for latent variables associated with observations. The efficiency of the sampling is critical to the success of the model in practical large scale applications. In this article, we introduce a blocking scheme to the collapsed Gibbs sampler for the LDA model which can, with a theoretical guarantee, improve chain mixing efficiency. We develop two procedures, an O(K)-step backward simulation and an O(log K)-step nested simulation, to directly sample the latent variables within each block. We demonstrate that the blocking scheme achieves substantial improvements in chain mixing compared to the state of the art single-site collapsed Gibbs sampler. We also show that when the number of topics is over hundreds, the nested-simulation blocking scheme can achieve a significant reduction in computation time compared to the single-site sampler. version:1
arxiv-1608-00938 | Evolutionary forces in language change | http://arxiv.org/abs/1608.00938 | id:1608.00938 author:Christopher A. Ahern, Mitchell G. Newberry, Robin Clark, Joshua B. Plotkin category:q-bio.PE cs.CL  published:2016-08-02 summary:Languages and genes are both transmitted from generation to generation, with opportunity for differential reproduction and survivorship of forms. Here we apply a rigorous inference framework, drawn from population genetics, to distinguish between two broad mechanisms of language change: drift and selection. Drift is change that results from stochasticity in transmission and it may occur in the absence of any intrinsic difference between linguistic forms; whereas selection is truly an evolutionary force arising from intrinsic differences -- for example, when one form is preferred by members of the population. Using large corpora of parsed texts spanning the 12th century to the 21st century, we analyze three examples of grammatical changes in English: the regularization of past-tense verbs, the rise of the periphrastic `do', and syntactic variation in verbal negation. We show that we can reject stochastic drift in favor of a selective force driving some of these language changes, but not others. The strength of drift depends on a word's frequency, and so drift provides an alternative explanation for why some words are more prone to change than others. Our results suggest an important role for stochasticity in language change, and they provide a null model against which selective theories of language evolution must be compared. version:1
arxiv-1608-00929 | Efficient Segmental Cascades for Speech Recognition | http://arxiv.org/abs/1608.00929 | id:1608.00929 author:Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu category:cs.CL  published:2016-08-02 summary:Discriminative segmental models offer a way to incorporate flexible feature functions into speech recognition. However, their appeal has been limited by their computational requirements, due to the large number of possible segments to consider. Multi-pass cascades of segmental models introduce features of increasing complexity in different passes, where in each pass a segmental model rescores lattices produced by a previous (simpler) segmental model. In this paper, we explore several ways of making segmental cascades efficient and practical: reducing the feature set in the first pass, frame subsampling, and various pruning approaches. In experiments on phonetic recognition, we find that with a combination of such techniques, it is possible to maintain competitive performance while greatly reducing decoding, pruning, and training time. version:1
arxiv-1608-00911 | Modeling Spatial and Temporal Cues for Multi-label Facial Action Unit Detection | http://arxiv.org/abs/1608.00911 | id:1608.00911 author:Wen-Sheng Chu, Fernando De la Torre, Jeffrey F. Cohn category:cs.CV  published:2016-08-02 summary:Facial action units (AUs) are essential to decode human facial expressions. Researchers have focused on training AU detectors with a variety of features and classifiers. However, several issues remain. These are spatial representation, temporal modeling, and AU correlation. Unlike most studies that tackle these issues separately, we propose a hybrid network architecture to jointly address them. Specifically, spatial representations are extracted by a Convolutional Neural Network (CNN), which, as analyzed in this paper, is able to reduce person-specific biases caused by hand-crafted features (eg, SIFT and Gabor). To model temporal dependencies, Long Short-Term Memory (LSTMs) are stacked on top of these representations, regardless of the lengths of input videos. The outputs of CNNs and LSTMs are further aggregated into a fusion network to produce per-frame predictions of 12 AUs. Our network naturally addresses the three issues, and leads to superior performance compared to existing methods that consider these issues independently. Extensive experiments were conducted on two large spontaneous datasets, GFT and BP4D, containing more than 400,000 frames coded with 12 AUs. On both datasets, we report significant improvement over a standard multi-label CNN and feature-based state-of-the-art. Finally, we provide visualization of the learned AU models, which, to our best knowledge, reveal how machines see facial AUs for the first time. version:1
arxiv-1608-00905 | PicHunt: Social Media Image Retrieval for Improved Law Enforcement | http://arxiv.org/abs/1608.00905 | id:1608.00905 author:Sonal Goel, Niharika Sachdeva, Ponnurangam Kumaraguru, A V Subramanyam, Divam Gupta category:cs.MM cs.CV  published:2016-08-02 summary:Law Enforcement Agencies (LEA) are increasingly using social media to identify and reduce crime for well-being and safety of the society. Images shared on social media hurting religious, political, communal and other sentiments of people, often instigate violence and create law & order situations in society. This results in the need for LEA to inspect the spread of such images and users propagating them on social media. In this paper, we present a comparison between different handcrafted features and a Convolutional Neural Network (CNN) model to retrieve similar images, which outperforms state-of-art hand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time image search system, robust to retrieve modified images that allows LEA to analyze the current spread of images, sentiments floating and details of users propagating such content. The system also aids officials to save time of manually analyzing the content by reducing the search space on an average by 67%. version:1
arxiv-1608-00895 | RETURNN: The RWTH Extensible Training framework for Universal Recurrent Neural Networks | http://arxiv.org/abs/1608.00895 | id:1608.00895 author:Patrick Doetsch, Albert Zeyer, Paul Voigtlaender, Ilya Kulikov, Ralf Schlüter, Hermann Ney category:cs.LG cs.CL cs.NE  published:2016-08-02 summary:In this work we release our extensible and easily configurable neural network training software. It provides a rich set of functional layers with a particular focus on efficient training of recurrent neural network topologies on multiple GPUs. The source of the software package is public and freely available for academic research purposes and can be used as a framework or as a standalone tool which supports a flexible configuration. The software allows to train state-of-the-art deep bidirectional long short-term memory (LSTM) models on both one dimensional data like speech or two dimensional data like handwritten text. It can be applied to a variety of natural language processing tasks and also supports more exotic components such as attention-based end-to-end networks or associative LSTMs. version:1
arxiv-1608-00892 | Knowledge Distillation for Small-footprint Highway Networks | http://arxiv.org/abs/1608.00892 | id:1608.00892 author:Liang Lu, Michelle Guo, Steve Renals category:cs.CL  published:2016-08-02 summary:Deep learning has significantly advanced state-of-the-art of speech recognition in the past few years. However, compared to conventional Gaussian mixture acoustic models, neural network models are usually much larger, and are therefore not very deployable in embedded devices. Previously, we investigated a compact highway deep neural network (HDNN) for acoustic modelling, which is a type of depth-gated feedforward neural network. We have shown that HDNN-based acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to plain deep neural network (DNN) acoustic models. In this paper, we push the boundary further by leveraging on the knowledge distillation technique that is also known as {\it teacher-student} training, i.e., we train the compact HDNN model with the supervision of a high accuracy cumbersome model. Furthermore, we also investigate sequence training and adaptation in the context of teacher-student training. Our experiments were performed on the AMI meeting speech recognition corpus. With this technique, we significantly improved the recognition accuracy of the HDNN acoustic model with less than 0.8 million parameters, and narrowed the gap between this model and the plain DNN with 30 million parameters. version:1
arxiv-1608-00887 | Towards Learning to Perceive and Reason About Liquids | http://arxiv.org/abs/1608.00887 | id:1608.00887 author:Connor Schenck, Dieter Fox category:cs.RO cs.CV  published:2016-08-02 summary:Recent advances in AI and robotics have claimed many incredible results with deep learning, yet no work to date has applied deep learning to the problem of liquid perception and reasoning. In this paper, we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids. We evaluate three models: a single-frame network, multi-frame network, and a LSTM recurrent network. Our results show that the best liquid detection results are achieved when aggregating data over multiple frames and that the LSTM network outperforms the other two in both tasks. This suggests that LSTM-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust, closed-loop controllers. version:1
arxiv-1608-00876 | Relational Similarity Machines | http://arxiv.org/abs/1608.00876 | id:1608.00876 author:Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed category:stat.ML cs.AI cs.LG  published:2016-08-02 summary:This paper proposes Relational Similarity Machines (RSM): a fast, accurate, and flexible relational learning framework for supervised and semi-supervised learning tasks. Despite the importance of relational learning, most existing methods are hard to adapt to different settings, due to issues with efficiency, scalability, accuracy, and flexibility for handling a wide variety of classification problems, data, constraints, and tasks. For instance, many existing methods perform poorly for multi-class classification problems, graphs that are sparsely labeled or network data with low relational autocorrelation. In contrast, the proposed relational learning framework is designed to be (i) fast for learning and inference at real-time interactive rates, and (ii) flexible for a variety of learning settings (multi-class problems), constraints (few labeled instances), and application domains. The experiments demonstrate the effectiveness of RSM for a variety of tasks and data. version:1
arxiv-1608-00866 | PageRank in Malware Categorization | http://arxiv.org/abs/1608.00866 | id:1608.00866 author:BooJoong Kang, Suleiman Y. Yerima, Kieran McLaughlin, Sakir Sezer category:cs.CR cs.LG  published:2016-08-02 summary:In this paper, we propose a malware categorization method that models malware behavior in terms of instructions using PageRank. PageRank computes ranks of web pages based on structural information and can also compute ranks of instructions that represent the structural information of the instructions in malware analysis methods. Our malware categorization method uses the computed ranks as features in machine learning algorithms. In the evaluation, we compare the effectiveness of different PageRank algorithms and also investigate bagging and boosting algorithms to improve the categorization accuracy. version:1
arxiv-1608-00860 | Hierarchically Compositional Kernels for Scalable Nonparametric Learning | http://arxiv.org/abs/1608.00860 | id:1608.00860 author:Jie Chen, Haim Avron, Vikas Sindhwani category:cs.LG stat.ML  published:2016-08-02 summary:We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the Nystr\"{o}m method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off-diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively, where $n$ is the number of training examples and $r$ is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller $r$. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions. version:1
arxiv-1608-00859 | Temporal Segment Networks: Towards Good Practices for Deep Action Recognition | http://arxiv.org/abs/1608.00859 | id:1608.00859 author:Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool category:cs.CV  published:2016-08-02 summary:Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( $ 69.4\% $) and UCF101 ($ 94.2\% $). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices. version:1
arxiv-1608-00853 | A study of the effect of JPG compression on adversarial images | http://arxiv.org/abs/1608.00853 | id:1608.00853 author:Gintare Karolina Dziugaite, Zoubin Ghahramani, Daniel M. Roy category:cs.CV cs.LG  published:2016-08-02 summary:Neural network image classifiers are known to be vulnerable to adversarial images, i.e., natural images which have been modified by an adversarial perturbation specifically designed to be imperceptible to humans yet fool the classifier. Not only can adversarial images be generated easily, but these images will often be adversarial for networks trained on disjoint subsets of data or with different architectures. Adversarial images represent a potential security risk as well as a serious machine learning challenge---it is clear that vulnerable neural networks perceive images very differently from humans. Noting that virtually every image classification data set is composed of JPG images, we evaluate the effect of JPG compression on the classification of adversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we found that JPG compression often reverses the drop in classification accuracy to a large extent, but not always. As the magnitude of the perturbations increases, JPG recompression alone is insufficient to reverse the effect. version:1
arxiv-1608-00848 | A New Android Malware Detection Approach Using Bayesian Classification | http://arxiv.org/abs/1608.00848 | id:1608.00848 author:Suleiman Y. Yerima, Sakir Sezer, Gavin McWilliams, Igor Muttik category:cs.CR cs.LG  published:2016-08-02 summary:Mobile malware has been growing in scale and complexity as smartphone usage continues to rise. Android has surpassed other mobile platforms as the most popular whilst also witnessing a dramatic increase in malware targeting the platform. A worrying trend that is emerging is the increasing sophistication of Android malware to evade detection by traditional signature-based scanners. As such, Android app marketplaces remain at risk of hosting malicious apps that could evade detection before being downloaded by unsuspecting users. Hence, in this paper we present an effective approach to alleviate this problem based on Bayesian classification models obtained from static code analysis. The models are built from a collection of code and app characteristics that provide indicators of potential malicious activities. The models are evaluated with real malware samples in the wild and results of experiments are presented to demonstrate the effectiveness of the proposed approach. version:1
arxiv-1608-00842 | Mitochondria-based Renal Cell Carcinoma Subtyping: Learning from Deep vs. Flat Feature Representations | http://arxiv.org/abs/1608.00842 | id:1608.00842 author:Peter J. Schüffler, Judy Sarungbam, Hassan Muhammad, Ed Reznik, Satish K. Tickoo, Thomas J. Fuchs category:cs.LG  published:2016-08-02 summary:Accurate subtyping of renal cell carcinoma (RCC) is of crucial importance for understanding disease progression and for making informed treatment decisions. New discoveries of significant alterations to mitochondria between subtypes make immunohistochemical (IHC) staining based image classification an imperative. Until now, accurate quantification and subtyping was made impossible by huge IHC variations, the absence of cell membrane staining for cytoplasm segmentation as well as the complete lack of systems for robust and reproducible image based classification. In this paper we present a comprehensive classification framework to overcome these challenges for tissue microarrays (TMA) of RCCs. We compare and evaluate models based on domain specific hand-crafted "flat"-features versus "deep" feature representations from various layers of a pre-trained convolutional neural network (CNN). The best model reaches a cross-validation accuracy of 89%, which demonstrates for the first time, that robust mitochondria-based subtyping of renal cancer is feasible version:1
arxiv-1608-00841 | Semantic Representations of Word Senses and Concepts | http://arxiv.org/abs/1608.00841 | id:1608.00841 author:José Camacho-Collados, Ignacio Iacobacci, Roberto Navigli, Mohammad Taher Pilehvar category:cs.CL  published:2016-08-02 summary:Representing the semantics of linguistic items in a machine-interpretable form has been a major goal of Natural Language Processing since its earliest days. Among the range of different linguistic items, words have attracted the most research attention. However, word representations have an important limitation: they conflate different meanings of a word into a single vector. Representations of word senses have the potential to overcome this inherent limitation. Indeed, the representation of individual word senses and concepts has recently gained in popularity with several experimental results showing that a considerable performance improvement can be achieved across different NLP applications upon moving from word level to the deeper sense and concept levels. Another interesting point regarding the representation of concepts and word senses is that these models can be seamlessly applied to other linguistic items, such as words, phrases and sentences. version:1
arxiv-1608-00835 | High Accuracy Android Malware Detection Using Ensemble Learning | http://arxiv.org/abs/1608.00835 | id:1608.00835 author:Suleiman Y. Yerima, Sakir Sezer, Igor Muttik category:cs.CR cs.LG  published:2016-08-02 summary:With over 50 billion downloads and more than 1.3 million apps in the Google official market, Android has continued to gain popularity amongst smartphone users worldwide. At the same time there has been a rise in malware targeting the platform, with more recent strains employing highly sophisticated detection avoidance techniques. As traditional signature based methods become less potent in detecting unknown malware, alternatives are needed for timely zero-day discovery. Thus this paper proposes an approach that utilizes ensemble learning for Android malware detection. It combines advantages of static analysis with the efficiency and performance of ensemble machine learning to improve Android malware detection accuracy. The machine learning models are built using a large repository of malware samples and benign apps from a leading antivirus vendor. Experimental results and analysis presented shows that the proposed method which uses a large feature space to leverage the power of ensemble learning is capable of 97.3 to 99 percent detection accuracy with very low false positive rates. version:1
arxiv-1608-00813 | Aggregating Binary Local Descriptors for Image Retrieval | http://arxiv.org/abs/1608.00813 | id:1608.00813 author:Giuseppe Amato, Fabrizio Falchi, Lucia Vadicamo category:cs.CV  published:2016-08-02 summary:In this paper, we report an extensive comparison among state-of-the-art aggregation methods applied to binary features. Then, we mathematically formalize the application of Fisher Kernels to Bernoulli Mixture Models. Finally, we investigate the combination of the aggregated binary features with the emerging Convolutional Neural Network (CNN) features. Our results show that aggregation methods on binary features are effective and represent a worthwhile alternative to the direct matching. Moreover, the combination of the CNN with the Fisher Vector (FV) built upon binary features allowed us to obtain a relative improvement over the CNN results that is in line with that recently obtained using the combination of the CNN with the FV built upon SIFTs. The advantage is that the extraction process of binary features is about two order of magnitude faster than SIFTs. version:1
arxiv-1607-08864 | The DLVHEX System for Knowledge Representation: Recent Advances (System Description) | http://arxiv.org/abs/1607.08864 | id:1607.08864 author:Christoph Redl category:cs.CL cs.AI cs.PL  published:2016-07-29 summary:The DLVHEX system implements the HEX-semantics, which integrates answer set programming (ASP) with arbitrary external sources. Since its first release ten years ago, significant advancements were achieved. Most importantly, the exploitation of properties of external sources led to efficiency improvements and flexibility enhancements of the language, and technical improvements on the system side increased user's convenience. In this paper, we present the current status of the system and point out the most important recent enhancements over early versions. While existing literature focuses on theoretical aspects and specific components, a bird's eye view of the overall system is missing. In order to promote the system for real-world applications, we further present applications which were already successfully realized on top of DLVHEX. This paper is under consideration for acceptance in Theory and Practice of Logic Programming. version:2
arxiv-1608-00797 | CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016 | http://arxiv.org/abs/1608.00797 | id:1608.00797 author:Yuanjun Xiong, Limin Wang, Zhe Wang, Bowen Zhang, Hang Song, Wei Li, Dahua Lin, Yu Qiao, Luc Van Gool, Xiaoou Tang category:cs.CV  published:2016-08-02 summary:This paper presents the method that underlies our submission to the untrimmed video classification task of ActivityNet Challenge 2016. We follow the basic pipeline of temporal segment networks and further raise the performance via a number of other techniques. Specifically, we use the latest deep model architecture, e.g., ResNet and Inception V3, and introduce new aggregation schemes (top-k and attention-weighted pooling). Additionally, we incorporate the audio as a complementary channel, extracting relevant information via a CNN applied to the spectrograms. With these techniques, we derive an ensemble of deep models, which, together, attains a high classification accuracy (mAP $93.23\%$) on the testing set and secured the first place in the challenge. version:1
arxiv-1608-00789 | New word analogy corpus for exploring embeddings of Czech words | http://arxiv.org/abs/1608.00789 | id:1608.00789 author:Lukáš Svoboda, Tomáš Brychcín category:cs.CL  published:2016-08-02 summary:The word embedding methods have been proven to be very useful in many tasks of NLP (Natural Language Processing). Much has been investigated about word embeddings of English words and phrases, but only little attention has been dedicated to other languages. Our goal in this paper is to explore the behavior of state-of-the-art word embedding methods on Czech, the language that is characterized by very rich morphology. We introduce new corpus for word analogy task that inspects syntactic, morphosyntactic and semantic properties of Czech words and phrases. We experiment with Word2Vec and GloVe algorithms and discuss the results on this corpus. The corpus is available for the research community. version:1
arxiv-1608-00785 | Shape and Centroid Independent Clustring Algorithm for Crowd Management Applications | http://arxiv.org/abs/1608.00785 | id:1608.00785 author:Yasser Mohammad Seddiq, A. A. Alharbiy, Moayyad Hamza Ghunaim category:cs.CV  published:2016-08-02 summary:Clustering techniques play an important role in data mining and its related applications. Among the challenging applications that require robust and real-time processing are crowd management and group trajectory applications. In this paper, a robust and low-complexity clustering algorithm is proposed. It is capable of processing data in a manner that is shape and centroid independent. The algorithm is of low complexity due to the novel technique to compute the matrix power. The algorithm was tested on real and synthetic data and test results are reported. version:1
arxiv-1608-00781 | Horn: A System for Parallel Training and Regularizing of Large-Scale Neural Networks | http://arxiv.org/abs/1608.00781 | id:1608.00781 author:Edward J. Yoon category:cs.DC cs.LG cs.NE  published:2016-08-02 summary:I introduce a new distributed system for effective training and regularizing of Large-Scale Neural Networks on distributed computing architectures. The experiments demonstrate the effectiveness of flexible model partitioning and parallelization strategies based on neuron-centric computation model, with an implementation of the collective and parallel dropout neural networks training. Experiments are performed on MNIST handwritten digits classification including results. version:1
arxiv-1608-00778 | Exponential Family Embeddings | http://arxiv.org/abs/1608.00778 | id:1608.00778 author:Maja R. Rudolph, Francisco J. R. Ruiz, Stephan Mandt, David M. Blei category:stat.ML cs.LG  published:2016-08-02 summary:Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications - neural activity of zebrafish, users' shopping behavior, and movie ratings - we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure. version:1
arxiv-1608-00775 | Dense semantic labeling of sub-decimeter resolution images with convolutional neural networks | http://arxiv.org/abs/1608.00775 | id:1608.00775 author:Michele Volpi, Devis Tuia category:cs.CV  published:2016-08-02 summary:Semantic labeling (or pixel-level land-cover classification) in ultra-high resolution imagery (< 10cm) requires statistical models able to learn high level concepts from spatial data, with large appearance variations. Convolutional Neural Networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction. In this paper we present a CNN-based system relying on an downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including i) state-of-the-art numerical accuracy, ii) improved geometric accuracy of predictions and iii) high efficiency at inference time. We test the proposed system on the Vaihingen and Potsdam sub-decimeter resolution datasets, involving semantic labeling of aerial images of 9cm and 5cm resolution, respectively. These datasets are composed by many large and fully annotated tiles allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures to the proposed one: standard patch classification, prediction of local label patches by employing only convolutions and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time. version:1
arxiv-1608-00762 | Interactive Removal and Ground Truth for Difficult Shadow Scenes | http://arxiv.org/abs/1608.00762 | id:1608.00762 author:Han Gong, Darren P. Cosker category:cs.CV  published:2016-08-02 summary:A user-centric method for fast, interactive, robust and high-quality shadow removal is presented. Our algorithm can perform detection and removal in a range of difficult cases: such as highly textured and colored shadows. To perform detection an on-the-fly learning approach is adopted guided by two rough user inputs for the pixels of the shadow and the lit area. After detection, shadow removal is performed by registering the penumbra to a normalized frame which allows us efficient estimation of non-uniform shadow illumination changes, resulting in accurate and robust removal. Another major contribution of this work is the first validated and multi-scene category ground truth for shadow removal algorithms. This data set containing 186 images eliminates inconsistencies between shadow and shadow-free images and provides a range of different shadow types such as soft, textured, colored and broken shadow. Using this data, the most thorough comparison of state-of-the-art shadow removal methods to date is performed, showing our proposed new algorithm to outperform the state-of-the-art across several measures and shadow category. To complement our dataset, an online shadow removal benchmark website is also presented to encourage future open comparisons in this challenging field of research. version:1
arxiv-1608-00753 | Semantically Guided Depth Upsampling | http://arxiv.org/abs/1608.00753 | id:1608.00753 author:Nick Schneider, Lukas Schneider, Peter Pinggera, Uwe Franke, Marc Pollefeys, Christoph Stiller category:cs.CV  published:2016-08-02 summary:We present a novel method for accurate and efficient up- sampling of sparse depth data, guided by high-resolution imagery. Our approach goes beyond the use of intensity cues only and additionally exploits object boundary cues through structured edge detection and semantic scene labeling for guidance. Both cues are combined within a geodesic distance measure that allows for boundary-preserving depth in- terpolation while utilizing local context. We model the observed scene structure by locally planar elements and formulate the upsampling task as a global energy minimization problem. Our method determines glob- ally consistent solutions and preserves fine details and sharp depth bound- aries. In our experiments on several public datasets at different levels of application, we demonstrate superior performance of our approach over the state-of-the-art, even for very sparse measurements. version:1
arxiv-1608-00737 | Context Discovery for Model Learning in Partially Observable Environments | http://arxiv.org/abs/1608.00737 | id:1608.00737 author:Nikolas J. Hemion category:cs.RO cs.AI cs.LG  published:2016-08-02 summary:The ability to learn a model is essential for the success of autonomous agents. Unfortunately, learning a model is difficult in partially observable environments, where latent environmental factors influence what the agent observes. In the absence of a supervisory training signal, autonomous agents therefore require a mechanism to autonomously discover these environmental factors, or sensorimotor contexts. This paper presents a method to discover sensorimotor contexts in partially observable environments, by constructing a hierarchical transition model. The method is evaluated in a simulation experiment, in which a robot learns that different rooms are characterized by different objects that are found in them. version:1
arxiv-1607-08692 | A Novel Bilingual Word Embedding Method for Lexical Translation Using Bilingual Sense Clique | http://arxiv.org/abs/1607.08692 | id:1607.08692 author:Rui Wang, Hai Zhao, Sabine Ploux, Bao-Liang Lu, Masao Utiyama, Eiichiro Sumita category:cs.CL  published:2016-07-29 summary:Most of the existing methods for bilingual word embedding only consider shallow context or simple co-occurrence information. In this paper, we propose a latent bilingual sense unit (Bilingual Sense Clique, BSC), which is derived from a maximum complete sub-graph of pointwise mutual information based graph over bilingual corpus. In this way, we treat source and target words equally and a separated bilingual projection processing that have to be used in most existing works is not necessary any more. Several dimension reduction methods are evaluated to summarize the BSC-word relationship. The proposed method is evaluated on bilingual lexicon translation tasks and empirical results show that bilingual sense embedding methods outperform existing bilingual word embedding methods. version:2
arxiv-1608-00712 | Size-Consistent Statistics for Anomaly Detection in Dynamic Networks | http://arxiv.org/abs/1608.00712 | id:1608.00712 author:Timothy La Fond, Jennifer Neville, Brian Gallagher category:cs.LG  published:2016-08-02 summary:An important task in network analysis is the detection of anomalous events in a network time series. These events could merely be times of interest in the network timeline or they could be examples of malicious activity or network malfunction. Hypothesis testing using network statistics to summarize the behavior of the network provides a robust framework for the anomaly detection decision process. Unfortunately, choosing network statistics that are dependent on confounding factors like the total number of nodes or edges can lead to incorrect conclusions (e.g., false positives and false negatives). In this dissertation we describe the challenges that face anomaly detection in dynamic network streams regarding confounding factors. We also provide two solutions to avoiding error due to confounding factors: the first is a randomization testing method that controls for confounding factors, and the second is a set of size-consistent network statistics which avoid confounding due to the most common factors, edge count and node count. version:1
arxiv-1608-00704 | Identifiable Phenotyping using Constrained Non-Negative Matrix Factorization | http://arxiv.org/abs/1608.00704 | id:1608.00704 author:Shalmali Joshi, Suriya Gunasekar, David Sontag, Joydeep Ghosh category:stat.ML cs.LG  published:2016-08-02 summary:This work proposes a new algorithm for automated and simultaneous phenotyping of multiple co-occurring medical conditions, also referred as comorbidities, using clinical notes from the electronic health records (EHRs). A basic latent factor estimation technique of non-negative matrix factorization (NMF) is augmented with domain specific constraints to obtain sparse latent factors that are anchored to a fixed set of chronic conditions. The proposed anchoring mechanism ensures a one-to-one identifiable and interpretable mapping between the latent factors and the target comorbidities. Qualitative assessment of the empirical results by clinical experts suggests that the proposed model learns clinically interpretable phenotypes while being predictive of 30 day mortality. The proposed method can be readily adapted to any non-negative EHR data across various healthcare institutions. version:1
arxiv-1608-00700 | A Survey of Visual Analysis of Human Motion and Its Applications | http://arxiv.org/abs/1608.00700 | id:1608.00700 author:Qifei Wang category:cs.CV cs.AI  published:2016-08-02 summary:This paper summarizes the recent progress in human motion analysis and its applications. In the beginning, we reviewed the motion capture systems and the representation model of human's motion data. Next, we sketched the advanced human motion data processing technologies, including motion data filtering, temporal alignment, and segmentation. The following parts overview the state-of-the-art approaches of action recognition and dynamics measuring since these two are the most active research areas in human motion analysis. The last part discusses some emerging applications of the human motion analysis in healthcare, human robot interaction, security surveillance, virtual reality and animation. The promising research topics of human motion analysis in the future is also summarized in the last part. version:1
arxiv-1608-00696 | Can we trust the bootstrap in high-dimension? | http://arxiv.org/abs/1608.00696 | id:1608.00696 author:Noureddine El Karoui, Elizabeth Purdom category:stat.ME math.ST stat.ML stat.TH 62G09  62J99  published:2016-08-02 summary:We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where $p<n$ but $p/n$ is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of $\beta$? (where $\beta$ is the true regression vector). We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression -- residual bootstrap and pairs bootstrap -- give very poor inference on $\beta$ as the ratio $p/n$ grows. We find that the residuals bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio $p/n$ grows. We also show that the jackknife resampling technique for estimating the variance of $\hat{\beta}$ severely overestimates the variance in high dimensions. We contribute alternative bootstrap procedures based on our theoretical results that mitigate these problems. However, the corrections depend on assumptions regarding the underlying data-generation model, suggesting that in high-dimensions it may be difficult to have universal, robust bootstrapping techniques. version:1
arxiv-1608-00075 | Online Nonnegative Matrix Factorization with General Divergences | http://arxiv.org/abs/1608.00075 | id:1608.00075 author:Renbo Zhao, Vincent Y. F. Tan, Huan Xu category:stat.ML cs.IT cs.NA math.IT math.OC  published:2016-07-30 summary:We develop a unified and systematic framework for performing online nonnegative matrix factorization under a wide variety of important divergences. The online nature of our algorithm makes it particularly amenable to large-scale data. We prove that the sequence of learned dictionaries converges almost surely to the set of critical points of the expected loss function. We do so by leveraging the theory of stochastic approximations and projected dynamical systems. This result substantially generalizes the previous results obtained only for the squared-$\ell_2$ loss. Moreover, the novel techniques involved in our analysis open new avenues for analyzing similar matrix factorization problems. The computational efficiency and the quality of the learned dictionary of our algorithm are verified empirically on both synthetic and real datasets. In particular, on the tasks of topic learning, shadow removal and image denoising, our algorithm achieves superior trade-offs between the quality of learned dictionary and running time over the batch and other online NMF algorithms. version:2
arxiv-1608-00668 | Global Vertices and the Noising Paradox | http://arxiv.org/abs/1608.00668 | id:1608.00668 author:Konstantinos A. Raftopoulos, Stefanos D. Kollias, Marin Ferecatu category:cs.CV  published:2016-08-02 summary:A theoretical and experimental analysis related to the identification of vertices of unknown shapes is presented. Shapes are seen as real functions of their closed boundary. Unlike traditional approaches, which see curvature as the rate of change of the tangent to the curve, an alternative global perspective of curvature is examined providing insight into the process of noise-enabled vertex localization. The analysis leads to a paradox, that certain vertices can be localized better in the presence of noise. The concept of noising is thus considered and a relevant global method for localizing "Global Vertices" is investigated. Theoretical analysis reveals that induced noise can help localizing certain vertices if combined with global descriptors. Experiments with noise and a comparison to localized methods validate the theoretical results. version:1
arxiv-1608-00667 | Can Active Learning Experience Be Transferred? | http://arxiv.org/abs/1608.00667 | id:1608.00667 author:Hong-Min Chu, Hsuan-Tien Lin category:cs.LG cs.AI  published:2016-08-02 summary:Active learning is an important machine learning problem in reducing the human labeling effort. Current active learning strategies are designed from human knowledge, and are applied on each dataset in an immutable manner. In other words, experience about the usefulness of strategies cannot be updated and transferred to improve active learning on other datasets. This paper initiates a pioneering study on whether active learning experience can be transferred. We first propose a novel active learning model that linearly aggregates existing strategies. The linear weights can then be used to represent the active learning experience. We equip the model with the popular linear upper- confidence-bound (LinUCB) algorithm for contextual bandit to update the weights. Finally, we extend our model to transfer the experience across datasets with the technique of biased regularization. Empirical studies demonstrate that the learned experience not only is competitive with existing strategies on most single datasets, but also can be transferred across datasets to improve the performance on future learning tasks. version:1
arxiv-1607-08691 | A Non-Parametric Learning Approach to Identify Online Human Trafficking | http://arxiv.org/abs/1607.08691 | id:1607.08691 author:Hamidreza Alvari, Paulo Shakarian, J. E. Kelly Snyder category:cs.LG stat.ML  published:2016-07-29 summary:Human trafficking is among the most challenging law enforcement problems which demands persistent fight against from all over the globe. In this study, we leverage readily available data from the website "Backpage"-- used for classified advertisement-- to discern potential patterns of human trafficking activities which manifest online and identify most likely trafficking related advertisements. Due to the lack of ground truth, we rely on two human analysts --one human trafficking victim survivor and one from law enforcement, for hand-labeling the small portion of the crawled data. We then present a semi-supervised learning approach that is trained on the available labeled and unlabeled data and evaluated on unseen data with further verification of experts. version:2
arxiv-1608-00647 | Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests | http://arxiv.org/abs/1608.00647 | id:1608.00647 author:Narges Razavian, Jake Marcus, David Sontag category:cs.LG  published:2016-08-02 summary:Disparate areas of machine learning have benefited from models that can take raw data with little preprocessing as input and learn rich representations of that raw data in order to perform well on a given prediction task. We evaluate this approach in healthcare by using longitudinal measurements of lab tests, one of the more raw signals of a patient's health state widely available in clinical data, to predict disease onsets. In particular, we train a Long-Short Term Memory (LSTM) network and two novel convolutional neural networks for multi-task prediction of disease onset for 133 conditions based on 18 common lab tests measured over time in a cohort of 298K patients derived from 8 years of administrative claims data. We compare the neural networks to a logistic regression with several hand-engineered, clinically relevant features. We find that the representation-based learning approaches significantly outperform this baseline. We believe that our work suggests a new avenue of patient risk stratification based solely on lab results. version:1
arxiv-1608-00627 | Learning Transferable Policies for Monocular Reactive MAV Control | http://arxiv.org/abs/1608.00627 | id:1608.00627 author:Shreyansh Daftry, J. Andrew Bagnell, Martial Hebert category:cs.RO cs.AI cs.LG  published:2016-08-01 summary:The ability to transfer knowledge gained in previous tasks into new contexts is one of the most important mechanisms of human learning. Despite this, adapting autonomous behavior to be reused in partially similar settings is still an open problem in current robotics research. In this paper, we take a small step in this direction and propose a generic framework for learning transferable motion policies. Our goal is to solve a learning problem in a target domain by utilizing the training data in a different but related source domain. We present this in the context of an autonomous MAV flight using monocular reactive control, and demonstrate the efficacy of our proposed approach through extensive real-world flight experiments in outdoor cluttered environments. version:1
arxiv-1608-00624 | Oracle Inequalities for High-dimensional Prediction | http://arxiv.org/abs/1608.00624 | id:1608.00624 author:Johannes Lederer, Lu Yu, Irina Gaynanova category:math.ST stat.ML stat.TH  published:2016-08-01 summary:The abundance of high-dimensional data in the modern sciences has generated tremendous interest in penalized estimators such as the lasso, scaled lasso, square-root lasso, elastic net, and many others. However, the common theoretical bounds for the predictive performance of these estimators hinge on strong, in practice unverifiable assumptions on the design. In this paper, we introduce a new set of oracle inequalities for prediction in high-dimensional linear regression. These bounds hold irrespective of the design matrix. Moreover, since the proofs rely only on convexity and continuity arguments, the bounds apply to a wide range of penalized estimators. Overall, the bounds demonstrate that generic estimators can provide consistent prediction with any design matrix. From a practical point of view, the bounds can help to identify the potential of specific estimators, and they can help to get a sense of the prediction accuracy in a given application. version:1
arxiv-1608-00621 | Energy-Economic Multiple Incremental/Decremental Kernel Ridge Regression for Green Clouds | http://arxiv.org/abs/1608.00621 | id:1608.00621 author:Bo-Wei Chen category:cs.LG stat.ML  published:2016-08-01 summary:This study presents an energy-economic approach for incremental/decremental learning based on kernel ridge regression, a frequently used regressor on clouds. To avoid reanalyzing the entire dataset when data change, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). Moreover, incremental/decremental analyses in empirical and intrinsic space are also introduced to handle with data matrices with a large number of samples or feature dimensions. At the end of this study, we further the proposed mechanism to statistical Kernelized Bayesian Regression, so that incremental/decremental analyses become applicable. version:1
arxiv-1608-00619 | Efficient Multiple Incremental and Decremental Ridge Support Vector Machines for Big Streams | http://arxiv.org/abs/1608.00619 | id:1608.00619 author:Bo-Wei Chen category:cs.LG stat.ML  published:2016-08-01 summary:This study presents a rapid multiple incremental and decremental mechanism based on WECs for support-vector analysis. The proposed method examines the characteristics of Ridge SVMs and devises a recursion-free function for predicting the Lagrangian multipliers of new samples. Such a function is derived from WECs and Ridge SVMs. With this function, all of the new Lagrangian multipliers can be computed at once without using any gradual step sizes. Moreover, such a function can relax a constraint, where the increment of all the new Lagrangian multipliers should be the same, thereby easily satisfying the requirement of KKT conditions. The ridge parameter of Ridge SVMs stabilizes the computation of inverse matrices during the update of existing Lagrangian multipliers. version:1
arxiv-1608-00612 | Structured prediction models for RNN based sequence labeling in clinical text | http://arxiv.org/abs/1608.00612 | id:1608.00612 author:Abhyuday Jagannatha, Hong Yu category:cs.CL  published:2016-08-01 summary:Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies for structured prediction in order to improve the exact phrase detection of various medical entities. version:1
arxiv-1608-00611 | Attention Tree: Learning Hierarchies of Visual Features for Large-Scale Image Recognition | http://arxiv.org/abs/1608.00611 | id:1608.00611 author:Priyadarshini Panda, Kaushik Roy category:cs.CV cs.LG cs.NE  published:2016-08-01 summary:One of the key challenges in machine learning is to design a computationally efficient multi-class classifier while maintaining the output accuracy and performance. In this paper, we present a tree-based classifier: Attention Tree (ATree) for large-scale image classification that uses recursive Adaboost training to construct a visual attention hierarchy. The proposed attention model is inspired from the biological 'selective tuning mechanism for cortical visual processing'. We exploit the inherent feature similarity across images in datasets to identify the input variability and use recursive optimization procedure, to determine data partitioning at each node, thereby, learning the attention hierarchy. A set of binary classifiers is organized on top of the learnt hierarchy to minimize the overall test-time complexity. The attention model maximizes the margins for the binary classifiers for optimal decision boundary modelling, leading to better performance at minimal complexity. The proposed framework has been evaluated on both Caltech-256 and SUN datasets and achieves accuracy improvement over state-of-the-art tree-based methods at significantly lower computational cost. version:1
arxiv-1608-00554 | Generalized Determinantal Point Processes: The Linear Case | http://arxiv.org/abs/1608.00554 | id:1608.00554 author:Damian Straszak, Nisheeth K. Vishnoi category:cs.DS math.PR stat.ML  published:2016-08-01 summary:A determinantal point process (DPP) over a universe $\{1,\ldots,m\}$ with respect to an $m \times m$ positive semidefinite matrix $L$ is a probability distribution where the probability of a subset $S \subseteq \{1,\ldots,m\}$ is proportional to the determinant of the principal minor of $L$ corresponding to $S.$ DPPs encapsulate a wide variety of known distributions and appear naturally (and surprisingly) in a wide variety of areas such as physics, mathematics and computer science. Several applications that use DPPs rely on the fact that they are computationally tractable -- i.e., there are algorithms for sampling from DPPs efficiently. Recently, there is growing interest in studying a generalization of DPPs in which the support of the distribution is a restricted family B of subsets of $\{1,2,\ldots, m\}$. Mathematically, these distributions, which we call generalized DPPs, include the well-studied hardcore distributions as special cases (when $L$ is diagonal). In applications, they can be used to refine models based on DPPs by imposing combinatorial constraints on the support of the distribution. In this paper we take first steps in a systematic study of computational questions concerning generalized DPPs. We introduce a natural class of linear families: roughly, a family B is said to be linear if there is a collection of $p$ linear forms that all elements of B satisfy. Important special cases of linear families are all sets of cardinality $k$ -- giving rise to $k$-DPPs -- and, more generally, partition matroids. On the positive side, we prove that, when $p$ is a constant, there is an efficient, exact sampling algorithm for linear DPPs. We complement these results by proving that, when $p$ is large, the computational problem related to such DPPs becomes $\#$P-hard. Our proof techniques rely and build on the interplay between polynomials and probability distributions. version:1
arxiv-1608-00550 | Theory of the GMM Kernel | http://arxiv.org/abs/1608.00550 | id:1608.00550 author:Ping Li, Cun-Hui Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT  published:2016-08-01 summary:We develop some theoretical results for a robust similarity measure named "generalized min-max" (GMM). This similarity has direct applications in machine learning as a positive definite kernel and can be efficiently computed via probabilistic hashing. Owing to the discrete nature, the hashed values can also be used for efficient near neighbor search. We prove the theoretical limit of GMM and the consistency result, assuming that the data follow an elliptical distribution, which is a very general family of distributions and includes the multivariate $t$-distribution as a special case. The consistency result holds as long as the data have bounded first moment (an assumption which essentially holds for datasets commonly encountered in practice). Furthermore, we establish the asymptotic normality of GMM. Compared to the "cosine" similarity which is routinely adopted in current practice in statistics and machine learning, the consistency of GMM requires much weaker conditions. Interestingly, when the data follow the $t$-distribution with $\nu$ degrees of freedom, GMM typically provides a better measure of similarity than "cosine" roughly when $\nu<8$ (which is already very close to normal). These theoretical results will help explain the recent success of GMM in learning tasks. version:1
arxiv-1608-00530 | Visible Progress on Adversarial Images and a New Saliency Map | http://arxiv.org/abs/1608.00530 | id:1608.00530 author:Dan Hendrycks, Kevin Gimpel category:cs.LG cs.CR cs.CV cs.NE  published:2016-08-01 summary:Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change the prediction of a classifier without causing the input to appear substantially different to the human perceptual system. We make progress on this AI Safety problem as it relates to image classification by training on images after a simple conversion to the YUV colorspace. We demonstrate that adversarial perturbations which modify YUV images are more conspicuous and less pathological than in RGB space. We then show how that whitening RGB images lets us visually see the difference fooling and benign images. Last we introduce a new saliency map to better understand misclassification. version:1
arxiv-1608-00525 | Modeling Context Between Objects for Referring Expression Understanding | http://arxiv.org/abs/1608.00525 | id:1608.00525 author:Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis category:cs.CV  published:2016-08-01 summary:Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region. version:1
arxiv-1608-00508 | Improving Phoneme segmentation with Recurrent Neural Networks | http://arxiv.org/abs/1608.00508 | id:1608.00508 author:Paul Michel, Okko Räsänen, Roland Thiollière, Emmanuel Dupoux category:cs.CL  published:2016-08-01 summary:Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods. version:1
arxiv-1608-00507 | Top-down Neural Attention by Excitation Backprop | http://arxiv.org/abs/1608.00507 | id:1608.00507 author:Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff category:cs.CV  published:2016-08-01 summary:We aim to model the top-down attention of a Convolutional Neural Network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. In experiments, we demonstrate the accuracy and generalizability of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. version:1
arxiv-1608-00501 | Supervised Classification of RADARSAT-2 Polarimetric Data for Different Land Features | http://arxiv.org/abs/1608.00501 | id:1608.00501 author:Abhishek Maity category:cs.CV I.4; I.5.3  published:2016-08-01 summary:The pixel percentage belonging to the user defined area that are assigned to cluster in a confusion matrix for RADARSAT-2 over Vancouver area has been analysed for classification. In this study, supervised Wishart and Support Vector Machine (SVM) classifiers over RADARSAT-2 (RS2) fine quadpol mode Single Look Complex (SLC) product data is computed and compared. In comparison with conventional single channel or dual channel polarization, RADARSAT-2 is fully polarimetric, making it to offer better land feature contrast for classification operation. version:1
arxiv-1607-08362 | Incremental Noising and its Fractal Behavior | http://arxiv.org/abs/1607.08362 | id:1607.08362 author:Konstantinos A. Raftopoulos, Marin Ferecatu, Dionyssios D. Sourlas, Stefanos D. Kollias category:cs.CV  published:2016-07-28 summary:This manuscript is about further elucidating the concept of noising. The concept of noising first appeared in \cite{CVPR14}, in the context of curvature estimation and vertex localization on planar shapes. There are indications that noising can play for global methods the role smoothing plays for local methods in this task. This manuscript is about investigating this claim by introducing incremental noising, in a recursive deterministic manner, analogous to how smoothing is extended to progressive smoothing in similar tasks. As investigating the properties and behavior of incremental noising is the purpose of this manuscript, a surprising connection between incremental noising and progressive smoothing is revealed by the experiments. To explain this phenomenon, the fractal and the space filling properties of the two methods respectively, are considered in a unifying context. version:2
arxiv-1608-00486 | Exploiting Temporal Information for DCNN-based Fine-Grained Object Classification | http://arxiv.org/abs/1608.00486 | id:1608.00486 author:ZongYuan Ge, Chris McCool, Conrad Sanderson, Peng Wang, Lingqiao Liu, Ian Reid, Peter Corke category:cs.CV cs.MM I.2.6; I.4; I.5  published:2016-08-01 summary:Fine-grained classification is a relatively new field that has concentrated on using information from a single image, while ignoring the enormous potential of using video data to improve classification. In this work we present the novel task of video-based fine-grained object classification, propose a corresponding new video dataset, and perform a systematic study of several recent deep convolutional neural network (DCNN) based approaches, which we specifically adapt to the task. We evaluate three-dimensional DCNNs, two-stream DCNNs, and bilinear DCNNs. Two forms of the two-stream approach are used, where spatial and temporal data from two independent DCNNs are fused either via early fusion (combination of the fully-connected layers) and late fusion (concatenation of the softmax outputs of the DCNNs). For bilinear DCNNs, information from the convolutional layers of the spatial and temporal DCNNs is combined via local co-occurrences. We then fuse the bilinear DCNN and early fusion of the two-stream approach to combine the spatial and temporal information at the local and global level (Spatio-Temporal Co-occurrence). Using the new and challenging video dataset of birds, classification performance is improved from 23.1% (using single images) to 41.1% when using the Spatio-Temporal Co-occurrence system. Incorporating automatically detected bounding box location further improves the classification accuracy to 53.6%. version:1
arxiv-1607-08539 | Structured Global Registration of RGB-D Scans in Indoor Environments | http://arxiv.org/abs/1607.08539 | id:1607.08539 author:Maciej Halber, Thomas Funkhouser category:cs.CV  published:2016-07-28 summary:RGB-D scanning of indoor environments (offices, homes, museums, etc.) is important for a variety of applications, including on-line real estate, virtual tourism, and virtual reality. To support these applications, we must register the RGB-D images acquired with an untracked, hand-held camera into a globally consistent and accurate 3D model. Current methods work effectively for small environments with trackable features, but often fail to reproduce large-scale structures (e.g., straight walls along corridors) or long-range relationships (e.g., parallel opposing walls in an office). In this paper, we investigate the idea of integrating a structural model into the global registration process. We introduce a fine-to-coarse algorithm that detects planar structures spanning multiple RGB-D frames and establishes geometric constraints between them as they become aligned. Detection and enforcement of these structural constraints in the inner loop of a global registration algorithm guides the solution towards more accurate global registrations, even without detecting loop closures. During experiments with a newly created benchmark for the SUN3D dataset, we find that this approach produces registration results with greater accuracy and better robustness than previous alternatives. version:2
arxiv-1608-00470 | Labeling Topics with Images using Neural Networks | http://arxiv.org/abs/1608.00470 | id:1608.00470 author:Nikolaos Aletras, Arpit Mittal category:cs.CL cs.CV  published:2016-08-01 summary:Topics generated by topic models are usually represented by lists of $t$ terms or alternatively using short phrases and images. The current state-of-the-art work on labeling topics using images selects images by re-ranking a small set of candidates for a given topic. In this paper, we present a more generic method that can estimate the degree of association between any arbitrary pair of an unseen topic and image using a deep neural network. Our method has better runtime performance $O(n)$ compared to $O(n^2)$ for the current state-of-the-art method, and is also significantly more accurate. version:1
arxiv-1608-00466 | Learning Semantically Coherent and Reusable Kernels in Convolution Neural Nets for Sentence Classification | http://arxiv.org/abs/1608.00466 | id:1608.00466 author:Madhusudan Lakshmana, Sundararajan Sellamanickam, Shirish Shevade, Keerthi Selvaraj category:cs.CL cs.LG cs.NE  published:2016-08-01 summary:The state-of-the-art CNN models give good performance on sentence classification tasks. The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and reusability of CNNs in these tasks. Semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned CNN model. We observe that the learned kernels do not have semantic coherence. Motivated by this observation, we propose to learn kernels with semantic coherence using clustering scheme combined with Word2Vec representation and domain knowledge such as SentiWordNet. We suggest a technique to visualize attention mechanism of CNNs for decision explanation purpose. Reusable property enables kernels learned on one problem to be used in another problem. This helps in efficient learning as only a few additional domain specific filters may have to be learned. We demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets. Experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties. version:1
arxiv-1608-00441 | Kernel Risk-Sensitive Loss: Definition, Properties and Application to Robust Adaptive Filtering | http://arxiv.org/abs/1608.00441 | id:1608.00441 author:Badong Chen, Lei Xing, Bin Xu, Haiquan Zhao, Nanning Zheng, Jose C. Principe category:stat.ML  published:2016-08-01 summary:Nonlinear similarity measures defined in kernel space, such as correntropy, can extract higher-order statistics of data and offer potentially significant performance improvement over their linear counterparts especially in non-Gaussian signal processing and machine learning. In this work, we propose a new similarity measure in kernel space, called the kernel risk-sensitive loss (KRSL), and provide some important properties. We apply the KRSL to adaptive filtering and investigate the robustness, and then develop the MKRSL algorithm and analyze the mean square convergence performance. Compared with correntropy, the KRSL can offer a more efficient performance surface, thereby enabling a gradient based method to achieve faster convergence speed and higher accuracy while still maintaining the robustness to outliers. Theoretical analysis results and superior performance of the new algorithm are confirmed by simulation. version:1
arxiv-1607-05947 | Color Homography Color Correction | http://arxiv.org/abs/1607.05947 | id:1607.05947 author:Graham D. Finlayson, Han Gong, Robert B. Fisher category:cs.CV  published:2016-07-20 summary:Homographies -- a mathematical formalism for relating image points across different camera viewpoints -- are at the foundations of geometric methods in computer vision and are used in geometric camera calibration, image registration, and stereo vision and other tasks. In this paper, we show the surprising result that colors across a change in viewing condition (changing light color, shading and camera) are also related by a homography. We propose a new color correction method based on color homography. Experiments demonstrate that solving the color homography problem leads to more accurate calibration. version:3
arxiv-1608-00367 | Accelerating the Super-Resolution Convolutional Neural Network | http://arxiv.org/abs/1608.00367 | id:1608.00367 author:Chao Dong, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2016-08-01 summary:As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors. version:1
arxiv-1608-00361 | Fast and robust pushbroom hyperspectral imaging via DMD-based scanning | http://arxiv.org/abs/1608.00361 | id:1608.00361 author:Reza Arablouei, Ethan Goan, Stephen Gensemer, Branislav Kusy category:cs.CV cs.DC physics.optics  published:2016-08-01 summary:We describe a new pushbroom hyperspectral imaging device that has no macro moving part. The main components of the proposed hyperspectral imager are a digital micromirror device (DMD), a CMOS image sensor with no filter as the spectral sensor, a CMOS color (RGB) image sensor as the auxiliary image sensor, and a diffraction grating. Using the image sensor pair, the device can simultaneously capture hyperspectral data as well as RGB images of the scene. The RGB images captured by the auxiliary image sensor can facilitate geometric co-registration of the hyperspectral image slices captured by the spectral sensor. In addition, the information discernible from the RGB images can lead to capturing the spectral data of only the regions of interest within the scene. The proposed hyperspectral imaging architecture is cost-effective, fast, and robust. It also enables a trade-off between resolution and speed. We have built an initial prototype based on the proposed design. The prototype can capture a hyperspectral image datacube with a spatial resolution of 192x192 pixels and a spectral resolution of 500 bands in less than thirty seconds. version:1
arxiv-1608-00359 | Discovering Latent States for Model Learning: Applying Sensorimotor Contingencies Theory and Predictive Processing to Model Context | http://arxiv.org/abs/1608.00359 | id:1608.00359 author:Nikolas J. Hemion category:cs.RO cs.AI cs.LG  published:2016-08-01 summary:Autonomous robots need to be able to adapt to unforeseen situations and to acquire new skills through trial and error. Reinforcement learning in principle offers a suitable methodological framework for this kind of autonomous learning. However current computational reinforcement learning agents mostly learn each individual skill entirely from scratch. How can we enable artificial agents, such as robots, to acquire some form of generic knowledge, which they could leverage for the learning of new skills? This paper argues that, like the brain, the cognitive system of artificial agents has to develop a world model to support adaptive behavior and learning. Inspiration is taken from two recent developments in the cognitive science literature: predictive processing theories of cognition, and the sensorimotor contingencies theory of perception. Based on these, a hypothesis is formulated about what the content of information might be that is encoded in an internal world model, and how an agent could autonomously acquire it. A computational model is described to formalize this hypothesis, and is evaluated in a series of simulation experiments. version:1
arxiv-1608-00354 | hdm: High-Dimensional Metrics | http://arxiv.org/abs/1608.00354 | id:1608.00354 author:Victor Chernozhukov, Chris Hansen, Martin Spindler category:stat.ME stat.ML  published:2016-08-01 summary:In this article the package High-dimensional Metrics (\texttt{hdm}) is introduced. It is a collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these parameters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included. version:1
arxiv-1608-00339 | Crowd-sourcing NLG Data: Pictures Elicit Better Data | http://arxiv.org/abs/1608.00339 | id:1608.00339 author:Jekaterina Novikova, Oliver Lemon, Verena Rieser category:cs.CL  published:2016-08-01 summary:Recent advances in corpus-based Natural Language Generation (NLG) hold the promise of being easily portable across domains, but require costly training data, consisting of meaning representations (MRs) paired with Natural Language (NL) utterances. In this work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and evaluating different MRs with which to elicit data. We show that pictorial MRs result in better NL data being collected than logic-based MRs: utterances elicited by pictorial MRs are judged as significantly more natural, more informative, and better phrased, with a significant increase in average quality ratings (around 0.5 points on a 6-point scale), compared to using the logical MRs. As the MR becomes more complex, the benefits of pictorial stimuli increase. The collected data will be released as part of this submission. version:1
arxiv-1608-00318 | A Neural Knowledge Language Model | http://arxiv.org/abs/1608.00318 | id:1608.00318 author:Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, Yoshua Bengio category:cs.CL cs.LG  published:2016-08-01 summary:Communicating knowledge is a primary purpose of language. However, current language models have significant limitations in their ability to encode or decode knowledge. This is mainly because they acquire knowledge based on statistical co-occurrences, even if most of the knowledge words are rarely observed named entities. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by knowledge graphs with RNN language models. At each time step, the model predicts a fact on which the observed word is supposed to be based. Then, a word is either generated from the vocabulary or copied from the knowledge graph. We train and test the model on a new dataset, WikiFacts. In experiments, we show that the NKLM significantly improves the perplexity while generating a much smaller number of unknown words. In addition, we demonstrate that the sampled descriptions include named entities which were used to be the unknown words in RNN language models. version:1
arxiv-1608-00310 | Video Summarization in a Multi-View Camera Network | http://arxiv.org/abs/1608.00310 | id:1608.00310 author:Rameswar Panda, Abir Das, Amit K. Roy-Chowdhury category:cs.CV  published:2016-08-01 summary:While most existing video summarization approaches aim to extract an informative summary of a single video, we propose a novel framework for summarizing multi-view videos by exploiting both intra- and inter-view content correlations in a joint embedding space. We learn the embedding by minimizing an objective function that has two terms: one due to intra-view correlations and another due to inter-view correlations across the multiple views. The solution can be obtained directly by solving one Eigen-value problem that is linear in the number of multi-view videos. We then employ a sparse representative selection approach over the learned embedding space to summarize the multi-view videos. Experimental results on several benchmark datasets demonstrate that our proposed approach clearly outperforms the state-of-the-art. version:1
arxiv-1608-00293 | Left-corner Methods for Syntactic Modeling with Universal Structural Constraints | http://arxiv.org/abs/1608.00293 | id:1608.00293 author:Hiroshi Noji category:cs.CL  published:2016-08-01 summary:The primary goal in this thesis is to identify better syntactic constraint or bias, that is language independent but also efficiently exploitable during sentence processing. We focus on a particular syntactic construction called center-embedding, which is well studied in psycholinguistics and noted to cause particular difficulty for comprehension. Since people use language as a tool for communication, one expects such complex constructions to be avoided for communication efficiency. From a computational perspective, center-embedding is closely relevant to a left-corner parsing algorithm, which can capture the degree of center-embedding of a parse tree being constructed. This connection suggests left-corner methods can be a tool to exploit the universal syntactic constraint that people avoid generating center-embedded structures. We explore such utilities of center-embedding as well as left-corner methods extensively through several theoretical and empirical examinations. Our primary task is unsupervised grammar induction. In this task, the input to the algorithm is a collection of sentences, from which the model tries to extract the salient patterns on them as a grammar. This is a particularly hard problem although we expect the universal constraint may help in improving the performance since it can effectively restrict the possible search space for the model. We build the model by extending the left-corner parsing algorithm for efficiently tabulating the search space except those involving center-embedding up to a specific degree. We examine the effectiveness of our approach on many treebanks, and demonstrate that often our constraint leads to better parsing performance. We thus conclude that left-corner methods are particularly useful for syntax-oriented systems, as it can exploit efficiently the inherent universal constraints in languages. version:1
arxiv-1608-00279 | Neural shrinkage for wavelet-based SAR despeckling | http://arxiv.org/abs/1608.00279 | id:1608.00279 author:Mario Mastriani, Alberto E. Giraldez category:cs.CV  published:2016-07-31 summary:The wavelet shrinkage denoising approach is able to maintain local regularity of a signal while suppressing noise. However, the conventional wavelet shrinkage based methods are not time-scale adaptive to track the local time-scale variation. In this paper, a new type of Neural Shrinkage (NS) is presented with a new class of shrinkage architecture for speckle reduction in Synthetic Aperture Radar (SAR) images. The numerical results indicate that the new method outperforms the standard filters, the standard wavelet shrinkage despeckling method, and previous NS. version:1
arxiv-1608-00277 | Fuzzy thresholding in wavelet domain for speckle reduction in Synthetic Aperture Radar images | http://arxiv.org/abs/1608.00277 | id:1608.00277 author:Mario Mastriani category:cs.CV  published:2016-07-31 summary:The application of wavelet transforms to Synthetic Aperture Radar (SAR) imagery has improved despeckling performance. To deduce the problem of filtering the multiplicative noise to the case of an additive noise, the wavelet decomposition is performed on the logarithm of the image gray levels. The detail coefficients produced by the bidimensional discrete wavelet transform (DWT-2D) needs to be thresholded to extract out the speckle in highest subbands. An initial threshold value is estimated according to the noise variance. In this paper, an additional fuzzy thresholding approach for automatic determination of the rate threshold level around the traditional wavelet noise thresholding (initial threshold) is applied, and used for the soft or hard-threshold performed on all the high frequency subimages. The filtered logarithmic image is then obtained by reconstruction from the thresholded coefficients. This process is applied a single time, and exclusively to the first level of decomposition. The exponential function of this reconstructed image gives the final filtered image. Experimental results on test images have demonstrated the effectiveness of this method compared to the most of methods in use at the moment. version:1
arxiv-1608-00274 | Denoising based on wavelets and deblurring via self-organizing map for Synthetic Aperture Radar images | http://arxiv.org/abs/1608.00274 | id:1608.00274 author:Mario Mastriani category:cs.CV  published:2016-07-31 summary:This work deals with unsupervised image deblurring. We present a new deblurring procedure on images provided by low-resolution synthetic aperture radar (SAR) or simply by multimedia in presence of multiplicative (speckle) or additive noise, respectively. The method we propose is defined as a two-step process. First, we use an original technique for noise reduction in wavelet domain. Then, the learning of a Kohonen self-organizing map (SOM) is performed directly on the denoised image to take out it the blur. This technique has been successfully applied to real SAR images, and the simulation results are presented to demonstrate the effectiveness of the proposed algorithms. version:1
arxiv-1608-00273 | Kalman's shrinkage for wavelet-based despeckling of SAR images | http://arxiv.org/abs/1608.00273 | id:1608.00273 author:Mario Mastriani, Alberto E. Giraldez category:cs.CV  published:2016-07-31 summary:In this paper, a new probability density function (pdf) is proposed to model the statistics of wavelet coefficients, and a simple Kalman's filter is derived from the new pdf using Bayesian estimation theory. Specifically, we decompose the speckled image into wavelet subbands, we apply the Kalman's filter to the high subbands, and reconstruct a despeckled image from the modified detail coefficients. Experimental results demonstrate that our method compares favorably to several other despeckling methods on test synthetic aperture radar (SAR) images. version:1
arxiv-1608-00270 | New wavelet-based superresolution algorithm for speckle reduction in SAR images | http://arxiv.org/abs/1608.00270 | id:1608.00270 author:Mario Mastriani category:cs.CV  published:2016-07-31 summary:This paper describes a novel projection algorithm, the Projection Onto Span Algorithm (POSA) for wavelet-based superresolution and removing speckle (in wavelet domain) of unknown variance from Synthetic Aperture Radar (SAR) images. Although the POSA is good as a new superresolution algorithm for image enhancement, image metrology and biometric identification, here one will use it like a tool of despeckling, being the first time that an algorithm of super-resolution is used for despeckling of SAR images. Specifically, the speckled SAR image is decomposed into wavelet subbands, POSA is applied to the high subbands, and reconstruct a SAR image from the modified detail coefficients. Experimental results demonstrate that the new method compares favorably to several other despeckling methods on test SAR images. version:1
arxiv-1608-00268 | Union is strength in lossy image compression | http://arxiv.org/abs/1608.00268 | id:1608.00268 author:Mario Mastriani category:cs.CV  published:2016-07-31 summary:In this work, we present a comparison between different techniques of image compression. First, the image is divided in blocks which are organized according to a certain scan. Later, several compression techniques are applied, combined or alone. Such techniques are: wavelets (Haar's basis), Karhunen-Loeve Transform, etc. Simulations show that the combined versions are the best, with minor Mean Squared Error (MSE), and higher Peak Signal to Noise Ratio (PSNR) and better image quality, even in the presence of noise. version:1
arxiv-1608-00265 | Denoising and compression in wavelet domain via projection onto approximation coefficients | http://arxiv.org/abs/1608.00265 | id:1608.00265 author:Mario Mastriani category:cs.CV  published:2016-07-31 summary:We describe a new filtering approach in the wavelet domain for image denoising and compression, based on the projections of details subbands coefficients (resultants of the splitting procedure, typical in wavelet domain) onto the approximation subband coefficients (much less noisy). The new algorithm is called Projection Onto Approximation Coefficients (POAC). As a result of this approach, only the approximation subband coefficients and three scalars are stored and/or transmitted to the channel. Besides, with the elimination of the details subbands coefficients, we obtain a bigger compression rate. Experimental results demonstrate that our approach compares favorably to more typical methods of denoising and compression in wavelet domain. version:1
arxiv-1608-00567 | Identification of repeats in DNA sequences using nucleotide distribution uniformity | http://arxiv.org/abs/1608.00567 | id:1608.00567 author:Changchuan Yin category:q-bio.GN cs.CE cs.CV cs.DS 92D20  92-08  published:2016-07-31 summary:Repetitive elements are important in genomic structures, functions and regulations, yet effective methods in precisely identifying repetitive elements in DNA sequences are not fully accessible, and the relationship between repetitive elements and periodicities of genomes is not clearly understood. We present an $\textit{ab initio}$ method to quantitatively detect repetitive elements and infer the consensus repeat pattern in repetitive elements. The method uses the measure of the distribution uniformity of nucleotides at periodic positions in DNA sequences or genomes. It can identify periodicities, consensus repeat patterns, copy numbers and perfect levels of repetitive elements. The results of using the method on different DNA sequences and genomes demonstrate efficacy and accuracy in identifying repeat patterns and periodicities. The complexity of the method is linear with respect to the lengths of the analyzed sequences. version:1
arxiv-1608-00250 | On Regularization Parameter Estimation under Covariate Shift | http://arxiv.org/abs/1608.00250 | id:1608.00250 author:Wouter M. Kouw, Marco Loog category:cs.LG stat.ML  published:2016-07-31 summary:This paper identifies a problem with the usual procedure for L2-regularization parameter estimation in a domain adaptation setting. In such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). The usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. The problem is that if one decides to use source validation data, the regularization parameter is underestimated. One possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. We conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter. version:1
arxiv-1608-00247 | Similarity Registration Problems for 2D/3D Ultrasound Calibration | http://arxiv.org/abs/1608.00247 | id:1608.00247 author:Francisco Vasconcelos, Donald Peebles, Sebastien Ourselin, Danail Stoyanov category:cs.CV  published:2016-07-31 summary:We propose a minimal solution for the similarity registration (rigid pose and scale) between two sets of 3D lines, and also between a set of co-planar points and a set of 3D lines. The first problem is solved up to 8 discrete solutions with a minimum of 2 line-line correspondences, while the second is solved up to 4 discrete solutions using 4 point-line correspondences. We use these algorithms to perform the extrinsic calibration between a pose tracking sensor and a 2D/3D ultrasound (US) curvilinear probe using a tracked needle as calibration target. The needle is tracked as a 3D line, and is scanned by the ultrasound as either a 3D line (3D US) or as a 2D point (2D US). Since the scale factor that converts US scan units to metric coordinates is unknown, the calibration is formulated as a similarity registration problem. We present results with both synthetic and real data and show that the minimum solutions outperform the correspondent non-minimal linear formulations. version:1
arxiv-1608-00242 | Input-Output Non-Linear Dynamical Systems applied to Physiological Condition Monitoring | http://arxiv.org/abs/1608.00242 | id:1608.00242 author:Konstantinos Georgatzis, Christopher K. I. Williams, Christopher Hawthorne category:cs.LG  published:2016-07-31 summary:We present a non-linear dynamical system for modelling the effect of drug infusions on the vital signs of patients admitted in Intensive Care Units (ICUs). More specifically we are interested in modelling the effect of a widely used anaesthetic drug (Propofol) on a patient's monitored depth of anaesthesia and haemodynamics. We compare our approach with one from the Pharmacokinetics/Pharmacodynamics (PK/PD) literature and show that we can provide significant improvements in performance without requiring the incorporation of expert physiological knowledge in our system. version:1
arxiv-1608-00229 | Variational Inference Background Subtraction Algorithm for in-Camera Acceleration in Thermal Imagery | http://arxiv.org/abs/1608.00229 | id:1608.00229 author:Konstantinos Makantasis, Antonis Nikitakis, Anastasios Doulamis, Nikolaos Doulamis, Yannis Papaefstathiou category:cs.CV  published:2016-07-31 summary:Detection of moving objects in videos is a crucial step towards successful surveillance and monitoring applications. A key component for such tasks is usually called background subtraction and tries to extract regions of interest from the image background for further processing or action. For this reason, its accuracy and its real-time performance is of great significance. Although, effective background subtraction methods have been proposed, only a few of them take into consideration the special characteristics of infrared imagery. In this work, we propose a novel background subtraction scheme, which models the thermal responses of each pixel as a mixture of Gaussians with unknown number of components. Following a Bayesian approach, our method automatically estimates the mixture structure, while simultaneously it avoids over/under fitting. The pixel density estimate is followed by an efficient and highly accurate updating mechanism, which permits our system to be automatically adapted to dynamically changing operation conditions. version:1
arxiv-1608-00220 | Learning Robust Features using Deep Learning for Automatic Seizure Detection | http://arxiv.org/abs/1608.00220 | id:1608.00220 author:Pierre Thodoroff, Joelle Pineau, Andrew Lim category:cs.LG cs.CV  published:2016-07-31 summary:We present and evaluate the capacity of a deep neural network to learn robust features from EEG to automatically detect seizures. This is a challenging problem because seizure manifestations on EEG are extremely variable both inter- and intra-patient. By simultaneously capturing spectral, temporal and spatial information our recurrent convolutional neural network learns a general spatially invariant representation of a seizure. The proposed approach exceeds significantly previous results obtained on cross-patient classifiers both in terms of sensitivity and false positive rate. Furthermore, our model proves to be robust to missing channel and variable electrode montage. version:1
arxiv-1608-00218 | Hyperparameter Transfer Learning through Surrogate Alignment for Efficient Deep Neural Network Training | http://arxiv.org/abs/1608.00218 | id:1608.00218 author:Ilija Ilievski, Jiashi Feng category:cs.LG cs.CV cs.NE stat.ML  published:2016-07-31 summary:Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method. version:1
arxiv-1608-00207 | Learning deep representation from coarse to fine for face alignment | http://arxiv.org/abs/1608.00207 | id:1608.00207 author:Zhiwen Shao, Shouhong Ding, Yiru Zhao, Qinchuan Zhang, Lizhuang Ma category:cs.CV  published:2016-07-31 summary:In this paper, we propose a novel face alignment method that trains deep convolutional network from coarse to fine. It divides given landmarks into principal subset and elaborate subset. We firstly keep a large weight for principal subset to make our network primarily predict their locations while slightly take elaborate subset into account. Next the weight of principal subset is gradually decreased until two subsets have equivalent weights. This process contributes to learn a good initial model and search the optimal model smoothly to avoid missing fairly good intermediate models in subsequent procedures. On the challenging COFW dataset [1], our method achieves 6.33% mean error with a reduction of 21.37% compared with the best previous result [2]. version:1
arxiv-1608-00203 | Automatic 3D Point Set Reconstruction from Stereo Laparoscopic Images using Deep Neural Networks | http://arxiv.org/abs/1608.00203 | id:1608.00203 author:Balint Antal category:cs.CV  published:2016-07-31 summary:In this paper, an automatic approach to predict 3D coordinates from stereo laparoscopic images is presented. The approach maps a vector of pixel intensities to 3D coordinates through training a six layer deep neural network. The architectural aspects of the approach is presented and in detail and the method is evaluated on a publicly available dataset with promising results. version:1
arxiv-1608-00199 | A Data-driven Approach for Human Pose Tracking Based on Spatio-temporal Pictorial Structure | http://arxiv.org/abs/1608.00199 | id:1608.00199 author:Soumitra Samanta, Bhabatosh Chanda category:cs.CV  published:2016-07-31 summary:In this paper, we present a data-driven approach for human pose tracking in video data. We formulate the human pose tracking problem as a discrete optimization problem based on spatio-temporal pictorial structure model and solve this problem in a greedy framework very efficiently. We propose the model to track the human pose by combining the human pose estimation from single image and traditional object tracking in a video. Our pose tracking objective function consists of the following terms: likeliness of appearance of a part within a frame, temporal displacement of the part from previous frame to the current frame, and the spatial dependency of a part with its parent in the graph structure. Experimental evaluation on benchmark datasets (VideoPose2, Poses in the Wild and Outdoor Pose) as well as on our newly build ICDPose dataset shows the usefulness of our proposed method. version:1
arxiv-1608-00187 | Visual Relationship Detection with Language Priors | http://arxiv.org/abs/1608.00187 | id:1608.00187 author:Cewu Lu, Ranjay Krishna, Michael Bernstein, Li Fei-Fei category:cs.CV  published:2016-07-31 summary:Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. "man riding bicycle" and "man pushing bicycle"). Consequently, the set of possible relationships is extremely large and it is difficult to obtain sufficient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. "man" and "bicycle") and predicates (e.g. "riding" and "pushing") independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to finetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval. version:1
arxiv-1608-00182 | Deep FisherNet for Object Classification | http://arxiv.org/abs/1608.00182 | id:1608.00182 author:Peng Tang, Xinggang Wang, Baoguang Shi, Xiang Bai, Wenyu Liu, Zhuowen Tu category:cs.CV cs.LG  published:2016-07-31 summary:Despite the great success of convolutional neural networks (CNN) for the image classification task on datasets like Cifar and ImageNet, CNN's representation power is still somewhat limited in dealing with object images that have large variation in size and clutter, where Fisher Vector (FV) has shown to be an effective encoding strategy. FV encodes an image by aggregating local descriptors with a universal generative Gaussian Mixture Model (GMM). FV however has limited learning capability and its parameters are mostly fixed after constructing the codebook. To combine together the best of the two worlds, we propose in this paper a neural network structure with FV layer being part of an end-to-end trainable system that is differentiable; we name our network FisherNet that is learnable using backpropagation. Our proposed FisherNet combines convolutional neural network training and Fisher Vector encoding in a single end-to-end structure. We observe a clear advantage of FisherNet over plain CNN and standard FV in terms of both classification accuracy and computational efficiency on the challenging PASCAL VOC object classification task. version:1
arxiv-1608-00168 | Sparse vs. Non-sparse: Which One Is Better for Practical Visual Tracking? | http://arxiv.org/abs/1608.00168 | id:1608.00168 author:Yashar Deldjoo, Shengping Zhang, Bahman Zanj, Paolo Cremonesi, Matteo Matteucci category:cs.CV  published:2016-07-30 summary:Recently, sparse representation based visual tracking methods have attracted increasing attention in the computer vision community. Although achieve superior performance to traditional tracking methods, however, a basic problem has not been answered yet --- that whether the sparsity constrain is really needed for visual tracking? To answer this question, in this paper, we first propose a robust non-sparse representation based tracker and then conduct extensive experiments to compare it against several state-of-the-art sparse representation based trackers. Our experiment results and analysis indicate that the proposed non-sparse tracker achieved competitive tracking accuracy with sparse trackers while having faster running speed, which support our non-sparse tracker to be used in practical applications. version:1
arxiv-1608-00161 | Localizing and Orienting Street Views Using Overhead Imagery | http://arxiv.org/abs/1608.00161 | id:1608.00161 author:Nam N. Vo, James Hays category:cs.CV  published:2016-07-30 summary:In this paper we aim to determine the location and orientation of a ground-level query image by matching to a reference database of overhead (e.g. satellite) images. For this task we collect a new dataset with one million pairs of street view and overhead images sampled from eleven U.S. cities. We explore several deep CNN architectures for cross-domain matching -- Classification, Hybrid, Siamese, and Triplet networks. Classification and Hybrid architectures are accurate but slow since they allow only partial feature precomputation. We propose a new loss function which significantly improves the accuracy of Siamese and Triplet embedding networks while maintaining their applicability to large-scale retrieval tasks like image geolocalization. This image matching task is challenging not just because of the dramatic viewpoint difference between ground-level and overhead imagery but because the orientation (i.e. azimuth) of the street views is unknown making correspondence even more difficult. We examine several mechanisms to match in spite of this -- training for rotation invariance, sampling possible rotations at query time, and explicitly predicting relative rotation of ground and overhead images with our deep networks. It turns out that explicit orientation supervision also improves location prediction accuracy. Our best performing architectures are roughly 2.5 times as accurate as the commonly used Siamese network baseline. version:1
arxiv-1608-00159 | Learning Tree-Structured Detection Cascades for Heterogeneous Networks of Embedded Devices | http://arxiv.org/abs/1608.00159 | id:1608.00159 author:Hamid Dadkhahi, Benjamin M. Marlin category:stat.ML cs.LG  published:2016-07-30 summary:In this paper, we present a new approach to learning cascaded classifiers for use in computing environments that involve networks of heterogeneous and resource-constrained, low-power embedded compute and sensing nodes. We present a generalization of the classical linear detection cascade to the case of tree-structured cascades where different branches of the tree execute on different physical compute nodes in the network. Different nodes have access to different features, as well as access to potentially different computation and energy resources. We concentrate on the problem of jointly learning the parameters for all of the classifiers in the cascade given a fixed cascade architecture and a known set of costs required to carry out the computation at each node.To accomplish the objective of joint learning of all detectors, we propose a novel approach to combining classifier outputs during training that better matches the hard cascade setting in which the learned system will be deployed. This work is motivated by research in the area of mobile health where energy efficient real time detectors integrating information from multiple wireless on-body sensors and a smart phone are needed for real-time monitoring and delivering just-in-time adaptive interventions. We apply our framework to the problem of cigarette smoking detection from a combination of wrist-worn actigraphy data and respiration chest band data. version:1
arxiv-1607-08194 | Convolutional Neural Networks Analyzed via Convolutional Sparse Coding | http://arxiv.org/abs/1607.08194 | id:1607.08194 author:Papyan Vardan, Yaniv Romano, Michael Elad category:stat.ML cs.LG  published:2016-07-27 summary:Convolutional neural networks (CNN) have led to remarkable results in various fields. In this scheme, a signal is convolved with learned filters and a non-linear function is applied on the response map. The obtained result is then fed to another layer that operates similarly, thereby creating a multi-layered structure. Despite its empirical success, a theoretical understanding of this scheme, termed forward pass, is lacking. Another popular paradigm is the sparse representation model, which assumes that a signal can be described as the multiplication of a dictionary by a sparse vector. A special case of this is the convolutional sparse coding (CSC) model, in which the dictionary assumes a convolutional structure. Unlike CNN, sparsity inspired models are accompanied by a thorough theoretical analysis. Indeed, such a study of the CSC model has been performed in a recent two-part work, establishing it as a reliable alternative to the common patch-based processing. Herein, we leverage the study of the CSC model, and bring a fresh view to CNN with a deeper theoretical understanding. Our analysis relies on the observation that akin to the signal, the sparse vector can also be modeled as a sparse composition of yet another set of atoms from a convolutional dictionary. This can be extended to more than two layers, resulting in our proposed multi-layered convolutional sparse model. In this work we address the following questions: 1) What is the relation between the CNN and the proposed model? 2) In particular, can we interpret the forward pass as a pursuit? 3) If so, can we leverage this connection to provide a theoretical foundation for the forward pass? Specifically, is this algorithm guaranteed to succeed under certain conditions? Is it stable to slight perturbations in its input? 4) Lastly, can we leverage the answers to the above, and propose alternatives to CNN's forward pass? version:2
arxiv-1608-00148 | Multi-task Learning with Weak Class Labels: Leveraging iEEG to Detect Cortical Lesions in Cryptogenic Epilepsy | http://arxiv.org/abs/1608.00148 | id:1608.00148 author:Bilal Ahmed, Thomas Thesen, Karen E. Blackmon, Ruben Kuzniecky, Orrin Devinsky, Jennifer G. Dy, Carla E. Brodley category:cs.CV  published:2016-07-30 summary:Multi-task learning (MTL) is useful for domains in which data originates from multiple sources that are individually under-sampled. MTL methods are able to learn classification models that have higher performance as compared to learning a single model by aggregating all the data together or learning a separate model for each data source. The performance of these methods relies on label accuracy. We address the problem of simultaneously learning multiple classifiers in the MTL framework when the training data has imprecise labels. We assume that there is an additional source of information that provides a score for each instance which reflects the certainty about its label. Modeling this score as being generated by an underlying ranking function, we augment the MTL framework with an added layer of supervision. This results in new MTL methods that are able to learn accurate classifiers while preserving the domain structure provided through the rank information. We apply these methods to the task of detecting abnormal cortical regions in the MRIs of patients suffering from focal epilepsy whose MRI were read as normal by expert neuroradiologists. In addition to the noisy labels provided by the results of surgical resection, we employ the results of an invasive intracranial-EEG exam as an additional source of label information. Our proposed methods are able to successfully detect abnormal regions for all patients in our dataset and achieve a higher performance as compared to baseline methods. version:1
arxiv-1608-00138 | Heterogeneous Strategy Particle Swarm Optimization | http://arxiv.org/abs/1608.00138 | id:1608.00138 author:Wen-Bo Du, Wen Ying, Gang Yan, Yan-Bo Zhu, Xian-Bin Cao category:cs.NE  published:2016-07-30 summary:PSO is a widely recognized optimization algorithm inspired by social swarm. In this brief we present a heterogeneous strategy particle swarm optimization (HSPSO), in which a proportion of particles adopt a fully informed strategy to enhance the converging speed while the rest are singly informed to maintain the diversity. Our extensive numerical experiments show that HSPSO algorithm is able to obtain satisfactory solutions, outperforming both PSO and the fully informed PSO. The evolution process is examined from both structural and microscopic points of view. We find that the cooperation between two types of particles can facilitate a good balance between exploration and exploitation, yielding better performance. We demonstrate the applicability of HSPSO on the filter design problem. version:1
arxiv-1608-00116 | Segmentation of Soft atherosclerotic plaques using active contour models | http://arxiv.org/abs/1608.00116 | id:1608.00116 author:Muhammad Moazzam Jawaid category:cs.CV  published:2016-07-30 summary:Detection of non-calcified plaques in the coronary tree is a challenging problem due to the nature of comprising substances. Hard plaques are easily discernible in CTA data cloud due to apparent bright behaviour, therefore many approaches have been proposed for automatic segmentation of calcified plaques. In contrast soft plaques show very small difference in intensity with respect to surrounding heart tissues & blood voxels. This similarity in intensity makes the isolation and detection of soft plaques very difficult. This work aims to develop framework for segmentation of vulnerable plaques with minimal user dependency. In first step automatic seed point has been established based on the fact that coronary artery behaves as tubular structure through axial slices. In the following step the behaviour of contrast agent has been modelled mathematically to reflect the dye diffusion in respective CTA volume. Consequently based on detected seed point & intensity behaviour, localized active contour segmentation has been applied to extract complete coronary tree. Bidirectional segmentation has been applied to avoid loss of coronary information due to the seed point location whereas auto adjustment feature of contour grabs new emerging branches. Medial axis for extracted coronary tree is generated using fast marching method for obtaining curve planar reformation for validation of contrast agent behaviour. Obtained coronary tree is to be evaluated for soft plaques in second phase of this research. version:1
arxiv-1608-00112 | Supervised Attentions for Neural Machine Translation | http://arxiv.org/abs/1608.00112 | id:1608.00112 author:Haitao Mi, Zhiguo Wang, Abe Ittycheriah category:cs.CL  published:2016-07-30 summary:In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the "true" alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. version:1
arxiv-1608-00104 | World Knowledge as Indirect Supervision for Document Clustering | http://arxiv.org/abs/1608.00104 | id:1608.00104 author:Chenguang Wang, Yangqiu Song, Dan Roth, Ming Zhang, Jiawei Han category:cs.LG cs.CL cs.IR  published:2016-07-30 summary:One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this paper, we provide an example of using world knowledge for domain dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features. version:1
arxiv-1608-00100 | Online Learning of Event Definitions | http://arxiv.org/abs/1608.00100 | id:1608.00100 author:Nikos Katzouris, Alexander Artikis, Georgios Paliouras category:cs.LG cs.AI  published:2016-07-30 summary:Systems for symbolic event recognition infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing among others, direct connections to machine learning, via Inductive Logic Programming (ILP). We present an ILP system for online learning of Event Calculus theories. To allow for a single-pass learning strategy, we use the Hoeffding bound for evaluating clauses on a subset of the input stream. We employ a decoupling scheme of the Event Calculus axioms during the learning process, that allows to learn each clause in isolation. Moreover, we use abductive-inductive logic programming techniques to handle unobserved target predicates. We evaluate our approach on an activity recognition application and compare it to a number of batch learning techniques. We obtain results of comparable predicative accuracy with significant speed-ups in training time. We also outperform hand-crafted rules and match the performance of a sound incremental learner that can only operate on noise-free datasets. This paper is under consideration for acceptance in TPLP. version:1
arxiv-1608-00092 | DeepSoft: A vision for a deep model of software | http://arxiv.org/abs/1608.00092 | id:1608.00092 author:Hoa Khanh Dam, Truyen Tran, John Grundy, Aditya Ghose category:cs.SE stat.ML  published:2016-07-30 summary:Although software analytics has experienced rapid growth as a research area, it has not yet reached its full potential for wide industrial adoption. Most of the existing work in software analytics still relies heavily on costly manual feature engineering processes, and they mainly address the traditional classification problems, as opposed to predicting future events. We present a vision for \emph{DeepSoft}, an \emph{end-to-end} generic framework for modeling software and its development process to predict future risks and recommend interventions. DeepSoft, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term temporal dependencies that occur in software evolution. Such deep learned patterns of software can be used to address a range of challenging problems such as code and task recommendation and prediction. DeepSoft provides a new approach for research into modeling of source code, risk prediction and mitigation, developer modeling, and automatically generating code patches from bug reports. version:1
arxiv-1608-00059 | Face Recognition Using Scattering Convolutional Network | http://arxiv.org/abs/1608.00059 | id:1608.00059 author:Shervin Minaee, Amirali Abdolrashidi, Yao Wang category:cs.CV  published:2016-07-30 summary:Face recognition has been an active research area in the past few decades. In general, face recognition can be very challenging due to variations in viewpoint, illumination, facial expression, etc. Therefore it is essential to extract features which are invariant to some or all of these variations. Here a new image representation, called scattering transform/network, has been used to extract features from faces. The scattering transform is a kind of convolutional network which provides a powerful multi-layer representation for signals. After extraction of scattering features, PCA is applied to reduce the dimensionality of the data and then a multi-class support vector machine is used to perform recognition. The proposed algorithm has been tested on three face datasets and achieved a very high recognition rate. version:1
arxiv-1608-00027 | gLOP: the global and Local Penalty for Capturing Predictive Heterogeneity | http://arxiv.org/abs/1608.00027 | id:1608.00027 author:Rhiannon V. Rose, Daniel J. Lizotte category:stat.ML cs.LG  published:2016-07-29 summary:When faced with a supervised learning problem, we hope to have rich enough data to build a model that predicts future instances well. However, in practice, problems can exhibit predictive heterogeneity: most instances might be relatively easy to predict, while others might be predictive outliers for which a model trained on the entire dataset does not perform well. Identifying these can help focus future data collection. We present gLOP, the global and Local Penalty, a framework for capturing predictive heterogeneity and identifying predictive outliers. gLOP is based on penalized regression for multitask learning, which improves learning by leveraging training signal information from related tasks. We give two optimization algorithms for gLOP, one space-efficient, and another giving the full regularization path. We also characterize uniqueness in terms of the data and tuning parameters, and present empirical results on synthetic data and on two health research problems. version:1
arxiv-1608-01965 | Text authorship identified using the dynamics of word co-occurrence networks | http://arxiv.org/abs/1608.01965 | id:1608.01965 author:Camilo Akimushkin, Diego R. Amancio, Osvaldo N. Oliveira Jr category:cs.CL  published:2016-07-29 summary:The identification of authorship in disputed documents still requires human expertise, which is now unfeasible for many tasks owing to the large volumes of text and authors in practical applications. In this study, we introduce a methodology based on the dynamics of word co-occurrence networks representing written texts to classify a corpus of 80 texts by 8 authors. The texts were divided into sections with equal number of linguistic tokens, from which time series were created for 12 topological metrics. The series were proven to be stationary (p-value>0.05), which permits to use distribution moments as learning attributes. With an optimized supervised learning procedure using a Radial Basis Function Network, 68 out of 80 texts were correctly classified, i.e. a remarkable 85% author matching success rate. Therefore, fluctuations in purely dynamic network metrics were found to characterize authorship, thus opening the way for the description of texts in terms of small evolving networks. Moreover, the approach introduced allows for comparison of texts with diverse characteristics in a simple, fast fashion. version:1
arxiv-1607-07423 | A Non-Parametric Control Chart For High Frequency Multivariate Data | http://arxiv.org/abs/1607.07423 | id:1607.07423 author:Deovrat Kakde, Sergriy Peredriy, Arin Chaudhuri, Anya Mcguirk category:cs.LG stat.AP stat.ME stat.ML 62N05  90B25 G.3; H.2.8  published:2016-07-25 summary:Support Vector Data Description (SVDD) is a machine learning technique used for single class classification and outlier detection. SVDD based K-chart was first introduced by Sun and Tsung for monitoring multivariate processes when underlying distribution of process parameters or quality characteristics depart from Normality. The method first trains a SVDD model on data obtained from stable or in-control operations of the process to obtain a threshold $R^2$ and kernel center a. For each new observation, its Kernel distance from the Kernel center a is calculated. The kernel distance is compared against the threshold $R^2$ to determine if the observation is within the control limits. The non-parametric K-chart provides an attractive alternative to the traditional control charts such as the Hotelling's $T^2$ charts when distribution of the underlying multivariate data is either non-normal or is unknown. But there are challenges when K-chart is deployed in practice. The K-chart requires calculating kernel distance of each new observation but there are no guidelines on how to interpret the kernel distance plot and infer about shifts in process mean or changes in process variation. This limits the application of K-charts in big-data applications such as equipment health monitoring, where observations are generated at a very high frequency. In this scenario, the analyst using the K-chart is inundated with kernel distance results at a very high frequency, generally without any recourse for detecting presence of any assignable causes of variation. We propose a new SVDD based control chart, called as $K_T$ chart, which addresses challenges encountered when using K-chart for big-data applications. The $K_T$ charts can be used to simultaneously track process variation and central tendency. We illustrate the successful use of $K_T$ chart using the Tennessee Eastman process data. version:3
arxiv-1607-08905 | Complexity of Discrete Energy Minimization Problems | http://arxiv.org/abs/1607.08905 | id:1607.08905 author:Mengtian Li, Alexander Shekhovtsov, Daniel Huber category:cs.CV cs.DS  published:2016-07-29 summary:Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models. The problem, in general, is notoriously intractable, and finding the global optimal solution is known to be NP-hard. However, is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time? We show in this paper that the answer is no. Specifically, we show that general energy minimization, even in the 2-label pairwise case, and planar energy minimization with three or more labels are exp-APX-complete. This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems, including constant factor approximations. Moreover, we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes -- PO, APX, and exp-APX, corresponding to problems that are solvable, approximable, and inapproximable in polynomial time. Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones. This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms. version:1
arxiv-1607-08898 | Personalized Emphasis Framing for Persuasive Message Generation | http://arxiv.org/abs/1607.08898 | id:1607.08898 author:Tao Ding, Shimei Pan category:cs.AI cs.CL  published:2016-07-29 summary:In this paper, we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals. With this framework, we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality (e.g., extraversion and conscientiousness) and basic human values (e.g., self-transcendence and hedonism). We also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation. version:1
arxiv-1607-08891 | Assessing Functional Neural Connectivity as an Indicator of Cognitive Performance | http://arxiv.org/abs/1607.08891 | id:1607.08891 author:Brian S. Helfer, James R. Williamson, Benjamin A. Miller, Joseph Perricone, Thomas F. Quatieri category:stat.ML q-bio.NC  published:2016-07-29 summary:Studies in recent years have demonstrated that neural organization and structure impact an individual's ability to perform a given task. Specifically, individuals with greater neural efficiency have been shown to outperform those with less organized functional structure. In this work, we compare the predictive ability of properties of neural connectivity on a working memory task. We provide two novel approaches for characterizing functional network connectivity from electroencephalography (EEG), and compare these features to the average power across frequency bands in EEG channels. Our first novel approach represents functional connectivity structure through the distribution of eigenvalues making up channel coherence matrices in multiple frequency bands. Our second approach creates a connectivity network at each frequency band, and assesses variability in average path lengths of connected components and degree across the network. Failures in digit and sentence recall on single trials are detected using a Gaussian classifier for each feature set, at each frequency band. The classifier results are then fused across frequency bands, with the resulting detection performance summarized using the area under the receiver operating characteristic curve (AUC) statistic. Fused AUC results of 0.63/0.58/0.61 for digit recall failure and 0.58/0.59/0.54 for sentence recall failure are obtained from the connectivity structure, graph variability, and channel power features respectively. version:1
arxiv-1607-08885 | Authorship Verification - An Approach based on Random Forest | http://arxiv.org/abs/1607.08885 | id:1607.08885 author:Promita Maitra, Souvick Ghosh, Dipankar Das category:cs.CL  published:2016-07-29 summary:Authorship attribution, being an important problem in many areas in-cluding information retrieval, computational linguistics, law and journalism etc., has been identified as a subject of increasingly research interest in the re-cent years. In case of Author Identification task in PAN at CLEF 2015, the main focus was given on cross-genre and cross-topic author verification tasks. We have used several word-based and style-based features to identify the dif-ferences between the known and unknown problems of one given set and label the unknown ones accordingly using a Random Forest based classifier. version:1
arxiv-1607-08883 | Labeling of Query Words using Conditional Random Field | http://arxiv.org/abs/1607.08883 | id:1607.08883 author:Satanu Ghosh, Souvick Ghosh, Dipankar Das category:cs.IR cs.CL  published:2016-07-29 summary:This paper describes our approach on Query Word Labeling as an attempt in the shared task on Mixed Script Information Retrieval at Forum for Information Retrieval Evaluation (FIRE) 2015. The query is written in Roman script and the words were in English or transliterated from Indian regional languages. A total of eight Indian languages were present in addition to English. We also identified the Named Entities and special symbols as part of our task. A CRF based machine learning framework was used for labeling the individual words with their corresponding language labels. We used a dictionary based approach for language identification. We also took into account the context of the word while identifying the language. Our system demonstrated an overall accuracy of 75.5% for token level language identification. The strict F-measure scores for the identification of token level language labels for Bengali, English and Hindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure of our system was 0.7498. version:1
arxiv-1607-08878 | Identifying and Harnessing the Building Blocks of Machine Learning Pipelines for Sensible Initialization of a Data Science Automation Tool | http://arxiv.org/abs/1607.08878 | id:1607.08878 author:Randal S. Olson, Jason H. Moore category:cs.NE cs.AI cs.LG  published:2016-07-29 summary:As data science continues to grow in popularity, there will be an increasing need to make data science tools more scalable, flexible, and accessible. In particular, automated machine learning (AutoML) systems seek to automate the process of designing and optimizing machine learning pipelines. In this chapter, we present a genetic programming-based AutoML system called TPOT that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification problem. Further, we analyze a large database of pipelines that were previously used to solve various supervised classification problems and identify 100 short series of machine learning operations that appear the most frequently, which we call the building blocks of machine learning pipelines. We harness these building blocks to initialize TPOT with promising solutions, and find that this sensible initialization method significantly improves TPOT's performance on one benchmark at no cost of significantly degrading performance on the others. Thus, sensible initialization with machine learning pipeline building blocks shows promise for GP-based AutoML systems, and should be further refined in future work. version:1
arxiv-1607-08877 | The Phylogenetic LASSO and the Microbiome | http://arxiv.org/abs/1607.08877 | id:1607.08877 author:Stephen T Rush, Christine H Lee, Washington Mio, Peter T Kim category:stat.ML q-bio.QM 62P10  published:2016-07-29 summary:Scientific investigations that incorporate next generation sequencing involve analyses of high-dimensional data where the need to organize, collate and interpret the outcomes are pressingly important. Currently, data can be collected at the microbiome level leading to the possibility of personalized medicine whereby treatments can be tailored at this scale. In this paper, we lay down a statistical framework for this type of analysis with a view toward synthesis of products tailored to individual patients. Although the paper applies the technique to data for a particular infectious disease, the methodology is sufficiently rich to be expanded to other problems in medicine, especially those in which coincident `-omics' covariates and clinical responses are simultaneously captured. version:1
arxiv-1607-08863 | Exponentially fast convergence to (strict) equilibrium via hedging | http://arxiv.org/abs/1607.08863 | id:1607.08863 author:Johanne Cohen, Amélie Héliou, Panayotis Mertikopoulos category:cs.GT cs.LG math.OC  published:2016-07-29 summary:Motivated by applications to data networks where fast convergence is essential, we analyze the problem of learning in generic N-person games that admit a Nash equilibrium in pure strategies. Specifically, we consider a scenario where players interact repeatedly and try to learn from past experience by small adjustments based on local - and possibly imperfect - payoff information. For concreteness, we focus on the so-called "hedge" variant of the exponential weights algorithm where players select an action with probability proportional to the exponential of the action's cumulative payoff over time. When players have perfect information on their mixed payoffs, the algorithm converges locally to a strict equilibrium and the rate of convergence is exponentially fast - of the order of $\mathcal{O}(\exp(-a\sum_{j=1}^{t}\gamma_{j}))$ where $a>0$ is a constant and $\gamma_{j}$ is the algorithm's step-size. In the presence of uncertainty, convergence requires a more conservative step-size policy, but with high probability, the algorithm remains locally convergent and achieves an exponential convergence rate. version:1
arxiv-1608-00514 | Dimensionality reduction based on Distance Preservation to Local Mean (DPLM) for SPD matrices and its application in BCI | http://arxiv.org/abs/1608.00514 | id:1608.00514 author:Alireza Davoudi, Saeed Shiry Ghidary, Khadijeh Sadatnejad category:cs.NA cs.CV G.1.6; I.5.4  published:2016-07-29 summary:In this paper, we propose a nonlinear dimensionality reduction algorithm for the manifold of Symmetric Positive Definite (SPD) matrices that considers the geometry of SPD matrices and provides a low dimensional representation of the manifold with high class discrimination. The proposed algorithm, tries to preserve the local structure of the data by preserving distance to local mean (DPLM) and also provides an implicit projection matrix. DPLM is linear in terms of the number of training samples and may use the label information when they are available in order to performance improvement in classification tasks. We performed several experiments on the multi-class dataset IIa from BCI competition IV. The results show that our approach as dimensionality reduction technique - leads to superior results in comparison with other competitor in the related literature because of its robustness against outliers. The experiments confirm that the combination of DPLM with FGMDM as the classifier leads to the state of the art performance on this dataset. version:1
arxiv-1607-08822 | SPICE: Semantic Propositional Image Caption Evaluation | http://arxiv.org/abs/1607.08822 | id:1607.08822 author:Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould category:cs.CV cs.CL  published:2016-07-29 summary:There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as `which caption-generator best understands colors?' and `can caption-generators count?' version:1
arxiv-1607-08811 | Can a CNN Recognize Catalan Diet? | http://arxiv.org/abs/1607.08811 | id:1607.08811 author:Pedro Herruzo, Marc Bolaños, Petia Radeva category:cs.CV  published:2016-07-29 summary:Nowadays, we can find several diseases related to the unhealthy diet habits of the population, such as diabetes, obesity, anemia, bulimia and anorexia. In many cases, these diseases are related to the food consumption of people. Mediterranean diet is scientifically known as a healthy diet that helps to prevent many metabolic diseases. In particular, our work focuses on the recognition of Mediterranean food and dishes. The development of this methodology would allow to analise the daily habits of users with wearable cameras, within the topic of lifelogging. By using automatic mechanisms we could build an objective tool for the analysis of the patient's behaviour, allowing specialists to discover unhealthy food patterns and understand the user's lifestyle. With the aim to automatically recognize a complete diet, we introduce a challenging multi-labeled dataset related to Mediterranean diet called FoodCAT. The first type of label provided consists of 115 food classes with an average of 400 images per dish, and the second one consists of 12 food categories with an average of 3800 pictures per class. This dataset will serve as a basis for the development of automatic diet recognition. In this context, deep learning and more specifically, Convolutional Neural Networks (CNNs), currently are state-of-the-art methods for automatic food recognition. In our work, we compare several architectures for image classification, with the purpose of diet recognition. Applying the best model for recognising food categories, we achieve a top-1 accuracy of 72.29\%, and top-5 of 97.07\%. In a complete diet recognition of dishes from Mediterranean diet, enlarged with the Food-101 dataset for international dishes recognition, we achieve a top-1 accuracy of 68.07\%, and top-5 of 89.53\%, for a total of 115+101 food classes. version:1
arxiv-1607-08810 | Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms | http://arxiv.org/abs/1607.08810 | id:1607.08810 author:Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, Naonori Ueda category:stat.ML cs.LG  published:2016-07-29 summary:Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks. version:1
arxiv-1607-08764 | SwiDeN : Convolutional Neural Networks For Depiction Invariant Object Recognition | http://arxiv.org/abs/1607.08764 | id:1607.08764 author:Ravi Kiran Sarvadevabhatla, Shiv Surya, Srinivas S S Kruthiventi, Venkatesh Babu R category:cs.CV  published:2016-07-29 summary:Current state of the art object recognition architectures achieve impressive performance but are typically specialized for a single depictive style (e.g. photos only, sketches only). In this paper, we present SwiDeN : our Convolutional Neural Network (CNN) architecture which recognizes objects regardless of how they are visually depicted (line drawing, realistic shaded drawing, photograph etc.). In SwiDeN, we utilize a novel `deep' depictive style-based switching mechanism which appropriately addresses the depiction-specific and depiction-invariant aspects of the problem. We compare SwiDeN with alternative architectures and prior work on a 50-category Photo-Art dataset containing objects depicted in multiple styles. Experimental results show that SwiDeN outperforms other approaches for the depiction-invariant object recognition problem. version:1
arxiv-1607-08756 | Data Filtering for Cluster Analysis by $\ell_0$-Norm Regularization | http://arxiv.org/abs/1607.08756 | id:1607.08756 author:Andrea Cristofari category:math.OC stat.ML  published:2016-07-29 summary:A data filtering method for cluster analysis is proposed, based on minimizing a least squares function with a weighted $\ell_0$-norm penalty. To overcome the discontinuity of the objective function, smooth non-convex functions are employed to approximate the $\ell_0$-norm. The convergence of the global minimum points of the approximating problems towards global minimum points of the original problem is stated. The proposed method also exploits a suitable technique to choose the penalty parameter. Numerical results on synthetic and real data sets are finally provided, showing how some existing clustering methods can take advantages from the proposed filtering strategy. version:1
arxiv-1607-08414 | SEMBED: Semantic Embedding of Egocentric Action Videos | http://arxiv.org/abs/1607.08414 | id:1607.08414 author:Michael Wray, Davide Moltisanti, Walterio Mayol-Cuevas, Dima Damen category:cs.CV  published:2016-07-28 summary:We present SEMBED, an approach for embedding an egocentric object interaction video in a semantic-visual graph to estimate the probability distribution over its potential semantic labels. When object interactions are annotated using unbounded choice of verbs, we embrace the wealth and ambiguity of these labels by capturing the semantic relationships as well as the visual similarities over motion and appearance features. We show how SEMBED can interpret a challenging dataset of 1225 freely annotated egocentric videos, outperforming SVM classification by more than 5%. version:2
arxiv-1607-08725 | Recurrent Neural Machine Translation | http://arxiv.org/abs/1607.08725 | id:1607.08725 author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-07-29 summary:The vanilla attention-based neural machine translation has achieved promising performance because of its capability in leveraging varying-length source annotations. However, this model still suffers from failures in long sentence translation, for its incapability in capturing long-term dependencies. In this paper, we propose a novel recurrent neural machine translation (RNMT), which not only preserves the ability to model varying-length source annotations but also better captures long-term dependencies. Instead of the conventional attention mechanism, RNMT employs a recurrent neural network to extract the context vector, where the target-side previous hidden state serves as its initial state, and the source annotations serve as its inputs. We refer to this new component as contexter. As the encoder, contexter and decoder in our model are all derivable recurrent neural networks, our model can still be trained end-to-end on large-scale corpus via stochastic algorithms. Experiments on Chinese-English translation tasks demonstrate the superiority of our model to attention-based neural machine translation, especially on long sentences. Besides, further analysis of the contexter revels that our model can implicitly reflect the alignment to source sentence. version:1
arxiv-1607-08723 | Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner | http://arxiv.org/abs/1607.08723 | id:1607.08723 author:Emmanuel Dupoux category:cs.CL cs.AI cs.LG  published:2016-07-29 summary:During their first years of life, infants learn the language(s) of their environment at an amazing speed despite large cross cultural variations in amount and complexity of the available language input. Understanding this simple fact still escapes current cognitive and linguistic theories. Recently, spectacular progress in the engineering science, notably, machine learning and wearable technology, offer the promise of revolutionizing the study of cognitive development. Machine learning offers powerful learning algorithms that can achieve human-like performance on many linguistic tasks. Wearable sensors can capture vast amounts of data, which enable the reconstruction of the sensory experience of infants in their natural environment. The project of 'reverse engineering' language development, i.e., of building an effective system that mimics infant's achievements appears therefore to be within reach. Here, we analyze the conditions under which such a project can contribute to our scientific understanding of early language development. We argue that instead of defining a sub-problem or simplifying the data, computational models should address the full complexity of the learning situation, and take as input the raw sensory signals available to infants. This implies that (1) accessible but privacy-preserving repositories of home data be setup and widely shared, and (2) models be evaluated at different linguistic levels through a benchmark of psycholinguist tests that can be passed by machines and humans alike, (3) linguistically and psychologically plausible learning architectures be scaled up to real data using probabilistic/optimization principles from machine learning. We discuss the feasibility of this approach and present preliminary results. version:1
arxiv-1607-08720 | TopicResponse: A Marriage of Topic Modelling and Rasch Modelling for Automatic Measurement in MOOCs | http://arxiv.org/abs/1607.08720 | id:1607.08720 author:Jiazhen He, Rui Zhang, James Bailey, Benjamin I. P. Rubinstein, Sandra Milligan category:cs.LG cs.CL cs.IR stat.ML  published:2016-07-29 summary:This paper proposes adapting topic models to the psychometric testing of MOOC students based on their online forum postings. We explore the suitability of using automatically discovered topics from MOOC forums to measure students' academic abilities in a subject domain, under the Rasch model, which is the most common Item Response Theory (IRT) model. The challenge is to discover topics that can fit the Rasch model as evidence of measuring educationally meaningful ability. To solve this challenge, we combine the Rasch model with non-negative matrix factorisation (NMF)-based topic modelling. We demonstrate the suitability of our approach with both quantitative experiments on three Coursera MOOCs, and with qualitative results of topic interpretability on a Discrete Optimisation MOOC. version:1
arxiv-1607-08707 | Image Prediction for Limited-angle Tomography via Deep Learning with Convolutional Neural Network | http://arxiv.org/abs/1607.08707 | id:1607.08707 author:Hanming Zhang, Liang Li, Kai Qiao, Linyuan Wang, Bin Yan, Lei Li, Guoen Hu category:physics.med-ph cs.CV  published:2016-07-29 summary:Limited angle problem is a challenging issue in x-ray computed tomography (CT) field. Iterative reconstruction methods that utilize the additional prior can suppress artifacts and improve image quality, but unfortunately require increased computation time. An interesting way is to restrain the artifacts in the images reconstructed from the practical ?ltered back projection (FBP) method. Frikel and Quinto have proved that the streak artifacts in FBP results could be characterized. It indicates that the artifacts created by FBP method have specific and similar characteristics in a stationary limited-angle scanning configuration. Based on this understanding, this work aims at developing a method to extract and suppress specific artifacts of FBP reconstructions for limited-angle tomography. A data-driven learning-based method is proposed based on a deep convolutional neural network. An end-to-end mapping between the FBP and artifact-free images is learned and the implicit features involving artifacts will be extracted and suppressed via nonlinear mapping. The qualitative and quantitative evaluations of experimental results indicate that the proposed method show a stable and prospective performance on artifacts reduction and detail recovery for limited angle tomography. The presented strategy provides a simple and efficient approach for improving image quality of the reconstruction results from limited projection data. version:1
arxiv-1607-08693 | Connecting Phrase based Statistical Machine Translation Adaptation | http://arxiv.org/abs/1607.08693 | id:1607.08693 author:Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama, Eiichro Sumita category:cs.CL  published:2016-07-29 summary:Although more additional corpora are now available for Statistical Machine Translation (SMT), only the ones which belong to the same or similar domains with the original corpus can indeed enhance SMT performance directly. Most of the existing adaptation methods focus on sentence selection. In comparison, phrase is a smaller and more fine grained unit for data selection, therefore we propose a straightforward and efficient connecting phrase based adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram adaptation. The proposed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performance are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods). version:1
arxiv-1607-08254 | Stochastic Frank-Wolfe Methods for Nonconvex Optimization | http://arxiv.org/abs/1607.08254 | id:1607.08254 author:Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG stat.ML  published:2016-07-27 summary:We study Frank-Wolfe methods for nonconvex stochastic and finite-sum optimization problems. Frank-Wolfe methods (in the convex case) have gained tremendous recent interest in machine learning and optimization communities due to their projection-free property and their ability to exploit structured constraints. However, our understanding of these algorithms in the nonconvex setting is fairly limited. In this paper, we propose nonconvex stochastic Frank-Wolfe methods and analyze their convergence properties. For objective functions that decompose into a finite-sum, we leverage ideas from variance reduction techniques for convex optimization to obtain new variance reduced nonconvex Frank-Wolfe methods that have provably faster convergence than the classical Frank-Wolfe method. Finally, we show that the faster convergence rates of our variance reduced methods also translate into improved convergence rates for the stochastic setting. version:2
arxiv-1607-08665 | Introspective Perception: Learning to Predict Failures in Vision Systems | http://arxiv.org/abs/1607.08665 | id:1607.08665 author:Shreyansh Daftry, Sam Zeng, J. Andrew Bagnell, Martial Hebert category:cs.RO cs.AI cs.CV  published:2016-07-28 summary:As robots aspire for long-term autonomous operations in complex dynamic environments, the ability to reliably take mission-critical decisions in ambiguous situations becomes critical. This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision. We call this self-evaluating capability as introspection. In this paper, we take a small step in this direction and propose a generic framework for introspective behavior in perception systems. Our goal is to learn a model to reliably predict failures in a given system, with respect to a task, directly from input sensor data. We present this in the context of vision-based autonomous MAV flight in outdoor natural environments, and show that it effectively handles uncertain situations. version:1
arxiv-1607-08659 | General Automatic Human Shape and Motion Capture Using Volumetric Contour Cues | http://arxiv.org/abs/1607.08659 | id:1607.08659 author:Helge Rhodin, Nadia Robertini, Dan Casas, Christian Richardt, Hans-Peter Seidel, Christian Theobalt category:cs.CV  published:2016-07-28 summary:Markerless motion capture algorithms require a 3D body with properly personalized skeleton dimension and/or body shape and appearance to successfully track a person. Unfortunately, many tracking methods consider model personalization a different problem and use manual or semi-automatic model initialization, which greatly reduces applicability. In this paper, we propose a fully automatic algorithm that jointly creates a rigged actor model commonly used for animation - skeleton, volumetric shape, appearance, and optionally a body surface - and estimates the actor's motion from multi-view video input only. The approach is rigorously designed to work on footage of general outdoor scenes recorded with very few cameras and without background subtraction. Our method uses a new image formation model with analytic visibility and analytically differentiable alignment energy. For reconstruction, 3D body shape is approximated as Gaussian density field. For pose and shape estimation, we minimize a new edge-based alignment energy inspired by volume raycasting in an absorbing medium. We further propose a new statistical human body model that represents the body surface, volumetric Gaussian density, as well as variability in skeleton shape. Given any multi-view sequence, our method jointly optimizes the pose and shape parameters of this model fully automatically in a spatiotemporal way. version:1
arxiv-1607-08647 | Asymptotic properties of Principal Component Analysis and shrinkage-bias adjustment under the Generalized Spiked Population model | http://arxiv.org/abs/1607.08647 | id:1607.08647 author:Rounak Dey, Seunggeun Lee category:math.ST stat.ML stat.TH  published:2016-07-28 summary:With the development of high-throughput technologies, principal component analysis (PCA) in the high-dimensional regime is of great interest. Most of the existing theoretical and methodological results for high-dimensional PCA are based on the spiked population model in which all the population eigenvalues are equal except for a few large ones. Due to the presence of local correlation among features, however, this assumption may not be satisfied in many real-world datasets. To address this issue, we investigated the asymptotic behaviors of PCA under the generalized spiked population model. Based on the theoretical results, we proposed a series of methods for the consistent estimation of population eigenvalues, angles between the sample and population eigenvectors, correlation coefficients between the sample and population principal component (PC) scores, and the shrinkage bias adjustment for the predicted PC scores. Using numerical experiments and real data examples from the genetics literature, we showed that our methods can greatly reduce bias and improve prediction accuracy. version:1
arxiv-1607-08634 | Attribute Learning for Network Intrusion Detection | http://arxiv.org/abs/1607.08634 | id:1607.08634 author:Jorge Luis Rivero Pérez, Bernardete Ribeiro category:cs.CR cs.LG  published:2016-07-28 summary:Network intrusion detection is one of the most visible uses for Big Data analytics. One of the main problems in this application is the constant rise of new attacks. This scenario, characterized by the fact that not enough labeled examples are available for the new classes of attacks is hardly addressed by traditional machine learning approaches. New findings on the capabilities of Zero-Shot learning (ZSL) approach makes it an interesting solution for this problem because it has the ability to classify instances of unseen classes. ZSL has inherently two stages: the attribute learning and the inference stage. In this paper we propose a new algorithm for the attribute learning stage of ZSL. The idea is to learn new values for the attributes based on decision trees (DT). Our results show that based on the rules extracted from the DT a better distribution for the attribute values can be found. We also propose an experimental setup for the evaluation of ZSL on network intrusion detection (NID). version:1
arxiv-1607-08601 | Limit theorems for eigenvectors of the normalized Laplacian for random graphs | http://arxiv.org/abs/1607.08601 | id:1607.08601 author:Minh Tang, Carey E. Priebe category:stat.ML 62H12  62H30  62B10  published:2016-07-28 summary:We prove a central limit theorem for the components of the eigenvectors corresponding to the $d$ largest eigenvalues of the normalized Laplacian matrix of a finite dimensional random dot product graph. As a corollary, we show that for stochastic blockmodel graphs, the rows of the spectral embedding of the normalized Laplacian converge to multivariate normals and furthermore the mean and the covariance matrix of each row are functions of the associated vertex's block membership. Together with prior results for the eigenvectors of the adjacency matrix, we then compare, via the Chernoff information between multivariate normal distributions, how the choice of embedding method impacts subsequent inference. We demonstrate that neither embedding method dominates with respect to the inference task of recovering the latent block assignments. version:1
arxiv-1607-08592 | Modeling selectional restrictions in a relational type system | http://arxiv.org/abs/1607.08592 | id:1607.08592 author:Erkki Luuk category:cs.CL cs.AI  published:2016-07-28 summary:Selectional restrictions are semantic constraints on forming certain complex types in natural language. The paper gives an overview of modeling selectional restrictions in a relational type system with morphological and syntactic types. We discuss some foundations of the system and ways of formalizing selectional restrictions. Keywords: type theory, selectional restrictions, syntax, morphology version:1
arxiv-1607-08584 | Connectionist Temporal Modeling for Weakly Supervised Action Labeling | http://arxiv.org/abs/1607.08584 | id:1607.08584 author:De-An Huang, Li Fei-Fei, Juan Carlos Niebles category:cs.CV  published:2016-07-28 summary:We propose a weakly-supervised framework for action labeling in video, where only the order of occurring actions is required during training time. The key challenge is that the per-frame alignments between the input (video) and label (action) sequences are unknown during training. We address this by introducing the Extended Connectionist Temporal Classification (ECTC) framework to efficiently evaluate all possible alignments via dynamic programming and explicitly enforce their consistency with frame-to-frame visual similarities. This protects the model from distractions of visually inconsistent or degenerated alignments without the need of temporal supervision. We further extend our framework to the semi-supervised case when a few frames are sparsely annotated in a video. With less than 1% of labeled frames per video, our method is able to outperform existing semi-supervised approaches and achieve comparable performance to that of fully supervised approaches. version:1
arxiv-1607-08569 | A Deep Primal-Dual Network for Guided Depth Super-Resolution | http://arxiv.org/abs/1607.08569 | id:1607.08569 author:Gernot Riegler, David Ferstl, Matthias Rüther, Horst Bischof category:cs.CV  published:2016-07-28 summary:In this paper we present a novel method to increase the spatial resolution of depth images. We combine a deep fully convolutional network with a non-local variational method in a deep primal-dual network. The joint network computes a noise-free, high-resolution estimate from a noisy, low-resolution input depth map. Additionally, a high-resolution intensity image is used to guide the reconstruction in the network. By unrolling the optimization steps of a first-order primal-dual algorithm and formulating it as a network, we can train our joint method end-to-end. This not only enables us to learn the weights of the fully convolutional network, but also to optimize all parameters of the variational method and its optimization procedure. The training of such a deep network requires a large dataset for supervision. Therefore, we generate high-quality depth maps and corresponding color images with a physically based renderer. In an exhaustive evaluation we show that our method outperforms the state-of-the-art on multiple benchmarks. version:1
arxiv-1607-08481 | A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second Order Statistics | http://arxiv.org/abs/1607.08481 | id:1607.08481 author:Friederike Laus, Johannes Persch, Gabriele Steidl category:math.NA cs.CV  published:2016-07-28 summary:Nonlocal patch-based methods, in particular the Bayes' approach of Lebrun, Buades and Morel (2013), are considered as state-of-the-art methods for denoising (color) images corrupted by white Gaussian noise of moderate variance. This paper is the first attempt to generalize this technique to manifold-valued images. Such images, for example images with phase or directional entries or with values in the manifold of symmetric positive definite matrices, are frequently encountered in real-world applications. Generalizing the normal law to manifolds is not canonical and different attempts have been considered. Here we focus on a straightforward intrinsic model and discuss the relation to other approaches for specific manifolds. We reinterpret the Bayesian approach of Lebrun et al. (2013) in terms of minimum mean squared error estimation, which motivates our definition of a corresponding estimator on the manifold. With this estimator at hand we present a nonlocal patch-based method for the restoration of manifold-valued images. Various proof of concept examples demonstrate the potential of the proposed algorithm. version:1
arxiv-1607-08477 | SSDH: Semi-supervised Deep Hashing for Large Scale Image Retrieval | http://arxiv.org/abs/1607.08477 | id:1607.08477 author:Jian Zhang, Yuxin Peng, Junchao Zhang category:cs.CV H.3.1  published:2016-07-28 summary:The hashing methods have been widely used for efficient similarity retrieval on large scale image datasets. The traditional hashing methods learn hash functions to generate binary codes from hand-crafted features, which achieve limited accuracy since the hand-crafted features cannot optimally represent the image content and preserve the semantic similarity. Recently, several deep hashing methods have shown better performance because the deep architectures generate more discriminative feature representations. However, these deep hashing methods are mainly designed for the supervised scenarios, which only exploit the semantic similarity information, but ignore the underlying data structures. In this paper, we propose the semi-supervised deep hashing (SSDH) method, to perform more effective hash learning by simultaneously preserving the semantic similarity and the underlying data structures. Our proposed approach can be divided into two phases. First, a deep network is designed to extensively exploit both the labeled and unlabeled data, in which we construct the similarity graph online in a mini-batch with the deep feature representations. To the best of our knowledge, our proposed deep network is the first deep hashing method that can perform the hash code learning and feature learning simultaneously in a semi-supervised fashion. Second, we propose a loss function suitable for the semi-supervised scenario by jointly minimizing the empirical error on the labeled data as well as the embedding error on both the labeled and unlabeled data, which can preserve the semantic similarity, as well as capture the meaningful neighbors on the underlying data structures for effective hashing. Experiment results on 4 widely used datasets show that the proposed approach outperforms state-of-the-art hashing methods. version:1
arxiv-1607-08458 | The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG source reconstruction | http://arxiv.org/abs/1607.08458 | id:1607.08458 author:Daniel Strohmeier, Yousra Bekhti, Jens Haueisen, Alexandre Gramfort category:stat.AP q-bio.NC stat.CO stat.ML  published:2016-07-28 summary:Source imaging based on magnetoencephalography (MEG) and electroencephalography (EEG) allows for the non-invasive analysis of brain activity with high temporal and good spatial resolution. As the bioelectromagnetic inverse problem is ill-posed, constraints are required. For the analysis of evoked brain activity, spatial sparsity of the neuronal activation is a common assumption. It is often taken into account using convex constraints based on the l1-norm. The resulting source estimates are however biased in amplitude and often suboptimal in terms of source selection due to high correlations in the forward model. In this work, we demonstrate that an inverse solver based on a block-separable penalty with a Frobenius norm per block and a l0.5-quasinorm over blocks addresses both of these issues. For solving the resulting non-convex optimization problem, we propose the iterative reweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on iterative reweighted convex surrogate optimization problems, which are solved efficiently using a block coordinate descent scheme and an active set strategy. We compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC approach based on two MEG data sets. We provide empirical evidence based on simulations and analysis of MEG data that the proposed method improves on the standard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support recovery, and stability. version:1
arxiv-1607-08456 | Kernel functions based on triplet similarity comparisons | http://arxiv.org/abs/1607.08456 | id:1607.08456 author:Matthäus Kleindessner, Ulrike von Luxburg category:stat.ML cs.DS cs.LG  published:2016-07-28 summary:We propose two ways of defining a kernel function on a data set when the only available information about the data set are similarity triplets of the form "Object A is more similar to object B than to object C". Studying machine learning and data mining problems based on such restricted information has become very popular in recent years since it can easily be provided by humans via crowd sourcing. While previous approaches try to construct a low-dimensional Euclidean embedding of the data set that reflects the given similarity triplets, we aim at defining meaningful kernel functions on the data set that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply all the standard kernel methods to solve tasks such as clustering, classification or principal component analysis on the data set. version:1
arxiv-1607-08438 | Faceless Person Recognition; Privacy Implications in Social Media | http://arxiv.org/abs/1607.08438 | id:1607.08438 author:Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele category:cs.CV cs.AI cs.CR  published:2016-07-28 summary:As we shift more of our lives into the virtual domain, the volume of data shared on the web keeps increasing and presents a threat to our privacy. This works contributes to the understanding of privacy implications of such data sharing by analysing how well people are recognisable in social media data. To facilitate a systematic study we define a number of scenarios considering factors such as how many heads of a person are tagged and if those heads are obfuscated or not. We propose a robust person recognition system that can handle large variations in pose and clothing, and can be trained with few training samples. Our results indicate that a handful of images is enough to threaten users' privacy, even in the presence of obfuscation. We show detailed experimental results, and discuss their implications. version:1
arxiv-1607-08434 | Video Registration in Egocentric Vision under Day and Night Illumination Changes | http://arxiv.org/abs/1607.08434 | id:1607.08434 author:Stefano Alletto, Giuseppe Serra, Rita Cucchiara category:cs.CV  published:2016-07-28 summary:With the spread of wearable devices and head mounted cameras, a wide range of application requiring precise user localization is now possible. In this paper we propose to treat the problem of obtaining the user position with respect to a known environment as a video registration problem. Video registration, i.e. the task of aligning an input video sequence to a pre-built 3D model, relies on a matching process of local keypoints extracted on the query sequence to a 3D point cloud. The overall registration performance is strictly tied to the actual quality of this 2D-3D matching, and can degrade if environmental conditions such as steep changes in lighting like the ones between day and night occur. To effectively register an egocentric video sequence under these conditions, we propose to tackle the source of the problem: the matching process. To overcome the shortcomings of standard matching techniques, we introduce a novel embedding space that allows us to obtain robust matches by jointly taking into account local descriptors, their spatial arrangement and their temporal robustness. The proposal is evaluated using unconstrained egocentric video sequences both in terms of matching quality and resulting registration performance using different 3D models of historical landmarks. The results show that the proposed method can outperform state of the art registration algorithms, in particular when dealing with the challenges of night and day sequences. version:1
arxiv-1607-08421 | Stereo Video Deblurring | http://arxiv.org/abs/1607.08421 | id:1607.08421 author:Anita Sellent, Carsten Rother, Stefan Roth category:cs.CV I.4.3  published:2016-07-28 summary:Videos acquired in low-light conditions often exhibit motion blur, which depends on the motion of the objects relative to the camera. This is not only visually unpleasing, but can hamper further processing. With this paper we are the first to show how the availability of stereo video can aid the challenging video deblurring task. We leverage 3D scene flow, which can be estimated robustly even under adverse conditions. We go beyond simply determining the object motion in two ways: First, we show how a piecewise rigid 3D scene flow representation allows to induce accurate blur kernels via local homographies. Second, we exploit the estimated motion boundaries of the 3D scene flow to mitigate ringing artifacts using an iterative weighting scheme. Being aware of 3D object motion, our approach can deal robustly with an arbitrary number of independently moving objects. We demonstrate its benefit over state-of-the-art video deblurring using quantitative and qualitative experiments on rendered scenes and real videos. version:1
arxiv-1607-08400 | Randomised Algorithm for Feature Selection and Classification | http://arxiv.org/abs/1607.08400 | id:1607.08400 author:Aida Brankovic, Alessandro Falsone, Maria Prandini, Luigi Piroddi category:cs.LG  published:2016-07-28 summary:We here introduce a novel classification approach adopted from the nonlinear model identification framework, which jointly addresses the feature selection and classifier design tasks. The classifier is constructed as a polynomial expansion of the original attributes and a model structure selection process is applied to find the relevant terms of the model. The selection method progressively refines a probability distribution defined on the model structure space, by extracting sample models from the current distribution and using the aggregate information obtained from the evaluation of the population of models to reinforce the probability of extracting the most important terms. To reduce the initial search space, distance correlation filtering can be applied as a preprocessing technique. The proposed method is evaluated and compared to other well-known feature selection and classification methods on standard benchmark classification problems. The results show the effectiveness of the proposed method with respect to competitor methods both in terms of classification accuracy and model complexity. The obtained models have a simple structure, easily amenable to interpretation and analysis. version:1
arxiv-1607-08381 | A Siamese Long Short-Term Memory Architecture for Human Re-Identification | http://arxiv.org/abs/1607.08381 | id:1607.08381 author:Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang category:cs.CV  published:2016-07-28 summary:Matching pedestrians across multiple camera views known as human re-identification (re-identification) is a challenging problem in visual surveillance. In the existing works concentrating on feature extraction, representations are formed locally and independent of other regions. We present a novel siamese Long Short-Term Memory (LSTM) architecture that can process image regions sequentially and enhance the discriminative capability of local feature representation by leveraging contextual information. The feedback connections and internal gating mechanism of the LSTM cells enable our model to memorize the spatial dependencies and selectively propagate relevant contextual information through the network. We demonstrate improved performance compared to the baseline algorithm with no LSTM units and promising results compared to state-of-the-art methods on Market-1501, CUHK03 and VIPeR datasets. Visualization of the internal mechanism of LSTM cells shows meaningful patterns can be learned by our method. version:1
arxiv-1607-08379 | Variational perturbation and extended Plefka approaches to dynamics on random networks: the case of the kinetic Ising model | http://arxiv.org/abs/1607.08379 | id:1607.08379 author:Ludovica Bachschmid-Romano, Claudia Battistin, Manfred Opper, Yasser Roudi category:cond-mat.dis-nn physics.data-an stat.ML  published:2016-07-28 summary:We describe and analyze some novel approaches for studying the dynamics of Ising spin glass models. We first briefly consider the variational approach based on minimizing the Kullback-Leibler divergence between independent trajectories and the real ones and note that this approach only coincides with the mean field equations from the saddle point approximation to the generating functional when the dynamics is defined through a logistic link function, which is the case for the kinetic Ising model with parallel update. We then spend the rest of the paper developing two ways of going beyond the saddle point approximation to the generating functional. In the first one, we develop a variational perturbative approximation to the generating functional by expanding the action around a quadratic function of the local fields and conjugate local fields whose parameters are optimized. We derive analytical expressions for the optimal parameters and show that when the optimization is suitably restricted, we recover the mean field equations that are exact for the fully asymmetric random couplings (M\'ezard and Sakellariou, 2011). However, without this restriction the results are different. We also describe an extended Plefka expansion in which in addition to the magnetization, we also fix the correlation and response functions. Finally, we numerically study the performance of these approximations for Sherrington-Kirkpatrick type couplings for various coupling strengths, degrees of coupling symmetry and external fields. We show that the dynamical equations derived from the extended Plefka expansion outperform the others in all regimes, although it is computationally more demanding. The unconstrained variational approach does not perform well in the small coupling regime, while it approaches dynamical TAP equations of (Roudi and Hertz, 2011) for strong couplings. version:1
arxiv-1607-08378 | Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification | http://arxiv.org/abs/1607.08378 | id:1607.08378 author:Rahul Rama Varior, Mrinal Haloi, Gang Wang category:cs.CV  published:2016-07-28 summary:Matching pedestrians across multiple camera views, known as human re-identification, is a challenging research problem that has numerous applications in visual surveillance. With the resurgence of Convolutional Neural Networks (CNNs), several end-to-end deep Siamese CNN architectures have been proposed for human re-identification with the objective of projecting the images of similar pairs (i.e. same identity) to be closer to each other and those of dissimilar pairs to be distant from each other. However, current networks extract fixed representations for each image regardless of other images which are paired with it and the comparison with other images is done only at the final level. In this setting, the network is at risk of failing to extract finer local patterns that may be essential to distinguish positive pairs from hard negative pairs. In this paper, we propose a gating function to selectively emphasize such fine common local patterns by comparing the mid-level features across pairs of images. This produces flexible representations for the same image according to the images they are paired with. We conduct experiments on the CUHK03, Market-1501 and VIPeR datasets and demonstrate improved performance compared to a baseline Siamese CNN architecture. version:1
arxiv-1607-08368 | Local Feature Detectors, Descriptors, and Image Representations: A Survey | http://arxiv.org/abs/1607.08368 | id:1607.08368 author:Yusuke Uchida category:cs.CV  published:2016-07-28 summary:With the advances in both stable interest region detectors and robust and distinctive descriptors, local feature-based image or object retrieval has become a popular research topic. %All of the local feature-based image retrieval system involves two important processes: local feature extraction and image representation. The other key technology for image retrieval systems is image representation such as the bag-of-visual words (BoVW), Fisher vector, or Vector of Locally Aggregated Descriptors (VLAD) framework. In this paper, we review local features and image representations for image retrieval. Because many and many methods are proposed in this area, these methods are grouped into several classes and summarized. In addition, recent deep learning-based approaches for image retrieval are briefly reviewed. version:1
arxiv-1607-08366 | 25 years of CNNs: Can we compare to human abstraction capabilities? | http://arxiv.org/abs/1607.08366 | id:1607.08366 author:Sebastian Stabinger, Antonio Rodríguez-Sánchez, Justus Piater category:cs.CV  published:2016-07-28 summary:We try to determine the progress made by convolutional neural networks over the past 25 years in classifying images into abstractc lasses. For this purpose we compare the performance of LeNet to that of GoogLeNet at classifying randomly generated images which are differentiated by an abstract property (e.g., one class contains two objects of the same size, the other class two objects of different sizes). Our results show that there is still work to do in order to solve vision problems humans are able to solve without much difficulty. version:1
arxiv-1607-08316 | Hyperparameter Optimization of Deep Neural Networks Using Non-Probabilistic RBF Surrogate Model | http://arxiv.org/abs/1607.08316 | id:1607.08316 author:Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, Christine Annette Shoemaker category:cs.AI cs.LG stat.ML  published:2016-07-28 summary:Recently, Bayesian optimization has been successfully applied for optimizing hyperparameters of deep neural networks, significantly outperforming the expert-set hyperparameter values. The methods approximate and minimize the validation error as a function of hyperparameter values through probabilistic models like Gaussian processes. However, probabilistic models that require a prior distribution of the errors may be not adequate for approximating very complex error functions of deep neural networks. In this work, we propose to employ radial basis function as the surrogate of the error functions for optimizing both continuous and integer hyperparameters. The proposed non-probabilistic algorithm, called Hyperparameter Optimization using RBF and DYCORS (HORD), searches the surrogate for the most promising hyperparameter values while providing a good balance between exploration and exploitation. Extensive evaluations demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as Spearmint and TPE, both in terms of finding a near optimal solution with fewer expensive function evaluations, and in terms of a final validation error. Further, HORD performs equally well in low- and high-dimensional hyperparameter spaces, and by avoiding expensive covariance computation can also scale to a high number of observations. version:1
arxiv-1607-08310 | Preterm Birth Prediction: Deriving Stable and Interpretable Rules from High Dimensional Data | http://arxiv.org/abs/1607.08310 | id:1607.08310 author:Truyen Tran, Wei Luo, Dinh Phung, Jonathan Morris, Kristen Rickard, Svetha Venkatesh category:stat.ML  published:2016-07-28 summary:Preterm births occur at an alarming rate of 10-15%. Preemies have a higher risk of infant mortality, developmental retardation and long-term disabilities. Predicting preterm birth is difficult, even for the most experienced clinicians. The most well-designed clinical study thus far reaches a modest sensitivity of 18.2-24.2% at specificity of 28.6-33.3%. We take a different approach by exploiting databases of normal hospital operations. We aims are twofold: (i) to derive an easy-to-use, interpretable prediction rule with quantified uncertainties, and (ii) to construct accurate classifiers for preterm birth prediction. Our approach is to automatically generate and select from hundreds (if not thousands) of possible predictors using stability-aware techniques. Derived from a large database of 15,814 women, our simplified prediction rule with only 10 items has sensitivity of 62.3% at specificity of 81.5%. version:1
arxiv-1607-08289 | Mammalian Value Systems | http://arxiv.org/abs/1607.08289 | id:1607.08289 author:Gopal P. Sarma, Nick J. Hay category:cs.AI cs.CY cs.HC cs.LG cs.RO  published:2016-07-28 summary:Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical "intelligence explosion," in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The "value alignment problem" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian kingdom provide important conceptual foundations relevant to describing human values. We argue that the notion of "mammalian value systems" points to a potential avenue for fundamental research in AI safety and AI ethics. version:1
arxiv-1607-08635 | A 58.6mW Real-Time Programmable Object Detector with Multi-Scale Multi-Object Support Using Deformable Parts Model on 1920x1080 Video at 30fps | http://arxiv.org/abs/1607.08635 | id:1607.08635 author:Amr Suleiman, Zhengdong Zhang, Vivienne Sze category:cs.CV cs.AR  published:2016-07-27 summary:This paper presents a programmable, energy-efficient and real-time object detection accelerator using deformable parts models (DPM), with 2x higher accuracy than traditional rigid body models. With 8 deformable parts detection, three methods are used to address the high computational complexity: classification pruning for 33x fewer parts classification, vector quantization for 15x memory size reduction, and feature basis projection for 2x reduction of the cost of each classification. The chip is implemented in 65nm CMOS technology, and can process HD (1920x1080) images at 30fps without any off-chip storage while consuming only 58.6mW (0.94nJ/pixel, 1168 GOPS/W). The chip has two classification engines to simultaneously detect two different classes of objects. With a tested high throughput of 60fps, the classification engines can be time multiplexed to detect even more than two object classes. It is energy scalable by changing the pruning factor or disabling the parts classification. version:1
arxiv-1607-08221 | MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition | http://arxiv.org/abs/1607.08221 | id:1607.08221 author:Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao category:cs.CV  published:2016-07-27 summary:In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world. version:1
arxiv-1607-08206 | Diagnostic Prediction Using Discomfort Drawings with IBTM | http://arxiv.org/abs/1607.08206 | id:1607.08206 author:Cheng Zhang, Hedvig Kjellstrom, Carl Henrik Ek, Bo C. Bertilson category:cs.LG  published:2016-07-27 summary:In this paper, we explore the possibility to apply machine learning to make diagnostic predictions using discomfort drawings. A discomfort drawing is an intuitive way for patients to express discomfort and pain related symptoms. These drawings have proven to be an effective method to collect patient data and make diagnostic decisions in real-life practice. A dataset from real-world patient cases is collected for which medical experts provide diagnostic labels. Next, we use a factorized multimodal topic model, Inter-Battery Topic Model (IBTM), to train a system that can make diagnostic predictions given an unseen discomfort drawing. The number of output diagnostic labels is determined by using mean-shift clustering on the discomfort drawing. Experimental results show reasonable predictions of diagnostic labels given an unseen discomfort drawing. Additionally, we generate synthetic discomfort drawings with IBTM given a diagnostic label, which results in typical cases of symptoms. The positive result indicates a significant potential of machine learning to be used for parts of the pain diagnostic process and to be a decision support system for physicians and other health care personal. version:1
arxiv-1607-08196 | Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home | http://arxiv.org/abs/1607.08196 | id:1607.08196 author:Lili Tao, Tilo Burghardt, Majid Mirmehdi, Dima Damen, Ashley Cooper, Sion Hannuna, Massimo Camplani, Adeline Paiement, Ian Craddock category:cs.CV  published:2016-07-27 summary:We present a new framework for vision-based estimation of calorific expenditure from RGB-D data - the first that is validated on physical gas exchange measurements and applied to daily living scenarios. Deriving a person's energy expenditure from sensors is an important tool in tracking physical activity levels for health and lifestyle monitoring. Most existing methods use metabolic lookup tables (METs) for a manual estimate or systems with inertial sensors which ultimately require users to wear devices. In contrast, the proposed pose-invariant and individual-independent vision framework allows for a remote estimation of calorific expenditure. We introduce, and evaluate our approach on, a new dataset called SPHERE-calorie, for which visual estimates can be compared against simultaneously obtained, indirect calorimetry measures based on gas exchange. % based on per breath gas exchange. We conclude from our experiments that the proposed vision pipeline is suitable for home monitoring in a controlled environment, with calorific expenditure estimates above accuracy levels of commonly used manual estimations via METs. With the dataset released, our work establishes a baseline for future research for this little-explored area of computer vision. version:1
arxiv-1607-08161 | Network-Guided Biomarker Discovery | http://arxiv.org/abs/1607.08161 | id:1607.08161 author:Chloé-Agathe Azencott category:stat.ML cs.LG q-bio.QM  published:2016-07-27 summary:Identifying measurable genetic indicators (or biomarkers) of a specific condition of a biological system is a key element of precision medicine. Indeed it allows to tailor diagnostic, prognostic and treatment choice to individual characteristics of a patient. In machine learning terms, biomarker discovery can be framed as a feature selection problem on whole-genome data sets. However, classical feature selection methods are usually underpowered to process these data sets, which contain orders of magnitude more features than samples. This can be addressed by making the assumption that genetic features that are linked on a biological network are more likely to work jointly towards explaining the phenotype of interest. We review here three families of methods for feature selection that integrate prior knowledge in the form of networks. version:1
arxiv-1607-08236 | Adaptive foveated single-pixel imaging with dynamic super-sampling | http://arxiv.org/abs/1607.08236 | id:1607.08236 author:David B. Phillips, Ming-Jie Sun, Jonathan M. Taylor, Matthew P. Edgar, Stephen M. Barnett, Graham G. Gibson, Miles J. Padgett category:cs.CV physics.optics  published:2016-07-27 summary:As an alternative to conventional multi-pixel cameras, single-pixel cameras enable images to be recorded using a single detector that measures the correlations between the scene and a set of patterns. However, to fully sample a scene in this way requires at least the same number of correlation measurements as there are pixels in the reconstructed image. Therefore single-pixel imaging systems typically exhibit low frame-rates. To mitigate this, a range of compressive sensing techniques have been developed which rely on a priori knowledge of the scene to reconstruct images from an under-sampled set of measurements. In this work we take a different approach and adopt a strategy inspired by the foveated vision systems found in the animal kingdom - a framework that exploits the spatio-temporal redundancy present in many dynamic scenes. In our single-pixel imaging system a high-resolution foveal region follows motion within the scene, but unlike a simple zoom, every frame delivers new spatial information from across the entire field-of-view. Using this approach we demonstrate a four-fold reduction in the time taken to record the detail of rapidly evolving features, whilst simultaneously accumulating detail of more slowly evolving regions over several consecutive frames. This tiered super-sampling technique enables the reconstruction of video streams in which both the resolution and the effective exposure-time spatially vary and adapt dynamically in response to the evolution of the scene. The methods described here can complement existing compressive sensing approaches and may be applied to enhance a variety of computational imagers that rely on sequential correlation measurements. version:1
arxiv-1607-08129 | Spatial probabilistic pulsatility model for enhancing photoplethysmographic imaging systems | http://arxiv.org/abs/1607.08129 | id:1607.08129 author:Robert Amelard, David A Clausi, Alexander Wong category:q-bio.QM cs.CV  published:2016-07-27 summary:Photolethysmographic imaging (PPGI) is a widefield non-contact biophotonic technology able to remotely monitor cardiovascular function over anatomical areas. Though spatial context can provide increased physiological insight, existing PPGI systems rely on coarse spatial averaging with no anatomical priors for assessing arterial pulsatility. Here, we developed a continuous probabilistic pulsatility model for importance-weighted blood pulse waveform extraction. Using a data-driven approach, the model was constructed using a 23 participant sample with large demographic variation (11/12 female/male, age 11-60 years, BMI 16.4-35.1 kg$\cdot$m$^{-2}$). Using time-synchronized ground-truth waveforms, spatial correlation priors were computed and projected into a co-aligned importance-weighted Cartesian space. A modified Parzen-Rosenblatt kernel density estimation method was used to compute the continuous resolution-agnostic probabilistic pulsatility model. The model identified locations that consistently exhibited pulsatility across the sample. Blood pulse waveform signals extracted with the model exhibited significantly stronger temporal correlation ($W=35,p<0.01$) and spectral SNR ($W=31,p<0.01$) compared to uniform spatial averaging. Heart rate estimation was in strong agreement with true heart rate ($r^2=0.9619$, error $(\mu,\sigma)=(0.52,1.69)$ bpm). version:1
arxiv-1607-08128 | Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image | http://arxiv.org/abs/1607.08128 | id:1607.08128 author:Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J. Black category:cs.CV  published:2016-07-27 summary:We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art. version:1
arxiv-1607-08112 | MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem | http://arxiv.org/abs/1607.08112 | id:1607.08112 author:Steffen Urban, Jens Leitloff, Stefan Hinz category:cs.CV  published:2016-07-27 summary:In this paper, a statistically optimal solution to the Perspective-n-Point (PnP) problem is presented. Many solutions to the PnP problem are geometrically optimal, but do not consider the uncertainties of the observations. In addition, it would be desirable to have an internal estimation of the accuracy of the estimated rotation and translation parameters of the camera pose. Thus, we propose a novel maximum likelihood solution to the PnP problem, that incorporates image observation uncertainties and remains real-time capable at the same time. Further, the presented method is general, as is works with 3D direction vectors instead of 2D image points and is thus able to cope with arbitrary central camera models. This is achieved by projecting (and thus reducing) the covariance matrices of the observations to the corresponding vector tangent space. version:1
arxiv-1607-08110 | Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD | http://arxiv.org/abs/1607.08110 | id:1607.08110 author:Joseph D. Ramsey, Daniel Malinsky category:stat.ML stat.CO  published:2016-07-27 summary:In this report we describe a tool for comparing the performance of causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as a set of packages in the TETRAD source code, which can be loaded up in an Integrated Development Environment (IDE) such as IntelliJ IDEA. Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The order of the algorithms in the output can be adjusted to the user's preference using a utility function over the statistics. Data sets from simulation can be saved along with their graphs to a file and loaded back in for further analysis, or used for analysis by other tools. version:1
arxiv-1607-08085 | Improving Semantic Embedding Consistency by Metric Learning for Zero-Shot Classification | http://arxiv.org/abs/1607.08085 | id:1607.08085 author:Maxime Bucher, Stéphane Herbin, Frédéric Jurie category:cs.CV cs.AI cs.LG math.ST stat.TH  published:2016-07-27 summary:This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images -- one of the main ingredients of zero-shot learning -- by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation. version:1
arxiv-1607-08074 | Mining Arguments from Cancer Documents Using Natural Language Processing and Ontologies | http://arxiv.org/abs/1607.08074 | id:1607.08074 author:Adrian Groza, Oana Popa category:cs.AI cs.CL  published:2016-07-27 summary:In the medical domain, the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments. As medical expertise occurs at different levels, part of the human agents have difficulties to face the huge amount of studies, but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic. To better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers. Our work here aims to fill the above technological gap. Quite aware of the difficulty of this task, we embark to this road by relying on the well-known interleaving of domain knowledge with natural language processing. To formalise the existing medical knowledge, we rely on ontologies. To structure the argumentation model we use also the expressivity and reasoning capabilities of Description Logics. To perform argumentation mining we formalise various linguistic patterns in a rule-based language. We tested our solution against a corpus of scientific papers related to breast cancer. The run experiments show a F-measure between 0.71 and 0.86 for identifying conclusions of an argument and between 0.65 and 0.86 for identifying premises of an argument. version:1
arxiv-1607-08064 | CNN based Patch Matching for Optical Flow with Thresholded Hinge Loss | http://arxiv.org/abs/1607.08064 | id:1607.08064 author:Christian Bailer, Kiran Varanasi, Didier Stricker category:cs.CV cs.LG cs.NE  published:2016-07-27 summary:Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a novel optical flow pipeline that uses patch-matching with CNN trained features at multiple scales. We show a novel way for calculating CNN based features for different scales, which performs better than existing methods. Furthermore, we introduce a new thresholded loss for Siamese networks and demonstrate that our novel loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Moreover, we discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low pass filtering feature maps can increase the robustness of features created by CNNs. We prove competitive performance of our approach by submitting it to the KITTI 2012 and KITTI 2015 evaluation portals and obtaining the best results on KITTI 2012 and the best for foreground objects in KITTI 2015. version:1
arxiv-1607-08040 | Visual Tracking via Shallow and Deep Collaborative Model | http://arxiv.org/abs/1607.08040 | id:1607.08040 author:Bohan Zhuang, Lijun Wang, Huchuan Lu category:cs.CV  published:2016-07-27 summary:In this paper, we propose a robust tracking method based on the collaboration of a generative model and a discriminative classifier, where features are learned by shallow and deep architectures, respectively. For the generative model, we introduce a block-based incremental learning scheme, in which a local binary mask is constructed to deal with occlusion. The similarity degrees between the local patches and their corresponding subspace are integrated to formulate a more accurate global appearance model. In the discriminative model, we exploit the advances of deep learning architectures to learn generic features which are robust to both background clutters and foreground appearance variations. To this end, we first construct a discriminative training set from auxiliary video sequences. A deep classification neural network is then trained offline on this training set. Through online fine-tuning, both the hierarchical feature extractor and the classifier can be adapted to the appearance change of the target for effective online tracking. The collaboration of these two models achieves a good balance in handling occlusion and target appearance change, which are two contradictory challenging factors in visual tracking. Both quantitative and qualitative evaluations against several state-of-the-art algorithms on challenging image sequences demonstrate the accuracy and the robustness of the proposed tracker. version:1
arxiv-1607-08022 | Instance Normalization: The Missing Ingredient for Fast Stylization | http://arxiv.org/abs/1607.08022 | id:1607.08022 author:Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky category:cs.CV  published:2016-07-27 summary:It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will be made available at https://github.com/DmitryUlyanov/texture_nets. version:1
arxiv-1607-08012 | Learning of Generalized Low-Rank Models: A Greedy Approach | http://arxiv.org/abs/1607.08012 | id:1607.08012 author:Quanming Yao, James T. Kwok category:cs.LG cs.NA math.OC  published:2016-07-27 summary:Learning of low-rank matrices is fundamental to many machine learning applications. A state-of-the-art algorithm is the rank-one matrix pursuit (R1MP). However, it can only be used in matrix completion problems with the square loss. In this paper, we develop a more flexible greedy algorithm for generalized low-rank models whose optimization objective can be smooth or nonsmooth, general convex or strongly convex. The proposed algorithm has low per-iteration time complexity and fast convergence rate. Experimental results show that it is much faster than the state-of-the-art, with comparable or even better prediction performance. version:1
arxiv-1607-07988 | ATGV-Net: Accurate Depth Super-Resolution | http://arxiv.org/abs/1607.07988 | id:1607.07988 author:Gernot Riegler, Matthias Rüther, Horst Bischof category:cs.CV  published:2016-07-27 summary:In this work we present a novel approach for single depth map super-resolution. Modern consumer depth sensors, especially Time-of-Flight sensors, produce dense depth measurements, but are affected by noise and have a low lateral resolution. We propose a method that combines the benefits of recent advances in machine learning based single image super-resolution, i.e. deep convolutional networks, with a variational method to recover accurate high-resolution depth maps. In particular, we integrate a variational method that models the piecewise affine structures apparent in depth data via an anisotropic total generalized variation regularization term on top of a deep network. We call our method ATGV-Net and train it end-to-end by unrolling the optimization procedure of the variational method. To train deep networks, a large corpus of training data with accurate ground-truth is required. We demonstrate that it is feasible to train our method solely on synthetic data that we generate in large quantities for this task. Our evaluations show that we achieve state-of-the-art results on three different benchmarks, as well as on a challenging Time-of-Flight dataset, all without utilizing an additional intensity image as guidance. version:1
arxiv-1607-07987 | A Multiple Kernel Learning Approach for Human Behavioral Task Classification using STN-LFP Signal | http://arxiv.org/abs/1607.07987 | id:1607.07987 author:Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud, Mohammad H. Mahoor category:cs.CV q-bio.NC  published:2016-07-27 summary:Deep Brain Stimulation (DBS) has gained increasing attention as an effective method to mitigate Parkinsons disease (PD) disorders. Existing DBS systems are open-loop such that the system parameters are not adjusted automatically based on patients behavior. Classification of human behavior is an important step in the design of the next generation of DBS systems that are closed-loop. This paper presents a classification approach to recognize such behavioral tasks using the subthalamic nucleus (STN) Local Field Potential (LFP) signals. In our approach, we use the time-frequency representation (spectrogram) of the raw LFP signals recorded from left and right STNs as the feature vectors. Then these features are combined together via Support Vector Machines (SVM) with Multiple Kernel Learning (MKL) formulation. The MKL-based classification method is utilized to classify different tasks: button press, mouth movement, speech, and arm movement. Our experiments show that the lp-norm MKL significantly outperforms single kernel SVM-based classifiers in classifying behavioral tasks of five subjects even using signals acquired with a low sampling rate of 10 Hz. This leads to a lower computational cost. version:1
arxiv-1607-07983 | A Continuous Optimization Approach for Efficient and Accurate Scene Flow | http://arxiv.org/abs/1607.07983 | id:1607.07983 author:Zhaoyang Lv, Chris Beall, Pablo F. Alcantarilla, Fuxin Li, Zsolt Kira, Frank Dellaert category:cs.CV  published:2016-07-27 summary:We propose a continuous optimization method for solving dense 3D scene flow problems from stereo imagery. As in recent work, we represent the dynamic 3D scene as a collection of rigidly moving planar segments. The scene flow problem then becomes the joint estimation of pixel-to-segment assignment, 3D position, normal vector and rigid motion parameters for each segment, leading to a complex and expensive discrete-continuous optimization problem. In contrast, we propose a purely continuous formulation which can be solved more efficiently. Using a fine superpixel segmentation that is fixed a-priori, we propose a factor graph formulation that decomposes the problem into photometric, geometric, and smoothing constraints. We initialize the solution with a novel, high-quality initialization method, then independently refine the geometry and motion of the scene, and finally perform a global non-linear refinement using Levenberg-Marquardt. We evaluate our method in the challenging KITTI Scene Flow benchmark, ranking in third position, while being 3 to 30 times faster than the top competitors. version:1
arxiv-1607-07959 | Using Kernel Methods and Model Selection for Prediction of Preterm Birth | http://arxiv.org/abs/1607.07959 | id:1607.07959 author:Ilia Vovsha, Ansaf Salleb-Aouissi, Anita Raja, Thomas Koch, Alex Rybchuk, Axinia Radeva, Ashwath Rajan, Yiwen Huang, Hatim Diab, Ashish Tomar, Ronald Wapner category:cs.LG stat.ML  published:2016-07-27 summary:We describe an application of machine learning to the problem of predicting preterm birth. We conduct a secondary analysis on a clinical trial dataset collected by the National In- stitute of Child Health and Human Development (NICHD) while focusing our attention on predicting different classes of preterm birth. We compare three approaches for deriving predictive models: a support vector machine (SVM) approach with linear and non-linear kernels, logistic regression with different model selection along with a model based on decision rules prescribed by physician experts for prediction of preterm birth. Our approach highlights the pre-processing methods applied to handle the inherent dynamics, noise and gaps in the data and describe techniques used to handle skewed class distributions. Empirical experiments demonstrate significant improvement in predicting preterm birth compared to past work. version:1
arxiv-1607-07956 | Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification | http://arxiv.org/abs/1607.07956 | id:1607.07956 author:Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, Katia Sycara category:cs.CL cs.AI  published:2016-07-27 summary:Due to the lack of structured knowledge applied in learning distributed representation of cate- gories, existing work cannot incorporate category hierarchies into entity information. We propose a framework that embeds entities and categories into a semantic space by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. The framework allows to com- pute meaningful semantic relatedness between entities and categories. Our framework can han- dle both single-word concepts and multiple-word concepts with superior performance on concept categorization and yield state of the art results on dataless hierarchical classification. version:1
arxiv-1607-07395 | Seeing the Forest from the Trees in Two Looks: Matrix Sketching by Cascaded Bilateral Sampling | http://arxiv.org/abs/1607.07395 | id:1607.07395 author:Kai Zhang, Chuanren Liu, Jie Zhang, Hui Xiong, Eric Xing, Jieping Ye category:cs.LG  published:2016-07-25 summary:Matrix sketching is aimed at finding close approximations of a matrix by factors of much smaller dimensions, which has important applications in optimization and machine learning. Given a matrix A of size m by n, state-of-the-art randomized algorithms take O(m * n) time and space to obtain its low-rank decomposition. Although quite useful, the need to store or manipulate the entire matrix makes it a computational bottleneck for truly large and dense inputs. Can we sketch an m-by-n matrix in O(m + n) cost by accessing only a small fraction of its rows and columns, without knowing anything about the remaining data? In this paper, we propose the cascaded bilateral sampling (CABS) framework to solve this problem. We start from demonstrating how the approximation quality of bilateral matrix sketching depends on the encoding powers of sampling. In particular, the sampled rows and columns should correspond to the code-vectors in the ground truth decompositions. Motivated by this analysis, we propose to first generate a pilot-sketch using simple random sampling, and then pursue more advanced, "follow-up" sampling on the pilot-sketch factors seeking maximal encoding powers. In this cascading process, the rise of approximation quality is shown to be lower-bounded by the improvement of encoding powers in the follow-up sampling step, thus theoretically guarantees the algorithmic boosting property. Computationally, our framework only takes linear time and space, and at the same time its performance rivals the quality of state-of-the-art algorithms consuming a quadratic amount of resources. Empirical evaluations on benchmark data fully demonstrate the potential of our methods in large scale matrix sketching and related areas. version:3
arxiv-1607-07032 | Is Faster R-CNN Doing Well for Pedestrian Detection? | http://arxiv.org/abs/1607.07032 | id:1607.07032 author:Liliang Zhang, Liang Lin, Xiaodan Liang, Kaiming He category:cs.CV 68U01  published:2016-07-24 summary:Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN [1, 2] have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN [2] for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative examples. Driven by these observations, we propose a very simple but effective baseline for pedestrian detection, using an RPN followed by boosted forests on shared, high-resolution convolutional feature maps. We comprehensively evaluate this method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting competitive accuracy and good speed. Code will be made publicly available. version:2
arxiv-1607-07939 | A Sensorimotor Reinforcement Learning Framework for Physical Human-Robot Interaction | http://arxiv.org/abs/1607.07939 | id:1607.07939 author:Ali Ghadirzadeh, Judith Bütepage, Atsuto Maki, Danica Kragic, Mårten Björkman category:cs.RO cs.LG  published:2016-07-27 summary:Modeling of physical human-robot collaborations is generally a challenging problem due to the unpredictive nature of human behavior. To address this issue, we present a data-efficient reinforcement learning framework which enables a robot to learn how to collaborate with a human partner. The robot learns the task from its own sensorimotor experiences in an unsupervised manner. The uncertainty of the human actions is modeled using Gaussian processes (GP) to implement action-value functions. Optimal action selection given the uncertain GP model is ensured by Bayesian optimization. We apply the framework to a scenario in which a human and a PR2 robot jointly control the ball position on a plank based on vision and force/torque data. Our experimental results show the suitability of the proposed method in terms of fast and data-efficient model learning, optimal action selection under uncertainties and equal role sharing between the partners. version:1
arxiv-1607-07931 | Synthetic Language Generation and Model Validation in BEAST2 | http://arxiv.org/abs/1607.07931 | id:1607.07931 author:Stuart Bradley category:cs.CL  published:2016-07-27 summary:Generating synthetic languages aids in the testing and validation of future computational linguistic models and methods. This thesis extends the BEAST2 phylogenetic framework to add linguistic sequence generation under multiple models. The new plugin is then used to test the effects of the phenomena of word borrowing on the inference process under two widely used phylolinguistic models. version:1
arxiv-1607-06280 | Explaining Classification Models Built on High-Dimensional Sparse Data | http://arxiv.org/abs/1607.06280 | id:1607.06280 author:Julie Moeyersoms, Brian d'Alessandro, Foster Provost, David Martens category:stat.ML cs.LG  published:2016-07-21 summary:Predictive modeling applications increasingly use data representing people's behavior, opinions, and interactions. Fine-grained behavior data often has different structure from traditional data, being very high-dimensional and sparse. Models built from these data are quite difficult to interpret, since they contain many thousands or even many millions of features. Listing features with large model coefficients is not sufficient, because the model coefficients do not incorporate information on feature presence, which is key when analysing sparse data. In this paper we introduce two alternatives for explaining predictive models by listing important features. We evaluate these alternatives in terms of explanation "bang for the buck,", i.e., how many examples' inferences are explained for a given number of features listed. The bottom line: (i) The proposed alternatives have double the bang-for-the-buck as compared to just listing the high-coefficient features, and (ii) interestingly, although they come from different sources and motivations, the two new alternatives provide strikingly similar rankings of important features. version:2
arxiv-1607-07903 | Product Offerings in Malicious Hacker Markets | http://arxiv.org/abs/1607.07903 | id:1607.07903 author:Ericsson Marin, Ahmad Diab, Paulo Shakarian category:cs.CR cs.LG  published:2016-07-26 summary:Marketplaces specializing in malicious hacking products - including malware and exploits - have recently become more prominent on the darkweb and deepweb. We scrape 17 such sites and collect information about such products in a unified database schema. Using a combination of manual labeling and unsupervised clustering, we examine a corpus of products in order to understand their various categories and how they become specialized with respect to vendor and marketplace. This initial study presents how we effectively employed unsupervised techniques to this data as well as the types of insights we gained on various categories of malicious hacking products. version:1
arxiv-1607-07837 | Fast Global Convergence of Online PCA | http://arxiv.org/abs/1607.07837 | id:1607.07837 author:Zeyuan Allen-Zhu, Yuanzhi Li category:math.OC cs.DS cs.LG math.NA stat.ML  published:2016-07-26 summary:We study online principle component analysis (PCA), that is to find the top $k$ eigenvectors of a $d\times d$ hidden matrix $\bf \Sigma$ with online data samples drawn from covariance matrix $\bf \Sigma$. We provide $global$ convergence for the low-rank generalization of Oja's algorithm, which is popularly used in practice but lacks theoretical understanding. Our convergence rate matches the lower bound in terms of the dependency on error, on eigengap and on dimension $d$; in addition, our convergence rate can be made gap-free, that is proportional to the approximation error and independent of the eigengap. In contrast, for general rank $k$, before our work (1) it was open to design any algorithm with efficient global convergence rate; and (2) it was open to design any algorithm with (even local) gap-free convergence rate. version:1
arxiv-1607-06871 | Deep Appearance Models: A Deep Boltzmann Machine Approach for Face Modeling | http://arxiv.org/abs/1607.06871 | id:1607.06871 author:Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui category:cs.CV  published:2016-07-23 summary:The "interpretation through synthesis" approach to analyze face images, particularly Active Appearance Models (AAMs) method, has become one of the most successful face modeling approaches over the last two decades. AAM models have ability to represent face images through synthesis using a controllable parameterized Principal Component Analysis (PCA) model. However, the accuracy and robustness of the synthesized faces of AAM are highly depended on the training sets and inherently on the generalizability of PCA subspaces. This paper presents a novel Deep Appearance Models (DAMs) approach, an efficient replacement for AAMs, to accurately capture both shape and texture of face images under large variations. In this approach, three crucial components represented in hierarchical layers are modeled using the Deep Boltzmann Machines (DBM) to robustly capture the variations of facial shapes and appearances. DAMs are therefore superior to AAMs in inferencing a representation for new face images under various challenging conditions. The proposed approach is evaluated in various applications to demonstrate its robustness and capabilities, i.e. facial super-resolution reconstruction, facial off-angle reconstruction or face frontalization, facial occlusion removal and age estimation using challenging face databases, i.e. Labeled Face Parts in the Wild (LFPW), Helen and FG-NET. version:2
arxiv-1607-07086 | An Actor-Critic Algorithm for Sequence Prediction | http://arxiv.org/abs/1607.07086 | id:1607.07086 author:Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio category:cs.LG  published:2016-07-24 summary:We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \textit{critic} network that is trained to predict the value of an output token, given the policy of an \textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. version:2
arxiv-1607-07770 | Approximate Policy Iteration for Budgeted Semantic Video Segmentation | http://arxiv.org/abs/1607.07770 | id:1607.07770 author:Behrooz Mahasseni, Sinisa Todorovic, Alan Fern category:cs.CV  published:2016-07-26 summary:This paper formulates and presents a solution to the new problem of budgeted semantic video segmentation. Given a video, the goal is to accurately assign a semantic class label to every pixel in the video within a specified time budget. Typical approaches to such labeling problems, such as Conditional Random Fields (CRFs), focus on maximizing accuracy but do not provide a principled method for satisfying a time budget. For video data, the time required by CRF and related methods is often dominated by the time to compute low-level descriptors of supervoxels across the video. Our key contribution is the new budgeted inference framework for CRF models that intelligently selects the most useful subsets of descriptors to run on subsets of supervoxels within the time budget. The objective is to maintain an accuracy as close as possible to the CRF model with no time bound, while remaining within the time budget. Our second contribution is the algorithm for learning a policy for the sparse selection of supervoxels and their descriptors for budgeted CRF inference. This learning algorithm is derived by casting our problem in the framework of Markov Decision Processes, and then instantiating a state-of-the-art policy learning algorithm known as Classification-Based Approximate Policy Iteration. Our experiments on multiple video datasets show that our learning approach and framework is able to significantly reduce computation time, and maintain competitive accuracy under varying budgets. version:1
arxiv-1607-07745 | Leveraging Unstructured Data to Detect Emerging Reliability Issues | http://arxiv.org/abs/1607.07745 | id:1607.07745 author:Deovrat Kakde, Arin Chaudhuri category:cs.AI stat.AP stat.ME stat.ML  published:2016-07-26 summary:Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. Loosely speaking, unstructured data refers to text data that is generated by humans. In after-sales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study. version:1
arxiv-1607-07716 | Joint Optical Flow and Temporally Consistent Semantic Segmentation | http://arxiv.org/abs/1607.07716 | id:1607.07716 author:Junhwa Hur, Stefan Roth category:cs.CV  published:2016-07-26 summary:The importance and demands of visual scene understanding have been steadily increasing along with the active development of autonomous systems. Consequently, there has been a large amount of research dedicated to semantic segmentation and dense motion estimation. In this paper, we propose a method for jointly estimating optical flow and temporally consistent semantic segmentation, which closely connects these two problem domains and leverages each other. Semantic segmentation provides information on plausible physical motion to its associated pixels, and accurate pixel-level temporal correspondences enhance the accuracy of semantic segmentation in the temporal domain. We demonstrate the benefits of our approach on the KITTI benchmark, where we observe performance gains for flow and segmentation. We achieve state-of-the-art optical flow results, and outperform all published algorithms by a large margin on challenging, but crucial dynamic objects. version:1
arxiv-1607-07684 | The Price of Anarchy in Auctions | http://arxiv.org/abs/1607.07684 | id:1607.07684 author:Tim Roughgarden, Vasilis Syrgkanis, Eva Tardos category:cs.GT cs.AI cs.LG  published:2016-07-26 summary:This survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings. This theory complements traditional economic techniques, which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings. We highlight three user-friendly analytical tools: smoothness-type inequalities, which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies; extension theorems, which extend such guarantees to randomized strategies, no-regret learning outcomes, and incomplete-information settings; and composition theorems, which extend such guarantees from simpler to more complex auctions. Combining these tools yields tight worst-case approximation guarantees for the equilibria of many widely-used auction formats. version:1
arxiv-1607-07680 | End-to-End Image Super-Resolution via Deep and Shallow Convolutional Networks | http://arxiv.org/abs/1607.07680 | id:1607.07680 author:Yifan Wang, Lijun Wang, Hongyu Wang, Peihua Li category:cs.CV  published:2016-07-26 summary:One impressive advantage of convolutional neural networks (CNNs) is their ability to automatically learn feature representation from raw pixels, eliminating the need for hand-designed procedures. However, recent methods for single image super-resolution (SR) fail to maintain this advantage. They utilize CNNs in two decoupled steps, i.e., first upsampling the low resolution (LR) image to the high resolution (HR) size with hand-designed techniques (e.g., bicubic interpolation), and then applying CNNs on the upsampled LR image to reconstruct HR results. In this paper, we seek an alternative and propose a new image SR method, which jointly learns the feature extraction, upsampling and HR reconstruction modules, yielding a completely end-to-end trainable deep CNN. As opposed to existing approaches, the proposed method conducts upsampling in the latent feature space with filters that are optimized for the task of image SR. In addition, the HR reconstruction is performed in a multi-scale manner to simultaneously incorporate both short- and long-range contextual information, ensuring more accurate restoration of HR images. To facilitate network training, a new training approach is designed, which jointly trains the proposed deep network with a relatively shallow network, leading to faster convergence and more superior performance. The proposed method is extensively evaluated on widely adopted data sets and improves the performance of state-of-the-art methods with a considerable margin. Moreover, in-depth ablation studies are conducted to verify the contribution of different network designs to image SR, providing additional insights for future research. version:1
arxiv-1607-07671 | Region-based semantic segmentation with end-to-end training | http://arxiv.org/abs/1607.07671 | id:1607.07671 author:Holger Caesar, Jasper Uijlings, Vittorio Ferrari category:cs.CV  published:2016-07-26 summary:We propose a novel method for semantic segmentation, the task of labeling each pixel in an image with a semantic class. Our method combines the advantages of the two main competing paradigms. Methods based on region classification offer proper spatial support for appearance measurements, but typically operate in two separate stages, none of which targets pixel labeling performance at the end of the pipeline. More recent fully convolutional methods are capable of end-to-end training for the final pixel labeling, but resort to fixed patches as spatial support. We show how to modify modern region-based approaches to enable end-to-end training for semantic segmentation. This is achieved via a differentiable region-to-pixel layer and a differentiable free-form Region-of-Interest pooling layer. Our method improves the state-of-the-art in terms of class-average accuracy with 64.0% on SIFT Flow and 49.9% on PASCAL Context, and is particularly accurate at object boundaries. version:1
arxiv-1607-07660 | Fundamental Matrices from Moving Objects Using Line Motion Barcodes | http://arxiv.org/abs/1607.07660 | id:1607.07660 author:Yoni Kasten, Gil Ben-Artzi, Shmuel Peleg, Michael Werman category:cs.CV  published:2016-07-26 summary:Computing the epipolar geometry between cameras with very different viewpoints is often very difficult. The appearance of objects can vary greatly, and it is difficult to find corresponding feature points. Prior methods searched for corresponding epipolar lines using points on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the "Motion Barcodes", a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction. version:1
arxiv-1607-07249 | An Evolutionary Algorithm to Learn SPARQL Queries for Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia | http://arxiv.org/abs/1607.07249 | id:1607.07249 author:Jörn Hees, Rouven Bauer, Joachim Folz, Damian Borth, Andreas Dengel category:cs.AI cs.DB cs.NE stat.ML  published:2016-07-25 summary:Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query. In this work we present an evolutionary algorithm to help with this challenging task. Given a training list of source-target node-pair examples our algorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The learned patterns can be visualised to form the basis for further investigation, or they can be used to predict target nodes for new source nodes. Amongst others, we apply our algorithm to a dataset of several hundred human associations (such as "circle - square") to find patterns for them in DBpedia. We show the scalability of the algorithm by running it against a SPARQL endpoint loaded with > 7.9 billion triples. Further, we use the resulting SPARQL queries to mimic human associations with a Mean Average Precision (MAP) of 39.9 % and a Recall@10 of 63.9 %. version:2
arxiv-1607-07657 | Machine Learned Resume-Job Matching Solution | http://arxiv.org/abs/1607.07657 | id:1607.07657 author:Yiou Lin, Hang Lei, Prince Clement Addo, Xiaoyu Li category:cs.CL  published:2016-07-26 summary:Job search through online matching engines nowadays are very prominent and beneficial to both job seekers and employers. But the solutions of traditional engines without understanding the semantic meanings of different resumes have not kept pace with the incredible changes in machine learning techniques and computing capability. These solutions are usually driven by manual rules and predefined weights of keywords which lead to an inefficient and frustrating search experience. To this end, we present a machine learned solution with rich features and deep learning methods. Our solution includes three configurable modules that can be plugged with little restrictions. Namely, unsupervised feature extraction, base classifiers training and ensemble method learning. In our solution, rather than using manual rules, machine learned methods to automatically detect the semantic similarity of positions are proposed. Then four competitive "shallow" estimators and "deep" estimators are selected. Finally, ensemble methods to bag these estimators and aggregate their individual predictions to form a final prediction are verified. Experimental results of over 47 thousand resumes show that our solution can significantly improve the predication precision current position, salary, educational background and company scale. version:1
arxiv-1607-07646 | Emotion-Based Crowd Representation for Abnormality Detection | http://arxiv.org/abs/1607.07646 | id:1607.07646 author:Hamidreza Rabiee, Javad Haddadnia, Hossein Mousavi, Moin Nabi, Vittorio Murino, Nicu Sebe category:cs.CV  published:2016-07-26 summary:In crowd behavior understanding, a model of crowd behavior need to be trained using the information extracted from video sequences. Since there is no ground-truth available in crowd datasets except the crowd behavior labels, most of the methods proposed so far are just based on low-level visual features. However, there is a huge semantic gap between low-level motion/appearance features and high-level concept of crowd behaviors. In this paper we propose an attribute-based strategy to alleviate this problem. While similar strategies have been recently adopted for object and action recognition, as far as we know, we are the first showing that the crowd emotions can be used as attributes for crowd behavior understanding. The main idea is to train a set of emotion-based classifiers, which can subsequently be used to represent the crowd motion. For this purpose, we collect a big dataset of video clips and provide them with both annotations of "crowd behaviors" and "crowd emotions". We show the results of the proposed method on our dataset, which demonstrate that the crowd emotions enable the construction of more descriptive models for crowd behaviors. We aim at publishing the dataset with the article, to be used as a benchmark for the communities. version:1
arxiv-1607-07639 | Scale Invariant Interest Points with Shearlets | http://arxiv.org/abs/1607.07639 | id:1607.07639 author:Miguel A. Duval-Poo, Nicoletta Noceti, Francesca Odone, Ernesto De Vito category:cs.CV  published:2016-07-26 summary:Shearlets are a relatively new directional multi-scale framework for signal analysis, which have been shown effective to enhance signal discontinuities such as edges and corners at multiple scales. In this work we address the problem of detecting and describing blob-like features in the shearlets framework. We derive a measure which is very effective for blob detection and closely related to the Laplacian of Gaussian. We demonstrate the measure satisfies the perfect scale invariance property in the continuous case. In the discrete setting, we derive algorithms for blob detection and keypoint description. Finally, we provide qualitative justifications of our findings as well as a quantitative evaluation on benchmark data. We also report an experimental evidence that our method is very suitable to deal with compressed and noisy images, thanks to the sparsity property of shearlets. version:1
arxiv-1607-07630 | Grounded Lexicon Acquisition - Case Studies in Spatial Language | http://arxiv.org/abs/1607.07630 | id:1607.07630 author:Michael Spranger category:cs.CL  published:2016-07-26 summary:This paper discusses grounded acquisition experiments of increasing complexity. Humanoid robots acquire English spatial lexicons from robot tutors. We identify how various spatial language systems, such as projective, absolute and proximal can be learned. The proposed learning mechanisms do not rely on direct meaning transfer or direct access to world models of interlocutors. Finally, we show how multiple systems can be acquired at the same time. version:1
arxiv-1607-07614 | Semantic Clustering for Robust Fine-Grained Scene Recognition | http://arxiv.org/abs/1607.07614 | id:1607.07614 author:Marian George, Mandar Dixit, Gábor Zogg, Nuno Vasconcelos category:cs.CV  published:2016-07-26 summary:In domain generalization, the knowledge learnt from one or multiple source domains is transferred to an unseen target domain. In this work, we propose a novel domain generalization approach for fine-grained scene recognition. We first propose a semantic scene descriptor that jointly captures the subtle differences between fine-grained scenes, while being robust to varying object configurations across domains. We model the occurrence patterns of objects in scenes, capturing the informativeness and discriminability of each object for each scene. We then transform such occurrences into scene probabilities for each scene image. Second, we argue that scene images belong to hidden semantic topics that can be discovered by clustering our semantic descriptors. To evaluate the proposed method, we propose a new fine-grained scene dataset in cross-domain settings. Extensive experiments on the proposed dataset and three benchmark scene datasets show the effectiveness of the proposed approach for fine-grained scene transfer, where we outperform state-of-the-art scene recognition and domain generalization methods. version:1
arxiv-1607-07611 | Learning Null Space Projections in Operational Space Formulation | http://arxiv.org/abs/1607.07611 | id:1607.07611 author:Hsiu-Chin Lin, Matthew Howard category:cs.LG cs.RO  published:2016-07-26 summary:In recent years, a number of tools have become available that recover the underlying control policy from constrained movements. However, few have explicitly considered learning the constraints of the motion and ways to cope with unknown environment. In this paper, we consider learning the null space projection matrix of a kinematically constrained system in the absence of any prior knowledge either on the underlying policy, the geometry, or dimensionality of the constraints. Our evaluations have demonstrated the effectiveness of the proposed approach on problems of differing dimensionality, and with different degrees of non-linearity. version:1
arxiv-1607-07607 | An Adaptive Matrix Factorization Approach for Personalized Recommender Systems | http://arxiv.org/abs/1607.07607 | id:1607.07607 author:Gianna M. Del Corso, Francesco Romani category:cs.LG math.NA stat.ML 65F99  published:2016-07-26 summary:Given a set $U$ of users and a set of items $I$, a dataset of recommendations can be viewed as a sparse rectangular matrix $A$ of size $ U \times I $ such that $a_{u,i}$ contains the rating the user $u$ assigns to item $i$, $a_{u,i}=?$ if the user $u$ has not rated the item $i$. The goal of a recommender system is to predict replacements to the missing observations $?$ in $A$ in order to make personalized recommendations meeting the user's tastes. A promising approach is the one based on the low-rank nonnegative matrix factorization of $A$ where items and users are represented in terms of a few vectors. These vector can be used for estimating the missing evaluations and to produce new recommendations. In this paper we propose an algorithm based on the nonnegative matrix Factorization approach for predicting the missing entries. Numerical test have been performed to estimate the accuracy in predicting the missing entries and in the recommendations provided and we have compared our technique with others in the literature. We have tested the algorithm on the MovieLens databases containing ratings of users on movies. version:1
arxiv-1607-07604 | Generic Feature Learning for Wireless Capsule Endoscopy Analysis | http://arxiv.org/abs/1607.07604 | id:1607.07604 author:Santi Seguí, Michal Drozdzal, Guillem Pascual, Petia Radeva, Carolina Malagelada, Fernando Azpiroz, Jordi Vitrià category:cs.CV  published:2016-07-26 summary:The interpretation and analysis of the wireless capsule endoscopy recording is a complex task which requires sophisticated computer aided decision (CAD) systems in order to help physicians with the video screening and, finally, with the diagnosis. Most of the CAD systems in the capsule endoscopy share a common system design, but use very different image and video representations. As a result, each time a new clinical application of WCE appears, new CAD system has to be designed from scratch. This characteristic makes the design of new CAD systems a very time consuming. Therefore, in this paper we introduce a system for small intestine motility characterization, based on Deep Convolutional Neural Networks, which avoids the laborious step of designing specific features for individual motility events. Experimental results show the superiority of the learned features over alternative classifiers constructed by using state of the art hand-crafted features. In particular, it reaches a mean classification accuracy of 96% for six intestinal motility events, outperforming the other classifiers by a large margin (a 14% relative performance increase). version:1
arxiv-1607-07602 | OntoCat: Automatically categorizing knowledge in API Documentation | http://arxiv.org/abs/1607.07602 | id:1607.07602 author:Niraj Kumar, Premkumar Devanbu category:cs.SE cs.AI cs.CL  published:2016-07-26 summary:Most application development happens in the context of complex APIs; reference documentation for APIs has grown tremendously in variety, complexity, and volume, and can be difficult to navigate. There is a growing need to develop well-organized ways to access the knowledge latent in the documentation; several research efforts deal with the organization (ontology) of API-related knowledge. Extensive knowledge-engineering work, supported by a rigorous qualitative analysis, by Maalej & Robillard [3] has identified a useful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain independent technique to extract the knowledge types from the given API reference documentation. Our system, OntoCat, introduces total nine different features and their semantic and statistical combinations to classify the different knowledge types. We tested OntoCat on python API reference documentation. Our experimental results show the effectiveness of the system and opens the scope of probably related research areas (i.e., user behavior, documentation quality, etc.). version:1
arxiv-1607-07590 | Simultaneous estimation of noise variance and number of peaks in Bayesian spectral deconvolution | http://arxiv.org/abs/1607.07590 | id:1607.07590 author:Satoru Tokuda, Kenji Nagata, Masato Okada category:physics.data-an cs.LG stat.ML  published:2016-07-26 summary:Heuristic identification of peaks from noisy complex spectra often leads to misunderstanding physical and chemical properties of matter. In this paper, we propose a framework based on Bayesian inference, which enables us to separate multi-peak spectra into single peaks statistically and is constructed in two steps. The first step is estimating both noise variance and number of peaks as hyperparameters based on Bayes free energy, which generally is not analytically tractable. The second step is fitting the parameters of each peak function to the given spectrum by calculating the posterior density, which has a problem of local minima and saddles since multi-peak models are nonlinear and hierarchical. Our framework enables escaping from local minima or saddles by using the exchange Monte Carlo method and calculates Bayes free energy. We discuss a simulation demonstrating how efficient our framework is and show that estimating both noise variance and number of peaks prevents overfitting, overpenalizing, and misunderstanding the precision of parameter estimation. version:1
arxiv-1607-07573 | Variational Mixture Models with Gamma or inverse-Gamma components | http://arxiv.org/abs/1607.07573 | id:1607.07573 author:A. Llera, D. Vidaurre, R. H. R. Pruim, C. F. Beckmann category:stat.ML  published:2016-07-26 summary:Mixture models with Gamma and or inverse-Gamma distributed mixture components are useful for medical image tissue segmentation or as post-hoc models for regression coefficients obtained from linear regression within a Generalised Linear Modeling framework (GLM), used in this case to separate stochastic (Gaussian) noise from some kind of positive or negative "activation" (modeled as Gamma or inverse-Gamma distributed). To date, the most common choice in this context it is Gaussian/Gamma mixture models learned through a maximum likelihood (ML) approach; we recently extended such algorithm for mixture models with inverse-Gamma components. Here, we introduce a fully analytical Variational Bayes (VB) learning framework for both Gamma and/or inverse-Gamma components. We use synthetic and resting state fMRI data to compare the performance of the ML and VB algorithms in terms of area under the curve and computational cost. We observed that the ML Gaussian/Gamma model is very expensive specially when considering high resolution images; furthermore, these solutions are highly variable and they occasionally can overestimate the activations severely. The Bayesian Gauss-Gamma is in general the fastest algorithm but provides too dense solutions. The maximum likelihood Gaussian/inverse-Gamma is also very fast but provides in general very sparse solutions. The variational Gaussian/inverse-Gamma mixture model is the most robust and its cost is acceptable even for high resolution images. Further, the presented methodology represents an essential building block that can be directly used in more complex inference tasks, specially designed to analyse MRI-fMRI data; such models include for example analytical variational mixture models with adaptive spatial regularization or better source models for new spatial blind source separation approaches. version:1
arxiv-1607-07565 | Grounding Dynamic Spatial Relations for Embodied (Robot) Interaction | http://arxiv.org/abs/1607.07565 | id:1607.07565 author:Michael Spranger, Jakob Suchan, Mehul Bhatt, Manfred Eppe category:cs.CL  published:2016-07-26 summary:This paper presents a computational model of the processing of dynamic spatial relations occurring in an embodied robotic interaction setup. A complete system is introduced that allows autonomous robots to produce and interpret dynamic spatial phrases (in English) given an environment of moving objects. The model unites two separate research strands: computational cognitive semantics and on commonsense spatial representation and reasoning. The model for the first time demonstrates an integration of these different strands. version:1
arxiv-1607-07215 | DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation | http://arxiv.org/abs/1607.07215 | id:1607.07215 author:Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, Victor Lempitsky category:cs.CV  published:2016-07-25 summary:In this work, we consider the task of generating highly-realistic images of a given face with a redirected gaze. We treat this problem as a specific instance of conditional image generation and suggest a new deep architecture that can handle this task very well as revealed by numerical comparison with prior art and a user study. Our deep architecture performs coarse-to-fine warping with an additional intensity correction of individual pixels. All these operations are performed in a feed-forward manner, and the parameters associated with different operations are learned jointly in the end-to-end fashion. After learning, the resulting neural network can synthesize images with manipulated gaze, while the redirection angle can be selected arbitrarily from a certain range and provided as an input to the network. version:2
arxiv-1607-07561 | Generic 3D Convolutional Fusion for image restoration | http://arxiv.org/abs/1607.07561 | id:1607.07561 author:Jiqing Wu, Radu Timofte, Luc Van Gool category:cs.CV  published:2016-07-26 summary:Also recently, exciting strides forward have been made in the area of image restoration, particularly for image denoising and single image super-resolution. Deep learning techniques contributed to this significantly. The top methods differ in their formulations and assumptions, so even if their average performance may be similar, some work better on certain image types and image regions than others. This complementarity motivated us to propose a novel 3D convolutional fusion (3DCF) method. Unlike other methods adapted to different tasks, our method uses the exact same convolutional network architecture to address both image denois- ing and single image super-resolution. As a result, our 3DCF method achieves substantial improvements (0.1dB-0.4dB PSNR) over the state-of-the-art methods that it fuses, and this on standard benchmarks for both tasks. At the same time, the method still is computationally efficient. version:1
arxiv-1607-07539 | Semantic Image Inpainting with Perceptual and Contextual Losses | http://arxiv.org/abs/1607.07539 | id:1607.07539 author:Raymond Yeh, Chen Chen, Teck Yian Lim, Mark Hasegawa-Johnson, Minh N. Do category:cs.CV  published:2016-07-26 summary:In this paper, we propose a novel method for image inpainting based on a Deep Convolutional Generative Adversarial Network (DCGAN). We define a loss function consisting of two parts: (1) a contextual loss that preserves similarity between the input corrupted image and the recovered image, and (2) a perceptual loss that ensures a perceptually realistic output image. Given a corrupted image with missing values, we use back-propagation on this loss to map the corrupted image to a smaller latent space. The mapped vector is then passed through the generative model to predict the missing content. The proposed framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting tasks with random 80% corruption and large blocky corruption. Experiments show that our method can successfully predict semantic information in the missing region and achieve pixel-level photorealism, which is impossible by almost all existing methods. version:1
arxiv-1607-07526 | On the Consistency of Exact and Approximate Nearest Neighbor with Noisy Data | http://arxiv.org/abs/1607.07526 | id:1607.07526 author:Wei Gao, Xin-Yi Niu, Zhi-Hua Zhou category:cs.LG  published:2016-07-26 summary:Nearest neighbor has been one of the simplest and most appealing non-parametric approaches in machine learning, pattern recognition, computer vision, etc. Empirical studies show the resistance of k-nearest neighbor to noise, yet the theoretical understanding is not clear. This work presents the consistency analysis on exact and approximate nearest neighbor in the random noise setting. Our theoretical studies show that k-nearest neighbor, in the noise setting, gets the same consistent rate as that in the noise-free setting, which verifies the robustness of k-nearest neighbor to random noise. The nearest neighbor (1-NN), however, is proven to be biased by random noise. For approximate $k$-nearest neighbor, we first generalize the Johnson-Lindenstrauss lemma to infinite set, and based on this result, we show that the approximate $k$-nearest neighbor is also robust to random noise as that of the exact k-nearest neighbor, and achieves faster convergence rate yet with a tradeoff between consistency and reduced dimension. Specifically, approximate $k$-nearest neighbor with sharp dimensional reduction tends to cause large deviation from the Bayes risk. version:1
arxiv-1607-07525 | Salient Object Subitizing | http://arxiv.org/abs/1607.07525 | id:1607.07525 author:Jianming Zhang, Shugao Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit Betke, Zhe Lin, Xiaohui Shen, Brian Price, Radomir Mech category:cs.CV  published:2016-07-26 summary:We study the problem of Salient Object Subitizing, i.e. predicting the existence and the number of salient objects in an image using holistic cues. This task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range (1-4). To this end, we present a salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. We show that using an end-to-end trained Convolutional Neural Network (CNN) model, we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. For images with multiple salient objects, our model also provides significantly better than chance performance without requiring any localization process. Moreover, we propose a method to improve the training of the CNN subitizing model by leveraging synthetic images. In experiments, we demonstrate the accuracy and generalizability of our CNN subitizing model and its applications in salient object detection and image retrieval. version:1
arxiv-1607-07519 | Deepr: A Convolutional Net for Medical Records | http://arxiv.org/abs/1607.07519 | id:1607.07519 author:Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, Svetha Venkatesh category:stat.ML cs.LG  published:2016-07-26 summary:Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space. version:1
arxiv-1607-07514 | Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder | http://arxiv.org/abs/1607.07514 | id:1607.07514 author:Soroush Vosoughi, Prashanth Vijayaraghavan, Deb Roy category:cs.CL cs.AI cs.NE cs.SI  published:2016-07-26 summary:We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages. version:1
arxiv-1607-06534 | The Landscape of Empirical Risk for Non-convex Losses | http://arxiv.org/abs/1607.06534 | id:1607.06534 author:Song Mei, Yu Bai, Andrea Montanari category:stat.ML  published:2016-07-22 summary:We revisit the problem of learning a noisy linear classifier by minimizing the empirical risk associated to the square loss. While the empirical risk is non-convex, we prove that its structure is remarkably simple. Namely, when the sample size is larger than $C \, d\log d$ (with $d$ the dimension, and $C$ a constant) the following happen with high probability: $(a)$ The empirical risk has a unique local minimum (which is also the global minimum); $(b)$ Gradient descent converges exponentially fast to the global minimizer, from any initialization; $(c)$ The global minimizer approaches the true parameter at nearly optimal rate. The core of our argument is to establish a uniform convergence result for the gradients and Hessians of the empirical risk. version:2
arxiv-1607-07387 | Symmetry-free SDP Relaxations for Affine Subspace Clustering | http://arxiv.org/abs/1607.07387 | id:1607.07387 author:Francesco Silvestri, Gerhard Reinelt, Christoph Schnörr category:math.OC cs.CV cs.DM stat.ML 62H30 and 90C22  published:2016-07-25 summary:We consider clustering problems where the goal is to determine an optimal partition of a given point set in Euclidean space in terms of a collection of affine subspaces. While there is vast literature on heuristics for this kind of problem, such approaches are known to be susceptible to poor initializations and getting trapped in bad local optima. We alleviate these issues by introducing a semidefinite relaxation based on Lasserre's method of moments. While a similiar approach is known for classical Euclidean clustering problems, a generalization to our more general subspace scenario is not straightforward, due to the high symmetry of the objective function that weakens any convex relaxation. We therefore introduce a new mechanism for symmetry breaking based on covering the feasible region with polytopes. Additionally, we introduce and analyze a deterministic rounding heuristic. version:1
arxiv-1607-07384 | Identifying Depression on Twitter | http://arxiv.org/abs/1607.07384 | id:1607.07384 author:Moin Nadeem category:cs.SI stat.ML  published:2016-07-25 summary:Social media has recently emerged as a premier method to disseminate information online. Through these online networks, tens of millions of individuals communicate their thoughts, personal experiences, and social ideals. We therefore explore the potential of social media to predict, even prior to onset, Major Depressive Disorder (MDD) in online personas. We employ a crowdsourced method to compile a list of Twitter users who profess to being diagnosed with depression. Using up to a year of prior social media postings, we utilize a Bag of Words approach to quantify each tweet. Lastly, we leverage several statistical classifiers to provide estimates to the risk of depression. Our work posits a new methodology for constructing our classifier by treating social as a text-classification problem, rather than a behavioral one on social media platforms. By using a corpus of 2.5M tweets, we achieved an 81% accuracy rate in classification, with a precision score of .86. We believe that this method may be helpful in developing tools that estimate the risk of an individual being depressed, can be employed by physicians, concerned individuals, and healthcare agencies to aid in diagnosis, even possibly enabling those suffering from depression to be more proactive about recovering from their mental health. version:1
arxiv-1607-06221 | A Perspective on Sentiment Analysis | http://arxiv.org/abs/1607.06221 | id:1607.06221 author:K Paramesha, K C Ravishankar category:cs.CL  published:2016-07-21 summary:Sentiment Analysis (SA) is indeed a fascinating area of research which has stolen the attention of researchers as it has many facets and more importantly it promises economic stakes in the corporate and governance sector. SA has been stemmed out of text analytics and established itself as a separate identity and a domain of research. The wide ranging results of SA have proved to influence the way some critical decisions are taken. Hence, it has become relevant in thorough understanding of the different dimensions of the input, output and the processes and approaches of SA. version:2
arxiv-1607-07330 | Evaluating Link Prediction Accuracy on Dynamic Networks with Added and Removed Edges | http://arxiv.org/abs/1607.07330 | id:1607.07330 author:Ruthwik R. Junuthula, Kevin S. Xu, Vijay K. Devabhaktuni category:cs.SI cs.LG physics.soc-ph stat.ME  published:2016-07-25 summary:The task of predicting future relationships in a social network, known as link prediction, has been studied extensively in the literature. Many link prediction methods have been proposed, ranging from common neighbors to probabilistic models. Recent work by Yang et al. has highlighted several challenges in evaluating link prediction accuracy. In dynamic networks where edges are both added and removed over time, the link prediction problem is more complex and involves predicting both newly added and newly removed edges. This results in new challenges in the evaluation of dynamic link prediction methods, and the recommendations provided by Yang et al. are no longer applicable, because they do not address edge removal. In this paper, we investigate several metrics currently used for evaluating accuracies of dynamic link prediction methods and demonstrate why they can be misleading in many cases. We provide several recommendations on evaluating dynamic link prediction accuracy, including separation into two categories of evaluation. Finally we propose a unified metric to characterize link prediction accuracy effectively using a single number. version:1
arxiv-1607-07329 | Accelerating Stochastic Composition Optimization | http://arxiv.org/abs/1607.07329 | id:1607.07329 author:Mengdi Wang, Ji Liu, Ethan X. Fang category:math.OC stat.ML  published:2016-07-25 summary:Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments. version:1
arxiv-1607-07304 | Tracking with multi-level features | http://arxiv.org/abs/1607.07304 | id:1607.07304 author:Roberto Henschel, Laura Leal-Taixé, Bodo Rosenhahn, Konrad Schindler category:cs.CV  published:2016-07-25 summary:We present a novel formulation of the multiple object tracking problem which integrates low and mid-level features. In particular, we formulate the tracking problem as a quadratic program coupling detections and dense point trajectories. Due to the computational complexity of the initial QP, we propose an approximation by two auxiliary problems, a temporal and spatial association, where the temporal subproblem can be efficiently solved by a linear program and the spatial association by a clustering algorithm. The objective function of the QP is used in order to find the optimal number of clusters, where each cluster ideally represents one person. Evaluation is provided for multiple scenarios, showing the superiority of our method with respect to classic tracking-by-detection methods and also other methods that greedily integrate low-level features. version:1
arxiv-1607-07295 | Learning Aligned Cross-Modal Representations from Weakly Aligned Data | http://arxiv.org/abs/1607.07295 | id:1607.07295 author:Lluis Castrejon, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba category:cs.CV  published:2016-07-25 summary:People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize cross-modal scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality. version:1
arxiv-1607-07270 | A Statistical Test for Joint Distributions Equivalence | http://arxiv.org/abs/1607.07270 | id:1607.07270 author:Francesco Solera, Andrea Palazzi category:cs.LG cs.CV stat.ML  published:2016-07-25 summary:We provide a distribution-free test that can be used to determine whether any two joint distributions $p$ and $q$ are statistically different by inspection of a large enough set of samples. Following recent efforts from Long et al. [1], we rely on joint kernel distribution embedding to extend the kernel two-sample test of Gretton et al. [2] to the case of joint probability distributions. Our main result can be directly applied to verify if a dataset-shift has occurred between training and test distributions in a learning framework, without further assuming the shift has occurred only in the input, in the target or in the conditional distribution. version:1
arxiv-1607-07262 | Automatic Attribute Discovery with Neural Activations | http://arxiv.org/abs/1607.07262 | id:1607.07262 author:Sirion Vittayakorn, Takayuki Umeda, Kazuhiko Murasaki, Kyoko Sudo, Takayuki Okatani, Kota Yamaguchi category:cs.CV  published:2016-07-25 summary:How can a machine learn to recognize visual attributes emerging out of online community without a definitive supervised dataset? This paper proposes an automatic approach to discover and analyze visual attributes from a noisy collection of image-text data on the Web. Our approach is based on the relationship between attributes and neural activations in the deep network. We characterize the visual property of the attribute word as a divergence within weakly-annotated set of images. We show that the neural activations are useful for discovering and learning a classifier that well agrees with human perception from the noisy real-world Web data. The empirical study suggests the layered structure of the deep neural networks also gives us insights into the perceptual depth of the given word. Finally, we demonstrate that we can utilize highly-activating neurons for finding semantically relevant regions. version:1
arxiv-1607-07220 | Local- and Holistic- Structure Preserving Image Super Resolution via Deep Joint Component Learning | http://arxiv.org/abs/1607.07220 | id:1607.07220 author:Yukai Shi, Keze Wang, Li Xu, Liang Lin category:cs.CV  published:2016-07-25 summary:Recently, machine learning based single image super resolution (SR) approaches focus on jointly learning representations for high-resolution (HR) and low-resolution (LR) image patch pairs to improve the quality of the super-resolved images. However, due to treat all image pixels equally without considering the salient structures, these approaches usually fail to produce visual pleasant images with sharp edges and fine details. To address this issue, in this work we present a new novel SR approach, which replaces the main building blocks of the classical interpolation pipeline by a flexible, content-adaptive deep neural networks. In particular, two well-designed structure-aware components, respectively capturing local- and holistic- image contents, are naturally incorporated into the fully-convolutional representation learning to enhance the image sharpness and naturalness. Extensively evaluations on several standard benchmarks (e.g., Set5, Set14 and BSD200) demonstrate that our approach can achieve superior results, especially on the image with salient structures, over many existing state-of-the-art SR methods under both quantitative and qualitative measures. version:1
arxiv-1607-07216 | Temporal Model Adaptation for Person Re-Identification | http://arxiv.org/abs/1607.07216 | id:1607.07216 author:Niki Martinel, Abir Das, Christian Micheloni, Amit K. Roy-Chowdhury category:cs.CV  published:2016-07-25 summary:Person re-identification is an open and challenging problem in computer vision. Majority of the efforts have been spent either to design the best feature representation or to learn the optimal matching metric. Most approaches have neglected the problem of adapting the selected features or the learned model over time. To address such a problem, we propose a temporal model adaptation scheme with human in the loop. We first introduce a similarity-dissimilarity learning method which can be trained in an incremental fashion by means of a stochastic alternating directions methods of multipliers optimization procedure. Then, to achieve temporal adaptation with limited human effort, we exploit a graph-based approach to present the user only the most informative probe-gallery matches that should be used to update the model. Results on three datasets have shown that our approach performs on par or even better than state-of-the-art approaches while reducing the manual pairwise labeling effort by about 80%. version:1
arxiv-1607-07195 | Higher-Order Factorization Machines | http://arxiv.org/abs/1607.07195 | id:1607.07195 author:Mathieu Blondel, Akinori Fujino, Naonori Ueda, Masakazu Ishihata category:stat.ML cs.LG  published:2016-07-25 summary:Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks. version:1
arxiv-1607-07186 | A Novel Mutual Information-based Feature Selection Algorithm | http://arxiv.org/abs/1607.07186 | id:1607.07186 author:Pietro Cassara, Alessandro Rozza category:cs.LG  published:2016-07-25 summary:From a machine learning point of view to identify a subset of relevant features from a real data set can be useful to improve the results achieved by classification methods and to reduce their time and space complexity. To achieve this goal, feature selection methods are usually employed. These approaches assume that the data contains redundant or irrelevant attributes that can be eliminated. In this work we propose a novel algorithm to manage the optimization problem that is the foundation of the Mutual Information feature selection methods thus to formalize a novel approach that is also able to automatically estimates the number of dimensions to retain. The main advantages of this new approach are: the ability to automatically estimate the number of features to retain, and the possibility to rank the features to select from the most probable to the less probable. Experiments on standard real data sets and the comparison with state-of-the-art feature selection techniques confirms the high quality of our approach. version:1
arxiv-1607-07160 | Large-Scale Video Search with Efficient Temporal Voting Structure | http://arxiv.org/abs/1607.07160 | id:1607.07160 author:Ersin Esen, Savas Ozkan, Ilkay Atil category:cs.CV  published:2016-07-25 summary:In this work, we propose a fast content-based video querying system for large-scale video search. The proposed system is distinguished from similar works with two major contributions. First contribution is superiority of joint usage of repeated content representation and efficient hashing mechanisms. Repeated content representation is utilized with a simple yet robust feature, which is based on edge energy of frames. Each of the representation is converted into hash code with Hamming Embedding method for further queries. Second contribution is novel queue-based voting scheme that leads to modest memory requirements with gradual memory allocation capability, contrary to complete brute-force temporal voting schemes. This aspect enables us to make queries on large video databases conveniently, even on commodity computers with limited memory capacity. Our results show that the system can respond to video queries on a large video database with fast query times, high recall rate and very low memory and disk requirements. version:1
arxiv-1607-07155 | A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection | http://arxiv.org/abs/1607.07155 | id:1607.07155 author:Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos category:cs.CV  published:2016-07-25 summary:A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects. version:1
arxiv-1607-07129 | Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure Estimation from Single and Multiple Images | http://arxiv.org/abs/1607.07129 | id:1607.07129 author:Yuan Gao, Alan L. Yuille category:cs.CV cs.CG  published:2016-07-25 summary:Many man-made objects have intrinsic symmetries and Manhattan structure. By assuming an orthographic projection model, this paper addresses the estimation of 3D structures and camera projection using symmetry and/or Manhattan structure cues, for the two cases when the input is a single image or multiple images from the same category, e.g. multiple different cars. Specifically, analysis on single image case implies that Manhattan alone is sufficient to recover the camera projection, then the 3D structure can be reconstructed uniquely exploiting symmetry. But Manhattan structure can be hard to observe from single image due to occlusion. Hence, we extend to the multiple image case which can also exploit symmetry but does not require Manhattan axes. We propose a new rigid structure from motion method, exploiting symmetry, using multiple images from the same category as input. Our results on Pascal3D+ dataset show that our methods can significantly outperform baseline methods. version:1
arxiv-1607-07110 | Deep nets for local manifold learning | http://arxiv.org/abs/1607.07110 | id:1607.07110 author:Charles K. Chui, H. N. Mhaskar category:cs.LG  published:2016-07-24 summary:The problem of extending a function $f$ defined on a training data $\mathcal{C}$ on an unknown manifold $\mathbb{X}$ to the entire manifold and a tubular neighborhood of this manifold is considered in this paper. For $\mathbb{X}$ embedded in a high dimensional ambient Euclidean space $\mathbb{R}^D$, a deep learning algorithm is developed for finding a local coordinate system for the manifold {\bf without eigen--decomposition}, which reduces the problem to the classical problem of function approximation on a low dimensional cube. Deep nets (or multilayered neural networks) are proposed to accomplish this approximation scheme by using the training data. Our methods do not involve such optimization techniques as back--propagation, while assuring optimal (a priori) error bounds on the output in terms of the number of derivatives of the target function. In addition, these methods are universal, in that they do not require a prior knowledge of the smoothness of the target function, but adjust the accuracy of approximation locally and automatically, depending only upon the local smoothness of the target function. Our ideas are easily extended to solve both the pre--image problem and the out--of--sample extension problem, with a priori bounds on the growth of the function thus extended. version:1
arxiv-1607-07078 | Effective Connectivity-Based Neural Decoding: A Causal Interaction-Driven Approach | http://arxiv.org/abs/1607.07078 | id:1607.07078 author:Saba Emrani, Hamid Krim category:cs.NE  published:2016-07-24 summary:We propose a geometric model-free causality measurebased on multivariate delay embedding that can efficiently detect linear and nonlinear causal interactions between time series with no prior information. We then exploit the proposed causal interaction measure in real MEG data analysis. The results are used to construct effective connectivity maps of brain activity to decode different categories of visual stimuli. Moreover, we discovered that the MEG-based effective connectivity maps as a response to structured images exhibit more geometric patterns, as disclosed by analyzing the evolution of toplogical structures of the underlying networks using persistent homology. Extensive simulation and experimental result have been carried out to substantiate the capabilities of the proposed approach. version:1
arxiv-1607-07057 | Latent Tree Language Model | http://arxiv.org/abs/1607.07057 | id:1607.07057 author:Tomas Brychcin category:cs.CL  published:2016-07-24 summary:It this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles. The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms. We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model. version:1
