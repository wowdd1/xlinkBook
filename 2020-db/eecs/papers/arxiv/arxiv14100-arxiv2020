arxiv-1511-06295 | Policy Distillation | http://arxiv.org/abs/1511.06295 | id:1511.06295 author:Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell category:cs.LG  published:2015-11-19 summary:Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent. version:2
arxiv-1511-07401 | MazeBase: A Sandbox for Learning from Games | http://arxiv.org/abs/1511.07401 | id:1511.07401 author:Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, Rob Fergus category:cs.LG cs.AI cs.NE  published:2015-11-23 summary:This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Despite the tasks' simplicity, the performance of the models is far from optimal, suggesting directions for future development. We also demonstrate the versatility of MazeBase by using it to emulate small combat scenarios from StarCraft. Models trained on the MazeBase version can be directly applied to StarCraft, where they consistently beat the in-game AI. version:2
arxiv-1507-01526 | Grid Long Short-Term Memory | http://arxiv.org/abs/1507.01526 | id:1507.01526 author:Nal Kalchbrenner, Ivo Danihelka, Alex Graves category:cs.NE cs.CL cs.LG  published:2015-07-06 summary:This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task. version:3
arxiv-1506-08669 | Efficient and Parsimonious Agnostic Active Learning | http://arxiv.org/abs/1506.08669 | id:1506.08669 author:Tzu-Kuo Huang, Alekh Agarwal, Daniel J. Hsu, John Langford, Robert E. Schapire category:cs.LG stat.ML  published:2015-06-29 summary:We develop a new active learning algorithm for the streaming setting satisfying three important properties: 1) It provably works for any classifier representation and classification problem including those with severe noise. 2) It is efficiently implementable with an ERM oracle. 3) It is more aggressive than all previous approaches satisfying 1 and 2. To do this we create an algorithm based on a newly defined optimization problem and analyze it. We also conduct the first experimental analysis of all efficient agnostic active learning algorithms, evaluating their strengths and weaknesses in different settings. version:3
arxiv-1510-01624 | Population-Contrastive-Divergence: Does Consistency help with RBM training? | http://arxiv.org/abs/1510.01624 | id:1510.01624 author:Oswin Krause, Asja Fischer, Christian Igel category:cs.LG cs.NE stat.ML  published:2015-10-06 summary:Estimating the log-likelihood gradient with respect to the parameters of a Restricted Boltzmann Machine (RBM) typically requires sampling using Markov Chain Monte Carlo (MCMC) techniques. To save computation time, the Markov chains are only run for a small number of steps, which leads to a biased estimate. This bias can cause RBM training algorithms such as Contrastive Divergence (CD) learning to deteriorate. We adopt the idea behind Population Monte Carlo (PMC) methods to devise a new RBM training algorithm termed Population-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a consistent estimate and may have a significantly lower bias. Its computational overhead is negligible compared to CD. However, the variance of the gradient estimate increases. We experimentally show that pop-CD can significantly outperform CD. In many cases, we observed a smaller bias and achieved higher log-likelihood values. However, when the RBM distribution has many hidden neurons, the consistent estimate of pop-CD may still have a considerable bias and the variance of the gradient estimate requires a smaller learning rate. Thus, despite its superior theoretical properties, it is not advisable to use pop-CD in its current form on large problems. version:3
arxiv-1601-01607 | NodIO, a JavaScript framework for volunteer-based evolutionary algorithms : first results | http://arxiv.org/abs/1601.01607 | id:1601.01607 author:Juan-J. Merelo, Mario García-Valdez, Pedro A. Castillo, Pablo García-Sánchez, P. de las Cuevas, Nuria Rico category:cs.DC cs.NE  published:2016-01-07 summary:JavaScript is an interpreted language mainly known for its inclusion in web browsers, making them a container for rich Internet based applications. This has inspired its use, for a long time, as a tool for evolutionary algorithms, mainly so in browser-based volunteer computing environments. Several libraries have also been published so far and are in use. However, the last years have seen a resurgence of interest in the language, becoming one of the most popular and thus spawning the improvement of its implementations, which are now the foundation of many new client-server applications. We present such an application for running distributed volunteer-based evolutionary algorithm experiments, and we make a series of measurements to establish the speed of JavaScript in evolutionary algorithms that can serve as a baseline for comparison with other distributed computing experiments. These experiments use different integer and floating point problems, and prove that the speed of JavaScript is actually competitive with other languages commonly used by the evolutionary algorithm practitioner. version:1
arxiv-1511-06350 | Structured Prediction Energy Networks | http://arxiv.org/abs/1511.06350 | id:1511.06350 author:David Belanger, Andrew McCallum category:cs.LG  published:2015-11-19 summary:We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of structured outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques. version:2
arxiv-1511-04306 | Deep Feature Learning for EEG Recordings | http://arxiv.org/abs/1511.04306 | id:1511.04306 author:Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn category:cs.NE cs.LG  published:2015-11-13 summary:We introduce and compare several strategies for learning discriminative features from electroencephalography (EEG) recordings using deep learning techniques. EEG data are generally only available in small quantities, they are high-dimensional with a poor signal-to-noise ratio, and there is considerable variability between individual subjects and recording sessions. Our proposed techniques specifically address these challenges for feature learning. Cross-trial encoding forces auto-encoders to focus on features that are stable across trials. Similarity-constraint encoders learn features that allow to distinguish between classes by demanding that two trials from the same class are more similar to each other than to trials from other classes. This tuple-based training approach is especially suitable for small datasets. Hydra-nets allow for separate processing pathways adapting to subsets of a dataset and thus combine the advantages of individual feature learning (better adaptation of early, low-level processing) with group model training (better generalization of higher-level processing in deeper layers). This way, models can, for instance, adapt to each subject individually to compensate for differences in spatial patterns due to anatomical differences or variance in electrode positions. The different techniques are evaluated using the publicly available OpenMIIR dataset of EEG recordings taken while participants listened to and imagined music. version:4
arxiv-1411-5915 | Robust EM kernel-based methods for linear system identification | http://arxiv.org/abs/1411.5915 | id:1411.5915 author:Giulio Bottegal, Aleksandr Y. Aravkin, Håkan Hjalmarsson, Gianluigi Pillonetto category:cs.SY stat.ML  published:2014-11-21 summary:Recent developments in system identifi?cation have brought attention to regularized kernel-based methods. This type of approach has been proven to compare favorably with classic parametric methods. However, current formulations are not robust with respect to outliers. In this paper, we introduce a novel method to robustify kernel-based system identi?cation methods. To this end, we model the output measurement noise using random variables with heavy-tailed probability density functions (pdfs), focusing on the Laplacian and the Student's t distributions. Exploiting the representation of these pdfs as scale mixtures of Gaussians, we cast our system identifi?cation problem into a Gaussian process regression framework, which requires estimating a number of hyperparameters of the data size order. To overcome this di?culty, we design a new maximum a posteriori (MAP) estimator of the hyperparameters, and solve the related optimization problem with a novel iterative scheme based on the Expectation-Maximization (EM) method. In presence of outliers, tests on simulated data and on a real system show a substantial performance improvement compared to currently used kernel-based methods for linear system identifi?cation. version:3
arxiv-1601-01544 | State Space representation of non-stationary Gaussian Processes | http://arxiv.org/abs/1601.01544 | id:1601.01544 author:Alessio Benavoli, Marco Zaffalon category:cs.LG stat.ML  published:2016-01-07 summary:The state space (SS) representation of Gaussian processes (GP) has recently gained a lot of interest. The main reason is that it allows to compute GPs based inferences in O(n), where $n$ is the number of observations. This implementation makes GPs suitable for Big Data. For this reason, it is important to provide a SS representation of the most important kernels used in machine learning. The aim of this paper is to show how to exploit the transient behaviour of SS models to map non-stationary kernels to SS models. version:1
arxiv-1511-06488 | Resiliency of Deep Neural Networks under Quantization | http://arxiv.org/abs/1511.06488 | id:1511.06488 author:Wonyong Sung, Sungho Shin, Kyuyeon Hwang category:cs.LG cs.NE  published:2015-11-20 summary:The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in 'complexity limited' networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient. This paper also presents the effective compression ratio to guide the trade-off between the network size and the precision when the hardware resource is limited. version:3
arxiv-1601-01530 | Leveraging Sentence-level Information with Encoder LSTM for Natural Language Understanding | http://arxiv.org/abs/1601.01530 | id:1601.01530 author:Gakuto Kurata, Bing Xiang, Bowen Zhou, Mo Yu category:cs.CL  published:2016-01-07 summary:Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling. In this paper, we first enhance LSTM-based sequence labeling to explicitly model label dependencies. Then we propose another enhancement to incorporate the global information spanning over the whole input sequence. The latter proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling. Combining these methods, we can predict the label sequence with considering label dependencies and information of whole input sequence. In the experiments of a slot filling task, which is an essential component of natural language understanding, with using the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%. version:1
arxiv-1601-01675 | Ensemble Methods of Classification for Power Systems Security Assessment | http://arxiv.org/abs/1601.01675 | id:1601.01675 author:Alexei Zhukov, Victor Kurbatsky, Nikita Tomin, Denis Sidorov, Daniil Panasetsky, Aoife Foley category:cs.AI cs.LG 68T05  published:2016-01-07 summary:One of the most promising approaches for complex technical systems analysis employs ensemble methods of classification. Ensemble methods enable to build a reliable decision rules for feature space classification in the presence of many possible states of the system. In this paper, novel techniques based on decision trees are used for evaluation of the reliability of the regime of electric power systems. We proposed hybrid approach based on random forests models and boosting models. Such techniques can be applied to predict the interaction of increasing renewable power, storage devices and swiching of smart loads from intelligent domestic appliances, heaters and air-conditioning units and electric vehicles with grid for enhanced decision making. The ensemble classification methods were tested on the modified 118-bus IEEE power system showing that proposed technique can be employed to examine whether the power system is secured under steady-state operating conditions. version:1
arxiv-1601-01507 | Fast Kronecker product kernel methods via sampled vec trick | http://arxiv.org/abs/1601.01507 | id:1601.01507 author:Antti Airola, Tapio Pahikkala category:stat.ML cs.LG  published:2016-01-07 summary:Kronecker product kernel provides the standard approach in the kernel methods literature for learning from pair-input data, where both data points and prediction tasks have their own feature representations. The methods allow simultaneous generalization to both new tasks and data unobserved in the training set, a setting known as zero-shot or zero-data learning. Such a setting occurs in numerous applications, including drug-target interaction prediction, collaborative filtering and information retrieval. Efficient training algorithms based on the so-called vec trick, that makes use of the special structure of the Kronecker product, are known for the case where the output matrix for the training set is fully observed, i.e. the correct output for each data point-task combination is available. In this work we generalize these results, proposing an efficient algorithm for sampled Kronecker product multiplication, where only a subset of the full Kronecker product is computed. This allows us to derive a general framework for training Kronecker kernel methods, as specific examples we implement Kronecker ridge regression and support vector machine algorithms. Experimental results demonstrate that the proposed approach leads to accurate models, while allowing order of magnitude improvements in training and prediction time. version:1
arxiv-1602-07960 | Measuring and Discovering Correlations in Large Data Sets | http://arxiv.org/abs/1602.07960 | id:1602.07960 author:Lijue Liu, Ming Li, Sha Wen category:stat.ME stat.ML  published:2016-01-07 summary:In this paper, a class of statistics named ART (the alternant recursive topology statistics) is proposed to measure the properties of correlation between two variables. A wide range of bi-variable correlations both linear and nonlinear can be evaluated by ART efficiently and equitably even if nothing is known about the specific types of those relationships. ART compensates the disadvantages of Reshef's model in which no polynomial time precise algorithm exists and the "local random" phenomenon can not be identified. As a class of nonparametric exploration statistics, ART is applied for analyzing a dataset of 10 American classical indexes, as a result, lots of bi-variable correlations are discovered. version:1
arxiv-1508-06944 | Continuous parameter working memory in a balanced chaotic neural network | http://arxiv.org/abs/1508.06944 | id:1508.06944 author:Nimrod Shaham, Yoram Burak category:cond-mat.dis-nn cs.NE q-bio.NC  published:2015-08-27 summary:Working memory, the ability to maintain information over time scales greater than those characterizing single neurons, is essential to many brain functions. It remains unclear whether neural networks in the balanced state, an important model for activity in the cortex, can support a continuum of stable states that would make it possible to store a continuous variable in working memory while also accounting for the stochastic behavior of single neurons. Here we propose a simple neural architecture that achieves this goal. We show analytically that in the limit of an infinite network a continuous parameter can be stored indefinitely on a continuum of balanced states. For finite networks we calculate the diffusivity along the attractor driven by the chaotic noise in the network, and show that it is inversely proportional to the system size. Thus, for large enough (but realistic) neural population sizes, and with suitable tuning of the network connections, it is possible to maintain continuous parameter values over time scales larger by several orders of magnitude than the single neuron time scale. version:3
arxiv-1601-00149 | A Submodule Clustering Method for Multi-way Data by Sparse and Low-Rank Representation | http://arxiv.org/abs/1601.00149 | id:1601.00149 author:Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, Zhouchen Lin category:cs.CV  published:2016-01-02 summary:A new submodule clustering method via sparse and low-rank representation for multi-way data is proposed in this paper. Instead of reshaping multi-way data into vectors, this method maintains their natural orders to preserve data intrinsic structures, e.g., image data kept as matrices. To implement clustering, the multi-way data, viewed as tensors, are represented by the proposed tensor sparse and low-rank model to obtain its submodule representation, called a free module, which is finally used for spectral clustering. The proposed method extends the conventional subspace clustering method based on sparse and low-rank representation to multi-way data submodule clustering by combining t-product operator. The new method is tested on several public datasets, including synthetical data, video sequences and toy images. The experiments show that the new method outperforms the state-of-the-art methods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation (LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation (RobustLatLRR) and Sparse Submodule Clustering method (SSmC). version:2
arxiv-1511-04103 | Basic Level Categorization Facilitates Visual Object Recognition | http://arxiv.org/abs/1511.04103 | id:1511.04103 author:Panqu Wang, Garrison W. Cottrell category:cs.CV  published:2015-11-12 summary:Recent advances in deep learning have led to significant progress in the computer vision field, especially for visual object recognition tasks. The features useful for object classification are learned by feed-forward deep convolutional neural networks (CNNs) automatically, and they are shown to be able to predict and decode neural representations in the ventral visual pathway of humans and monkeys. However, despite the huge amount of work on optimizing CNNs, there has not been much research focused on linking CNNs with guiding principles from the human visual cortex. In this work, we propose a network optimization strategy inspired by both of the developmental trajectory of children's visual object recognition capabilities, and Bar (2003), who hypothesized that basic level information is carried in the fast magnocellular pathway through the prefrontal cortex (PFC) and then projected back to inferior temporal cortex (IT), where subordinate level categorization is achieved. We instantiate this idea by training a deep CNN to perform basic level object categorization first, and then train it on subordinate level categorization. We apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC 2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%, demonstrating the effectiveness of the method. We also show that subsequent transfer learning on smaller datasets gives superior results. version:3
arxiv-1506-06442 | A Deep Memory-based Architecture for Sequence-to-Sequence Learning | http://arxiv.org/abs/1506.06442 | id:1506.06442 author:Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li, Qun Liu category:cs.CL cs.LG cs.NE  published:2015-06-22 summary:We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations between the representations. The types of transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, DEEPMEMORY can model complicated relations between sequences necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on sequenceto-sequence data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is broad enough to subsume the state-of-the-art neural translation model in (Bahdanau et al., 2015) as its special case, while significantly improving upon the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system Moses with a small vocabulary and a modest parameter size. version:4
arxiv-1601-01432 | Block-Diagonal Sparse Representation by Learning a Linear Combination Dictionary for Recognition | http://arxiv.org/abs/1601.01432 | id:1601.01432 author:Xinglin Piao, Yongli Hu, Yanfeng Sun, Junbin Gao, Baocai Yin category:cs.CV  published:2016-01-07 summary:In a sparse representation based recognition scheme, it is critical to learn a desired dictionary, aiming both good representational power and discriminative performance. In this paper, we propose a new dictionary learning model for recognition applications, in which three strategies are adopted to achieve these two objectives simultaneously. First, a block-diagonal constraint is introduced into the model to eliminate the correlation between classes and enhance the discriminative performance. Second, a low-rank term is adopted to model the coherence within classes for refining the sparse representation of each class. Finally, instead of using the conventional over-complete dictionary, a specific dictionary constructed from the linear combination of the training samples is proposed to enhance the representational power of the dictionary and to improve the robustness of the sparse representation model. The proposed method is tested on several public datasets. The experimental results show the method outperforms most state-of-the-art methods. version:1
arxiv-1601-01431 | Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis | http://arxiv.org/abs/1601.01431 | id:1601.01431 author:Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu category:cs.CV  published:2016-01-07 summary:The probabilistic principal component analysis (PPCA) is built upon a global linear mapping, with which it is insufficient to model complex data variation. This paper proposes a mixture of bilateral-projection probabilistic principal component analysis model (mixB2DPPCA) on 2D data. With multi-components in the mixture, this model can be seen as a soft cluster algorithm and has capability of modeling data with complex structures. A Bayesian inference scheme has been proposed based on the variational EM (Expectation-Maximization) approach for learning model parameters. Experiments on some publicly available databases show that the performance of mixB2DPPCA has been largely improved, resulting in more accurate reconstruction errors and recognition rates than the existing PCA-based algorithms. version:1
arxiv-1601-01422 | Stochastic Dykstra Algorithms for Metric Learning on Positive Semi-Definite Cone | http://arxiv.org/abs/1601.01422 | id:1601.01422 author:Tomoki Matsuzawa, Raissa Relator, Jun Sese, Tsuyoshi Kato category:cs.CV  published:2016-01-07 summary:Recently, covariance descriptors have received much attention as powerful representations of set of points. In this research, we present a new metric learning algorithm for covariance descriptors based on the Dykstra algorithm, in which the current solution is projected onto a half-space at each iteration, and runs at O(n^3) time. We empirically demonstrate that randomizing the order of half-spaces in our Dykstra-based algorithm significantly accelerates the convergence to the optimal solution. Furthermore, we show that our approach yields promising experimental results on pattern recognition tasks. version:1
arxiv-1511-06457 | DOC: Deep OCclusion Estimation From A Single Image | http://arxiv.org/abs/1511.06457 | id:1511.06457 author:Peng Wang, Alan Yuille category:cs.CV cs.LG  published:2015-11-20 summary:Recovering the occlusion relationships between objects is a fundamental human visual ability which yields important information about the 3D world. In this paper we propose a deep network architecture, called DOC, which acts on a single image, detects object boundaries and estimates the border ownership (i.e. which side of the boundary is foreground and which is background). We represent occlusion relations by a binary edge map, to indicate the object boundary, and an occlusion orientation variable which is tangential to the boundary and whose direction specifies border ownership by a left-hand rule, see Fig.1. We train two related deep convolutional neural networks, called DOC, which exploit local and non-local image cues to estimate this representation and hence recover occlusion relations. In order to train and test DOC we construct a large-scale instance occlusion boundary dataset using PASCAL VOC images, which we call the PASCAL instance occlusion dataset (PIOD). This contains 10,000 images and hence is two orders of magnitude larger than existing occlusion datasets for outdoor images. We test two variants of DOC on PIOD and on the BSDS occlusion dataset and show they outperform state-of-the-art methods typically by more than 5AP. Finally, we perform numerous experiments investigating multiple settings of DOC and transfer between BSDS and PIOD, which provides more insights for further study of occlusion estimation. version:3
arxiv-1601-01411 | Learning Kernels for Structured Prediction using Polynomial Kernel Transformations | http://arxiv.org/abs/1601.01411 | id:1601.01411 author:Chetan Tonde, Ahmed Elgammal category:cs.LG stat.ML  published:2016-01-07 summary:Learning the kernel functions used in kernel methods has been a vastly explored area in machine learning. It is now widely accepted that to obtain 'good' performance, learning a kernel function is the key challenge. In this work we focus on learning kernel representations for structured regression. We propose use of polynomials expansion of kernels, referred to as Schoenberg transforms and Gegenbaur transforms, which arise from the seminal result of Schoenberg (1938). These kernels can be thought of as polynomial combination of input features in a high dimensional reproducing kernel Hilbert space (RKHS). We learn kernels over input and output for structured data, such that, dependency between kernel features is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC) to measure this. We also give an efficient, matrix decomposition-based algorithm to learn these kernel transformations, and demonstrate state-of-the-art results on several real-world datasets. version:1
arxiv-1601-01297 | Angrier Birds: Bayesian reinforcement learning | http://arxiv.org/abs/1601.01297 | id:1601.01297 author:Imanol Arrieta Ibarra, Bernardo Ramos, Lars Roemheld category:cs.AI cs.LG  published:2016-01-06 summary:We train a reinforcement learner to play a simplified version of the game Angry Birds. The learner is provided with a game state in a manner similar to the output that could be produced by computer vision algorithms. We improve on the efficiency of regular {\epsilon}-greedy Q-Learning with linear function approximation through more systematic exploration in Randomized Least Squares Value Iteration (RLSVI), an algorithm that samples its policy from a posterior distribution on optimal policies. With larger state-action spaces, efficient exploration becomes increasingly important, as evidenced by the faster learning in RLSVI. version:2
arxiv-1511-09263 | Scalable and Accurate Online Feature Selection for Big Data | http://arxiv.org/abs/1511.09263 | id:1511.09263 author:Kui Yu, Xindong Wu, Wei Ding, Jian Pei category:cs.LG  published:2015-11-30 summary:Feature selection is important in many big data applications. There are at least two critical challenges. Firstly, in many applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, feature selection has to be highly scalable, preferably in an online manner such that each feature can be processed in a sequential scan. In this paper, we develop SAOLA, a Scalable and Accurate OnLine Approach for feature selection. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel online pairwise comparison techniques to address the two challenges and maintain a parsimonious model over time in an online manner. Furthermore, to tackle the dimensionality that arrives by groups, we extend our SAOLA algorithm, and then propose a novel group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the level of both groups and individual features simultaneously. An empirical study using a series of benchmark real data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art feature selection methods. version:2
arxiv-1511-04590 | Empirical performance upper bounds for image and video captioning | http://arxiv.org/abs/1511.04590 | id:1511.04590 author:Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio category:cs.CV cs.CL stat.ML  published:2015-11-14 summary:The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets. version:4
arxiv-1601-01339 | Quality Adaptive Low-Rank Based JPEG Decoding with Applications | http://arxiv.org/abs/1601.01339 | id:1601.01339 author:Xiao Shu, Xiaolin Wu category:cs.CV  published:2016-01-06 summary:Small compression noises, despite being transparent to human eyes, can adversely affect the results of many image restoration processes, if left unaccounted for. Especially, compression noises are highly detrimental to inverse operators of high-boosting (sharpening) nature, such as deblurring and superresolution against a convolution kernel. By incorporating the non-linear DCT quantization mechanism into the formulation for image restoration, we propose a new sparsity-based convex programming approach for joint compression noise removal and image restoration. Experimental results demonstrate significant performance gains of the new approach over existing image restoration methods. version:1
arxiv-1507-07595 | Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity | http://arxiv.org/abs/1507.07595 | id:1507.07595 author:Jason D. Lee, Qihang Lin, Tengyu Ma, Tianbao Yang category:math.OC cs.LG stat.ML  published:2015-07-27 summary:We study distributed optimization algorithms for minimizing the average of convex functions. The applications include empirical risk minimization problems in statistical machine learning where the datasets are large and have to be stored on different machines. We design a distributed stochastic variance reduced gradient algorithm that, under certain conditions on the condition number, simultaneously achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. Our method and its accelerated extension also outperform existing distributed algorithms in terms of the rounds of communication as long as the condition number is not too large compared to the size of data in each machine. We also prove a lower bound for the number of rounds of communication for a broad class of distributed first-order methods including the proposed algorithms in this paper. We show that our accelerated distributed stochastic variance reduced gradient algorithm achieves this lower bound so that it uses the fewest rounds of communication among all distributed first-order algorithms. version:2
arxiv-1601-01232 | Shape Animation with Combined Captured and Simulated Dynamics | http://arxiv.org/abs/1601.01232 | id:1601.01232 author:Benjamin Allain, Li Wang, Jean-Sebastien Franco, Franck Hetroy, Edmond Boyer category:cs.GR cs.CV  published:2016-01-06 summary:We present a novel volumetric animation generation framework to create new types of animations from raw 3D surface or point cloud sequence of captured real performances. The framework considers as input time incoherent 3D observations of a moving shape, and is thus particularly suitable for the output of performance capture platforms. In our system, a suitable virtual representation of the actor is built from real captures that allows seamless combination and simulation with virtual external forces and objects, in which the original captured actor can be reshaped, disassembled or reassembled from user-specified virtual physics. Instead of using the dominant surface-based geometric representation of the capture, which is less suitable for volumetric effects, our pipeline exploits Centroidal Voronoi tessellation decompositions as unified volumetric representation of the real captured actor, which we show can be used seamlessly as a building block for all processing stages, from capture and tracking to virtual physic simulation. The representation makes no human specific assumption and can be used to capture and re-simulate the actor with props or other moving scenery elements. We demonstrate the potential of this pipeline for virtual reanimation of a real captured event with various unprecedented volumetric visual effects, such as volumetric distortion, erosion, morphing, gravity pull, or collisions. version:1
arxiv-1601-01218 | Adaptive and Efficient Nonlinear Channel Equalization for Underwater Acoustic Communication | http://arxiv.org/abs/1601.01218 | id:1601.01218 author:Dariush Kari, Nuri Denizcan Vanli, Suleyman Serdar Kozat category:cs.LG cs.IT cs.SD math.IT  published:2016-01-06 summary:We investigate underwater acoustic (UWA) channel equalization and introduce hierarchical and adaptive nonlinear channel equalization algorithms that are highly efficient and provide significantly improved bit error rate (BER) performance. Due to the high complexity of nonlinear equalizers and poor performance of linear ones, to equalize highly difficult underwater acoustic channels, we employ piecewise linear equalizers. However, in order to achieve the performance of the best piecewise linear model, we use a tree structure to hierarchically partition the space of the received signal. Furthermore, the equalization algorithm should be completely adaptive, since due to the highly non-stationary nature of the underwater medium, the optimal MSE equalizer as well as the best piecewise linear equalizer changes in time. To this end, we introduce an adaptive piecewise linear equalization algorithm that not only adapts the linear equalizer at each region but also learns the complete hierarchical structure with a computational complexity only polynomial in the number of nodes of the tree. Furthermore, our algorithm is constructed to directly minimize the final squared error without introducing any ad-hoc parameters. We demonstrate the performance of our algorithms through highly realistic experiments performed on accurately simulated underwater acoustic channels. version:1
arxiv-1601-01216 | Automatic 3D object detection of Proteins in Fluorescent labeled microscope images with spatial statistical analysis | http://arxiv.org/abs/1601.01216 | id:1601.01216 author:Ramin Norousi, Volker J. Schmid category:cs.CV  published:2016-01-06 summary:Since manual object detection is very inaccurate and time consuming, some automatic object detection tools have been developed in recent years. At the moment, there is no image analysis software available which provides an automatic, objective assessment of 3D foci which is generally applicable. Complications arise from discrete foci which are very close or even come in contact to other foci, moreover they are of variable sizes and show variable signal-to-noise, and must be analyzed fully in 3D. Therefore we introduce the 3D-OSCOS (3D-Object Segmentation and Colocalization Analysis based on Spatial statistics) algorithm which is implemented as a user-friendly toolbox for interactive detection of 3D objects and visualization of labeled images. version:1
arxiv-1506-00575 | A Riemannian low-rank method for optimization over semidefinite matrices with block-diagonal constraints | http://arxiv.org/abs/1506.00575 | id:1506.00575 author:Nicolas Boumal category:math.OC cs.CV stat.CO  published:2015-06-01 summary:We propose a new algorithm to solve optimization problems of the form $\min f(X)$ for a smooth function $f$ under the constraints that $X$ is positive semidefinite and the diagonal blocks of $X$ are small identity matrices. Such problems often arise as the result of relaxing a rank constraint (lifting). In particular, many estimation tasks involving phases, rotations, orthonormal bases or permutations fit in this framework, and so do certain relaxations of combinatorial problems such as Max-Cut. The proposed algorithm exploits the facts that (1) such formulations admit low-rank solutions, and (2) their rank-restricted versions are smooth optimization problems on a Riemannian manifold. Combining insights from both the Riemannian and the convex geometries of the problem, we characterize when second-order critical points of the smooth problem reveal KKT points of the semidefinite problem. We compare against state of the art, mature software and find that, on certain interesting problem instances, what we call the staircase method is orders of magnitude faster, is more accurate and scales better. Code is available. version:2
arxiv-1601-01195 | Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON 2015 | http://arxiv.org/abs/1601.01195 | id:1601.01195 author:Kamal Sarkar category:cs.CL 68T50  published:2016-01-06 summary:This paper discusses the experiments carried out by us at Jadavpur University as part of the participation in ICON 2015 task: POS Tagging for Code-mixed Indian Social Media Text. The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information from dictionary as well as some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for Bengali-English, Hindi-English and Tamil-English Language pairs. Our system has been trained and tested on the datasets released for ICON 2015 shared task: POS Tagging For Code-mixed Indian Social Media Text. In constrained mode, our system obtains average overall accuracy (averaged over all three language pairs) of 75.60% which is very close to other participating two systems (76.79% for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In unconstrained mode, our system obtains average overall accuracy of 70.65% which is also close to the system (72.85% for AMRITA_CEN) which obtains the highest average overall accuracy. version:1
arxiv-1601-01190 | On Bayesian index policies for sequential resource allocation | http://arxiv.org/abs/1601.01190 | id:1601.01190 author:Emilie Kaufmann category:stat.ML  published:2016-01-06 summary:This paper is about index policies for minimizing (frequentist) regret in a stochastic multi-armed bandit model, that are inspired by a Bayesian view on the problem. Our main contribution is to prove the asymptotic optimality of Bayes-UCB, an algorithm based on quantiles of posterior distributions, when the rewards distributions belong to a one-dimensional exponential family, for a large class of prior distributions. We also show that the Bayesian literature gives new insight on what kind of exploration rates could be used in frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian optimal solution or the Finite Horizon Gittins indices suggest the introduction of two algorithms, KL-UCB + and KL-UCB-H + , whose asymptotic optimality is also established. version:1
arxiv-1408-3264 | A brief survey on deep belief networks and introducing a new object oriented toolbox (DeeBNet) | http://arxiv.org/abs/1408.3264 | id:1408.3264 author:Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour category:cs.CV cs.LG cs.MS cs.NE 68T01  published:2014-08-14 summary:Nowadays, this is very popular to use the deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. DBNs have many ability like feature extraction and classification that are used in many applications like image processing, speech processing and etc. This paper introduces a new object oriented MATLAB toolbox with most of abilities needed for the implementation of DBNs. In the new version, the toolbox can be used in Octave. According to the results of the experiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups (text) datasets, it was shown that the toolbox can learn automatically a good representation of the input from unlabeled data with better discrimination between different classes. Also on all datasets, the obtained classification errors are comparable to those of state of the art classifiers. In addition, the toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and our new FEPCD method), different sparsity methods (quadratic, rate distortion and our new normal method), different RBM types (generative and discriminative), using GPU, etc. The toolbox is a user-friendly open source software and is freely available on the website http://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html . version:7
arxiv-1601-01157 | A simple technique for improving multi-class classification with neural networks | http://arxiv.org/abs/1601.01157 | id:1601.01157 author:Thomas Kopinski, Alexander Gepperth, Uwe Handmann category:cs.LG  published:2016-01-06 summary:We present a novel method to perform multi-class pattern classification with neural networks and test it on a challenging 3D hand gesture recognition problem. Our method consists of a standard one-against-all (OAA) classification, followed by another network layer classifying the resulting class scores, possibly augmented by the original raw input vector. This allows the network to disambiguate hard-to-separate classes as the distribution of class scores carries considerable information as well, and is in fact often used for assessing the confidence of a decision. We show that by this approach we are able to significantly boost our results, overall as well as for particular difficult cases, on the hard 10-class gesture classification task. version:1
arxiv-1601-01145 | Vehicle Classification using Transferable Deep Neural Network Features | http://arxiv.org/abs/1601.01145 | id:1601.01145 author:Yiren Zhou, Ngai-Man Cheung category:cs.CV  published:2016-01-06 summary:We address vehicle detection on rear view vehicle images captured from a distance along multi-lane highways, and vehicle classification using transferable features from Deep Neural Network. We address the following problems that are specific to our application: how to utilize dash lane markings to assist vehicle detection, what features are useful for classification on vehicle categories, and how to utilize Deep Neural Network when the size of the labelled data is limited. Experiment results suggest our approach outperforms other state-of-the-art. version:1
arxiv-1601-01142 | Streaming Gibbs Sampling for LDA Model | http://arxiv.org/abs/1601.01142 | id:1601.01142 author:Yang Gao, Jianfei Chen, Jun Zhu category:cs.LG stat.ML  published:2016-01-06 summary:Streaming variational Bayes (SVB) is successful in learning LDA models in an online manner. However previous attempts toward developing online Monte-Carlo methods for LDA have little success, often by having much worse perplexity than their batch counterparts. We present a streaming Gibbs sampling (SGS) method, an online extension of the collapsed Gibbs sampling (CGS). Our empirical study shows that SGS can reach similar perplexity as CGS, much better than SVB. Our distributed version of SGS, DSGS, is much more scalable than SVB mainly because the updates' communication complexity is small. version:1
arxiv-1601-01121 | A pragmatic approach to multi-class classification | http://arxiv.org/abs/1601.01121 | id:1601.01121 author:Thomas Kopinski, Stéphane Magand, Uwe Handmann, Alexander Gepperth category:cs.LG  published:2016-01-06 summary:We present a novel hierarchical approach to multi-class classification which is generic in that it can be applied to different classification models (e.g., support vector machines, perceptrons), and makes no explicit assumptions about the probabilistic structure of the problem as it is usually done in multi-class classification. By adding a cascade of additional classifiers, each of which receives the previous classifier's output in addition to regular input data, the approach harnesses unused information that manifests itself in the form of, e.g., correlations between predicted classes. Using multilayer perceptrons as a classification model, we demonstrate the validity of this approach by testing it on a complex ten-class 3D gesture recognition task. version:1
arxiv-1601-00732 | Low-Rank Representation over the Manifold of Curves | http://arxiv.org/abs/1601.00732 | id:1601.00732 author:Stephen Tierney, Junbin Gao, Yi Guo, Zhengwu Zhang category:cs.CV cs.LG  published:2016-01-05 summary:In machine learning it is common to interpret each data point as a vector in Euclidean space. However the data may actually be functional i.e.\ each data point is a function of some variable such as time and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function. In this paper we propose a method to analyse subspace structure of the functional data by using the state of the art Low-Rank Representation (LRR). Experimental evaluation on synthetic and real data reveals that this method massively outperforms conventional LRR in tasks concerning functional data. version:2
arxiv-1502-01271 | INRIASAC: Simple Hypernym Extraction Methods | http://arxiv.org/abs/1502.01271 | id:1502.01271 author:Gregory Grefenstette category:cs.CL  published:2015-02-04 summary:Given a set of terms from a given domain, how can we structure them into a taxonomy without manual intervention? This is the task 17 of SemEval 2015. Here we present our simple taxonomy structuring techniques which, despite their simplicity, ranked first in this 2015 benchmark. We use large quantities of text (English Wikipedia) and simple heuristics such as term overlap and document and sentence co-occurrence to produce hypernym lists. We describe these techniques and pre-sent an initial evaluation of results. version:2
arxiv-1512-05919 | A Planning based Framework for Essay Generation | http://arxiv.org/abs/1512.05919 | id:1512.05919 author:Bing Qin, Duyu Tang, Xinwei Geng, Dandan Ning, Jiahao Liu, Ting Liu category:cs.CL  published:2015-12-18 summary:Generating an article automatically with computer program is a challenging task in artificial intelligence and natural language processing. In this paper, we target at essay generation, which takes as input a topic word in mind and generates an organized article under the theme of the topic. We follow the idea of text planning \cite{Reiter1997} and develop an essay generation framework. The framework consists of three components, including topic understanding, sentence extraction and sentence reordering. For each component, we studied several statistical algorithms and empirically compared between them in terms of qualitative or quantitative analysis. Although we run experiments on Chinese corpus, the method is language independent and can be easily adapted to other language. We lay out the remaining challenges and suggest avenues for future research. version:2
arxiv-1601-01100 | Memory Matters: Convolutional Recurrent Neural Network for Scene Text Recognition | http://arxiv.org/abs/1601.01100 | id:1601.01100 author:Guo Qiang, Tu Dan, Li Guohui, Lei Jun category:cs.CV  published:2016-01-06 summary:Text recognition in natural scene is a challenging problem due to the many factors affecting text appearance. In this paper, we presents a method that directly transcribes scene text images to text without needing of sophisticated character segmentation. We leverage recent advances of deep neural networks to model the appearance of scene text images with temporal dynamics. Specifically, we integrates convolutional neural network (CNN) and recurrent neural network (RNN) which is motivated by observing the complementary modeling capabilities of the two models. The main contribution of this work is investigating how temporal memory helps in an segmentation free fashion for this specific problem. By using long short-term memory (LSTM) blocks as hidden units, our model can retain long-term memory compared with HMMs which only maintain short-term state dependences. We conduct experiments on Street View House Number dataset containing highly variable number images. The results demonstrate the superiority of the proposed method over traditional HMM based methods. version:1
arxiv-1506-01497 | Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | http://arxiv.org/abs/1506.01497 | id:1506.01497 author:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun category:cs.CV  published:2015-06-04 summary:State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available. version:3
arxiv-1510-07945 | Learning Multi-Domain Convolutional Neural Networks for Visual Tracking | http://arxiv.org/abs/1510.07945 | id:1510.07945 author:Hyeonseob Nam, Bohyung Han category:cs.CV  published:2015-10-27 summary:We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify the target in each domain. We train the network with respect to each domain iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance compared with state-of-the-art methods in existing tracking benchmarks. version:2
arxiv-1601-01085 | Incorporating Structural Alignment Biases into an Attentional Neural Translation Model | http://arxiv.org/abs/1601.01085 | id:1601.01085 author:Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, Gholamreza Haffari category:cs.CL  published:2016-01-06 summary:Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting. version:1
arxiv-1506-08009 | Skopus: Exact discovery of the most interesting sequential patterns under Leverage | http://arxiv.org/abs/1506.08009 | id:1506.08009 author:Francois Petitjean, Tao Li, Nikolaj Tatti, Geoffrey I. Webb category:cs.AI cs.LG stat.ML  published:2015-06-26 summary:This paper presents a framework for exact discovery of the most interesting sequential patterns. It combines (1) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. Our interestingness measure is based on comparing the pattern support with the average support of its sister patterns, obtained by permuting (to certain extent) the items of the pattern. The larger the support compared to the expectation, the more interesting is the pattern. We build on these two elements to exactly extract the k sequential patterns with highest leverage, consistent with our definition of expected support. We conduct experiments on both synthetic data with known patterns and real-world datasets; both experiments confirm the consistency and relevance of our approach with regard to the state of the art. version:2
arxiv-1601-01073 | Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism | http://arxiv.org/abs/1601.01073 | id:1601.01073 author:Orhan Firat, Kyunghyun Cho, Yoshua Bengio category:cs.CL stat.ML  published:2016-01-06 summary:We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs. version:1
arxiv-1601-01060 | Low-rank Matrix Factorization under General Mixture Noise Distributions | http://arxiv.org/abs/1601.01060 | id:1601.01060 author:Xiangyong Cao, Qian Zhao, Deyu Meng, Yang Chen, Zongben Xu category:cs.CV  published:2016-01-06 summary:Many computer vision problems can be posed as learning a low-dimensional subspace from high dimensional data. The low rank matrix factorization (LRMF) represents a commonly utilized subspace learning strategy. Most of the current LRMF techniques are constructed on the optimization problems using L1-norm and L2-norm losses, which mainly deal with Laplacian and Gaussian noises, respectively. To make LRMF capable of adapting more complex noise, this paper proposes a new LRMF model by assuming noise as Mixture of Exponential Power (MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combining the penalized likelihood method with MoEP distributions. Such setting facilitates the learned LRMF model capable of automatically fitting the real noise through MoEP distributions. Each component in this mixture is adapted from a series of preliminary super- or sub-Gaussian candidates. Moreover, by facilitating the local continuity of noise components, we embed Markov random field into the PMoEP model and further propose the advanced PMoEP-MRF model. An Expectation Maximization (EM) algorithm and a variational EM (VEM) algorithm are also designed to infer the parameters involved in the proposed PMoEP and the PMoEP-MRF model, respectively. The superseniority of our methods is demonstrated by extensive experiments on synthetic data, face modeling, hyperspectral image restoration and background subtraction. version:1
arxiv-1412-6156 | Achieving Exact Cluster Recovery Threshold via Semidefinite Programming | http://arxiv.org/abs/1412.6156 | id:1412.6156 author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.DS math.PR  published:2014-11-24 summary:The binary symmetric stochastic block model deals with a random graph of $n$ vertices partitioned into two equal-sized clusters, such that each pair of vertices is connected independently with probability $p$ within clusters and $q$ across clusters. In the asymptotic regime of $p=a \log n/n$ and $q=b \log n/n$ for fixed $a,b$ and $n \to \infty$, we show that the semidefinite programming relaxation of the maximum likelihood estimator achieves the optimal threshold for exactly recovering the partition from the graph with probability tending to one, resolving a conjecture of Abbe et al. \cite{Abbe14}. Furthermore, we show that the semidefinite programming relaxation also achieves the optimal recovery threshold in the planted dense subgraph model containing a single cluster of size proportional to $n$. version:2
arxiv-1511-08102 | L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs | http://arxiv.org/abs/1511.08102 | id:1511.08102 author:Matey Neykov, Jun S. Liu, Tianxi Cai category:math.ST stat.ML stat.TH  published:2015-11-25 summary:It is known that for a certain class of single index models (SIM) $Y = f(\boldsymbol{X}^\intercal\boldsymbol{\beta}_0, \varepsilon)$, support recovery is impossible when $\boldsymbol{X} \sim \mathcal{N}(0, \mathbb{I}_{p \times p})$ and a model complexity adjusted sample size is below a critical threshold. Recently, optimal algorithms based on Sliced Inverse Regression (SIR) were suggested. These algorithms work provably under the assumption that the design matrix $\boldsymbol{X}$ comes from an i.i.d. Gaussian distribution. In the present paper we analyze algorithms based on covariance screening and least squares with $L_1$ penalization (i.e. LASSO) and demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample size in terms of support recovery, albeit under slightly different assumptions on $f$ and $\varepsilon$ compared to the SIR based algorithms. Furthermore, we show more generally, that LASSO succeeds in recovering the signed support of $\boldsymbol{\beta}_0$ if $\boldsymbol{X} \sim \mathcal{N}(0, \boldsymbol{\Sigma})$, and the covariance $\boldsymbol{\Sigma}$ satisfies the irrepresentable condition. Our work extends existing results on the support recovery of LASSO for the linear model, to a certain class of SIM. version:2
arxiv-1601-00998 | Forecasting Social Navigation in Crowded Complex Scenes | http://arxiv.org/abs/1601.00998 | id:1601.00998 author:Alexandre Robicquet, Alexandre Alahi, Amir Sadeghian, Bryan Anenberg, John Doherty, Eli Wu, Silvio Savarese category:cs.CV cs.RO cs.SI  published:2016-01-05 summary:When humans navigate a crowed space such as a university campus or the sidewalks of a busy street, they follow common sense rules based on social etiquette. In this paper, we argue that in order to enable the design of new algorithms that can take fully advantage of these rules to better solve tasks such as target tracking or trajectory forecasting, we need to have access to better data in the first place. To that end, we contribute the very first large scale dataset (to the best of our knowledge) that collects images and videos of various types of targets (not just pedestrians, but also bikers, skateboarders, cars, buses, golf carts) that navigate in a real-world outdoor environment such as a university campus. We present an extensive evaluation where different methods for trajectory forecasting are evaluated and compared. Moreover, we present a new algorithm for trajectory prediction that exploits the complexity of our new dataset and allows to: i) incorporate inter-class interactions into trajectory prediction models (e.g, pedestrian vs bike) as opposed to just intra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree to which the social forces are regulating an interaction. We call the latter "social sensitivity"and it captures the sensitivity to which a target is responding to a certain interaction. An extensive experimental evaluation demonstrates the effectiveness of our novel approach. version:1
arxiv-1601-00978 | Crater Detection via Convolutional Neural Networks | http://arxiv.org/abs/1601.00978 | id:1601.00978 author:Joseph Paul Cohen, Henry Z. Lo, Tingting Lu, Wei Ding category:cs.CV  published:2016-01-05 summary:Craters are among the most studied geomorphic features in the Solar System because they yield important information about the past and present geological processes and provide information about the relative ages of observed geologic formations. We present a method for automatic crater detection using advanced machine learning to deal with the large amount of satellite imagery collected. The challenge of automatically detecting craters comes from their is complex surface because their shape erodes over time to blend into the surface. Bandeira provided a seminal dataset that embodied this challenge that is still an unsolved pattern recognition problem to this day. There has been work to solve this challenge based on extracting shape and contrast features and then applying classification models on those features. The limiting factor in this existing work is the use of hand crafted filters on the image such as Gabor or Sobel filters or Haar features. These hand crafted methods rely on domain knowledge to construct. We would like to learn the optimal filters and features based on training examples. In order to dynamically learn filters and features we look to Convolutional Neural Networks (CNNs) which have shown their dominance in computer vision. The power of CNNs is that they can learn image filters which generate features for high accuracy classification. version:1
arxiv-1601-00955 | Optimally Pruning Decision Tree Ensembles With Feature Cost | http://arxiv.org/abs/1601.00955 | id:1601.00955 author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.LG  published:2016-01-05 summary:We consider the problem of learning decision rules for prediction with feature budget constraint. In particular, we are interested in pruning an ensemble of decision trees to reduce expected feature cost while maintaining high prediction accuracy for any test example. We propose a novel 0-1 integer program formulation for ensemble pruning. Our pruning formulation is general - it takes any ensemble of decision trees as input. By explicitly accounting for feature-sharing across trees together with accuracy/cost trade-off, our method is able to significantly reduce feature cost by pruning subtrees that introduce more loss in terms of feature cost than benefit in terms of prediction accuracy gain. Theoretically, we prove that a linear programming relaxation produces the exact solution of the original integer program. This allows us to use efficient convex optimization tools to obtain an optimally pruned ensemble for any given budget. Empirically, we see that our pruning algorithm significantly improves the performance of the state of the art ensemble method BudgetRF. version:1
arxiv-1506-05254 | Gradient Estimation Using Stochastic Computation Graphs | http://arxiv.org/abs/1506.05254 | id:1506.05254 author:John Schulman, Nicolas Heess, Theophane Weber, Pieter Abbeel category:cs.LG  published:2015-06-17 summary:In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions. version:3
arxiv-1601-00925 | Complex Decomposition of the Negative Distance kernel | http://arxiv.org/abs/1601.00925 | id:1601.00925 author:Tim vor der Brück, Steffen Eger, Alexander Mehler category:cs.LG  published:2016-01-05 summary:A Support Vector Machine (SVM) has become a very popular machine learning method for text classification. One reason for this relates to the range of existing kernels which allow for classifying data that is not linearly separable. The linear, polynomial and RBF (Gaussian Radial Basis Function) kernel are commonly used and serve as a basis of comparison in our study. We show how to derive the primal form of the quadratic Power Kernel (PK) -- also called the Negative Euclidean Distance Kernel (NDK) -- by means of complex numbers. We exemplify the NDK in the framework of text categorization using the Dewey Document Classification (DDC) as the target scheme. Our evaluation shows that the power kernel produces F-scores that are comparable to the reference kernels, but is -- except for the linear kernel -- faster to compute. Finally, we show how to extend the NDK-approach by including the Mahalanobis distance. version:1
arxiv-1601-00909 | The high-conductance state enables neural sampling in networks of LIF neurons | http://arxiv.org/abs/1601.00909 | id:1601.00909 author:Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE physics.bio-ph  published:2016-01-05 summary:The apparent stochasticity of in-vivo neural circuits has long been hypothesized to represent a signature of ongoing stochastic inference in the brain. More recently, a theoretical framework for neural sampling has been proposed, which explains how sample-based inference can be performed by networks of spiking neurons. One particular requirement of this approach is that the neural response function closely follows a logistic curve. Analytical approaches to calculating neural response functions have been the subject of many theoretical studies. In order to make the problem tractable, particular assumptions regarding the neural or synaptic parameters are usually made. However, biologically significant activity regimes exist which are not covered by these approaches: Under strong synaptic bombardment, as is often the case in cortex, the neuron is shifted into a high-conductance state (HCS) characterized by a small membrane time constant. In this regime, synaptic time constants and refractory periods dominate membrane dynamics. The core idea of our approach is to separately consider two different "modes" of spiking dynamics: burst spiking and transient quiescence, in which the neuron does not spike for longer periods. We treat the former by propagating the PDF of the effective membrane potential from spike to spike within a burst, while using a diffusion approximation for the latter. We find that our prediction of the neural response function closely matches simulation data. Moreover, in the HCS scenario, we show that the neural response function becomes symmetric and can be well approximated by a logistic function, thereby providing the correct dynamics in order to perform neural sampling. We hereby provide not only a normative framework for Bayesian inference in cortex, but also powerful applications of low-power, accelerated neuromorphic systems to relevant machine learning tasks. version:1
arxiv-1601-00901 | Joint learning of ontology and semantic parser from text | http://arxiv.org/abs/1601.00901 | id:1601.00901 author:Janez Starc, Dunja Mladenić category:cs.AI cs.CL  published:2016-01-05 summary:Semantic parsing methods are used for capturing and representing semantic meaning of text. Meaning representation capturing all the concepts in the text may not always be available or may not be sufficiently complete. Ontologies provide a structured and reasoning-capable way to model the content of a collection of texts. In this work, we present a novel approach to joint learning of ontology and semantic parser from text. The method is based on semi-automatic induction of a context-free grammar from semantically annotated text. The grammar parses the text into semantic trees. Both, the grammar and the semantic trees are used to learn the ontology on several levels -- classes, instances, taxonomic and non-taxonomic relations. The approach was evaluated on the first sentences of Wikipedia pages describing people. version:1
arxiv-1601-00893 | The Role of Context Types and Dimensionality in Learning Word Embeddings | http://arxiv.org/abs/1601.00893 | id:1601.00893 author:Oren Melamud, David McClosky, Siddharth Patwardhan, Mohit Bansal category:cs.CL  published:2016-01-05 summary:We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words. version:1
arxiv-1507-03857 | MMSE of probabilistic low-rank matrix estimation: Universality with respect to the output channel | http://arxiv.org/abs/1507.03857 | id:1507.03857 author:Thibault Lesieur, Florent Krzakala, Lenka Zdeborová category:cs.IT cond-mat.stat-mech math.IT stat.ML  published:2015-07-14 summary:This paper considers probabilistic estimation of a low-rank matrix from non-linear element-wise measurements of its elements. We derive the corresponding approximate message passing (AMP) algorithm and its state evolution. Relying on non-rigorous but standard assumptions motivated by statistical physics, we characterize the minimum mean squared error (MMSE) achievable information theoretically and with the AMP algorithm. Unlike in related problems of linear estimation, in the present setting the MMSE depends on the output channel only trough a single parameter - its Fisher information. We illustrate this striking finding by analysis of submatrix localization, and of detection of communities hidden in a dense stochastic block model. For this example we locate the computational and statistical boundaries that are not equal for rank larger than four. version:2
arxiv-1511-08724 | On the convergence of cycle detection for navigational reinforcement learning | http://arxiv.org/abs/1511.08724 | id:1511.08724 author:Tom J. Ameloot, Jan Van den Bussche category:cs.LG cs.AI  published:2015-11-27 summary:We consider a reinforcement learning framework where agents have to navigate from start states to goal states. We prove convergence of a cycle-detection learning algorithm on a class of tasks that we call reducible. Reducible tasks have an acyclic solution. We also syntactically characterize the form of the final policy. This characterization can be used to precisely detect the convergence point in a simulation. Our result demonstrates that even simple algorithms can be successful in learning a large class of nontrivial tasks. In addition, our framework is elementary in the sense that we only use basic concepts to formally prove convergence. version:2
arxiv-1601-00825 | Gamifying Video Object Segmentation | http://arxiv.org/abs/1601.00825 | id:1601.00825 author:Simone Palazzo, Concetto Spampinato, Daniela Giordano category:cs.CV  published:2016-01-05 summary:Video object segmentation can be considered as one of the most challenging computer vision problems. Indeed, so far, no existing solution is able to effectively deal with the peculiarities of real-world videos, especially in cases of articulated motion and object occlusions; limitations that appear more evident when we compare their performance with the human one. However, manually segmenting objects in videos is largely impractical as it requires a lot of human time and concentration. To address this problem, in this paper we propose an interactive video object segmentation method, which exploits, on one hand, the capability of humans to identify correctly objects in visual scenes, and on the other hand, the collective human brainpower to solve challenging tasks. In particular, our method relies on a web game to collect human inputs on object locations, followed by an accurate segmentation phase achieved by optimizing an energy function encoding spatial and temporal constraints between object regions as well as human-provided input. Performance analysis carried out on challenging video datasets with some users playing the game demonstrated that our method shows a better trade-off between annotation times and segmentation accuracy than interactive video annotation and automated video object segmentation approaches. version:1
arxiv-1505-01429 | Geometry-Aware Neighborhood Search for Learning Local Models for Image Reconstruction | http://arxiv.org/abs/1505.01429 | id:1505.01429 author:Julio Cesar Ferreira, Elif Vural, Christine Guillemot category:cs.CV cs.IT math.IT math.OC  published:2015-05-06 summary:Local learning of sparse image models has proven to be very effective to solve inverse problems in many computer vision applications. To learn such models, the data samples are often clustered using the K-means algorithm with the Euclidean distance as a dissimilarity metric. However, the Euclidean distance may not always be a good dissimilarity measure for comparing data samples lying on a manifold. In this paper, we propose two algorithms for determining a local subset of training samples from which a good local model can be computed for reconstructing a given input test sample, where we take into account the underlying geometry of the data. The first algorithm, called Adaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive scheme which can be seen as an out-of-sample extension of the replicator graph clustering method for local model learning. The second method, called Geometry-driven Overlapping Clusters (GOC), is a less complex nonadaptive alternative for training subset selection. The proposed AGNN and GOC methods are evaluated in image super-resolution, deblurring and denoising applications and shown to outperform spectral clustering, soft clustering, and geodesic distance based subset selection in most settings. version:3
arxiv-1601-00816 | Open challenges in understanding development and evolution of speech forms: The roles of embodied self-organization, motivation and active exploration | http://arxiv.org/abs/1601.00816 | id:1601.00816 author:Pierre-Yves Oudeyer category:cs.AI cs.CL cs.CY cs.LG  published:2016-01-05 summary:This article discusses open scientific challenges for understanding development and evolution of speech forms, as a commentary to Moulin-Frier et al. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models of the origins of speech forms, with a focus on their assumptions , we study the fundamental question of how speech can be formed out of non--speech, at both developmental and evolutionary scales. In particular, we emphasize the importance of embodied self-organization , as well as the role of mechanisms of motivation and active curiosity-driven exploration in speech formation. Finally , we discuss an evolutionary-developmental perspective of the origins of speech. version:1
arxiv-1512-05665 | Probabilistic Programming with Gaussian Process Memoization | http://arxiv.org/abs/1512.05665 | id:1512.05665 author:Ulrich Schaechtle, Ben Zinberg, Alexey Radul, Kostas Stathis, Vikash K. Mansinghka category:cs.LG cs.AI stat.ML  published:2015-12-17 summary:Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each. version:2
arxiv-1601-00781 | Robust Method of Vote Aggregation and Proposition Verification for Invariant Local Features | http://arxiv.org/abs/1601.00781 | id:1601.00781 author:Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas category:cs.CV  published:2016-01-05 summary:This paper presents a method for analysis of the vote space created from the local features extraction process in a multi-detection system. The method is opposed to the classic clustering approach and gives a high level of control over the clusters composition for further verification steps. Proposed method comprises of the graphical vote space presentation, the proposition generation, the two-pass iterative vote aggregation and the cascade filters for verification of the propositions. Cascade filters contain all of the minor algorithms needed for effective object detection verification. The new approach does not have the drawbacks of the classic clustering approaches and gives a substantial control over process of detection. Method exhibits an exceptionally high detection rate in conjunction with a low false detection chance in comparison to alternative methods. version:1
arxiv-1509-03557 | Estimating Absolute-Phase Maps Using ESPIRiT and Virtual Conjugate Coils | http://arxiv.org/abs/1509.03557 | id:1509.03557 author:Martin Uecker, Michael Lustig category:cs.CV cs.CE physics.med-ph  published:2015-07-17 summary:Purpose: To develop an ESPIRiT-based method to estimate coil sensitivities with image phase as a building block for efficient and robust image reconstruction with phase constraints. Theory and Methods: ESPIRiT is a new framework for calibration of the coil sensitivities and reconstruction in parallel Magnetic Resonance Imaging (MRI). Applying ESPIRiT to a combined set of physical and virtual conjugate coils (VCC-ESPIRiT) implicitly exploits conjugate symmetry in k-space similar to VCC-GRAPPA. Based on this method, a new post-processing step is proposed for the explicit computation of coil sensitivities that include the absolute phase of the image. The accuracy of the computed maps is directly validated using a test based on projection onto fully sampled coil images and also indirectly in phase-constrained parallel-imaging reconstructions. Results: The proposed method can estimate accurate sensitivities which include low-resolution image phase. In case of high-frequency phase variations VCC-ESPIRiT yields an additional set of maps that indicates the existence of a high-frequency phase component. Taking this additional set of maps into account can improve the robustness of phase-constrained parallel imaging. Conclusion: The extended VCC-ESPIRiT is a useful tool for phase-constrained imaging. version:2
arxiv-1401-6013 | Efficient Background Modeling Based on Sparse Representation and Outlier Iterative Removal | http://arxiv.org/abs/1401.6013 | id:1401.6013 author:Linhao Li, Ping Wang, Qinghua Hu, Sijia Cai category:cs.CV  published:2014-01-23 summary:Background modeling is a critical component for various vision-based applications. Most traditional methods tend to be inefficient when solving large-scale problems. In this paper, we introduce sparse representation into the task of large scale stable background modeling, and reduce the video size by exploring its 'discriminative' frames. A cyclic iteration process is then proposed to extract the background from the discriminative frame set. The two parts combine to form our Sparse Outlier Iterative Removal (SOIR) algorithm. The algorithm operates in tensor space to obey the natural data structure of videos. Experimental results show that a few discriminative frames determine the performance of the background extraction. Further, SOIR can achieve high accuracy and high speed simultaneously when dealing with real video sequences. Thus, SOIR has an advantage in solving large-scale tasks. version:2
arxiv-1507-04793 | Sharp Time--Data Tradeoffs for Linear Inverse Problems | http://arxiv.org/abs/1507.04793 | id:1507.04793 author:Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi category:cs.IT cs.LG math.IT math.OC math.ST stat.TH  published:2015-07-16 summary:In this paper we characterize sharp time-data tradeoffs for optimization problems used for solving linear inverse problems. We focus on the minimization of a least-squares objective subject to a constraint defined as the sub-level set of a penalty function. We present a unified convergence analysis of the gradient projection algorithm applied to such problems. We sharply characterize the convergence rate associated with a wide variety of random measurement ensembles in terms of the number of measurements and structural complexity of the signal with respect to the chosen penalty function. The results apply to both convex and nonconvex constraints, demonstrating that a linear convergence rate is attainable even though the least squares objective is not strongly convex in these settings. When specialized to Gaussian measurements our results show that such linear convergence occurs when the number of measurements is merely 4 times the minimal number required to recover the desired signal at all (a.k.a. the phase transition). We also achieve a slower but geometric rate of convergence precisely above the phase transition point. Extensive numerical results suggest that the derived rates exactly match the empirical performance. version:2
arxiv-1601-00741 | Learning Preferences for Manipulation Tasks from Online Coactive Feedback | http://arxiv.org/abs/1601.00741 | id:1601.00741 author:Ashesh Jain, Shikhar Sharma, Thorsten Joachims, Ashutosh Saxena category:cs.RO cs.AI cs.LG  published:2016-01-05 summary:We consider the problem of learning preferences over trajectories for mobile manipulators such as personal robots and assembly line robots. The preferences we learn are more intricate than simple geometric constraints on trajectories; they are rather governed by the surrounding context of various objects and human interactions in the environment. We propose a coactive online learning framework for teaching preferences in contextually rich environments. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this coactive preference feedback can be more easily elicited than demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We implement our algorithm on two high degree-of-freedom robots, PR2 and Baxter, and present three intuitive mechanisms for providing such incremental feedback. In our experimental evaluation we consider two context rich settings -- household chores and grocery store checkout -- and show that users are able to train the robot with just a few feedbacks (taking only a few minutes).\footnote{Parts of this work has been published at NIPS and ISRR conferences~\citep{Jain13,Jain13b}. This journal submission presents a consistent full paper, and also includes the proof of regret bounds, more details of the robotic system, and a thorough related work.} version:1
arxiv-1406-2616 | PlanIt: A Crowdsourcing Approach for Learning to Plan Paths from Large Scale Preference Feedback | http://arxiv.org/abs/1406.2616 | id:1406.2616 author:Ashesh Jain, Debarghya Das, Jayesh K Gupta, Ashutosh Saxena category:cs.RO cs.AI cs.LG  published:2014-06-10 summary:We consider the problem of learning user preferences over robot trajectories for environments rich in objects and humans. This is challenging because the criterion defining a good trajectory varies with users, tasks and interactions in the environment. We represent trajectory preferences using a cost function that the robot learns and uses it to generate good trajectories in new environments. We design a crowdsourcing system - PlanIt, where non-expert users label segments of the robot's trajectory. PlanIt allows us to collect a large amount of user feedback, and using the weak and noisy labels from PlanIt we learn the parameters of our model. We test our approach on 122 different environments for robotic navigation and manipulation tasks. Our extensive experiments show that the learned cost function generates preferred trajectories in human environments. Our crowdsourcing system is publicly available for the visualization of the learned costs and for providing preference feedback: \url{http://planit.cs.cornell.edu} version:3
arxiv-1601-00740 | Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture | http://arxiv.org/abs/1601.00740 | id:1601.00740 author:Ashesh Jain, Hema S Koppula, Shane Soh, Bharad Raghavan, Avi Singh, Ashutosh Saxena category:cs.RO cs.CV cs.LG  published:2016-01-05 summary:Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. In this work we propose a vehicular sensor-rich platform and learning algorithms for maneuver anticipation. For this purpose we equip a car with cameras, Global Positioning System (GPS), and a computing device to capture the driving context from both inside and outside of the car. In order to anticipate maneuvers, we propose a sensory-fusion deep learning architecture which jointly learns to anticipate and fuse multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We propose a novel training procedure which allows the network to predict the future given only a partial temporal context. We introduce a diverse data set with 1180 miles of natural freeway and city driving, and show that we can anticipate maneuvers 3.5 seconds before they occur in real-time with a precision and recall of 90.5\% and 87.4\% respectively. version:1
arxiv-1601-00722 | Matrix Variate RBM and Its Applications | http://arxiv.org/abs/1601.00722 | id:1601.00722 author:Guanglei Qi, Yanfeng Sun, Junbin Gao, Yongli Hu, Jinghua Li category:cs.CV  published:2016-01-05 summary:Restricted Boltzmann Machine (RBM) is an importan- t generative model modeling vectorial data. While applying an RBM in practice to images, the data have to be vec- torized. This results in high-dimensional data and valu- able spatial information has got lost in vectorization. In this paper, a Matrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed by generalizing the classic RBM to explicitly model matrix data. In the new RBM model, both input and hidden variables are in matrix forms which are connected by bilinear transforms. The MVRBM has much less model parameters, resulting in a faster train- ing algorithm while retaining comparable performance as the classic RBM. The advantages of the MVRBM have been demonstrated on two real-world applications: Image super- resolution and handwritten digit recognition. version:1
arxiv-1512-08301 | Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency | http://arxiv.org/abs/1512.08301 | id:1512.08301 author:Shiliang Zhang, Cong Liu, Hui Jiang, Si Wei, Lirong Dai, Yu Hu category:cs.NE  published:2015-12-28 summary:In this paper, we propose a novel neural network structure, namely \emph{feedforward sequential memory networks (FSMN)}, to model long-term dependency in time series without using recurrent feedback. The proposed FSMN is a standard fully-connected feedforward neural network equipped with some learnable memory blocks in its hidden layers. The memory blocks use a tapped-delay line structure to encode the long context information into a fixed-size representation as short-term memory mechanism. We have evaluated the proposed FSMNs in several standard benchmark tasks, including speech recognition and language modelling. Experimental results have shown FSMNs significantly outperform the conventional recurrent neural networks (RNN), including LSTMs, in modeling sequential signals like speech or language. Moreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs due to the inherent non-recurrent model structure. version:2
arxiv-1601-00022 | Event Specific Multimodal Pattern Mining with Image-Caption Pairs | http://arxiv.org/abs/1601.00022 | id:1601.00022 author:Hongzhi Li, Joseph G. Ellis, Shih-Fu Chang category:cs.CV  published:2015-12-31 summary:In this paper we describe a novel framework and algorithms for discovering image patch patterns from a large corpus of weakly supervised image-caption pairs generated from news events. Current pattern mining techniques attempt to find patterns that are representative and discriminative, we stipulate that our discovered patterns must also be recognizable by humans and preferably with meaningful names. We propose a new multimodal pattern mining approach that leverages the descriptive captions often accompanying news images to learn semantically meaningful image patch patterns. The mutltimodal patterns are then named using words mined from the associated image captions for each pattern. A novel evaluation framework is provided that demonstrates our patterns are 26.2% more semantically meaningful than those discovered by the state of the art vision only pipeline, and that we can provide tags for the discovered images patches with 54.5% accuracy with no direct supervision. Our methods also discover named patterns beyond those covered by the existing image datasets like ImageNet. To the best of our knowledge this is the first algorithm developed to automatically mine image patch patterns that have strong semantic meaning specific to high-level news events, and then evaluate these patterns based on that criteria. version:2
arxiv-1601-00087 | Sentiment/Subjectivity Analysis Survey for Languages other than English | http://arxiv.org/abs/1601.00087 | id:1601.00087 author:Mohammed Korayem category:cs.CL  published:2016-01-01 summary:Subjective and sentiment analysis has gained considerable attention recently. Most of the resources and systems built so far are done for English. The need for designing systems for other languages is increasing. This paper surveys different ways used for building systems for subjective and sentiment analysis for languages other than English. There are three different types of systems used for building these systems. The first (and the best) one is the language specific systems. The second type of systems involves reusing or transferring sentiment resources from English to the target language. The third type of methods is based on using language independent methods. The paper presents a separate section devoted to Arabic sentiment analysis. version:2
arxiv-1601-00710 | Multi-Source Neural Translation | http://arxiv.org/abs/1601.00710 | id:1601.00710 author:Barret Zoph, Kevin Knight category:cs.CL  published:2016-01-05 summary:We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model. version:1
arxiv-1601-00706 | Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis | http://arxiv.org/abs/1601.00706 | id:1601.00706 author:Jimei Yang, Scott Reed, Ming-Hsuan Yang, Honglak Lee category:cs.LG cs.AI cs.CV  published:2016-01-05 summary:An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision. version:1
arxiv-1601-00701 | Nonlinear Hebbian learning as a unifying principle in receptive field formation | http://arxiv.org/abs/1601.00701 | id:1601.00701 author:Carlos S. N. Brito, Wulfram Gerstner category:q-bio.NC cs.LG  published:2016-01-04 summary:The development of sensory receptive fields has been modeled in the past by a variety of models including normative models such as sparse coding or independent component analysis and bottom-up models such as spike-timing dependent plasticity or the Bienenstock-Cooper-Munro model of synaptic plasticity. Here we show that the above variety of approaches can all be unified into a single common principle, namely Nonlinear Hebbian Learning. When Nonlinear Hebbian Learning is applied to natural images, receptive field shapes were strongly constrained by the input statistics and preprocessing, but exhibited only modest variation across different choices of nonlinearities in neuron models or synaptic plasticity rules. Neither overcompleteness nor sparse network activity are necessary for the development of localized receptive fields. The analysis of alternative sensory modalities such as auditory models or V2 development lead to the same conclusions. In all examples, receptive fields can be predicted a priori by reformulating an abstract model as nonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural statistics can account for many aspects of receptive field formation across models and sensory modalities. version:1
arxiv-1601-00626 | Scalable Models for Computing Hierarchies in Information Networks | http://arxiv.org/abs/1601.00626 | id:1601.00626 author:Baoxu Shi, Tim Weninger category:cs.AI cs.DL cs.LG  published:2016-01-04 summary:Information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation. Fortunately, many statistical and computational models exist that automatically generate hierarchies; however, the existing approaches do not consider linkages in information {\em networks} that are increasingly common in real-world scenarios. Current approaches also tend to present topics as an abstract probably distribution over words, etc rather than as tangible nodes from the original network. Furthermore, the statistical techniques present in many previous works are not yet capable of processing data at Web-scale. In this paper we present the Hierarchical Document Topic Model (HDTM), which uses a distributed vertex-programming process to calculate a nonparametric Bayesian generative model. Experiments on three medium size data sets and the entire Wikipedia dataset show that HDTM can infer accurate hierarchies even over large information networks. version:1
arxiv-1601-00620 | Distant IE by Bootstrapping Using Lists and Document Structure | http://arxiv.org/abs/1601.00620 | id:1601.00620 author:Lidong Bing, Mingyang Ling, Richard C. Wang, William W. Cohen category:cs.CL  published:2016-01-04 summary:Distant labeling for information extraction (IE) suffers from noisy training data. We describe a way of reducing the noise associated with distant IE by identifying coupling constraints between potential instance labels. As one example of coupling, items in a list are likely to have the same label. A second example of coupling comes from analysis of document structure: in some corpora, sections can be identified such that items in the same section are likely to have the same label. Such sections do not exist in all corpora, but we show that augmenting a large corpus with coupling constraints from even a small, well-structured corpus can improve performance substantially, doubling F1 on one task. version:1
arxiv-1511-04906 | Performing Highly Accurate Predictions Through Convolutional Networks for Actual Telecommunication Challenges | http://arxiv.org/abs/1511.04906 | id:1511.04906 author:Jaime Zaratiegui, Ana Montoro, Federico Castanedo category:cs.LG cs.CV  published:2015-11-16 summary:We investigated how the application of deep learning, specifically the use of convolutional networks trained with GPUs, can help to build better predictive models in telecommunication business environments, and fill this gap. In particular, we focus on the non-trivial problem of predicting customer churn in telecommunication operators. Our model, called WiseNet, consists of a convolutional network and a novel encoding method that transforms customer activity data and Call Detail Records (CDRs) into images. Experimental evaluation with several machine learning classifiers supports the ability of WiseNet for learning features when using structured input data. For this type of telecommunication business problems, we found that WiseNet outperforms machine learning models with hand-crafted features, and does not require the labor-intensive step of feature engineering. Furthermore, the same model has been applied without retraining to a different market, achieving consistent results. This confirms the generalization property of WiseNet and the ability to extract useful representations. version:2
arxiv-1601-00599 | Multimodal Classification of Events in Social Media | http://arxiv.org/abs/1601.00599 | id:1601.00599 author:Matthias Zeppelzauer, Daniel Schopfhauser category:cs.CV cs.IR cs.MM  published:2016-01-04 summary:A large amount of social media hosted on platforms like Flickr and Instagram is related to social events. The task of social event classification refers to the distinction of event and non-event-related content as well as the classification of event types (e.g. sports events, concerts, etc.). In this paper, we provide an extensive study of textual, visual, as well as multimodal representations for social event classification. We investigate strengths and weaknesses of the modalities and study synergy effects between the modalities. Experimental results obtained with our multimodal representation outperform state-of-the-art methods and provide a new baseline for future research. version:1
arxiv-1512-07450 | Interacting Behavior and Emerging Complexity | http://arxiv.org/abs/1512.07450 | id:1512.07450 author:Alyssa Adams, Hector Zenil, Eduardo Hermo Reyes, Joost Joosten category:cs.NE cs.CC nlin.CG q-bio.PE  published:2015-12-23 summary:Can we quantify the change of complexity throughout evolutionary processes? We attempt to address this question through an empirical approach. In very general terms, we simulate two simple organisms on a computer that compete over limited available resources. We implement Global Rules that determine the interaction between two Elementary Cellular Automata on the same grid. Global Rules change the complexity of the state evolution output which suggests that some complexity is intrinsic to the interaction rules themselves. The largest increases in complexity occurred when the interacting elementary rules had very little complexity, suggesting that they are able to accept complexity through interaction only. We also found that some Class 3 or 4 CA rules are more fragile than others to Global Rules, while others are more robust, hence suggesting some intrinsic properties of the rules independent of the Global Rule choice. We provide statistical mappings of Elementary Cellular Automata exposed to Global Rules and different initial conditions onto different complexity classes. version:3
arxiv-1601-00595 | Robust non-linear regression analysis: A greedy approach employing kernels and application to image denoising | http://arxiv.org/abs/1601.00595 | id:1601.00595 author:George Papageorgiou, Pantelis Bouboulis, Sergios Theodoridis category:cs.LG stat.ML  published:2016-01-04 summary:We consider the task of robust non-linear estimation in the presence of both bounded noise and outliers. Assuming that the unknown non-linear function belongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to accurately estimate the coefficients of the kernel regression matrix. Due to the existence of outliers, common techniques such as the Kernel Ridge Regression (KRR), or the Support Vector Regression (SVR) turn out to be inadequate. Instead, we employ sparse modeling arguments to model and estimate the outliers, adopting a greedy approach. In particular, the proposed robust scheme, i.e., Kernel Greedy Algorithm for Robust Denoising (KGARD), is a modification of the classical Orthogonal Matching Pursuit (OMP) algorithm. In a nutshell, the proposed scheme alternates between a KRR task and an OMP-like selection step. Convergence properties as well as theoretical results concerning the identification of the outliers are provided. Moreover, KGARD is compared against other cutting edge methods (using toy examples) to demonstrate its performance and verify the aforementioned theoretical results. Finally, the proposed robust estimation framework is applied to the task of image denoising, showing that it can enhance the denoising process significantly, when outliers are present. version:1
arxiv-1601-00574 | NFL Play Prediction | http://arxiv.org/abs/1601.00574 | id:1601.00574 author:Brendan Teich, Roman Lutz, Valentin Kassarnig category:cs.LG  published:2016-01-04 summary:Based on NFL game data we try to predict the outcome of a play in multiple different ways. An application of this is the following: by plugging in various play options one could determine the best play for a given situation in real time. While the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call "progress". We see this work as a first step to include predictive analysis into NFL playcalling. version:1
arxiv-1511-03163 | Semi-supervised Tuning from Temporal Coherence | http://arxiv.org/abs/1511.03163 | id:1511.03163 author:Davide Maltoni, Vincenzo Lomonaco category:cs.LG stat.ML  published:2015-11-10 summary:Recent works demonstrated the usefulness of temporal coherence to regularize supervised training or to learn invariant features with deep architectures. In particular, enforcing smooth output changes while presenting temporally-closed frames from video sequences, proved to be an effective strategy. In this paper we prove the efficacy of temporal coherence for semi-supervised incremental tuning. We show that a deep architecture, just mildly trained in a supervised manner, can progressively improve its classification accuracy, if exposed to video sequences of unlabeled data. The extent to which, in some cases, a semi-supervised tuning allows to improve classification accuracy (approaching the supervised one) is somewhat surprising. A number of control experiments pointed out the fundamental role of temporal coherence. version:3
arxiv-1601-00543 | Approximate Message Passing with Nearest Neighbor Sparsity Pattern Learning | http://arxiv.org/abs/1601.00543 | id:1601.00543 author:Xiangming Meng, Sheng Wu, Linling Kuang, Defeng, Huang, Jianhua Lu category:cs.IT cs.LG math.IT  published:2016-01-04 summary:We consider the problem of recovering clustered sparse signals with no prior knowledge of the sparsity pattern. Beyond simple sparsity, signals of interest often exhibits an underlying sparsity pattern which, if leveraged, can improve the reconstruction performance. However, the sparsity pattern is usually unknown a priori. Inspired by the idea of k-nearest neighbor (k-NN) algorithm, we propose an efficient algorithm termed approximate message passing with nearest neighbor sparsity pattern learning (AMP-NNSPL), which learns the sparsity pattern adaptively. AMP-NNSPL specifies a flexible spike and slab prior on the unknown signal and, after each AMP iteration, sets the sparse ratios as the average of the nearest neighbor estimates via expectation maximization (EM). Experimental results on both synthetic and real data demonstrate the superiority of our proposed algorithm both in terms of reconstruction performance and computational complexity. version:1
arxiv-1511-06406 | Denoising Criterion for Variational Auto-Encoding Framework | http://arxiv.org/abs/1511.06406 | id:1511.06406 author:Daniel Jiwoong Im, Sungjin Ahn, Roland Memisevic, Yoshua Bengio category:cs.LG  published:2015-11-19 summary:Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tractable bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets. version:2
arxiv-1505-05007 | Modelling-based experiment retrieval: A case study with gene expression clustering | http://arxiv.org/abs/1505.05007 | id:1505.05007 author:Paul Blomstedt, Ritabrata Dutta, Sohan Seth, Alvis Brazma, Samuel Kaski category:stat.ML cs.IR cs.LG  published:2015-05-19 summary:Motivation: Public and private repositories of experimental data are growing to sizes that require dedicated methods for finding relevant data. To improve on the state of the art of keyword searches from annotations, methods for content-based retrieval have been proposed. In the context of gene expression experiments, most methods retrieve gene expression profiles, requiring each experiment to be expressed as a single profile, typically of case vs. control. A more general, recently suggested alternative is to retrieve experiments whose models are good for modelling the query dataset. However, for very noisy and high-dimensional query data, this retrieval criterion turns out to be very noisy as well. Results: We propose doing retrieval using a denoised model of the query dataset, instead of the original noisy dataset itself. To this end, we introduce a general probabilistic framework, where each experiment is modelled separately and the retrieval is done by finding related models. For retrieval of gene expression experiments, we use a probabilistic model called product partition model, which induces a clustering of genes that show similar expression patterns across a number of samples. The suggested metric for retrieval using clusterings is the normalized information distance. Empirical results finally suggest that inference for the full probabilistic model can be approximated with good performance using computationally faster heuristic clustering approaches (e.g. $k$-means). The method is highly scalable and straightforward to apply to construct a general-purpose gene expression experiment retrieval method. Availability: The method can be implemented using standard clustering algorithms and normalized information distance, available in many statistical software packages. version:4
arxiv-1601-00504 | Learning relationships between data obtained independently | http://arxiv.org/abs/1601.00504 | id:1601.00504 author:Alexandra Carpentier, Teresa Schlueter category:stat.ML  published:2016-01-04 summary:The aim of this paper is to provide a new method for learning the relationships between data that have been obtained independently. Unlike existing methods like matching, the proposed technique does not require any contextual information, provided that the dependency between the variables of interest is monotone. It can therefore be easily combined with matching in order to exploit the advantages of both methods. This technique can be described as a mix between quantile matching, and deconvolution. We provide for it a theoretical and an empirical validation. version:1
arxiv-1601-00449 | Fitting Spectral Decay with the $k$-Support Norm | http://arxiv.org/abs/1601.00449 | id:1601.00449 author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:cs.LG stat.ML  published:2016-01-04 summary:The spectral $k$-support norm enjoys good estimation properties in low rank matrix learning problems, empirically outperforming the trace norm. Its unit ball is the convex hull of rank $k$ matrices with unit Frobenius norm. In this paper we generalize the norm to the spectral $(k,p)$-support norm, whose additional parameter $p$ can be used to tailor the norm to the decay of the spectrum of the underlying model. We characterize the unit ball and we explicitly compute the norm. We further provide a conditional gradient method to solve regularization problems with the norm, and we derive an efficient algorithm to compute the Euclidean projection on the unit ball in the case $p=\infty$. In numerical experiments, we show that allowing $p$ to vary significantly improves performance over the spectral $k$-support norm on various matrix completion benchmarks, and better captures the spectral decay of the underlying model. version:1
arxiv-1601-00414 | Kernel Sparse Subspace Clustering on Symmetric Positive Definite Manifolds | http://arxiv.org/abs/1601.00414 | id:1601.00414 author:Ming Yin, Yi Guo, Junbin Gao, Zhaoshui He, Shengli Xie category:cs.CV  published:2016-01-04 summary:Sparse subspace clustering (SSC), as one of the most successful subspace clustering methods, has achieved notable clustering accuracy in computer vision tasks. However, SSC applies only to vector data in Euclidean space. As such, there is still no satisfactory approach to solve subspace clustering by ${\it self-expressive}$ principle for symmetric positive definite (SPD) matrices which is very useful in computer vision. In this paper, by embedding the SPD matrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspace clustering method is constructed on the SPD manifold through an appropriate Log-Euclidean kernel, termed as kernel sparse subspace clustering on the SPD Riemannian manifold (KSSCR). By exploiting the intrinsic Riemannian geometry within data, KSSCR can effectively characterize the geodesic distance between SPD matrices to uncover the underlying subspace structure. Experimental results on two famous database demonstrate that the proposed method achieves better clustering results than the state-of-the-art approaches. version:1
arxiv-1509-09002 | Convergence of Stochastic Gradient Descent for PCA | http://arxiv.org/abs/1509.09002 | id:1509.09002 author:Ohad Shamir category:cs.LG math.OC stat.ML  published:2015-09-30 summary:We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in $\reals^d$. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this paper, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in \cite{hardt2014noisy}. Moreover, under an eigengap assumption, we show that the same techniques lead to new SGD convergence guarantees with better dependence on the eigengap. version:2
arxiv-1601-00400 | Multi-task CNN Model for Attribute Prediction | http://arxiv.org/abs/1601.00400 | id:1601.00400 author:Abrar H. Abdulnabi, Gang Wang, Jiwen Lu, Kui Jia category:cs.CV  published:2016-01-04 summary:This paper proposes a joint multi-task learning algorithm to better predict attributes in images using deep convolutional neural networks (CNN). We consider learning binary semantic attributes through a multi-task CNN model, where each CNN will predict one binary attribute. The multi-task learning allows CNN models to simultaneously share visual knowledge among different attribute categories. Each CNN will generate attribute-specific feature representations, and then we apply multi-task learning on the features to predict their attributes. In our multi-task framework, we propose a method to decompose the overall model's parameters into a latent task matrix and combination matrix. Furthermore, under-sampled classifiers can leverage shared statistics from other classifiers to improve their performance. Natural grouping of attributes is applied such that attributes in the same group are encouraged to share more knowledge. Meanwhile, attributes in different groups will generally compete with each other, and consequently share less knowledge. We show the effectiveness of our method on two popular attribute datasets. version:1
arxiv-1601-00396 | Automatic Detection and Decoding of Photogrammetric Coded Targets | http://arxiv.org/abs/1601.00396 | id:1601.00396 author:Udaya Wijenayake, Sung-In Choi, Soon-Yong Park category:cs.CV  published:2016-01-04 summary:Close-range Photogrammetry is widely used in many industries because of the cost effectiveness and efficiency of the technique. In this research, we introduce an automated coded target detection method which can be used to enhance the efficiency of the Photogrammetry. version:1
arxiv-1601-00393 | On the Reducibility of Submodular Functions | http://arxiv.org/abs/1601.00393 | id:1601.00393 author:Jincheng Mei, Hao Zhang, Bao-Liang Lu category:cs.LG stat.ML  published:2016-01-04 summary:The scalability of submodular optimization methods is critical for their usability in practice. In this paper, we study the reducibility of submodular functions, a property that enables us to reduce the solution space of submodular optimization problems without performance loss. We introduce the concept of reducibility using marginal gains. Then we show that by adding perturbation, we can endow irreducible functions with reducibility, based on which we propose the perturbation-reduction optimization framework. Our theoretical analysis proves that given the perturbation scales, the reducibility gain could be computed, and the performance loss has additive upper bounds. We further conduct empirical studies and the results demonstrate that our proposed framework significantly accelerates existing optimization methods for irreducible submodular functions with a cost of only small performance losses. version:1
arxiv-1502-04681 | Unsupervised Learning of Video Representations using LSTMs | http://arxiv.org/abs/1502.04681 | id:1502.04681 author:Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov category:cs.LG cs.CV cs.NE  published:2015-02-16 summary:We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations ("percepts") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance. version:3
arxiv-1601-00350 | Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in Wireless Sensor Networks | http://arxiv.org/abs/1601.00350 | id:1601.00350 author:Hadi Zayyani, Mehdi Korki, Farrokh Marvasti category:stat.ML cs.IT cs.LG math.IT  published:2016-01-03 summary:This letter proposes a sparse diffusion steepest-descent algorithm for one bit compressed sensing in wireless sensor networks. The approach exploits the diffusion strategy from distributed learning in the one bit compressed sensing framework. To estimate a common sparse vector cooperatively from only the sign of measurements, steepest-descent is used to minimize the suitable global and local convex cost functions. A diffusion strategy is suggested for distributive learning of the sparse vector. Simulation results show the effectiveness of the proposed distributed algorithm compared to the state-of-the-art non distributive algorithms in the one bit compressed sensing framework. version:1
arxiv-1512-06612 | Backward and Forward Language Modeling for Constrained Sentence Generation | http://arxiv.org/abs/1512.06612 | id:1512.06612 author:Lili Mou, Rui Yan, Ge Li, Lu Zhang, Zhi Jin category:cs.CL cs.LG cs.NE  published:2015-12-21 summary:Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN's hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a specific word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality. version:2
arxiv-1511-02954 | Reducing the Training Time of Neural Networks by Partitioning | http://arxiv.org/abs/1511.02954 | id:1511.02954 author:Conrado S. Miranda, Fernando J. Von Zuben category:cs.NE cs.LG  published:2015-11-10 summary:This paper presents a new method for pre-training neural networks that can decrease the total training time for a neural network while maintaining the final performance, which motivates its use on deep neural networks. By partitioning the training task in multiple training subtasks with sub-models, which can be performed independently and in parallel, it is shown that the size of the sub-models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub-models used for the pre-training. The sub-models are then merged to provide a pre-trained initial set of weights for the original model. The proposed method is independent of the other aspects of the training, such as architecture of the neural network, training method, and objective, making it compatible with a wide range of existing approaches. The speedup without loss of performance is validated experimentally on MNIST and on CIFAR10 data sets, also showing that even performing the subtasks sequentially can decrease the training time. Moreover, we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems. version:2
arxiv-1601-00260 | Image Resolution Enhancement by Using Interpolation Followed by Iterative Back Projection | http://arxiv.org/abs/1601.00260 | id:1601.00260 author:Pejman Rasti, Hasan Demirel, Gholamreza Anbarjafari category:cs.CV  published:2016-01-03 summary:In this paper, we propose a new super resolution technique based on the interpolation followed by registering them using iterative back projection (IBP). Low resolution images are being interpolated and then the interpolated images are being registered in order to generate a sharper high resolution image. The proposed technique has been tested on Lena, Elaine, Pepper, and Baboon. The quantitative peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) results as well as the visual results show the superiority of the proposed technique over the conventional and state-of-art image super resolution techniques. For Lena's image, the PSNR is 6.52 dB higher than the bicubic interpolation. version:1
arxiv-1601-00236 | Supervised Dimensionality Reduction via Distance Correlation Maximization | http://arxiv.org/abs/1601.00236 | id:1601.00236 author:Praneeth Vepakomma, Chetan Tonde, Ahmed Elgammal category:cs.LG stat.ML  published:2016-01-03 summary:In our work, we propose a novel formulation for supervised dimensionality reduction based on a nonlinear dependency criterion called Statistical Distance Correlation, Szekely et. al. (2007). We propose an objective which is free of distributional assumptions on regression variables and regression model assumptions. Our proposed formulation is based on learning a low-dimensional feature representation $\mathbf{z}$, which maximizes the squared sum of Distance Correlations between low dimensional features $\mathbf{z}$ and response $y$, and also between features $\mathbf{z}$ and covariates $\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective using the Generalized Minimization Maximizaiton method of \Parizi et. al. (2015). We show superior empirical results on multiple datasets proving the effectiveness of our proposed approach over several relevant state-of-the-art supervised dimensionality reduction methods. version:1
arxiv-1601-00212 | Supervised Texture Segmentation: A Comparative Study | http://arxiv.org/abs/1601.00212 | id:1601.00212 author:Omar S. Al-Kadi category:cs.CV  published:2016-01-02 summary:This paper aims to compare between four different types of feature extraction approaches in terms of texture segmentation. The feature extraction methods that were used for segmentation are Gabor filters (GF), Gaussian Markov random fields (GMRF), run-length matrix (RLM) and co-occurrence matrix (GLCM). It was shown that the GF performed best in terms of quality of segmentation while the GLCM localises the texture boundaries better as compared to the other methods. version:1
arxiv-1601-00211 | A fractal dimension based optimal wavelet packet analysis technique for classification of meningioma brain tumours | http://arxiv.org/abs/1601.00211 | id:1601.00211 author:Omar S. Al-Kadi category:cs.CV  published:2016-01-02 summary:With the heterogeneous nature of tissue texture, using a single resolution approach for optimum classification might not suffice. In contrast, a multiresolution wavelet packet analysis can decompose the input signal into a set of frequency subbands giving the opportunity to characterise the texture at the appropriate frequency channel. An adaptive best bases algorithm for optimal bases selection for meningioma histopathological images is proposed, via applying the fractal dimension (FD) as the bases selection criterion in a tree-structured manner. Thereby, the most significant subband that better identifies texture discontinuities will only be chosen for further decomposition, and its fractal signature would represent the extracted feature vector for classification. The best basis selection using the FD outperformed the energy based selection approaches, achieving an overall classification accuracy of 91.25% as compared to 83.44% and 73.75% for the co-occurrence matrix and energy texture signatures; respectively. version:1
arxiv-1601-00210 | Susceptibility of texture measures to noise: an application to lung tumor CT images | http://arxiv.org/abs/1601.00210 | id:1601.00210 author:O. S. Al-Kadi, D. Watson category:cs.CV  published:2016-01-02 summary:Five different texture methods are used to investigate their susceptibility to subtle noise occurring in lung tumor Computed Tomography (CT) images caused by acquisition and reconstruction deficiencies. Noise of Gaussian and Rayleigh distributions with varying mean and variance was encountered in the analyzed CT images. Fisher and Bhattacharyya distance measures were used to differentiate between an original extracted lung tumor region of interest (ROI) with a filtered and noisy reconstructed versions. Through examining the texture characteristics of the lung tumor areas by five different texture measures, it was determined that the autocovariance measure was least affected and the gray level co-occurrence matrix was the most affected by noise. Depending on the selected ROI size, it was concluded that the number of extracted features from each texture measure increases susceptibility to noise. version:1
arxiv-1601-00199 | A Unified Framework for Compositional Fitting of Active Appearance Models | http://arxiv.org/abs/1601.00199 | id:1601.00199 author:Joan Alabort-i-Medina, Stefanos Zafeiriou category:cs.CV  published:2016-01-02 summary:Active Appearance Models (AAMs) are one of the most popular and well-established techniques for modeling deformable objects in computer vision. In this paper, we study the problem of fitting AAMs using Compositional Gradient Descent (CGD) algorithms. We present a unified and complete view of these algorithms and classify them with respect to three main characteristics: i) cost function; ii) type of composition; and iii) optimization method. Furthermore, we extend the previous view by: a) proposing a novel Bayesian cost function that can be interpreted as a general probabilistic formulation of the well-known project-out loss; b) introducing two new types of composition, asymmetric and bidirectional, that combine the gradients of both image and appearance model to derive better conver- gent and more robust CGD algorithms; and c) providing new valuable insights into existent CGD algorithms by reinterpreting them as direct applications of the Schur complement and the Wiberg method. Finally, in order to encourage open research and facilitate future comparisons with our work, we make the implementa- tion of the algorithms studied in this paper publicly available as part of the Menpo Project. version:1
arxiv-1601-00191 | An Improved Intelligent Agent for Mining Real-Time Databases Using Modified Cortical Learning Algorithms | http://arxiv.org/abs/1601.00191 | id:1601.00191 author:N. E. Osegi category:cs.NE  published:2016-01-02 summary:Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTM have been developed by Numenta Incorporation from which variations and modifications are currently being investigated upon. HTM offers better promises as a future computational model of the neocortex the seat of intelligence in the brain. Currently, intelligent agents are embedded in almost every modern day electronic system found in homes, offices and industries worldwide. In this paper, we present a first step in realising useful HTM like applications specifically for mining a synthetic and real time dataset based on a novel intelligent agent framework, and demonstrate how a modified version of this very important computational technique will lead to improved recognition. version:1
arxiv-1308-3779 | Adaptive Independent Sticky MCMC algorithms | http://arxiv.org/abs/1308.3779 | id:1308.3779 author:L. Martino, R. Casarin, F. Leisen, D. Luengo category:stat.CO stat.ML  published:2013-08-17 summary:In this work, we introduce a novel class of adaptive Monte Carlo methods, called adaptive independent sticky MCMC algorithms, for efficient sampling from a generic target probability density function (pdf). The new class of algorithms employs adaptive non-parametric proposal densities which become closer and closer to the target as the number of iterations increases. The proposal pdf is built using interpolation procedures based on a set of support points which is constructed iteratively based on previously drawn samples. The algorithm's efficiency is ensured by a test that controls the evolution of the set of support points. This extra stage controls the computational cost and the convergence of the proposal density to the target. Each part of the novel family of algorithms is discussed and several examples are provided. Although the novel algorithms are presented for univariate target densities, we show that they can be easily extended to the multivariate context within a Gibbs-type sampler. The ergodicity is ensured and discussed. Exhaustive numerical examples illustrate the efficiency of sticky schemes, both as a stand-alone methods to sample from complicated one-dimensional pdfs and within Gibbs in order to draw from multi-dimensional target distributions. version:4
arxiv-1601-00142 | Joint Estimation of Precision Matrices in Heterogeneous Populations | http://arxiv.org/abs/1601.00142 | id:1601.00142 author:Takumi Saegusa, Ali Shojaie category:stat.ML  published:2016-01-02 summary:We introduce a general framework for estimation of inverse covariance, or precision, matrices from heterogeneous populations. The proposed framework uses a Laplacian shrinkage penalty to encourage similarity among estimates from disparate, but related, subpopulations, while allowing for differences among matrices. We propose an efficient alternating direction method of multipliers (ADMM) algorithm for parameter estimation, as well as its extension for faster computation in high dimensions by thresholding the empirical covariance matrix to identify the joint block diagonal structure in the estimated precision matrices. We establish both variable selection and norm consistency of the proposed estimator for distributions with exponential or polynomial tails. Further, to extend the applicability of the method to the settings with unknown populations structure, we propose a Laplacian penalty based on hierarchical clustering, and discuss conditions under which this data-driven choice results in consistent estimation of precision matrices in heterogenous populations. Extensive numerical studies and applications to gene expression data from subtypes of cancer with distinct clinical outcomes indicate the potential advantages of the proposed method over existing approaches. version:1
arxiv-1511-07425 | Real-Time Anomalous Behavior Detection and Localization in Crowded Scenes | http://arxiv.org/abs/1511.07425 | id:1511.07425 author:Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hosseini category:cs.CV  published:2015-11-21 summary:In this paper, we propose an accurate and real-time anomaly detection and localization in crowded scenes, and two descriptors for representing anomalous behavior in video are proposed. We consider a video as being a set of cubic patches. Based on the low likelihood of an anomaly occurrence, and the redundancy of structures in normal patches in videos, two (global and local) views are considered for modeling the video. Our algorithm has two components, for (1) representing the patches using local and global descriptors, and for (2) modeling the training patches using a new representation. We have two Gaussian models for all training patches respect to global and local descriptors. The local and global features are based on structure similarity between adjacent patches and the features that are learned in an unsupervised way. We propose a fusion strategy to combine the two descriptors as the output of our system. Experimental results show that our algorithm performs like a state-of-the-art method on several standard datasets, but even is more time-efficient. version:2
arxiv-1601-00119 | Discriminative Sparsity for Sonar ATR | http://arxiv.org/abs/1601.00119 | id:1601.00119 author:John McKay, Raghu Raj, Vishal Monga, Jason Isaacs category:cs.CV  published:2016-01-01 summary:Advancements in Sonar image capture have enabled researchers to apply sophisticated object identification algorithms in order to locate targets of interest in images such as mines. Despite progress in this field, modern sonar automatic target recognition (ATR) approaches lack robustness to the amount of noise one would expect in real-world scenarios, the capability to handle blurring incurred from the physics of image capture, and the ability to excel with relatively few training samples. We address these challenges by adapting modern sparsity-based techniques with dictionaries comprising of training from each class. We develop new discriminative (as opposed to generative) sparse representations which can help automatically classify targets in Sonar imaging. Using a simulated SAS data set from the Naval Surface Warfare Center (NSWC), we obtained compelling classification rates for multi-class problems even in cases with considerable noise and sparsity in training samples. version:1
arxiv-1601-00088 | Demystifying Symmetric Smoothing Filters | http://arxiv.org/abs/1601.00088 | id:1601.00088 author:Stanley H. Chan, Todd Zickler, Yue M. Lu category:cs.CV  published:2016-01-01 summary:Many patch-based image denoising algorithms can be formulated as applying a smoothing filter to the noisy image. Expressed as matrices, the smoothing filters must be row normalized so that each row sums to unity. Surprisingly, if we apply a column normalization before the row normalization, the performance of the smoothing filter can often be significantly improved. Prior works showed that such performance gain is related to the Sinkhorn-Knopp balancing algorithm, an iterative procedure that symmetrizes a row-stochastic matrix to a doubly-stochastic matrix. However, a complete understanding of the performance gain phenomenon is still lacking. In this paper, we study the performance gain phenomenon from a statistical learning perspective. We show that Sinkhorn-Knopp is equivalent to an Expectation-Maximization (EM) algorithm of learning a Product of Gaussians (PoG) prior of the image patches. By establishing the correspondence between the steps of Sinkhorn-Knopp and the EM algorithm, we provide a geometrical interpretation of the symmetrization process. The new PoG model also allows us to develop a new denoising algorithm called Product of Gaussian Non-Local-Means (PoG-NLM). PoG-NLM is an extension of the Sinkhorn-Knopp and is a generalization of the classical non-local means. Despite its simple formulation, PoG-NLM outperforms many existing smoothing filters and has a similar performance compared to BM3D. version:1
arxiv-1601-00027 | Computational Pathology: Challenges and Promises for Tissue Analysis | http://arxiv.org/abs/1601.00027 | id:1601.00027 author:Thomas J. Fuchs, Joachim M. Buhmann category:cs.CV cs.AI  published:2015-12-31 summary:The histological assessment of human tissue has emerged as the key challenge for detection and treatment of cancer. A plethora of different data sources ranging from tissue microarray data to gene expression, proteomics or metabolomics data provide a detailed overview of the health status of a patient. Medical doctors need to assess these information sources and they rely on data driven automatic analysis tools. Methods for classification, grouping and segmentation of heterogeneous data sources as well as regression of noisy dependencies and estimation of survival probabilities enter the processing workflow of a pathology diagnosis system at various stages. This paper reports on state-of-the-art of the design and effectiveness of computational pathology workflows and it discusses future research directions in this emergent field of medical informatics and diagnostic machine learning. version:1
arxiv-1601-00025 | Write a Classifier: Predicting Visual Classifiers from Unstructured Text Descriptions | http://arxiv.org/abs/1601.00025 | id:1601.00025 author:Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh category:cs.CV cs.CL cs.LG  published:2015-12-31 summary:People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the linear classifier parameters for new classes. We also propose a generic kernelized models where a kernel classifier, in the form defined by the representer theorem, is predicted. The kernelized models allow defining any two RKHS kernel functions in the visual space and text space, respectively, and could be useful for other applications. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers for two fine-grained categorization datasets, and the results indicate successful predictions of our final model against several baselines that we designed. version:1
arxiv-1601-00024 | Selecting Near-Optimal Learners via Incremental Data Allocation | http://arxiv.org/abs/1601.00024 | id:1601.00024 author:Ashish Sabharwal, Horst Samulowitz, Gerald Tesauro category:cs.LG stat.ML  published:2015-12-31 summary:We study a novel machine learning (ML) problem setting of sequentially allocating small subsets of training data amongst a large set of classifiers. The goal is to select a classifier that will give near-optimal accuracy when trained on all data, while also minimizing the cost of misallocated samples. This is motivated by large modern datasets and ML toolkits with many combinations of learning algorithms and hyper-parameters. Inspired by the principle of "optimism under uncertainty," we propose an innovative strategy, Data Allocation using Upper Bounds (DAUB), which robustly achieves these objectives across a variety of real-world datasets. We further develop substantial theoretical support for DAUB in an idealized setting where the expected accuracy of a classifier trained on $n$ samples can be known exactly. Under these conditions we establish a rigorous sub-linear bound on the regret of the approach (in terms of misallocated data), as well as a rigorous bound on suboptimality of the selected classifier. Our accuracy estimates using real-world datasets only entail mild violations of the theoretical scenario, suggesting that the practical behavior of DAUB is likely to approach the idealized behavior. version:1
arxiv-1512-09327 | Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server | http://arxiv.org/abs/1512.09327 | id:1512.09327 author:Yee Whye Teh, Leonard Hasenclever, Thibaut Lienart, Sebastian Vollmer, Stefan Webb, Balaji Lakshminarayanan, Charles Blundell category:cs.LG stat.ML  published:2015-12-31 summary:This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a dataset is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Markov chain Monte Carlo (MCMC) sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks. version:1
arxiv-1409-3970 | A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data | http://arxiv.org/abs/1409.3970 | id:1409.3970 author:Yin Zheng, Yu-Jin Zhang, Hugo Larochelle category:cs.CV cs.IR cs.LG cs.NE  published:2014-09-13 summary:Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set. version:3
arxiv-1501-03291 | Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models | http://arxiv.org/abs/1501.03291 | id:1501.03291 author:Michael U. Gutmann, Jukka Corander category:stat.ML stat.CO stat.ME  published:2015-01-14 summary:Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude. version:3
arxiv-1512-09302 | Linear Convergence of Proximal Gradient Algorithm with Extrapolation for a Class of Nonconvex Nonsmooth Minimization Problems | http://arxiv.org/abs/1512.09302 | id:1512.09302 author:Bo Wen, Xiaojun Chen, Ting Kei Pong category:math.OC stat.ML  published:2015-12-31 summary:In this paper, we study the proximal gradient algorithm with extrapolation for minimizing the sum of a Lipschitz differentiable function and a proper closed convex function. Under the error bound condition used in [19] for analyzing the convergence of the proximal gradient algorithm, we show that there exists a threshold such that if the extrapolation coefficients are chosen below this threshold, then the sequence generated converges $R$-linearly to a stationary point of the problem. Moreover, the corresponding sequence of objective values is also $R$-linearly convergent. In addition, the threshold reduces to $1$ for convex problems and, as a consequence, we obtain the $R$-linear convergence of the sequence generated by the FISTA with the fixed restart. Finally, again for convex problems, we show that the successive changes of the iterates vanish for many choices of sequences of extrapolation coefficients that approach the threshold. In particular, this conclusion can be shown to hold for the sequence generated by the FISTA. version:1
arxiv-1512-09295 | Strategies and Principles of Distributed Machine Learning on Big Data | http://arxiv.org/abs/1512.09295 | id:1512.09295 author:Eric P. Xing, Qirong Ho, Pengtao Xie, Wei Dai category:stat.ML cs.DC cs.LG  published:2015-12-31 summary:The rise of Big Data has led to new demands for Machine Learning (ML) systems to learn complex models with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics thereupon. In order to run ML algorithms at such scales, on a distributed cluster with 10s to 1000s of machines, it is often the case that significant engineering efforts are required --- and one might fairly ask if such engineering truly falls within the domain of ML research or not. Taking the view that Big ML systems can benefit greatly from ML-rooted statistical and algorithmic insights --- and that ML researchers should therefore not shy away from such systems design --- we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of Big ML systems and architectures, with the goal of understanding how to make them efficient, generally-applicable, and supported with convergence and scaling guarantees. They concern four key questions which traditionally receive little attention in ML research: How to distribute an ML program over a cluster? How to bridge ML computation with inter-machine communication? How to perform such communication? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and grow the area that lies between ML and systems. version:1
arxiv-1512-09272 | Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions | http://arxiv.org/abs/1512.09272 | id:1512.09272 author:B G Vijay Kumar, Gustavo Carneiro, Ian Reid category:cs.CV  published:2015-12-31 summary:Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset. version:1
arxiv-1512-09251 | Solving the G-problems in less than 500 iterations: Improved efficient constrained optimization by surrogate modeling and adaptive parameter control | http://arxiv.org/abs/1512.09251 | id:1512.09251 author:Samineh Bagheri, Wolfgang Konen, Michael Emmerich, Thomas Bäck category:math.OC cs.NE stat.ML  published:2015-12-31 summary:Constrained optimization of high-dimensional numerical problems plays an important role in many scientific and industrial applications. Function evaluations in many industrial applications are severely limited and no analytical information about objective function and constraint functions is available. For such expensive black-box optimization tasks, the constraint optimization algorithm COBRA was proposed, making use of RBF surrogate modeling for both the objective and the constraint functions. COBRA has shown remarkable success in solving reliably complex benchmark problems in less than 500 function evaluations. Unfortunately, COBRA requires careful adjustment of parameters in order to do so. In this work we present a new self-adjusting algorithm SACOBRA, which is based on COBRA and capable to achieve high-quality results with very few function evaluations and no parameter tuning. It is shown with the help of performance profiles on a set of benchmark problems (G-problems, MOPTA08) that SACOBRA consistently outperforms any COBRA algorithm with fixed parameter setting. We analyze the importance of the several new elements in SACOBRA and find that each element of SACOBRA plays a role to boost up the overall optimization performance. We discuss the reasons behind and get in this way a better understanding of high-quality RBF surrogate modeling. version:1
arxiv-1512-09227 | Denoising and Completion of 3D Data via Multidimensional Dictionary Learning | http://arxiv.org/abs/1512.09227 | id:1512.09227 author:Zemin Zhang, Shuchin Aeron category:cs.LG cs.CV cs.DS  published:2015-12-31 summary:In this paper a new dictionary learning algorithm for multidimensional data is proposed. Unlike most conventional dictionary learning methods which are derived for dealing with vectors or matrices, our algorithm, named KTSVD, learns a multidimensional dictionary directly via a novel algebraic approach for tensor factorization as proposed in [3, 12, 13]. Using this approach one can define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D data to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based on the idea of sparse coding (using group-sparsity over multidimensional coefficient vectors), alternates between estimating a compact representation and dictionary learning. We analyze our KTSVD algorithm and demonstrate its result on video completion and multispectral image denoising. version:1
arxiv-1410-2505 | Recovery of Sparse Signals Using Multiple Orthogonal Least Squares | http://arxiv.org/abs/1410.2505 | id:1410.2505 author:Jian Wang, Ping Li category:stat.ME cs.IT cs.LG math.IT  published:2014-10-09 summary:We study the problem of recovering sparse signals from compressed linear measurements. This problem, often referred to as sparse recovery or sparse reconstruction, has generated a great deal of interest in recent years. To recover the sparse signals, we propose a new method called multiple orthogonal least squares (MOLS), which extends the well-known orthogonal least squares (OLS) algorithm by allowing multiple $L$ indices to be chosen per iteration. Owing to inclusion of multiple support indices in each selection, the MOLS algorithm converges in much fewer iterations and improves the computational efficiency over the conventional OLS algorithm. Theoretical analysis shows that MOLS ($L > 1$) performs exact recovery of all $K$-sparse signals within $K$ iterations if the measurement matrix satisfies the restricted isometry property (RIP) with isometry constant $\delta_{LK} < \frac{\sqrt{L}}{\sqrt{K} + 2 \sqrt{L}}.$ The recovery performance of MOLS in the noisy scenario is also studied. It is shown that stable recovery of sparse signals can be achieved with the MOLS algorithm when the signal-to-noise ratio (SNR) scales linearly with the sparsity level of input signals. version:2
arxiv-1512-09206 | Nonparametric mixture of Gaussian graphical models | http://arxiv.org/abs/1512.09206 | id:1512.09206 author:Kevin Lee, Lingzhou Xue category:stat.ME stat.ML  published:2015-12-31 summary:Graphical model has been widely used to investigate the complex dependence structure of high-dimensional data, and it is common to assume that observed data follow a homogeneous graphical model. However, observations usually come from different resources and have heterogeneous hidden commonality in real-world applications. Thus, it is of great importance to estimate heterogeneous dependencies and discover subpopulation with certain commonality across the whole population. In this work, we introduce a novel regularized estimation scheme for learning nonparametric mixture of Gaussian graphical models, which extends the methodology and applicability of Gaussian graphical models and mixture models. We propose a unified penalized likelihood approach to effectively estimate nonparametric functional parameters and heterogeneous graphical parameters. We further design an efficient generalized effective EM algorithm to address three significant challenges: high-dimensionality, non-convexity, and label switching. Theoretically, we study both the algorithmic convergence of our proposed algorithm and the asymptotic properties of our proposed estimators. Numerically, we demonstrate the performance of our method in simulation studies and a real application to estimate human brain functional connectivity from ADHD imaging data, where two heterogeneous conditional dependencies are explained through profiling demographic variables and supported by existing scientific findings. version:1
arxiv-1512-09204 | Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies | http://arxiv.org/abs/1512.09204 | id:1512.09204 author:Weici Hu, Peter I. Frazier category:cs.LG cs.AI stat.ML  published:2015-12-31 summary:We consider effort allocation in crowdsourcing, where we wish to assign labeling tasks to imperfect homogeneous crowd workers to maximize overall accuracy in a continuous-time Bayesian setting, subject to budget and time constraints. The Bayes-optimal policy for this problem is the solution to a partially observable Markov decision process, but the curse of dimensionality renders the computation infeasible. Based on the Lagrangian Relaxation technique in Adelman & Mersereau (2008), we provide a computationally tractable instance-specific upper bound on the value of this Bayes-optimal policy, which can in turn be used to bound the optimality gap of any other sub-optimal policy. In an approach similar in spirit to the Whittle index for restless multiarmed bandits, we provide an index policy for effort allocation in crowdsourcing and demonstrate numerically that it outperforms other stateof- arts and performs close to optimal solution. version:1
arxiv-1512-09170 | Statistical Query Algorithms for Stochastic Convex Optimization | http://arxiv.org/abs/1512.09170 | id:1512.09170 author:Vitaly Feldman, Cristobal Guzman, Santosh Vempala category:cs.LG cs.DS  published:2015-12-30 summary:Stochastic convex optimization, where the objective is the expectation of a random convex function, is an important and widely used method with numerous applications in machine learning, statistics, operations research and other areas. We study the complexity of stochastic convex optimization given only statistical query (SQ) access to the objective function. We show that well-known and popular methods, including first-order iterative methods and polynomial-time methods, can be implemented using only statistical queries. For many cases of interest we derive nearly matching upper and lower bounds on the estimation (sample) complexity including linear optimization in the most general setting. We then present several consequences for machine learning, differential privacy and proving concrete lower bounds on the power of convex optimization based methods. A new technical ingredient of our work is SQ algorithms for estimating the mean vector of a distribution over vectors in $\mathbb{R}^d$ with optimal estimation complexity. This is a natural problem and we show that our solutions can be used to get substantially improved SQ versions of Perceptron and other online algorithms for learning halfspaces. version:1
arxiv-1512-09156 | Low rank approximation and decomposition of large matrices using error correcting codes | http://arxiv.org/abs/1512.09156 | id:1512.09156 author:Shashanka Ubaru, Arya Mazumdar, Yousef Saad category:cs.IT cs.LG cs.NA math.IT  published:2015-12-30 summary:Low rank approximation is an important tool used in many applications of signal processing and machine learning. Recently, randomized sketching algorithms were proposed to effectively construct low rank approximations and obtain approximate singular value decompositions of large matrices. Similar ideas were used to solve least squares regression problems. In this paper, we show how matrices from error correcting codes can be used to find such low rank approximations and matrix decompositions, and extend the framework to linear least squares regression problems. The benefits of using these code matrices are the following: (i) They are easy to generate and they reduce randomness significantly. (ii) Code matrices with mild properties satisfy the subspace embedding property, and have a better chance of preserving the geometry of an entire subspace of vectors. (iii) For parallel and distributed applications, code matrices have significant advantages over structured random matrices and Gaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices, which require sampling $O(k\log k)$ columns for a rank-$k$ approximation, the log factor is not necessary for certain types of code matrices. That is, $(1+\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$ approximation with $O(k/\epsilon)$ samples. (v) Fast multiplication is possible with structured code matrices, so fast approximations can be achieved for general dense input matrices. (vi) For least squares regression problem $\min\ Ax-b\ _2$ where $A\in \mathbb{R}^{n\times d}$, the $(1+\epsilon)$ relative error approximation can be achieved with $O(d/\epsilon)$ samples, with high probability, when certain code matrices are used. version:1
arxiv-1512-09049 | LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing | http://arxiv.org/abs/1512.09049 | id:1512.09049 author:Chenliang Xu, Jason J. Corso category:cs.CV  published:2015-12-30 summary:Supervoxel segmentation has strong potential to be incorporated into early video analysis as superpixel segmentation has in image analysis. However, there are many plausible supervoxel methods and little understanding as to when and where each is most appropriate. Indeed, we are not aware of a single comparative study on supervoxel segmentation. To that end, we study seven supervoxel algorithms, including both off-line and streaming methods, in the context of what we consider to be a good supervoxel: namely, spatiotemporal uniformity, object/region boundary detection, region compression and parsimony. For the evaluation we propose a comprehensive suite of seven quality metrics to measure these desirable supervoxel characteristics. In addition, we evaluate the methods in a supervoxel classification task as a proxy for subsequent high-level uses of the supervoxels in video analysis. We use six existing benchmark video datasets with a variety of content-types and dense human annotations. Our findings have led us to conclusive evidence that the hierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) and temporal superpixels (TSP) methods are the top-performers among the seven methods. They all perform well in terms of segmentation accuracy, but vary in regard to the other desiderata: GBH captures object boundaries best; SWA has the best potential for region compression; and TSP achieves the best undersegmentation error. version:1
arxiv-1512-09041 | Actor-Action Semantic Segmentation with Grouping Process Models | http://arxiv.org/abs/1512.09041 | id:1512.09041 author:Chenliang Xu, Jason J. Corso category:cs.CV  published:2015-12-30 summary:Actor-action semantic segmentation made an important step toward advanced video understanding problems: what action is happening; who is performing the action; and where is the action in space-time. Current models for this problem are local, based on layered CRFs, and are unable to capture long-ranging interaction of video parts. We propose a new model that combines these local labeling CRFs with a hierarchical supervoxel decomposition. The supervoxels provide cues for possible groupings of nodes, at various scales, in the CRFs to encourage adaptive, high-order groups for more effective labeling. Our model is dynamic and continuously exchanges information during inference: the local CRFs influence what supervoxels in the hierarchy are active, and these active nodes influence the connectivity in the CRF; we hence call it a grouping process model. The experimental results on a recent large-scale video dataset show a large margin of 60% relative improvement over the state of the art, which demonstrates the effectiveness of the dynamic, bidirectional flow between labeling and grouping. version:1
arxiv-1504-01132 | Recursive Partitioning for Heterogeneous Causal Effects | http://arxiv.org/abs/1504.01132 | id:1504.01132 author:Susan Athey, Guido Imbens category:stat.ML  published:2015-04-05 summary:In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the "ground truth" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine. version:3
arxiv-1512-08996 | Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices | http://arxiv.org/abs/1512.08996 | id:1512.08996 author:Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou category:stat.ML stat.AP stat.ME  published:2015-12-30 summary:A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel Markov chain that sends the latent gamma random variables at time $(t-1)$ as the shape parameters of those at time $t$, which are linked to observed or latent counts under the Poisson likelihood. The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results. version:1
arxiv-1501-06218 | Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction | http://arxiv.org/abs/1501.06218 | id:1501.06218 author:Mingyuan Zhou category:stat.ML cs.SI  published:2015-01-25 summary:A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a Bernoulli-Poisson link. The model describes both homophily and stochastic equivalence, and is scalable to big sparse networks by focusing its computation on pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the models' scalability and state-of-the-art performance. version:2
arxiv-1512-08982 | Technical Report: a tool for measuring Prosodic Accommodation | http://arxiv.org/abs/1512.08982 | id:1512.08982 author:Sucheta Ghosh category:cs.SD cs.CL  published:2015-12-30 summary:Social interaction is a dynamic and joint activity where all participants are engaged and coordinate their behaviour in the co-construction of meaning. It is observed that conversational partners adapt their pitch, intensity and timing behaviour to their in- terlocutors. The majority of research has focused on its linear manifestation over the course of an interaction. De Looze et al hypothesised that it evolves dynamically with functional social aspects. In the work of De Looze et al, they proposed through their praat based feature extraction and matlab based visualisation that one can visualise prosodic accommodation at the positive correlation threshold values. and the capture of its dynamic manifestation. Here we seek to build a complete system for measuring prosodic accommodation with matlab. This work uses data collected in a pilot training scenario where senior pilot and co-pilot are engaged in conversation during the flight. Additionally, we use the data from a ship wreck task by two pilots. We also attempt to evaluate this measures of accommodation with ground truth labels given by a trainer of Crew (or sometimes Crisis) Resource Management (CRM). version:1
arxiv-1511-02199 | The Poisson Gamma Belief Network | http://arxiv.org/abs/1511.02199 | id:1511.02199 author:Mingyuan Zhou, Yulai Cong, Bo Chen category:stat.ML stat.ME  published:2015-11-06 summary:To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer. version:2
arxiv-1504-06837 | Assessing binary classifiers using only positive and unlabeled data | http://arxiv.org/abs/1504.06837 | id:1504.06837 author:Marc Claesen, Jesse Davis, Frank De Smet, Bart De Moor category:stat.ML cs.IR cs.LG I.5.2  published:2015-04-26 summary:Assessing the performance of a learned model is a crucial part of machine learning. However, in some domains only positive and unlabeled examples are available, which prohibits the use of most standard evaluation metrics. We propose an approach to estimate any metric based on contingency tables, including ROC and PR curves, using only positive and unlabeled data. Estimating these performance metrics is essentially reduced to estimating the fraction of (latent) positives in the unlabeled set, assuming known positives are a random sample of all positives. We provide theoretical bounds on the quality of our estimates, illustrate the importance of estimating the fraction of positives in the unlabeled set and demonstrate empirically that we are able to reliably estimate ROC and PR curves on real data. version:2
arxiv-1512-08903 | Online Keyword Spotting with a Character-Level Recurrent Neural Network | http://arxiv.org/abs/1512.08903 | id:1512.08903 author:Kyuyeon Hwang, Minjae Lee, Wonyong Sung category:cs.CL cs.LG cs.NE  published:2015-12-30 summary:In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-to-end trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations. version:1
arxiv-1512-08887 | Estimation of the sample covariance matrix from compressive measurements | http://arxiv.org/abs/1512.08887 | id:1512.08887 author:Farhad Pourkamali-Anaraki category:stat.ML cs.LG  published:2015-12-30 summary:This paper focuses on the estimation of the sample covariance matrix from low-dimensional random projections of data known as compressive measurements. In particular, we present an unbiased estimator to extract the covariance structure from compressive measurements obtained by a general class of random projection matrices consisting of i.i.d. zero-mean entries and finite first four moments. In contrast to previous works, we make no structural assumptions about the underlying covariance matrix such as being low-rank. In fact, our analysis is based on a non-Bayesian data setting which requires no distributional assumptions on the set of data samples. Furthermore, inspired by the generality of the projection matrices, we propose an approach to covariance estimation that utilizes very sparse random projections with Bernoulli entries. Therefore, our algorithm can be used to estimate the covariance matrix in applications with limited memory and computation power at the acquisition devices. Experimental results demonstrate that our approach allows for accurate estimation of the sample covariance matrix on several real-world data sets, including video data. version:1
arxiv-1508-04257 | Learning Meta-Embeddings by Using Ensembles of Embedding Sets | http://arxiv.org/abs/1508.04257 | id:1508.04257 author:Wenpeng Yin, Hinrich Schütze category:cs.CL  published:2015-08-18 summary:Word embeddings -- distributed representations of words -- in deep learning are beneficial for many tasks in natural language processing (NLP). However, different embedding sets vary greatly in quality and characteristics of the captured semantics. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning meta-embeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of meta-embeddings compared to individual embedding sets. One advantage of meta-embeddings is the increased vocabulary coverage. We will release our meta-embeddings publicly. version:2
arxiv-1512-08298 | Post-Regularization Inference for Dynamic Nonparanormal Graphical Models | http://arxiv.org/abs/1512.08298 | id:1512.08298 author:Junwei Lu, Mladen Kolar, Han Liu category:stat.ML  published:2015-12-28 summary:We propose a novel class of dynamic nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Thorough numerical simulations and an application to a neural imaging dataset support the usefulness of our method. version:2
arxiv-1512-08861 | Sharp Computational-Statistical Phase Transitions via Oracle Computational Model | http://arxiv.org/abs/1512.08861 | id:1512.08861 author:Zhaoran Wang, Quanquan Gu, Han Liu category:stat.ML  published:2015-12-30 summary:We study the fundamental tradeoffs between computational tractability and statistical accuracy for a general family of hypothesis testing problems with combinatorial structures. Based upon an oracle model of computation, which captures the interactions between algorithms and data, we establish a general lower bound that explicitly connects the minimum testing risk under computational budget constraints with the intrinsic probabilistic and combinatorial structures of statistical problems. This lower bound mirrors the classical statistical lower bound by Le Cam (1986) and allows us to quantify the optimal statistical performance achievable given limited computational budgets in a systematic fashion. Under this unified framework, we sharply characterize the statistical-computational phase transition for two testing problems, namely, normal mean detection and sparse principal component detection. For normal mean detection, we consider two combinatorial structures, namely, sparse set and perfect matching. For these problems we identify significant gaps between the optimal statistical accuracy that is achievable under computational tractability constraints and the classical statistical lower bounds. Compared with existing works on computational lower bounds for statistical problems, which consider general polynomial-time algorithms on Turing machines, and rely on computational hardness hypotheses on problems like planted clique detection, we focus on the oracle computational model, which covers a broad range of popular algorithms, and do not rely on unproven hypotheses. Moreover, our result provides an intuitive and concrete interpretation for the intrinsic computational intractability of high-dimensional statistical problems. One byproduct of our result is a lower bound for a strict generalization of the matrix permanent problem, which is of independent interest. version:1
arxiv-1512-08849 | Learning Natural Language Inference with LSTM | http://arxiv.org/abs/1512.08849 | id:1512.08849 author:Shuohang Wang, Jing Jiang category:cs.CL cs.AI cs.NE  published:2015-12-30 summary:Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for the NLI task. In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neutral attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a matching-LSTM that performs word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. Our experiments on the SNLI corpus show that our model outperforms the state of the art, achieving an accuracy of 86.1% on the test data. version:1
arxiv-1506-05439 | Learning with a Wasserstein Loss | http://arxiv.org/abs/1506.05439 | id:1506.05439 author:Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, Tomaso Poggio category:cs.LG cs.CV stat.ML  published:2015-06-17 summary:Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric. version:3
arxiv-1512-08819 | Joint limiting laws for high-dimensional independence tests | http://arxiv.org/abs/1512.08819 | id:1512.08819 author:Danning Li, Lingzhou Xue category:math.ST stat.ME stat.ML stat.OT stat.TH 62H12  60F05  published:2015-12-30 summary:Testing independence is of significant interest in many important areas of large-scale inference. Using extreme-value form statistics to test against sparse alternatives and using quadratic form statistics to test against dense alternatives are two important testing procedures for high-dimensional independence. However, quadratic form statistics suffer from low power against sparse alternatives, and extreme-value form statistics suffer from low power against dense alternatives with small disturbances and may have size distortions due to its slow convergence. For real-world applications, it is important to derive powerful testing procedures against more general alternatives. Based on intermediate limiting distributions, we derive (model-free) joint limiting laws of extreme-value form and quadratic form statistics, and surprisingly, we prove that they are asymptotically independent. Given such asymptotic independencies, we propose (model-free) testing procedures to boost the power against general alternatives and also retain the correct asymptotic size. Under the high-dimensional setting, we derive the closed-form limiting null distributions, and obtain their explicit rates of uniform convergence. We prove their consistent statistical powers against general alternatives. We demonstrate the performance of our proposed test statistics in simulation studies. Our work provides very helpful insights to high-dimensional independence tests, and fills an important gap. version:1
arxiv-1512-08814 | Combined statistical and model based texture features for improved image classification | http://arxiv.org/abs/1512.08814 | id:1512.08814 author:Omar Al-Kadi category:cs.CV  published:2015-12-29 summary:This paper aims to improve the accuracy of texture classification based on extracting texture features using five different texture methods and classifying the patterns using a naive Bayesian classifier. Three statistical-based and two model-based methods are used to extract texture features from eight different texture images, then their accuracy is ranked after using each method individually and in pairs. The accuracy improved up to 97.01% when model based -Gaussian Markov random field (GMRF) and fractional Brownian motion (fBm) - were used together for classification as compared to the highest achieved using each of the five different methods alone; and proved to be better in classifying as compared to statistical methods. Also, using GMRF with statistical based methods, such as Gray level co-occurrence (GLCM) and run-length (RLM) matrices, improved the overall accuracy to 96.94% and 96.55%; respectively. version:1
arxiv-1512-08787 | Matrix Completion Under Monotonic Single Index Models | http://arxiv.org/abs/1512.08787 | id:1512.08787 author:Ravi Ganti, Laura Balzano, Rebecca Willett category:stat.ML cs.LG  published:2015-12-29 summary:Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between low-rank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach. version:1
arxiv-1412-3328 | Memory vectors for similarity search in high-dimensional spaces | http://arxiv.org/abs/1412.3328 | id:1412.3328 author:Ahmet Iscen, Teddy Furon, Vincent Gripon, Michael Rabbat, Hervé Jégou category:cs.CV cs.DB  published:2014-12-10 summary:We study an indexing architecture to store and search in a database of high-dimensional vectors. This architecture is composed of several memory units, each of which summarizes a fraction of the database by a single representative vector.The potential similarity of the query to one of the vectors stored in the memory unit is gauged by a simple correlation with the memory unit's representative vector. This representative optimizes the test of the following hypothesis: the query is independent from any vector in the memory unit vs. the query is a simple perturbation of one of the stored vectors. Compared to exhaustive search, our approach finds the most similar database vectors significantly faster without a noticeable reduction in search quality. Interestingly, the reduction of complexity is provably better in high-dimensional spaces. We empirically demonstrate its practical interest in a large-scale image search scenario with off-the-shelf state-of-the-art descriptors. version:5
arxiv-1512-08673 | Error Bounds for Compressed Sensing Algorithms With Group Sparsity: A Unified Approach | http://arxiv.org/abs/1512.08673 | id:1512.08673 author:M. Eren Ahsen, M. Vidyasagar category:stat.ML 62J99  published:2015-12-29 summary:In compressed sensing, in order to recover a sparse or nearly sparse vector from possibly noisy measurements, the most popular approach is $\ell_1$-norm minimization. Upper bounds for the $\ell_2$- norm of the error between the true and estimated vectors are given in [1] and reviewed in [2], while bounds for the $\ell_1$-norm are given in [3]. When the unknown vector is not conventionally sparse but is "group sparse" instead, a variety of alternatives to the $\ell_1$-norm have been proposed in the literature, including the group LASSO, sparse group LASSO, and group LASSO with tree structured overlapping groups. However, no error bounds are available for any of these modified objective functions. In the present paper, a unified approach is presented for deriving upper bounds on the error between the true vector and its approximation, based on the notion of decomposable and $\gamma$-decomposable norms. The bounds presented cover all of the norms mentioned above, and also provide a guideline for choosing norms in future to accommodate alternate forms of sparsity. version:1
arxiv-1512-08669 | Robust Scene Text Recognition Using Sparse Coding based Features | http://arxiv.org/abs/1512.08669 | id:1512.08669 author:Da-Han Wang, Hanzi Wang, Dong Zhang, Jonathan Li, David Zhang category:cs.CV  published:2015-12-29 summary:In this paper, we propose an effective scene text recognition method using sparse coding based features, called Histograms of Sparse Codes (HSC) features. For character detection, we use the HSC features instead of using the Histograms of Oriented Gradients (HOG) features. The HSC features are extracted by computing sparse codes with dictionaries that are learned from data using K-SVD, and aggregating per-pixel sparse codes to form local histograms. For word recognition, we integrate multiple cues including character detection scores and geometric contexts in an objective function. The final recognition results are obtained by searching for the words which correspond to the maximum value of the objective function. The parameters in the objective function are learned using the Minimum Classification Error (MCE) training method. Experiments on several challenging datasets demonstrate that the proposed HSC-based scene text recognition method outperforms HOG-based methods significantly and outperforms most state-of-the-art methods. version:1
arxiv-1512-08648 | A framework for robust object multi-detection with a vote aggregation and a cascade filtering | http://arxiv.org/abs/1512.08648 | id:1512.08648 author:Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas category:cs.CV  published:2015-12-29 summary:This paper presents a framework designed for the multi-object detection purposes and adjusted for the application of product search on the market shelves. The framework uses a single feedback loop and a pattern resizing mechanism to demonstrate the top effectiveness of the state-of-the-art local features. A high detection rate with a low false detection chance can be achieved with use of only one pattern per object and no manual parameters adjustments. The method incorporates well known local features and a basic matching process to create a reliable voting space. Further steps comprise of metric transformations, graphical vote space representation, two-phase vote aggregation process and a cascade of verifying filters. version:1
arxiv-1512-08643 | Hypothesis Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity | http://arxiv.org/abs/1512.08643 | id:1512.08643 author:Eugene Belilovsky, Gaël Varoquaux, Matthew B. Blaschko category:stat.ML  published:2015-12-29 summary:Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two population calls for comparing these estimated GGMs. We study the problem of identifying differences in Gaussian Graphical Models (GGMs) known to have similar structure. We aim to characterize the uncertainty of differences with confidence intervals obtained using a para-metric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-number-of-samples settings. Quantifying the uncertainty of the parameters selected by the sparse penalty is an important question in applications such as neuroimaging or bioinformatics. Indeed, selected variables can be interpreted to build theoretical understanding or to make therapeutic decisions. Characterizing the distributions of sparse regression models is inherently challenging since the penalties produce a biased estimator. Recent work has shown how one can invoke the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give us confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we focus on deriving the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso. We show that we can debias and characterize the distribution in an efficient manner. We then go on to show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in Gaussian graphical models. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism. version:1
arxiv-1512-00961 | Triplet Spike Time Dependent Plasticity: A floating-gate Implementation | http://arxiv.org/abs/1512.00961 | id:1512.00961 author:Roshan Gopalakrishnan, Arindam Basu category:cs.NE  published:2015-12-03 summary:Synapse plays an important role of learning in a neural network; the learning rules which modify the synaptic strength based on the timing difference between the pre- and post-synaptic spike occurrence is termed as Spike Time Dependent Plasticity (STDP). The most commonly used rule posits weight change based on time difference between one pre- and one post spike and is hence termed doublet STDP (DSTDP). However, D-STDP could not reproduce results of many biological experiments; a triplet STDP (T-STDP) that considers triplets of spikes as the fundamental unit has been proposed recently to explain these observations. This paper describes the compact implementation of a synapse using single floating-gate (FG) transistor that can store a weight in a nonvolatile manner and demonstrate the triplet STDP (T-STDP) learning rule by modifying drain voltages according to triplets of spikes. We describe a mathematical procedure to obtain control voltages for the FG device for T-STDP and also show measurement results from a FG synapse fabricated in TSMC 0.35um CMOS process to support the theory. Possible VLSI implementation of drain voltage waveform generator circuits are also presented with simulation results. version:3
arxiv-1512-08602 | Tight Bounds for Approximate Carathéodory and Beyond | http://arxiv.org/abs/1512.08602 | id:1512.08602 author:Vahab Mirrokni, Renato Paes Leme, Adrian Vladu, Sam Chiu-wai Wong category:cs.DS cs.LG math.OC  published:2015-12-29 summary:We give a deterministic nearly-linear time algorithm for approximating any point inside a convex polytope with a sparse convex combination of the polytope's vertices. Our result provides a constructive proof for the Approximate Carath\'{e}odory Problem, which states that any point inside a polytope contained in the $\ell_p$ ball of radius $D$ can be approximated to within $\epsilon$ in $\ell_p$ norm by a convex combination of only $O\left(D^2 p/\epsilon^2\right)$ vertices of the polytope for $p \geq 2$. We also show that this bound is tight, using an argument based on anti-concentration for the binomial distribution. Along the way of establishing the upper bound, we develop a technique for minimizing norms over convex sets with complicated geometry; this is achieved by running Mirror Descent on a dual convex function obtained via Sion's Theorem. As simple extensions of our method, we then provide new algorithms for submodular function minimization and SVM training. For submodular function minimization we obtain a simplification and (provable) speed-up over Wolfe's algorithm, the method commonly found to be the fastest in practice. For SVM training, we obtain $O(1/\epsilon^2)$ convergence for arbitrary kernels; each iteration only requires matrix-vector operations involving the kernel matrix, so we overcome the obstacle of having to explicitly store the kernel or compute its Cholesky factorization. version:1
arxiv-1512-08575 | Optimal Selective Attention in Reactive Agents | http://arxiv.org/abs/1512.08575 | id:1512.08575 author:Roy Fox, Naftali Tishby category:cs.LG cs.IT math.IT  published:2015-12-29 summary:In POMDPs, information about the hidden state, delivered through observations, is both valuable to the agent, allowing it to base its actions on better informed internal states, and a "curse", exploding the size and diversity of the internal state space. One attempt to deal with this is to focus on reactive policies, that only base their actions on the most recent observation. However, even reactive policies can be demanding on resources, and agents need to pay selective attention to only some of the information available to them in observations. In this report we present the minimum-information principle for selective attention in reactive agents. We further motivate this approach by reducing the general problem of optimal control in POMDPs, to reactive control with complex observations. Lastly, we explore a newly discovered phenomenon of this optimization process - period doubling bifurcations. This necessitates periodic policies, and raises many more questions regarding stability, periodicity and chaos in optimal control. version:1
arxiv-1512-08571 | Structured Pruning of Deep Convolutional Neural Networks | http://arxiv.org/abs/1512.08571 | id:1512.08571 author:Sajid Anwar, Kyuyeon Hwang, Wonyong Sung category:cs.NE cs.LG stat.ML  published:2015-12-29 summary:Real time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks, which are channel wise, kernel wise and intra kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, parallel computing environments and hardware based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by computing the misclassification rate with corresponding connectivity pattern. The pruned network is re-trained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra kernel strided sparsity with a simple constraint can significantly reduce the size of kernel and feature map matrices. The pruned network is finally fixed point optimized with reduced word length precision. This results in significant reduction in the total storage size providing advantages for on-chip memory based implementations of deep neural networks. version:1
arxiv-1512-08569 | Analyzing Walter Skeat's Forty-Five Parallel Extracts of William Langland's Piers Plowman | http://arxiv.org/abs/1512.08569 | id:1512.08569 author:Roger Bilisoly category:stat.AP cs.CL  published:2015-12-29 summary:Walter Skeat published his critical edition of William Langland's 14th century alliterative poem, Piers Plowman, in 1886. In preparation for this he located forty-five manuscripts, and to compare dialects, he published excerpts from each of these. This paper does three statistical analyses using these excerpts, each of which mimics a task he did in writing his critical edition. First, he combined multiple versions of a poetic line to create a best line, which is compared to the mean string that is computed by a generalization of the arithmetic mean that uses edit distance. Second, he claims that a certain subset of manuscripts varies little. This is quantified by computing a string variance, which is closely related to the above generalization of the mean. Third, he claims that the manuscripts fall into three groups, which is a clustering problem that is addressed by using edit distance. The overall goal is to develop methodology that would be of use to a literary critic. version:1
arxiv-1512-08580 | A Simple Baseline for Travel Time Estimation using Large-Scale Trip Data | http://arxiv.org/abs/1512.08580 | id:1512.08580 author:Hongjian Wang, Zhenhui Li, Yu-Hsuan Kuo, Dan Kifer category:cs.LG cs.CY H.2.8; I.2.6  published:2015-12-28 summary:The increased availability of large-scale trajectory data around the world provides rich information for the study of urban dynamics. For example, New York City Taxi Limousine Commission regularly releases source-destination information about trips in the taxis they regulate. Taxi data provide information about traffic patterns, and thus enable the study of urban flow -- what will traffic between two locations look like at a certain date and time in the future? Existing big data methods try to outdo each other in terms of complexity and algorithmic sophistication. In the spirit of "big data beats algorithms", we present a very simple baseline which outperforms state-of-the-art approaches, including Bing Maps and Baidu Maps (whose APIs permit large scale experimentation). Such a travel time estimation baseline has several important uses, such as navigation (fast travel time estimates can serve as approximate heuristics for A search variants for path finding) and trip planning (which uses operating hours for popular destinations along with travel time estimates to create an itinerary). version:1
arxiv-1512-08475 | MRF-based multispectral image fusion using an adaptive approach based on edge-guided interpolation | http://arxiv.org/abs/1512.08475 | id:1512.08475 author:Mohammad Reza Khosravi, Suleiman Mansouri, Ahmad Keshavarz, Habib Rostami category:cs.CV  published:2015-12-28 summary:In interpretation of remote sensing images, it is possible that the images which are supplied by different sensors wouldn't be understandable or we could not get vital information from them. For better visual perception of images, it is essential to operate series of pre-processing and elementary corrections and then operate a series of main processing for more precise analysis on the image. There are several approaches for processing which depend on type of remote sensing image. The discussed approach in this article, i.e. image fusion, is using natural colors of an optical image for adding color to gray-scale satellite image which gives us the ability to better observe the HR image of the OLI sensor of Landsat. This process previously with emphasis on details of fusion technique was performed, but we are going to relieve concept of interpolation process that did not have suitable attentions in past. In fact we see many important software tools such as ENVI and ERDAS as most famous remote sensing image processing software tools have only classical interpolation techniques (such as BL and CC). Therefore ENVI-based and ERDAS-based researches in image fusion area and even other fusion researches often do not use new and better interpolations and only are concentrating on fusion details for achievement of better quality, so we only focus on interpolation impact in fusion quality in a specific application, i.e. Landsat multi-spectral images. The important feature of this approach is using a statistical, adaptive, edge-guided and MRF-based interpolation method for improving color quality in MRF-based images with maintenance of high resolution in practice. Numerical Simulations show selection of suitable interpolation technique in MRF-based images creates better quality rather than classical interpolations. version:1
arxiv-1512-08457 | Approximate Hubel-Wiesel Modules and the Data Structures of Neural Computation | http://arxiv.org/abs/1512.08457 | id:1512.08457 author:Joel Z. Leibo, Julien Cornebise, Sergio Gómez, Demis Hassabis category:cs.NE q-bio.NC  published:2015-12-28 summary:This paper describes a framework for modeling the interface between perception and memory on the algorithmic level of analysis. It is consistent with phenomena associated with many different brain regions. These include view-dependence (and invariance) effects in visual psychophysics and inferotemporal cortex physiology, as well as episodic memory recall interference effects associated with the medial temporal lobe. The perspective developed here relies on a novel interpretation of Hubel and Wiesel's conjecture for how receptive fields tuned to complex objects, and invariant to details, could be achieved. It complements existing accounts of two-speed learning systems in neocortex and hippocampus (e.g., McClelland et al. 1995) while significantly expanding their scope to encompass a unified view of the entire pathway from V1 to hippocampus. version:1
arxiv-1512-08424 | Graph entropies in texture segmentation of images | http://arxiv.org/abs/1512.08424 | id:1512.08424 author:Martin Welk category:cs.CV  published:2015-12-28 summary:We study the applicability of a set of texture descriptors introduced in recent work by the author to texture-based segmentation of images. The texture descriptors under investigation result from applying graph indices from quantitative graph theory to graphs encoding the local structure of images. The underlying graphs arise from the computation of morphological amoebas as structuring elements for adaptive morphology, either as weighted or unweighted Dijkstra search trees or as edge-weighted pixel graphs within structuring elements. In the present paper we focus on texture descriptors in which the graph indices are entropy-based, and use them in a geodesic active contour framework for image segmentation. Experiments on several synthetic and one real-world image are shown to demonstrate texture segmentation by this approach. Forthermore, we undertake an attempt to analyse selected entropy-based texture descriptors with regard to what information about texture they actually encode. Whereas this analysis uses some heuristic assumptions, it indicates that the graph-based texture descriptors are related to fractal dimension measures that have been proven useful in texture analysis. version:1
arxiv-1507-03372 | Ordered Decompositional DAG Kernels Enhancements | http://arxiv.org/abs/1507.03372 | id:1507.03372 author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG  published:2015-07-13 summary:In this paper, we show how the Ordered Decomposition DAGs (ODD) kernel framework, a framework that allows the definition of graph kernels from tree kernels, allows to easily define new state-of-the-art graph kernels. Here we consider a fast graph kernel based on the Subtree kernel (ST), and we propose various enhancements to increase its expressiveness. The proposed DAG kernel has the same worst-case complexity as the one based on ST, but an improved expressivity due to an augmented set of features. Moreover, we propose a novel weighting scheme for the features, which can be applied to other kernels of the ODD framework. These improvements allow the proposed kernels to improve on the classification performances of the ST-based kernel for several real-world datasets, reaching state-of-the-art performances. version:2
arxiv-1512-08413 | Outlier Detection In Large-scale Traffic Data By Naïve Bayes Method and Gaussian Mixture Model Method | http://arxiv.org/abs/1512.08413 | id:1512.08413 author:Philip Lam, Lili Wang, Henry Y. T. Ngan, Nelson H. C. Yung, Anthony G. O. Yeh category:cs.CV  published:2015-12-28 summary:It is meaningful to detect outliers in traffic data for traffic management. However, this is a massive task for people from large-scale database to distinguish outliers. In this paper, we present two methods: Kernel Smoothing Na\"ive Bayes (NB) method and Gaussian Mixture Model (GMM) method to automatically detect any hardware errors as well as abnormal traffic events in traffic data collected at a four-arm junction in Hong Kong. Traffic data was recorded in a video format, and converted to spatial-temporal (ST) traffic signals by statistics. The ST signals are then projected to a two-dimensional (2D) (x,y)-coordinate plane by Principal Component Analysis (PCA) for dimension reduction. We assume that inlier data are normal distributed. As such, the NB and GMM methods are successfully applied in outlier detection (OD) for traffic data. The kernel smooth NB method assumes the existence of kernel distributions in traffic data and uses Bayes' Theorem to perform OD. In contrast, the GMM method believes the traffic data is formed by the mixture of Gaussian distributions and exploits confidence region for OD. This paper would address the modeling of each method and evaluate their respective performances. Experimental results show that the NB algorithm with Triangle kernel and GMM method achieve up to 93.78% and 94.50% accuracies, respectively. version:1
arxiv-1512-04392 | Automatic Incident Classification for Big Traffic Data by Adaptive Boosting SVM | http://arxiv.org/abs/1512.04392 | id:1512.04392 author:Li-Li Wang, Henry Y. T. Ngan, Nelson H. C. Yung category:cs.LG  published:2015-12-14 summary:Modern cities experience heavy traffic flows and congestions regularly across space and time. Monitoring traffic situations becomes an important challenge for the Traffic Control and Surveillance Systems (TCSS). In advanced TCSS, it is helpful to automatically detect and classify different traffic incidents such as severity of congestion, abnormal driving pattern, abrupt or illegal stop on road, etc. Although most TCSS are equipped with basic incident detection algorithms, they are however crude to be really useful as an automated tool for further classification. In literature, there is a lack of research for Automated Incident Classification (AIC). Therefore, a novel AIC method is proposed in this paper to tackle such challenges. In the proposed method, traffic signals are firstly extracted from captured videos and converted as spatial-temporal (ST) signals. Based on the characteristics of the ST signals, a set of realistic simulation data are generated to construct an extended big traffic database to cover a variety of traffic situations. Next, a Mean-Shift filter is introduced to suppress the effect of noise and extract significant features from the ST signals. The extracted features are then associated with various types of traffic data: one normal type (inliers) and multiple abnormal types (outliers). For the classification, an adaptive boosting classifier is trained to detect outliers in traffic data automatically. Further, a Support Vector Machine (SVM) based method is adopted to train the model for identifying the categories of outliers. In short, this hybrid approach is called an Adaptive Boosting Support Vector Machines (AB-SVM) method. Experimental results show that the proposed AB-SVM method achieves a satisfied result with more than 92% classification accuracy on average. version:2
arxiv-1502-01228 | Linear-time Online Action Detection From 3D Skeletal Data Using Bags of Gesturelets | http://arxiv.org/abs/1502.01228 | id:1502.01228 author:Moustafa Meshry, Mohamed E. Hussein, Marwan Torki category:cs.CV  published:2015-02-04 summary:Sliding window is one direct way to extend a successful recognition system to handle the more challenging detection problem. While action recognition decides only whether or not an action is present in a pre-segmented video sequence, action detection identifies the time interval where the action occurred in an unsegmented video stream. Sliding window approaches for action detection can however be slow as they maximize a classifier score over all possible sub-intervals. Even though new schemes utilize dynamic programming to speed up the search for the optimal sub-interval, they require offline processing on the whole video sequence. In this paper, we propose a novel approach for online action detection based on 3D skeleton sequences extracted from depth data. It identifies the sub-interval with the maximum classifier score in linear time. Furthermore, it is invariant to temporal scale variations and is suitable for real-time applications with low latency. version:6
arxiv-1512-08279 | Using Causal Discovery to Track Information Flow in Spatio-Temporal Data - A Testbed and Experimental Results Using Advection-Diffusion Simulations | http://arxiv.org/abs/1512.08279 | id:1512.08279 author:Imme Ebert-Uphoff, Yi Deng category:cs.LG  published:2015-12-27 summary:Causal discovery algorithms based on probabilistic graphical models have emerged in geoscience applications for the identification and visualization of dynamical processes. The key idea is to learn the structure of a graphical model from observed spatio-temporal data, which indicates information flow, thus pathways of interactions, in the observed physical system. Studying those pathways allows geoscientists to learn subtle details about the underlying dynamical mechanisms governing our planet. Initial studies using this approach on real-world atmospheric data have shown great potential for scientific discovery. However, in these initial studies no ground truth was available, so that the resulting graphs have been evaluated only by whether a domain expert thinks they seemed physically plausible. This paper seeks to fill this gap. We develop a testbed that emulates two dynamical processes dominant in many geoscience applications, namely advection and diffusion, in a 2D grid. Then we apply the causal discovery based information tracking algorithms to the simulation data to study how well the algorithms work for different scenarios and to gain a better understanding of the physical meaning of the graph results, in particular of instantaneous connections. We make all data sets used in this study available to the community as a benchmark. Keywords: Information flow, graphical model, structure learning, causal discovery, geoscience. version:1
arxiv-1512-08269 | Statistical and Computational Guarantees for the Baum-Welch Algorithm | http://arxiv.org/abs/1512.08269 | id:1512.08269 author:Fanny Yang, Sivaraman Balakrishnan, Martin J. Wainwright category:stat.ML cs.IT math.IT math.ST stat.TH  published:2015-12-27 summary:The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates, guaranteeing geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions. version:1
arxiv-1512-08240 | Robust Semi-supervised Least Squares Classification by Implicit Constraints | http://arxiv.org/abs/1512.08240 | id:1512.08240 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2015-12-27 summary:We introduce the implicitly constrained least squares (ICLS) classifier, a novel semi-supervised version of the least squares classifier. This classifier minimizes the squared loss on the labeled data among the set of parameters implied by all possible labelings of the unlabeled data. Unlike other discriminative semi-supervised methods, this approach does not introduce explicit additional assumptions into the objective function, but leverages implicit assumptions already present in the choice of the supervised least squares classifier. This method can be formulated as a quadratic programming problem and its solution can be found using a simple gradient descent procedure. We prove that, in a limited 1-dimensional setting, this approach never leads to performance worse than the supervised classifier. Experimental results show that also in the general multidimensional case performance improvements can be expected, both in terms of the squared loss that is intrinsic to the classifier, as well as in terms of the expected classification error. version:1
arxiv-1511-04176 | Sequence to Sequence Learning for Optical Character Recognition | http://arxiv.org/abs/1511.04176 | id:1511.04176 author:Devendra Kumar Sahu, Mohak Sukhwani category:cs.CV  published:2015-11-13 summary:We propose an end-to-end recurrent encoder-decoder based sequence learning approach for printed text Optical Character Recognition (OCR). In contrast to present day existing state-of-art OCR solution which uses connectionist temporal classification (CTC) output layer, our approach makes minimalistic assumptions on the structure and length of the sequence. We use a two step encoder-decoder approach -- (a) A recurrent encoder reads a variable length printed text word image and encodes it to a fixed dimensional embedding. (b) This fixed dimensional embedding is subsequently comprehended by decoder structure which converts it into a variable length text output. Our architecture gives competitive performance relative to connectionist temporal classification (CTC) output layer while being executed in more natural settings. The learnt deep word image embedding from encoder can be used for printed text based retrieval systems. The expressive fixed dimensional embedding for any variable length input expedites the task of retrieval and makes it more efficient which is not possible with other recurrent neural network architectures. We empirically investigate the expressiveness and the learnability of long short term memory (LSTMs) in the sequence to sequence learning regime by training our network for prediction tasks in segmentation free printed text OCR. The utility of the proposed architecture for printed text is demonstrated by quantitative and qualitative evaluation of two tasks -- word prediction and retrieval. version:2
arxiv-1512-08204 | New Perspectives on $k$-Support and Cluster Norms | http://arxiv.org/abs/1512.08204 | id:1512.08204 author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:cs.LG stat.ML  published:2015-12-27 summary:We study a regularizer which is defined as a parameterized infimum of quadratics, and which we call the box-norm. We show that the k-support norm, a regularizer proposed by [Argyriou et al, 2012] for sparse vector prediction problems, belongs to this family, and the box-norm can be generated as a perturbation of the former. We derive an improved algorithm to compute the proximity operator of the squared box-norm, and we provide a method to compute the norm. We extend the norms to matrices, introducing the spectral k-support norm and spectral box-norm. We note that the spectral box-norm is essentially equivalent to the cluster norm, a multitask learning regularizer introduced by [Jacob et al. 2009a], and which in turn can be interpreted as a perturbation of the spectral k-support norm. Centering the norm is important for multitask learning and we also provide a method to use centered versions of the norms as regularizers. Numerical experiments indicate that the spectral k-support and box-norms and their centered variants provide state of the art performance in matrix completion and multitask learning problems respectively. version:1
arxiv-1506-07194 | Advanced statistical methods for eye movement analysis and modeling: a gentle introduction | http://arxiv.org/abs/1506.07194 | id:1506.07194 author:Giuseppe Boccignone category:physics.data-an cs.CV q-bio.NC G.3; I.5; I.4  published:2015-06-23 summary:In this Chapter we show that by considering eye movements, and in particular, the resulting sequence of gaze shifts, a stochastic process, a wide variety of tools become available for analyses and modelling beyond conventional statistical methods. Such tools encompass random walk analyses and more complex techniques borrowed from the pattern recognition and machine learning fields. After a brief, though critical, probabilistic tour of current computational models of eye movements and visual attention, we lay down the basis for gaze shift pattern analysis. To this end, the concepts of Markov Processes, the Wiener process and related random walks within the Gaussian framework of the Central Limit Theorem will be introduced. Then, we will deliberately violate fundamental assumptions of the Central Limit Theorem to elicit a larger perspective, rooted in statistical physics, for analysing and modelling eye movements in terms of anomalous, non-Gaussian, random walks and modern foraging theory. Eventually, by resorting to machine learning techniques, we discuss how the analyses of movement patterns can develop into the inference of hidden patterns of the mind: inferring the observer's task, assessing cognitive impairments, classifying expertise. version:3
arxiv-1508-07415 | Mixed Gaussian-Impulse Noise Removal from Highly Corrupted Images via Adaptive Local and Nonlocal Statistical Priors | http://arxiv.org/abs/1508.07415 | id:1508.07415 author:Nasser Eslahi, Hami Mahdavinataj, Ali Aghagolzadeh category:cs.CV 62F15  published:2015-08-29 summary:The motivation of this paper is to introduce a novel framework for the restoration of images corrupted by mixed Gaussian-impulse noise. To this aim, first, an adaptive curvelet thresholding criterion is proposed which tries to adaptively remove the perturbations appeared during denoising process. Then, a new statistical regularization term, called joint adaptive statistical prior (JASP), is established which enforces both the local and nonlocal statistical consistencies, simultaneously, in a unified manner. Furthermore, a novel technique for mixed Gaussian plus impulse noise removal using JASP in a variational scheme is developed--we refer to it as De-JASP. To efficiently solve the above variational scheme, an efficient alternating minimization algorithm based on split Bregman iterative framework is developed. Extensive experimental results manifest the effectiveness of the proposed method comparing with the current state-of-the-art methods in mixed Gaussian-impulse noise removal. version:2
arxiv-1512-08178 | Electricity Demand Forecasting by Multi-Task Learning | http://arxiv.org/abs/1512.08178 | id:1512.08178 author:Jean-Baptiste Fiot, Francesco Dinuzzo category:cs.LG  published:2015-12-27 summary:We explore the application of kernel-based multi-task learning techniques to forecast the demand of electricity in multiple nodes of a distribution network. We show that recently developed output kernel learning techniques are particularly well suited to solve this problem, as they allow to flexibly model the complex seasonal effects that characterize electricity demand data, while learning and exploiting correlations between multiple demand profiles. We also demonstrate that kernels with a multiplicative structure yield superior predictive performance with respect to the widely adopted (generalized) additive models. Our study is based on residential and industrial smart meter data provided by the Irish Commission for Energy Regulation (CER). version:1
arxiv-1509-05488 | TransG : A Generative Mixture Model for Knowledge Graph Embedding | http://arxiv.org/abs/1509.05488 | id:1509.05488 author:Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu category:cs.CL  published:2015-09-18 summary:Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of \textbf{multiple relation semantics} that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, \textbf{TransG}. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines. version:4
arxiv-1512-08169 | Self-Excitation: An Enabler for Online Thermal Estimation and Model Predictive Control of Buildings | http://arxiv.org/abs/1512.08169 | id:1512.08169 author:Peter Radecki, Brandon Hencey category:cs.SY cs.LG  published:2015-12-27 summary:This paper investigates a method to improve buildings' thermal predictive control performance via online identification and excitation (active learning process) that minimally disrupts normal operations. In previous studies we have demonstrated scalable methods to acquire multi-zone thermal models of passive buildings using a gray-box approach that leverages building topology and measurement data. Here we extend the method to multi-zone actively controlled buildings and examine how to improve the thermal model estimation by using the controller to excite unknown portions of the building's dynamics. Comparing against a baseline thermostat controller, we demonstrate the utility of both the initially acquired and improved thermal models within a Model Predictive Control (MPC) framework, which anticipates weather uncertainty and time-varying temperature set-points. A simulation study demonstrates self-excitation improves model estimation, which corresponds to improved MPC energy savings and occupant comfort. By coupling building topology, estimation, and control routines into a single online framework, we have demonstrated the potential for low-cost scalable methods to actively learn and control buildings to ensure occupant comfort and minimize energy usage, all while using the existing building's HVAC sensors and hardware. version:1
arxiv-1601-02947 | Online Model Estimation for Predictive Thermal Control of Buildings | http://arxiv.org/abs/1601.02947 | id:1601.02947 author:Peter Radecki, Brandon Hencey category:cs.SY cs.LG  published:2015-12-27 summary:This study proposes a general, scalable method to learn control-oriented thermal models of buildings that could enable wide-scale deployment of cost-effective predictive controls. An Unscented Kalman Filter augmented for parameter and disturbance estimation is shown to accurately learn and predict a building's thermal response. Recent studies of heating, ventilating, and air conditioning (HVAC) systems have shown significant energy savings with advanced model predictive control (MPC). A scalable cost-effective method to readily acquire accurate, robust models of individual buildings' unique thermal envelopes has historically been elusive and hindered the widespread deployment of prediction-based control systems. Continuous commissioning and lifetime performance of these thermal models requires deployment of on-line data-driven system identification and parameter estimation routines. We propose a novel gray-box approach using an Unscented Kalman Filter based on a multi-zone thermal network and validate it with EnergyPlus simulation data. The filter quickly learns parameters of a thermal network during periods of known or constrained loads and then characterizes unknown loads in order to provide accurate 24+ hour energy predictions. This study extends our initial investigation by formalizing parameter and disturbance estimation routines and demonstrating results across a year-long study. version:1
arxiv-1509-04681 | Large-Scale Optimization Algorithms for Sparse Conditional Gaussian Graphical Models | http://arxiv.org/abs/1509.04681 | id:1509.04681 author:Calvin McCarter, Seyoung Kim category:stat.ML  published:2015-09-15 summary:This paper addresses the problem of scalable optimization for L1-regularized conditional Gaussian graphical models. Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables. While highly scalable optimization methods exist for sparse Gaussian graphical model estimation, state-of-the-art methods for conditional Gaussian graphical models are not efficient enough and more importantly, fail due to memory constraints for very large problems. In this paper, we propose a new optimization procedure based on a Newton method that efficiently iterates over two sub-problems, leading to drastic improvement in computation time compared to the previous methods. We then extend our method to scale to large problems under memory constraints, using block coordinate descent to limit memory usage while achieving fast convergence. Using synthetic and genomic data, we show that our methods can solve one million dimensional problems to high accuracy in a little over a day on a single machine. version:2
arxiv-1512-08133 | The Utility of Abstaining in Binary Classification | http://arxiv.org/abs/1512.08133 | id:1512.08133 author:Akshay Balsubramani category:cs.LG  published:2015-12-26 summary:We explore the problem of binary classification in machine learning, with a twist - the classifier is allowed to abstain on any datum, professing ignorance about the true class label without committing to any prediction. This is directly motivated by applications like medical diagnosis and fraud risk assessment, in which incorrect predictions have potentially calamitous consequences. We focus on a recent spate of theoretically driven work in this area that characterizes how allowing abstentions can lead to fewer errors in very general settings. Two areas are highlighted: the surprising possibility of zero-error learning, and the fundamental tradeoff between predicting sufficiently often and avoiding incorrect predictions. We review efficient algorithms with provable guarantees for each of these areas. We also discuss connections to other scenarios, notably active learning, as they suggest promising directions of further inquiry in this emerging field. version:1
arxiv-1502-02558 | K2-ABC: Approximate Bayesian Computation with Kernel Embeddings | http://arxiv.org/abs/1502.02558 | id:1502.02558 author:Mijung Park, Wittawat Jitkrittum, Dino Sejdinovic category:stat.ML cs.LG  published:2015-02-09 summary:Complicated generative models often result in a situation where computing the likelihood of observed data is intractable, while simulating from the conditional density given a parameter value is relatively easy. Approximate Bayesian Computation (ABC) is a paradigm that enables simulation-based posterior inference in such cases by measuring the similarity between simulated and observed data in terms of a chosen set of summary statistics. However, there is no general rule to construct sufficient summary statistics for complex models. Insufficient summary statistics will "leak" information, which leads to ABC algorithms yielding samples from an incorrect (partial) posterior. In this paper, we propose a fully nonparametric ABC paradigm which circumvents the need for manually selecting summary statistics. Our approach, K2-ABC, uses maximum mean discrepancy (MMD) as a dissimilarity measure between the distributions over observed and simulated data. MMD is easily estimated as the squared difference between their empirical kernel embeddings. Experiments on a simulated scenario and a real-world biological problem illustrate the effectiveness of the proposed algorithm. version:4
arxiv-1512-08103 | Data Driven Robust Image Guided Depth Map Restoration | http://arxiv.org/abs/1512.08103 | id:1512.08103 author:Wei Liu, Yun Gu, Chunhua Shen, Xiaogang Chen, Qiang Wu, Jie Yang category:cs.CV  published:2015-12-26 summary:Depth maps captured by modern depth cameras such as Kinect and Time-of-Flight (ToF) are usually contaminated by missing data, noises and suffer from being of low resolution. In this paper, we present a robust method for high-quality restoration of a degraded depth map with the guidance of the corresponding color image. We solve the problem in an energy optimization framework that consists of a novel robust data term and smoothness term. To accommodate not only the noise but also the inconsistency between depth discontinuities and the color edges, we model both the data term and smoothness term with a robust exponential error norm function. We propose to use Iteratively Re-weighted Least Squares (IRLS) methods for efficiently solving the resulting highly non-convex optimization problem. More importantly, we further develop a data-driven adaptive parameter selection scheme to properly determine the parameter in the model. We show that the proposed approach can preserve fine details and sharp depth discontinuities even for a large upsampling factor ($8\times$ for example). Experimental results on both simulated and real datasets demonstrate that the proposed method outperforms recent state-of-the-art methods in coping with the heavy noise, preserving sharp depth discontinuities and suppressing the texture copy artifacts. version:1
arxiv-1512-08086 | Part-Stacked CNN for Fine-Grained Visual Categorization | http://arxiv.org/abs/1512.08086 | id:1512.08086 author:Shaoli Huang, Zhe Xu, Dacheng Tao, Ya Zhang category:cs.CV  published:2015-12-26 summary:In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy. In this paper, we propose a novel Part-Stacked CNN architecture that explicitly explains the fine-grained recognition process by modeling subtle differences from object parts. Based on manually-labeled strong part annotations, the proposed architecture consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that en- codes object-level and part-level cues simultaneously. By adopting a set of sharing strategies between the computation of multiple object parts, the proposed architecture is very efficient running at 20 frames/sec during inference. Experimental results on the CUB-200-2011 dataset reveal the effectiveness of the proposed architecture, from both the perspective of classification accuracy and model interpretability. version:1
arxiv-1512-08066 | The Improvement of Negative Sentences Translation in English-to-Korean Machine Translation | http://arxiv.org/abs/1512.08066 | id:1512.08066 author:Chung-Hyok Jang, Kwang-Hyok Kim category:cs.CL  published:2015-12-26 summary:This paper describes the algorithm for translating English negative sentences into Korean in English-Korean Machine Translation (EKMT). The proposed algorithm is based on the comparative study of English and Korean negative sentences. The earlier translation software cannot translate English negative sentences into accurate Korean equivalents. We established a new algorithm for the negative sentence translation and evaluated it. version:1
arxiv-1512-08065 | Inverse Reinforcement Learning via Deep Gaussian Process | http://arxiv.org/abs/1512.08065 | id:1512.08065 author:Ming Jin, Costas Spanos category:cs.LG cs.RO stat.ML  published:2015-12-26 summary:The report proposes a new approach for inverse reinforcement learning based on deep Gaussian process (GP), which is capable of learning complicated reward structures with few demonstrations. The model stacks multiple latent GP layers to learn abstract representations of the state feature space, which is linked to the demonstrations through the Maximum Entropy learning framework. As analytic derivation of the model evidence is prohibitive due to the nonlinearity of latent variables, variational inference is employed for approximate inference, based on a special choice of variational distributions. This guards the model from over training, achieving the Automatic Occam's Razor. Experiments on the benchmark test, i.e., object world, as well as a new setup, i.e., binary world, are performed, where the proposed method outperforms state-of-the-art approaches. version:1
arxiv-1512-08064 | Statistical Learning under Nonstationary Mixing Processes | http://arxiv.org/abs/1512.08064 | id:1512.08064 author:Steve Hanneke, Tommi Jaakkola, Liu Yang category:cs.LG stat.ML  published:2015-12-26 summary:We study a special case of the problem of statistical learning without the i.i.d. assumption. Specifically, we suppose a learning method is presented with a sequence of data points, and required to make a prediction (e.g., a classification) for each one, and can then observe the loss incurred by this prediction. We go beyond traditional analyses, which have focused on stationary mixing processes or nonstationary product processes, by combining these two relaxations to allow nonstationary mixing processes. We are particularly interested in the case of $\beta$-mixing processes, with the sum of changes in marginal distributions growing sublinearly in the number of samples. Under these conditions, we propose a learning method, and establish that for bounded VC subgraph classes, the cumulative excess risk grows sublinearly in the number of predictions, at a quantified rate. version:1
arxiv-1512-08051 | A Multiresolution Clinical Decision Support System Based on Fractal Model Design for Classification of Histological Brain Tumours | http://arxiv.org/abs/1512.08051 | id:1512.08051 author:Omar S. Al-Kadi category:cs.CV  published:2015-12-25 summary:Tissue texture is known to exhibit a heterogeneous or non-stationary nature, therefore using a single resolution approach for optimum classification might not suffice. A clinical decision support system that exploits the subband textural fractal characteristics for best bases selection of meningioma brain histopathological image classification is proposed. Each subband is analysed using its fractal dimension instead of energy, which has the advantage of being less sensitive to image intensity and abrupt changes in tissue texture. The most significant subband that best identifies texture discontinuities will be chosen for further decomposition, and its fractal characteristics would represent the optimal feature vector for classification. The performance was tested using the support vector machine (SVM), Bayesian and k-nearest neighbour (kNN) classifiers and a leave-one-patient-out method was employed for validation. Our method outperformed the classical energy based selection approaches, achieving for SVM, Bayesian and kNN classifiers an overall classification accuracy of 94.12%, 92.50% and 79.70%, as compared to 86.31%, 83.19% and 51.63% for the co-occurrence matrix, and 76.01%, 73.50% and 50.69% for the energy texture signatures, respectively. These results indicate the potential usefulness as a decision support system that could complement radiologists diagnostic capability to discriminate higher order statistical textural information, for which it would be otherwise difficult via ordinary human vision. version:1
arxiv-1512-08049 | Texture measures combination for improved meningioma classification of histopathological images | http://arxiv.org/abs/1512.08049 | id:1512.08049 author:Omar S. Al-Kadi category:cs.CV  published:2015-12-25 summary:Providing an improved technique which can assist pathologists in correctly classifying meningioma tumours with a significant accuracy is our main objective. The proposed technique, which is based on optimum texture measure combination, inspects the separability of the RGB colour channels and selects the channel which best segments the cell nuclei of the histopathological images. The morphological gradient was applied to extract the region of interest for each subtype and for elimination of possible noise (e.g. cracks) which might occur during biopsy preparation. Meningioma texture features are extracted by four different texture measures (two model-based and two statistical-based) and then corresponding features are fused together in different combinations after excluding highly correlated features, and a Bayesian classifier was used for meningioma subtype discrimination. The combined Gaussian Markov random field and run-length matrix texture measures outperformed all other combinations in terms of quantitatively characterising the meningioma tissue, achieving an overall classification accuracy of 92.50%, improving from 83.75% which is the best accuracy achieved if the texture measures are used individually. version:1
arxiv-1512-08047 | Assessment of texture measures susceptibility to noise in conventional and contrast enhanced computed tomography lung tumour images | http://arxiv.org/abs/1512.08047 | id:1512.08047 author:Omar Sultan Al-Kadi category:cs.CV  published:2015-12-25 summary:Noise is one of the major problems that hinder an effective texture analysis of disease in medical images, which may cause variability in the reported diagnosis. In this paper seven texture measurement methods (two wavelet, two model and three statistical based) were applied to investigate their susceptibility to subtle noise caused by acquisition and reconstruction deficiencies in computed tomography (CT) images. Features of lung tumours were extracted from two different conventional and contrast enhanced CT image data-sets under filtered and noisy conditions. When measuring the noise in the background open-air region of the analysed CT images, noise of Gaussian and Rayleigh distributions with varying mean and variance was encountered, and Fisher distance was used to differentiate between an original extracted lung tumour region of interest (ROI) with the filtered and noisy reconstructed versions. It was determined that the wavelet packet (WP) and fractal dimension measures were the least affected, while the Gaussian Markov random field, run-length and co-occurrence matrices were the most affected by noise. Depending on the selected ROI size, it was concluded that texture measures with fewer extracted features can decrease susceptibility to noise, with the WP and the Gabor filter having a stable performance in both filtered and noisy CT versions and for both data-sets. Knowing how robust each texture measure under noise presence is can assist physicians using an automated lung texture classification system in choosing the appropriate feature extraction algorithm for a more accurate diagnosis. version:1
arxiv-1511-03729 | Larger-Context Language Modelling | http://arxiv.org/abs/1511.03729 | id:1511.03729 author:Tian Wang, Kyunghyun Cho category:cs.CL  published:2015-11-11 summary:In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed model improves perplexity significantly. In the experi- ments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger- context language model, we discover that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily. version:2
arxiv-1512-08008 | Discovering topic structures of a temporally evolving document corpus | http://arxiv.org/abs/1512.08008 | id:1512.08008 author:Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh category:cs.IR cs.LG  published:2015-12-25 summary:In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, splitting, and merging. The power of the proposed framework is demonstrated on two medical literature corpora concerned with the autism spectrum disorder (ASD) and the metabolic syndrome (MetS) -- both increasingly important research subjects with significant social and healthcare consequences. In addition to the collected ASD and metabolic syndrome literature corpora which we made freely available, our contribution also includes an extensive empirical analysis of the proposed framework. We describe a detailed and careful examination of the effects that our algorithms's free parameters have on its output, and discuss the significance of the findings both in the context of the practical application of our algorithm as well as in the context of the existing body of work on temporal topic analysis. Our quantitative analysis is followed by several qualitative case studies highly relevant to the current research on ASD and MetS, on which our algorithm is shown to capture well the actual developments in these fields. version:1
arxiv-1512-07982 | Inducing Generalized Multi-Label Rules with Learning Classifier Systems | http://arxiv.org/abs/1512.07982 | id:1512.07982 author:Fani A. Tzima, Miltiadis Allamanis, Alexandros Filotheou, Pericles A. Mitkas category:cs.NE cs.LG  published:2015-12-25 summary:In recent years, multi-label classification has attracted a significant body of research, motivated by real-life applications, such as text classification and medical diagnoses. Although sparsely studied in this context, Learning Classifier Systems are naturally well-suited to multi-label classification problems, whose search space typically involves multiple highly specific niches. This is the motivation behind our current work that introduces a generalized multi-label rule format -- allowing for flexible label-dependency modeling, with no need for explicit knowledge of which correlations to search for -- and uses it as a guide for further adapting the general Michigan-style supervised Learning Classifier System framework. The integration of the aforementioned rule format and framework adaptations results in a novel algorithm for multi-label classification whose behavior is studied through a set of properly defined artificial problems. The proposed algorithm is also thoroughly evaluated on a set of multi-label datasets and found competitive to other state-of-the-art multi-label classification methods. version:1
arxiv-1512-07980 | Micro-Differential Evolution: Diversity Enhancement and Comparative Study | http://arxiv.org/abs/1512.07980 | id:1512.07980 author:Hojjat Salehinejad, Shahryar Rahnamayan, Hamid R. Tizhoosh category:cs.NE  published:2015-12-25 summary:The differential evolution (DE) algorithm suffers from high computational time due to slow nature of evaluation. In contrast, micro-DE (MDE) algorithms employ a very small population size, which can converge faster to a reasonable solution. However, these algorithms are vulnerable to a premature convergence as well as to high risk of stagnation. In this paper, MDE algorithm with vectorized random mutation factor (MDEVM) is proposed, which utilizes the small size population benefit while empowers the exploration ability of mutation factor through randomizing it in the decision variable level. The idea is supported by analyzing mutation factor using Monte-Carlo based simulations. To facilitate the usage of MDE algorithms with very-small population sizes, new mutation schemes for population sizes less than four are also proposed. Furthermore, comprehensive comparative simulations and analysis on performance of the MDE algorithms over various mutation schemes, population sizes, problem types (i.e. uni-modal, multi-modal, and composite), problem dimensionalities, and mutation factor ranges are conducted by considering population diversity analysis for stagnation and trapping in local optimum situations. The studies are conducted on 28 benchmark functions provided for the IEEE CEC-2013 competition. Experimental results demonstrate high performance and convergence speed of the proposed MDEVM algorithm. version:1
arxiv-1512-07960 | Histogram Meets Topic Model: Density Estimation by Mixture of Histograms | http://arxiv.org/abs/1512.07960 | id:1512.07960 author:Hideaki Kim, Hiroshi Sawada category:stat.ML  published:2015-12-25 summary:The histogram method is a powerful non-parametric approach for estimating the probability density function of a continuous variable. But the construction of a histogram, compared to the parametric approaches, demands a large number of observations to capture the underlying density function. Thus it is not suitable for analyzing a sparse data set, a collection of units with a small size of data. In this paper, by employing the probabilistic topic model, we develop a novel Bayesian approach to alleviating the sparsity problem in the conventional histogram estimation. Our method estimates a unit's density function as a mixture of basis histograms, in which the number of bins for each basis, as well as their heights, is determined automatically. The estimation procedure is performed by using the fast and easy-to-implement collapsed Gibbs sampling. We apply the proposed method to synthetic data, showing that it performs well. version:1
arxiv-1508-01006 | Relation Classification via Recurrent Neural Network | http://arxiv.org/abs/1508.01006 | id:1508.01006 author:Dongxu Zhang, Dong Wang category:cs.CL cs.LG cs.NE  published:2015-08-05 summary:Deep learning has gained much success in sentence-level relation classification. For example, convolutional neural networks (CNN) have delivered competitive performance without much effort on feature engineering as the conventional pattern-based methods. Thus a lot of works have been produced based on CNN structures. However, a key issue that has not been well addressed by the CNN-based method is the lack of capability to learn temporal features, especially long-distance dependency between nominal pairs. In this paper, we propose a simple framework based on recurrent neural networks (RNN) and compare it with CNN-based model. To show the limitation of popular used SemEval-2010 Task 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al., 2014). Experiments on two different datasets strongly indicates that the RNN-based model can deliver better performance on relation classification, and it is particularly capable of learning long-distance relation patterns. This makes it suitable for real-world applications where complicated expressions are often involved. version:2
arxiv-1512-07951 | A Combined Deep-Learning and Deformable-Model Approach to Fully Automatic Segmentation of the Left Ventricle in Cardiac MRI | http://arxiv.org/abs/1512.07951 | id:1512.07951 author:M. R. Avendi, A. Kheradvar, H. Jafarkhani category:cs.CV  published:2015-12-25 summary:Segmentation of the left ventricle (LV) from cardiac magnetic resonance imaging (MRI) datasets is an essential step for calculation of clinical indices such as ventricular volume and ejection fraction. In this work, we employ deep learning algorithms combined with deformable models to develop and evaluate a fully automatic segmentation tool for the LV from short-axis cardiac MRI datasets. The method employs deep learning algorithms to learn the segmentation task from the ground true data. Convolutional networks are employed to automatically detect the LV chamber in MRI dataset. Stacked autoencoders are utilized to infer the shape of the LV. The inferred shape is incorporated into deformable models to improve the accuracy and robustness of the segmentation. We validated our method using 45 cardiac MR datasets taken from the MICCAI 2009 LV segmentation challenge and showed that it outperforms the state-of-the art methods. Excellent agreement with the ground truth was achieved. Validation metrics, percentage of good contours, Dice metric, average perpendicular distance and conformity, were computed as 96.69%, 0.94, 1.81mm and 0.86, versus those of 79.2%-95.62%, 0.87-0.9, 1.76-2.97mm and 0.67-0.78, obtained by other methods, respectively. version:1
arxiv-1512-07947 | Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain Stochastically Fully Connected Conditional Random Fields | http://arxiv.org/abs/1512.07947 | id:1512.07947 author:Edward Li, Farzad Khalvati, Mohammad Javad Shafiee, Masoom A. Haider, Alexander Wong category:cs.CV physics.med-ph stat.ME  published:2015-12-25 summary:Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology for the screening and diagnosis of frequently occurring cancers. However image quality may suffer by long acquisition times for MRIs due to patient motion, as well as result in great patient discomfort. Reducing MRI acquisition time can reduce patient discomfort and as a result reduces motion artifacts from the acquisition process. Compressive sensing strategies, when applied to MRI, have been demonstrated to be effective at decreasing acquisition times significantly by sparsely sampling the \emph{k}-space during the acquisition process. However, such a strategy requires advanced reconstruction algorithms to produce high quality and reliable images from compressive sensing MRI. This paper proposes a new reconstruction approach based on cross-domain stochastically fully connected conditional random fields (CD-SFCRF) for compressive sensing MRI. The CD-SFCRF introduces constraints in both \emph{k}-space and spatial domains within a stochastically fully connected graphical model to produce improved MRI reconstruction. Experimental results using T2-weighted (T2w) imaging and diffusion-weighted imaging (DWI) of the prostate show strong performance in preserving fine details and tissue structures in the reconstructed images when compared to other tested methods even at low sampling rates. version:1
arxiv-1512-01110 | Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization | http://arxiv.org/abs/1512.01110 | id:1512.01110 author:Yang Song, Jun Zhu category:cs.NA cs.AI cs.LG  published:2015-12-03 summary:Bayesian matrix completion has been studied based on a low-rank matrix factorization formulation with promising results. However, little work has been done on Bayesian matrix completion based on the more direct spectral regularization formulation. We fill this gap by presenting a novel Bayesian matrix completion method based on spectral regularization. In order to circumvent the difficulties of dealing with the orthonormality constraints of singular vectors, we derive a new equivalent form with relaxed constraints, which then leads us to design an adaptive version of spectral regularization feasible for Bayesian inference. Our Bayesian method requires no parameter tuning and can infer the number of latent factors automatically. Experiments on synthetic and real datasets demonstrate encouraging results on rank recovery and collaborative filtering, with notably good results for very sparse matrices. version:2
arxiv-1512-07942 | Multi-Level Cause-Effect Systems | http://arxiv.org/abs/1512.07942 | id:1512.07942 author:Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI  published:2015-12-25 summary:We present a domain-general account of causation that applies to settings in which macro-level causal relations between two systems are of interest, but the relevant causal features are poorly understood and have to be aggregated from vast arrays of micro-measurements. Our approach generalizes that of Chalupka et al. (2015) to the setting in which the macro-level effect is not specified. We formalize the connection between micro- and macro-variables in such situations and provide a coherent framework describing causal relations at multiple levels of analysis. We present an algorithm that discovers macro-variable causes and effects from micro-level measurements obtained from an experiment. We further show how to design experiments to discover macro-variables from observational micro-variable data. Finally, we show that under specific conditions, one can identify multiple levels of causal structure. Throughout the article, we use a simulated neuroscience multi-unit recording experiment to illustrate the ideas and the algorithms. version:1
arxiv-1505-07414 | Sufficient Forecasting Using Factor Models | http://arxiv.org/abs/1505.07414 | id:1505.07414 author:Jianqing Fan, Lingzhou Xue, Jiawei Yao category:math.ST stat.ME stat.ML stat.TH  published:2015-05-27 summary:We consider forecasting a single time series when there is a large number of predictors and a possible nonlinear effect. The dimensionality was first reduced via a high-dimensional (approximate) factor model implemented by the principal component analysis. Using the extracted factors, we develop a novel forecasting method called the sufficient forecasting, which provides a set of sufficient predictive indices, inferred from high-dimensional predictors, to deliver additional predictive power. The projected principal component analysis will be employed to enhance the accuracy of inferred factors when a semi-parametric (approximate) factor model is assumed. Our method is also applicable to cross-sectional sufficient regression using extracted factors. The connection between the sufficient forecasting and the deep learning architecture is explicitly stated. The sufficient forecasting correctly estimates projection indices of the underlying factors even in the presence of a nonparametric forecasting function. The proposed method extends the sufficient dimension reduction to high-dimensional regimes by condensing the cross-sectional information through factor models. We derive asymptotic properties for the estimate of the central subspace spanned by these projection directions as well as the estimates of the sufficient predictive indices. We further show that the natural method of running multiple regression of target on estimated factors yields a linear estimate that actually falls into this central subspace. Our method and theory allow the number of predictors to be larger than the number of observations. We finally demonstrate that the sufficient forecasting improves upon the linear forecasting in both simulation studies and an empirical study of forecasting macroeconomic variables. version:2
arxiv-1512-07928 | Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network | http://arxiv.org/abs/1512.07928 | id:1512.07928 author:Seunghoon Hong, Junhyuk Oh, Bohyung Han, Honglak Lee category:cs.CV  published:2015-12-24 summary:We propose a novel weakly-supervised semantic segmentation algorithm based on Deep Convolutional Neural Network (DCNN). Contrary to existing weakly-supervised approaches, our algorithm exploits auxiliary segmentation annotations available for different categories to guide segmentations on images with only image-level class labels. To make the segmentation knowledge transferrable across categories, we design a decoupled encoder-decoder architecture with attention model. In this architecture, the model generates spatial highlights of each category presented in an image using an attention model, and subsequently generates foreground segmentation for each highlighted region using decoder. Combining attention model, we show that the decoder trained with segmentation annotations in different categories can boost the performance of weakly-supervised semantic segmentation. The proposed algorithm demonstrates substantially improved performance compared to the state-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012 dataset when our model is trained with the annotations in 60 exclusive categories in Microsoft COCO dataset. version:1
arxiv-1304-5245 | Feature Elimination in Kernel Machines in moderately high dimensions | http://arxiv.org/abs/1304.5245 | id:1304.5245 author:Sayan Dasgupta, Yair Goldberg, Michael Kosorok category:stat.ML 68T05  62G08  published:2013-04-18 summary:We develop an approach for feature elimination in statistical learning with kernel machines, based on recursive elimination of features.We present theoretical properties of this method and show that it is uniformly consistent in finding the correct feature space under certain generalized assumptions.We present four case studies to show that the assumptions are met in most practical situations and present simulation results to demonstrate performance of the proposed approach. version:2
arxiv-1512-07815 | Truncated Max-of-Convex Models | http://arxiv.org/abs/1512.07815 | id:1512.07815 author:Pankaj Pansari, M. Pawan Kumar category:cs.CV  published:2015-12-24 summary:Truncated convex models (TCM) are special cases of pairwise random fields that have been widely used in computer vision. However, by restricting the order of the potentials to be at most two, they fail to capture useful image statistics. We propose a natural generalization of TCM to high-order random fields, which we call truncated max-of-convex models (TMCM). The energy function of TMCM consists of two types of potentials: (i) unary potentials, which have no restriction on their form; and (ii) high-order potentials, which are the sum of the truncation of the m largest convex distances over disjoint pairs of random variables in an arbitrary size clique. The use of a convex distance function encourages smoothness, while truncation allows for discontinuities in the labeling. By using m > 1, TMCM provides robustness towards errors in the clique definition. In order to minimize the energy function of a TMCM over all possible labelings, we design an efficient st-mincut based range expansion algorithm. We prove the accuracy of our algorithm by establishing strong multiplicative bounds for several special cases of interest. version:1
arxiv-1512-07797 | The Lovász Hinge: A Convex Surrogate for Submodular Losses | http://arxiv.org/abs/1512.07797 | id:1512.07797 author:Jiaqian Yu, Matthew Blaschko category:stat.ML cs.LG  published:2015-12-24 summary:Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel surrogate loss function for submodular losses, the Lov{\'a}sz hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. We prove that the Lov{\'a}sz hinge is convex and yields an extension. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through several set prediction tasks, including on the PASCAL VOC and Microsoft COCO datasets. version:1
arxiv-1412-3773 | Distinguishing cause from effect using observational data: methods and benchmarks | http://arxiv.org/abs/1412.3773 | id:1412.3773 author:Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, Bernhard Schölkopf category:cs.LG cs.AI stat.ML stat.OT  published:2014-12-11 summary:The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X, Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: Additive Noise Methods (ANM) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 datasets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the "ground truth" causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the additive-noise method originally proposed by Hoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of 0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method. version:3
arxiv-1512-07783 | Hardware Architecture for Large Parallel Array of Random Feature Extractors applied to Image Recognition | http://arxiv.org/abs/1512.07783 | id:1512.07783 author:Aakash Patil, Shanlan Shen, Enyi Yao, Arindam Basu category:cs.ET cs.NE C.3; C.5.4; I.5.5  published:2015-12-24 summary:We demonstrate a low-power and compact hardware implementation of Random Feature Extractor (RFE) core. With complex tasks like Image Recognition requiring a large set of features, we show how weight reuse technique can allow to virtually expand the random features available from RFE core. Further, we show how to avoid computation cost wasted for propagating "incognizant" or redundant random features. For proof of concept, we validated our approach by using our RFE core as the first stage of Extreme Learning Machine (ELM)--a two layer neural network--and were able to achieve $>97\%$ accuracy on MNIST database of handwritten digits. ELM's first stage of RFE is done on an analog ASIC occupying $5$mm$\times5$mm area in $0.35\mu$m CMOS and consuming $5.95$ $\mu$J/classify while using $\approx 5000$ effective hidden neurons. The ELM second stage consisting of just adders can be implemented as digital circuit with estimated power consumption of $20.9$ nJ/classify. With a total energy consumption of only $5.97$ $\mu$J/classify, this low-power mixed signal ASIC can act as a co-processor in portable electronic gadgets with cameras. version:1
arxiv-1511-01442 | Low-Rank Approximation of Weighted Tree Automata | http://arxiv.org/abs/1511.01442 | id:1511.01442 author:Guillaume Rabusseau, Borja Balle, Shay B. Cohen category:cs.LG cs.FL  published:2015-11-04 summary:We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima. version:2
arxiv-1512-07748 | Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips | http://arxiv.org/abs/1512.07748 | id:1512.07748 author:Tomohiko Nakamura, Eita Nakamura, Shigeki Sagayama category:cs.SD cs.LG cs.MM  published:2015-12-24 summary:This paper discusses real-time alignment of audio signals of music performance to the corresponding score (a.k.a. score following) which can handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips) in performances. This type of score following is particularly useful in automatic accompaniment for practices and rehearsals, where errors and repeats/skips are often made. Simple extensions of the algorithms previously proposed in the literature are not applicable in these situations for scores of practical length due to the problem of large computational complexity. To cope with this problem, we present two hidden Markov models of monophonic performance with errors and arbitrary repeats/skips, and derive efficient score-following algorithms with an assumption that the prior probability distributions of score positions before and after repeats/skips are independent from each other. We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data. Further improvements and extension for polyphonic signals are also discussed. version:1
arxiv-1512-05449 | Differential Evolution with Event-Triggered Impulsive Control | http://arxiv.org/abs/1512.05449 | id:1512.05449 author:Wei Du, Sunney Yung Sun Leung, Yang Tang, Athanasios V. Vasilakos category:cs.NE cs.SY math.OC  published:2015-12-17 summary:Differential evolution (DE) is a simple but powerful evolutionary algorithm, which has been widely and successfully used in various areas. In this paper, an event-triggered impulsive control scheme (ETI) is introduced to improve the performance of DE. Impulsive control, the concept of which derives from control theory, aims at regulating the states of a network by instantly adjusting the states of a fraction of nodes at certain instants, and these instants are determined by event-triggered mechanism (ETM). By introducing impulsive control and ETM into DE, we hope to change the search performance of the population in a positive way after revising the positions of some individuals at certain moments. At the end of each generation, the impulsive control operation is triggered when the update rate of the population declines or equals to zero. In detail, inspired by the concepts of impulsive control, two types of impulses are presented within the framework of DE in this paper: stabilizing impulses and destabilizing impulses. Stabilizing impulses help the individuals with lower rankings instantly move to a desired state determined by the individuals with better fitness values. Destabilizing impulses randomly alter the positions of inferior individuals within the range of the current population. By means of intelligently modifying the positions of a part of individuals with these two kinds of impulses, both exploitation and exploration abilities of the whole population can be meliorated. In addition, the proposed ETI is flexible to be incorporated into several state-of-the-art DE variants. Experimental results over the CEC 2014 benchmark functions exhibit that the developed scheme is simple yet effective, which significantly improves the performance of the considered DE algorithms. version:2
arxiv-1512-07716 | Fast Parallel SVM using Data Augmentation | http://arxiv.org/abs/1512.07716 | id:1512.07716 author:Hugh Perkins, Minjie Xu, Jun Zhu, Bo Zhang category:cs.LG  published:2015-12-24 summary:As one of the most popular classifiers, linear SVMs still have challenges in dealing with very large-scale problems, even though linear or sub-linear algorithms have been developed recently on single machines. Parallel computing methods have been developed for learning large-scale SVMs. However, existing methods rely on solving local sub-optimization problems. In this paper, we develop a novel parallel algorithm for learning large-scale linear SVM. Our approach is based on a data augmentation equivalent formulation, which casts the problem of learning SVM as a Bayesian inference problem, for which we can develop very efficient parallel sampling methods. We provide empirical results for this parallel sampling SVM, and provide extensions for SVR, non-linear kernels, and provide a parallel implementation of the Crammer and Singer model. This approach is very promising in its own right, and further is a very useful technique to parallelize a broader family of general maximum-margin models. version:1
arxiv-1512-06498 | Harnessing the Deep Net Object Models for Enhancing Human Action Recognition | http://arxiv.org/abs/1512.06498 | id:1512.06498 author:O. V. Ramana Murthy, Roland Goecke category:cs.CV  published:2015-12-21 summary:In this study, the influence of objects is investigated in the scenario of human action recognition with large number of classes. We hypothesize that the objects the humans are interacting will have good say in determining the action being performed. Especially, if the objects are non-moving, such as objects appearing in the background, features such as spatio-temporal interest points, dense trajectories may fail to detect them. Hence we propose to detect objects using pre-trained object detectors in every frame statically. Trained Deep network models are used as object detectors. Information from different layers in conjunction with different encoding techniques is extensively studied to obtain the richest feature vectors. This technique is observed to yield state-of-the-art performance on HMDB51 and UCF101 datasets. version:2
arxiv-1512-07712 | Fast Acquisition for Quantitative MRI Maps: Sparse Recovery from Non-linear Measurements | http://arxiv.org/abs/1512.07712 | id:1512.07712 author:Anupriya Gogna, Angshul Majumdar category:cs.CV  published:2015-12-24 summary:This work addresses the problem of estimating proton density and T1 maps from two partially sampled K-space scans such that the total acquisition time remains approximately the same as a single scan. Existing multi parametric non linear curve fitting techniques require a large number (8 or more) of echoes to estimate the maps resulting in prolonged (clinically infeasible) acquisition times. Our simulation results show that our method yields very accurate and robust results from only two partially sampled scans (total scan time being the same as a single echo MRI). We model PD and T1 maps to be sparse in some transform domain. The PD map is recovered via standard Compressed Sensing based recovery technique. Estimating the T1 map requires solving an analysis prior sparse recovery problem from non linear measurements, since the relationship between T1 values and intensity values or K space samples is not linear. For the first time in this work, we propose an algorithm for analysis prior sparse recovery for non linear measurements. We have compared our approach with the only existing technique based on matrix factorization from non linear measurements; our method yields considerably superior results. version:1
arxiv-1512-07685 | Service Choreography, SBVR, and Time | http://arxiv.org/abs/1512.07685 | id:1512.07685 author:Nurulhuda A. Manaf, Sotiris Moschoyiannis, Paul Krause category:cs.SE cs.CL  published:2015-12-24 summary:We propose the use of structured natural language (English) in specifying service choreographies, focusing on the what rather than the how of the required coordination of participant services in realising a business application scenario. The declarative approach we propose uses the OMG standard Semantics of Business Vocabulary and Rules (SBVR) as a modelling language. The service choreography approach has been proposed for describing the global orderings of the invocations on interfaces of participant services. We therefore extend SBVR with a notion of time which can capture the coordination of the participant services, in terms of the observable message exchanges between them. The extension is done using existing modelling constructs in SBVR, and hence respects the standard specification. The idea is that users - domain specialists rather than implementation specialists - can verify the requested service composition by directly reading the structured English used by SBVR. At the same time, the SBVR model can be represented in formal logic so it can be parsed and executed by a machine. version:1
arxiv-1512-07666 | Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks | http://arxiv.org/abs/1512.07666 | id:1512.07666 author:Chunyuan Li, Changyou Chen, David Carlson, Lawrence Carin category:stat.ML  published:2015-12-23 summary:Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models. version:1
arxiv-1512-07662 | High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep Models | http://arxiv.org/abs/1512.07662 | id:1512.07662 author:Chunyuan Li, Changyou Chen, Kai Fan, Lawrence Carin category:stat.ML  published:2015-12-23 summary:Learning in deep models using Bayesian methods has generated significant attention recently. This is largely because of the feasibility of modern Bayesian methods to yield scalable learning and inference, while maintaining a measure of uncertainty in the model parameters. Stochastic gradient MCMC algorithms (SG-MCMC) are a family of diffusion-based sampling methods for large-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradient thermostats (mSGNHT) augment each parameter of interest, with a momentum and a thermostat variable to maintain stationary distributions as target posterior distributions. As the number of variables in a continuous-time diffusion increases, its numerical approximation error becomes a practical bottleneck, so better use of a numerical integrator is desirable. To this end, we propose use of an efficient symmetric splitting integrator in mSGNHT, instead of the traditional Euler integrator. We demonstrate that the proposed scheme is more accurate, robust, and converges faster. These properties are demonstrated to be desirable in Bayesian deep learning. Extensive experiments on two canonical models and their deep extensions demonstrate that the proposed scheme improves general Bayesian posterior sampling, particularly for deep models. version:1
arxiv-1512-07650 | The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms | http://arxiv.org/abs/1512.07650 | id:1512.07650 author:Yahel David, Nahum Shimkin category:stat.ML cs.AI cs.LG  published:2015-12-23 summary:We consider the Max $K$-Armed Bandit problem, where a learning agent is faced with several stochastic arms, each a source of i.i.d. rewards of unknown distribution. At each time step the agent chooses an arm, and observes the reward of the obtained sample. Each sample is considered here as a separate item with the reward designating its value, and the goal is to find an item with the highest possible value. Our basic assumption is a known lower bound on the {\em tail function} of the reward distributions. Under the PAC framework, we provide a lower bound on the sample complexity of any $(\epsilon,\delta)$-correct algorithm, and propose an algorithm that attains this bound up to logarithmic factors. We analyze the robustness of the proposed algorithm and in addition, we compare the performance of this algorithm to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage. Interestingly, when the maximal rewards of the arms happen to be similar, the latter approach may provide better performance. version:1
arxiv-1512-07638 | Satisficing in multi-armed bandit problems | http://arxiv.org/abs/1512.07638 | id:1512.07638 author:Paul Reverdy, Vaibhav Srivastava, Naomi Ehrich Leonard category:cs.LG math.OC stat.ML  published:2015-12-23 summary:Satisficing is a relaxation of maximizing and allows for less risky decision-making in the face of uncertainty. We propose two sets of satisficing objectives for the multi-armed bandit problem, where the objective is to achieve reward-based decision-making performance above a given threshold. We show that these new problems are equivalent to various standard multi-armed bandit problems with maximizing objectives and use the equivalence to find bounds on performance. The different objectives can result in qualitatively different behavior; for example, agents explore their options continually in one case and only a finite number of times in another. For the case of Gaussian rewards we show an additional equivalence between the two sets of satisficing objectives that allows algorithms developed for one set to be applied to the other. We then develop variants of the Upper Credible Limit (UCL) algorithm that solve the problems with satisficing objectives and show that these modified UCL algorithms achieve efficient satisficing performance. version:1
arxiv-1506-04088 | Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes | http://arxiv.org/abs/1506.04088 | id:1506.04088 author:Ryan Giordano, Tamara Broderick, Michael Jordan category:stat.ML  published:2015-06-12 summary:Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables---both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data. version:2
arxiv-1512-07548 | k-Means Clustering Is Matrix Factorization | http://arxiv.org/abs/1512.07548 | id:1512.07548 author:Christian Bauckhage category:stat.ML  published:2015-12-23 summary:We show that the objective function of conventional k-means clustering can be expressed as the Frobenius norm of the difference of a data matrix and a low rank approximation of that data matrix. In short, we show that k-means clustering is a matrix factorization problem. These notes are meant as a reference and intended to provide a guided tour towards a result that is often mentioned but seldom made explicit in the literature. version:1
arxiv-1504-02174 | Connectivity Preserving Multivalued Functions in Digital Topology | http://arxiv.org/abs/1504.02174 | id:1504.02174 author:Laurence Boxer, P. Christopher Staecker category:cs.CV math.GN I.4.m  published:2015-04-09 summary:We study connectivity preserving multivalued functions between digital images. This notion generalizes that of continuous multivalued functions studied mostly in the setting of the digital plane $Z^2$. We show that connectivity preserving multivalued functions, like continuous multivalued functions, are appropriate models for digital morpholological operations. Connectivity preservation, unlike continuity, is preserved by compositions, and generalizes easily to higher dimensions and arbitrary adjacency relations. version:2
arxiv-1511-06052 | Putting Things in Context: Community-specific Embedding Projections for Sentiment Analysis | http://arxiv.org/abs/1511.06052 | id:1511.06052 author:Yi Yang, Jacob Eisenstein category:cs.CL cs.AI cs.SI  published:2015-11-19 summary:Variation in language is ubiquitous, and is particularly evident in newer forms of writing such as social media. Fortunately, variation is not random, but is usually linked to social factors. By exploiting linguistic homophily --- the tendency of socially linked individuals to use language similarly --- it is possible to build models that are more robust to variation. In this paper, we focus on social network communities, which make it possible to generalize sociolinguistic properties from authors in the training set to authors in the test sets, without requiring demographic author metadata. We detect communities via standard graph clustering algorithms, and then exploit these communities by learning community-specific projections of word embeddings. These projections capture shifts in word meaning in different social groups; by modeling them, we are able to improve the overall accuracy of Twitter sentiment analysis by a significant margin over competitive prior work. version:2
arxiv-1512-07502 | Convolutional Architecture Exploration for Action Recognition and Image Classification | http://arxiv.org/abs/1512.07502 | id:1512.07502 author:J. T. Turner, David Aha, Leslie Smith, Kalyan Moy Gupta category:cs.CV  published:2015-12-23 summary:Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a software package for the training, classifying, and feature extraction of images. The UCF Sports Action dataset is a widely used machine learning dataset that has 200 videos taken in 720x480 resolution of 9 different sporting activities: diving, golf, swinging, kicking, lifting, horseback riding, running, skateboarding, swinging (various gymnastics), and walking. In this report we report on a caffe feature extraction pipeline of images taken from the videos of the UCF Sports Action dataset. A similar test was performed on overfeat, and results were inferior to caffe. This study is intended to explore the architecture and hyper parameters needed for effective static analysis of action in videos and classification over a variety of image datasets. version:1
arxiv-1512-07446 | Adaptive Ensemble Learning with Confidence Bounds | http://arxiv.org/abs/1512.07446 | id:1512.07446 author:Cem Tekin, Jinsung Yoon. Mihael category:cs.LG stat.ML  published:2015-12-23 summary:Extracting actionable intelligence from distributed, heterogeneous, correlated and high-dimensional data sources requires run-time processing and learning both locally and globally. In the last decade, a large number of meta-learning techniques have been proposed in which local learners make online predictions based on their locally-collected data instances, and feed these predictions to an ensemble learner, which fuses them and issues a global prediction. However, most of these works do not provide performance guarantees or, when they do, these guarantees are asymptotic. None of these existing works provide confidence estimates about the issued predictions or rate of learning guarantees for the ensemble learner. In this paper, we provide a systematic ensemble learning method called Hedged Bandits, which comes with both long run (asymptotic) and short run (rate of learning) performance guarantees. Moreover, we show that our proposed method outperforms all existing ensemble learning techniques, even in the presence of concept drift. We illustrate the performance of Hedged Bandits in the context of medical informatics. However, the proposed methods have numerous other applications, including network monitoring and security, online recommendation systems, social networks, smart cities, etc. version:1
arxiv-1512-07422 | Adaptive Algorithms for Online Convex Optimization with Long-term Constraints | http://arxiv.org/abs/1512.07422 | id:1512.07422 author:Rodolphe Jenatton, Jim Huang, Cédric Archambeau category:stat.ML cs.LG math.OC  published:2015-12-23 summary:We present an adaptive online gradient descent algorithm to solve online convex optimization problems with long-term constraints , which are constraints that need to be satisfied when accumulated over a finite number of rounds T , but can be violated in intermediate rounds. For some user-defined trade-off parameter $\beta$ $\in$ (0, 1), the proposed algorithm achieves cumulative regret bounds of O(T^max{$\beta$,1--$\beta$}) and O(T^(1--$\beta$/2)) for the loss and the constraint violations respectively. Our results hold for convex losses and can handle arbitrary convex constraints without requiring knowledge of the number of rounds in advance. Our contributions improve over the best known cumulative regret bounds by Mahdavi, et al. (2012) that are respectively O(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and O(T^2/3) when further restricting to polyhedral domains. We supplement the analysis with experiments validating the performance of our algorithm in practice. version:1
arxiv-1507-01073 | Convex Factorization Machine for Regression | http://arxiv.org/abs/1507.01073 | id:1507.01073 author:Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Suleiman A Khan, Samuel Kaski, Hiroshi Mamitsuka, Yi Chang category:stat.ML cs.LG  published:2015-07-04 summary:We propose the convex factorization machine (CFM), which is a convex variant of the widely used Factorization Machines (FMs). Specifically, we employ a linear+quadratic model and regularize the linear term with the $\ell_2$-regularizer and the quadratic term with the trace norm regularizer. Then, we formulate the CFM optimization as a semidefinite programming problem and propose an efficient optimization procedure with Hazan's algorithm. A key advantage of CFM over existing FMs is that it can find a globally optimal solution, while FMs may get a poor locally optimal solution since the objective function of FMs is non-convex. In addition, the proposed algorithm is simple yet effective and can be implemented easily. Finally, CFM is a general factorization method and can also be used for other factorization problems including multi-view matrix factorization problems. Through synthetic and movielens datasets, we first show that the proposed CFM achieves results competitive to FMs. Furthermore, in a toxicogenomics prediction task, we show that CFM outperforms a state-of-the-art tensor factorization method. version:3
arxiv-1512-07344 | A Deep Generative Deconvolutional Image Model | http://arxiv.org/abs/1512.07344 | id:1512.07344 author:Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin category:cs.CV cs.LG stat.ML  published:2015-12-23 summary:A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic {\em unpooling} is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks. version:1
arxiv-1512-07336 | Latent Variable Modeling with Diversity-Inducing Mutual Angular Regularization | http://arxiv.org/abs/1512.07336 | id:1512.07336 author:Pengtao Xie, Yuntian Deng, Eric Xing category:cs.LG stat.ML  published:2015-12-23 summary:Latent Variable Models (LVMs) are a large family of machine learning models providing a principled and effective way to extract underlying patterns, structure and knowledge from observed data. Due to the dramatic growth of volume and complexity of data, several new challenges have emerged and cannot be effectively addressed by existing LVMs: (1) How to capture long-tail patterns that carry crucial information when the popularity of patterns is distributed in a power-law fashion? (2) How to reduce model complexity and computational cost without compromising the modeling power of LVMs? (3) How to improve the interpretability and reduce the redundancy of discovered patterns? To addresses the three challenges discussed above, we develop a novel regularization technique for LVMs, which controls the geometry of the latent space during learning to enable the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually different from each other, to accomplish long-tail coverage, low redundancy, and better interpretability. We propose a mutual angular regularizer (MAR) to encourage the components in LVMs to have larger mutual angles. The MAR is non-convex and non-smooth, entailing great challenges for optimization. To cope with this issue, we derive a smooth lower bound of the MAR and optimize the lower bound instead. We show that the monotonicity of the lower bound is closely aligned with the MAR to qualify the lower bound as a desirable surrogate of the MAR. Using neural network (NN) as an instance, we analyze how the MAR affects the generalization performance of NN. On two popular latent variable models --- restricted Boltzmann machine and distance metric learning, we demonstrate that MAR can effectively capture long-tail patterns, reduce model complexity without sacrificing expressivity and improve interpretability. version:1
arxiv-1412-8285 | Marginal likelihood and model selection for Gaussian latent tree and forest models | http://arxiv.org/abs/1412.8285 | id:1412.8285 author:Mathias Drton, Shaowei Lin, Luca Weihs, Piotr Zwiernik category:stat.ME math.ST stat.ML stat.TH  published:2014-12-29 summary:Gaussian latent tree models, or more generally, Gaussian latent forest models have Fisher-information matrices that become singular along interesting submodels, namely, models that correspond to subforests. For these singularities, we compute the real log-canonical thresholds (also known as stochastic complexities or learning coefficients) that quantify the large-sample behavior of the marginal likelihood in Bayesian inference. This provides the information needed for a recently introduced generalization of the Bayesian information criterion. Our mathematical developments treat the general setting of Laplace integrals whose phase functions are sums of squared differences between monomials and constants. We clarify how in this case real log-canonical thresholds can be computed using polyhedral geometry, and we show how to apply the general theory to the Laplace integrals associated with Gaussian latent tree and forest models. In simulations and a data example, we demonstrate how the mathematical knowledge can be applied in model selection. version:2
arxiv-1512-07331 | Plug-and-Play Priors for Bright Field Electron Tomography and Sparse Interpolation | http://arxiv.org/abs/1512.07331 | id:1512.07331 author:Suhas Sreehari, S. V. Venkatakrishnan, Brendt Wohlberg, Lawrence F. Drummy, Jeffrey P. Simmons, Charles A. Bouman category:cs.CV  published:2015-12-23 summary:Many material and biological samples in scientific imaging are characterized by non-local repeating structures. These are studied using scanning electron microscopy and electron tomography. Sparse sampling of individual pixels in a 2D image acquisition geometry, or sparse sampling of projection images with large tilt increments in a tomography experiment, can enable high speed data acquisition and minimize sample damage caused by the electron beam. In this paper, we present an algorithm for electron tomographic reconstruction and sparse image interpolation that exploits the non-local redundancy in images. We adapt a framework, termed plug-and-play (P&P) priors, to solve these imaging problems in a regularized inversion setting. The power of the P&P approach is that it allows a wide array of modern denoising algorithms to be used as a "prior model" for tomography and image interpolation. We also present sufficient mathematical conditions that ensure convergence of the P&P approach, and we use these insights to design a new non-local means denoising algorithm. Finally, we demonstrate that the algorithm produces higher quality reconstructions on both simulated and real electron microscope data, along with improved convergence properties compared to other methods. version:1
arxiv-1512-07314 | Mid-level Representation for Visual Recognition | http://arxiv.org/abs/1512.07314 | id:1512.07314 author:Moin Nabi category:cs.CV  published:2015-12-23 summary:Visual Recognition is one of the fundamental challenges in AI, where the goal is to understand the semantics of visual data. Employing mid-level representation, in particular, shifted the paradigm in visual recognition. The mid-level image/video representation involves discovering and training a set of mid-level visual patterns (e.g., parts and attributes) and represent a given image/video utilizing them. The mid-level patterns can be extracted from images and videos using the motion and appearance information of visual phenomenas. This thesis targets employing mid-level representations for different high-level visual recognition tasks, namely (i)image understanding and (ii)video understanding. In the case of image understanding, we focus on object detection/recognition task. We investigate on discovering and learning a set of mid-level patches to be used for representing the images of an object category. We specifically employ the discriminative patches in a subcategory-aware webly-supervised fashion. We, additionally, study the outcomes provided by employing the subcategory-based models for undoing dataset bias. version:1
arxiv-1512-07281 | Topical differences between Chinese language Twitter and Sina Weibo | http://arxiv.org/abs/1512.07281 | id:1512.07281 author:Qian Zhang, Bruno Gonçalves category:cs.SI cs.CL physics.soc-ph  published:2015-12-22 summary:Sina Weibo, China's most popular microblogging platform, is currently used by over $500M$ users and is considered to be a proxy of Chinese social life. In this study, we contrast the discussions occurring on Sina Weibo and on Chinese language Twitter in order to observe two different strands of Chinese culture: people within China who use Sina Weibo with its government imposed restrictions and those outside that are free to speak completely anonymously. We first propose a simple ad-hoc algorithm to identify topics of Tweets and Weibo. Different from previous works on micro-message topic detection, our algorithm considers topics of the same contents but with different \#tags. Our algorithm can also detect topics for Tweets and Weibos without any \#tags. Using a large corpus of Weibo and Chinese language tweets, covering the period from January $1$ to December $31$, $2012$, we obtain a list of topics using clustered \#tags that we can then use to compare the two platforms. Surprisingly, we find that there are no common entries among the Top $100$ most popular topics. Furthermore, only $9.2\%$ of tweets correspond to the Top $1000$ topics on Sina Weibo platform, and conversely only $4.4\%$ of weibos were found to discuss the most popular Twitter topics. Our results reveal significant differences in social attention on the two platforms, with most popular topics on Sina Weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in Sina Weibo. version:1
arxiv-1507-07583 | Relating Cascaded Random Forests to Deep Convolutional Neural Networks for Semantic Segmentation | http://arxiv.org/abs/1507.07583 | id:1507.07583 author:David L. Richmond, Dagmar Kainmueller, Michael Y. Yang, Eugene W. Myers, Carsten Rother category:cs.CV  published:2015-07-27 summary:We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Random Forests (RF) and Neural Networks (NN). The main contribution of this work is to explore the relationship between two special forms of these techniques: stacked RFs and deep Convolutional Neural Networks (CNN). We show that there exists a mapping from stacked RF to deep CNN, and an approximate mapping back. This insight gives two major practical benefits: Firstly, deep CNNs can be intelligently constructed and initialized, which is crucial when dealing with a limited amount of training data. Secondly, it can be utilized to create a new stacked RF with improved performance. Furthermore, this mapping yields a new CNN architecture, that is well suited for pixel-wise semantic labeling. We experimentally verify these practical benefits for two different application scenarios in computer vision and biology, where the layout of parts is important: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. version:2
arxiv-1411-3972 | Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components | http://arxiv.org/abs/1411.3972 | id:1411.3972 author:Philipp Geiger, Kun Zhang, Mingming Gong, Dominik Janzing, Bernhard Schölkopf category:stat.ML  published:2014-11-14 summary:A widely applied approach to causal inference from a non-experimental time series $X$, often referred to as "(linear) Granger causal analysis", is to regress present on past and interpret the regression matrix $\hat{B}$ causally. However, if there is an unmeasured time series $Z$ that influences $X$, then this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if one had additional information such as $Z$. In this paper we take a different approach: We assume that $X$ together with some hidden $Z$ forms a first order vector autoregressive (VAR) process with transition matrix $A$, and argue why it is more valid to interpret $A$ causally instead of $\hat{B}$. Then we examine under which conditions the most important parts of $A$ are identifiable or almost identifiable from only $X$. Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from $X$ to $Z$. We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using $X$. version:4
arxiv-1503-01428 | Probabilistic Label Relation Graphs with Ising Models | http://arxiv.org/abs/1503.01428 | id:1503.01428 author:Nan Ding, Jia Deng, Kevin Murphy, Hartmut Neven category:cs.LG  published:2015-03-04 summary:We consider classification problems in which the label space has structure. A common example is hierarchical label spaces, corresponding to the case where one label subsumes another (e.g., animal subsumes dog). But labels can also be mutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To jointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy and exclusion) graph was introduced in [7]. This combined a conditional random field (CRF) with a deep neural network (DNN), resulting in state of the art results when applied to visual object classification problems where the training labels were drawn from different levels of the ImageNet hierarchy (e.g., an image might be labeled with the basic level category "dog", rather than the more specific label "husky"). In this paper, we extend the HEX model to allow for soft or probabilistic relations between labels, which is useful when there is uncertainty about the relationship between two labels (e.g., an antelope is "sort of" furry, but not to the same degree as a grizzly bear). We call our new model pHEX, for probabilistic HEX. We show that the pHEX graph can be converted to an Ising model, which allows us to use existing off-the-shelf inference methods (in contrast to the HEX method, which needed specialized inference algorithms). Experimental results show significant improvements in a number of large-scale visual object classification tasks, outperforming the previous HEX model. version:3
arxiv-1512-07155 | Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web | http://arxiv.org/abs/1512.07155 | id:1512.07155 author:Shugao Ma, Sarah Adel Bargal, Jianming Zhang, Leonid Sigal, Stan Sclaroff category:cs.CV  published:2015-12-22 summary:Recently, attempts have been made to collect millions of videos to train CNN models for action recognition in videos. However, curating such large-scale video datasets requires immense human labor, and training CNNs on millions of videos demands huge computational resources. In contrast, collecting action images from the Web is much easier and training on images requires much less computation. In addition, labeled web images tend to contain discriminative action poses, which highlight discriminative portions of a video's temporal progression. We explore the question of whether we can utilize web action images to train better CNN models for action recognition in videos. We collect 23.8K manually filtered images from the Web that depict the 101 actions in the UCF101 action video dataset. We show that by utilizing web action images along with videos in training, significant performance boosts of CNN models can be achieved. We then investigate the scalability of the process by leveraging crawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M video frames by 393K unfiltered images and get comparable performance. version:1
arxiv-1512-07146 | Refined Error Bounds for Several Learning Algorithms | http://arxiv.org/abs/1512.07146 | id:1512.07146 author:Steve Hanneke category:cs.LG math.ST stat.ML stat.TH  published:2015-12-22 summary:This article studies the achievable guarantees on the error rates of certain learning algorithms, with particular focus on refining logarithmic factors. Many of the results are based on a general technique for obtaining bounds on the error rates of sample-consistent classifiers with monotonic error regions, in the realizable case. We prove bounds of this type expressed in terms of either the VC dimension or the sample compression size. This general technique also enables us to derive several new bounds on the error rates of general sample-consistent learning algorithms, as well as refined bounds on the label complexity of the CAL active learning algorithm. Additionally, we establish a simple necessary and sufficient condition for the existence of a distribution-free bound on the error rates of all sample-consistent learning rules, converging at a rate inversely proportional to the sample size. We also study learning in the presence of classification noise, deriving a new excess error rate guarantee for general VC classes under Tsybakov's noise condition, and establishing a simple and general necessary and sufficient condition for the minimax excess risk under bounded noise to converge at a rate inversely proportional to the sample size. version:1
arxiv-1512-07143 | SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation | http://arxiv.org/abs/1512.07143 | id:1512.07143 author:Mariella Dimiccoli, Marc Bolaños, Estefania Talavera, Maedeh Aghaei, Stavri G. Nikolov, Petia Radeva category:cs.AI cs.CV  published:2015-12-22 summary:While wearable cameras are becoming increasingly popular, locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming processes. This paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments. First, contextual and semantic information is extracted for each image by employing a Convolutional Neural Networks approach. Later, by integrating language processing, a vocabulary of concepts is defined in a semantic space. Finally, by exploiting the temporal coherence in photo streams, images which share contextual and semantic attributes are grouped together. The resulting temporal segmentation is particularly suited for further analysis, ranging from activity and event recognition to semantic indexing and summarization. Experiments over egocentric sets of nearly 17,000 images, show that the proposed approach outperforms state-of-the-art methods. version:1
arxiv-1504-00284 | A New Vision of Collaborative Active Learning | http://arxiv.org/abs/1504.00284 | id:1504.00284 author:Adrian Calma, Tobias Reitmaier, Bernhard Sick, Paul Lukowicz, Mark Embrechts category:cs.LG stat.ML  published:2015-04-01 summary:Active learning (AL) is a learning paradigm where an active learner has to train a model (e.g., a classifier) which is in principal trained in a supervised way, but in AL it has to be done by means of a data set with initially unlabeled samples. To get labels for these samples, the active learner has to ask an oracle (e.g., a human expert) for labels. The goal is to maximize the performance of the model and to minimize the number of queries at the same time. In this article, we first briefly discuss the state of the art and own, preliminary work in the field of AL. Then, we propose the concept of collaborative active learning (CAL). With CAL, we will overcome some of the harsh limitations of current AL. In particular, we envision scenarios where an expert may be wrong for various reasons, there might be several or even many experts with different expertise, the experts may label not only samples but also knowledge at a higher level such as rules, and we consider that the labeling costs depend on many conditions. Moreover, in a CAL process human experts will profit by improving their own knowledge, too. version:3
arxiv-1406-1780 | A Comprehensive Approach to Mode Clustering | http://arxiv.org/abs/1406.1780 | id:1406.1780 author:Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman category:stat.ME stat.ML  published:2014-06-06 summary:Mode clustering is a nonparametric method for clustering that defines clusters using the basins of attraction of a density estimator's modes. We provide several enhancements to mode clustering: (i) a soft variant of cluster assignment, (ii) a measure of connectivity between clusters, (iii) a technique for choosing the bandwidth, (iv) a method for denoising small clusters, and (v) an approach to visualizing the clusters. Combining all these enhancements gives us a complete procedure for clustering in multivariate problems. We also compare mode clustering to other clustering methods in several examples version:4
arxiv-1512-07108 | Recent Advances in Convolutional Neural Networks | http://arxiv.org/abs/1512.07108 | id:1512.07108 author:Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang category:cs.CV cs.LG cs.NE  published:2015-12-22 summary:In the last few years, deep learning has lead to very good performance on a variety of problems, such as object recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Due to the lack of training data and computing power in early days, it is hard to train a large high-capacity convolutional neural network without overfitting. Recently, with the rapid growth of data size and the increasing power of graphics processor unit, many researchers have improved the convolutional neural networks and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. Besides, we also introduce some applications of convolutional neural networks in computer vision. version:1
arxiv-1505-06072 | Diffusion Methods for Classification with Pairwise Relationships | http://arxiv.org/abs/1505.06072 | id:1505.06072 author:Pedro F. Felzenszwalb, Benar F. Svaiter category:cs.AI cs.CV  published:2015-05-22 summary:We define two algorithms for propagating information in classification problems with pairwise relationships. The algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs. The approach is also related to message passing algorithms, including belief propagation and mean field methods. The algorithms we describe are guaranteed to converge on graphs with arbitrary topology. Moreover they always converge to a unique fixed point, independent of initialization. We prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a Markov random field. The theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon Markov decision process. We illustrate the practical application of the algorithms under study with numerical experiments in image restoration, stereo depth estimation and binary classification on a grid. version:3
arxiv-1508-04945 | DeepWriterID: An End-to-end Online Text-independent Writer Identification System | http://arxiv.org/abs/1508.04945 | id:1508.04945 author:Weixin Yang, Lianwen Jin, Manfei Liu category:cs.CV cs.LG stat.ML  published:2015-08-20 summary:Owing to the rapid growth of touchscreen mobile terminals and pen-based interfaces, handwriting-based writer identification systems are attracting increasing attention for personal authentication, digital forensics, and other applications. However, most studies on writer identification have not been satisfying because of the insufficiency of data and difficulty of designing good features under various conditions of handwritings. Hence, we introduce an end-to-end system, namely DeepWriterID, employed a deep convolutional neural network (CNN) to address these problems. A key feature of DeepWriterID is a new method we are proposing, called DropSegment. It designs to achieve data augmentation and improve the generalized applicability of CNN. For sufficient feature representation, we further introduce path signature feature maps to improve performance. Experiments were conducted on the NLPR handwriting database. Even though we only use pen-position information in the pen-down state of the given handwriting samples, we achieved new state-of-the-art identification rates of 95.72% for Chinese text and 98.51% for English text. version:2
arxiv-1511-09033 | The Multiverse Loss for Robust Transfer Learning | http://arxiv.org/abs/1511.09033 | id:1511.09033 author:Etai Littwin, Lior Wolf category:cs.CV  published:2015-11-29 summary:Deep learning techniques are renowned for supporting effective transfer learning. However, as we demonstrate, the transferred representations support only a few modes of separation and much of its dimensionality is unutilized. In this work, we suggest to learn, in the source domain, multiple orthogonal classifiers. We prove that this leads to a reduced rank representation, which, however, supports more discriminative directions. Interestingly, the softmax probabilities produced by the multiple classifiers are likely to be identical. Experimental results, on CIFAR-100 and LFW, further demonstrate the effectiveness of our method. version:2
arxiv-1512-07080 | Cost-based Feature Transfer for Vehicle Occupant Classification | http://arxiv.org/abs/1512.07080 | id:1512.07080 author:Toby Perrett, Majid Mirmehdi, Eduardo Dias category:cs.CV I.4.9  published:2015-12-22 summary:Knowledge of human presence and interaction in a vehicle is of growing interest to vehicle manufacturers for design and safety purposes. We present a framework to perform the tasks of occupant detection and occupant classification for automatic child locks and airbag suppression. It operates for all passenger seats, using a single overhead camera. A transfer learning technique is introduced to make full use of training data from all seats whilst still maintaining some control over the bias, necessary for a system designed to penalize certain misclassifications more than others. An evaluation is performed on a challenging dataset with both weighted and unweighted classifiers, demonstrating the effectiveness of the transfer process. version:1
arxiv-1512-07074 | Move from Perturbed scheme to exponential weighting average | http://arxiv.org/abs/1512.07074 | id:1512.07074 author:Chunyang Xiao category:cs.LG  published:2015-12-22 summary:In an online decision problem, one makes decisions often with a pool of decision sequence called experts but without knowledge of the future. After each step, one pays a cost based on the decision and observed rate. One reasonal goal would be to perform as well as the best expert in the pool. The modern and well-known way to attain this goal is the algorithm of exponential weighting. However, recently, another algorithm called follow the perturbed leader is developed and achieved about the same performance. In our work, we first show the properties shared in common by the two algorithms which explain the similarities on the performance. Next we will show that for a specific perturbation, the two algorithms are identical. Finally, we show with some examples that follow-the-leader style algorithms extend naturally to a large class of structured online problems for which the exponential algorithms are inefficient. version:1
arxiv-1506-05274 | Partial Functional Correspondence | http://arxiv.org/abs/1506.05274 | id:1506.05274 author:Emanuele Rodolà, Luca Cosmo, Michael M. Bronstein, Andrea Torsello, Daniel Cremers category:cs.CV  published:2015-06-17 summary:In this paper, we propose a method for computing partial functional correspondence between non-rigid shapes. We use perturbation analysis to show how removal of shape parts changes the Laplace-Beltrami eigenfunctions, and exploit it as a prior on the spectral representation of the correspondence. Corresponding parts are optimization variables in our problem and are used to weight the functional correspondence; we are looking for the largest and most regular (in the Mumford-Shah sense) parts that minimize correspondence distortion. We show that our approach can cope with very challenging correspondence settings. version:2
arxiv-1512-07046 | News Across Languages - Cross-Lingual Document Similarity and Event Tracking | http://arxiv.org/abs/1512.07046 | id:1512.07046 author:Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz Skraba, Blaz Fortuna, Marko Grobelnik category:cs.IR cs.CL  published:2015-12-22 summary:In today's world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm. version:1
arxiv-1512-07041 | Implementation of deep learning algorithm for automatic detection of brain tumors using intraoperative IR-thermal mapping data | http://arxiv.org/abs/1512.07041 | id:1512.07041 author:A. V. Makarenko, M. G. Volovik category:cs.CV cs.LG q-bio.QM stat.ML I.5.1; I.4.8; J.3  published:2015-12-22 summary:The efficiency of deep machine learning for automatic delineation of tumor areas has been demonstrated for intraoperative neuronavigation using active IR-mapping with the use of the cold test. The proposed approach employs a matrix IR-imager to remotely register the space-time distribution of surface temperature pattern, which is determined by the dynamics of local cerebral blood flow. The advantages of this technique are non-invasiveness, zero risks for the health of patients and medical staff, low implementation and operational costs, ease and speed of use. Traditional IR-diagnostic technique has a crucial limitation - it involves a diagnostician who determines the boundaries of tumor areas, which gives rise to considerable uncertainty, which can lead to diagnosis errors that are difficult to control. The current study demonstrates that implementing deep learning algorithms allows to eliminate the explained drawback. version:1
arxiv-1512-07030 | Deep Learning with S-shaped Rectified Linear Activation Units | http://arxiv.org/abs/1512.07030 | id:1512.07030 author:Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, Shuicheng Yan category:cs.CV  published:2015-12-22 summary:Rectified linear activation units are important components for state-of-the-art deep convolutional networks. In this paper, we propose a novel S-shaped rectified linear activation unit (SReLU) to learn both convex and non-convex functions, imitating the multiple function forms given by the two fundamental laws, namely the Webner-Fechner law and the Stevens law, in psychophysics and neural sciences. Specifically, SReLU consists of three piecewise linear functions, which are formulated by four learnable parameters. The SReLU is learned jointly with the training of the whole deep network through back propagation. During the training phase, to initialize SReLU in different layers, we propose a "freezing" method to degenerate SReLU into a predefined leaky rectified linear unit in the initial several training epochs and then adaptively learn the good initial values. SReLU can be universally used in the existing deep networks with negligible additional parameters and computation cost. Experiments with two popular CNN architectures, Network in Network and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100, MNIST and ImageNet demonstrate that SReLU achieves remarkable improvement compared to other activation functions. version:1
arxiv-1512-05670 | Towards automating the generation of derivative nouns in Sanskrit by simulating Panini | http://arxiv.org/abs/1512.05670 | id:1512.05670 author:Amrith Krishna, Pawan Goyal category:cs.CL  published:2015-12-17 summary:About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal with generation of derivative nouns, making it one of the largest topical sections in Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76. This section is a systematic arrangement of rules that enumerates various affixes that are used in the derivation under specific semantic relations. We propose a system that automates the process of generation of derivative nouns as per the rules in Astadhyayi. The proposed system follows a completely object oriented approach, that models each rule as a class of its own and then groups them as rule groups. The rule groups are decided on the basis of selective grouping of rules by virtue of anuvrtti. The grouping of rules results in an inheritance network of rules which is a directed acyclic graph. Every rule group has a head rule and the head rule notifies all the direct member rules of the group about the environment which contains all the details about data entities, participating in the derivation process. The system implements this mechanism using multilevel inheritance and observer design patterns. The system focuses not only on generation of the desired final form, but also on the correctness of sequence of rules applied to make sure that the derivation has taken place in strict adherence to Astadhyayi. The proposed system's design allows to incorporate various conflict resolution methods mentioned in authentic texts and hence the effectiveness of those rules can be validated with the results from the system. We also present cases where we have checked the applicability of the system with the rules which are not specifically applicable to derivation of derivative nouns, in order to see the effectiveness of the proposed schema as a generic system for modeling Astadhyayi. version:2
arxiv-1512-06999 | FAASTA: A fast solver for total-variation regularization of ill-conditioned problems with application to brain imaging | http://arxiv.org/abs/1512.06999 | id:1512.06999 author:Gaël Varoquaux, Michael Eickenberg, Elvis Dohmatob, Bertand Thirion category:q-bio.NC cs.LG stat.CO stat.ML  published:2015-12-22 summary:The total variation (TV) penalty, as many other analysis-sparsity problems, does not lead to separable factors or a proximal operatorwith a closed-form expression, such as soft thresholding for the $\ell\_1$ penalty. As a result, in a variational formulation of an inverse problem or statisticallearning estimation, it leads to challenging non-smooth optimization problemsthat are often solved with elaborate single-step first-order methods. When thedata-fit term arises from empirical measurements, as in brain imaging, it isoften very ill-conditioned and without simple structure. In this situation, in proximal splitting methods, the computation cost of thegradient step can easily dominate each iteration. Thus it is beneficialto minimize the number of gradient steps.We present fAASTA, a variant of FISTA, that relies on an internal solver forthe TV proximal operator, and refines its tolerance to balance computationalcost of the gradient and the proximal steps. We give benchmarks andillustrations on "brain decoding": recovering brain maps from noisymeasurements to predict observed behavior. The algorithm as well as theempirical study of convergence speed are valuable for any non-exact proximaloperator, in particular analysis-sparsity problems. version:1
arxiv-1512-06992 | On the Differential Privacy of Bayesian Inference | http://arxiv.org/abs/1512.06992 | id:1512.06992 author:Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis category:cs.AI cs.CR cs.LG math.ST stat.ML stat.TH  published:2015-12-22 summary:We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on proba-bilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na{\"i}ve Bayes and Bayesian linear regression illustrate the application of our mechanisms. version:1
arxiv-1512-06963 | Multi-Instance Visual-Semantic Embedding | http://arxiv.org/abs/1512.06963 | id:1512.06963 author:Zhou Ren, Hailin Jin, Zhe Lin, Chen Fang, Alan Yuille category:cs.CV  published:2015-12-22 summary:Visual-semantic embedding models have been recently proposed and shown to be effective for image classification and zero-shot learning, by mapping images into a continuous semantic label space. Although several approaches have been proposed for single-label embedding tasks, handling images with multiple labels (which is a more general setting) still remains an open problem, mainly due to the complex underlying corresponding relationship between image and its labels. In this work, we present Multi-Instance visual-semantic Embedding model (MIE) for embedding images associated with either single or multiple labels. Our model discovers and maps semantically-meaningful image subregions to their corresponding labels. And we demonstrate the superiority of our method over the state-of-the-art on two tasks, including multi-label image annotation and zero-shot learning. version:1
arxiv-1512-05742 | A Survey of Available Corpora for Building Data-Driven Dialogue Systems | http://arxiv.org/abs/1512.05742 | id:1512.05742 author:Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau category:cs.CL cs.AI cs.HC cs.LG stat.ML I.2.6; I.2.7; I.2.1  published:2015-12-17 summary:During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets and how they can be used to learn diverse dialogue strategies. We also describe other potential uses of these datasets, such as methods for transfer learning between datasets and the use of external knowledge, and discuss appropriate choice of evaluation metrics for the learning objective. version:2
arxiv-1507-08750 | Action-Conditional Video Prediction using Deep Networks in Atari Games | http://arxiv.org/abs/1507.08750 | id:1507.08750 author:Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh category:cs.LG cs.AI cs.CV  published:2015-07-31 summary:Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs. version:2
arxiv-1512-06929 | Facility Deployment Decisions through Warp Optimizaton of Regressed Gaussian Processes | http://arxiv.org/abs/1512.06929 | id:1512.06929 author:Anthony Scopatz category:math.OC physics.data-an stat.ML  published:2015-12-22 summary:A method for quickly determining deployment schedules that meet a given fuel cycle demand is presented here. This algorithm is fast enough to perform in situ within low-fidelity fuel cycle simulators. It uses Gaussian process regression models to predict the production curve as a function of time and the number of deployed facilities. Each of these predictions is measured against the demand curve using the dynamic time warping distance. The minimum distance deployment schedule is evaluated in a full fuel cycle simulation, whose generated production curve then informs the model on the next optimization iteration. The method converges within five to ten iterations to a distance that is less than one percent of the total deployable production. A representative once-through fuel cycle is used to demonstrate the methodology for reactor deployment. version:1
arxiv-1512-06925 | Transformed Residual Quantization for Approximate Nearest Neighbor Search | http://arxiv.org/abs/1512.06925 | id:1512.06925 author:Jiangbo Yuan, Xiuwen Liu category:cs.CV  published:2015-12-22 summary:The success of product quantization (PQ) for fast nearest neighbor search depends on the exponentially reduced complexities of both storage and computation with respect to the codebook size. Recent efforts have been focused on employing sophisticated optimization strategies, or seeking more effective models. Residual quantization (RQ) is such an alternative that holds the same property as PQ in terms of the aforementioned complexities. In addition to being a direct replacement of PQ, hybrids of PQ and RQ can yield more gains for approximate nearest neighbor search. This motivated us to propose a novel approach to optimizing RQ and the related hybrid models. With an observation of the general randomness increase in a residual space, we propose a new strategy that jointly learns a local transformation per residual cluster with an ultimate goal to reduce overall quantization errors. We have shown that our approach can achieve significantly better accuracy on nearest neighbor search than both the original and the optimized PQ on several very large scale benchmarks. version:1
arxiv-1512-06900 | Predicting the Co-Evolution of Event and Knowledge Graphs | http://arxiv.org/abs/1512.06900 | id:1512.06900 author:Cristóbal Esteban, Volker Tresp, Yinchong Yang, Stephan Baier, Denis Krompaß category:cs.LG  published:2015-12-21 summary:Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Knowledge graphs are typically treated as static: A knowledge graph grows more links when more facts become available but the ground truth values associated with links is considered time invariant. In this paper we address the issue of knowledge graphs where triple states depend on time. We assume that changes in the knowledge graph always arrive in form of events, in the sense that the events are the gateway to the knowledge graph. We train an event prediction model which uses both knowledge graph background information and information on recent events. By predicting future events, we also predict likely changes in the knowledge graph and thus obtain a model for the evolution of the knowledge graph as well. Our experiments demonstrate that our approach performs well in a clinical application, a recommendation engine and a sensor network application. version:1
arxiv-1507-04635 | Black-Box Policy Search with Probabilistic Programs | http://arxiv.org/abs/1507.04635 | id:1507.04635 author:Jan-Willem van de Meent, David Tolpin, Brooks Paige, Frank Wood category:stat.ML cs.AI  published:2015-07-16 summary:In this work, we explore how probabilistic programs can be used to represent policies in sequential decision problems. In this formulation, a probabilistic program is a black-box stochastic simulator for both the problem domain and the agent. We relate classic policy gradient techniques to recently introduced black-box variational methods which generalize to probabilistic program inference. We present case studies in the Canadian traveler problem, Rock Sample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study illustrates how programs can efficiently represent policies using moderate numbers of parameters. version:3
arxiv-1505-03442 | Noncrossing Ordinal Classification | http://arxiv.org/abs/1505.03442 | id:1505.03442 author:Xingye Qiao category:stat.ML stat.CO 62H30  published:2015-05-13 summary:Ordinal data are often seen in real applications. Regular multicategory classification methods are not designed for this data type and a more proper treatment is needed. We consider a framework of ordinal classification which pools the results from binary classifiers together. An inherent difficulty of this framework is that the class prediction can be ambiguous due to boundary crossing. To fix this issue, we propose a noncrossing ordinal classification method which materializes the framework by imposing noncrossing constraints. An asymptotic study of the proposed method is conducted. We show by simulated and data examples that the proposed method can improve the classification performance for ordinal data without the ambiguity caused by boundary crossings. version:3
arxiv-1512-06790 | Car Segmentation and Pose Estimation using 3D Object Models | http://arxiv.org/abs/1512.06790 | id:1512.06790 author:Siddharth Mahendran, René Vidal category:cs.CV  published:2015-12-21 summary:Image segmentation and 3D pose estimation are two key cogs in any algorithm for scene understanding. However, state-of-the-art CRF-based models for image segmentation rely mostly on 2D object models to construct top-down high-order potentials. In this paper, we propose new top-down potentials for image segmentation and pose estimation based on the shape and volume of a 3D object model. We show that these complex top-down potentials can be easily decomposed into standard forms for efficient inference in both the segmentation and pose estimation tasks. Experiments on a car dataset show that knowledge of segmentation helps perform pose estimation better and vice versa. version:1
arxiv-1512-06789 | Information-Theoretic Bounded Rationality | http://arxiv.org/abs/1512.06789 | id:1512.06789 author:Pedro A. Ortega, Daniel A. Braun, Justin Dyer, Kee-Eung Kim, Naftali Tishby category:stat.ML cs.AI cs.SY math.OC  published:2015-12-21 summary:Bounded rationality, that is, decision-making and planning under resource limitations, is widely regarded as an important open problem in artificial intelligence, reinforcement learning, computational neuroscience and economics. This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas. We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions. This functional possesses three crucial properties: it controls the size of the solution space; it has Monte Carlo planners that are exact, yet bypass the need for exhaustive search; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions. We discuss the single-step decision-making case, and show how to extend it to sequential decisions using equivalence transformations. This extension yields a very general class of decision problems that encompass classical decision rules (e.g. EXPECTIMAX and MINIMAX) as limit cases, as well as trust- and risk-sensitive planning. version:1
arxiv-1512-06785 | Beyond Classification: Latent User Interests Profiling from Visual Contents Analysis | http://arxiv.org/abs/1512.06785 | id:1512.06785 author:Longqi Yang, Cheng-Kang Hsieh, Deborah Estrin category:cs.IR cs.CV cs.SI  published:2015-12-21 summary:User preference profiling is an important task in modern online social networks (OSN). With the proliferation of image-centric social platforms, such as Pinterest, visual contents have become one of the most informative data streams for understanding user preferences. Traditional approaches usually treat visual content analysis as a general classification problem where one or more labels are assigned to each image. Although such an approach simplifies the process of image analysis, it misses the rich context and visual cues that play an important role in people's perception of images. In this paper, we explore the possibilities of learning a user's latent visual preferences directly from image contents. We propose a distance metric learning method based on Deep Convolutional Neural Networks (CNN) to directly extract similarity information from visual contents and use the derived distance metric to mine individual users' fine-grained visual preferences. Through our preliminary experiments using data from 5,790 Pinterest users, we show that even for the images within the same category, each user possesses distinct and individually-identifiable visual preferences that are consistent over their lifetime. Our results underscore the untapped potential of finer-grained visual preference profiling in understanding users' preferences. version:1
arxiv-1512-06730 | Multilinear Subspace Clustering | http://arxiv.org/abs/1512.06730 | id:1512.06730 author:Eric Kernfeld, Nathan Majumder, Shuchin Aeron, Misha Kilmer category:cs.IT cs.CV cs.LG math.IT stat.ML  published:2015-12-21 summary:In this paper we present a new model and an algorithm for unsupervised clustering of 2-D data such as images. We assume that the data comes from a union of multilinear subspaces (UOMS) model, which is a specific structured case of the much studied union of subspaces (UOS) model. For segmentation under this model, we develop Multilinear Subspace Clustering (MSC) algorithm and evaluate its performance on the YaleB and Olivietti image data sets. We show that MSC is highly competitive with existing algorithms employing the UOS model in terms of clustering performance while enjoying improvement in computational complexity. version:1
arxiv-1512-06709 | Sparse Coding with Fast Image Alignment via Large Displacement Optical Flow | http://arxiv.org/abs/1512.06709 | id:1512.06709 author:Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV  published:2015-12-21 summary:Sparse representation-based classifiers have shown outstanding accuracy and robustness in image classification tasks even with the presence of intense noise and occlusion. However, it has been discovered that the performance degrades significantly either when test image is not aligned with the dictionary atoms or the dictionary atoms themselves are not aligned with each other, in which cases the sparse linear representation assumption fails. In this paper, having both training and test images misaligned, we introduce a novel sparse coding framework that is able to efficiently adapt the dictionary atoms to the test image via large displacement optical flow. In the proposed algorithm, every dictionary atom is automatically aligned with the input image and the sparse code is then recovered using the adapted dictionary atoms. A corresponding supervised dictionary learning algorithm is also developed for the proposed framework. Experimental results on digit datasets recognition verify the efficacy and robustness of the proposed algorithm. version:1
arxiv-1505-06027 | Weakly-Supervised Alignment of Video With Text | http://arxiv.org/abs/1505.06027 | id:1505.06027 author:Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid category:cs.CV cs.CL  published:2015-05-22 summary:Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time stamp for every sentence. Given vectorial features for both video and text, we propose to cast this task as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [7], we evaluate our method on a challenging dataset of videos with associated textual descriptions [36], using both bag-of-words and continuous representations for text. version:2
arxiv-1512-06643 | The 2015 Sheffield System for Transcription of Multi-Genre Broadcast Media | http://arxiv.org/abs/1512.06643 | id:1512.06643 author:Oscar Saz, Mortaza Doulaty, Salil Deena, Rosanna Milner, Raymond W. M. Ng, Madina Hasan, Yulan Liu, Thomas Hain category:cs.CL  published:2015-12-21 summary:We describe the University of Sheffield system for participation in the 2015 Multi-Genre Broadcast (MGB) challenge task of transcribing multi-genre broadcast shows. Transcription was one of four tasks proposed in the MGB challenge, with the aim of advancing the state of the art of automatic speech recognition, speaker diarisation and automatic alignment of subtitles for broadcast media. Four topics are investigated in this work: Data selection techniques for training with unreliable data, automatic speech segmentation of broadcast media shows, acoustic modelling and adaptation in highly variable environments, and language modelling of multi-genre shows. The final system operates in multiple passes, using an initial unadapted decoding stage to refine segmentation, followed by three adapted passes: a hybrid DNN pass with input features normalised by speaker-based cepstral normalisation, another hybrid stage with input features normalised by speaker feature-MLLR transformations, and finally a bottleneck-based tandem stage with noise and speaker factorisation. The combination of these three system outputs provides a final error rate of 27.5% on the official development set, consisting of 47 multi-genre shows. version:1
arxiv-1512-06566 | Local and global gestalt laws: A neurally based spectral approach | http://arxiv.org/abs/1512.06566 | id:1512.06566 author:Marta Favali, Giovanna Citti, Alessandro Sarti category:cs.CV  published:2015-12-21 summary:A mathematical model of figure-ground articulation is presented, taking into account both local and global gestalt laws. The model is compatible with the functional architecture of the primary visual cortex (V1). Particularly the local gestalt law of good continuity is described by means of suitable connectivity kernels, that are derived from Lie group theory and are neurally implemented in long range connectivity in V1. Different kernels are compatible with the geometric structure of cortical connectivity and they are derived as the fundamental solutions of the Fokker Planck, the Sub-Riemannian Laplacian and the isotropic Laplacian equations. The kernels are used to construct matrices of connectivity among the features present in a visual stimulus. Global gestalt constraints are then introduced in terms of spectral analysis of the connectivity matrix, showing that this processing can be cortically implemented in V1 by mean field neural equations. This analysis performs grouping of local features and individuates perceptual units with the highest saliency. Numerical simulations are performed and results are obtained applying the technique to a number of stimuli. version:1
arxiv-1512-06539 | Spatial Phase-Sweep: Increasing temporal resolution of transient imaging using a light source array | http://arxiv.org/abs/1512.06539 | id:1512.06539 author:Ryuichi Tadano, Adithya Kumar Pediredla, Kaushik Mitra, Ashok Veeraraghavan category:cs.CV  published:2015-12-21 summary:Transient imaging or light-in-flight techniques capture the propagation of an ultra-short pulse of light through a scene, which in effect captures the optical impulse response of the scene. Recently, it has been shown that we can capture transient images using commercially available Time-of-Flight (ToF) systems such as Photonic Mixer Devices (PMD). In this paper, we propose `spatial phase-sweep', a technique that exploits the speed of light to increase the temporal resolution beyond the 100 picosecond limit imposed by current electronics. Spatial phase-sweep uses a linear array of light sources with spatial separation of about 3 mm between them, thereby resulting in a time shift of about 10 picoseconds, which translates into 100 Gfps of transient imaging in theory. We demonstrate a prototype and transient imaging results using spatial phase-sweep. version:1
arxiv-1510-02879 | ADAAPT: A Deep Architecture for Adaptive Policy Transfer from Multiple Sources | http://arxiv.org/abs/1510.02879 | id:1510.02879 author:Janarthanan Rajendran, P Prasanna, Balaraman Ravindran, Mitesh M. Khapra category:cs.AI cs.LG  published:2015-10-10 summary:The ability to transfer knowledge from learnt source tasks to a new target task can be very useful in speeding up the learning process of a Reinforcement Learning agent. This has been receiving a lot of attention, but the application of transfer poses two serious challenges which have not been adequately addressed in the past. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of speeding it up. Secondly, the agent should be able to do selective transfer which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer, which addresses these challenges. We test ADAAPT using two different instantiations: One as ADAAPTive REINFORCE algorithm for direct policy search and another as ADAAPTive Actor-Critic where the actor uses ADAAPT. Empirical evaluations on simulated domains show that ADAAPT can be effectively used for policy transfer from multiple source MDPs sharing the same state and action space. version:2
arxiv-1512-06492 | Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect | http://arxiv.org/abs/1512.06492 | id:1512.06492 author:Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy category:cs.CV cs.AI  published:2015-12-21 summary:This paper summarizes the recent progress we have made for the computer vision technologies in physical therapy with the accessible and affordable devices. We first introduce the remote health coaching system we build with Microsoft Kinect. Since the motion data captured by Kinect is noisy, we investigate the data accuracy of Kinect with respect to the high accuracy motion capture system. We also propose an outlier data removal algorithm based on the data distribution. In order to generate the kinematic parameter from the noisy data captured by Kinect, we propose a kinematic filtering algorithm based on Unscented Kalman Filter and the kinematic model of human skeleton. The proposed algorithm can obtain smooth kinematic parameter with reduced noise compared to the kinematic parameter generated from the raw motion data from Kinect. version:1
arxiv-1506-04395 | Reading Scene Text in Deep Convolutional Sequences | http://arxiv.org/abs/1506.04395 | id:1506.04395 author:Pan He, Weilin Huang, Yu Qiao, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2015-06-14 summary:We develop a Deep-Text Recurrent Network (DTRN) that regards scene text reading as a sequence labelling problem. We leverage recent advances of deep convolutional neural networks to generate an ordered high-level sequence from a whole word image, avoiding the difficult character segmentation problem. Then a deep recurrent model, building on long short-term memory (LSTM), is developed to robustly recognize the generated CNN sequences, departing from most existing approaches recognising each character independently. Our model has a number of appealing properties in comparison to existing scene text recognition methods: (i) It can recognise highly ambiguous words by leveraging meaningful context information, allowing it to work reliably without either pre- or post-processing; (ii) the deep CNN feature is robust to various image distortions; (iii) it retains the explicit order information in word image, which is essential to discriminate word strings; (iv) the model does not depend on pre-defined dictionary, and it can process unknown words and arbitrary strings. Codes for the DTRN will be available. version:2
arxiv-1512-06430 | Behavioral Modeling for Churn Prediction: Early Indicators and Accurate Predictors of Custom Defection and Loyalty | http://arxiv.org/abs/1512.06430 | id:1512.06430 author:Muhammad R. Khan, Johua Manoj, Anikate Singh, Joshua Blumenstock category:cs.LG  published:2015-12-20 summary:Churn prediction, or the task of identifying customers who are likely to discontinue use of a service, is an important and lucrative concern of firms in many different industries. As these firms collect an increasing amount of large-scale, heterogeneous data on the characteristics and behaviors of customers, new methods become possible for predicting churn. In this paper, we present a unified analytic framework for detecting the early warning signs of churn, and assigning a "Churn Score" to each customer that indicates the likelihood that the particular individual will churn within a predefined amount of time. This framework employs a brute force approach to feature engineering, then winnows the set of relevant attributes via feature selection, before feeding the final feature-set into a suite of supervised learning algorithms. Using several terabytes of data from a large mobile phone network, our method identifies several intuitive - and a few surprising - early warning signs of churn, and our best model predicts whether a subscriber will churn with 89.4% accuracy. version:1
arxiv-1506-02557 | Variational Dropout and the Local Reparameterization Trick | http://arxiv.org/abs/1506.02557 | id:1506.02557 author:Diederik P. Kingma, Tim Salimans, Max Welling category:stat.ML cs.LG stat.CO  published:2015-06-08 summary:We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments. version:2
arxiv-1512-06388 | Revisiting Differentially Private Regression: Lessons From Learning Theory and their Consequences | http://arxiv.org/abs/1512.06388 | id:1512.06388 author:Xi Wu, Matthew Fredrikson, Wentao Wu, Somesh Jha, Jeffrey F. Naughton category:cs.CR cs.DB cs.LG  published:2015-12-20 summary:Private regression has received attention from both database and security communities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzed the functional mechanism (Zhang et al. VLDB 2012) for training linear regression models over medical data. Unfortunately, they found that model accuracy is already unacceptable with differential privacy when $\varepsilon = 5$. We address this issue, presenting an explicit connection between differential privacy and stable learning theory through which a substantially better privacy/utility tradeoff can be obtained. Perhaps more importantly, our theory reveals that the most basic mechanism in differential privacy, output perturbation, can be used to obtain a better tradeoff for all convex-Lipschitz-bounded learning tasks. Since output perturbation is simple to implement, it means that our approach is potentially widely applicable in practice. We go on to apply it on the same medical data as used by Fredrikson et al. Encouragingly, we achieve accurate models even for $\varepsilon = 0.1$. In the last part of this paper, we study the impact of our improved differentially private mechanisms on model inversion attacks, a privacy attack introduced by Fredrikson et al. We observe that the improved tradeoff makes the resulting differentially private model more susceptible to inversion attacks. We analyze this phenomenon formally. version:1
arxiv-1509-06569 | Tensorizing Neural Networks | http://arxiv.org/abs/1509.06569 | id:1509.06569 author:Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, Dmitry Vetrov category:cs.LG cs.NE  published:2015-09-22 summary:Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times. version:2
arxiv-1512-06337 | Kernel principal component analysis network for image classification | http://arxiv.org/abs/1512.06337 | id:1512.06337 author:Dan Wu, Jiasong Wu, Rui Zeng, Longyu Jiang, Lotfi Senhadji, Huazhong Shu category:cs.LG cs.CV  published:2015-12-20 summary:In order to classify the nonlinear feature with linear classifier and improve the classification accuracy, a deep learning network named kernel principal component analysis network (KPCANet) is proposed. First, mapping the data into higher space with kernel principal component analysis to make the data linearly separable. Then building a two-layer KPCANet to obtain the principal components of image. Finally, classifying the principal components with linearly classifier. Experimental results show that the proposed KPCANet is effective in face recognition, object recognition and hand-writing digits recognition, it also outperforms principal component analysis network (PCANet) generally as well. Besides, KPCANet is invariant to illumination and stable to occlusion and slight deformation. version:1
arxiv-1505-03001 | Detecting the large entries of a sparse covariance matrix in sub-quadratic time | http://arxiv.org/abs/1505.03001 | id:1505.03001 author:Ofer Shwartz, Boaz Nadler category:stat.CO cs.LG stat.ML  published:2015-05-12 summary:The covariance matrix of a $p$-dimensional random variable is a fundamental quantity in data analysis. Given $n$ i.i.d. observations, it is typically estimated by the sample covariance matrix, at a computational cost of $O(np^{2})$ operations. When $n,p$ are large, this computation may be prohibitively slow. Moreover, in several contemporary applications, the population matrix is approximately sparse, and only its few large entries are of interest. This raises the following question, at the focus of our work: Assuming approximate sparsity of the covariance matrix, can its large entries be detected much faster, say in sub-quadratic time, without explicitly computing all its $p^{2}$ entries? In this paper, we present and theoretically analyze two randomized algorithms that detect the large entries of an approximately sparse sample covariance matrix using only $O(np\text{ poly log } p)$ operations. Furthermore, assuming sparsity of the population matrix, we derive sufficient conditions on the underlying random variable and on the number of samples $n$, for the sample covariance matrix to satisfy our approximate sparsity requirements. Finally, we illustrate the performance of our algorithms via several simulations. version:2
arxiv-1410-4812 | Inference and Mixture Modeling with the Elliptical Gamma Distribution | http://arxiv.org/abs/1410.4812 | id:1410.4812 author:Reshad Hosseini, Suvrit Sra, Lucas Theis, Matthias Bethge category:stat.CO math.OC stat.ML  published:2014-10-17 summary:We study modeling and inference with the Elliptical Gamma Distribution (EGD). We consider maximum likelihood (ML) estimation for EGD scatter matrices, a task for which we develop new fixed-point algorithms. Our algorithms are efficient and converge to global optima despite nonconvexity. Moreover, they turn out to be much faster than both a well-known iterative algorithm of Kent & Tyler (1991) and sophisticated manifold optimization algorithms. Subsequently, we invoke our ML algorithms as subroutines for estimating parameters of a mixture of EGDs. We illustrate our methods by applying them to model natural image statistics---the proposed EGD mixture model yields the most parsimonious model among several competing approaches. version:2
arxiv-1512-04509 | On non-iterative training of a neural classifier | http://arxiv.org/abs/1512.04509 | id:1512.04509 author:K. Eswaran, K. Damodhar Rao category:cs.CV cs.LG cs.NE 62M45  published:2015-12-14 summary:Recently an algorithm, was discovered, which separates points in n-dimension by planes in such a manner that no two points are left un-separated by at least one plane{[}1-3{]}. By using this new algorithm we show that there are two ways of classification by a neural network, for a large dimension feature space, both of which are non-iterative and deterministic. To demonstrate the power of both these methods we apply them exhaustively to the classical pattern recognition problem: The Fisher-Anderson's, IRIS flower data set and present the results. It is expected these methods will now be widely used for the training of neural networks for Deep Learning not only because of their non-iterative and deterministic nature but also because of their efficiency and speed and will supersede other classification methods which are iterative in nature and rely on error minimization. version:2
arxiv-1512-06293 | A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction | http://arxiv.org/abs/1512.06293 | id:1512.06293 author:Thomas Wiatowski, Helmut Bölcskei category:cs.IT cs.AI cs.LG math.FA math.IT stat.ML  published:2015-12-19 summary:Deep convolutional neural networks have led to breakthrough results in practical feature extraction applications. The mathematical analysis of such networks was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on semi-discrete shift-invariant wavelet frames and modulus non-linearities in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. The purpose of this paper is to develop Mallat's theory further by allowing for general convolution kernels, or in more technical parlance, general semi-discrete shift-invariant frames (including Weyl-Heisenberg, curvelet, shearlet, ridgelet, and wavelet frames) and general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), as well as pooling through sub-sampling, all of which can be different in different network layers. The resulting generalized network enables extraction of significantly wider classes of features than those resolved by Mallat's wavelet-modulus scattering network. We prove deformation stability for a larger class of deformations than those considered by Mallat, and we establish a new translation invariance result which is of vertical nature in the sense of the network depth determining the amount of invariance. Moreover, our results establish that deformation stability and vertical translation invariance are guaranteed by the network structure per se rather than the specific convolution kernels and non-linearities. This offers an explanation for the tremendous success of deep convolutional neural networks in a wide variety of practical feature extraction applications. The mathematical techniques we employ are based on continuous frame theory. version:1
arxiv-1512-06285 | Neutro-Connectedness Cut | http://arxiv.org/abs/1512.06285 | id:1512.06285 author:Min Xian, Yingtao Zhang, H. D. Cheng, Fei Xu, Jianrui Ding category:cs.CV  published:2015-12-19 summary:Interactive image segmentation is a challenging task and received increasing attention recently; however, two major drawbacks exist in interactive segmentation approaches. First, the segmentation performance of ROI-based methods is sensitive to the initial ROI: different ROIs may produce results with great difference. Second, most seed-based methods need intense interactions, and are not applicable in many cases. In this work, we generalize the Neutro-Connectedness (NC) to be independent of top-down priors of objects and to model image topology with indeterminacy measurement on image regions, propose a novel method for determining object and background regions, which is applied to exclude isolated background regions and enforce label consistency, and put forward a hybrid interactive segmentation method, Neutro-Connectedness Cut (NC-Cut), which can overcome the above two problems by utilizing both pixel-wise appearance information and region-based NC properties. We evaluate the proposed NC-Cut by employing two image datasets (265 images), and demonstrate that the proposed approach outperforms state-of-the-art interactive image segmentation methods (Grabcut, MILCut, One-Cut, {{GC}_max}^sum and pPBC). version:1
arxiv-1509-08588 | Estimating network edge probabilities by neighborhood smoothing | http://arxiv.org/abs/1509.08588 | id:1509.08588 author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML  published:2015-09-29 summary:The problem of estimating probabilities of network edges from the observed adjacency matrix has important applications to predicting missing links and network denoising. It has usually been addressed by estimating the graphon, a function that determines the matrix of edge probabilities, but is ill-defined without strong assumptions on the network structure. Here we propose a novel computationally efficient method based on neighborhood smoothing to estimate the expectation of the adjacency matrix directly, without making the strong structural assumptions graphon estimation requires. The neighborhood smoothing method requires little tuning, has a competitive mean-squared error rate, and outperforms many benchmark methods on the task of link prediction in both simulated and real networks. version:2
arxiv-1511-06545 | A dense subgraph based algorithm for compact salient image region detection | http://arxiv.org/abs/1511.06545 | id:1511.06545 author:Souradeep Chakraborty, Pabitra Mitra category:cs.CV  published:2015-11-20 summary:We present an algorithm for graph based saliency computation that utilizes the underlying dense subgraphs in finding visually salient regions in an image. To compute the salient regions, the model first obtains a saliency map using random walks on a Markov chain. Next, k-dense subgraphs are detected to further enhance the salient regions in the image. Dense subgraphs convey more information about local graph structure than simple centrality measures. To generate the Markov chain, intensity and color features of an image in addition to region compactness is used. For evaluating the proposed model, we do extensive experiments on benchmark image data sets. The proposed method performs comparable to well-known algorithms in salient region detection. version:2
arxiv-1512-06235 | Multistage SFM: A Coarse-to-Fine Approach for 3D Reconstruction | http://arxiv.org/abs/1512.06235 | id:1512.06235 author:Rajvi Shah, Aditya Deshpande, P J Narayanan category:cs.CV  published:2015-12-19 summary:Several methods have been proposed for large-scale 3D reconstruction from large, unorganized image collections. A large reconstruction problem is typically divided into multiple components which are reconstructed independently using structure from motion (SFM) and later merged together. Incremental SFM methods are most popular for the basic structure recovery of a single component. They are robust and effective but are strictly sequential in nature. We present a multistage approach for SFM reconstruction of a single component that breaks the sequential nature of the incremental SFM methods. Our approach begins with quickly building a coarse 3D model using only a fraction of features from given images. The coarse model is then enriched by localizing remaining images and matching and triangulating remaining features in subsequent stages. These stages are made efficient and highly parallel by leveraging the geometry of the coarse model. Our method produces similar quality models as compared to incremental SFM methods while being notably fast and parallel. version:1
arxiv-1512-06228 | Using machine learning for medium frequency derivative portfolio trading | http://arxiv.org/abs/1512.06228 | id:1512.06228 author:Abhijit Sharang, Chetan Rao category:q-fin.TR cs.LG stat.ML  published:2015-12-19 summary:We use machine learning for designing a medium frequency trading strategy for a portfolio of 5 year and 10 year US Treasury note futures. We formulate this as a classification problem where we predict the weekly direction of movement of the portfolio using features extracted from a deep belief network trained on technical indicators of the portfolio constituents. The experimentation shows that the resulting pipeline is effective in making a profitable trade. version:1
arxiv-1412-4021 | A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging | http://arxiv.org/abs/1412.4021 | id:1412.4021 author:Dat Quoc Nguyen, Dai Quoc Nguyen, Dang Duc Pham, Son Bao Pham category:cs.CL  published:2014-12-12 summary:In this paper, we propose a new approach to construct a system of transformation rules for the Part-of-Speech (POS) tagging task. Our approach is based on an incremental knowledge acquisition method where rules are stored in an exception structure and new rules are only added to correct the errors of existing rules; thus allowing systematic control of the interaction between the rules. Experimental results on 13 languages show that our approach is fast in terms of training time and tagging speed. Furthermore, our approach obtains very competitive accuracy in comparison to state-of-the-art POS and morphological taggers. version:5
arxiv-1512-06223 | Combining patch-based strategies and non-rigid registration-based label fusion methods | http://arxiv.org/abs/1512.06223 | id:1512.06223 author:Carlos Platero, M. Carmen Tobar category:cs.CV  published:2015-12-19 summary:The objective of this study is to develop a patch-based labeling method that cooperates with a label fusion using non-rigid registrations. We present a novel patch-based label fusion method, whose selected patches and their weights are calculated from a combination of similarity measures between patches using intensity-based distances and labeling-based distances, where a previous labeling of the target image is inferred through a label fusion method using non-rigid registrations. These combined similarity measures result in better selection of the patches, and their weights are more robust, which improves the segmentation results compared to other label fusion methods, including the conventional patch-based labeling method. To evaluate the performance and the robustness of the proposed label fusion method, we employ two available databases of T1-weighted (T1W) magnetic resonance imaging (MRI) of human brains. We compare our approach with other label fusion methods in the automatic hippocampal segmentation from T1W-MRI. Our label fusion method yields mean Dice coefficients of 0.847 and 0.798 for the two databases used with mean times of approximately 180 and 320 seconds, respectively. The collaboration between the patch-based labeling method and the label fusion using non-rigid registrations is given in the several levels: (a) The pre-selection of the patches in the atlases are improved, (b) The weights of our selected patches are also more robust, (c) our approach imposes geometrical restrictions, such as shape priors, and (d) the work-flow is very efficient. We show that the proposed approach is very competitive with respect to recently reported methods. version:1
arxiv-1512-06222 | A new robust adaptive algorithm for underwater acoustic channel equalization | http://arxiv.org/abs/1512.06222 | id:1512.06222 author:Dariush Kari, Muhammed Omer Sayin, Suleyman Serdar Kozat category:cs.SD cs.IT cs.LG math.IT  published:2015-12-19 summary:We introduce a novel family of adaptive robust equalizers for highly challenging underwater acoustic (UWA) channel equalization. Since the underwater environment is highly non-stationary and subjected to impulsive noise, we use adaptive filtering techniques based on a relative logarithmic cost function inspired by the competitive methods from the online learning literature. To improve the convergence performance of the conventional linear equalization methods, while mitigating the stability issues, we intrinsically combine different norms of the error in the cost function, using logarithmic functions. Hence, we achieve a comparable convergence performance to least mean fourth (LMF) equalizer, while significantly enhancing the stability performance in such an adverse communication medium. We demonstrate the performance of our algorithms through highly realistic experiments performed on accurately simulated underwater acoustic channels. version:1
arxiv-1512-06216 | Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines | http://arxiv.org/abs/1512.06216 | id:1512.06216 author:Hao Zhang, Zhiting Hu, Jinliang Wei, Pengtao Xie, Gunhee Kim, Qirong Ho, Eric Xing category:cs.LG cs.CV cs.DC  published:2015-12-19 summary:Deep learning (DL) has achieved notable successes in many machine learning tasks. A number of frameworks have been developed to expedite the process of designing and training deep neural networks (DNNs), such as Caffe, Torch and Theano. Currently they can harness multiple GPUs on a single machine, but are unable to use GPUs that are distributed across multiple machines; as even average-sized DNNs can take days to train on a single GPU with 100s of GBs to TBs of data, distributed GPUs present a prime opportunity for scaling up DL. However, the limited bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training, and prevents its trivial realization. To investigate how to adapt existing frameworks to efficiently support distributed GPUs, we propose Poseidon, a scalable system architecture for distributed inter-machine communication in existing DL frameworks. We integrate Poseidon with Caffe and evaluate its performance at training DNNs for object recognition. Poseidon features three key contributions that accelerate DNN training on clusters: (1) a three-level hybrid architecture that allows Poseidon to support both CPU-only and GPU-equipped clusters, (2) a distributed wait-free backpropagation (DWBP) algorithm to improve GPU utilization and to balance communication, and (3) a structure-aware communication protocol (SACP) to minimize communication overheads. We empirically show that Poseidon converges to same objectives as a single machine, and achieves state-of-art training speedup across multiple models and well-established datasets using a commodity GPU cluster of 8 nodes (e.g. 4.5x speedup on AlexNet, 4x on GoogLeNet, 4x on CIFAR-10). On the much larger ImageNet22K dataset, Poseidon with 8 nodes achieves better speedup and competitive accuracy to recent CPU-based distributed systems such as Adam and Le et al., which use 10s to 1000s of nodes. version:1
arxiv-1512-06173 | Discriminative Subnetworks with Regularized Spectral Learning for Global-state Network Data | http://arxiv.org/abs/1512.06173 | id:1512.06173 author:Xuan Hong Dang, Ambuj K. Singh, Petko Bogdanov, Hongyuan You, Bayyuan Hsu category:cs.LG  published:2015-12-19 summary:Data mining practitioners are facing challenges from data with network structure. In this paper, we address a specific class of global-state networks which comprises of a set of network instances sharing a similar structure yet having different values at local nodes. Each instance is associated with a global state which indicates the occurrence of an event. The objective is to uncover a small set of discriminative subnetworks that can optimally classify global network values. Unlike most existing studies which explore an exponential subnetwork space, we address this difficult problem by adopting a space transformation approach. Specifically, we present an algorithm that optimizes a constrained dual-objective function to learn a low-dimensional subspace that is capable of discriminating networks labelled by different global states, while reconciling with common network topology sharing across instances. Our algorithm takes an appealing approach from spectral graph learning and we show that the globally optimum solution can be achieved via matrix eigen-decomposition. version:1
arxiv-1512-06171 | Regularized Estimation of Piecewise Constant Gaussian Graphical Models: The Group-Fused Graphical Lasso | http://arxiv.org/abs/1512.06171 | id:1512.06171 author:Alexander J. Gibberd, James D. B. Nelson category:stat.ME stat.CO stat.ML  published:2015-12-19 summary:The time-evolving precision matrix of a piecewise-constant Gaussian graphical model encodes the dynamic conditional dependency structure of a multivariate time-series. Traditionally, graphical models are estimated under the assumption that data is drawn identically from a generating distribution. Introducing sparsity and sparse-difference inducing priors we relax these assumptions and propose a novel regularized M-estimator to jointly estimate both the graph and changepoint structure. The resulting estimator possesses the ability to therefore favor sparse dependency structures and/or smoothly evolving graph structures, as required. Moreover, our approach extends current methods to allow estimation of changepoints that are grouped across multiple dependencies in a system. An efficient algorithm for estimating structure is proposed. We study the empirical recovery properties in a synthetic setting. The qualitative effect of grouped changepoint estimation is then demonstrated by applying the method on two real-world data-sets. version:1
arxiv-1406-4824 | What is India speaking: The "Hinglish" invasion | http://arxiv.org/abs/1406.4824 | id:1406.4824 author:Rana D. Parshad, Vineeta Chand, Neha Sinha, Nitu Kumari category:cs.CL math.DS  published:2014-06-12 summary:While language competition models of diachronic language shift are increasingly sophisticated, drawing on sociolinguistic components like variable language prestige, distance from language centers and intermediate bilingual transitionary populations, in one significant way they fall short. They fail to consider contact-based outcomes resulting in mixed language practices, e.g. outcome scenarios such as creoles or unmarked code switching as an emergent communicative norm. On these lines something very interesting is uncovered in India, where traditionally there have been monolingual Hindi speakers and Hindi/English bilinguals, but virtually no monolingual English speakers. While the Indian census data reports a sharp increase in the proportion of Hindi/English bilinguals, we argue that the number of Hindi/English bilinguals in India is inaccurate, given a new class of urban individuals speaking a mixed lect of Hindi and English, popularly known as "Hinglish". Based on predator-prey, sociolinguistic theories, salient local ecological factors and the rural-urban divide in India, we propose a new mathematical model of interacting monolingual Hindi speakers, Hindi/English bilinguals and Hinglish speakers. The model yields globally asymptotic stable states of coexistence, as well as bilingual extinction. To validate our model, sociolinguistic data from different Indian classes are contrasted with census reports: We see that purported urban Hindi/English bilinguals are unable to maintain fluent Hindi speech and instead produce Hinglish, whereas rural speakers evidence monolingual Hindi. Thus we present evidence for the first time where an unrecognized mixed lect involving English but not "English", has possibly taken over a sizeable faction of a large global population. version:2
arxiv-1512-06098 | Expectation propagation for diffusion processes by moment closure approximations | http://arxiv.org/abs/1512.06098 | id:1512.06098 author:Botond Cseke, David Schnoerr, Manfred Opper, Guido Sanguinetti category:stat.ML  published:2015-12-18 summary:We consider the inverse problem of reconstructing the trajectory of a diffusion process from discrete and continuous time (soft) observations. We cast the problem in a Bayesian framework and derive approximations to the posterior distributions of state space marginals using variational approximate inference. The resulting optimisation algorithm is a hybrid expectation propagation/variational message passing algorithm. We then show how the approximation can be extended to a wide class of discrete-state Markovian jump processes by making use of the chemical Langevin equation. Our empirical results show that this is a computationally feasible and accurate method to approach these intractable classes of inverse problems. version:1
arxiv-1202-0840 | Lossy Compression via Sparse Linear Regression: Performance under Minimum-distance Encoding | http://arxiv.org/abs/1202.0840 | id:1202.0840 author:Ramji Venkataramanan, Antony Joseph, Sekhar Tatikonda category:cs.IT math.IT stat.ML  published:2012-02-03 summary:We study a new class of codes for lossy compression with the squared-error distortion criterion, designed using the statistical framework of high-dimensional linear regression. Codewords are linear combinations of subsets of columns of a design matrix. Called a Sparse Superposition or Sparse Regression codebook, this structure is motivated by an analogous construction proposed recently by Barron and Joseph for communication over an AWGN channel. For i.i.d Gaussian sources and minimum-distance encoding, we show that such a code can attain the Shannon rate-distortion function with the optimal error exponent, for all distortions below a specified value. It is also shown that sparse regression codes are robust in the following sense: a codebook designed to compress an i.i.d Gaussian source of variance $\sigma^2$ with (squared-error) distortion $D$ can compress any ergodic source of variance less than $\sigma^2$ to within distortion $D$. Thus the sparse regression ensemble retains many of the good covering properties of the i.i.d random Gaussian ensemble, while having having a compact representation in terms of a matrix whose size is a low-order polynomial in the block-length. version:4
arxiv-1512-06086 | Bayesian anti-sparse coding | http://arxiv.org/abs/1512.06086 | id:1512.06086 author:Clément Elvira, Pierre Chainais, Nicolas Dobigeon category:stat.ML physics.data-an stat.ME  published:2015-12-18 summary:Sparse representations have proven their efficiency in solving a wide class of inverse problems encountered in signal and image processing. Conversely, enforcing the information to be spread uniformly over representation coefficients exhibits relevant properties in various applications such as digital communications. Anti-sparse regularization can be naturally expressed through an $\ell_{\infty}$-norm penalty. This paper derives a probabilistic formulation of such representations. A new probability distribution, referred to as the democratic prior, is first introduced. Its main properties as well as three random variate generators for this distribution are derived. Then this probability distribution is used as a prior to promote anti-sparsity in a Gaussian linear inverse problem, yielding a fully Bayesian formulation of anti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposed to generate samples according to the posterior distribution. The first one is a standard Gibbs sampler. The second one uses Metropolis-Hastings moves that exploit the proximity mapping of the log-posterior distribution. These samples are used to approximate maximum a posteriori and minimum mean square error estimators of both parameters and hyperparameters. Simulations on synthetic data illustrate the performances of the two proposed samplers, for both complete and over-complete dictionaries. All results are compared to the recent deterministic variational FITRA algorithm. version:1
arxiv-1512-06075 | Modeling Colors of Single Attribute Variations with Application to Food Appearance | http://arxiv.org/abs/1512.06075 | id:1512.06075 author:Yaser Yacoob category:cs.CV  published:2015-12-18 summary:This paper considers the intra-image color-space of an object or a scene when these are subject to a dominant single-source of variation. The source of variation can be intrinsic or extrinsic (i.e., imaging conditions) to the object. We observe that the quantized colors for such objects typically lie on a planar subspace of RGB, and in some cases linear or polynomial curves on this plane are effective in capturing these color variations. We also observe that the inter-image color sub-spaces are robust as long as drastic illumination change is not involved. We illustrate the use of this analysis for: discriminating between shading-change and reflectance-change for patches, and object detection, segmentation and recognition based on a single exemplar. We focus on images of food items to illustrate the effectiveness of the proposed approach. version:1
arxiv-1512-06061 | Asymptotic Behavior of Mean Partitions in Consensus Clustering | http://arxiv.org/abs/1512.06061 | id:1512.06061 author:Brijnesh Jain category:cs.LG stat.ML  published:2015-12-18 summary:Although consistency is a minimum requirement of any estimator, little is known about consistency of the mean partition approach in consensus clustering. This contribution studies the asymptotic behavior of mean partitions. We show that under normal assumptions, the mean partition approach is consistent and asymptotic normal. To derive both results, we represent partitions as points of some geometric space, called orbit space. Then we draw on results from the theory of Fr\'echet means and stochastic programming. The asymptotic properties hold for continuous extensions of standard cluster criteria (indices). The results justify consensus clustering using finite but sufficiently large sample sizes. Furthermore, the orbit space framework provides a mathematical foundation for studying further statistical, geometrical, and analytical properties of sets of partitions. version:1
arxiv-1512-05300 | Multiregion Bilinear Convolutional Neural Networks for Person Re-Identification | http://arxiv.org/abs/1512.05300 | id:1512.05300 author:Evgeniya Ustinova, Yaroslav Ganin, Victor Lempitsky category:cs.CV  published:2015-12-16 summary:In this work we explore the applicability of the recently proposed CNN architecture, called Bilinear CNN, and its new modification that we call multi-region Bilinear CNN to the person re-identification problem. Originally, Bilinear CNNs were introduced for fine-grained classification and proved to be both simple and high-performing architectures. Bilinear CNN allows to build an orderless descriptor for an image using outer product of features outputted from two separate feature extractors. Based on this approach, Multiregion Bilinear CNN, apply bilinear pooling over multiple regions for extracting rich and useful descriptors that retain some spatial information. We show than when embedded into a standard "siamese" type learning, bilinear CNNs and in particular their multi-region variants can improve re-identification performance compared to standard CNNs and achieve state-of-the-art accuracy on the largest person re-identification datasets available at the moment, namely CUHK03 and Market-1501. version:2
arxiv-1502-02355 | High dimensional errors-in-variables models with dependent measurements | http://arxiv.org/abs/1502.02355 | id:1502.02355 author:Mark Rudelson, Shuheng Zhou category:math.ST stat.ML stat.TH  published:2015-02-09 summary:Suppose that we observe $y \in \mathbb{R}^f$ and $X \in \mathbb{R}^{f \times m}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & X_0 \beta^* + \epsilon \\ X & = & X_0 + W \end{eqnarray*} where $X_0$ is a $f \times m$ design matrix with independent subgaussian row vectors, $\epsilon \in \mathbb{R}^f$ is a noise vector and $W$ is a mean zero $f \times m$ random noise matrix with independent subgaussian column vectors, independent of $X_0$ and $\epsilon$. This model is significantly different from those analyzed in the literature in the sense that we allow the measurement error for each covariate to be a dependent vector across its $f$ observations. Such error structures appear in the science literature when modeling the trial-to-trial fluctuations in response strength shared across a set of neurons. Under sparsity and restrictive eigenvalue type of conditions, we show that one is able to recover a sparse vector $\beta^* \in \mathbb{R}^m$ from the model given a single observation matrix $X$ and the response vector $y$. We establish consistency in estimating $\beta^*$ and obtain the rates of convergence in the $\ell_q$ norm, where $q = 1, 2$ for the Lasso-type estimator, and for $q \in [1, 2]$ for a Dantzig-type conic programming estimator. We show error bounds which approach that of the regular Lasso and the Dantzig selector in case the errors in $W$ are tending to 0. version:2
arxiv-1512-06014 | Multiclass Classification of Cervical Cancer Tissues by Hidden Markov Model | http://arxiv.org/abs/1512.06014 | id:1512.06014 author:Sabyasachi Mukhopadhyay, Sanket Nandan, Indrajit Kurmi category:cs.CV  published:2015-12-18 summary:In this paper, we report a hidden Markov model based multiclass classification of cervical cancer tissues. This model has been validated directly over time series generated by the medium refractive index fluctuations extracted from differential interference contrast images of healthy and different stages of cancer tissues. The method shows promising results for multiclass classification with higher accuracy. version:1
