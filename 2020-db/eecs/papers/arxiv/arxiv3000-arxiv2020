arxiv-1306-2533 | DISCOMAX: A Proximity-Preserving Distance Correlation Maximization Algorithm | http://arxiv.org/abs/1306.2533 | id:1306.2533 author:Praneeth Vepakomma, Ahmed Elgammal category:cs.LG stat.ML  published:2013-06-11 summary:In a regression setting we propose algorithms that reduce the dimensionality of the features while simultaneously maximizing a statistical measure of dependence known as distance correlation between the low-dimensional features and a response variable. This helps in solving the prediction problem with a low-dimensional set of features. Our setting is different from subset-selection algorithms where the problem is to choose the best subset of features for regression. Instead, we attempt to generate a new set of low-dimensional features as in a feature-learning setting. We attempt to keep our proposed approach as model-free and our algorithm does not assume the application of any specific regression model in conjunction with the low-dimensional features that it learns. The algorithm is iterative and is fomulated as a combination of the majorization-minimization and concave-convex optimization procedures. We also present spectral radius based convergence results for the proposed iterations. version:2
arxiv-1306-6078 | A Computational Approach to Politeness with Application to Social Factors | http://arxiv.org/abs/1306.6078 | id:1306.6078 author:Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, Christopher Potts category:cs.CL cs.SI physics.soc-ph I.2.7  published:2013-06-25 summary:We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains. We use our framework to study the relationship between politeness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community. version:1
arxiv-1306-6041 | Learning, Generalization, and Functional Entropy in Random Automata Networks | http://arxiv.org/abs/1306.6041 | id:1306.6041 author:Alireza Goudarzi, Christof Teuscher, Natali Gulbahce, Thimo Rohlf category:cs.NE cond-mat.dis-nn nlin.AO nlin.CD physics.bio-ph C.1.3; I.2.6; I.5  published:2013-06-25 summary:It has been shown \citep{broeck90:physicalreview,patarnello87:europhys} that feedforward Boolean networks can learn to perform specific simple tasks and generalize well if only a subset of the learning examples is provided for learning. Here, we extend this body of work and show experimentally that random Boolean networks (RBNs), where both the interconnections and the Boolean transfer functions are chosen at random initially, can be evolved by using a state-topology evolution to solve simple tasks. We measure the learning and generalization performance, investigate the influence of the average node connectivity $K$, the system size $N$, and introduce a new measure that allows to better describe the network's learning and generalization behavior. We show that the connectivity of the maximum entropy networks scales as a power-law of the system size $N$. Our results show that networks with higher average connectivity $K$ (supercritical) achieve higher memorization and partial generalization. However, near critical connectivity, the networks show a higher perfect generalization on the even-odd task. version:1
arxiv-0910-4397 | The Geometry of Generalized Binary Search | http://arxiv.org/abs/0910.4397 | id:0910.4397 author:Robert D. Nowak category:stat.ML cs.IT math.IT math.ST stat.TH  published:2009-10-22 summary:This paper investigates the problem of determining a binary-valued function through a sequence of strategically selected queries. The focus is an algorithm called Generalized Binary Search (GBS). GBS is a well-known greedy algorithm for determining a binary-valued function through a sequence of strategically selected queries. At each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search. This paper develops novel incoherence and geometric conditions under which GBS achieves the information-theoretically optimal query complexity; i.e., given a collection of N hypotheses, GBS terminates with the correct function after no more than a constant times log N queries. Furthermore, a noise-tolerant version of GBS is developed that also achieves the optimal query complexity. These results are applied to learning halfspaces, a problem arising routinely in image processing and machine learning. version:5
arxiv-1306-5998 | DNA Reservoir Computing: A Novel Molecular Computing Approach | http://arxiv.org/abs/1306.5998 | id:1306.5998 author:Alireza Goudarzi, Matthew R. Lakin, Darko Stefanovic category:cs.NE cs.ET nlin.AO nlin.CD physics.bio-ph  published:2013-06-25 summary:We propose a novel molecular computing approach based on reservoir computing. In reservoir computing, a dynamical core, called a reservoir, is perturbed with an external input signal while a readout layer maps the reservoir dynamics to a target output. Computation takes place as a transformation from the input space to a high-dimensional spatiotemporal feature space created by the transient dynamics of the reservoir. The readout layer then combines these features to produce the target output. We show that coupled deoxyribozyme oscillators can act as the reservoir. We show that despite using only three coupled oscillators, a molecular reservoir computer could achieve 90% accuracy on a benchmark temporal problem. version:1
arxiv-1306-5860 | Supersparse Linear Integer Models for Predictive Scoring Systems | http://arxiv.org/abs/1306.5860 | id:1306.5860 author:Berk Ustun, Stefano Traca, Cynthia Rudin category:stat.ML  published:2013-06-25 summary:We introduce Supersparse Linear Integer Models (SLIM) as a tool to create scoring systems for binary classification. We derive theoretical bounds on the true risk of SLIM scoring systems, and present experimental results to show that SLIM scoring systems are accurate, sparse, and interpretable classification models. version:1
arxiv-1301-4679 | Cellular Tree Classifiers | http://arxiv.org/abs/1301.4679 | id:1301.4679 author:GÃ©rard Biau, Luc Devroye category:stat.ML cs.LG math.ST stat.TH  published:2013-01-20 summary:The cellular tree classifier model addresses a fundamental problem in the design of classifiers for a parallel or distributed computing world: Given a data set, is it sufficient to apply a majority rule for classification, or shall one split the data into two or more parts and send each part to a potentially different computer (or cell) for further processing? At first sight, it seems impossible to define with this paradigm a consistent classifier as no cell knows the "original data size", $n$. However, we show that this is not so by exhibiting two different consistent classifiers. The consistency is universal but is only shown for distributions with nonatomic marginals. version:2
arxiv-1211-3500 | Accelerated Canonical Polyadic Decomposition by Using Mode Reduction | http://arxiv.org/abs/1211.3500 | id:1211.3500 author:Guoxu Zhou, Andrzej Cichocki, Shengli Xie category:cs.NA cs.LG math.NA  published:2012-11-15 summary:Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widely applied to analyze high order tensors. Existing CPD methods use alternating least square (ALS) iterations and hence need to unfold tensors to each of the $N$ modes frequently, which is one major bottleneck of efficiency for large-scale data and especially when $N$ is large. To overcome this problem, in this paper we proposed a new CPD method which converts the original $N$th ($N>3$) order tensor to a 3rd-order tensor first. Then the full CPD is realized by decomposing this mode reduced tensor followed by a Khatri-Rao product projection procedure. This way is quite efficient as unfolding to each of the $N$ modes are avoided, and dimensionality reduction can also be easily incorporated to further improve the efficiency. We show that, under mild conditions, any $N$th-order CPD can be converted into a 3rd-order case but without destroying the essential uniqueness, and theoretically gives the same results as direct $N$-way CPD methods. Simulations show that, compared with state-of-the-art CPD methods, the proposed method is more efficient and escape from local solutions more easily. version:2
arxiv-1306-5824 | Constrained Optimization for a Subset of the Gaussian Parsimonious Clustering Models | http://arxiv.org/abs/1306.5824 | id:1306.5824 author:Ryan P. Browne, Sanjeena Subedi, Paul McNicholas category:stat.CO math.ST stat.ML stat.TH  published:2013-06-25 summary:The expectation-maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates when data are incomplete or are treated as being incomplete. The EM algorithm and its variants are commonly used for parameter estimation in applications of mixture models for clustering and classification. This despite the fact that even the Gaussian mixture model likelihood surface contains many local maxima and is singularity riddled. Previous work has focused on circumventing this problem by constraining the smallest eigenvalue of the component covariance matrices. In this paper, we consider constraining the smallest eigenvalue, the largest eigenvalue, and both the smallest and largest within the family setting. Specifically, a subset of the GPCM family is considered for model-based clustering, where we use a re-parameterized version of the famous eigenvalue decomposition of the component covariance matrices. Our approach is illustrated using various experiments with simulated and real data. version:1
arxiv-1110-6228 | The AdaBoost Flow | http://arxiv.org/abs/1110.6228 | id:1110.6228 author:A. Lykov, S. Muzychka, K. Vaninsky category:stat.ML math-ph math.MP 62-07  published:2011-10-28 summary:We introduce a dynamical system which we call the AdaBoost flow. The flow is defined by a system of ODEs with control. We show that three algorithms of the AdaBoost family (i) the AdaBoost algorithm of Schapire and Freund (ii) the arc-gv algorithm of Breiman (iii) the confidence rated prediction of Schapire and Singer can be can be embedded in the AdaBoost flow. The nontrivial part of the AdaBoost flow equations coincides with the equations of dynamics of nonperiodic Toda system written in terms of spectral variables. We provide a novel invariant geometrical description of the AdaBoost algorithm as a gradient flow on a foliation defined by level sets of the potential function. We propose a new approach for constructing boosting algorithms as a continuous time gradient flow on measures defined by various metrics and potential functions. Finally we explain similarity of the AdaBoost algorithm with the Perelman's construction for the Ricci flow. version:4
arxiv-1209-3431 | Recovering Block-structured Activations Using Compressive Measurements | http://arxiv.org/abs/1209.3431 | id:1209.3431 author:Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh category:stat.ML  published:2012-09-15 summary:We consider the problems of detection and localization of a contiguous block of weak activation in a large matrix, from a small number of noisy, possibly adaptive, compressive (linear) measurements. This is closely related to the problem of compressed sensing, where the task is to estimate a sparse vector using a small number of linear measurements. Contrary to results in compressed sensing, where it has been shown that neither adaptivity nor contiguous structure help much, we show that for reliable localization the magnitude of the weakest signals is strongly influenced by both structure and the ability to choose measurements adaptively while for detection neither adaptivity nor structure reduce the requirement on the magnitude of the signal. We characterize the precise tradeoffs between the various problem parameters, the signal strength and the number of measurements required to reliably detect and localize the block of activation. The sufficient conditions are complemented with information theoretic lower bounds. version:2
arxiv-1306-5793 | A State-Space Approach for Optimal Traffic Monitoring via Network Flow Sampling | http://arxiv.org/abs/1306.5793 | id:1306.5793 author:Michael Kallitsis, Stilian Stoev, George Michailidis category:cs.SY cs.NI stat.AP stat.ML  published:2013-06-24 summary:The robustness and integrity of IP networks require efficient tools for traffic monitoring and analysis, which scale well with traffic volume and network size. We address the problem of optimal large-scale flow monitoring of computer networks under resource constraints. We propose a stochastic optimization framework where traffic measurements are done by exploiting the spatial (across network links) and temporal relationship of traffic flows. Specifically, given the network topology, the state-space characterization of network flows and sampling constraints at each monitoring station, we seek an optimal packet sampling strategy that yields the best traffic volume estimation for all flows of the network. The optimal sampling design is the result of a concave minimization problem; then, Kalman filtering is employed to yield a sequence of traffic estimates for each network flow. We evaluate our algorithm using real-world Internet2 data. version:1
arxiv-1306-5702 | Modeling The Stable Operating Envelope For Partially Stable Combustion Engines Using Class Imbalance Learning | http://arxiv.org/abs/1306.5702 | id:1306.5702 author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Jeff Sterniak, Dennis Assanis category:cs.NE  published:2013-06-24 summary:Advanced combustion technologies such as homogeneous charge compression ignition (HCCI) engines have a narrow stable operating region defined by complex control strategies such as exhaust gas recirculation (EGR) and variable valve timing among others. For such systems, it is important to identify the operating envelope or the boundary of stable operation for diagnostics and control purposes. Obtaining a good model of the operating envelope using physics becomes intractable owing to engine transient effects. In this paper, a machine learning based approach is employed to identify the stable operating boundary of HCCI combustion directly from experimental data. Owing to imbalance in class proportions in the data, two approaches are considered. A re-sampling (under-sampling, over-sampling) based approach is used to develop models using existing algorithms while a cost-sensitive approach is used to modify the learning algorithm without modifying the data set. Support vector machines and recently developed extreme learning machines are used for model development and results compared against linear classification methods show that cost-sensitive versions of ELM and SVM algorithms are well suited to model the HCCI operating envelope. The prediction results indicate that the models have the potential to be used for predicting HCCI instability based on sensor measurement history. version:1
arxiv-1306-5667 | Using Genetic Programming to Model Software | http://arxiv.org/abs/1306.5667 | id:1306.5667 author:W. B. Langdon, M. Harman category:cs.NE cs.AI  published:2013-06-24 summary:We study a generic program to investigate the scope for automatically customising it for a vital current task, which was not considered when it was first written. In detail, we show genetic programming (GP) can evolve models of aspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequences to the human genome. version:1
arxiv-1301-3611 | Jitter-Adaptive Dictionary Learning - Application to Multi-Trial Neuroelectric Signals | http://arxiv.org/abs/1301.3611 | id:1301.3611 author:Sebastian Hitziger, Maureen Clerc, Alexandre Gramfort, Sandrine Saillet, Christian BÃ©nar, ThÃ©odore Papadopoulo category:stat.ML 62-07  published:2013-01-16 summary:Dictionary Learning has proven to be a powerful tool for many image processing tasks, where atoms are typically defined on small image patches. As a drawback, the dictionary only encodes basic structures. In addition, this approach treats patches of different locations in one single set, which means a loss of information when features are well-aligned across signals. This is the case, for instance, in multi-trial magneto- or electroencephalography (M/EEG). Learning the dictionary on the entire signals could make use of the alignement and reveal higher-level features. In this case, however, small missalignements or phase variations of features would not be compensated for. In this paper, we propose an extension to the common dictionary learning framework to overcome these limitations by allowing atoms to adapt their position across signals. The method is validated on simulated and real neuroelectric data. version:4
arxiv-1306-5487 | Model Reframing by Feature Context Change | http://arxiv.org/abs/1306.5487 | id:1306.5487 author:Celestine-Periale Ma category:cs.LG  published:2013-06-23 summary:The feature space (including both input and output variables) characterises a data mining problem. In predictive (supervised) problems, the quality and availability of features determines the predictability of the dependent variable, and the performance of data mining models in terms of misclassification or regression error. Good features, however, are usually difficult to obtain. It is usual that many instances come with missing values, either because the actual value for a given attribute was not available or because it was too expensive. This is usually interpreted as a utility or cost-sensitive learning dilemma, in this case between misclassification (or regression error) costs and attribute tests costs. Both misclassification cost (MC) and test cost (TC) can be integrated into a single measure, known as joint cost (JC). We introduce methods and plots (such as the so-called JROC plots) that can work with any of-the-shelf predictive technique, including ensembles, such that we re-frame the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results. version:1
arxiv-1307-2560 | Exploiting Data Parallelism in the yConvex Hypergraph Algorithm for Image Representation using GPGPUs | http://arxiv.org/abs/1307.2560 | id:1307.2560 author:Saurabh Jha, Tejaswi Agarwal, B. Rajesh Kanna category:cs.DC cs.CV I.3  published:2013-06-23 summary:To define and identify a region-of-interest (ROI) in a digital image, the shape descriptor of the ROI has to be described in terms of its boundary characteristics. To address the generic issues of contour tracking, the yConvex Hypergraph (yCHG) model was proposed by Kanna et al [1]. In this work, we propose a parallel approach to implement the yCHG model by exploiting massively parallel cores of NVIDIA's Compute Unified Device Architecture (CUDA). We perform our experiments on the MODIS satellite image database by NASA, and based on our analysis we observe that the performance of the serial implementation is better on smaller images, but once the threshold is achieved in terms of image resolution, the parallel implementation outperforms its sequential counterpart by 2 to 10 times (2x-10x). We also conclude that an increase in the number of hyperedges in the ROI of a given size does not impact the performance of the overall algorithm. version:1
arxiv-1306-5480 | Characterizing Ambiguity in Light Source Invariant Shape from Shading | http://arxiv.org/abs/1306.5480 | id:1306.5480 author:Benjamin Kunsberg, Steven W. Zucker category:cs.CV q-bio.NC  published:2013-06-23 summary:Shape from shading is a classical inverse problem in computer vision. This shape reconstruction problem is inherently ill-defined; it depends on the assumed light source direction. We introduce a novel mathematical formulation for calculating local surface shape based on covariant derivatives of the shading flow field, rather than the customary integral minimization or P.D.E approaches. On smooth surfaces, we show second derivatives of brightness are independent of the light sources and can be directly related to surface properties. We use these measurements to define the matching local family of surfaces that can result from any given shading patch, changing the emphasis to characterizing ambiguity in the problem. We give an example of how these local surface ambiguities collapse along certain image contours and how this can be used for the reconstruction problem. version:1
arxiv-1306-1840 | Loss-Proportional Subsampling for Subsequent ERM | http://arxiv.org/abs/1306.1840 | id:1306.1840 author:Paul Mineiro, Nikos Karampatziakis category:cs.LG stat.ML  published:2013-06-07 summary:We propose a sampling scheme suitable for reducing a data set prior to selecting a hypothesis with minimum empirical risk. The sampling only considers a subset of the ultimate (unknown) hypothesis set, but can nonetheless guarantee that the final excess risk will compare favorably with utilizing the entire original data set. We demonstrate the practical benefits of our approach on a large dataset which we subsample and subsequently fit with boosted trees. version:2
arxiv-1306-5368 | A Variational Approximations-DIC Rubric for Parameter Estimation and Mixture Model Selection Within a Family Setting | http://arxiv.org/abs/1306.5368 | id:1306.5368 author:Sanjeena Subedi, Paul McNicholas category:stat.ME stat.CO stat.ML  published:2013-06-23 summary:Mixture model-based clustering has become an increasingly popular data analysis technique since its introduction fifty years ago, and is now commonly utilized within the family setting. Families of mixture models arise when the component parameters, usually the component covariance matrices, are decomposed and a number of constraints are imposed. Within the family setting, we need to choose the member of the family, i.e., the appropriate covariance structure, in addition to the number of mixture components. To date, the Bayesian information criterion (BIC) has proven most effective for model selection, and the expectation-maximization (EM) algorithm is usually used for parameter estimation. To date, this EM-BIC rubric has monopolized the literature on families of mixture models. We deviate from this rubric, using variational Bayes approximations for parameter estimation and the deviance information criterion for model selection. The variational Bayes approach alleviates some of the computational complexities associated with the EM algorithm by constructing a tight lower bound on the complex marginal likelihood and maximizing this lower bound by minimizing the associated Kullback-Leibler divergence. We use this approach on the most famous family of Gaussian mixture models within the literature, and real and simulated data are used to compare our approach to the EM-BIC rubric. version:1
arxiv-1306-5362 | A Statistical Perspective on Algorithmic Leveraging | http://arxiv.org/abs/1306.5362 | id:1306.5362 author:Ping Ma, Michael W. Mahoney, Bin Yu category:stat.ME cs.LG stat.ML  published:2013-06-23 summary:One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method. In this paper, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms. A detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets. The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance. version:1
arxiv-1306-5349 | Song-based Classification techniques for Endangered Bird Conservation | http://arxiv.org/abs/1306.5349 | id:1306.5349 author:Erick Stattner, Wilfried Segretier, Martine Collard, Philippe Hunel, Nicolas Vidot category:cs.LG  published:2013-06-22 summary:The work presented in this paper is part of a global framework which long term goal is to design a wireless sensor network able to support the observation of a population of endangered birds. We present the first stage for which we have conducted a knowledge discovery approach on a sample of acoustical data. We use MFCC features extracted from bird songs and we exploit two knowledge discovery techniques. One that relies on clustering-based approaches, that highlights the homogeneity in the songs of the species. The other, based on predictive modeling, that demonstrates the good performances of various machine learning techniques for the identification process. The knowledge elicited provides promising results to consider a widespread study and to elicit guidelines for designing a first version of the automatic approach for data collection based on acoustic sensors. version:1
arxiv-1306-5310 | Online dictionary learning for kernel LMS. Analysis and forward-backward splitting algorithm | http://arxiv.org/abs/1306.5310 | id:1306.5310 author:Wei Gao, Jie Chen, CÃ©dric Richard, Jianguo Huang category:stat.ML  published:2013-06-22 summary:Adaptive filtering algorithms operating in reproducing kernel Hilbert spaces have demonstrated superiority over their linear counterpart for nonlinear system identification. Unfortunately, an undesirable characteristic of these methods is that the order of the filters grows linearly with the number of input data. This dramatically increases the computational burden and memory requirement. A variety of strategies based on dictionary learning have been proposed to overcome this severe drawback. Few, if any, of these works analyze the problem of updating the dictionary in a time-varying environment. In this paper, we present an analytical study of the convergence behavior of the Gaussian least-mean-square algorithm in the case where the statistics of the dictionary elements only partially match the statistics of the input data. This allows us to emphasize the need for updating the dictionary in an online way, by discarding the obsolete elements and adding appropriate ones. We introduce a kernel least-mean-square algorithm with L1-norm regularization to automatically perform this task. The stability in the mean of this method is analyzed, and its performance is tested with experiments. version:1
arxiv-1306-5308 | Cognitive Interpretation of Everyday Activities: Toward Perceptual Narrative Based Visuo-Spatial Scene Interpretation | http://arxiv.org/abs/1306.5308 | id:1306.5308 author:Mehul Bhatt, Jakob Suchan, Carl Schultz category:cs.AI cs.CV cs.HC cs.RO  published:2013-06-22 summary:We position a narrative-centred computational model for high-level knowledge representation and reasoning in the context of a range of assistive technologies concerned with "visuo-spatial perception and cognition" tasks. Our proposed narrative model encompasses aspects such as \emph{space, events, actions, change, and interaction} from the viewpoint of commonsense reasoning and learning in large-scale cognitive systems. The broad focus of this paper is on the domain of "human-activity interpretation" in smart environments, ambient intelligence etc. In the backdrop of a "smart meeting cinematography" domain, we position the proposed narrative model, preliminary work on perceptual narrativisation, and the immediate outlook on constructing general-purpose open-source tools for perceptual narrativisation. ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- Cognitive Simulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10 Vision and Scene Understanding: Architecture and control structures, Motion, Perceptual reasoning, Shape, Video analysis General keywords: cognitive systems; human-computer interaction; spatial cognition and computation; commonsense reasoning; spatial and temporal reasoning; assistive technologies version:1
arxiv-1202-2518 | Segmenting DNA sequence into `words' | http://arxiv.org/abs/1202.2518 | id:1202.2518 author:Wang Liang category:q-bio.GN cs.CL  published:2012-02-12 summary:This paper presents a novel method to segment/decode DNA sequences based on n-grams statistical language model. Firstly, we find the length of most DNA 'words' is 12 to 15 bps by analyzing the genomes of 12 model species. Then we design an unsupervised probability based approach to segment the DNA sequences. The benchmark of segmenting method is also proposed. version:4
arxiv-1306-5293 | New Approach of Estimating PSNR-B For De-blocked Images | http://arxiv.org/abs/1306.5293 | id:1306.5293 author:S. Aruna Mastani, K. Shilpa category:cs.CV  published:2013-06-22 summary:Measurement of image quality is very crucial to many image processing applications. Quality metrics are used to measure the quality of improvement in the images after they are processed and compared with the original images. Compression is one of the applications where it is required to monitor the quality of decompressed or decoded image. JPEG compression is the lossy compression which is most prevalent technique for image codecs. But it suffers from blocking artifacts. Various deblocking filters are used to reduce blocking artifacts. The efficiency of deblocking filters which improves visual signals degraded by blocking artifacts from compression will also be studied. Objective quality metrics like PSNR, SSIM, and PSNRB for analyzing the quality of deblocked images will be studied. We introduce a new approach of PSNR-B for analyzing quality of deblocked images. Simulation results show that new approach of PSNR-B called modified PSNR-B. it gives even better results compared to existing well known blockiness specific indices version:1
arxiv-1306-5263 | Discriminative Training: Learning to Describe Video with Sentences, from Video Described with Sentences | http://arxiv.org/abs/1306.5263 | id:1306.5263 author:Haonan Yu, Jeffrey Mark Siskind category:cs.CV cs.CL  published:2013-06-21 summary:We present a method for learning word meanings from complex and realistic video clips by discriminatively training (DT) positive sentential labels against negative ones, and then use the trained word models to generate sentential descriptions for new video. This new work is inspired by recent work which adopts a maximum likelihood (ML) framework to address the same problem using only positive sentential labels. The new method, like the ML-based one, is able to automatically determine which words in the sentence correspond to which concepts in the video (i.e., ground words to meanings) in a weakly supervised fashion. While both DT and ML yield comparable results with sufficient training data, DT outperforms ML significantly with smaller training sets because it can exploit negative training labels to better constrain the learning problem. version:1
arxiv-1202-3505 | Near-optimal Coresets For Least-Squares Regression | http://arxiv.org/abs/1202.3505 | id:1202.3505 author:Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail category:cs.DS cs.LG  published:2012-02-16 summary:We study (constrained) least-squares regression as well as multiple response least-squares regression and ask the question of whether a subset of the data, a coreset, suffices to compute a good approximate solution to the regression. We give deterministic, low order polynomial-time algorithms to construct such coresets with approximation guarantees, together with lower bounds indicating that there is not much room for improvement upon our results. version:2
arxiv-1109-5664 | Deterministic Feature Selection for $k$-means Clustering | http://arxiv.org/abs/1109.5664 | id:1109.5664 author:Christos Boutsidis, Malik Magdon-Ismail category:cs.LG cs.DS  published:2011-09-26 summary:We study feature selection for $k$-means clustering. Although the literature contains many methods with good empirical performance, algorithms with provable theoretical behavior have only recently been developed. Unfortunately, these algorithms are randomized and fail with, say, a constant probability. We address this issue by presenting a deterministic feature selection algorithm for k-means with theoretical guarantees. At the heart of our algorithm lies a deterministic method for decompositions of the identity. version:4
arxiv-1306-5170 | Clinical Relationships Extraction Techniques from Patient Narratives | http://arxiv.org/abs/1306.5170 | id:1306.5170 author:Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, Mohamed Monier Hassan category:cs.IR cs.CL  published:2013-06-21 summary:The Clinical E-Science Framework (CLEF) project was used to extract important information from medical texts by building a system for the purpose of clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. The system is divided into two parts, one part concerns with the identification of relationships between clinically important entities in the text. The full parses and domain-specific grammars had been used to apply many approaches to extract the relationship. In the second part of the system, statistical machine learning (ML) approaches are applied to extract relationship. A corpus of oncology narratives that hand annotated with clinical relationships can be used to train and test a system that has been designed and implemented by supervised machine learning (ML) approaches. Many features can be extracted from these texts that are used to build a model by the classifier. Multiple supervised machine learning algorithms can be applied for relationship extraction. Effects of adding the features, changing the size of the corpus, and changing the type of the algorithm on relationship extraction are examined. Keywords: Text mining; information extraction; NLP; entities; and relations. version:1
arxiv-1306-5151 | Fine-Grained Visual Classification of Aircraft | http://arxiv.org/abs/1306.5151 | id:1306.5151 author:Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, Andrea Vedaldi category:cs.CV  published:2013-06-21 summary:This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding. version:1
arxiv-1306-5096 | Computer Aided ECG Analysis - State of the Art and Upcoming Challenges | http://arxiv.org/abs/1306.5096 | id:1306.5096 author:Marko Velic, Ivan Padavic, Sinisa Car category:cs.CV  published:2013-06-21 summary:In this paper we present current achievements in computer aided ECG analysis and their applicability in real world medical diagnosis process. Most of the current work is covering problems of removing noise, detecting heartbeats and rhythm-based analysis. There are some advancements in particular ECG segments detection and beat classifications but with limited evaluations and without clinical approvals. This paper presents state of the art advancements in those areas till present day. Besides this short computer science and signal processing literature review, paper covers future challenges regarding the ECG signal morphology analysis deriving from the medical literature review. Paper is concluded with identified gaps in current advancements and testing, upcoming challenges for future research and a bullseye test is suggested for morphology analysis evaluation. version:1
arxiv-1306-5070 | 3-SAT Problem A New Memetic-PSO Algorithm | http://arxiv.org/abs/1306.5070 | id:1306.5070 author:Nasser Lotfi, Jamshid Tamouk, Mina Farmanbar category:cs.AI cs.NE  published:2013-06-21 summary:3-SAT problem is of great importance to many technical and scientific applications. This paper presents a new hybrid evolutionary algorithm for solving this satisfiability problem. 3-SAT problem has the huge search space and hence it is known as a NP-hard problem. So, deterministic approaches are not applicable in this context. Thereof, application of evolutionary processing approaches and especially PSO will be very effective for solving these kinds of problems. In this paper, we introduce a new evolutionary optimization technique based on PSO, Memetic algorithm and local search approaches. When some heuristics are mixed, their advantages are collected as well and we can reach to the better outcomes. Finally, we test our proposed algorithm over some benchmarks used by some another available algorithms. Obtained results show that our new method leads to the suitable results by the appropriate time. Thereby, it achieves a better result in compared with the existent approaches such as pure genetic algorithm and some verified types version:1
arxiv-1210-2022 | Locally adaptive factor processes for multivariate time series | http://arxiv.org/abs/1210.2022 | id:1210.2022 author:Daniele Durante, Bruno Scarpa, David B. Dunson category:stat.AP stat.ML  published:2012-10-07 summary:In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application. version:2
arxiv-1306-4966 | Determining Points on Handwritten Mathematical Symbols | http://arxiv.org/abs/1306.4966 | id:1306.4966 author:Rui Hu, Stephen M. Watt category:cs.CV cs.CY  published:2013-06-20 summary:In a variety of applications, such as handwritten mathematics and diagram labelling, it is common to have symbols of many different sizes in use and for the writing not to follow simple baselines. In order to understand the scale and relative positioning of individual characters, it is necessary to identify the location of certain expected features. These are typically identified by particular points in the symbols, for example, the baseline of a lower case "p" would be identified by the lowest part of the bowl, ignoring the descender. We investigate how to find these special points automatically so they may be used in a number of problems, such as improving two-dimensional mathematical recognition and in handwriting neatening, while preserving the original style. version:1
arxiv-1306-4943 | Failure of Calibration is Typical | http://arxiv.org/abs/1306.4943 | id:1306.4943 author:Gordon Belot category:math.ST stat.ML stat.TH  published:2013-06-20 summary:Schervish (1985b) showed that every forecasting system is noncalibrated for uncountably many data sequences that it might see. This result is strengthened here: from a topological point of view, failure of calibration is typical and calibration rare. Meanwhile, Bayesian forecasters are certain that they are calibrated---this invites worries about the connection between Bayesianism and rationality. version:1
arxiv-1306-4908 | Recognition of Named-Event Passages in News Articles | http://arxiv.org/abs/1306.4908 | id:1306.4908 author:Luis Marujo, Wang Ling, Anatole Gershman, Jaime Carbonell, JoÃ£o P. Neto, David Matos category:cs.CL cs.IR  published:2013-06-20 summary:We extend the concept of Named Entities to Named Events - commonly occurring events such as battles and earthquakes. We propose a method for finding specific passages in news articles that contain information about such events and report our preliminary evaluation results. Collecting "Gold Standard" data presents many problems, both practical and conceptual. We present a method for obtaining such data using the Amazon Mechanical Turk service. version:1
arxiv-1306-4905 | From-Below Approximations in Boolean Matrix Factorization: Geometry and New Algorithm | http://arxiv.org/abs/1306.4905 | id:1306.4905 author:Radim Belohlavek, Martin Trnecka category:cs.NA cs.LG  published:2013-06-20 summary:We present new results on Boolean matrix factorization and a new algorithm based on these results. The results emphasize the significance of factorizations that provide from-below approximations of the input matrix. While the previously proposed algorithms do not consider the possibly different significance of different matrix entries, our results help measure such significance and suggest where to focus when computing factors. An experimental evaluation of the new algorithm on both synthetic and real data demonstrates its good performance in terms of good coverage by the first k factors as well as a small number of factors needed for exact decomposition and indicates that the algorithm outperforms the available ones in these terms. We also propose future research topics. version:1
arxiv-1306-4890 | Key Phrase Extraction of Lightly Filtered Broadcast News | http://arxiv.org/abs/1306.4890 | id:1306.4890 author:Luis Marujo, Ricardo Ribeiro, David Martins de Matos, JoÃ£o P. Neto, Anatole Gershman, Jaime Carbonell category:cs.CL cs.IR  published:2013-06-20 summary:This paper explores the impact of light filtering on automatic key phrase extraction (AKE) applied to Broadcast News (BN). Key phrases are words and expressions that best characterize the content of a document. Key phrases are often used to index the document or as features in further processing. This makes improvements in AKE accuracy particularly important. We hypothesized that filtering out marginally relevant sentences from a document would improve AKE accuracy. Our experiments confirmed this hypothesis. Elimination of as little as 10% of the document sentences lead to a 2% improvement in AKE precision and recall. AKE is built over MAUI toolkit that follows a supervised learning approach. We trained and tested our AKE method on a gold standard made of 8 BN programs containing 110 manually annotated news stories. The experiments were conducted within a Multimedia Monitoring Solution (MMS) system for TV and radio news/programs, running daily, and monitoring 12 TV and 4 radio channels. version:1
arxiv-1301-5650 | Regularization and nonlinearities for neural language models: when are they needed? | http://arxiv.org/abs/1301.5650 | id:1301.5650 author:Marius Pachitariu, Maneesh Sahani category:stat.ML cs.LG  published:2013-01-23 summary:Neural language models (LMs) based on recurrent neural networks (RNN) are some of the most successful word and character-level LMs. Why do they work so well, in particular better than linear neural LMs? Possible explanations are that RNNs have an implicitly better regularization or that RNNs have a higher capacity for storing patterns due to their nonlinearities or both. Here we argue for the first explanation in the limit of little training data and the second explanation for large amounts of text data. We show state-of-the-art performance on the popular and small Penn dataset when RNN LMs are regularized with random dropout. Nonetheless, we show even better performance from a simplified, much less expressive linear RNN model without off-diagonal entries in the recurrent matrix. We call this model an impulse-response LM (IRLM). Using random dropout, column normalization and annealed learning rates, IRLMs develop neurons that keep a memory of up to 50 words in the past and achieve a perplexity of 102.5 on the Penn dataset. On two large datasets however, the same regularization methods are unsuccessful for both models and the RNN's expressivity allows it to overtake the IRLM by 10 and 20 percent perplexity, respectively. Despite the perplexity gap, IRLMs still outperform RNNs on the Microsoft Research Sentence Completion (MRSC) task. We develop a slightly modified IRLM that separates long-context units (LCUs) from short-context units and show that the LCUs alone achieve a state-of-the-art performance on the MRSC task of 60.8%. Our analysis indicates that a fruitful direction of research for neural LMs lies in developing more accessible internal representations, and suggests an optimization regime of very high momentum terms for effectively training such models. version:2
arxiv-1306-4886 | Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization | http://arxiv.org/abs/1306.4886 | id:1306.4886 author:Luis Marujo, Anatole Gershman, Jaime Carbonell, Robert Frederking, JoÃ£o P. Neto category:cs.CL cs.IR  published:2013-06-20 summary:Fast and effective automated indexing is critical for search and personalized services. Key phrases that consist of one or more words and represent the main concepts of the document are often used for the purpose of indexing. In this paper, we investigate the use of additional semantic features and pre-processing steps to improve automatic key phrase extraction. These features include the use of signal words and freebase categories. Some of these features lead to significant improvements in the accuracy of the results. We also experimented with 2 forms of document pre-processing that we call light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-reference normalization unifies several written forms of the same named entity into a unique form. We also needed a "Gold Standard" - a set of labeled documents for training and evaluation. While the subjective nature of key phrase selection precludes a true "Gold Standard", we used Amazon's Mechanical Turk service to obtain a useful approximation. Our data indicates that the biggest improvements in performance were due to shallow semantic features, news categories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of deeper semantic features such as Freebase sub-categories was not beneficial by itself, but in combination with pre-processing, did cause slight improvements in the nDCG scores. version:1
arxiv-1306-0404 | Iterative Grassmannian Optimization for Robust Image Alignment | http://arxiv.org/abs/1306.0404 | id:1306.0404 author:Jun He, Dejiao Zhang, Laura Balzano, Tao Tao category:cs.CV math.OC stat.ML  published:2013-06-03 summary:Robust high-dimensional data processing has witnessed an exciting development in recent years, as theoretical results have shown that it is possible using convex programming to optimize data fit to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face recognition, the opportunity to process massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA, or "Transformed GRASTA (Grassmannian Robust Adaptive Subspace Tracking Algorithm)". t-GRASTA iteratively performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate a decomposition of a collection of images into a low-rank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4 $\times$ faster than state-of-the-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images. version:2
arxiv-1205-4839 | Off-Policy Actor-Critic | http://arxiv.org/abs/1205.4839 | id:1205.4839 author:Thomas Degris, Martha White, Richard S. Sutton category:cs.LG  published:2012-05-22 summary:This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems. version:5
arxiv-1306-4793 | Evolving Boolean Regulatory Networks with Epigenetic Control | http://arxiv.org/abs/1306.4793 | id:1306.4793 author:Larry Bull category:cs.NE q-bio.MN  published:2013-06-20 summary:The significant role of epigenetic mechanisms within natural systems has become increasingly clear. This paper uses a recently presented abstract, tunable Boolean genetic regulatory network model to explore aspects of epigenetics. It is shown how dynamically controlling transcription via a DNA methylation-inspired mechanism can be selected for by simulated evolution under various single and multiple cell scenarios. Further, it is shown that the effects of such control can be inherited without detriment to fitness. version:1
arxiv-1306-4758 | Analysing Word Importance for Image Annotation | http://arxiv.org/abs/1306.4758 | id:1306.4758 author:Payal Gulati, A. K. Sharma category:cs.IR cs.CV  published:2013-06-20 summary:Image annotation provides several keywords automatically for a given image based on various tags to describe its contents which is useful in Image retrieval. Various researchers are working on text based and content based image annotations [7,9]. It is seen, in traditional Image annotation approaches, annotation words are treated equally without considering the importance of each word in real world. In context of this, in this work, images are annotated with keywords based on their frequency count and word correlation. Moreover this work proposes an approach to compute importance score of candidate keywords, having same frequency count. version:1
arxiv-1209-6425 | Gene selection with guided regularized random forest | http://arxiv.org/abs/1209.6425 | id:1209.6425 author:Houtao Deng, George Runger category:cs.LG cs.CE  published:2012-09-28 summary:The regularized random forest (RRF) was recently proposed for feature selection by building only one ensemble. In RRF the features are evaluated on a part of the training data at each tree node. We derive an upper bound for the number of distinct Gini information gain values in a node, and show that many features can share the same information gain at a node with a small number of instances and a large number of features. Therefore, in a node with a small number of instances, RRF is likely to select a feature not strongly relevant. Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In GRRF, the importance scores from an ordinary random forest (RF) are used to guide the feature selection process in RRF. Experiments on 10 gene data sets show that the accuracy performance of GRRF is, in general, more robust than RRF when their parameters change. GRRF is computationally efficient, can select compact feature subsets, and has competitive accuracy performance, compared to RRF, varSelRF and LASSO logistic regression (with evaluations from an RF classifier). Also, RF applied to the features selected by RRF with the minimal regularization outperforms RF applied to all the features for most of the data sets considered here. Therefore, if accuracy is considered more important than the size of the feature subset, RRF with the minimal regularization may be considered. We use the accuracy performance of RF, a strong classifier, to evaluate feature selection methods, and illustrate that weak classifiers are less capable of capturing the information contained in a feature subset. Both RRF and GRRF were implemented in the "RRF" R package available at CRAN, the official R package archive. version:3
arxiv-1306-4753 | Galerkin Methods for Complementarity Problems and Variational Inequalities | http://arxiv.org/abs/1306.4753 | id:1306.4753 author:Geoffrey J. Gordon category:cs.LG cs.AI math.OC  published:2013-06-20 summary:Complementarity problems and variational inequalities arise in a wide variety of areas, including machine learning, planning, game theory, and physical simulation. In all of these areas, to handle large-scale problem instances, we need fast approximate solution methods. One promising idea is Galerkin approximation, in which we search for the best answer within the span of a given set of basis functions. Bertsekas proposed one possible Galerkin method for variational inequalities. However, this method can exhibit two problems in practice: its approximation error is worse than might be expected based on the ability of the basis to represent the desired solution, and each iteration requires a projection step that is not always easy to implement efficiently. So, in this paper, we present a new Galerkin method with improved behavior: our new error bounds depend directly on the distance from the true solution to the subspace spanned by our basis, and the only projections we require are onto the feasible region or onto the span of our basis. version:1
arxiv-1306-4746 | Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance | http://arxiv.org/abs/1306.4746 | id:1306.4746 author:Daniel Paul Barrett, Jeffrey Mark Siskind category:cs.CV  published:2013-06-20 summary:We propose a method which can detect events in videos by modeling the change in appearance of the event participants over time. This method makes it possible to detect events which are characterized not by motion, but by the changing state of the people or objects involved. This is accomplished by using object detectors as output models for the states of a hidden Markov model (HMM). The method allows an HMM to model the sequence of poses of the event participants over time, and is effective for poses of humans and inanimate objects. The ability to use existing object-detection methods as part of an event model makes it possible to leverage ongoing work in the object-detection community. A novel training method uses an EM loop to simultaneously learn the temporal structure and object models automatically, without the need to specify either the individual poses to be modeled or the frames in which they occur. The E-step estimates the latent assignment of video frames to HMM states, while the M-step estimates both the HMM transition probabilities and state output models, including the object detectors, which are trained on the weighted subset of frames assigned to their state. A new dataset was gathered because little work has been done on events characterized by changing object pose, and suitable datasets are not available. Our method produced results superior to that of comparison systems on this dataset. version:1
arxiv-1306-4724 | Computer simulation based parameter selection for resistance exercise | http://arxiv.org/abs/1306.4724 | id:1306.4724 author:Ognjen Arandjelovic category:cs.CV cs.HC  published:2013-06-20 summary:In contrast to most scientific disciplines, sports science research has been characterized by comparatively little effort investment in the development of relevant phenomenological models. Scarcer yet is the application of said models in practice. We present a framework which allows resistance training practitioners to employ a recently proposed neuromuscular model in actual training program design. The first novelty concerns the monitoring aspect of coaching. A method for extracting training performance characteristics from loosely constrained video sequences, effortlessly and with minimal human input, using computer vision is described. The extracted data is subsequently used to fit the underlying neuromuscular model. This is achieved by solving an inverse dynamics problem corresponding to a particular exercise. Lastly, a computer simulation of hypothetical training bouts, using athlete-specific capability parameters, is used to predict the effected adaptation and changes in performance. The software described here allows the practitioner to manipulate hypothetical training parameters and immediately see their effect on predicted adaptation for a specific athlete. Thus, this work presents a holistic view of the monitoring-assessment-adjustment loop. version:1
arxiv-1306-2499 | Using Arabic Wordnet for semantic indexation in information retrieval system | http://arxiv.org/abs/1306.2499 | id:1306.2499 author:Mohammed Alaeddine Abderrahim, Mohammed El Amine Abderrahim, Mohammed Amine Chikh category:cs.IR cs.CL  published:2013-06-11 summary:In the context of arabic Information Retrieval Systems (IRS) guided by arabic ontology and to enable those systems to better respond to user requirements, this paper aims to representing documents and queries by the best concepts extracted from Arabic Wordnet. Identified concepts belonging to Arabic WordNet synsets are extracted from documents and queries, and those having a single sense are expanded. The expanded query is then used by the IRS to retrieve the relevant documents searched. Our experiments are based primarily on a medium size corpus of arabic text. The results obtained shown us that there are a global improvement in the performance of the arabic IRS. version:2
arxiv-1306-4629 | Non-Correlated Character Recognition using Artificial Neural Network | http://arxiv.org/abs/1306.4629 | id:1306.4629 author:Tirtharaj Dash, Tanistha Nayak category:cs.NE cs.CV  published:2013-06-19 summary:This paper investigates a method of Handwritten English Character Recognition using Artificial Neural Network (ANN). This work has been done in offline Environment for non correlated characters, which do not possess any linear relationships among them. We test that whether the particular tested character belongs to a cluster or not. The implementation is carried out in Matlab environment and successfully tested. Fifty-two sets of English alphabets are used to train the ANN and test the network. The algorithms are tested with 26 capital letters and 26 small letters. The testing result showed that the proposed ANN based algorithm showed a maximum recognition rate of 85%. version:1
arxiv-1306-4622 | Solution to Quadratic Equation Using Genetic Algorithm | http://arxiv.org/abs/1306.4622 | id:1306.4622 author:Tanistha Nayak, Tirtharaj Dash category:cs.NE  published:2013-06-19 summary:Solving Quadratic equation is one of the intrinsic interests as it is the simplest nonlinear equations. A novel approach for solving Quadratic Equation based on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are a technique to solve problems which need optimization. Generation of trial solutions have been formed by this method. Many examples have been worked out, and in most cases we find out the exact solution. We have discussed the effect of different parameters on the performance of the developed algorithm. The results are concluded after rigorous testing on different equations. version:1
arxiv-1306-4621 | English Character Recognition using Artificial Neural Network | http://arxiv.org/abs/1306.4621 | id:1306.4621 author:Tirtharaj Dash, Tanistha Nayak category:cs.NE  published:2013-06-19 summary:This work focuses on development of a Offline Hand Written English Character Recognition algorithm based on Artificial Neural Network (ANN). The ANN implemented in this work has single output neuron which shows whether the tested character belongs to a particular cluster or not. The implementation is carried out completely in 'C' language. Ten sets of English alphabets (small-26, capital-26) were used to train the ANN and 5 sets of English alphabets were used to test the network. The characters were collected from different persons over duration of about 25 days. The algorithm was tested with 5 capital letters and 5 small letter sets. However, the result showed that the algorithm recognized English alphabet patterns with maximum accuracy of 92.59% and False Rejection Rate (FRR) of 0%. version:1
arxiv-1306-4592 | Time Efficient Approach To Offline Hand Written Character Recognition Using Associative Memory Net | http://arxiv.org/abs/1306.4592 | id:1306.4592 author:Tirtharaj Dash category:cs.NE cs.CV  published:2013-06-19 summary:In this paper, an efficient Offline Hand Written Character Recognition algorithm is proposed based on Associative Memory Net (AMN). The AMN used in this work is basically auto associative. The implementation is carried out completely in 'C' language. To make the system perform to its best with minimal computation time, a Parallel algorithm is also developed using an API package OpenMP. Characters are mainly English alphabets (Small (26), Capital (26)) collected from system (52) and from different persons (52). The characters collected from system are used to train the AMN and characters collected from different persons are used for testing the recognition ability of the net. The detailed analysis showed that the network recognizes the hand written characters with recognition rate of 72.20% in average case. However, in best case, it recognizes the collected hand written characters with 88.5%. The developed network consumes 3.57 sec (average) in Serial implementation and 1.16 sec (average) in Parallel implementation using OpenMP. version:1
arxiv-1205-5050 | A lasso for hierarchical interactions | http://arxiv.org/abs/1205.5050 | id:1205.5050 author:Jacob Bien, Jonathan Taylor, Robert Tibshirani category:stat.ME math.ST stat.ML stat.TH  published:2012-05-22 summary:We add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting "saved" by the hierarchy constraint. We distinguish between parameter sparsity - the number of nonzero coefficients - and practical sparsity - the number of raw variables one must measure to make a new prediction. Hierarchy focuses on the latter, which is more closely tied to important data collection concerns such as cost, time and effort. We develop an algorithm, available in the R package hierNet, and perform an empirical study of our method. version:3
arxiv-1306-4447 | Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers | http://arxiv.org/abs/1306.4447 | id:1306.4447 author:Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali category:cs.CR cs.LG stat.ML  published:2013-06-19 summary:Machine Learning (ML) algorithms are used to train computers to perform a variety of complex tasks and improve with experience. Computers learn how to recognize patterns, make unintended decisions, or react to a dynamic environment. Certain trained machines may be more effective than others because they are based on more suitable ML algorithms or because they were trained through superior training sets. Although ML algorithms are known and publicly released, training sets may not be reasonably ascertainable and, indeed, may be guarded as trade secrets. While much research has been performed about the privacy of the elements of training sets, in this paper we focus our attention on ML classifiers and on the statistical information that can be unconsciously or maliciously revealed from them. We show that it is possible to infer unexpected but useful information from ML classifiers. In particular, we build a novel meta-classifier and train it to hack other classifiers, obtaining meaningful information about their training sets. This kind of information leakage can be exploited, for example, by a vendor to build more effective classifiers or to simply acquire trade secrets from a competitor's apparatus, potentially violating its intellectual property rights. version:1
arxiv-1112-0698 | Machine Learning with Operational Costs | http://arxiv.org/abs/1112.0698 | id:1112.0698 author:Theja Tulabandhula, Cynthia Rudin category:stat.ML cs.AI math.OC  published:2011-12-03 summary:This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization. version:4
arxiv-1306-4410 | Joint estimation of sparse multivariate regression and conditional graphical models | http://arxiv.org/abs/1306.4410 | id:1306.4410 author:Junhui Wang category:stat.ML cs.LG  published:2013-06-19 summary:Multivariate regression model is a natural generalization of the classical univari- ate regression model for ?tting multiple responses. In this paper, we propose a high- dimensional multivariate conditional regression model for constructing sparse estimates of the multivariate regression coe?cient matrix that accounts for the dependency struc- ture among the multiple responses. The proposed method decomposes the multivariate regression problem into a series of penalized conditional log-likelihood of each response conditioned on the covariates and other responses. It allows simultaneous estimation of the sparse regression coe?cient matrix and the sparse inverse covariance matrix. The asymptotic selection consistency and normality are established for the diverging dimension of the covariates and number of responses. The e?ectiveness of the pro- posed method is also demonstrated in a variety of simulated examples as well as an application to the Glioblastoma multiforme cancer data. version:1
arxiv-1306-4345 | An Overview of the Research on Texture Based Plant Leaf Classification | http://arxiv.org/abs/1306.4345 | id:1306.4345 author:Vishakha Metre, Jayshree Ghorpade category:cs.CV  published:2013-06-18 summary:Plant classification has a broad application prospective in agriculture and medicine, and is especially significant to the biology diversity research. As plants are vitally important for environmental protection, it is more important to identify and classify them accurately. Plant leaf classification is a technique where leaf is classified based on its different morphological features. The goal of this paper is to provide an overview of different aspects of texture based plant leaf classification and related things. At last we will be concluding about the efficient method i.e. the method that gives better performance compared to the other methods. version:1
arxiv-1207-2743 | The evolutionary origins of modularity | http://arxiv.org/abs/1207.2743 | id:1207.2743 author:Jeff Clune, Jean-Baptiste Mouret, Hod Lipson category:q-bio.PE cs.NE q-bio.MN q-bio.NC  published:2012-07-11 summary:A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks--their organization as functional, sparsely connected subunits--but there is no consensus regarding why modularity itself evolved. While most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyze research in numerous disciplines, including neuroscience, genetics and harnessing evolution for engineering purposes. version:2
arxiv-1011-1161 | Multiarmed Bandit Problems with Delayed Feedback | http://arxiv.org/abs/1011.1161 | id:1011.1161 author:Sudipto Guha, Kamesh Munagala, Martin Pal category:cs.DS cs.LG  published:2010-11-04 summary:In this paper we initiate the study of optimization of bandit type problems in scenarios where the feedback of a play is not immediately known. This arises naturally in allocation problems which have been studied extensively in the literature, albeit in the absence of delays in the feedback. We study this problem in the Bayesian setting. In presence of delays, no solution with provable guarantees is known to exist with sub-exponential running time. We show that bandit problems with delayed feedback that arise in allocation settings can be forced to have significant structure, with a slight loss in optimality. This structure gives us the ability to reason about the relationship of single arm policies to the entangled optimum policy, and eventually leads to a O(1) approximation for a significantly general class of priors. The structural insights we develop are of key interest and carry over to the setting where the feedback of an action is available instantaneously, and we improve all previous results in this setting as well. version:3
arxiv-1212-2136 | A class of random fields on complete graphs with tractable partition function | http://arxiv.org/abs/1212.2136 | id:1212.2136 author:Boris Flach category:cs.LG stat.ML  published:2012-12-10 summary:The aim of this short note is to draw attention to a method by which the partition function and marginal probabilities for a certain class of random fields on complete graphs can be computed in polynomial time. This class includes Ising models with homogeneous pairwise potentials but arbitrary (inhomogeneous) unary potentials. Similarly, the partition function and marginal probabilities can be computed in polynomial time for random fields on complete bipartite graphs, provided they have homogeneous pairwise potentials. We expect that these tractable classes of large scale random fields can be very useful for the evaluation of approximation algorithms by providing exact error estimates. version:2
arxiv-1306-3530 | Generalized Beta Divergence | http://arxiv.org/abs/1306.3530 | id:1306.3530 author:Y. Kenan Yilmaz category:stat.ML  published:2013-06-14 summary:This paper generalizes beta divergence beyond its classical form associated with power variance functions of Tweedie models. Generalized form is represented by a compact definite integral as a function of variance function of the exponential dispersion model. This compact integral form simplifies derivations of many properties such as scaling, translation and expectation of the beta divergence. Further, we show that beta divergence and (half of) the statistical deviance are equivalent measures. version:2
arxiv-1306-4152 | Bioclimating Modelling: A Machine Learning Perspective | http://arxiv.org/abs/1306.4152 | id:1306.4152 author:Maumita Bhattacharya category:cs.LG stat.ML 68T05  published:2013-06-18 summary:Many machine learning (ML) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. Applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. However, success of machine learning-based approaches depends on a number of factors. While it can be safely said that no particular ML technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behaviour to ensure informed choice of techniques. This paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. Considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling. version:1
arxiv-1306-4139 | Punjabi Language Interface to Database: a brief review | http://arxiv.org/abs/1306.4139 | id:1306.4139 author:Preeti Verma, Suket Arora, Kamaljit Batra category:cs.CL cs.HC  published:2013-06-18 summary:Unlike most user-computer interfaces, a natural language interface allows users to communicate fluently with a computer system with very little preparation. Databases are often hard to use in cooperating with the users because of their rigid interface. A good NLIDB allows a user to enter commands and ask questions in native language and then after interpreting respond to the user in native language. For a large number of applications requiring interaction between humans and the computer systems, it would be convenient to provide the end-user friendly interface. Punjabi language interface to database would proof fruitful to native people of Punjab, as it provides ease to them to use various e-governance applications like Punjab Sewa, Suwidha, Online Public Utility Forms, Online Grievance Cell, Land Records Management System,legacy matters, e-District, agriculture, etc. Punjabi is the mother tongue of more than 110 million people all around the world. According to available information, Punjabi ranks 10th from top out of a total of 6,900 languages recognized internationally by the United Nations. This paper covers a brief overview of the Natural language interface to database, its different components, its advantages, disadvantages, approaches and techniques used. The paper ends with the work done on Punjabi language interface to database and future enhancements that can be done. version:1
arxiv-1306-4134 | Dialogue System: A Brief Review | http://arxiv.org/abs/1306.4134 | id:1306.4134 author:Suket Arora, Kamaljeet Batra, Sarabjit Singh category:cs.CL  published:2013-06-18 summary:A Dialogue System is a system which interacts with human in natural language. At present many universities are developing the dialogue system in their regional language. This paper will discuss about dialogue system, its components, challenges and its evaluation. This paper helps the researchers for getting info regarding dialogues system. version:1
arxiv-1306-4103 | Group Symmetry and non-Gaussian Covariance Estimation | http://arxiv.org/abs/1306.4103 | id:1306.4103 author:Ilya Soloveychik, Ami Wiesel category:stat.ML  published:2013-06-18 summary:We consider robust covariance estimation with group symmetry constraints. Non-Gaussian covariance estimation, e.g., Tyler scatter estimator and Multivariate Generalized Gaussian distribution methods, usually involve non-convex minimization problems. Recently, it was shown that the underlying principle behind their success is an extended form of convexity over the geodesics in the manifold of positive definite matrices. A modern approach to improve estimation accuracy is to exploit prior knowledge via additional constraints, e.g., restricting the attention to specific classes of covariances which adhere to prior symmetry structures. In this paper, we prove that such group symmetry constraints are also geodesically convex and can therefore be incorporated into various non-Gaussian covariance estimators. Practical examples of such sets include: circulant, persymmetric and complex/quaternion proper structures. We provide a simple numerical technique for finding maximum likelihood estimates under such constraints, and demonstrate their performance advantage using synthetic experiments. version:1
arxiv-1306-4079 | A Novel Block-DCT and PCA Based Image Perceptual Hashing Algorithm | http://arxiv.org/abs/1306.4079 | id:1306.4079 author:Zeng Jie category:cs.CV  published:2013-06-18 summary:Image perceptual hashing finds applications in content indexing, large-scale image database management, certification and authentication and digital watermarking. We propose a Block-DCT and PCA based image perceptual hash in this article and explore the algorithm in the application of tamper detection. The main idea of the algorithm is to integrate color histogram and DCT coefficients of image blocks as perceptual feature, then to compress perceptual features as inter-feature with PCA, and to threshold to create a robust hash. The robustness and discrimination properties of the proposed algorithm are evaluated in detail. Our algorithms first construct a secondary image, derived from input image by pseudo-randomly extracting features that approximately capture semi-global geometric characteristics. From the secondary image (which does not perceptually resemble the input), we further extract the final features which can be used as a hash value (and can be further suitably quantized). In this paper, we use spectral matrix invariants as embodied by Singular Value Decomposition. Surprisingly, formation of the secondary image turns out be quite important since it not only introduces further robustness, but also enhances the security properties. Indeed, our experiments reveal that our hashing algorithms extract most of the geometric information from the images and hence are robust to severe perturbations (e.g. up to %50 cropping by area with 20 degree rotations) on images while avoiding misclassification. Experimental results show that the proposed image perceptual hash algorithm can effectively address the tamper detection problem with advantageous robustness and discrimination. version:1
arxiv-1305-3633 | Classification for Big Dataset of Bioacoustic Signals Based on Human Scoring System and Artificial Neural Network | http://arxiv.org/abs/1305.3633 | id:1305.3633 author:Mohammad Pourhomayoun, Peter Dugan, Marian Popescu, Denise Risch, Hal Lewis, Christopher Clark category:cs.CV  published:2013-05-15 summary:In this paper, we propose a method to improve sound classification performance by combining signal features, derived from the time-frequency spectrogram, with human perception. The method presented herein exploits an artificial neural network (ANN) and learns the signal features based on the human perception knowledge. The proposed method is applied to a large acoustic dataset containing 24 months of nearly continuous recordings. The results show a significant improvement in performance of the detection-classification system; yielding as much as 20% improvement in true positive rate for a given false positive rate. version:2
arxiv-1305-3635 | Bioacoustic Signal Classification Based on Continuous Region Processing, Grid Masking and Artificial Neural Network | http://arxiv.org/abs/1305.3635 | id:1305.3635 author:Mohammad Pourhomayoun, Peter Dugan, Marian Popescu, Christopher Clark category:cs.CV  published:2013-05-15 summary:In this paper, we develop a novel method based on machine-learning and image processing to identify North Atlantic right whale (NARW) up-calls in the presence of high levels of ambient and interfering noise. We apply a continuous region algorithm on the spectrogram to extract the regions of interest, and then use grid masking techniques to generate a small feature set that is then used in an artificial neural network classifier to identify the NARW up-calls. It is shown that the proposed technique is effective in detecting and capturing even very faint up-calls, in the presence of ambient and interfering noises. The method is evaluated on a dataset recorded in Massachusetts Bay, United States. The dataset includes 20000 sound clips for training, and 10000 sound clips for testing. The results show that the proposed technique can achieve an error rate of less than FPR = 4.5% for a 90% true positive rate. version:2
arxiv-1306-3920 | Discriminating word senses with tourist walks in complex networks | http://arxiv.org/abs/1306.3920 | id:1306.3920 author:Thiago C. Silva, Diego R. Amancio category:cs.CL cs.SI physics.soc-ph  published:2013-06-17 summary:Patterns of topological arrangement are widely used for both animal and human brains in the learning process. Nevertheless, automatic learning techniques frequently overlook these patterns. In this paper, we apply a learning technique based on the structural organization of the data in the attribute space to the problem of discriminating the senses of 10 polysemous words. Using two types of characterization of meanings, namely semantical and topological approaches, we have observed significative accuracy rates in identifying the suitable meanings in both techniques. Most importantly, we have found that the characterization based on the deterministic tourist walk improves the disambiguation process when one compares with the discrimination achieved with traditional complex networks measurements such as assortativity and clustering coefficient. To our knowledge, this is the first time that such deterministic walk has been applied to such a kind of problem. Therefore, our finding suggests that the tourist walk characterization may be useful in other related applications. version:1
arxiv-1306-3917 | On Finding the Largest Mean Among Many | http://arxiv.org/abs/1306.3917 | id:1306.3917 author:Kevin Jamieson, Matthew Malloy, Robert Nowak, Sebastien Bubeck category:stat.ML cs.LG  published:2013-06-17 summary:Sampling from distributions to find the one with the largest mean arises in a broad range of applications, and it can be mathematically modeled as a multi-armed bandit problem in which each distribution is associated with an arm. This paper studies the sample complexity of identifying the best arm (largest mean) in a multi-armed bandit problem. Motivated by large-scale applications, we are especially interested in identifying situations where the total number of samples that are necessary and sufficient to find the best arm scale linearly with the number of arms. We present a single-parameter multi-armed bandit model that spans the range from linear to superlinear sample complexity. We also give a new algorithm for best arm identification, called PRISM, with linear sample complexity for a wide range of mean distributions. The algorithm, like most exploration procedures for multi-armed bandits, is adaptive in the sense that the next arms to sample are selected based on previous samples. We compare the sample complexity of adaptive procedures with simpler non-adaptive procedures using new lower bounds. For many problem instances, the increased sample complexity required by non-adaptive procedures is a polynomial factor of the number of arms. version:1
arxiv-1306-3905 | Stability of Multi-Task Kernel Regression Algorithms | http://arxiv.org/abs/1306.3905 | id:1306.3905 author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML  published:2013-06-17 summary:We study the stability properties of nonlinear multi-task regression in reproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a. multi-task kernels, are appropriate for learning prob- lems with nonscalar outputs like multi-task learning and structured out- put prediction. We show that multi-task kernel regression algorithms are uniformly stable in the general case of infinite-dimensional output spaces. We then derive under mild assumption on the kernel generaliza- tion bounds of such algorithms, and we show their consistency even with non Hilbert-Schmidt operator-valued kernels . We demonstrate how to apply the results to various multi-task kernel regression methods such as vector-valued SVR and functional ridge regression. version:1
arxiv-1306-3862 | Bayesian methods for low-rank matrix estimation: short survey and theoretical study | http://arxiv.org/abs/1306.3862 | id:1306.3862 author:Pierre Alquier category:stat.ML  published:2013-06-17 summary:The problem of low-rank matrix estimation recently received a lot of attention due to challenging applications. A lot of work has been done on rank-penalized methods and convex relaxation, both on the theoretical and applied sides. However, only a few papers considered Bayesian estimation. In this paper, we review the different type of priors considered on matrices to favour low-rank. We also prove that the obtained Bayesian estimators, under suitable assumptions, enjoys the same optimality properties as the ones based on penalization. version:1
arxiv-1306-3860 | Cluster coloring of the Self-Organizing Map: An information visualization perspective | http://arxiv.org/abs/1306.3860 | id:1306.3860 author:Peter Sarlin, Samuel RÃ¶nnqvist category:cs.LG cs.HC  published:2013-06-17 summary:This paper takes an information visualization perspective to visual representations in the general SOM paradigm. This involves viewing SOM-based visualizations through the eyes of Bertin's and Tufte's theories on data graphics. The regular grid shape of the Self-Organizing Map (SOM), while being a virtue for linking visualizations to it, restricts representation of cluster structures. From the viewpoint of information visualization, this paper provides a general, yet simple, solution to projection-based coloring of the SOM that reveals structures. First, the proposed color space is easy to construct and customize to the purpose of use, while aiming at being perceptually correct and informative through two separable dimensions. Second, the coloring method is not dependent on any specific method of projection, but is rather modular to fit any objective function suitable for the task at hand. The cluster coloring is illustrated on two datasets: the iris data, and welfare and poverty indicators. version:1
arxiv-1306-3828 | Non-Uniform Blind Deblurring with a Spatially-Adaptive Sparse Prior | http://arxiv.org/abs/1306.3828 | id:1306.3828 author:Haichao Zhang, David Wipf category:cs.CV  published:2013-06-17 summary:Typical blur from camera shake often deviates from the standard uniform convolutional script, in part because of problematic rotations which create greater blurring away from some unknown center point. Consequently, successful blind deconvolution requires the estimation of a spatially-varying or non-uniform blur operator. Using ideas from Bayesian inference and convex analysis, this paper derives a non-uniform blind deblurring algorithm with several desirable, yet previously-unexplored attributes. The underlying objective function includes a spatially adaptive penalty which couples the latent sharp image, non-uniform blur operator, and noise level together. This coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted. Remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics. The algorithm can be implemented using a majorization-minimization strategy that is virtually parameter free. Detailed theoretical analysis and empirical validation on real images serve to validate the proposed method. version:1
arxiv-1306-4375 | Discrete perceptrons | http://arxiv.org/abs/1306.4375 | id:1306.4375 author:Mihailo Stojnic category:math.PR math-ph math.MP stat.ML  published:2013-06-17 summary:Perceptrons have been known for a long time as a promising tool within the neural networks theory. The analytical treatment for a special class of perceptrons started in seminal work of Gardner \cite{Gar88}. Techniques initially employed to characterize perceptrons relied on a statistical mechanics approach. Many of such predictions obtained in \cite{Gar88} (and in a follow-up \cite{GarDer88}) were later on established rigorously as mathematical facts (see, e.g. \cite{SchTir02,SchTir03,TalBook,StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13}). These typically related to spherical perceptrons. A lot of work has been done related to various other types of perceptrons. Among the most challenging ones are what we will refer to as the discrete perceptrons. An introductory statistical mechanics treatment of such perceptrons was given in \cite{GutSte90}. Relying on results of \cite{Gar88}, \cite{GutSte90} characterized many of the features of several types of discrete perceptrons. We in this paper, consider a similar subclass of discrete perceptrons and provide a mathematically rigorous set of results related to their performance. As it will turn out, many of the statistical mechanics predictions obtained for discrete predictions will in fact appear as mathematically provable bounds. This will in a way emulate a similar type of behavior we observed in \cite{StojnicGardGen13,StojnicGardSphNeg13,StojnicGardSphErr13} when studying spherical perceptrons. version:1
arxiv-1306-3809 | Spherical perceptron as a storage memory with limited errors | http://arxiv.org/abs/1306.3809 | id:1306.3809 author:Mihailo Stojnic category:math.PR math-ph math.MP stat.ML  published:2013-06-17 summary:It has been known for a long time that the classical spherical perceptrons can be used as storage memories. Seminal work of Gardner, \cite{Gar88}, started an analytical study of perceptrons storage abilities. Many of the Gardner's predictions obtained through statistical mechanics tools have been rigorously justified. Among the most important ones are of course the storage capacities. The first rigorous confirmations were obtained in \cite{SchTir02,SchTir03} for the storage capacity of the so-called positive spherical perceptron. These were later reestablished in \cite{TalBook} and a bit more recently in \cite{StojnicGardGen13}. In this paper we consider a variant of the spherical perceptron that operates as a storage memory but allows for a certain fraction of errors. In Gardner's original work the statistical mechanics predictions in this directions were presented sa well. Here, through a mathematically rigorous analysis, we confirm that the Gardner's predictions in this direction are in fact provable upper bounds on the true values of the storage capacity. Moreover, we then present a mechanism that can be used to lower these bounds. Numerical results that we present indicate that the Garnder's storage capacity predictions may, in a fairly wide range of parameters, be not that far away from the true values. version:1
arxiv-1210-5041 | Navigation domain representation for interactive multiview imaging | http://arxiv.org/abs/1210.5041 | id:1210.5041 author:Thomas Maugey, Ismael Daribo, Gene Cheung, Pascal Frossard category:cs.MM cs.CV  published:2012-10-18 summary:Enabling users to interactively navigate through different viewpoints of a static scene is a new interesting functionality in 3D streaming systems. While it opens exciting perspectives towards rich multimedia applications, it requires the design of novel representations and coding techniques in order to solve the new challenges imposed by interactive navigation. Interactivity clearly brings new design constraints: the encoder is unaware of the exact decoding process, while the decoder has to reconstruct information from incomplete subsets of data since the server can generally not transmit images for all possible viewpoints due to resource constrains. In this paper, we propose a novel multiview data representation that permits to satisfy bandwidth and storage constraints in an interactive multiview streaming system. In particular, we partition the multiview navigation domain into segments, each of which is described by a reference image and some auxiliary information. The auxiliary information enables the client to recreate any viewpoint in the navigation segment via view synthesis. The decoder is then able to navigate freely in the segment without further data request to the server; it requests additional data only when it moves to a different segment. We discuss the benefits of this novel representation in interactive navigation systems and further propose a method to optimize the partitioning of the navigation domain into independent segments, under bandwidth and storage constraints. Experimental results confirm the potential of the proposed representation; namely, our system leads to similar compression performance as classical inter-view coding, while it provides the high level of flexibility that is required for interactive streaming. Hence, our new framework represents a promising solution for 3D data representation in novel interactive multimedia services. version:2
arxiv-1306-3729 | Spectral Experts for Estimating Mixtures of Linear Regressions | http://arxiv.org/abs/1306.3729 | id:1306.3729 author:Arun Tejasvi Chaganty, Percy Liang category:cs.LG stat.ML  published:2013-06-17 summary:Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima. In this paper, we develop a new computationally efficient and provably consistent estimator for a mixture of linear regressions, a simple instance of a discriminative latent-variable model. Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using a tensor power method. We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM). version:1
arxiv-1306-3627 | Bayesian test of significance for conditional independence: The multinomial model | http://arxiv.org/abs/1306.3627 | id:1306.3627 author:Pablo de Morais Andrade, Julio Michael Stern, Carlos Alberto de BraganÃ§a Pereira category:stat.CO stat.ML 47N30 G.3  published:2013-06-16 summary:Conditional independence tests (CI tests) have received special attention lately in Machine Learning and Computational Intelligence related literature as an important indicator of the relationship among the variables used by their models. In the field of Probabilistic Graphical Models (PGM)--which includes Bayesian Networks (BN) models--CI tests are especially important for the task of learning the PGM structure from data. In this paper, we propose the Full Bayesian Significance Test (FBST) for tests of conditional independence for discrete datasets. FBST is a powerful Bayesian test for precise hypothesis, as an alternative to frequentist's significance tests (characterized by the calculation of the \emph{p-value}). version:1
arxiv-1306-3584 | Recurrent Convolutional Neural Networks for Discourse Compositionality | http://arxiv.org/abs/1306.3584 | id:1306.3584 author:Nal Kalchbrenner, Phil Blunsom category:cs.CL  published:2013-06-15 summary:The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment. version:1
arxiv-1306-3574 | Early stopping and non-parametric regression: An optimal data-dependent stopping rule | http://arxiv.org/abs/1306.3574 | id:1306.3574 author:Garvesh Raskutti, Martin J. Wainwright, Bin Yu category:stat.ML  published:2013-06-15 summary:The strategy of early stopping is a regularization technique based on choosing a stopping time for an iterative algorithm. Focusing on non-parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient-descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the $L^2(P)$ and $L^2(P_n)$ norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator. version:1
arxiv-1306-3560 | iCub World: Friendly Robots Help Building Good Vision Data-Sets | http://arxiv.org/abs/1306.3560 | id:1306.3560 author:Sean Ryan Fanello, Carlo Ciliberto, Matteo Santoro, Lorenzo Natale, Giorgio Metta, Lorenzo Rosasco, Francesca Odone category:cs.CV  published:2013-06-15 summary:In this paper we present and start analyzing the iCub World data-set, an object recognition data-set, we acquired using a Human-Robot Interaction (HRI) scheme and the iCub humanoid robot platform. Our set up allows for rapid acquisition and annotation of data with corresponding ground truth. While more constrained in its scopes -- the iCub world is essentially a robotics research lab -- we demonstrate how the proposed data-set poses challenges to current recognition systems. The iCubWorld data-set is publicly available. The data-set can be downloaded from: http://www.iit.it/en/projects/data-sets.html. version:1
arxiv-1306-3558 | Outlying Property Detection with Numerical Attributes | http://arxiv.org/abs/1306.3558 | id:1306.3558 author:Fabrizio Angiulli, Fabio Fassetti, Luigi Palopoli, Giuseppe Manco category:cs.LG cs.DB stat.ML  published:2013-06-15 summary:The outlying property detection problem is the problem of discovering the properties distinguishing a given object, known in advance to be an outlier in a database, from the other database objects. In this paper, we analyze the problem within a context where numerical attributes are taken into account, which represents a relevant case left open in the literature. We introduce a measure to quantify the degree the outlierness of an object, which is associated with the relative likelihood of the value, compared to the to the relative likelihood of other objects in the database. As a major contribution, we present an efficient algorithm to compute the outlierness relative to significant subsets of the data. The latter subsets are characterized in a "rule-based" fashion, and hence the basis for the underlying explanation of the outlierness. version:1
arxiv-1306-0407 | Constructive Setting of the Density Ratio Estimation Problem and its Rigorous Solution | http://arxiv.org/abs/1306.0407 | id:1306.0407 author:Vladimir Vapnik, Igor Braga, Rauf Izmailov category:stat.ML  published:2013-06-03 summary:We introduce a general constructive setting of the density ratio estimation problem as a solution of a (multidimensional) integral equation. In this equation, not only its right hand side is known approximately, but also the integral operator is defined approximately. We show that this ill-posed problem has a rigorous solution and obtain the solution in a closed form. The key element of this solution is the novel V-matrix, which captures the geometry of the observed samples. We compare our method with three well-known previously proposed ones. Our experimental results demonstrate the good potential of the new approach. version:2
arxiv-1306-2091 | A framework for (under)specifying dependency syntax without overloading annotators | http://arxiv.org/abs/1306.2091 | id:1306.2091 author:Nathan Schneider, Brendan O'Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A. Smith, Chris Dyer, Jason Baldridge category:cs.CL  published:2013-06-10 summary:We introduce a framework for lightweight dependency syntax annotation. Our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workflow. Moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. We demonstrate the efficacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations. version:2
arxiv-1306-3476 | Hyperparameter Optimization and Boosting for Classifying Facial Expressions: How good can a "Null" Model be? | http://arxiv.org/abs/1306.3476 | id:1306.3476 author:James Bergstra, David D. Cox category:cs.CV cs.LG stat.ML  published:2013-06-14 summary:One of the goals of the ICML workshop on representation and learning is to establish benchmark scores for a new data set of labeled facial expressions. This paper presents the performance of a "Null" model consisting of convolutions with random weights, PCA, pooling, normalization, and a linear readout. Our approach focused on hyperparameter optimization rather than novel model components. On the Facial Expression Recognition Challenge held by the Kaggle website, our hyperparameter optimization approach achieved a score of 60% accuracy on the test data. This paper also introduces a new ensemble construction variant that combines hyperparameter optimization with the construction of ensembles. This algorithm constructed an ensemble of four models that scored 65.5% accuracy. These scores rank 12th and 5th respectively among the 56 challenge participants. It is worth noting that our approach was developed prior to the release of the data set, and applied without modification; our strong competition performance suggests that the TPE hyperparameter optimization algorithm and domain expertise encoded in our Null model can generalize to new image classification data sets. version:1
arxiv-1306-3474 | Classifying Single-Trial EEG during Motor Imagery with a Small Training Set | http://arxiv.org/abs/1306.3474 | id:1306.3474 author:Yijun Wang category:cs.LG cs.HC stat.ML  published:2013-06-14 summary:Before the operation of a motor imagery based brain-computer interface (BCI) adopting machine learning techniques, a cumbersome training procedure is unavoidable. The development of a practical BCI posed the challenge of classifying single-trial EEG with a small training set. In this letter, we addressed this problem by employing a series of signal processing and machine learning approaches to alleviate overfitting and obtained test accuracy similar to training accuracy on the datasets from BCI Competition III and our own experiments. version:1
arxiv-1305-4561 | Random crossings in dependency trees | http://arxiv.org/abs/1305.4561 | id:1305.4561 author:Ramon Ferrer-i-Cancho category:cs.CL cs.DM cs.SI physics.soc-ph  published:2013-05-20 summary:It has been hypothesized that the rather small number of crossings in real syntactic dependency trees is a side-effect of pressure for dependency length minimization. Here we answer a related important research question: what would be the expected number of crossings if the natural order of a sentence was lost? We show that this number depends only on the number of vertices of the dependency tree (the sentence length) and the second moment of vertex degrees. The expected number of crossings is minimum for a star tree (crossings are impossible) and maximum for a linear tree (the number of crossings is of the order of the square of the sequence length). version:2
arxiv-1306-3415 | Live-wire 3D medical images segmentation | http://arxiv.org/abs/1306.3415 | id:1306.3415 author:Ognjen Arandjelovic category:cs.CV  published:2013-06-14 summary:This report describes the design, implementation, evaluation and original enhancements to the Live-Wire method for 2D and 3D image segmentation. Live-Wire 2D employs a semi-automatic paradigm; the user is asked to select a few boundary points of the object to segment, to steer the process in the right direction, while the result is displayed in real time. In our implementation segmentation is extended to three dimensions by performing this process on a slice-by-slice basis. User's time and involvement is further reduced by allowing him to specify object contours in planes orthogonal to the slices. If these planes are chosen strategically, Live-Wire 3D can perform 2D segmentation in the plane of each slice automatically. This report also proposes two improvements to the original method, path heating and a new graph edge feature function based on variance of path properties along the boundary. We show that these improvements lead up to a 33% reduction in interaction with the user, and improved delineation in presence of strong interfering edges. version:1
arxiv-1306-3409 | Constrained fractional set programs and their application in local clustering and community detection | http://arxiv.org/abs/1306.3409 | id:1306.3409 author:Thomas BÃ¼hler, Syama Sundar Rangapuram, Simon Setzer, Matthias Hein category:stat.ML cs.LG math.OC  published:2013-06-14 summary:The (constrained) minimization of a ratio of set functions is a problem frequently occurring in clustering and community detection. As these optimization problems are typically NP-hard, one uses convex or spectral relaxations in practice. While these relaxations can be solved globally optimally, they are often too loose and thus lead to results far away from the optimum. In this paper we show that every constrained minimization problem of a ratio of non-negative set functions allows a tight relaxation into an unconstrained continuous optimization problem. This result leads to a flexible framework for solving constrained problems in network analysis. While a globally optimal solution for the resulting non-convex problem cannot be guaranteed, we outperform the loose convex or spectral relaxations by a large margin on constrained local clustering problems. version:1
arxiv-1306-3331 | Sparse Recovery of Streaming Signals Using L1-Homotopy | http://arxiv.org/abs/1306.3331 | id:1306.3331 author:M. Salman Asif, Justin Romberg category:cs.IT math.IT math.OC stat.ML  published:2013-06-14 summary:Most of the existing methods for sparse signal recovery assume a static system: the unknown signal is a finite-length vector for which a fixed set of linear measurements and a sparse representation basis are available and an L1-norm minimization program is solved for the reconstruction. However, the same representation and reconstruction framework is not readily applicable in a streaming system: the unknown signal changes over time, and it is measured and reconstructed sequentially over small time intervals. In this paper, we discuss two such streaming systems and a homotopy-based algorithm for quickly solving the associated L1-norm minimization programs: 1) Recovery of a smooth, time-varying signal for which, instead of using block transforms, we use lapped orthogonal transforms for sparse representation. 2) Recovery of a sparse, time-varying signal that follows a linear dynamic model. For both the systems, we iteratively process measurements over a sliding interval and estimate sparse coefficients by solving a weighted L1-norm minimization program. Instead of solving a new L1 program from scratch at every iteration, we use an available signal estimate as a starting point in a homotopy formulation. Starting with a warm-start vector, our homotopy algorithm updates the solution in a small number of computationally inexpensive steps as the system changes. The homotopy algorithm presented in this paper is highly versatile as it can update the solution for the L1 problem in a number of dynamical settings. We demonstrate with numerical experiments that our proposed streaming recovery framework outperforms the methods that represent and reconstruct a signal as independent, disjoint blocks, in terms of quality of reconstruction, and that our proposed homotopy-based updating scheme outperforms current state-of-the-art solvers in terms of the computation time and complexity. version:1
arxiv-1306-3297 | Matching objects across the textured-smooth continuum | http://arxiv.org/abs/1306.3297 | id:1306.3297 author:Ognjen Arandjelovic category:cs.CV  published:2013-06-14 summary:The problem of 3D object recognition is of immense practical importance, with the last decade witnessing a number of breakthroughs in the state of the art. Most of the previous work has focused on the matching of textured objects using local appearance descriptors extracted around salient image points. The recently proposed bag of boundaries method was the first to address directly the problem of matching smooth objects using boundary features. However, no previous work has attempted to achieve a holistic treatment of the problem by jointly using textural and shape features which is what we describe herein. Due to the complementarity of the two modalities, we fuse the corresponding matching scores and learn their relative weighting in a data specific manner by optimizing discriminative performance on synthetically distorted data. For the textural description of an object we adopt a representation in the form of a histogram of SIFT based visual words. Similarly the apparent shape of an object is represented by a histogram of discretized features capturing local shape. On a large public database of a diverse set of objects, the proposed method is shown to outperform significantly both purely textural and purely shape based approaches for matching across viewpoint variation. version:1
arxiv-1306-3294 | Feature Learning by Multidimensional Scaling and its Applications in Object Recognition | http://arxiv.org/abs/1306.3294 | id:1306.3294 author:Quan Wang, Kim L. Boyer category:cs.CV  published:2013-06-14 summary:We present the MDS feature learning framework, in which multidimensional scaling (MDS) is applied on high-level pairwise image distances to learn fixed-length vector representations of images. The aspects of the images that are captured by the learned features, which we call MDS features, completely depend on what kind of image distance measurement is employed. With properly selected semantics-sensitive image distances, the MDS features provide rich semantic information about the images that is not captured by other feature extraction techniques. In our work, we introduce the iterated Levenberg-Marquardt algorithm for solving MDS, and study the MDS feature learning with IMage Euclidean Distance (IMED) and Spatial Pyramid Matching (SPM) distance. We present experiments on both synthetic data and real images --- the publicly accessible UIUC car image dataset. The MDS features based on SPM distance achieve exceptional performance for the car recognition task. version:1
arxiv-1306-3212 | Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation | http://arxiv.org/abs/1306.3212 | id:1306.3212 author:Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon, Pradeep Ravikumar category:cs.LG stat.ML  published:2013-06-13 summary:The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods. version:1
arxiv-1306-3084 | Segmentation et InterprÃ©tation de Nuages de Points pour la ModÃ©lisation d'Environnements Urbains | http://arxiv.org/abs/1306.3084 | id:1306.3084 author:Jorge Hernandez, Beatriz Marcotegui category:cs.CV  published:2013-06-13 summary:Dans cet article, nous pr\'esentons une m\'ethode pour la d\'etection et la classification d'artefacts au niveau du sol, comme phase de filtrage pr\'ealable \`a la mod\'elisation d'environnements urbains. La m\'ethode de d\'etection est r\'ealis\'ee sur l'image profondeur, une projection de nuage de points sur un plan image o\`u la valeur du pixel correspond \`a la distance du point au plan. En faisant l'hypoth\`ese que les artefacts sont situ\'es au sol, ils sont d\'etect\'es par une transformation de chapeau haut de forme par remplissage de trous sur l'image de profondeur. Les composantes connexes ainsi obtenues, sont ensuite caract\'eris\'ees et une analyse des variables est utilis\'ee pour la s\'election des caract\'eristiques les plus discriminantes. Les composantes connexes sont donc classifi\'ees en quatre cat\'egories (lampadaires, pi\'etons, voitures et "Reste") \`a l'aide d'un algorithme d'apprentissage supervis\'e. La m\'ethode a \'et\'e test\'ee sur des nuages de points de la ville de Paris, en montrant de bons r\'esultats de d\'etection et de classification dans l'ensemble de donn\'ees.---In this article, we present a method for detection and classification of artifacts at the street level, in order to filter cloud point, facilitating the urban modeling process. Our approach exploits 3D information by using range image, a projection of 3D points onto an image plane where the pixel intensity is a function of the measured distance between 3D points and the plane. By assuming that the artifacts are on the ground, they are detected using a Top-Hat of the hole filling algorithm of range images. Then, several features are extracted from the detected connected components and a stepwise forward variable/model selection by using the Wilk's Lambda criterion is performed. Afterward, CCs are classified in four categories (lampposts, pedestrians, cars and others) by using a supervised machine learning method. The proposed method was tested on cloud points of Paris, and have shown satisfactory results on the whole dataset. version:1
arxiv-1306-3058 | Physeter catodon localization by sparse coding | http://arxiv.org/abs/1306.3058 | id:1306.3058 author:SÃ©bastien Paris, Yann Doh, HervÃ© Glotin, Xanadu Halkias, Joseph Razik category:cs.LG cs.CE stat.ML  published:2013-06-13 summary:This paper presents a spermwhale' localization architecture using jointly a bag-of-features (BoF) approach and machine learning framework. BoF methods are known, especially in computer vision, to produce from a collection of local features a global representation invariant to principal signal transformations. Our idea is to regress supervisely from these local features two rough estimates of the distance and azimuth thanks to some datasets where both acoustic events and ground-truth position are now available. Furthermore, these estimates can feed a particle filter system in order to obtain a precise spermwhale' position even in mono-hydrophone configuration. Anti-collision system and whale watching are considered applications of this work. version:1
arxiv-1306-3036 | The Ripple Pond: Enabling Spiking Networks to See | http://arxiv.org/abs/1306.3036 | id:1306.3036 author:Saeed Afshar, Gregory Cohen, Runchun Wang, Andre van Schaik, Jonathan Tapson, Torsten Lehmann, Tara Julia Hamilton category:cs.NE q-bio.NC  published:2013-06-13 summary:In this paper we present the biologically inspired Ripple Pond Network (RPN), a simply connected spiking neural network that, operating together with recently proposed PolyChronous Networks (PCN), enables rapid, unsupervised, scale and rotation invariant object recognition using efficient spatio-temporal spike coding. The RPN has been developed as a hardware solution linking previously implemented neuromorphic vision and memory structures capable of delivering end-to-end high-speed, low-power and low-resolution recognition for mobile and autonomous applications where slow, highly sophisticated and power hungry signal processing solutions are ineffective. Key aspects in the proposed approach include utilising the spatial properties of physically embedded neural networks and propagating waves of activity therein for information processing, using dimensional collapse of imagery information into amenable temporal patterns and the use of asynchronous frames for information binding. version:1
arxiv-1306-3032 | A Face-like Structure Detection on Planet and Satellite Surfaces using Image Processing | http://arxiv.org/abs/1306.3032 | id:1306.3032 author:Kazutaka Kurihara, Masakazu Takasu, Kazuhiro Sasao, Hal Seki, Takayuki Narabu, Mitsuo Yamamoto, Satoshi Iida, Hiroyuki Yamamoto category:cs.CV  published:2013-06-13 summary:This paper demonstrates that face-like structures are everywhere, and can be de-tected automatically even with computers. Huge amount of satellite images of the Earth, the Moon, the Mars are explored and many interesting face-like structure are detected. Throughout this fact, we believe that science and technologies can alert people not to easily become an occultist. version:1
arxiv-1306-3018 | Second Order Swarm Intelligence | http://arxiv.org/abs/1306.3018 | id:1306.3018 author:Vitorino Ramos, David M. S. Rodrigues, Jorge LouÃ§Ã£ category:cs.NE 68T05  published:2013-06-13 summary:An artificial Ant Colony System (ACS) algorithm to solve general-purpose combinatorial Optimization Problems (COP) that extends previous AC models [21] by the inclusion of a negative pheromone, is here described. Several Travelling Salesman Problem (TSP) were used as benchmark. We show that by using two different sets of pheromones, a second-order co-evolved compromise between positive and negative feedbacks achieves better results than single positive feedback systems. The algorithm was tested against known NP-complete combinatorial Optimization Problems, running on symmetrical TSP's. We show that the new algorithm compares favourably against these benchmarks, accordingly to recent biological findings by Robinson [26,27], and Gruter [28] where "No entry" signals and negative feedback allows a colony to quickly reallocate the majority of its foragers to superior food patches. This is the first time an extended ACS algorithm is implemented with these successful characteristics. version:1
arxiv-1306-3003 | Non-parametric Power-law Data Clustering | http://arxiv.org/abs/1306.3003 | id:1306.3003 author:Xuhui Fan, Yiling Zeng, Longbing Cao category:cs.LG cs.CV stat.ML  published:2013-06-13 summary:It has always been a great challenge for clustering algorithms to automatically determine the cluster numbers according to the distribution of datasets. Several approaches have been proposed to address this issue, including the recent promising work which incorporate Bayesian Nonparametrics into the $k$-means clustering procedure. This approach shows simplicity in implementation and solidity in theory, while it also provides a feasible way to inference in large scale datasets. However, several problems remains unsolved in this pioneering work, including the power-law data applicability, mechanism to merge centers to avoid the over-fitting problem, clustering order problem, e.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely \emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor Process, \emph{pyp-means} treats clusters differently by dynamically and adaptively changing the threshold to guarantee the generation of power-law clustering results. Also, one center agglomeration procedure is integrated into the implementation to be able to merge small but close clusters and then adaptively determine the cluster number. With more discussion on the clustering order, the convergence proof, complexity analysis and extension to spectral clustering, our approach is compared with traditional clustering algorithm and variational inference methods. The advantages and properties of pyp-means are validated by experiments on both synthetic datasets and real world datasets. version:1
arxiv-1303-3163 | A Greedy Approximation of Bayesian Reinforcement Learning with Probably Optimistic Transition Model | http://arxiv.org/abs/1303.3163 | id:1303.3163 author:Kenji Kawaguchi, Mauricio Araya category:cs.AI cs.LG stat.ML  published:2013-03-13 summary:Bayesian Reinforcement Learning (RL) is capable of not only incorporating domain knowledge, but also solving the exploration-exploitation dilemma in a natural way. As Bayesian RL is intractable except for special cases, previous work has proposed several approximation methods. However, these methods are usually too sensitive to parameter values, and finding an acceptable parameter setting is practically impossible in many applications. In this paper, we propose a new algorithm that greedily approximates Bayesian RL to achieve robustness in parameter space. We show that for a desired learning behavior, our proposed algorithm has a polynomial sample complexity that is lower than those of existing algorithms. We also demonstrate that the proposed algorithm naturally outperforms other existing algorithms when the prior distributions are not significantly misleading. On the other hand, the proposed algorithm cannot handle greatly misspecified priors as well as the other algorithms can. This is a natural consequence of the fact that the proposed algorithm is greedier than the other algorithms. Accordingly, we discuss a way to select an appropriate algorithm for different tasks based on the algorithms' greediness. We also introduce a new way of simplifying Bayesian planning, based on which future work would be able to derive new algorithms. version:3
arxiv-1306-3002 | A Convergence Theorem for the Graph Shift-type Algorithms | http://arxiv.org/abs/1306.3002 | id:1306.3002 author:Xuhui Fan, Longbing Cao category:stat.ML cs.LG  published:2013-06-13 summary:Graph Shift (GS) algorithms are recently focused as a promising approach for discovering dense subgraphs in noisy data. However, there are no theoretical foundations for proving the convergence of the GS Algorithm. In this paper, we propose a generic theoretical framework consisting of three key GS components: simplex of generated sequence set, monotonic and continuous objective function and closed mapping. We prove that GS algorithms with such components can be transformed to fit the Zangwill's convergence theorem, and the sequence set generated by the GS procedures always terminates at a local maximum, or at worst, contains a subsequence which converges to a local maximum of the similarity measure function. The framework is verified by expanding it to other GS-type algorithms and experimental results. version:1
arxiv-1306-2999 | Dynamic Infinite Mixed-Membership Stochastic Blockmodel | http://arxiv.org/abs/1306.2999 | id:1306.2999 author:Xuhui Fan, Longbing Cao, Richard Yi Da Xu category:cs.SI cs.LG stat.ML  published:2013-06-13 summary:Directional and pairwise measurements are often used to model inter-relationships in a social network setting. The Mixed-Membership Stochastic Blockmodel (MMSB) was a seminal work in this area, and many of its capabilities were extended since then. In this paper, we propose the \emph{Dynamic Infinite Mixed-Membership stochastic blockModel (DIM3)}, a generalised framework that extends the existing work to a potentially infinite number of communities and mixture memberships for each of the network's nodes. This model is in a dynamic setting, where additional model parameters are introduced to reflect the degree of persistence between one's memberships at consecutive times. Accordingly, two effective posterior sampling strategies and their results are presented using both synthetic and real data. version:1
arxiv-1206-4074 | A Linear Approximation to the chi^2 Kernel with Geometric Convergence | http://arxiv.org/abs/1206.4074 | id:1206.4074 author:Fuxin Li, Guy Lebanon, Cristian Sminchisescu category:cs.LG cs.CV stat.ML  published:2012-06-18 summary:We propose a new analytical approximation to the $\chi^2$ kernel that converges geometrically. The analytical approximation is derived with elementary methods and adapts to the input distribution for optimal convergence rate. Experiments show the new approximation leads to improved performance in image classification and semantic segmentation tasks using a random Fourier feature approximation of the $\exp-\chi^2$ kernel. Besides, out-of-core principal component analysis (PCA) methods are introduced to reduce the dimensionality of the approximation and achieve better performance at the expense of only an additional constant factor to the time complexity. Moreover, when PCA is performed jointly on the training and unlabeled testing data, further performance improvements can be obtained. Experiments conducted on the PASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets show statistically significant improvements over alternative approximation methods. version:3
arxiv-1306-2918 | Reinforcement learning with restrictions on the action set | http://arxiv.org/abs/1306.2918 | id:1306.2918 author:Mario Bravo, Mathieu Faure category:cs.GT cs.LG math.PR  published:2013-06-12 summary:Consider a 2-player normal-form game repeated over time. We introduce an adaptive learning procedure, where the players only observe their own realized payoff at each stage. We assume that agents do not know their own payoff function, and have no information on the other player. Furthermore, we assume that they have restrictions on their own action set such that, at each stage, their choice is limited to a subset of their action set. We prove that the empirical distributions of play converge to the set of Nash equilibria for zero-sum and potential games, and games where one player has two actions. version:1
arxiv-1306-2906 | Robust Support Vector Machines for Speaker Verification Task | http://arxiv.org/abs/1306.2906 | id:1306.2906 author:Kawthar Yasmine Zergat, Abderrahmane Amrouche category:cs.LG cs.SD stat.ML  published:2013-06-12 summary:An important step in speaker verification is extracting features that best characterize the speaker voice. This paper investigates a front-end processing that aims at improving the performance of speaker verification based on the SVMs classifier, in text independent mode. This approach combines features based on conventional Mel-cepstral Coefficients (MFCCs) and Line Spectral Frequencies (LSFs) to constitute robust multivariate feature vectors. To reduce the high dimensionality required for training these feature vectors, we use a dimension reduction method called principal component analysis (PCA). In order to evaluate the robustness of these systems, different noisy environments have been used. The obtained results using TIMIT database showed that, using the paradigm that combines these spectral cues leads to a significant improvement in verification accuracy, especially with PCA reduction for low signal-to-noise ratio noisy environment. version:1
arxiv-1306-2863 | Random Drift Particle Swarm Optimization | http://arxiv.org/abs/1306.2863 | id:1306.2863 author:Jun Sun, Xiaojun Wu, Vasile Palade, Wei Fang, Yuhui Shi category:cs.AI cs.NE math.OC 68T20  published:2013-06-12 summary:The random drift particle swarm optimization (RDPSO) algorithm, inspired by the free electron model in metal conductors placed in an external electric field, is presented, systematically analyzed and empirically studied in this paper. The free electron model considers that electrons have both a thermal and a drift motion in a conductor that is placed in an external electric field. The motivation of the RDPSO algorithm is described first, and the velocity equation of the particle is designed by simulating the thermal motion as well as the drift motion of the electrons, both of which lead the electrons to a location with minimum potential energy in the external electric field. Then, a comprehensive analysis of the algorithm is made, in order to provide a deep insight into how the RDPSO algorithm works. It involves a theoretical analysis and the simulation of the stochastic dynamical behavior of a single particle in the RDPSO algorithm. The search behavior of the algorithm itself is also investigated in detail, by analyzing the interaction between the particles. Some variants of the RDPSO algorithm are proposed by incorporating different random velocity components with different neighborhood topologies. Finally, empirical studies on the RDPSO algorithm are performed by using a set of benchmark functions from the CEC2005 benchmark suite. Based on the theoretical analysis of the particle's behavior, two methods of controlling the algorithmic parameters are employed, followed by an experimental analysis on how to select the parameter values, in order to obtain a good overall performance of the RDPSO algorithm and its variants in real-world applications. A further performance comparison between the RDPSO algorithms and other variants of PSO is made to prove the efficiency of the RDPSO algorithms. version:1
arxiv-1306-2838 | The Quantum Challenge in Concept Theory and Natural Language Processing | http://arxiv.org/abs/1306.2838 | id:1306.2838 author:Diederik Aerts, Jan Broekaert, Sandro Sozzo, Tomas Veloz category:cs.CL cs.IR quant-ph  published:2013-06-12 summary:The mathematical formalism of quantum theory has been successfully used in human cognition to model decision processes and to deliver representations of human knowledge. As such, quantum cognition inspired tools have improved technologies for Natural Language Processing and Information Retrieval. In this paper, we overview the quantum cognition approach developed in our Brussels team during the last two decades, specifically our identification of quantum structures in human concepts and language, and the modeling of data from psychological and corpus-text-based experiments. We discuss our quantum-theoretic framework for concepts and their conjunctions/disjunctions in a Fock-Hilbert space structure, adequately modeling a large amount of data collected on concept combinations. Inspired by this modeling, we put forward elements for a quantum contextual and meaning-based approach to information technologies in which 'entities of meaning' are inversely reconstructed from texts, which are considered as traces of these entities' states. version:1
arxiv-1306-2795 | Recurrent Convolutional Neural Networks for Scene Parsing | http://arxiv.org/abs/1306.2795 | id:1306.2795 author:Pedro H. O. Pinheiro, Ronan Collobert category:cs.CV  published:2013-06-12 summary:Scene parsing is a technique that consist on giving a label to all pixels in an image according to the class they belong to. To ensure a good visual coherence and a high class accuracy, it is essential for a scene parser to capture image long range dependencies. In a feed-forward architecture, this can be simply achieved by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach consisting of a recurrent convolutional neural network which allows us to consider a large input context, while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation methods, nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time. version:1
arxiv-1304-1018 | Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks | http://arxiv.org/abs/1304.1018 | id:1304.1018 author:Dimitri Palaz, Ronan Collobert, Mathew Magimai. -Doss category:cs.LG cs.CL cs.NE  published:2013-04-03 summary:In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine learning techniques, more specifically in the field of image processing and text processing, have shown that such divide and conquer strategy (i.e., separating feature extraction and modeling steps) may not be necessary. Motivated from these studies, in the framework of convolutional neural networks (CNNs), this paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probability estimates. On TIMIT phoneme recognition task, we study different ANN architectures to show the benefit of CNNs and compare the proposed approach against conventional approach where, spectral-based feature MFCC is extracted and modeled by a multilayer perceptron. Our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. It indicates that CNNs can learn features relevant for phoneme classification automatically from the raw speech signal. version:2
arxiv-1306-2759 | Horizontal and Vertical Ensemble with Deep Representation for Classification | http://arxiv.org/abs/1306.2759 | id:1306.2759 author:Jingjing Xie, Bing Xu, Zhang Chuang category:cs.LG stat.ML  published:2013-06-12 summary:Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard. version:1
arxiv-1306-2727 | Sparse Representation-based Image Quality Assessment | http://arxiv.org/abs/1306.2727 | id:1306.2727 author:Tanaya Guha, Ehsan Nezhadarya, Rabab K Ward category:cs.CV cs.MM  published:2013-06-12 summary:A successful approach to image quality assessment involves comparing the structural information between a distorted and its reference image. However, extracting structural information that is perceptually important to our visual system is a challenging task. This paper addresses this issue by employing a sparse representation-based approach and proposes a new metric called the \emph{sparse representation-based quality} (SPARQ) \emph{index}. The proposed method learns the inherent structures of the reference image as a set of basis vectors, such that any structure in the image can be represented by a linear combination of only a few of those basis vectors. This sparse strategy is employed because it is known to generate basis vectors that are qualitatively similar to the receptive field of the simple cells present in the mammalian primary visual cortex. The visual quality of the distorted image is estimated by comparing the structures of the reference and the distorted images in terms of the learnt basis vectors resembling cortical cells. Our approach is evaluated on six publicly available subject-rated image quality assessment datasets. The proposed SPARQ index consistently exhibits high correlation with the subjective ratings on all datasets and performs better or at par with the state-of-the-art. version:1
arxiv-1211-3907 | Distance Majorization and Its Applications | http://arxiv.org/abs/1211.3907 | id:1211.3907 author:Eric C. Chi, Hua Zhou, Kenneth Lange category:math.OC stat.CO stat.ML  published:2012-11-16 summary:The problem of minimizing a continuously differentiable convex function over an intersection of closed convex sets is ubiquitous in applied mathematics. It is particularly interesting when it is easy to project onto each separate set, but nontrivial to project onto their intersection. Algorithms based on Newton's method such as the interior point method are viable for small to medium-scale problems. However, modern applications in statistics, engineering, and machine learning are posing problems with potentially tens of thousands of parameters or more. We revisit this convex programming problem and propose an algorithm that scales well with dimensionality. Our proposal is an instance of a sequential unconstrained minimization technique and revolves around three ideas: the majorization-minimization (MM) principle, the classical penalty method for constrained optimization, and quasi-Newton acceleration of fixed-point algorithms. The performance of our distance majorization algorithms is illustrated in several applications. version:5
arxiv-1306-2663 | Large Margin Low Rank Tensor Analysis | http://arxiv.org/abs/1306.2663 | id:1306.2663 author:Guoqiang Zhong, Mohamed Cheriet category:cs.LG cs.NA 57-04  published:2013-06-11 summary:Other than vector representations, the direct objects of human cognition are generally high-order tensors, such as 2D images and 3D textures. From this fact, two interesting questions naturally arise: How does the human brain represent these tensor perceptions in a "manifold" way, and how can they be recognized on the "manifold"? In this paper, we present a supervised model to learn the intrinsic structure of the tensors embedded in a high dimensional Euclidean space. With the fixed point continuation procedures, our model automatically and jointly discovers the optimal dimensionality and the representations of the low dimensional embeddings. This makes it an effective simulation of the cognitive process of human brain. Furthermore, the generalization of our model based on similarity between the learned low dimensional embeddings can be viewed as counterpart of recognition of human brain. Experiments on applications for object recognition and face recognition demonstrate the superiority of our proposed model over state-of-the-art approaches. version:1
arxiv-1306-2624 | Stopping Criterion for the Mean Shift Iterative Algorithm | http://arxiv.org/abs/1306.2624 | id:1306.2624 author:Yasel GarcÃ©s SuÃ¡rez, Esley Torres, Osvaldo Pereira, Claudia PÃ©rez, Roberto RogrÃ­guez category:cs.CV math.RA  published:2013-06-11 summary:Image segmentation is a critical step in computer vision tasks constituting an essential issue for pattern recognition and visual interpretation. In this paper, we propose a new stopping criterion for the mean shift iterative algorithm by using images defined in Zn ring, with the goal of reaching a better segmentation. We carried out also a study on the weak and strong of equivalence classes between two images. An analysis on the convergence with this new stopping criterion is carried out too. version:1
arxiv-1306-2599 | Hand Gesture Recognition Based on Karhunen-Loeve Transform | http://arxiv.org/abs/1306.2599 | id:1306.2599 author:Joyeeta Singha, Karen Das category:cs.CV  published:2013-06-11 summary:In this paper, we have proposed a system based on K-L Transform to recognize different hand gestures. The system consists of five steps: skin filtering, palm cropping, edge detection, feature extraction, and classification. Firstly the hand is detected using skin filtering and palm cropping was performed to extract out only the palm portion of the hand. The extracted image was then processed using the Canny Edge Detection technique to extract the outline images of palm. After palm extraction, the features of hand were extracted using K-L Transform technique and finally the input gesture was recognized using proper classifier. In our system, we have tested for 10 different hand gestures, and recognizing rate obtained was 96%. Hence we propose an easy approach to recognize different hand gestures. version:1
arxiv-1306-2593 | A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure (A framework for probabilistic modeling of spoken languages and their phonology) | http://arxiv.org/abs/1306.2593 | id:1306.2593 author:Elaine Tsiang category:cs.SD cs.CL I.2.7  published:2013-06-11 summary:We formulate a phonetic-prosodic space based on attributes as perceptual observables, rather than articulatory specifications. We propose an alphabet as markers in the phonetic subspace, aiming for a resolution sufficient to support recognition of all spoken languages. The prosodic subspace is made up of directly measurable physical variables. With the proposed alphabet, traditional diphthongs naturally generalize to a broader class of language-neutral phonotactic constraints, indicating a correlation structure similar to that of the traditional sonority-based syllable. We define a stochastic structure on the phone strings based on this diphthongal constraint, and show how a specific spoken language can be defined as a specific set of probability distributions of this stochastic structure. Furthermore, phonological variations within a spoken language can be modeled as varying probability distributions restricted to the phonetic subspace, conditioned on different values in the prosodic subspace. version:1
arxiv-1206-4812 | A biological gradient descent for prediction through a combination of STDP and homeostatic plasticity | http://arxiv.org/abs/1206.4812 | id:1206.4812 author:Mathieu Galtier, Gilles Wainrib category:q-bio.NC cs.NE math.DS  published:2012-06-21 summary:Identifying, formalizing and combining biological mechanisms which implement known brain functions, such as prediction, is a main aspect of current research in theoretical neuroscience. In this letter, the mechanisms of Spike Timing Dependent Plasticity (STDP) and homeostatic plasticity, combined in an original mathematical formalism, are shown to shape recurrent neural networks into predictors. Following a rigorous mathematical treatment, we prove that they implement the online gradient descent of a distance between the network activity and its stimuli. The convergence to an equilibrium, where the network can spontaneously reproduce or predict its stimuli, does not suffer from bifurcation issues usually encountered in learning in recurrent neural networks. version:2
arxiv-1306-2554 | The association problem in wireless networks: a Policy Gradient Reinforcement Learning approach | http://arxiv.org/abs/1306.2554 | id:1306.2554 author:Richard Combes, Ilham El Bouloumi, Stephane Senecal, Zwi Altman category:cs.NI cs.IT cs.LG math.IT  published:2013-06-11 summary:The purpose of this paper is to develop a self-optimized association algorithm based on PGRL (Policy Gradient Reinforcement Learning), which is both scalable, stable and robust. The term robust means that performance degradation in the learning phase should be forbidden or limited to predefined thresholds. The algorithm is model-free (as opposed to Value Iteration) and robust (as opposed to Q-Learning). The association problem is modeled as a Markov Decision Process (MDP). The policy space is parameterized. The parameterized family of policies is then used as expert knowledge for the PGRL. The PGRL converges towards a local optimum and the average cost decreases monotonically during the learning process. The properties of the solution make it a good candidate for practical implementation. Furthermore, the robustness property allows to use the PGRL algorithm in an "always-on" learning mode. version:1
arxiv-1303-0775 | Hybrid Maximum Likelihood Modulation Classification Using Multiple Radios | http://arxiv.org/abs/1303.0775 | id:1303.0775 author:Onur Ozdemir, Ruoyu Li, Pramod K. Varshney category:cs.IT math.IT stat.ML  published:2013-03-04 summary:The performance of a modulation classifier is highly sensitive to channel signal-to-noise ratio (SNR). In this paper, we focus on amplitude-phase modulations and propose a modulation classification framework based on centralized data fusion using multiple radios and the hybrid maximum likelihood (ML) approach. In order to alleviate the computational complexity associated with ML estimation, we adopt the Expectation Maximization (EM) algorithm. Due to SNR diversity, the proposed multi-radio framework provides robustness to channel SNR. Numerical results show the superiority of the proposed approach with respect to single radio approaches as well as to modulation classifiers using moments based estimators. version:2
arxiv-1306-2295 | Markov random fields factorization with context-specific independences | http://arxiv.org/abs/1306.2295 | id:1306.2295 author:Alejandro Edera, Facundo Bromberg, Federico SchlÃ¼ter category:cs.AI cs.LG  published:2013-06-10 summary:Markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences. version:1
arxiv-1306-2290 | Asymptotically Optimal Sequential Estimation of the Mean Based on Inclusion Principle | http://arxiv.org/abs/1306.2290 | id:1306.2290 author:Xinjia Chen category:math.ST cs.LG math.PR stat.TH  published:2013-06-10 summary:A large class of problems in sciences and engineering can be formulated as the general problem of constructing random intervals with pre-specified coverage probabilities for the mean. Wee propose a general approach for statistical inference of mean values based on accumulated observational data. We show that the construction of such random intervals can be accomplished by comparing the endpoints of random intervals with confidence sequences for the mean. Asymptotic results are obtained for such sequential methods. version:1
arxiv-1306-2281 | A Kernel Test for Three-Variable Interactions | http://arxiv.org/abs/1306.2281 | id:1306.2281 author:Dino Sejdinovic, Arthur Gretton, Wicher Bergsma category:stat.ME stat.ML  published:2013-06-10 summary:We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable, but their combined effect has a strong influence. This makes the Lancaster test especially suited to finding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures. version:1
arxiv-1306-2257 | Using the quaternion's representation of individuals in swarm intelligence and evolutionary computation | http://arxiv.org/abs/1306.2257 | id:1306.2257 author:Iztok Fister, Iztok Fister Jr category:cs.NE  published:2013-06-10 summary:This paper introduces a novel idea for representation of individuals using quaternions in swarm intelligence and evolutionary algorithms. Quaternions are a number system, which extends complex numbers. They are successfully applied to problems of theoretical physics and to those areas needing fast rotation calculations. We propose the application of quaternions in optimization, more precisely, we have been using quaternions for representation of individuals in Bat algorithm. The preliminary results of our experiments when optimizing a test-suite consisting of ten standard functions showed that this new algorithm significantly improved the results of the original Bat algorithm. Moreover, the obtained results are comparable with other swarm intelligence and evolutionary algorithms, like the artificial bees colony, and differential evolution. We believe that this representation could also be successfully applied to other swarm intelligence and evolutionary algorithms. version:1
arxiv-1307-2457 | Detection of Outer Rotations on 3D-Vector Fields with Iterative Geometric Correlation and its Efficiency | http://arxiv.org/abs/1307.2457 | id:1307.2457 author:Roxana Bujack, Gerik Scheuermann, Eckhard Hitzer category:cs.CV cs.GR  published:2013-06-10 summary:Correlation is a common technique for the detection of shifts. Its generalization to the multidimensional geometric correlation in Clifford algebras has been proven a useful tool for color image processing, because it additionally contains information about a rotational misalignment. But so far the exact correction of a three-dimensional outer rotation could only be achieved in certain special cases. In this paper we prove that applying the geometric correlation iteratively has the potential to detect the outer rotational misalignment for arbitrary three-dimensional vector fields. We further present the explicit iterative algorithm, analyze its efficiency detecting the rotational misalignment in the color space of a color image. The experiments suggest a method for the acceleration of the algorithm, which is practically tested with great success. version:1
arxiv-1111-1802 | Combinatorial clustering and the beta negative binomial process | http://arxiv.org/abs/1111.1802 | id:1111.1802 author:Tamara Broderick, Lester Mackey, John Paisley, Michael I. Jordan category:stat.ME stat.ML  published:2011-11-08 summary:We develop a Bayesian nonparametric approach to a general family of latent class problems in which individuals can belong simultaneously to multiple classes and where each class can be exhibited multiple times by an individual. We introduce a combinatorial stochastic process known as the negative binomial process (NBP) as an infinite-dimensional prior appropriate for such problems. We show that the NBP is conjugate to the beta process, and we characterize the posterior distribution under the beta-negative binomial process (BNBP) and hierarchical models based on the BNBP (the HBNBP). We study the asymptotic properties of the BNBP and develop a three-parameter extension of the BNBP that exhibits power-law behavior. We derive MCMC algorithms for posterior inference under the HBNBP, and we present experiments using these algorithms in the domains of image segmentation, object recognition, and document analysis. version:5
arxiv-1306-2194 | Adaptive Noisy Clustering | http://arxiv.org/abs/1306.2194 | id:1306.2194 author:Michael Chichignoud, SÃ©bastien Loustau category:math.ST stat.ML stat.TH  published:2013-06-10 summary:The problem of adaptive noisy clustering is investigated. Given a set of noisy observations $Z_i=X_i+\epsilon_i$, $i=1,...,n$, the goal is to design clusters associated with the law of $X_i$'s, with unknown density $f$ with respect to the Lebesgue measure. Since we observe a corrupted sample, a direct approach as the popular {\it $k$-means} is not suitable in this case. In this paper, we propose a noisy $k$-means minimization, which is based on the $k$-means loss function and a deconvolution estimator of the density $f$. In particular, this approach suffers from the dependence on a bandwidth involved in the deconvolution kernel. Fast rates of convergence for the excess risk are proposed for a particular choice of the bandwidth, which depends on the smoothness of the density $f$. Then, we turn out into the main issue of the paper: the data-driven choice of the bandwidth. We state an adaptive upper bound for a new selection rule, called ERC (Empirical Risk Comparison). This selection rule is based on the Lepski's principle, where empirical risks associated with different bandwidths are compared. Finally, we illustrate that this adaptive rule can be used in many statistical problems of $M$-estimation where the empirical risk depends on a nuisance parameter. version:1
arxiv-1306-2159 | Image segmentation by optimal and hierarchical piecewise constant approximations | http://arxiv.org/abs/1306.2159 | id:1306.2159 author:M. Kharinov category:cs.CV  published:2013-06-10 summary:Piecewise constant image approximations of sequential number of segments or clusters of disconnected pixels are treated. The method of majorizing of optimal approximation sequence by hierarchical sequence of image approximations is proposed. A generalization for multidimensional case of color and multispectral images is foreseen. version:1
arxiv-1306-2158 | "Not not bad" is not "bad": A distributional account of negation | http://arxiv.org/abs/1306.2158 | id:1306.2158 author:Karl Moritz Hermann, Edward Grefenstette, Phil Blunsom category:cs.CL 68T50 I.2.7  published:2013-06-10 summary:With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models. version:1
arxiv-1212-2834 | Dictionary Subselection Using an Overcomplete Joint Sparsity Model | http://arxiv.org/abs/1212.2834 | id:1212.2834 author:Mehrdad Yaghoobi, Laurent Daudet, Michael E. Davies category:cs.LG math.OC stat.ML  published:2012-12-12 summary:Many natural signals exhibit a sparse representation, whenever a suitable describing model is given. Here, a linear generative model is considered, where many sparsity-based signal processing techniques rely on such a simplified model. As this model is often unknown for many classes of the signals, we need to select such a model based on the domain knowledge or using some exemplar signals. This paper presents a new exemplar based approach for the linear model (called the dictionary) selection, for such sparse inverse problems. The problem of dictionary selection, which has also been called the dictionary learning in this setting, is first reformulated as a joint sparsity model. The joint sparsity model here differs from the standard joint sparsity model as it considers an overcompleteness in the representation of each signal, within the range of selected subspaces. The new dictionary selection paradigm is examined with some synthetic and realistic simulations. version:2
arxiv-1306-2119 | Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n) | http://arxiv.org/abs/1306.2119 | id:1306.2119 author:Francis Bach, Eric Moulines category:cs.LG math.OC stat.ML  published:2013-06-10 summary:We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/n^{1/2}). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches. version:1
arxiv-1306-2118 | A Novel Approach for Single Gene Selection Using Clustering and Dimensionality Reduction | http://arxiv.org/abs/1306.2118 | id:1306.2118 author:E. N. Sathishkumar, K. Thangavel, T. Chandrasekhar category:cs.CE cs.LG  published:2013-06-10 summary:We extend the standard rough set-based approach to deal with huge amounts of numeric attributes versus small amount of available objects. Here, a novel approach of clustering along with dimensionality reduction; Hybrid Fuzzy C Means-Quick Reduct (FCMQR) algorithm is proposed for single gene selection. Gene selection is a process to select genes which are more informative. It is one of the important steps in knowledge discovery. The problem is that all genes are not important in gene expression data. Some of the genes may be redundant, and others may be irrelevant and noisy. In this study, the entire dataset is divided in proper grouping of similar genes by applying Fuzzy C Means (FCM) algorithm. A high class discriminated genes has been selected based on their degree of dependence by applying Quick Reduct algorithm based on Rough Set Theory to all the resultant clusters. Average Correlation Value (ACV) is calculated for the high class discriminated genes. The clusters which have the ACV value a s 1 is determined as significant clusters, whose classification accuracy will be equal or high when comparing to the accuracy of the entire dataset. The proposed algorithm is evaluated using WEKA classifiers and compared. Finally, experimental results related to the leukemia cancer data confirm that our approach is quite promising, though it surely requires further research. version:1
arxiv-1306-2102 | Discriminative k-means clustering | http://arxiv.org/abs/1306.2102 | id:1306.2102 author:Ognjen Arandjelovic category:cs.CV  published:2013-06-10 summary:The k-means algorithm is a partitional clustering method. Over 60 years old, it has been successfully used for a variety of problems. The popularity of k-means is in large part a consequence of its simplicity and efficiency. In this paper we are inspired by these appealing properties of k-means in the development of a clustering algorithm which accepts the notion of "positively" and "negatively" labelled data. The goal is to discover the cluster structure of both positive and negative data in a manner which allows for the discrimination between the two sets. The usefulness of this idea is demonstrated practically on the problem of face recognition, where the task of learning the scope of a person's appearance should be done in a manner which allows this face to be differentiated from others. version:1
arxiv-1306-2100 | Discriminative extended canonical correlation analysis for pattern set matching | http://arxiv.org/abs/1306.2100 | id:1306.2100 author:Ognjen Arandjelovic category:cs.CV  published:2013-06-10 summary:In this paper we address the problem of matching sets of vectors embedded in the same input space. We propose an approach which is motivated by canonical correlation analysis (CCA), a statistical technique which has proven successful in a wide variety of pattern recognition problems. Like CCA when applied to the matching of sets, our extended canonical correlation analysis (E-CCA) aims to extract the most similar modes of variability within two sets. Our first major contribution is the formulation of a principled framework for robust inference of such modes from data in the presence of uncertainty associated with noise and sampling randomness. E-CCA retains the efficiency and closed form computability of CCA, but unlike it, does not possess free parameters which cannot be inferred directly from data (inherent data dimensionality, and the number of canonical correlations used for set similarity computation). Our second major contribution is to show that in contrast to CCA, E-CCA is readily adapted to match sets in a discriminative learning scheme which we call discriminative extended canonical correlation analysis (DE-CCA). Theoretical contributions of this paper are followed by an empirical evaluation of its premises on the task of face recognition from sets of rasterized appearance images. The results demonstrate that our approach, E-CCA, already outperforms both CCA and its quasi-discriminative counterpart constrained CCA (C-CCA), for all values of their free parameters. An even greater improvement is achieved with the discriminative variant, DE-CCA. version:1
arxiv-1306-2094 | Predicting Risk-of-Readmission for Congestive Heart Failure Patients: A Multi-Layer Approach | http://arxiv.org/abs/1306.2094 | id:1306.2094 author:Kiyana Zolfaghar, Nele Verbiest, Jayshree Agarwal, Naren Meadem, Si-Chi Chin, Senjuti Basu Roy, Ankur Teredesai, David Hazel, Paul Amoroso, Lester Reed category:cs.LG stat.AP  published:2013-06-10 summary:Mitigating risk-of-readmission of Congestive Heart Failure (CHF) patients within 30 days of discharge is important because such readmissions are not only expensive but also critical indicator of provider care and quality of treatment. Accurately predicting the risk-of-readmission may allow hospitals to identify high-risk patients and eventually improve quality of care by identifying factors that contribute to such readmissions in many scenarios. In this paper, we investigate the problem of predicting risk-of-readmission as a supervised learning problem, using a multi-layer classification approach. Earlier contributions inadequately attempted to assess a risk value for 30 day readmission by building a direct predictive model as opposed to our approach. We first split the problem into various stages, (a) at risk in general (b) risk within 60 days (c) risk within 30 days, and then build suitable classifiers for each stage, thereby increasing the ability to accurately predict the risk using multiple layers of decision. The advantage of our approach is that we can use different classification models for the subtasks that are more suited for the respective problems. Moreover, each of the subtasks can be solved using different features and training data leading to a highly confident diagnosis or risk compared to a one-shot single layer approach. An experimental evaluation on actual hospital patient record data from Multicare Health Systems shows that our model is significantly better at predicting risk-of-readmission of CHF patients within 30 days after discharge compared to prior attempts. version:1
arxiv-1306-2084 | Logistic Tensor Factorization for Multi-Relational Data | http://arxiv.org/abs/1306.2084 | id:1306.2084 author:Maximilian Nickel, Volker Tresp category:stat.ML cs.LG  published:2013-06-10 summary:Tensor factorizations have become increasingly popular approaches for various learning tasks on structured data. In this work, we extend the RESCAL tensor factorization, which has shown state-of-the-art results for multi-relational learning, to account for the binary nature of adjacency tensors. We study the improvements that can be gained via this approach on various benchmark datasets and show that the logistic extension can improve the prediction results significantly. version:1
arxiv-1306-2081 | 3D model retrieval using global and local radial distances | http://arxiv.org/abs/1306.2081 | id:1306.2081 author:Bo Li, Henry Johan category:cs.GR cs.CV cs.IR  published:2013-06-10 summary:3D model retrieval techniques can be classified as histogram-based, view-based and graph-based approaches. We propose a hybrid shape descriptor which combines the global and local radial distance features by utilizing the histogram-based and view-based approaches respectively. We define an area-weighted global radial distance with respect to the center of the bounding sphere of the model and encode its distribution into a 2D histogram as the global radial distance shape descriptor. We then uniformly divide the bounding cube of a 3D model into a set of small cubes and define their centers as local centers. Then, we compute the local radial distance of a point based on the nearest local center. By sparsely sampling a set of views and encoding the local radial distance feature on the rendered views by color coding, we extract the local radial distance shape descriptor. Based on these two shape descriptors, we develop a hybrid radial distance shape descriptor for 3D model retrieval. Experiment results show that our hybrid shape descriptor outperforms several typical histogram-based and view-based approaches. version:1
arxiv-1306-2035 | Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation | http://arxiv.org/abs/1306.2035 | id:1306.2035 author:Martin Azizyan, Aarti Singh, Larry Wasserman category:stat.ML cs.LG math.ST stat.TH  published:2013-06-09 summary:While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering. version:1
arxiv-1306-2003 | Comparing Edge Detection Methods based on Stochastic Entropies and Distances for PolSAR Imagery | http://arxiv.org/abs/1306.2003 | id:1306.2003 author:AbraÃ£o D. C. Nascimento, Michelle M. Horta, Alejandro C. Frery, Renato J. Cintra category:math.ST cs.CV stat.TH  published:2013-06-09 summary:Polarimetric synthetic aperture radar (PolSAR) has achieved a prominent position as a remote imaging method. However, PolSAR images are contaminated by speckle noise due to the coherent illumination employed during the data acquisition. This noise provides a granular aspect to the image, making its processing and analysis (such as in edge detection) hard tasks. This paper discusses seven methods for edge detection in multilook PolSAR images. In all methods, the basic idea consists in detecting transition points in the finest possible strip of data which spans two regions. The edge is contoured using the transitions points and a B-spline curve. Four stochastic distances, two differences of entropies, and the maximum likelihood criterion were used under the scaled complex Wishart distribution; the first six stem from the h-phi class of measures. The performance of the discussed detection methods was quantified and analyzed by the computational time and probability of correct edge detection, with respect to the number of looks, the backscatter matrix as a whole, the SPAN, the covariance an the spatial resolution. The detection procedures were applied to three real PolSAR images. Results provide evidence that the methods based on the Bhattacharyya distance and the difference of Shannon entropies outperform the other techniques. version:1
arxiv-1211-1716 | Blind Signal Separation in the Presence of Gaussian Noise | http://arxiv.org/abs/1211.1716 | id:1211.1716 author:Mikhail Belkin, Luis Rademacher, James Voss category:cs.LG cs.DS stat.ML  published:2012-11-07 summary:A prototypical blind signal separation problem is the so-called cocktail party problem, with n people talking simultaneously and n different microphones within a room. The goal is to recover each speech signal from the microphone inputs. Mathematically this can be modeled by assuming that we are given samples from an n-dimensional random variable X=AS, where S is a vector whose coordinates are independent random variables corresponding to each speaker. The objective is to recover the matrix A^{-1} given random samples from X. A range of techniques collectively known as Independent Component Analysis (ICA) have been proposed to address this problem in the signal processing and machine learning literature. Many of these techniques are based on using the kurtosis or other cumulants to recover the components. In this paper we propose a new algorithm for solving the blind signal separation problem in the presence of additive Gaussian noise, when we are given samples from X=AS+\eta, where \eta is drawn from an unknown, not necessarily spherical n-dimensional Gaussian distribution. Our approach is based on a method for decorrelating a sample with additive Gaussian noise under the assumption that the underlying distribution is a linear transformation of a distribution with independent components. Our decorrelation routine is based on the properties of cumulant tensors and can be combined with any standard cumulant-based method for ICA to get an algorithm that is provably robust in the presence of Gaussian noise. We derive polynomial bounds for the sample complexity and error propagation of our method. version:2
arxiv-1306-1927 | Learning About Meetings | http://arxiv.org/abs/1306.1927 | id:1306.1927 author:Been Kim, Cynthia Rudin category:stat.AP cs.CL  published:2013-06-08 summary:Most people participate in meetings almost every day, multiple times a day. The study of meetings is important, but also challenging, as it requires an understanding of social signals and complex interpersonal dynamics. Our aim this work is to use a data-driven approach to the science of meetings. We provide tentative evidence that: i) it is possible to automatically detect when during the meeting a key decision is taking place, from analyzing only the local dialogue acts, ii) there are common patterns in the way social dialogue acts are interspersed throughout a meeting, iii) at the time key decisions are made, the amount of time left in the meeting can be predicted from the amount of time that has passed, iv) it is often possible to predict whether a proposal during a meeting will be accepted or rejected based entirely on the language (the set of persuasive words) used by the speaker. version:1
arxiv-1209-4825 | Efficient Regularized Least-Squares Algorithms for Conditional Ranking on Relational Data | http://arxiv.org/abs/1209.4825 | id:1209.4825 author:Tapio Pahikkala, Antti Airola, Michiel Stock, Bernard De Baets, Willem Waegeman category:cs.LG stat.ML  published:2012-09-21 summary:In domains like bioinformatics, information retrieval and social network analysis, one can find learning tasks where the goal consists of inferring a ranking of objects, conditioned on a particular target object. We present a general kernel framework for learning conditional rankings from various types of relational data, where rankings can be conditioned on unseen data objects. We propose efficient algorithms for conditional ranking by optimizing squared regression and ranking loss functions. We show theoretically, that learning with the ranking loss is likely to generalize better than with the regression loss. Further, we prove that symmetry or reciprocity properties of relations can be efficiently enforced in the learned models. Experiments on synthetic and real-world data illustrate that the proposed methods deliver state-of-the-art performance in terms of predictive power and computational efficiency. Moreover, we also show empirically that incorporating symmetry or reciprocity properties can improve the generalization performance. version:2
arxiv-1306-1913 | Emotional Expression Classification using Time-Series Kernels | http://arxiv.org/abs/1306.1913 | id:1306.1913 author:Andras Lorincz, Laszlo Jeni, Zoltan Szabo, Jeffrey Cohn, Takeo Kanade category:cs.CV cs.LG stat.ML  published:2013-06-08 summary:Estimation of facial expressions, as spatio-temporal processes, can take advantage of kernel methods if one considers facial landmark positions and their motion in 3D space. We applied support vector classification with kernels derived from dynamic time-warping similarity measures. We achieved over 99% accuracy - measured by area under ROC curve - using only the 'motion pattern' of the PCA compressed representation of the marker point vector, the so-called shape parameters. Beyond the classification of full motion patterns, several expressions were recognized with over 90% accuracy in as few as 5-6 frames from their onset, about 200 milliseconds. version:1
arxiv-1306-1894 | Speckle Reduction with Adaptive Stack Filters | http://arxiv.org/abs/1306.1894 | id:1306.1894 author:MarÃ­a Elena Buemi, Alejandro C. Frery, Heitor S. Ramos category:cs.CV  published:2013-06-08 summary:Stack filters are a special case of non-linear filters. They have a good performance for filtering images with different types of noise while preserving edges and details. A stack filter decomposes an input image into stacks of binary images according to a set of thresholds. Each binary image is then filtered by a Boolean function, which characterizes the filter. Adaptive stack filters can be computed by training using a prototype (ideal) image and its corrupted version, leading to optimized filters with respect to a loss function. In this work we propose the use of training with selected samples for the estimation of the optimal Boolean function. We study the performance of adaptive stack filters when they are applied to speckled imagery, in particular to Synthetic Aperture Radar (SAR) images. This is done by evaluating the quality of the filtered images through the use of suitable image quality indexes and by measuring the classification accuracy of the resulting images. We used SAR images as input, since they are affected by speckle noise that makes classification a difficult task. version:1
arxiv-1306-1851 | A Factor Graph Approach to Joint OFDM Channel Estimation and Decoding in Impulsive Noise Environments | http://arxiv.org/abs/1306.1851 | id:1306.1851 author:Marcel Nassar, Philip Schniter, Brian L. Evans category:cs.IT math.IT stat.ML  published:2013-06-07 summary:We propose a novel receiver for orthogonal frequency division multiplexing (OFDM) transmissions in impulsive noise environments. Impulsive noise arises in many modern wireless and wireline communication systems, such as Wi-Fi and powerline communications, due to uncoordinated interference that is much stronger than thermal noise. We first show that the bit-error-rate optimal receiver jointly estimates the propagation channel coefficients, the noise impulses, the finite-alphabet symbols, and the unknown bits. We then propose a near-optimal yet computationally tractable approach to this joint estimation problem using loopy belief propagation. In particular, we merge the recently proposed "generalized approximate message passing" (GAMP) algorithm with the forward-backward algorithm and soft-input soft-output decoding using a "turbo" approach. Numerical results indicate that the proposed receiver drastically outperforms existing receivers under impulsive noise and comes within 1 dB of the matched-filter bound. Meanwhile, with N tones, the proposed factor-graph-based receiver has only O(N log N) complexity, and it can be parallelized. version:1
arxiv-1306-1812 | Orbital-free Bond Breaking via Machine Learning | http://arxiv.org/abs/1306.1812 | id:1306.1812 author:John C. Snyder, Matthias Rupp, Katja Hansen, Leo Blooston, Klaus-Robert MÃ¼ller, Kieron Burke category:physics.chem-ph cond-mat.mtrl-sci stat.ML  published:2013-06-07 summary:Machine learning is used to approximate the kinetic energy of one dimensional diatomics as a functional of the electron density. The functional can accurately dissociate a diatomic, and can be systematically improved with training. Highly accurate self-consistent densities and molecular forces are found, indicating the possibility for ab-initio molecular dynamics simulations. version:1
arxiv-1306-6944 | The DeLiVerMATH project - Text analysis in mathematics | http://arxiv.org/abs/1306.6944 | id:1306.6944 author:Ulf SchÃ¶neberg, Wolfram Sperber category:cs.CL cs.DL cs.IR  published:2013-06-07 summary:A high-quality content analysis is essential for retrieval functionalities but the manual extraction of key phrases and classification is expensive. Natural language processing provides a framework to automatize the process. Here, a machine-based approach for the content analysis of mathematical texts is described. A prototype for key phrase extraction and classification of mathematical texts is presented. version:1
arxiv-1306-1716 | Fast greedy algorithm for subspace clustering from corrupted and incomplete data | http://arxiv.org/abs/1306.1716 | id:1306.1716 author:Alexander Petukhov, Inna Kozlov category:cs.LG cs.DS math.NA stat.ML  published:2013-06-07 summary:We describe the Fast Greedy Sparse Subspace Clustering (FGSSC) algorithm providing an efficient method for clustering data belonging to a few low-dimensional linear or affine subspaces. The main difference of our algorithm from predecessors is its ability to work with noisy data having a high rate of erasures (missed entries with the known coordinates) and errors (corrupted entries with unknown coordinates). We discuss here how to implement the fast version of the greedy algorithm with the maximum efficiency whose greedy strategy is incorporated into iterations of the basic algorithm. We provide numerical evidences that, in the subspace clustering capability, the fast greedy algorithm outperforms not only the existing state-of-the art SSC algorithm taken by the authors as a basic algorithm but also the recent GSSC algorithm. At the same time, its computational cost is only slightly higher than the cost of SSC. The numerical evidence of the algorithm significant advantage is presented for a few synthetic models as well as for the Extended Yale B dataset of facial images. In particular, the face recognition misclassification rate turned out to be 6-20 times lower than for the SSC algorithm. We provide also the numerical evidence that the FGSSC algorithm is able to perform clustering of corrupted data efficiently even when the sum of subspace dimensions significantly exceeds the dimension of the ambient space. version:1
arxiv-1306-1679 | Clifford Fourier-Mellin transform with two real square roots of -1 in Cl(p,q), p+q=2 | http://arxiv.org/abs/1306.1679 | id:1306.1679 author:Eckhard Hitzer category:math.RA cs.CV  published:2013-06-07 summary:We describe a non-commutative generalization of the complex Fourier-Mellin transform to Clifford algebra valued signal functions over the domain $\R^{p,q}$ taking values in Cl(p,q), p+q=2. Keywords: algebra, Fourier transforms; Logic, set theory, and algebra, Fourier analysis, Integral transforms version:1
arxiv-1306-1676 | Algebraic foundations of split hypercomplex nonlinear adaptive filtering | http://arxiv.org/abs/1306.1676 | id:1306.1676 author:Eckhard Hitzer category:cs.CV math.RA 60G35  15A66  published:2013-06-07 summary:A split hypercomplex learning algorithm for the training of nonlinear finite impulse response adaptive filters for the processing of hypercomplex signals of any dimension is proposed. The derivation strictly takes into account the laws of hypercomplex algebra and hypercomplex calculus, some of which have been neglected in existing learning approaches (e.g. for quaternions). Already in the case of quaternions we can predict improvements in performance of hypercomplex processes. The convergence of the proposed algorithms is rigorously analyzed. Keywords: Quaternionic adaptive filtering, Hypercomplex adaptive filtering, Nonlinear adaptive filtering, Hypercomplex Multilayer Perceptron, Clifford geometric algebra version:1
arxiv-1210-6001 | Reducing statistical time-series problems to binary classification | http://arxiv.org/abs/1210.6001 | id:1210.6001 author:Daniil Ryabko, JÃ©rÃ©mie Mary category:cs.LG stat.ML  published:2012-10-22 summary:We show how binary classification methods developed to work on i.i.d. data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. The algorithms that we construct for solving these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the proposed algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data. version:3
arxiv-1306-1669 | Quaternionic Fourier-Mellin Transform | http://arxiv.org/abs/1306.1669 | id:1306.1669 author:Eckhard Hitzer category:math.RA cs.CV  published:2013-06-07 summary:In this contribution we generalize the classical Fourier Mellin transform [S. Dorrode and F. Ghorbel, Robust and efficient Fourier-Mellin transform approximations for gray-level image reconstruction and complete invariant description, Computer Vision and Image Understanding, 83(1) (2001), 57-78, DOI 10.1006/cviu.2001.0922.], which transforms functions $f$ representing, e.g., a gray level image defined over a compact set of $\mathbb{R}^2$. The quaternionic Fourier Mellin transform (QFMT) applies to functions $f: \mathbb{R}^2 \rightarrow \mathbb{H}$, for which $ f $ is summable over $\mathbb{R}_+^* \times \mathbb{S}^1$ under the measure $d\theta \frac{dr}{r}$. $\mathbb{R}_+^*$ is the multiplicative group of positive and non-zero real numbers. We investigate the properties of the QFMT similar to the investigation of the quaternionic Fourier Transform (QFT) in [E. Hitzer, Quaternion Fourier Transform on Quaternion Fields and Generalizations, Advances in Applied Clifford Algebras, 17(3) (2007), 497-517.; E. Hitzer, Directional Uncertainty Principle for Quaternion Fourier Transforms, Advances in Applied Clifford Algebras, 20(2) (2010), 271-284, online since 08 July 2009.]. version:1
arxiv-1306-1653 | Non-constant bounded holomorphic functions of hyperbolic numbers - Candidates for hyperbolic activation functions | http://arxiv.org/abs/1306.1653 | id:1306.1653 author:Eckhard Hitzer category:cs.NE cs.CV math.RA  published:2013-06-07 summary:The Liouville theorem states that bounded holomorphic complex functions are necessarily constant. Holomorphic functions fulfill the socalled Cauchy-Riemann (CR) conditions. The CR conditions mean that a complex $z$-derivative is independent of the direction. Holomorphic functions are ideal for activation functions of complex neural networks, but the Liouville theorem makes them useless. Yet recently the use of hyperbolic numbers, lead to the construction of hyperbolic number neural networks. We will describe the Cauchy-Riemann conditions for hyperbolic numbers and show that there exists a new interesting type of bounded holomorphic functions of hyperbolic numbers, which are not constant. We give examples of such functions. They therefore substantially expand the available candidates for holomorphic activation functions for hyperbolic number neural networks. Keywords: Hyperbolic numbers, Liouville theorem, Cauchy-Riemann conditions, bounded holomorphic functions version:1
arxiv-1306-1650 | OPS-QFTs: A new type of quaternion Fourier transforms based on the orthogonal planes split with one or two general pure quaternions | http://arxiv.org/abs/1306.1650 | id:1306.1650 author:Eckhard Hitzer category:math.RA cs.CV 15A66  42A38  published:2013-06-07 summary:We explain the orthogonal planes split (OPS) of quaternions based on the arbitrary choice of one or two linearly independent pure unit quaternions $f,g$. Next we systematically generalize the quaternionic Fourier transform (QFT) applied to quaternion fields to conform with the OPS determined by $f,g$, or by only one pure unit quaternion $f$, comment on their geometric meaning, and establish inverse transformations. Keywords: Clifford geometric algebra, quaternion geometry, quaternion Fourier transform, inverse Fourier transform, orthogonal planes split version:1
arxiv-1306-1619 | Statistical Denoising for single molecule fluorescence microscopic images | http://arxiv.org/abs/1306.1619 | id:1306.1619 author:Ji Won Yoon category:cs.CV  published:2013-06-07 summary:Single molecule fluorescence microscopy is a powerful technique for uncovering detailed information about biological systems, both in vitro and in vivo. In such experiments, the inherently low signal to noise ratios mean that accurate algorithms to separate true signal and background noise are essential to generate meaningful results. To this end, we have developed a new and robust method to reduce noise in single molecule fluorescence images by using a Gaussian Markov Random Field (GMRF) prior in a Bayesian framework. Two different strategies are proposed to build the prior - an intrinsic GMRF, with a stationary relationship between pixels and a heterogeneous intrinsic GMRF, with a differently weighted relationship between pixels classified as molecules and background. Testing with synthetic and real experimental fluorescence images demonstrates that the heterogeneous intrinsic GMRF is superior to other conventional de-noising approaches. version:1
arxiv-1306-1609 | Vesselness features and the inverse compositional AAM for robust face recognition using thermal IR | http://arxiv.org/abs/1306.1609 | id:1306.1609 author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV  published:2013-06-07 summary:Over the course of the last decade, infrared (IR) and particularly thermal IR imaging based face recognition has emerged as a promising complement to conventional, visible spectrum based approaches which continue to struggle when applied in the real world. While inherently insensitive to visible spectrum illumination changes, IR images introduce specific challenges of their own, most notably sensitivity to factors which affect facial heat emission patterns, e.g. emotional state, ambient temperature, and alcohol intake. In addition, facial expression and pose changes are more difficult to correct in IR images because they are less rich in high frequency detail which is an important cue for fitting any deformable model. We describe a novel method which addresses these challenges. To normalize for pose and facial expression changes we generate a synthetic frontal image of a face in a canonical, neutral facial expression from an image of the face in an arbitrary pose and facial expression. This is achieved by piecewise affine warping which follows active appearance model (AAM) fitting. This is the first publication which explores the use of an AAM on thermal IR images; we propose a pre-processing step which enhances detail in thermal images, making AAM convergence faster and more accurate. To overcome the problem of thermal IR image sensitivity to the pattern of facial temperature emissions we describe a representation based on reliable anatomical features. In contrast to previous approaches, our representation is not binary; rather, our method accounts for the reliability of the extracted features. This makes the proposed representation much more robust both to pose and scale changes. The effectiveness of the proposed approach is demonstrated on the largest public database of thermal IR images of faces on which it achieved 100% identification, significantly outperforming previous methods. version:1
arxiv-1306-2268 | Accomplishable Tasks in Knowledge Representation | http://arxiv.org/abs/1306.2268 | id:1306.2268 author:Keehang Kwon, Mi-Young Park category:cs.AI cs.CL  published:2013-06-07 summary:Knowledge Representation (KR) is traditionally based on the logic of facts, expressed in boolean logic. However, facts about an agent can also be seen as a set of accomplished tasks by the agent. This paper proposes a new approach to KR: the notion of task logical KR based on Computability Logic. This notion allows the user to represent both accomplished tasks and accomplishable tasks by the agent. This notion allows us to build sophisticated KRs about many interesting agents, which have not been supported by previous logical languages. version:1
arxiv-1306-1822 | Illumination-invariant face recognition from a single image across extreme pose using a dual dimension AAM ensemble in the thermal infrared spectrum | http://arxiv.org/abs/1306.1822 | id:1306.1822 author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV  published:2013-06-07 summary:Over the course of the last decade, infrared (IR) and particularly thermal IR imaging based face recognition has emerged as a promising complement to conventional, visible spectrum based approaches which continue to struggle when applied in practice. While inherently insensitive to visible spectrum illumination changes, IR data introduces specific challenges of its own, most notably sensitivity to factors which affect facial heat emission patterns, e.g. emotional state, ambient temperature, and alcohol intake. In addition, facial expression and pose changes are more difficult to correct in IR images because they are less rich in high frequency detail which is an important cue for fitting any deformable model. In this paper we describe a novel method which addresses these major challenges. Specifically, when comparing two thermal IR images of faces, we mutually normalize their poses and facial expressions by using an active appearance model (AAM) to generate synthetic images of the two faces with a neutral facial expression and in the same view (the average of the two input views). This is achieved by piecewise affine warping which follows AAM fitting. A major contribution of our work is the use of an AAM ensemble in which each AAM is specialized to a particular range of poses and a particular region of the thermal IR face space. Combined with the contributions from our previous work which addressed the problem of reliable AAM fitting in the thermal IR spectrum, and the development of a person-specific representation robust to transient changes in the pattern of facial temperature emissions, the proposed ensemble framework accurately matches faces across the full range of yaw from frontal to profile, even in the presence of scale variation (e.g. due to the varying distance of a subject from the camera). version:1
arxiv-1306-1603 | Infrared face recognition: a literature review | http://arxiv.org/abs/1306.1603 | id:1306.1603 author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV  published:2013-06-07 summary:Automatic face recognition (AFR) is an area with immense practical potential which includes a wide range of commercial and law enforcement applications, and it continues to be one of the most active research areas of computer vision. Even after over three decades of intense research, the state-of-the-art in AFR continues to improve, benefiting from advances in a range of different fields including image processing, pattern recognition, computer graphics and physiology. However, systems based on visible spectrum images continue to face challenges in the presence of illumination, pose and expression changes, as well as facial disguises, all of which can significantly decrease their accuracy. Amongst various approaches which have been proposed in an attempt to overcome these limitations, the use of infrared (IR) imaging has emerged as a particularly promising research direction. This paper presents a comprehensive and timely review of the literature on this subject. version:1
arxiv-1305-0445 | Deep Learning of Representations: Looking Forward | http://arxiv.org/abs/1305.0445 | id:1305.0445 author:Yoshua Bengio category:cs.LG  published:2013-05-02 summary:Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges. version:2
arxiv-1304-7851 | North Atlantic Right Whale Contact Call Detection | http://arxiv.org/abs/1304.7851 | id:1304.7851 author:Rami Abousleiman, Guangzhi Qu, Osamah Rawashdeh category:cs.LG cs.SD  published:2013-04-30 summary:The North Atlantic right whale (Eubalaena glacialis) is an endangered species. These whales continuously suffer from deadly vessel impacts alongside the eastern coast of North America. There have been countless efforts to save the remaining 350 - 400 of them. One of the most prominent works is done by Marinexplore and Cornell University. A system of hydrophones linked to satellite connected-buoys has been deployed in the whales habitat. These hydrophones record and transmit live sounds to a base station. These recording might contain the right whale contact call as well as many other noises. The noise rate increases rapidly in vessel-busy areas such as by the Boston harbor. This paper presents and studies the problem of detecting the North Atlantic right whale contact call with the presence of noise and other marine life sounds. A novel algorithm was developed to preprocess the sound waves before a tree based hierarchical classifier is used to classify the data and provide a score. The developed model was trained with 30,000 data points made available through the Cornell University Whale Detection Challenge program. Results showed that the developed algorithm had close to 85% success rate in detecting the presence of the North Atlantic right whale. version:2
arxiv-1306-1520 | Policy Search: Any Local Optimum Enjoys a Global Performance Guarantee | http://arxiv.org/abs/1306.1520 | id:1306.1520 author:Bruno Scherrer, Matthieu Geist category:cs.LG cs.AI cs.RO math.OC  published:2013-06-06 summary:Local Policy Search is a popular reinforcement learning approach for handling large state spaces. Formally, it searches locally in a paramet erized policy space in order to maximize the associated value function averaged over some predefined distribution. It is probably commonly b elieved that the best one can hope in general from such an approach is to get a local optimum of this criterion. In this article, we show th e following surprising result: \emph{any} (approximate) \emph{local optimum} enjoys a \emph{global performance guarantee}. We compare this g uarantee with the one that is satisfied by Direct Policy Iteration, an approximate dynamic programming algorithm that does some form of Poli cy Search: if the approximation error of Local Policy Search may generally be bigger (because local search requires to consider a space of s tochastic policies), we argue that the concentrability coefficient that appears in the performance bound is much nicer. Finally, we discuss several practical and theoretical consequences of our analysis. version:1
arxiv-1306-0842 | Kernel Mean Estimation and Stein's Effect | http://arxiv.org/abs/1306.0842 | id:1306.0842 author:Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur Gretton, Bernhard SchÃ¶lkopf category:stat.ML cs.LG math.ST stat.TH  published:2013-06-04 summary:A mean function in reproducing kernel Hilbert space, or a kernel mean, is an important part of many applications ranging from kernel principal component analysis to Hilbert-space embedding of distributions. Given finite samples, an empirical average is the standard estimate for the true kernel mean. We show that this estimator can be improved via a well-known phenomenon in statistics called Stein's phenomenon. After consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard. Focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. Empirical evaluations on several benchmark applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator. version:2
arxiv-1306-1467 | Highly Scalable, Parallel and Distributed AdaBoost Algorithm using Light Weight Threads and Web Services on a Network of Multi-Core Machines | http://arxiv.org/abs/1306.1467 | id:1306.1467 author:Munther Abualkibash, Ahmed ElSayed, Ausif Mahmood category:cs.DC cs.LG  published:2013-06-06 summary:AdaBoost is an important algorithm in machine learning and is being widely used in object detection. AdaBoost works by iteratively selecting the best amongst weak classifiers, and then combines several weak classifiers to obtain a strong classifier. Even though AdaBoost has proven to be very effective, its learning execution time can be quite large depending upon the application e.g., in face detection, the learning time can be several days. Due to its increasing use in computer vision applications, the learning time needs to be drastically reduced so that an adaptive near real time object detection system can be incorporated. In this paper, we develop a hybrid parallel and distributed AdaBoost algorithm that exploits the multiple cores in a CPU via light weight threads, and also uses multiple machines via a web service software architecture to achieve high scalability. We present a novel hierarchical web services based distributed architecture and achieve nearly linear speedup up to the number of processors available to us. In comparison with the previously published work, which used a single level master-slave parallel and distributed implementation [1] and only achieved a speedup of 2.66 on four nodes, we achieve a speedup of 95.1 on 31 workstations each having a quad-core processor, resulting in a learning time of only 4.8 seconds per feature. version:1
arxiv-1306-1462 | K-Algorithm A Modified Technique for Noise Removal in Handwritten Documents | http://arxiv.org/abs/1306.1462 | id:1306.1462 author:Kanika Bansal, Rajiv Kumar category:cs.CV  published:2013-06-06 summary:OCR has been an active research area since last few decades. OCR performs the recognition of the text in the scanned document image and converts it into editable form. The OCR process can have several stages like pre-processing, segmentation, recognition and post processing. The pre-processing stage is a crucial stage for the success of OCR, which mainly deals with noise removal. In the present paper, a modified technique for noise removal named as K-Algorithm has been proposed, which has two stages as filtering and binarization. The proposed technique shows improvised results in comparison to median filtering technique. version:1
arxiv-1306-1392 | PyHST2: an hybrid distributed code for high speed tomographic reconstruction with iterative reconstruction and a priori knowledge capabilities | http://arxiv.org/abs/1306.1392 | id:1306.1392 author:Alessandro Mirone, Emmanuelle Gouillart, Emmanuel Brun, Paul Tafforeau, Jerome Kieffer category:math.NA cs.CV  published:2013-06-06 summary:We present the PyHST2 code which is in service at ESRF for phase-contrast and absorption tomography. This code has been engineered to sustain the high data flow typical of the third generation synchrotron facilities (10 terabytes per experiment) by adopting a distributed and pipelined architecture. The code implements, beside a default filtered backprojection reconstruction, iterative reconstruction techniques with a-priori knowledge. These latter are used to improve the reconstruction quality or in order to reduce the required data volume and reach a given quality goal. The implemented a-priori knowledge techniques are based on the total variation penalisation and a new recently found convex functional which is based on overlapping patches. We give details of the different methods and their implementations while the code is distributed under free license. We provide methods for estimating, in the absence of ground-truth data, the optimal parameters values for a-priori techniques. version:1
arxiv-1306-1358 | Geometric operations implemented by conformal geometric algebra neural nodes | http://arxiv.org/abs/1306.1358 | id:1306.1358 author:Eckhard Hitzer category:cs.CV cs.NE math.RA  published:2013-06-06 summary:Geometric algebra is an optimal frame work for calculating with vectors. The geometric algebra of a space includes elements that represent all the its subspaces (lines, planes, volumes, ...). Conformal geometric algebra expands this approach to elementary representations of arbitrary points, point pairs, lines, circles, planes and spheres. Apart from including curved objects, conformal geometric algebra has an elegant unified quaternion like representation for all proper and improper Euclidean transformations, including reflections at spheres, general screw transformations and scaling. Expanding the concepts of real and complex neurons we arrive at the new powerful concept of conformal geometric algebra neurons. These neurons can easily take the above mentioned geometric objects or sets of these objects as inputs and apply a wide range of geometric transformations via the geometric algebra valued weights. version:1
arxiv-1306-1343 | The User Feedback on SentiWordNet | http://arxiv.org/abs/1306.1343 | id:1306.1343 author:Andrea Esuli category:cs.CL cs.IR I.2.7  published:2013-06-06 summary:With the release of SentiWordNet 3.0 the related Web interface has been restyled and improved in order to allow users to submit feedback on the SentiWordNet entries, in the form of the suggestion of alternative triplets of values for an entry. This paper reports on the release of the user feedback collected so far and on the plans for the future. version:1
arxiv-1306-4631 | Table of Content detection using Machine Learning | http://arxiv.org/abs/1306.4631 | id:1306.4631 author:Rachana Parikh, Avani R. Vasant category:cs.LG cs.DL cs.IR  published:2013-06-06 summary:Table of content (TOC) detection has drawn attention now a day because it plays an important role in digitization of multipage document. Generally book document is multipage document. So it becomes necessary to detect Table of Content page for easy navigation of multipage document and also to make information retrieval faster for desirable data from the multipage document. All the Table of content pages follow the different layout, different way of presenting the contents of the document like chapter, section, subsection etc. This paper introduces a new method to detect Table of content using machine learning technique with different features. With the main aim to detect Table of Content pages is to structure the document according to their contents. version:1
arxiv-1306-1326 | Performance analysis of unsupervised feature selection methods | http://arxiv.org/abs/1306.1326 | id:1306.1326 author:A. Nisthana Parveen, H. Hannah Inbarani, E. N. Sathishkumar category:cs.LG  published:2013-06-06 summary:Feature selection (FS) is a process which attempts to select more informative features. In some cases, too many redundant or irrelevant features may overpower main features for classification. Feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms. The main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are applied to discover discriminative features that will be the most adequate ones for classification. Efficiency of the approaches is evaluated using standard classification metrics. version:1
arxiv-1306-4633 | A Fuzzy Based Approach to Text Mining and Document Clustering | http://arxiv.org/abs/1306.4633 | id:1306.4633 author:Sumit Goswami, Mayank Singh Shishodia category:cs.LG cs.IR  published:2013-06-06 summary:Fuzzy logic deals with degrees of truth. In this paper, we have shown how to apply fuzzy logic in text mining in order to perform document clustering. We took an example of document clustering where the documents had to be clustered into two categories. The method involved cleaning up the text and stemming of words. Then, we chose m number of features which differ significantly in their word frequencies (WF), normalized by document length, between documents belonging to these two clusters. The documents to be clustered were represented as a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was used to cluster these documents into two clusters. After the FCM execution finished, the documents in the two clusters were analysed for the values of their respective m features. It was known that documents belonging to a document type, say X, tend to have higher WF values for some particular features. If the documents belonging to a cluster had higher WF values for those same features, then that cluster was said to represent X. By fuzzy logic, we not only get the cluster name, but also the degree to which a document belongs to a cluster. version:1
arxiv-1306-1323 | Verdict Accuracy of Quick Reduct Algorithm using Clustering and Classification Techniques for Gene Expression Data | http://arxiv.org/abs/1306.1323 | id:1306.1323 author:T. Chandrasekhar, K. Thangavel, E. N. Sathishkumar category:cs.LG cs.CE stat.ML  published:2013-06-06 summary:In most gene expression data, the number of training samples is very small compared to the large number of genes involved in the experiments. However, among the large amount of genes, only a small fraction is effective for performing a certain task. Furthermore, a small subset of genes is desirable in developing gene expression based diagnostic tools for delivering reliable and understandable results. With the gene selection results, the cost of biological experiment and decision can be greatly reduced by analyzing only the marker genes. An important application of gene expression data in functional genomics is to classify samples according to their gene expression profiles. Feature selection (FS) is a process which attempts to select more informative features. It is one of the important steps in knowledge discovery. Conventional supervised FS methods evaluate various feature subsets using an evaluation function or metric to select only those features which are related to the decision classes of the data under consideration. This paper studies a feature selection method based on rough set theory. Further K-Means, Fuzzy C-Means (FCM) algorithm have implemented for the reduced feature set without considering class labels. Then the obtained results are compared with the original class labels. Back Propagation Network (BPN) has also been used for classification. Then the performance of K-Means, FCM, and BPN are analyzed through the confusion matrix. It is found that the BPN is performing well comparatively. version:1
arxiv-1306-1301 | Recognition of Indian Sign Language in Live Video | http://arxiv.org/abs/1306.1301 | id:1306.1301 author:Joyeeta Singha, Karen Das category:cs.CV  published:2013-06-06 summary:Sign Language Recognition has emerged as one of the important area of research in Computer Vision. The difficulty faced by the researchers is that the instances of signs vary with both motion and appearance. Thus, in this paper a novel approach for recognizing various alphabets of Indian Sign Language is proposed where continuous video sequences of the signs have been considered. The proposed system comprises of three stages: Preprocessing stage, Feature Extraction and Classification. Preprocessing stage includes skin filtering, histogram matching. Eigen values and Eigen Vectors were considered for feature extraction stage and finally Eigen value weighted Euclidean distance is used to recognize the sign. It deals with bare hands, thus allowing the user to interact with the system in natural way. We have considered 24 different alphabets in the video sequences and attained a success rate of 96.25%. version:1
arxiv-1301-7641 | Multi-scale Discriminant Saliency with Wavelet-based Hidden Markov Tree Modelling | http://arxiv.org/abs/1301.7641 | id:1301.7641 author:Anh Cat Le Ngo, Kenneth Li-Minn Ang, Guoping Qiu, Jasmine Kah-Phooi Seng category:cs.CV  published:2013-01-31 summary:The bottom-up saliency, an early stage of humans' visual attention, can be considered as a binary classification problem between centre and surround classes. Discriminant power of features for the classification is measured as mutual information between distributions of image features and corresponding classes . As the estimated discrepancy very much depends on considered scale level, multi-scale structure and discriminant power are integrated by employing discrete wavelet features and Hidden Markov Tree (HMT). With wavelet coefficients and Hidden Markov Tree parameters, quad-tree like label structures are constructed and utilized in maximum a posterior probability (MAP) of hidden class variables at corresponding dyadic sub-squares. Then, a saliency value for each square block at each scale level is computed with discriminant power principle. Finally, across multiple scales is integrated the final saliency map by an information maximization rule. Both standard quantitative tools such as NSS, LCC, AUC and qualitative assessments are used for evaluating the proposed multi-scale discriminant saliency (MDIS) method against the well-know information based approach AIM on its released image collection with eye-tracking data. Simulation results are presented and analysed to verify the validity of MDIS as well as point out its limitation for further research direction. version:2
arxiv-1306-1298 | Multiclass Semi-Supervised Learning on Graphs using Ginzburg-Landau Functional Minimization | http://arxiv.org/abs/1306.1298 | id:1306.1298 author:Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus category:stat.ML cs.LG math.ST physics.data-an stat.TH I.5.3  published:2013-06-06 summary:We present a graph-based variational algorithm for classification of high-dimensional data, generalizing the binary diffuse interface model to the case of multiple classes. Motivated by total variation techniques, the method involves minimizing an energy functional made up of three terms. The first two terms promote a stepwise continuous classification function with sharp transitions between classes, while preserving symmetry among the class labels. The third term is a data fidelity term, allowing us to incorporate prior information into the model in a semi-supervised framework. The performance of the algorithm on synthetic data, as well as on the COIL and MNIST benchmark datasets, is competitive with state-of-the-art graph-based multiclass segmentation methods. version:1
arxiv-1211-2227 | Efficient learning of simplices | http://arxiv.org/abs/1211.2227 | id:1211.2227 author:Joseph Anderson, Navin Goyal, Luis Rademacher category:cs.LG cs.DS stat.ML  published:2012-11-09 summary:We show an efficient algorithm for the following problem: Given uniformly random points from an arbitrary n-dimensional simplex, estimate the simplex. The size of the sample and the number of arithmetic operations of our algorithm are polynomial in n. This answers a question of Frieze, Jerrum and Kannan [FJK]. Our result can also be interpreted as efficiently learning the intersection of n+1 half-spaces in R^n in the model where the intersection is bounded and we are given polynomially many uniform samples from it. Our proof uses the local search technique from Independent Component Analysis (ICA), also used by [FJK]. Unlike these previous algorithms, which were based on analyzing the fourth moment, ours is based on the third moment. We also show a direct connection between the problem of learning a simplex and ICA: a simple randomized reduction to ICA from the problem of learning a simplex. The connection is based on a known representation of the uniform measure on a simplex. Similar representations lead to a reduction from the problem of learning an affine transformation of an n-dimensional l_p ball to ICA. version:3
arxiv-1306-1185 | Multiclass Total Variation Clustering | http://arxiv.org/abs/1306.1185 | id:1306.1185 author:Xavier Bresson, Thomas Laurent, David Uminsky, James H. von Brecht category:stat.ML cs.LG math.OC  published:2013-06-05 summary:Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. version:1
arxiv-1303-6310 | A hybrid bat algorithm | http://arxiv.org/abs/1303.6310 | id:1303.6310 author:Iztok Fister Jr., DuÅ¡an Fister, Xin-She Yang category:cs.NE  published:2013-03-25 summary:Swarm intelligence is a very powerful technique to be used for optimization purposes. In this paper we present a new swarm intelligence algorithm, based on the bat algorithm. The Bat algorithm is hybridized with differential evolution strategies. Besides showing very promising results of the standard benchmark functions, this hybridization also significantly improves the original bat algorithm. version:3
arxiv-1209-1797 | Securing Your Transactions: Detecting Anomalous Patterns In XML Documents | http://arxiv.org/abs/1209.1797 | id:1209.1797 author:Eitan Menahem, Alon Schclar, Lior Rokach, Yuval Elovici category:cs.CR cs.LG C.2.0; K.4.4; I.2.6  published:2012-09-09 summary:XML transactions are used in many information systems to store data and interact with other systems. Abnormal transactions, the result of either an on-going cyber attack or the actions of a benign user, can potentially harm the interacting systems and therefore they are regarded as a threat. In this paper we address the problem of anomaly detection and localization in XML transactions using machine learning techniques. We present a new XML anomaly detection framework, XML-AD. Within this framework, an automatic method for extracting features from XML transactions was developed as well as a practical method for transforming XML features into vectors of fixed dimensionality. With these two methods in place, the XML-AD framework makes it possible to utilize general learning algorithms for anomaly detection. Central to the functioning of the framework is a novel multi-univariate anomaly detection algorithm, ADIFA. The framework was evaluated on four XML transactions datasets, captured from real information systems, in which it achieved over 89% true positive detection rate with less than a 0.2% false positive rate. version:3
arxiv-1306-1083 | Discriminative Parameter Estimation for Random Walks Segmentation: Technical Report | http://arxiv.org/abs/1306.1083 | id:1306.1083 author:Pierre-Yves Baudin, Danny Goodman, Puneet Kumar, Noura Azzabou, Pierre G. Carlier, Nikos Paragios, M. Pawan Kumar category:cs.CV cs.LG  published:2013-06-05 summary:The Random Walks (RW) algorithm is one of the most e - cient and easy-to-use probabilistic segmentation methods. By combining contrast terms with prior terms, it provides accurate segmentations of medical images in a fully automated manner. However, one of the main drawbacks of using the RW algorithm is that its parameters have to be hand-tuned. we propose a novel discriminative learning framework that estimates the parameters using a training dataset. The main challenge we face is that the training samples are not fully supervised. Speci cally, they provide a hard segmentation of the images, instead of a proba-bilistic segmentation. We overcome this challenge by treating the optimal probabilistic segmentation that is compatible with the given hard segmentation as a latent variable. This allows us to employ the latent support vector machine formulation for parameter estimation. We show that our approach signi cantly outperforms the baseline methods on a challenging dataset consisting of real clinical 3D MRI volumes of skeletal muscles. version:1
arxiv-1306-1052 | Fast Dual Variational Inference for Non-Conjugate LGMs | http://arxiv.org/abs/1306.1052 | id:1306.1052 author:Mohammad Emtiyaz Khan, Aleksandr Y. Aravkin, Michael P. Friedlander, Matthias Seeger category:stat.ML math.OC stat.CO  published:2013-06-05 summary:Latent Gaussian models (LGMs) are widely used in statistics and machine learning. Bayesian inference in non-conjugate LGMs is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods. Algorithms based on variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use. However, the structure of the optimization problems associated with these approximations remains poorly understood, and standard solvers take too long to converge. We derive a novel dual variational inference approach that exploits the convexity property of the VG approximations. We obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods. Using real-world data, we demonstrate these advantages on a variety of LGMs, including Gaussian process classification, and latent Gaussian Markov random fields. version:1
arxiv-1306-1034 | ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control | http://arxiv.org/abs/1306.1034 | id:1306.1034 author:Mehul Bhatt, Jakob Suchan, Christian Freksa category:cs.AI cs.CV cs.HC  published:2013-06-05 summary:We construe smart meeting cinematography with a focus on professional situations such as meetings and seminars, possibly conducted in a distributed manner across socio-spatially separated groups. The basic objective in smart meeting cinematography is to interpret professional interactions involving people, and automatically produce dynamic recordings of discussions, debates, presentations etc in the presence of multiple communication modalities. Typical modalities include gestures (e.g., raising one's hand for a question, applause), voice and interruption, electronic apparatus (e.g., pressing a button), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instance of smart meeting cinematography concept, aims to: (a) develop functionality-driven benchmarks with respect to the interpretation and control capabilities of human-cinematographers, real-time video editors, surveillance personnel, and typical human performance in everyday situations; (b) Develop general tools for the commonsense cognitive interpretation of dynamic scenes from the viewpoint of visuo-spatial cognition centred perceptual narrativisation. Particular emphasis is placed on declarative representations and interfacing mechanisms that seamlessly integrate within large-scale cognitive (interaction) systems and companion technologies consisting of diverse AI sub-components. For instance, the envisaged tools would provide general capabilities for high-level commonsense reasoning about space, events, actions, change, and interaction. version:1
arxiv-1306-1023 | Quaternion Fourier Transform on Quaternion Fields and Generalizations | http://arxiv.org/abs/1306.1023 | id:1306.1023 author:Eckhard Hitzer category:math.RA cs.CV math-ph math.MP  published:2013-06-05 summary:We treat the quaternionic Fourier transform (QFT) applied to quaternion fields and investigate QFT properties useful for applications. Different forms of the QFT lead us to different Plancherel theorems. We relate the QFT computation for quaternion fields to the QFT of real signals. We research the general linear ($GL$) transformation behavior of the QFT with matrices, Clifford geometric algebra and with examples. We finally arrive at wide-ranging non-commutative multivector FT generalizations of the QFT. Examples given are new volume-time and spacetime algebra Fourier transformations. version:1
arxiv-1305-7331 | Alternating Decision trees for early diagnosis of dengue fever | http://arxiv.org/abs/1305.7331 | id:1305.7331 author:M. Naresh Kumar category:cs.LG q-bio.QM stat.AP  published:2013-05-31 summary:Dengue fever is a flu-like illness spread by the bite of an infected mosquito which is fast emerging as a major health problem. Timely and cost effective diagnosis using clinical and laboratory features would reduce the mortality rates besides providing better grounds for clinical management and disease surveillance. We wish to develop a robust and effective decision tree based approach for predicting dengue disease. Our analysis is based on the clinical characteristics and laboratory measurements of the diseased individuals. We have developed and trained an alternating decision tree with boosting and compared its performance with C4.5 algorithm for dengue disease diagnosis. Of the 65 patient records a diagnosis establishes that 53 individuals have been confirmed to have dengue fever. An alternating decision tree based algorithm was able to differentiate the dengue fever using the clinical and laboratory data with number of correctly classified instances as 89%, F-measure of 0.86 and receiver operator characteristics (ROC) of 0.826 as compared to C4.5 having correctly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617 respectively. Alternating decision tree based approach with boosting has been able to predict dengue fever with a higher degree of accuracy than C4.5 based decision tree using simple clinical and laboratory features. Further analysis on larger data sets is required to improve the sensitivity and specificity of the alternating decision trees. version:2
arxiv-1306-0974 | Distributed Bayesian inference for consistent labeling of tracked objects in non-overlapping camera networks | http://arxiv.org/abs/1306.0974 | id:1306.0974 author:Jiuqing Wan, Li Liu category:cs.CV  published:2013-06-05 summary:One of the fundamental requirements for visual surveillance using non-overlapping camera networks is the correct labeling of tracked objects on each camera in a consistent way,in the sense that the captured tracklets, or observations in this paper, of the same object at different cameras should be assigned with the same label. In this paper, we formulate this task as a Bayesian inference problem and propose a distributed inference framework in which the posterior distribution of labeling variable corresponding to each observation, conditioned on all history appearance and spatio-temporal evidence made in the whole networks, is calculated based solely on local information processing on each camera and mutual information exchanging between neighboring cameras. In our framework, the number of objects presenting in the monitored region, i.e. the sampling space of labeling variables, does not need to be specified beforehand. Instead, it can be determined automatically on the fly. In addition, we make no assumption about the appearance distribution of a single object, but use similarity scores between appearance pairs, given by advanced object re-identification algorithm, as appearance likelihood for inference. This feature makes our method very flexible and competitive when observing condition undergoes large changes across camera views. To cope with the problem of missing detection, which is critical for distributed inference, we consider an enlarged neighborhood of each camera during inference and use a mixture model to describe the higher order spatio-temporal constraints. The robustness of the algorithm against missing detection is improved at the cost of slightly increased computation and communication burden at each camera node. Finally, we demonstrate the effectiveness of our method through experiments on an indoor Office Building dataset and an outdoor Campus Garden dataset. version:1
arxiv-1306-0963 | Inferring Robot Task Plans from Human Team Meetings: A Generative Modeling Approach with Logic-Based Prior | http://arxiv.org/abs/1306.0963 | id:1306.0963 author:Been Kim, Caleb M. Chacha, Julie Shah category:cs.AI cs.CL cs.RO stat.ML  published:2013-06-05 summary:We aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains, such as military field operations and disaster response. Deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. A human operator then translates the agreed upon plan into machine instructions for the robots. We present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. Our approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans. This hybrid approach enables us to overcome the challenge of performing inference over the large solution space with only a small amount of noisy data from the team planning session. We validate the algorithm through human subject experimentation and show we are able to infer a human team's final plan with 83% accuracy on average. We also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a PR2 robot. To the best of our knowledge, this is the first work that integrates a logical planning technique within a generative model to perform plan inference. version:1
arxiv-1009-5773 | Fast Reinforcement Learning for Energy-Efficient Wireless Communications | http://arxiv.org/abs/1009.5773 | id:1009.5773 author:Nicholas Mastronarde, Mihaela van der Schaar category:cs.LG  published:2010-09-29 summary:We consider the problem of energy-efficient point-to-point transmission of delay-sensitive data (e.g. multimedia data) over a fading channel. Existing research on this topic utilizes either physical-layer centric solutions, namely power-control and adaptive modulation and coding (AMC), or system-level solutions based on dynamic power management (DPM); however, there is currently no rigorous and unified framework for simultaneously utilizing both physical-layer centric and system-level techniques to achieve the minimum possible energy consumption, under delay constraints, in the presence of stochastic and a priori unknown traffic and channel conditions. In this report, we propose such a framework. We formulate the stochastic optimization problem as a Markov decision process (MDP) and solve it online using reinforcement learning. The advantages of the proposed online method are that (i) it does not require a priori knowledge of the traffic arrival and channel statistics to determine the jointly optimal power-control, AMC, and DPM policies; (ii) it exploits partial information about the system so that less information needs to be learned than when using conventional reinforcement learning algorithms; and (iii) it obviates the need for action exploration, which severely limits the adaptation speed and run-time performance of conventional reinforcement learning algorithms. Our results show that the proposed learning algorithms can converge up to two orders of magnitude faster than a state-of-the-art learning algorithm for physical layer power-control and up to three orders of magnitude faster than conventional reinforcement learning algorithms. version:4
arxiv-1306-0686 | Online Learning under Delayed Feedback | http://arxiv.org/abs/1306.0686 | id:1306.0686 author:Pooria Joulani, AndrÃ¡s GyÃ¶rgy, Csaba SzepesvÃ¡ri category:cs.LG cs.AI stat.ML  published:2013-06-04 summary:Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity. version:2
arxiv-1306-0886 | $\propto$SVM for learning with label proportions | http://arxiv.org/abs/1306.0886 | id:1306.0886 author:Felix X. Yu, Dong Liu, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang category:cs.LG stat.ML  published:2013-06-04 summary:We study the problem of learning with label proportions in which the training data is provided in groups and only the proportion of each class in each group is known. We propose a new method called proportion-SVM, or $\propto$SVM, which explicitly models the latent unknown instance labels together with the known group label proportions in a large-margin framework. Unlike the existing works, our approach avoids making restrictive assumptions about the data. The $\propto$SVM model leads to a non-convex integer programming problem. In order to solve it efficiently, we propose two algorithms: one based on simple alternating optimization and the other based on a convex relaxation. Extensive experiments on standard datasets show that $\propto$SVM outperforms the state-of-the-art, especially for larger group sizes. version:1
arxiv-1306-0897 | Urban ozone concentration forecasting with artificial neural network in Corsica | http://arxiv.org/abs/1306.0897 | id:1306.0897 author:Wani W. Tamas, Gilles Notton, Christophe Paoli, Cyril Voyant, Marie Laure Nivet, AurÃ©lia Balu category:cs.NE  published:2013-06-04 summary:Atmospheric pollutants concentration forecasting is an important issue in air quality monitoring. Qualitair Corse, the organization responsible for monitoring air quality in Corsica (France) region, needs to develop a short-term prediction model to lead its mission of information towards the public. Various deterministic models exist for meso-scale or local forecasting, but need powerful large variable sets, a good knowledge of atmospheric processes, and can be inaccurate because of local climatical or geographical particularities, as observed in Corsica, a mountainous island located in a Mediterranean Sea. As a result, we focus in this study on statistical models, and particularly Artificial Neural Networks (ANN) that have shown good results in the prediction of ozone concentration at horizon h+1 with data measured locally. The purpose of this study is to build a predictor to realize predictions of ozone and PM10 at horizon d+1 in Corsica in order to be able to anticipate pollution peak formation and to take appropriated prevention measures. Specific meteorological conditions are known to lead to particular pollution event in Corsica (e.g. Saharan dust event). Therefore, several ANN models will be used, for meteorological conditions clustering and for operational forecasting. version:1
arxiv-1306-0896 | Finding Numerical Solutions of Diophantine Equations using Ant Colony Optimization | http://arxiv.org/abs/1306.0896 | id:1306.0896 author:Siby Abraham, Sugata Sanyal, Mukund Sanglikar category:cs.NE cs.ET  published:2013-06-04 summary:The paper attempts to find numerical solutions of Diophantine equations, a challenging problem as there are no general methods to find solutions of such equations. It uses the metaphor of foraging habits of real ants. The ant colony optimization based procedure starts with randomly assigned locations to a fixed number of artificial ants. Depending upon the quality of these positions, ants deposit pheromone at the nodes. A successor node is selected from the topological neighborhood of each of the nodes based on this stochastic pheromone deposit. If an ant bumps into an already encountered node, the pheromone is updated correspondingly. A suitably defined pheromone evaporation strategy guarantees that premature convergence does not take place. The experimental results, which compares with those of other machine intelligence techniques, validate the effectiveness of the proposed method. version:1
arxiv-1305-4094 | Evolutionary optimization of an experimental apparatus | http://arxiv.org/abs/1305.4094 | id:1305.4094 author:I. Geisel, K. Cordes, J. Mahnke, S. JÃ¶llenbeck, J. Ostermann, J. Arlt, W. Ertmer, C. Klempt category:quant-ph cond-mat.quant-gas cs.NE  published:2013-05-17 summary:In recent decades, cold atom experiments have become increasingly complex. While computers control most parameters, optimization is mostly done manually. This is a time-consuming task for a high-dimensional parameter space with unknown correlations. Here we automate this process using a genetic algorithm based on Differential Evolution. We demonstrate that this algorithm optimizes 21 correlated parameters and that it is robust against local maxima and experimental noise. The algorithm is flexible and easy to implement. Thus, the presented scheme can be applied to a wide range of experimental optimization tasks. version:2
arxiv-1306-0895 | Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances | http://arxiv.org/abs/1306.0895 | id:1306.0895 author:Marco Cuturi category:stat.ML  published:2013-06-04 summary:Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem. version:1
arxiv-1306-0733 | Fast Gradient-Based Inference with Continuous Latent Variable Models in Auxiliary Form | http://arxiv.org/abs/1306.0733 | id:1306.0733 author:Diederik P Kingma category:cs.LG stat.ML  published:2013-06-04 summary:We propose a technique for increasing the efficiency of gradient-based inference and learning in Bayesian networks with multiple layers of continuous latent vari- ables. We show that, in many cases, it is possible to express such models in an auxiliary form, where continuous latent variables are conditionally deterministic given their parents and a set of independent auxiliary variables. Variables of mod- els in this auxiliary form have much larger Markov blankets, leading to significant speedups in gradient-based inference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-based optimization. The relative efficiency is confirmed in ex- periments. version:1
arxiv-1203-2498 | Fault detection system for Arabic language | http://arxiv.org/abs/1203.2498 | id:1203.2498 author:Riadh Bouslimi, Houda Amraoui category:cs.CL  published:2012-03-08 summary:The study of natural language, especially Arabic, and mechanisms for the implementation of automatic processing is a fascinating field of study, with various potential applications. The importance of tools for natural language processing is materialized by the need to have applications that can effectively treat the vast mass of information available nowadays on electronic forms. Among these tools, mainly driven by the necessity of a fast writing in alignment to the actual daily life speed, our interest is on the writing auditors. The morphological and syntactic properties of Arabic make it a difficult language to master, and explain the lack in the processing tools for that language. Among these properties, we can mention: the complex structure of the Arabic word, the agglutinative nature, lack of vocalization, the segmentation of the text, the linguistic richness, etc. version:2
arxiv-1306-0178 | Using a bag of Words for Automatic Medical Image Annotation with a Latent Semantic | http://arxiv.org/abs/1306.0178 | id:1306.0178 author:Riadh Bouslimi, Abir Messaoudi, Jalel Akaichi category:cs.IR cs.CV  published:2013-06-02 summary:We present in this paper a new approach for the automatic annotation of medical images, using the approach of "bag-of-words" to represent the visual content of the medical image combined with text descriptors based approach tf.idf and reduced by latent semantic to extract the co-occurrence between terms and visual terms. A medical report is composed of a text describing a medical image. First, we are interested to index the text and extract all relevant terms using a thesaurus containing MeSH medical concepts. In a second phase, the medical image is indexed while recovering areas of interest which are invariant to change in scale, light and tilt. To annotate a new medical image, we use the approach of "bagof-words" to recover the feature vector. Indeed, we use the vector space model to retrieve similar medical image from the database training. The calculation of the relevance value of an image to the query image is based on the cosine function. We conclude with an experiment carried out on five types of radiological imaging to evaluate the performance of our system of medical annotation. The results showed that our approach works better with more images from the radiology of the skull. version:2
arxiv-1301-3551 | Information Theoretic Learning with Infinitely Divisible Kernels | http://arxiv.org/abs/1301.3551 | id:1301.3551 author:Luis G. Sanchez Giraldo, Jose C. Principe category:cs.LG cs.CV  published:2013-01-16 summary:In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art. version:6
arxiv-1304-1572 | Spectral Descriptors for Graph Matching | http://arxiv.org/abs/1304.1572 | id:1304.1572 author:Nan Hu, Leonidas Guibas category:cs.CV  published:2013-04-04 summary:In this paper, we consider the weighted graph matching problem. Recently, approaches to this problem based on spectral methods have gained significant attention. We propose two graph spectral descriptors based on the graph Laplacian, namely a Laplacian family signature (LFS) on nodes, and a pairwise heat kernel distance on edges. We show the stability of both our descriptors under small perturbation of edges and nodes. In addition, we show that our pairwise heat kernel distance is a noise-tolerant approximation of the classical adjacency matrix-based second order compatibility function. These nice properties suggest a descriptor-based matching scheme, for which we set up an integer quadratic problem (IQP) and apply an approximate solver to find a near optimal solution. We have tested our matching method on a set of randomly generated graphs, the widely-used CMU house sequence and a set of real images. These experiments show the superior performance of our selected node signatures and edge descriptors for graph matching, as compared with other existing signature-based matchings and adjacency matrix-based matchings. version:4
arxiv-1306-0626 | Provable Inductive Matrix Completion | http://arxiv.org/abs/1306.0626 | id:1306.0626 author:Prateek Jain, Inderjit S. Dhillon category:cs.LG cs.IT math.IT stat.ML  published:2013-06-04 summary:Consider a movie recommendation system where apart from the ratings information, side information such as user's age or movie's genre is also available. Unlike standard matrix completion, in this setting one should be able to predict inductively on new users/movies. In this paper, we study the problem of inductive matrix completion in the exact recovery setting. That is, we assume that the ratings matrix is generated by applying feature vectors to a low-rank matrix and the goal is to recover back the underlying matrix. Furthermore, we generalize the problem to that of low-rank matrix estimation using rank-1 measurements. We study this generic problem and provide conditions that the set of measurements should satisfy so that the alternating minimization method (which otherwise is a non-convex method with no convergence guarantees) is able to recover back the {\em exact} underlying low-rank matrix. In addition to inductive matrix completion, we show that two other low-rank estimation problems can be studied in our framework: a) general low-rank matrix sensing using rank-1 measurements, and b) multi-label regression with missing labels. For both the problems, we provide novel and interesting bounds on the number of measurements required by alternating minimization to provably converges to the {\em exact} low-rank matrix. In particular, our analysis for the general low rank matrix sensing problem significantly improves the required storage and computational cost than that required by the RIP-based matrix sensing methods \cite{RechtFP2007}. Finally, we provide empirical validation of our approach and demonstrate that alternating minimization is able to recover the true matrix for the above mentioned problems using a small number of measurements. version:1
arxiv-0812-2291 | Characterizing Truthful Multi-Armed Bandit Mechanisms | http://arxiv.org/abs/0812.2291 | id:0812.2291 author:Moshe Babaioff, Yogeshwer Sharma, Aleksandrs Slivkins category:cs.DS cs.GT cs.LG  published:2008-12-12 summary:We consider a multi-round auction setting motivated by pay-per-click auctions for Internet advertising. In each round the auctioneer selects an advertiser and shows her ad, which is then either clicked or not. An advertiser derives value from clicks; the value of a click is her private information. Initially, neither the auctioneer nor the advertisers have any information about the likelihood of clicks on the advertisements. The auctioneer's goal is to design a (dominant strategies) truthful mechanism that (approximately) maximizes the social welfare. If the advertisers bid their true private values, our problem is equivalent to the "multi-armed bandit problem", and thus can be viewed as a strategic version of the latter. In particular, for both problems the quality of an algorithm can be characterized by "regret", the difference in social welfare between the algorithm and the benchmark which always selects the same "best" advertisement. We investigate how the design of multi-armed bandit algorithms is affected by the restriction that the resulting mechanism must be truthful. We find that truthful mechanisms have certain strong structural properties -- essentially, they must separate exploration from exploitation -- and they incur much higher regret than the optimal multi-armed bandit algorithms. Moreover, we provide a truthful mechanism which (essentially) matches our lower bound on regret. version:7
arxiv-1303-1849 | Revisiting the Nystrom Method for Improved Large-Scale Machine Learning | http://arxiv.org/abs/1303.1849 | id:1303.1849 author:Alex Gittens, Michael W. Mahoney category:cs.LG cs.DS cs.NA  published:2013-03-07 summary:We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods; they characterize the effects of common data preprocessing steps on the performance of these algorithms; and they point to important differences between uniform sampling and nonuniform sampling methods based on leverage scores. In addition, our empirical results illustrate that existing theory is so weak that it does not provide even a qualitative guide to practice. Thus, we complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds---e.g. improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error---and they point to future directions to make these algorithms useful in even larger-scale machine learning applications. version:2
arxiv-1306-0539 | On the Performance Bounds of some Policy Search Dynamic Programming Algorithms | http://arxiv.org/abs/1306.0539 | id:1306.0539 author:Bruno Scherrer category:cs.AI cs.LG  published:2013-06-03 summary:We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative--exponential in $\frac{1}{\epsilon}$-- increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI. version:1
arxiv-1306-0442 | Evolutionary Approach for the Containers Bin-Packing Problem | http://arxiv.org/abs/1306.0442 | id:1306.0442 author:R. Kammarti, I. Ayachi, M. Ksouri, P. Borne category:cs.NE  published:2013-06-03 summary:This paper deals with the resolution of combinatorial optimization problems, particularly those concerning the maritime transport scheduling. We are interested in the management platforms in a river port and more specifically in container organisation operations with a view to minimizing the number of container rehandlings. Subsequently, we rmeet customers delivery deadlines and we reduce ship stoppage time In this paper, we propose a genetic algorithm to solve this problem and we present some experiments and results. version:1
arxiv-1304-7717 | The Randomized Dependence Coefficient | http://arxiv.org/abs/1304.7717 | id:1304.7717 author:David Lopez-Paz, Philipp Hennig, Bernhard SchÃ¶lkopf category:stat.ML  published:2013-04-29 summary:We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R\'enyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper. version:2
arxiv-1112-2318 | Low-rank optimization with trace norm penalty | http://arxiv.org/abs/1112.2318 | id:1112.2318 author:B. Mishra, G. Meyer, F. Bach, R. Sepulchre category:math.OC cs.LG  published:2011-12-11 summary:The paper addresses the problem of low-rank trace norm minimization. We propose an algorithm that alternates between fixed-rank optimization and rank-one updates. The fixed-rank optimization is characterized by an efficient factorization that makes the trace norm differentiable in the search space and the computation of duality gap numerically tractable. The search space is nonlinear but is equipped with a particular Riemannian structure that leads to efficient computations. We present a second-order trust-region algorithm with a guaranteed quadratic rate of convergence. Overall, the proposed optimization scheme converges super-linearly to the global solution while maintaining complexity that is linear in the number of rows and columns of the matrix. To compute a set of solutions efficiently for a grid of regularization parameters we propose a predictor-corrector approach that outperforms the naive warm-restart approach on the fixed-rank quotient manifold. The performance of the proposed algorithm is illustrated on problems of low-rank matrix completion and multivariate linear regression. version:2
arxiv-1211-2512 | Minimal cost feature selection of data with normal distribution measurement errors | http://arxiv.org/abs/1211.2512 | id:1211.2512 author:Hong Zhao, Fan Min, William Zhu category:cs.AI cs.LG  published:2012-11-12 summary:Minimal cost feature selection is devoted to obtain a trade-off between test costs and misclassification costs. This issue has been addressed recently on nominal data. In this paper, we consider numerical data with measurement errors and study minimal cost feature selection in this model. First, we build a data model with normal distribution measurement errors. Second, the neighborhood of each data item is constructed through the confidence interval. Comparing with discretized intervals, neighborhoods are more reasonable to maintain the information of data. Third, we define a new minimal total cost feature selection problem through considering the trade-off between test costs and misclassification costs. Fourth, we proposed a backtracking algorithm with three effective pruning techniques to deal with this problem. The algorithm is tested on four UCI data sets. Experimental results indicate that the pruning techniques are effective, and the algorithm is efficient for data sets with nearly one thousand objects. version:2
arxiv-1212-3185 | Cost-Sensitive Feature Selection of Data with Errors | http://arxiv.org/abs/1212.3185 | id:1212.3185 author:Hong Zhao, Fan Min, William Zhu category:cs.LG  published:2012-12-13 summary:In data mining applications, feature selection is an essential process since it reduces a model's complexity. The cost of obtaining the feature values must be taken into consideration in many domains. In this paper, we study the cost-sensitive feature selection problem on numerical data with measurement errors, test costs and misclassification costs. The major contributions of this paper are four-fold. First, a new data model is built to address test costs and misclassification costs as well as error boundaries. Second, a covering-based rough set with measurement errors is constructed. Given a confidence interval, the neighborhood is an ellipse in a two-dimension space, or an ellipsoidal in a three-dimension space, etc. Third, a new cost-sensitive feature selection problem is defined on this covering-based rough set. Fourth, both backtracking and heuristic algorithms are proposed to deal with this new problem. The algorithms are tested on six UCI (University of California - Irvine) data sets. Experimental results show that (1) the pruning techniques of the backtracking algorithm help reducing the number of operations significantly, and (2) the heuristic algorithm usually obtains optimal results. This study is a step toward realistic applications of cost-sensitive learning. version:3
arxiv-1306-0271 | KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles | http://arxiv.org/abs/1306.0271 | id:1306.0271 author:Marina Danilevsky, Chi Wang, Nihit Desai, Jingyi Guo, Jiawei Han category:cs.LG cs.IR  published:2013-06-03 summary:We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for topical keyphrase generation and ranking. By shifting from the unigram-centric traditional methods of unsupervised keyphrase extraction to a phrase-centric approach, we are able to directly compare and rank phrases of different lengths. We construct a topical keyphrase ranking function which implements the four criteria that represent high quality topical keyphrases (coverage, purity, phraseness, and completeness). The effectiveness of our approach is demonstrated on two collections of content-representative titles in the domains of Computer Science and Physics. version:1
arxiv-1304-7948 | Convolutional Neural Networks learn compact local image descriptors | http://arxiv.org/abs/1304.7948 | id:1304.7948 author:Christian Osendorfer, Justin Bayer, Patrick van der Smagt category:cs.CV  published:2013-04-30 summary:A standard deep convolutional neural network paired with a suitable loss function learns compact local image descriptors that perform comparably to state-of-the art approaches. version:2
arxiv-1305-4268 | Dynamic Covariance Models for Multivariate Financial Time Series | http://arxiv.org/abs/1305.4268 | id:1305.4268 author:Yue Wu, JosÃ© Miguel HernÃ¡ndez-Lobato, Zoubin Ghahramani category:stat.ME stat.ML  published:2013-05-18 summary:The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models. version:2
arxiv-1301-2628 | Robust Text Detection in Natural Scene Images | http://arxiv.org/abs/1301.2628 | id:1301.2628 author:Xu-Cheng Yin, Xuwang Yin, Kaizhu Huang, Hong-Wei Hao category:cs.CV cs.IR cs.LG I.5.4  published:2013-01-11 summary:Text detection in natural scene images is an important prerequisite for many content-based image analysis tasks. In this paper, we propose an accurate and robust method for detecting texts in natural scene images. A fast and effective pruning algorithm is designed to extract Maximally Stable Extremal Regions (MSERs) as character candidates using the strategy of minimizing regularized variations. Character candidates are grouped into text candidates by the ingle-link clustering algorithm, where distance weights and threshold of the clustering algorithm are learned automatically by a novel self-training distance metric learning algorithm. The posterior probabilities of text candidates corresponding to non-text are estimated with an character classifier; text candidates with high probabilities are then eliminated and finally texts are identified with a text classifier. The proposed system is evaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measure is over 76% and is significantly better than the state-of-the-art performance of 71%. Experimental results on a publicly available multilingual dataset also show that our proposed method can outperform the other competitive method with the f measure increase of over 9 percent. Finally, we have setup an online demo of our proposed scene text detection system at http://kems.ustb.edu.cn/learning/yin/dtext. version:3
arxiv-1306-1491 | Gaussian Process-Based Decentralized Data Fusion and Active Sensing for Mobility-on-Demand System | http://arxiv.org/abs/1306.1491 | id:1306.1491 author:Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan category:cs.RO cs.DC cs.LG cs.MA  published:2013-06-02 summary:Mobility-on-demand (MoD) systems have recently emerged as a promising paradigm of one-way vehicle sharing for sustainable personal urban mobility in densely populated cities. In this paper, we enhance the capability of a MoD system by deploying robotic shared vehicles that can autonomously cruise the streets to be hailed by users. A key challenge to managing the MoD system effectively is that of real-time, fine-grained mobility demand sensing and prediction. This paper presents a novel decentralized data fusion and active sensing algorithm for real-time, fine-grained mobility demand sensing and prediction with a fleet of autonomous robotic vehicles in a MoD system. Our Gaussian process (GP)-based decentralized data fusion algorithm can achieve a fine balance between predictive power and time efficiency. We theoretically guarantee its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the GP model: The computation of such a sparse approximate GP model can thus be distributed among the MoD vehicles, hence achieving efficient and scalable demand prediction. Though our decentralized active sensing strategy is devised to gather the most informative demand data for demand prediction, it can achieve a dual effect of fleet rebalancing to service the mobility demands. Empirical evaluation on real-world mobility demand data shows that our proposed algorithm can achieve a better balance between predictive accuracy and time efficiency than state-of-the-art algorithms. version:1
arxiv-1306-0202 | Declarative Modeling and Bayesian Inference of Dark Matter Halos | http://arxiv.org/abs/1306.0202 | id:1306.0202 author:Gabriel Kronberger category:stat.ML astro-ph.IM  published:2013-06-02 summary:Probabilistic programming allows specification of probabilistic models in a declarative manner. Recently, several new software systems and languages for probabilistic programming have been developed on the basis of newly developed and improved methods for approximate inference in probabilistic models. In this contribution a probabilistic model for an idealized dark matter localization problem is described. We first derive the probabilistic model for the inference of dark matter locations and masses, and then show how this model can be implemented using BUGS and Infer.NET, two software systems for probabilistic programming. Finally, the different capabilities of both systems are discussed. The presented dark matter model includes mainly non-conjugate factors, thus, it is difficult to implement this model with Infer.NET. version:1
arxiv-1306-0155 | Dynamic Ad Allocation: Bandits with Budgets | http://arxiv.org/abs/1306.0155 | id:1306.0155 author:Aleksandrs Slivkins category:cs.LG cs.DS  published:2013-06-01 summary:We consider an application of multi-armed bandits to internet advertising (specifically, to dynamic ad allocation in the pay-per-click model, with uncertainty on the click probabilities). We focus on an important practical issue that advertisers are constrained in how much money they can spend on their ad campaigns. This issue has not been considered in the prior work on bandit-based approaches for ad allocation, to the best of our knowledge. We define a simple, stylized model where an algorithm picks one ad to display in each round, and each ad has a \emph{budget}: the maximal amount of money that can be spent on this ad. This model admits a natural variant of UCB1, a well-known algorithm for multi-armed bandits with stochastic rewards. We derive strong provable guarantees for this algorithm. version:1
arxiv-1306-0152 | An Analysis of the Connections Between Layers of Deep Neural Networks | http://arxiv.org/abs/1306.0152 | id:1306.0152 author:Eugenio Culurciello, Jonghoon Jin, Aysegul Dundar, Jordan Bates category:cs.CV  published:2013-06-01 summary:We present an analysis of different techniques for selecting the connection be- tween layers of deep neural networks. Traditional deep neural networks use ran- dom connection tables between layers to keep the number of connections small and tune to different image features. This kind of connection performs adequately in supervised deep networks because their values are refined during the training. On the other hand, in unsupervised learning, one cannot rely on back-propagation techniques to learn the connections between layers. In this work, we tested four different techniques for connecting the first layer of the network to the second layer on the CIFAR and SVHN datasets and showed that the accuracy can be im- proved up to 3% depending on the technique used. We also showed that learning the connections based on the co-occurrences of the features does not confer an advantage over a random connection table in small networks. This work is helpful to improve the efficiency of connections between the layers of unsupervised deep neural networks. version:1
arxiv-1306-0139 | Image Inpainting by Kriging Interpolation Technique | http://arxiv.org/abs/1306.0139 | id:1306.0139 author:Firas A. Jassim category:cs.CV  published:2013-06-01 summary:Image inpainting is the art of predicting damaged regions of an image. The manual way of image inpainting is a time consuming. Therefore, there must be an automatic digital method for image inpainting that recovers the image from the damaged regions. In this paper, a novel statistical image inpainting algorithm based on Kriging interpolation technique was proposed. Kriging technique automatically fills the damaged region in an image using the information available from its surrounding regions in such away that it uses the spatial correlation structure of points inside the k-by-k block. Kriging has the ability to face the challenge of keeping the structure and texture information as the size of damaged region heighten. Experimental results showed that, Kriging has a high PSNR value when recovering a variety of test images from scratches and text as damaged regions. version:1
arxiv-1306-0125 | Understanding ACT-R - an Outsider's Perspective | http://arxiv.org/abs/1306.0125 | id:1306.0125 author:Jacob Whitehill category:cs.LG  published:2013-06-01 summary:The ACT-R theory of cognition developed by John Anderson and colleagues endeavors to explain how humans recall chunks of information and how they solve problems. ACT-R also serves as a theoretical basis for "cognitive tutors", i.e., automatic tutoring systems that help students learn mathematics, computer programming, and other subjects. The official ACT-R definition is distributed across a large body of literature spanning many articles and monographs, and hence it is difficult for an "outsider" to learn the most important aspects of the theory. This paper aims to provide a tutorial to the core components of the ACT-R theory. version:1
arxiv-1303-0309 | One-Class Support Measure Machines for Group Anomaly Detection | http://arxiv.org/abs/1303.0309 | id:1303.0309 author:Krikamol Muandet, Bernhard SchÃ¶lkopf category:stat.ML cs.LG  published:2013-03-01 summary:We propose one-class support measure machines (OCSMMs) for group anomaly detection which aims at recognizing anomalous aggregate behaviors of data points. The OCSMMs generalize well-known one-class support vector machines (OCSVMs) to a space of probability measures. By formulating the problem as quantile estimation on distributions, we can establish an interesting connection to the OCSVMs and variable kernel density estimators (VKDEs) over the input space on which the distributions are defined, bridging the gap between large-margin methods and kernel density estimators. In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications. version:2
arxiv-1306-0090 | Harmony search algorithm for the container storage problem | http://arxiv.org/abs/1306.0090 | id:1306.0090 author:I. Ayachi, R. Kammarti, M. Ksouri, P. Borne, Lagis Ecole Centrale de Lille, Lacs Ecole Nationale category:cs.NE  published:2013-06-01 summary:Recently a new metaheuristic called harmony search was developed. It mimics the behaviors of musicians improvising to find the better state harmony. In this paper, this algorithm is described and applied to solve the container storage problem in the harbor. The objective of this problem is to determine a valid containers arrangement, which meets customers delivery deadlines, reduces the number of container rehandlings and minimizes the ship idle time. In this paper, an adaptation of the harmony search algorithm to the container storage problem is detailed and some experimental results are presented and discussed. The proposed approach was compared to a genetic algorithm previously applied to the same problem and recorded a good results. version:1
arxiv-1302-4387 | Online Learning with Switching Costs and Other Adaptive Adversaries | http://arxiv.org/abs/1302.4387 | id:1302.4387 author:Nicolo Cesa-Bianchi, Ofer Dekel, Ohad Shamir category:cs.LG stat.ML  published:2013-02-18 summary:We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player's performance using a new notion of regret, also known as policy regret, which better captures the adversary's adaptiveness to the player's behavior. In a setting where losses are allowed to drift, we characterize ---in a nearly complete manner--- the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is $\widetilde{\Theta}(T^{2/3})$. Interestingly, this rate is significantly worse than the $\Theta(\sqrt{T})$ rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force $\widetilde{\Theta}(T^{2/3})$ regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies. version:2
arxiv-1306-0040 | Expectation-maximization for logistic regression | http://arxiv.org/abs/1306.0040 | id:1306.0040 author:James G. Scott, Liang Sun category:stat.CO math.ST stat.ML stat.TH  published:2013-05-31 summary:We present a family of expectation-maximization (EM) algorithms for binary and negative-binomial logistic regression, drawing a sharp connection with the variational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our results allow a version of this variational-Bayes approach to be re-interpreted as a true EM algorithm. We study several interesting features of the algorithm, and of this previously unrecognized connection with variational Bayes. We also generalize the approach to sparsity-promoting priors, and to an online method whose convergence properties are easily established. This latter method compares favorably with stochastic-gradient descent in situations with marked collinearity. version:1
arxiv-1302-5348 | Graph-based Generalization Bounds for Learning Binary Relations | http://arxiv.org/abs/1302.5348 | id:1302.5348 author:Ben London, Bert Huang, Lise Getoor category:cs.LG  published:2013-02-21 summary:We investigate the generalizability of learned binary relations: functions that map pairs of instances to a logical indicator. This problem has application in numerous areas of machine learning, such as ranking, entity resolution and link prediction. Our learning framework incorporates an example labeler that, given a sequence $X$ of $n$ instances and a desired training size $m$, subsamples $m$ pairs from $X \times X$ without replacement. The challenge in analyzing this learning scenario is that pairwise combinations of random variables are inherently dependent, which prevents us from using traditional learning-theoretic arguments. We present a unified, graph-based analysis, which allows us to analyze this dependence using well-known graph identities. We are then able to bound the generalization error of learned binary relations using Rademacher complexity and algorithmic stability. The rate of uniform convergence is partially determined by the labeler's subsampling process. We thus examine how various assumptions about subsampling affect generalization; under a natural random subsampling process, our bounds guarantee $\tilde{O}(1/\sqrt{n})$ uniform convergence. version:3
arxiv-1303-1733 | Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss | http://arxiv.org/abs/1303.1733 | id:1303.1733 author:Ben London, Theodoros Rekatsinas, Bert Huang, Lise Getoor category:cs.LG  published:2013-03-07 summary:We propose a modular framework for multi-relational learning via tensor decomposition. In our learning setting, the training data contains multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a function of a linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using quasi-Newton optimization of a weighted objective function. Sparsity in the observed data is captured by the weighted objective, leading to improved accuracy when training data is limited. Exploiting sparsity also improves efficiency, potentially up to an order of magnitude over unweighted approaches. In addition, our framework accommodates arbitrary combinations of smooth, task-specific loss functions, making it better suited for learning different types of relations. For the typical cases of real-valued functions and binary relations, we propose several loss functions and derive the associated parameter gradients. We evaluate our method on synthetic and real data, showing significant improvements in both accuracy and scalability over related factorization techniques. version:2
arxiv-1302-1610 | Adaptive low rank and sparse decomposition of video using compressive sensing | http://arxiv.org/abs/1302.1610 | id:1302.1610 author:Fei Yang, Hong Jiang, Zuowei Shen, Wei Deng, Dimitris Metaxas category:cs.IT cs.CV math.IT  published:2013-02-06 summary:We address the problem of reconstructing and analyzing surveillance videos using compressive sensing. We develop a new method that performs video reconstruction by low rank and sparse decomposition adaptively. Background subtraction becomes part of the reconstruction. In our method, a background model is used in which the background is learned adaptively as the compressive measurements are processed. The adaptive method has low latency, and is more robust than previous methods. We will present experimental results to demonstrate the advantages of the proposed method. version:2
arxiv-1301-6265 | Neural Networks Built from Unreliable Components | http://arxiv.org/abs/1301.6265 | id:1301.6265 author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav Varshney category:cs.NE cs.IT math.IT  published:2013-01-26 summary:Recent advances in associative memory design through strutured pattern sets and graph-based inference algorithms have allowed the reliable learning and retrieval of an exponential number of patterns. Both these and classical associative memories, however, have assumed internally noiseless computational nodes. This paper considers the setting when internal computations are also noisy. Even if all components are noisy, the final error probability in recall can often be made exceedingly small, as we characterize. There is a threshold phenomenon. We also show how to optimize inference algorithm parameters when knowing statistical properties of internal noise. version:4
arxiv-1304-4077 | A new Bayesian ensemble of trees classifier for identifying multi-class labels in satellite images | http://arxiv.org/abs/1304.4077 | id:1304.4077 author:Reshu Agarwal, Pritam Ranjan, Hugh Chipman category:stat.ME cs.CV cs.LG  published:2013-04-15 summary:Classification of satellite images is a key component of many remote sensing applications. One of the most important products of a raw satellite image is the classified map which labels the image pixels into meaningful classes. Though several parametric and non-parametric classifiers have been developed thus far, accurate labeling of the pixels still remains a challenge. In this paper, we propose a new reliable multiclass-classifier for identifying class labels of a satellite image in remote sensing applications. The proposed multiclass-classifier is a generalization of a binary classifier based on the flexible ensemble of regression trees model called Bayesian Additive Regression Trees (BART). We used three small areas from the LANDSAT 5 TM image, acquired on August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) over Kings County, Nova Scotia, Canada to classify the land-use. Several prediction accuracy and uncertainty measures have been used to compare the reliability of the proposed classifier with the state-of-the-art classifiers in remote sensing. version:2
arxiv-1105-0875 | A Risk Comparison of Ordinary Least Squares vs Ridge Regression | http://arxiv.org/abs/1105.0875 | id:1105.0875 author:Paramveer S. Dhillon, Dean P. Foster, Sham M. Kakade, Lyle H. Ungar category:stat.ML  published:2011-05-04 summary:We compare the risk of ridge regression to a simple variant of ordinary least squares, in which one simply projects the data onto a finite dimensional subspace (as specified by a Principal Component Analysis) and then performs an ordinary (un-regularized) least squares regression in this subspace. This note shows that the risk of this ordinary least squares method is within a constant factor (namely 4) of the risk of ridge regression. version:2
arxiv-1305-7476 | Theoretical formulation and analysis of the deterministic dendritic cell algorithm | http://arxiv.org/abs/1305.7476 | id:1305.7476 author:Feng Gu, Julie Greensmith, Uwe Aickelin category:cs.NE cs.DS  published:2013-05-31 summary:As one of the emerging algorithms in the field of Artificial Immune Systems (AIS), the Dendritic Cell Algorithm (DCA) has been successfully applied to a number of challenging real-world problems. However, one criticism is the lack of a formal definition, which could result in ambiguity for understanding the algorithm. Moreover, previous investigations have mainly focused on its empirical aspects. Therefore, it is necessary to provide a formal definition of the algorithm, as well as to perform runtime analyses to revealits theoretical aspects. In this paper, we define the deterministic version of the DCA, named the dDCA, using set theory and mathematical functions. Runtime analyses of the standard algorithm and the one with additional segmentation are performed. Our analysis suggests that the standard dDCA has a runtime complexity of O(n2) for the worst-case scenario, where n is the number of input data instances. The introduction of segmentation changes the algorithm's worst case runtime complexity to O(max(nN; nz)), for DC population size N with size of each segment z. Finally, two runtime variables of the algorithm are formulated based on the input data, to understand its runtime behaviour as guidelines for further development. version:1
arxiv-1305-7465 | Wavelet feature extraction and genetic algorithm for biomarker detection in colorectal cancer data | http://arxiv.org/abs/1305.7465 | id:1305.7465 author:Yihui Liu, Uwe Aickelin, Jan Feyereisl, Lindy G. Durrant category:cs.NE cs.CE  published:2013-05-31 summary:Biomarkers which predict patient's survival can play an important role in medical diagnosis and treatment. How to select the significant biomarkers from hundreds of protein markers is a key step in survival analysis. In this paper a novel method is proposed to detect the prognostic biomarkers of survival in colorectal cancer patients using wavelet analysis, genetic algorithm, and Bayes classifier. One dimensional discrete wavelet transform (DWT) is normally used to reduce the dimensionality of biomedical data. In this study one dimensional continuous wavelet transform (CWT) was proposed to extract the features of colorectal cancer data. One dimensional CWT has no ability to reduce dimensionality of data, but captures the missing features of DWT, and is complementary part of DWT. Genetic algorithm was performed on extracted wavelet coefficients to select the optimized features, using Bayes classifier to build its fitness function. The corresponding protein markers were located based on the position of optimized features. Kaplan-Meier curve and Cox regression model were used to evaluate the performance of selected biomarkers. Experiments were conducted on colorectal cancer dataset and several significant biomarkers were detected. A new protein biomarker CD46 was found to significantly associate with survival time. version:1
arxiv-1305-7454 | Privileged Information for Data Clustering | http://arxiv.org/abs/1305.7454 | id:1305.7454 author:Jan Feyereisl, Uwe Aickelin category:cs.LG stat.ML  published:2013-05-31 summary:Many machine learning algorithms assume that all input samples are independently and identically distributed from some common distribution on either the input space X, in the case of unsupervised learning, or the input and output space X x Y in the case of supervised and semi-supervised learning. In the last number of years the relaxation of this assumption has been explored and the importance of incorporation of additional information within machine learning algorithms became more apparent. Traditionally such fusion of information was the domain of semi-supervised learning. More recently the inclusion of knowledge from separate hypothetical spaces has been proposed by Vapnik as part of the supervised setting. In this work we are interested in exploring Vapnik's idea of master-class learning and the associated learning using privileged information, however within the unsupervised setting. Adoption of the advanced supervised learning paradigm for the unsupervised setting instigates investigation into the difference between privileged and technical data. By means of our proposed aRi-MAX method stability of the KMeans algorithm is improved and identification of the best clustering solution is achieved on an artificial dataset. Subsequently an information theoretic dot product based algorithm called P-Dot is proposed. This method has the ability to utilize a wide variety of clustering techniques, individually or in combination, while fusing privileged and technical data for improved clustering. Application of the P-Dot method to the task of digit recognition confirms our findings in a real-world scenario. version:1
arxiv-1211-6687 | Robustness Analysis of Hottopixx, a Linear Programming Model for Factoring Nonnegative Matrices | http://arxiv.org/abs/1211.6687 | id:1211.6687 author:Nicolas Gillis category:stat.ML cs.LG cs.NA math.OC  published:2012-11-28 summary:Although nonnegative matrix factorization (NMF) is NP-hard in general, it has been shown very recently that it is tractable under the assumption that the input nonnegative data matrix is close to being separable (separability requires that all columns of the input matrix belongs to the cone spanned by a small subset of these columns). Since then, several algorithms have been designed to handle this subclass of NMF problems. In particular, Bittorf, Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming model, referred to as Hottopixx. In this paper, we provide a new and more general robustness analysis of their method. In particular, we design a provably more robust variant using a post-processing strategy which allows us to deal with duplicates and near duplicates in the dataset. version:4
arxiv-1305-7434 | Motif Detection Inspired by Immune Memory (JORS) | http://arxiv.org/abs/1305.7434 | id:1305.7434 author:William Wilson, Phil Birkin, Uwe Aickelin category:cs.NE  published:2013-05-31 summary:The search for patterns or motifs in data represents an area of key interest to many researchers. In this paper we present the Motif Tracking Algorithm, a novel immune inspired pattern identification tool that is able to identify variable length unknown motifs which repeat within time series data. The algorithm searches from a neutral perspective that is independent of the data being analysed and the underlying motifs. In this paper we test the flexibility of the motif tracking algorithm by applying it to the search for patterns in two industrial data sets. The algorithm is able to identify a population of meaningful motifs in both cases, and the value of these motifs is discussed. version:1
arxiv-1305-7432 | Real-world Transfer of Evolved Artificial Immune System Behaviours between Small and Large Scale Robotic Platforms | http://arxiv.org/abs/1305.7432 | id:1305.7432 author:Amanda Whitbrook, Uwe Aickelin, Jonathan M. Garibaldi category:cs.NE cs.RO  published:2013-05-31 summary:In mobile robotics, a solid test for adaptation is the ability of a control system to function not only in a diverse number of physical environments, but also on a number of different robotic platforms. This paper demonstrates that a set of behaviours evolved in simulation on a miniature robot (epuck) can be transferred to a much larger-scale platform (Pioneer), both in simulation and in the real world. The chosen architecture uses artificial evolution of epuck behaviours to obtain a genetic sequence, which is then employed to seed an idiotypic, artificial immune system (AIS) on the Pioneers. Despite numerous hardware and software differences between the platforms, navigation and target-finding experiments show that the evolved behaviours transfer very well to the larger robot when the idiotypic AIS technique is used. In contrast, transferability is poor when reinforcement learning alone is used, which validates the adaptability of the chosen architecture. version:1
arxiv-1305-7416 | The Dendritic Cell Algorithm for Intrusion Detection | http://arxiv.org/abs/1305.7416 | id:1305.7416 author:Feng Gu, Julie Greensmith, Uwe Aickelin category:cs.CR cs.NE  published:2013-05-31 summary:As one of the solutions to intrusion detection problems, Artificial Immune Systems (AIS) have shown their advantages. Unlike genetic algorithms, there is no one archetypal AIS, instead there are four major paradigms. Among them, the Dendritic Cell Algorithm (DCA) has produced promising results in various applications. The aim of this chapter is to demonstrate the potential for the DCA as a suitable candidate for intrusion detection problems. We review some of the commonly used AIS paradigms for intrusion detection problems and demonstrate the advantages of one particular algorithm, the DCA. In order to clearly describe the algorithm, the background to its development and a formal definition are given. In addition, improvements to the original DCA are presented and their implications are discussed, including previous work done on an online analysis component with segmentation and ongoing work on automated data preprocessing. Based on preliminary results, both improvements appear to be promising for online anomaly-based intrusion detection. version:1
arxiv-1305-7344 | Joint Modeling and Registration of Cell Populations in Cohorts of High-Dimensional Flow Cytometric Data | http://arxiv.org/abs/1305.7344 | id:1305.7344 author:Saumyadipta Pyne, Kui Wang, Jonathan Irish, Pablo Tamayo, Marc-Danie Nazaire, Tarn Duong, Sharon Lee, Shu-Kay Ng, David Hafler, Ronald Levy, Garry Nolan, Jill Mesirov, Geoffrey J. McLachlan category:stat.ML  published:2013-05-31 summary:In systems biomedicine, an experimenter encounters different potential sources of variation in data such as individual samples, multiple experimental conditions, and multi-variable network-level responses. In multiparametric cytometry, which is often used for analyzing patient samples, such issues are critical. While computational methods can identify cell populations in individual samples, without the ability to automatically match them across samples, it is difficult to compare and characterize the populations in typical experiments, such as those responding to various stimulations or distinctive of particular patients or time-points, especially when there are many samples. Joint Clustering and Matching (JCM) is a multi-level framework for simultaneous modeling and registration of populations across a cohort. JCM models every population with a robust multivariate probability distribution. Simultaneously, JCM fits a random-effects model to construct an overall batch template -- used for registering populations across samples, and classifying new samples. By tackling systems-level variation, JCM supports practical biomedical applications involving large cohorts. version:1
arxiv-1302-5526 | Stochastic dynamics of lexicon learning in an uncertain and nonuniform world | http://arxiv.org/abs/1302.5526 | id:1302.5526 author:Rainer Reisenauer, Kenny Smith, Richard A. Blythe category:physics.soc-ph cond-mat.stat-mech cs.CL q-bio.NC  published:2013-02-22 summary:We study the time taken by a language learner to correctly identify the meaning of all words in a lexicon under conditions where many plausible meanings can be inferred whenever a word is uttered. We show that the most basic form of cross-situational learning - whereby information from multiple episodes is combined to eliminate incorrect meanings - can perform badly when words are learned independently and meanings are drawn from a nonuniform distribution. If learners further assume that no two words share a common meaning, we find a phase transition between a maximally-efficient learning regime, where the learning time is reduced to the shortest it can possibly be, and a partially-efficient regime where incorrect candidate meanings for words persist at late times. We obtain exact results for the word-learning process through an equivalence to a statistical mechanical problem of enumerating loops in the space of word-meaning mappings. version:2
arxiv-1305-7311 | Robust Hyperspectral Unmixing with Correntropy based Metric | http://arxiv.org/abs/1305.7311 | id:1305.7311 author:Ying Wang, Chunhong Pan, Shiming Xiang, Feiyun Zhu category:cs.CV  published:2013-05-31 summary:Hyperspectral unmixing is one of the crucial steps for many hyperspectral applications. The problem of hyperspectral unmixing has proven to be a difficult task in unsupervised work settings where the endmembers and abundances are both unknown. What is more, this task becomes more challenging in the case that the spectral bands are degraded with noise. This paper presents a robust model for unsupervised hyperspectral unmixing. Specifically, our model is developed with the correntropy based metric where the non-negative constraints on both endmembers and abundances are imposed to keep physical significance. In addition, a sparsity prior is explicitly formulated to constrain the distribution of the abundances of each endmember. To solve our model, a half-quadratic optimization technique is developed to convert the original complex optimization problem into an iteratively re-weighted NMF with sparsity constraints. As a result, the optimization of our model can adaptively assign small weights to noisy bands and give more emphasis on noise-free bands. In addition, with sparsity constraints, our model can naturally generate sparse abundances. Experiments on synthetic and real data demonstrate the effectiveness of our model in comparison to the related state-of-the-art unmixing models. version:1
arxiv-1305-7255 | Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery | http://arxiv.org/abs/1305.7255 | id:1305.7255 author:Dominique Perraul-Joncas, Marina Meila category:stat.ML  published:2013-05-30 summary:In recent years, manifold learning has become increasingly popular as a tool for performing non-linear dimensionality reduction. This has led to the development of numerous algorithms of varying degrees of complexity that aim to recover man ifold geometry using either local or global features of the data. Building on the Laplacian Eigenmap and Diffusionmaps framework, we propose a new paradigm that offers a guarantee, under reasonable assumptions, that any manifo ld learning algorithm will preserve the geometry of a data set. Our approach is based on augmenting the output of embedding algorithms with geometric informatio n embodied in the Riemannian metric of the manifold. We provide an algorithm for estimating the Riemannian metric from data and demonstrate possible application s of our approach in a variety of examples. version:1
arxiv-1302-6105 | Image restoration using sparse approximations of spatially varying blur operators in the wavelet domain | http://arxiv.org/abs/1302.6105 | id:1302.6105 author:Paul Escande, Pierre Weiss, Francois Malgouyres category:math.OC cs.CV math.NA  published:2013-02-25 summary:Restoration of images degraded by spatially varying blurs is an issue of increasing importance in the context of photography, satellite or microscopy imaging. One of the main difficulty to solve this problem comes from the huge dimensions of the blur matrix. It prevents the use of naive approaches for performing matrix-vector multiplications. In this paper, we propose to approximate the blur operator by a matrix sparse in the wavelet domain. We justify this approach from a mathematical point of view and investigate the approximation quality numerically. We finish by showing that the sparsity pattern of the matrix can be pre-defined, which is central in tasks such as blind deconvolution. version:2
arxiv-1207-3994 | Model Selection for Degree-corrected Block Models | http://arxiv.org/abs/1207.3994 | id:1207.3994 author:Xiaoran Yan, Cosma Rohilla Shalizi, Jacob E. Jensen, Florent Krzakala, Cristopher Moore, Lenka Zdeborova, Pan Zhang, Yaojia Zhu category:cs.SI cond-mat.stat-mech math.ST physics.soc-ph stat.ML stat.TH  published:2012-07-17 summary:The proliferation of models for networks raises challenging problems of model selection: the data are sparse and globally dependent, and models are typically high-dimensional and have large numbers of latent variables. Together, these issues mean that the usual model-selection criteria do not work properly for networks. We illustrate these challenges, and show one way to resolve them, by considering the key network-analysis problem of dividing a graph into communities or blocks of nodes with homogeneous patterns of links to the rest of the network. The standard tool for doing this is the stochastic block model, under which the probability of a link between two nodes is a function solely of the blocks to which they belong. This imposes a homogeneous degree distribution within each block; this can be unrealistic, so degree-corrected block models add a parameter for each node, modulating its over-all degree. The choice between ordinary and degree-corrected block models matters because they make very different inferences about communities. We present the first principled and tractable approach to model selection between standard and degree-corrected block models, based on new large-graph asymptotics for the distribution of log-likelihood ratios under the stochastic block model, finding substantial departures from classical results for sparse graphs. We also develop linear-time approximations for log-likelihoods under both the stochastic block model and the degree-corrected model, using belief propagation. Applications to simulated and real networks show excellent agreement with our approximations. Our results thus both solve the practical problem of deciding on degree correction, and point to a general approach to model selection in network analysis. version:2
arxiv-1305-7181 | Lensless Imaging by Compressive Sensing | http://arxiv.org/abs/1305.7181 | id:1305.7181 author:Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford category:cs.CV  published:2013-05-30 summary:In this paper, we propose a lensless compressive imaging architecture. The architecture consists of two components, an aperture assembly and a sensor. No lens is used. The aperture assembly consists of a two dimensional array of aperture elements. The transmittance of each aperture element is independently controllable. The sensor is a single detection element. A compressive sensing matrix is implemented by adjusting the transmittance of the individual aperture elements according to the values of the sensing matrix. The proposed architecture is simple and reliable because no lens is used. The architecture can be used for capturing images of visible and other spectra such as infrared, or millimeter waves, in surveillance applications for detecting anomalies or extracting features such as speed of moving objects. Multiple sensors may be used with a single aperture assembly to capture multi-view images simultaneously. A prototype was built by using a LCD panel and a photoelectric sensor for capturing images of visible spectrum. version:1
arxiv-1305-7169 | Structural and Functional Discovery in Dynamic Networks with Non-negative Matrix Factorization | http://arxiv.org/abs/1305.7169 | id:1305.7169 author:Shawn Mankad, George Michailidis category:cs.SI physics.soc-ph stat.ML  published:2013-05-30 summary:Time series of graphs are increasingly prevalent in modern data and pose unique challenges to visual exploration and pattern extraction. This paper describes the development and application of matrix factorizations for exploration and time-varying community detection in time-evolving graph sequences. The matrix factorization model allows the user to home in on and display interesting, underlying structure and its evolution over time. The methods are scalable to weighted networks with a large number of time points or nodes, and can accommodate sudden changes to graph topology. Our techniques are demonstrated with several dynamic graph series from both synthetic and real world data, including citation and trade networks. These examples illustrate how users can steer the techniques and combine them with existing methods to discover and display meaningful patterns in sizable graphs over many time points. version:1
arxiv-1305-7144 | Immune System Approaches to Intrusion Detection - A Review (ICARIS) | http://arxiv.org/abs/1305.7144 | id:1305.7144 author:Uwe Aickelin, Julie Greensmith, Jamie Twycross category:cs.CR cs.NE  published:2013-05-30 summary:The use of artificial immune systems in intrusion detection is an appealing concept for two reasons. Firstly, the human immune system provides the human body with a high level of protection from invading pathogens, in a robust, self-organised and distributed manner. Secondly, current techniques used in computer security are not able to cope with the dynamic and increasingly complex nature of computer systems and their security. It is hoped that biologically inspired approaches in this area, including the use of immune-based systems will be able to meet this challenge. Here we collate the algorithms used, the development of the systems and the outcome of their implementation. It provides an introduction and review of the key developments within this field, in addition to making suggestions for future research. version:1
arxiv-1305-7130 | Memory Implementations - Current Alternatives | http://arxiv.org/abs/1305.7130 | id:1305.7130 author:William Wilson, Uwe Aickelin category:cs.AI cs.NE  published:2013-05-30 summary:Memory can be defined as the ability to retain and recall information in a diverse range of forms. It is a vital component of the way in which we as human beings operate on a day to day basis. Given a particular situation, decisions are made and actions undertaken in response to that situation based on our memory of related prior events and experiences. By utilising our memory we can anticipate the outcome of our chosen actions to avoid unexpected or unwanted events. In addition, as we subtly alter our actions and recognise altered outcomes we learn and create new memories, enabling us to improve the efficiency of our actions over time. However, as this process occurs so naturally in the subconscious its importance is often overlooked. version:1
arxiv-1305-7111 | Test cost and misclassification cost trade-off using reframing | http://arxiv.org/abs/1305.7111 | id:1305.7111 author:Celestine Periale Ma, JosÃ© HernÃ¡ndez-Orallo category:cs.LG  published:2013-05-30 summary:Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results. version:1
arxiv-1305-7057 | Predicting the Severity of Breast Masses with Data Mining Methods | http://arxiv.org/abs/1305.7057 | id:1305.7057 author:Sahar A. Mokhtar, Alaa. M. Elsayad category:cs.LG stat.ML  published:2013-05-30 summary:Mammography is the most effective and available tool for breast cancer screening. However, the low positive predictive value of breast biopsy resulting from mammogram interpretation leads to approximately 70% unnecessary biopsies with benign outcomes. Data mining algorithms could be used to help physicians in their decisions to perform a breast biopsy on a suspicious lesion seen in a mammogram image or to perform a short term follow-up examination instead. In this research paper data mining classification algorithms; Decision Tree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM) are analyzed on mammographic masses data set. The purpose of this study is to increase the ability of physicians to determine the severity (benign or malignant) of a mammographic mass lesion from BI-RADS attributes and the patient,s age. The whole data set is divided for training the models and test them by the ratio of 70:30% respectively and the performances of classification algorithms are compared through three statistical measures; sensitivity, specificity, and classification accuracy. Accuracy of DT, ANN and SVM are 78.12%, 80.56% and 81.25% of test samples respectively. Our analysis shows that out of these three classification models SVM predicts severity of breast cancer with least error rate and highest accuracy. version:1
arxiv-1305-7056 | Dienstplanerstellung in Krankenhaeusern mittels genetischer Algorithmen | http://arxiv.org/abs/1305.7056 | id:1305.7056 author:Uwe Aickelin category:cs.NE  published:2013-05-30 summary:This thesis investigates the use of problem-specific knowledge to enhance a genetic algorithm approach to multiple-choice optimisation problems. It shows that such information can significantly enhance performance, but that the choice of information and the way it is included are important factors for success. version:1
arxiv-1305-7053 | A Local Active Contour Model for Image Segmentation with Intensity Inhomogeneity | http://arxiv.org/abs/1305.7053 | id:1305.7053 author:Kaihua Zhang, Lei Zhang, Kin-Man Lam, David Zhang category:cs.CV  published:2013-05-30 summary:A novel locally statistical active contour model (ACM) for image segmentation in the presence of intensity inhomogeneity is presented in this paper. The inhomogeneous objects are modeled as Gaussian distributions of different means and variances, and a moving window is used to map the original image into another domain, where the intensity distributions of inhomogeneous objects are still Gaussian but are better separated. The means of the Gaussian distributions in the transformed domain can be adaptively estimated by multiplying a bias field with the original signal within the window. A statistical energy functional is then defined for each local region, which combines the bias field, the level set function, and the constant approximating the true signal of the corresponding object. Experiments on both synthetic and real images demonstrate the superiority of our proposed algorithm to state-of-the-art and representative methods. version:1
arxiv-1305-7014 | Tweets Miner for Stock Market Analysis | http://arxiv.org/abs/1305.7014 | id:1305.7014 author:Bohdan Pavlyshenko category:cs.IR cs.CL cs.SI  published:2013-05-30 summary:In this paper, we present a software package for the data mining of Twitter microblogs for the purpose of using them for the stock market analysis. The package is written in R langauge using apropriate R packages. The model of tweets has been considered. We have also compared stock market charts with frequent sets of keywords in Twitter microblogs messages. version:1
arxiv-1301-3226 | The Expressive Power of Word Embeddings | http://arxiv.org/abs/1301.3226 | id:1301.3226 author:Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena category:cs.LG cs.CL stat.ML  published:2013-01-15 summary:We seek to better understand the difference in quality of the several publicly released embeddings. We propose several tasks that help to distinguish the characteristics of different embeddings. Our evaluation of sentiment polarity and synonym/antonym relations shows that embeddings are able to capture surprisingly nuanced semantics even in the absence of sentence structure. Moreover, benchmarking the embeddings shows great variance in quality and characteristics of the semantics captured by the tested embeddings. Finally, we show the impact of varying the number of dimensions and the resolution of each dimension on the effective useful features captured by the embedding space. Our contributions highlight the importance of embeddings for NLP tasks and the effect of their quality on the final results. version:4
arxiv-1305-6918 | Video Human Segmentation using Fuzzy Object Models and its Application to Body Pose Estimation of Toddlers for Behavior Studies | http://arxiv.org/abs/1305.6918 | id:1305.6918 author:Thiago V. Spina, Mariano Tepper, Amy Esler, Vassilios Morellas, Nikolaos Papanikolopoulos, Alexandre X. FalcÃ£o, Guillermo Sapiro category:cs.CV  published:2013-05-29 summary:Video object segmentation is a challenging problem due to the presence of deformable, connected, and articulated objects, intra- and inter-object occlusions, object motion, and poor lighting. Some of these challenges call for object models that can locate a desired object and separate it from its surrounding background, even when both share similar colors and textures. In this work, we extend a fuzzy object model, named cloud system model (CSM), to handle video segmentation, and evaluate it for body pose estimation of toddlers at risk of autism. CSM has been successfully used to model the parts of the brain (cerebrum, left and right brain hemispheres, and cerebellum) in order to automatically locate and separate them from each other, the connected brain stem, and the background in 3D MR-images. In our case, the objects are articulated parts (2D projections) of the human body, which can deform, cause self-occlusions, and move along the video. The proposed CSM extension handles articulation by connecting the individual clouds, body parts, of the system using a 2D stickman model. The stickman representation naturally allows us to extract 2D body pose measures of arm asymmetry patterns during unsupported gait of toddlers, a possible behavioral marker of autism. The results show that our method can provide insightful knowledge to assist the specialist's observations during real in-clinic assessments. version:1
arxiv-1107-2104 | An estimation of distribution algorithm with adaptive Gibbs sampling for unconstrained global optimization | http://arxiv.org/abs/1107.2104 | id:1107.2104 author:JonÃ¡s Velasco, Mario A. Saucedo-Espinosa, Hugo Jair Escalante, Karlo Mendoza, CÃ©sar Emilio Villarreal-RodrÃ­guez, Ãscar L. ChacÃ³n-MondragÃ³n, AdriÃ¡n RodrÃ­guez, Arturo Berrones category:cs.NE math.OC stat.ML  published:2011-07-11 summary:In this paper is proposed a new heuristic approach belonging to the field of evolutionary Estimation of Distribution Algorithms (EDAs). EDAs builds a probability model and a set of solutions is sampled from the model which characterizes the distribution of such solutions. The main framework of the proposed method is an estimation of distribution algorithm, in which an adaptive Gibbs sampling is used to generate new promising solutions and, in combination with a local search strategy, it improves the individual solutions produced in each iteration. The Estimation of Distribution Algorithm with Adaptive Gibbs Sampling we are proposing in this paper is called AGEDA. We experimentally evaluate and compare this algorithm against two deterministic procedures and several stochastic methods in three well known test problems for unconstrained global optimization. It is empirically shown that our heuristic is robust in problems that involve three central aspects that mainly determine the difficulty of global optimization problems, namely high-dimensionality, multi-modality and non-smoothness. version:2
arxiv-1305-6883 | Rotation invariants of two dimensional curves based on iterated integrals | http://arxiv.org/abs/1305.6883 | id:1305.6883 author:Joscha Diehl category:cs.CV stat.ML  published:2013-05-29 summary:We introduce a novel class of rotation invariants of two dimensional curves based on iterated integrals. The invariants we present are in some sense complete and we describe an algorithm to calculate them, giving explicit computations up to order six. We present an application to online (stroke-trajectory based) character recognition. This seems to be the first time in the literature that the use of iterated integrals of a curve is proposed for (invariant) feature extraction in machine learning applications. version:1
arxiv-1304-5862 | Multi-Label Classifier Chains for Bird Sound | http://arxiv.org/abs/1304.5862 | id:1304.5862 author:Forrest Briggs, Xiaoli Z. Fern, Jed Irvine category:cs.LG cs.SD stat.ML Computer Science  published:2013-04-22 summary:Bird sound data collected with unattended microphones for automatic surveys, or mobile devices for citizen science, typically contain multiple simultaneously vocalizing birds of different species. However, few works have considered the multi-label structure in birdsong. We propose to use an ensemble of classifier chains combined with a histogram-of-segments representation for multi-label classification of birdsong. The proposed method is compared with binary relevance and three multi-instance multi-label learning (MIML) algorithms from prior work (which focus more on structure in the sound, and less on structure in the label sets). Experiments are conducted on two real-world birdsong datasets, and show that the proposed method usually outperforms binary relevance (using the same features and base-classifier), and is better in some cases and worse in others compared to the MIML algorithms. version:2
arxiv-1305-2949 | Unsupervised ensemble of experts (EoE) framework for automatic binarization of document images | http://arxiv.org/abs/1305.2949 | id:1305.2949 author:Reza Farrahi Moghaddam, Fereydoun Farrahi Moghaddam, Mohamed Cheriet category:cs.CV  published:2013-05-13 summary:In recent years, a large number of binarization methods have been developed, with varying performance generalization and strength against different benchmarks. In this work, to leverage on these methods, an ensemble of experts (EoE) framework is introduced, to efficiently combine the outputs of various methods. The proposed framework offers a new selection process of the binarization methods, which are actually the experts in the ensemble, by introducing three concepts: confidentness, endorsement and schools of experts. The framework, which is highly objective, is built based on two general principles: (i) consolidation of saturated opinions and (ii) identification of schools of experts. After building the endorsement graph of the ensemble for an input document image based on the confidentness of the experts, the saturated opinions are consolidated, and then the schools of experts are identified by thresholding the consolidated endorsement graph. A variation of the framework, in which no selection is made, is also introduced that combines the outputs of all experts using endorsement-dependent weights. The EoE framework is evaluated on the set of participating methods in the H-DIBCO'12 contest and also on an ensemble generated from various instances of grid-based Sauvola method with promising performance. version:2
arxiv-1305-6650 | Active Sensing as Bayes-Optimal Sequential Decision Making | http://arxiv.org/abs/1305.6650 | id:1305.6650 author:Sheeraz Ahmad, Angela J. Yu category:cs.AI cs.CV  published:2013-05-28 summary:Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy [Najemnik & Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and effort. We simulate these algorithms on a simple visual search task to illustrate scenarios in which context-sensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the C-DAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate the more complex problem involving peripheral vision, and we notice that the difference between C-DAC and statistical policies becomes even more evident in this scenario. version:1
arxiv-1305-6646 | Normalized Online Learning | http://arxiv.org/abs/1305.6646 | id:1305.6646 author:Stephane Ross, Paul Mineiro, John Langford category:cs.LG stat.ML  published:2013-05-28 summary:We introduce online learning algorithms which are independent of feature scales, proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale. This has several useful effects: there is no need to pre-normalize data, the test-time and test-space complexity are reduced, and the algorithms are more robust. version:1
arxiv-1305-6568 | Reinforcement Learning for the Soccer Dribbling Task | http://arxiv.org/abs/1305.6568 | id:1305.6568 author:Arthur Carvalho, Renato Oliveira category:cs.LG cs.RO stat.ML  published:2013-05-28 summary:We propose a reinforcement learning solution to the \emph{soccer dribbling task}, a scenario in which a soccer agent has to go from the beginning to the end of a region keeping possession of the ball, as an adversary attempts to gain possession. While the adversary uses a stationary policy, the dribbler learns the best action to take at each decision point. After defining meaningful variables to represent the state space, and high-level macro-actions to incorporate domain knowledge, we describe our application of the reinforcement learning algorithm \emph{Sarsa} with CMAC for function approximation. Our experiments show that, after the training period, the dribbler is able to accomplish its task against a strong adversary around 58% of the time. version:1
arxiv-1305-6537 | A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian Network Structures | http://arxiv.org/abs/1305.6537 | id:1305.6537 author:Arthur Carvalho category:cs.NE cs.AI  published:2013-05-28 summary:We propose a cooperative coevolutionary genetic algorithm for learning Bayesian network structures from fully observable data sets. Since this problem can be decomposed into two dependent subproblems, that is to find an ordering of the nodes and an optimal connectivity matrix, our algorithm uses two subpopulations, each one representing a subtask. We describe the empirical results obtained with simulations of the Alarm and Insurance networks. We show that our algorithm outperforms the deterministic algorithm K2. version:1
arxiv-1305-6441 | Matrices of forests, analysis of networks, and ranking problems | http://arxiv.org/abs/1305.6441 | id:1305.6441 author:Pavel Chebotarev, Rafig Agaev category:math.CO cs.CV cs.DM cs.NI  published:2013-05-28 summary:The matrices of spanning rooted forests are studied as a tool for analysing the structure of networks and measuring their properties. The problems of revealing the basic bicomponents, measuring vertex proximity, and ranking from preference relations / sports competitions are considered. It is shown that the vertex accessibility measure based on spanning forests has a number of desirable properties. An interpretation for the stochastic matrix of out-forests in terms of information dissemination is given. version:1
arxiv-1102-4110 | Joint and individual variation explained (JIVE) for integrated analysis of multiple data types | http://arxiv.org/abs/1102.4110 | id:1102.4110 author:Eric F. Lock, Katherine A. Hoadley, J. S. Marron, Andrew B. Nobel category:stat.ML stat.AP stat.ME  published:2011-02-20 summary:Research in several fields now requires the analysis of data sets in which multiple high-dimensional types of data are available for a common set of objects. In particular, The Cancer Genome Atlas (TCGA) includes data from several diverse genomic technologies on the same cancerous tumor samples. In this paper we introduce Joint and Individual Variation Explained (JIVE), a general decomposition of variation for the integrated analysis of such data sets. The decomposition consists of three terms: a low-rank approximation capturing joint variation across data types, low-rank approximations for structured variation individual to each data type, and residual noise. JIVE quantifies the amount of joint variation between data types, reduces the dimensionality of the data and provides new directions for the visual exploration of joint and individual structures. The proposed method represents an extension of Principal Component Analysis and has clear advantages over popular two-block methods such as Canonical Correlation Analysis and Partial Least Squares. A JIVE analysis of gene expression and miRNA data on Glioblastoma Multiforme tumor samples reveals gene-miRNA associations and provides better characterization of tumor types. Data and software are available at https://genome.unc.edu/jive/ version:2
arxiv-1304-1876 | Proceedings of the 37th Annual Workshop of the Austrian Association for Pattern Recognition (ÃAGM/AAPR), 2013 | http://arxiv.org/abs/1304.1876 | id:1304.1876 author:Justus Piater, Antonio RodrÃ­guez-SÃ¡nchez category:cs.CV I.4; I.5; I.2.10  published:2013-04-06 summary:This volume represents the proceedings of the 37th Annual Workshop of the Austrian Association for Pattern Recognition (\"OAGM/AAPR), held May 23-24, 2013, in Innsbruck, Austria. version:3
arxiv-1305-6239 | Optimal rates of convergence for persistence diagrams in Topological Data Analysis | http://arxiv.org/abs/1305.6239 | id:1305.6239 author:FrÃ©dÃ©ric Chazal, Marc Glisse, Catherine LabruÃ¨re, Bertrand Michel category:math.ST cs.CG cs.LG math.GT stat.TH  published:2013-05-27 summary:Computational topology has recently known an important development toward data analysis, giving birth to the field of topological data analysis. Topological persistence, or persistent homology, appears as a fundamental tool in this field. In this paper, we study topological persistence in general metric spaces, with a statistical approach. We show that the use of persistent homology can be naturally considered in general statistical frameworks and persistence diagrams can be used as statistics with interesting convergence properties. Some numerical experiments are performed in various contexts to illustrate our results. version:1
arxiv-1305-6238 | Extended Lambek calculi and first-order linear logic | http://arxiv.org/abs/1305.6238 | id:1305.6238 author:Richard Moot category:cs.CL cs.LO  published:2013-05-27 summary:First-order multiplicative intuitionistic linear logic (MILL1) can be seen as an extension of the Lambek calculus. In addition to the fragment of MILL1 which corresponds to the Lambek calculus (of Moot & Piazza 2001), I will show fragments of MILL1 which generate the multiple context-free languages and which correspond to the Displacement calculus of Morrilll e.a. version:1
arxiv-1212-3214 | Integrating Prior Knowledge Into Prognostic Biomarker Discovery based on Network Structure | http://arxiv.org/abs/1212.3214 | id:1212.3214 author:Yupeng Cun, Holger FrÃ¶hlich category:q-bio.GN stat.ML  published:2012-12-13 summary:Background: Predictive, stable and interpretable gene signatures are generally seen as an important step towards a better personalized medicine. During the last decade various methods have been proposed for that purpose. However, one important obstacle for making gene signatures a standard tool in clinics is the typical low reproducibility of these signatures combined with the difficulty to achieve a clear biological interpretation. For that purpose in the last years there has been a growing interest in approaches that try to integrate information from molecular interaction networks. Results: We propose a novel algorithm, called FrSVM, which integrates protein-protein interaction network information into gene selection for prognostic biomarker discovery. Our method is a simple filter based approach, which focuses on central genes with large differences in their expression. Compared to several other competing methods our algorithm reveals a significantly better prediction performance and higher signature stability. More- over, obtained gene lists are highly enriched with known disease genes and drug targets. We extendd our approach further by integrating information on candidate disease genes and targets of disease associated Transcript Factors (TFs). version:2
arxiv-1305-6215 | On some interrelations of generalized $q$-entropies and a generalized Fisher information, including a CramÃ©r-Rao inequality | http://arxiv.org/abs/1305.6215 | id:1305.6215 author:Jean-FranÃ§ois Bercher category:cs.IT cond-mat.other math.IT stat.ML  published:2013-05-27 summary:In this communication, we describe some interrelations between generalized $q$-entropies and a generalized version of Fisher information. In information theory, the de Bruijn identity links the Fisher information and the derivative of the entropy. We show that this identity can be extended to generalized versions of entropy and Fisher information. More precisely, a generalized Fisher information naturally pops up in the expression of the derivative of the Tsallis entropy. This generalized Fisher information also appears as a special case of a generalized Fisher information for estimation problems. Indeed, we derive here a new Cram\'er-Rao inequality for the estimation of a parameter, which involves a generalized form of Fisher information. This generalized Fisher information reduces to the standard Fisher information as a particular case. In the case of a translation parameter, the general Cram\'er-Rao inequality leads to an inequality for distributions which is saturated by generalized $q$-Gaussian distributions. These generalized $q$-Gaussians are important in several areas of physics and mathematics. They are known to maximize the $q$-entropies subject to a moment constraint. The Cram\'er-Rao inequality shows that the generalized $q$-Gaussians also minimize the generalized Fisher information among distributions with a fixed moment. Similarly, the generalized $q$-Gaussians also minimize the generalized Fisher information among distributions with a given $q$-entropy. version:1
arxiv-1305-6213 | Some results on a $Ï$-divergence, an~extended~Fisher information and~generalized~CramÃ©r-Rao inequalities | http://arxiv.org/abs/1305.6213 | id:1305.6213 author:Jean-FranÃ§ois Bercher category:cs.IT math.IT stat.ML  published:2013-05-27 summary:We propose a modified $\chi^{\beta}$-divergence, give some of its properties, and show that this leads to the definition of a generalized Fisher information. We give generalized Cram\'er-Rao inequalities, involving this Fisher information, an extension of the Fisher information matrix, and arbitrary norms and power of the estimation error. In the case of a location parameter, we obtain new characterizations of the generalized $q$-Gaussians, for instance as the distribution with a given moment that minimizes the generalized Fisher information. Finally we indicate how the generalized Fisher information can lead to new uncertainty relations. version:1
arxiv-1305-6129 | Information-Theoretic Approach to Efficient Adaptive Path Planning for Mobile Robotic Environmental Sensing | http://arxiv.org/abs/1305.6129 | id:1305.6129 author:Kian Hsiang Low, John M. Dolan, Pradeep Khosla category:cs.LG cs.AI cs.MA cs.RO  published:2013-05-27 summary:Recent research in robot exploration and mapping has focused on sampling environmental hotspot fields. This exploration task is formalized by Low, Dolan, and Khosla (2008) in a sequential decision-theoretic planning under uncertainty framework called MASP. The time complexity of solving MASP approximately depends on the map resolution, which limits its use in large-scale, high-resolution exploration and mapping. To alleviate this computational difficulty, this paper presents an information-theoretic approach to MASP (iMASP) for efficient adaptive path planning; by reformulating the cost-minimizing iMASP as a reward-maximizing problem, its time complexity becomes independent of map resolution and is less sensitive to increasing robot team size as demonstrated both theoretically and empirically. Using the reward-maximizing dual, we derive a novel adaptive variant of maximum entropy sampling, thus improving the induced exploration policy performance. It also allows us to establish theoretical bounds quantifying the performance advantage of optimal adaptive over non-adaptive policies and the performance quality of approximately optimal vs. optimal adaptive policies. We show analytically and empirically the superior performance of iMASP-based policies for sampling the log-Gaussian process to that of policies for the widely-used Gaussian process in mapping the hotspot field. Lastly, we provide sufficient conditions that, when met, guarantee adaptivity has no benefit under an assumed environment model. version:1
arxiv-1209-4690 | Regression trees for longitudinal and multiresponse data | http://arxiv.org/abs/1209.4690 | id:1209.4690 author:Wei-Yin Loh, Wei Zheng category:stat.ML stat.AP stat.ME  published:2012-09-21 summary:Previous algorithms for constructing regression tree models for longitudinal and multiresponse data have mostly followed the CART approach. Consequently, they inherit the same selection biases and computational difficulties as CART. We propose an alternative, based on the GUIDE approach, that treats each longitudinal data series as a curve and uses chi-squared tests of the residual curve patterns to select a variable to split each node of the tree. Besides being unbiased, the method is applicable to data with fixed and random time points and with missing values in the response or predictor variables. Simulation results comparing its mean squared prediction error with that of MVPART are given, as well as examples comparing it with standard linear mixed effects and generalized estimating equation models. Conditions for asymptotic consistency of regression tree function estimates are also given. version:2
arxiv-1305-6046 | Supervised Feature Selection for Diagnosis of Coronary Artery Disease Based on Genetic Algorithm | http://arxiv.org/abs/1305.6046 | id:1305.6046 author:Sidahmed Mokeddem, Baghdad Atmani, Mostefa Mokaddem category:cs.LG cs.CE  published:2013-05-26 summary:Feature Selection (FS) has become the focus of much research on decision support systems areas for which data sets with tremendous number of variables are analyzed. In this paper we present a new method for the diagnosis of Coronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes Naive (BN) based FS. Basically, CAD dataset contains two classes defined with 13 features. In GA BN algorithm, GA generates in each iteration a subset of attributes that will be evaluated using the BN in the second step of the selection procedure. The final set of attribute contains the most relevant feature model that increases the accuracy. The algorithm in this case produces 85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the Algorithm is then compared with the use of Support Vector Machine (SVM), MultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of classification accuracy for those algorithms are respectively 83.5%, 83.16% and 80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared with other FS algorithms. The Obtained results have shown very promising outcomes for the diagnosis of CAD. version:1
arxiv-1208-3561 | Efficient Active Learning of Halfspaces: an Aggressive Approach | http://arxiv.org/abs/1208.3561 | id:1208.3561 author:Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz category:cs.LG  published:2012-08-17 summary:We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings. version:3
arxiv-1305-5918 | Reduce Meaningless Words for Joint Chinese Word Segmentation and Part-of-speech Tagging | http://arxiv.org/abs/1305.5918 | id:1305.5918 author:Kaixu Zhang, Maosong Sun category:cs.CL  published:2013-05-25 summary:Conventional statistics-based methods for joint Chinese word segmentation and part-of-speech tagging (S&T) have generalization ability to recognize new words that do not appear in the training data. An undesirable side effect is that a number of meaningless words will be incorrectly created. We propose an effective and efficient framework for S&T that introduces features to significantly reduce meaningless words generation. A general lexicon, Wikepedia and a large-scale raw corpus of 200 billion characters are used to generate word-based features for the wordhood. The word-lattice based framework consists of a character-based model and a word-based model in order to employ our word-based features. Experiments on Penn Chinese treebank 5 show that this method has a 62.9% reduction of meaningless word generation in comparison with the baseline. As a result, the F1 measure for segmentation is increased to 0.984. version:1
arxiv-1305-5905 | ÃAGM/AAPR 2013 - The 37th Annual Workshop of the Austrian Association for Pattern Recognition | http://arxiv.org/abs/1305.5905 | id:1305.5905 author:Justus Piater, Antonio J. RodrÃ­guez SÃ¡nchez category:cs.CV  published:2013-05-25 summary:In this editorial, the organizers summarize facts and background about the event. version:1
arxiv-1305-5829 | A Symmetric Rank-one Quasi Newton Method for Non-negative Matrix Factorization | http://arxiv.org/abs/1305.5829 | id:1305.5829 author:Shu-Zhen Lai, Hou-Biao Li, Zu-Tao Zhang category:math.NA cs.LG cs.NA 15A18  published:2013-05-24 summary:As we all known, the nonnegative matrix factorization (NMF) is a dimension reduction method that has been widely used in image processing, text compressing and signal processing etc. In this paper, an algorithm for nonnegative matrix approximation is proposed. This method mainly bases on the active set and the quasi-Newton type algorithm, by using the symmetric rank-one and negative curvature direction technologies to approximate the Hessian matrix. Our method improves the recent results of those methods in [Pattern Recognition, 45(2012)3557-3565; SIAM J. Sci. Comput., 33(6)(2011)3261-3281; Neural Computation, 19(10)(2007)2756-2779, etc.]. Moreover, the object function decreases faster than many other NMF methods. In addition, some numerical experiments are presented in the synthetic data, imaging processing and text clustering. By comparing with the other six nonnegative matrix approximation methods, our experiments confirm to our analysis. version:1
arxiv-1305-5826 | Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations | http://arxiv.org/abs/1305.5826 | id:1305.5826 author:Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet category:stat.ML cs.DC cs.LG  published:2013-05-24 summary:Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP. version:1
arxiv-1209-5350 | Learning Topic Models and Latent Bayesian Networks Under Expansion Constraints | http://arxiv.org/abs/1209.5350 | id:1209.5350 author:Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade category:stat.ML cs.LG stat.AP  published:2012-09-24 summary:Unsupervised estimation of latent variable models is a fundamental problem central to numerous applications of machine learning and statistics. This work presents a principled approach for estimating broad classes of such models, including probabilistic topic models and latent linear Bayesian networks, using only second-order observed moments. The sufficient conditions for identifiability of these models are primarily based on weak expansion constraints on the topic-word matrix, for topic models, and on the directed acyclic graph, for Bayesian networks. Because no assumptions are made on the distribution among the latent variables, the approach can handle arbitrary correlations among the topics or latent factors. In addition, a tractable learning method via $\ell_1$ optimization is proposed and studied in numerical experiments. version:3
arxiv-1305-5785 | An Inventory of Preposition Relations | http://arxiv.org/abs/1305.5785 | id:1305.5785 author:Vivek Srikumar, Dan Roth category:cs.CL  published:2013-05-24 summary:We describe an inventory of semantic relations that are expressed by prepositions. We define these relations by building on the word sense disambiguation task for prepositions and propose a mapping from preposition senses to the relation labels by collapsing semantically related senses across prepositions. version:1
arxiv-1305-5782 | Adapting the Stochastic Block Model to Edge-Weighted Networks | http://arxiv.org/abs/1305.5782 | id:1305.5782 author:Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.LG cs.SI physics.data-an  published:2013-05-24 summary:We generalize the stochastic block model to the important case in which edges are annotated with weights drawn from an exponential family distribution. This generalization introduces several technical difficulties for model estimation, which we solve using a Bayesian approach. We introduce a variational algorithm that efficiently approximates the model's posterior distribution for dense graphs. In specific numerical experiments on edge-weighted networks, this weighted stochastic block model outperforms the common approach of first applying a single threshold to all weights and then applying the classic stochastic block model, which can obscure latent block structure in networks. This model will enable the recovery of latent structure in a broader range of network data than was previously possible. version:1
arxiv-1305-5756 | Flooding edge or node weighted graphs | http://arxiv.org/abs/1305.5756 | id:1305.5756 author:Fernand Meyer category:cs.CV  published:2013-05-24 summary:Reconstruction closings have all properties of a physical flooding of a topographic surface. They are precious for simplifying gradient images or, filling unwanted catchment basins, on which a subsequent watershed transform extracts the targeted objects. Flooding a topographic surface may be modeled as flooding a node weighted graph (TG), with unweighted edges, the node weights representing the ground level. The progression of a flooding may also be modeled on the region adjacency graph (RAG) of a topographic surface. On a RAG each node represents a catchment basin and edges connect neighboring nodes. The edges are weighted by the altitude of the pass point between both adjacent regions. The graph is flooded from sources placed at the marker positions and each node is assigned to the source by which it has been flooded. The level of the flood is represented on the nodes on each type of graphs. The same flooding may thus be modeled on a TG or on a RAG. We characterize all valid floodings on both types of graphs, as they should verify the laws of hydrostatics. We then show that each flooding of a node weighted graph also is a flooding of an edge weighted graph with appropriate edge weights. The highest flooding under a ceiling function may be interpreted as the shortest distance to the root for the ultrametric flooding distance in an augmented graph. The ultrametric distance between two nodes is the minimal altitude of a flooding for which both nodes are flooded. This remark permits to flood edge or node weighted graphs by using shortest path algorithms. It appears that the collection of all lakes of a RAG has the structure of a dendrogram, on which the highest flooding under a ceiling function may be rapidly found. version:1
arxiv-1305-5734 | Characterizing A Database of Sequential Behaviors with Latent Dirichlet Hidden Markov Models | http://arxiv.org/abs/1305.5734 | id:1305.5734 author:Yin Song, Longbing Cao, Xuhui Fan, Wei Cao, Jian Zhang category:stat.ML cs.LG H.2.8; F.1.2  published:2013-05-24 summary:This paper proposes a generative model, the latent Dirichlet hidden Markov models (LDHMM), for characterizing a database of sequential behaviors (sequences). LDHMMs posit that each sequence is generated by an underlying Markov chain process, which are controlled by the corresponding parameters (i.e., the initial state vector, transition matrix and the emission matrix). These sequence-level latent parameters for each sequence are modeled as latent Dirichlet random variables and parameterized by a set of deterministic database-level hyper-parameters. Through this way, we expect to model the sequence in two levels: the database level by deterministic hyper-parameters and the sequence-level by latent parameters. To learn the deterministic hyper-parameters and approximate posteriors of parameters in LDHMMs, we propose an iterative algorithm under the variational EM framework, which consists of E and M steps. We examine two different schemes, the fully-factorized and partially-factorized forms, for the framework, based on different assumptions. We present empirical results of behavior modeling and sequence classification on three real-world data sets, and compare them to other related models. The experimental results prove that the proposed LDHMMs produce better generalization performance in terms of log-likelihood and deliver competitive results on the sequence classification problem. version:1
arxiv-1305-5728 | Edge Detection in Radar Images Using Weibull Distribution | http://arxiv.org/abs/1305.5728 | id:1305.5728 author:Ali El-Zaart, Wafaa Kamel Al-Jibory category:cs.CV  published:2013-05-24 summary:Radar images can reveal information about the shape of the surface terrain as well as its physical and biophysical properties. Radar images have long been used in geological studies to map structural features that are revealed by the shape of the landscape. Radar imagery also has applications in vegetation and crop type mapping, landscape ecology, hydrology, and volcanology. Image processing is using for detecting for objects in radar images. Edge detection; which is a method of determining the discontinuities in gray level images; is a very important initial step in Image processing. Many classical edge detectors have been developed over time. Some of the well-known edge detection operators based on the first derivative of the image are Roberts, Prewitt, Sobel which is traditionally implemented by convolving the image with masks. Also Gaussian distribution has been used to build masks for the first and second derivative. However, this distribution has limit to only symmetric shape. This paper will use to construct the masks, the Weibull distribution which was more general than Gaussian because it has symmetric and asymmetric shape. The constructed masks are applied to images and we obtained good results. version:1
arxiv-1204-1685 | Density-sensitive semisupervised inference | http://arxiv.org/abs/1204.1685 | id:1204.1685 author:Martin Azizyan, Aarti Singh, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH  published:2012-04-07 summary:Semisupervised methods are techniques for using labeled data $(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$ to make predictions. These methods invoke some assumptions that link the marginal distribution $P_X$ of X to the regression function f(x). For example, it is common to assume that f is very smooth over high density regions of $P_X$. Many of the methods are ad-hoc and have been shown to work in specific examples but are lacking a theoretical foundation. We provide a minimax framework for analyzing semisupervised methods. In particular, we study methods based on metrics that are sensitive to the distribution $P_X$. Our model includes a parameter $\alpha$ that controls the strength of the semisupervised assumption. We then use the data to adapt to $\alpha$. version:2
arxiv-1110-6084 | The multi-armed bandit problem with covariates | http://arxiv.org/abs/1110.6084 | id:1110.6084 author:Vianney Perchet, Philippe Rigollet category:math.ST cs.LG stat.ML stat.TH  published:2011-10-27 summary:We consider a multi-armed bandit problem in a setting where each arm produces a noisy reward realization which depends on an observable random covariate. As opposed to the traditional static multi-armed bandit problem, this setting allows for dynamically changing rewards that better describe applications where side information is available. We adopt a nonparametric model where the expected rewards are smooth functions of the covariate and where the hardness of the problem is captured by a margin parameter. To maximize the expected cumulative reward, we introduce a policy called Adaptively Binned Successive Elimination (abse) that adaptively decomposes the global problem into suitably "localized" static bandit problems. This policy constructs an adaptive partition using a variant of the Successive Elimination (se) policy. Our results include sharper regret bounds for the se policy in a static bandit problem and minimax optimal regret bounds for the abse policy in the dynamic problem. version:3
arxiv-1305-5663 | Applications of Clifford's Geometric Algebra | http://arxiv.org/abs/1305.5663 | id:1305.5663 author:Eckhard Hitzer, Tohru Nitta, Yasuaki Kuroe category:math.RA cs.CV  published:2013-05-24 summary:We survey the development of Clifford's geometric algebra and some of its engineering applications during the last 15 years. Several recently developed applications and their merits are discussed in some detail. We thus hope to clearly demonstrate the benefit of developing problem solutions in a unified framework for algebra and geometry with the widest possible scope: from quantum computing and electromagnetism to satellite navigation, from neural computing to camera geometry, image processing, robotics and beyond. version:1
arxiv-1302-6421 | ML4PG in Computer Algebra verification | http://arxiv.org/abs/1302.6421 | id:1302.6421 author:JÃ³nathan Heras, Ekaterina Komendantskaya category:cs.LO cs.LG  published:2013-02-26 summary:ML4PG is a machine-learning extension that provides statistical proof hints during the process of Coq/SSReflect proof development. In this paper, we use ML4PG to find proof patterns in the CoqEAL library -- a library that was devised to verify the correctness of Computer Algebra algorithms. In particular, we use ML4PG to help us in the formalisation of an efficient algorithm to compute the inverse of triangular matrices. version:3
arxiv-1303-1420 | Verifying a platform for digital imaging: a multi-tool strategy | http://arxiv.org/abs/1303.1420 | id:1303.1420 author:JÃ³nathan Heras, Gadea Mata, Ana Romero, Julio Rubio, RubÃ©n SÃ¡enz category:cs.SE cs.CV  published:2013-03-05 summary:Fiji is a Java platform widely used by biologists and other experimental scientists to process digital images. In particular, in our research - made together with a biologists team; we use Fiji in some pre-processing steps before undertaking a homological digital processing of images. In a previous work, we have formalised the correctness of the programs which use homological techniques to analyse digital images. However, the verification of Fiji's pre-processing step was missed. In this paper, we present a multi-tool approach filling this gap, based on the combination of Why/Krakatoa, Coq and ACL2. version:2
arxiv-1108-4973 | Learning from Complex Systems: On the Roles of Entropy and Fisher Information in Pairwise Isotropic Gaussian Markov Random Fields | http://arxiv.org/abs/1108.4973 | id:1108.4973 author:Alexandre L. M. Levada category:cs.IT cs.AI cs.CV math.IT stat.CO  published:2011-08-25 summary:Markov Random Field models are powerful tools for the study of complex systems. However, little is known about how the interactions between the elements of such systems are encoded, especially from an information-theoretic perspective. In this paper, our goal is to enlight the connection between Fisher information, Shannon entropy, information geometry and the behavior of complex systems modeled by isotropic pairwise Gaussian Markov random fields. We propose analytical expressions to compute local and global versions of these measures using Besag's pseudo-likelihood function, characterizing the system's behavior through its \emph{Fisher curve}, a parametric trajectory accross the information space that provides a geometric representation for the study of complex systems. Computational experiments show how the proposed tools can be useful in extrating relevant information from complex patterns. The obtained results quantify and support our main conclusion, which is: in terms of information, moving towards higher entropy states (A --> B) is different from moving towards lower entropy states (B --> A), since the \emph{Fisher curves} are not the same given a natural orientation (the direction of time). version:12
arxiv-1104-2930 | Cluster Forests | http://arxiv.org/abs/1104.2930 | id:1104.2930 author:Donghui Yan, Aiyou Chen, Michael I. Jordan category:stat.ME cs.LG stat.ML  published:2011-04-14 summary:With inspiration from Random Forests (RF) in the context of classification, a new clustering ensemble method---Cluster Forests (CF) is proposed. Geometrically, CF randomly probes a high-dimensional data cloud to obtain "good local clusterings" and then aggregates via spectral clustering to obtain cluster assignments for the whole dataset. The search for good local clusterings is guided by a cluster quality measure kappa. CF progressively improves each local clustering in a fashion that resembles the tree growth in RF. Empirical studies on several real-world datasets under two different performance metrics show that CF compares favorably to its competitors. Theoretical analysis reveals that the kappa measure makes it possible to grow the local clustering in a desirable way---it is "noise-resistant". A closed-form expression is obtained for the mis-clustering rate of spectral clustering under a perturbation model, which yields new insights into some aspects of spectral clustering. version:3
arxiv-1305-5399 | A Primal Condition for Approachability with Partial Monitoring | http://arxiv.org/abs/1305.5399 | id:1305.5399 author:Shie Mannor, Vianney Perchet, Gilles Stoltz category:math.OC cs.GT cs.LG stat.ML  published:2013-05-23 summary:In approachability with full monitoring there are two types of conditions that are known to be equivalent for convex sets: a primal and a dual condition. The primal one is of the form: a set C is approachable if and only all containing half-spaces are approachable in the one-shot game; while the dual one is of the form: a convex set C is approachable if and only if it intersects all payoff sets of a certain form. We consider approachability in games with partial monitoring. In previous works (Perchet 2011; Mannor et al. 2011) we provided a dual characterization of approachable convex sets; we also exhibited efficient strategies in the case where C is a polytope. In this paper we provide primal conditions on a convex set to be approachable with partial monitoring. They depend on a modified reward function and lead to approachability strategies, based on modified payoff functions, that proceed by projections similarly to Blackwell's (1956) strategy; this is in contrast with previously studied strategies in this context that relied mostly on the signaling structure and aimed at estimating well the distributions of the signals received. Our results generalize classical results by Kohlberg 1975 (see also Mertens et al. 1994) and apply to games with arbitrary signaling structure as well as to arbitrary convex sets. version:1
arxiv-1304-7359 | Constant conditional entropy and related hypotheses | http://arxiv.org/abs/1304.7359 | id:1304.7359 author:Ramon Ferrer-i-Cancho, Åukasz DÄbowski, FermÃ­n Moscoso del Prado MartÃ­n category:cond-mat.stat-mech cs.CL cs.IT math.IT physics.data-an  published:2013-04-27 summary:Constant entropy rate (conditional entropies must remain constant as the sequence length increases) and uniform information density (conditional probabilities must remain constant as the sequence length increases) are two information theoretic principles that are argued to underlie a wide range of linguistic phenomena. Here we revise the predictions of these principles to the light of Hilberg's law on the scaling of conditional entropy in language and related laws. We show that constant entropy rate (CER) and two interpretations for uniform information density (UID), full UID and strong UID, are inconsistent with these laws. Strong UID implies CER but the reverse is not true. Full UID, a particular case of UID, leads to costly uncorrelated sequences that are totally unrealistic. We conclude that CER and its particular cases are incomplete hypotheses about the scaling of conditional entropies. version:2
arxiv-1305-5306 | A Supervised Neural Autoregressive Topic Model for Simultaneous Image Classification and Annotation | http://arxiv.org/abs/1305.5306 | id:1305.5306 author:Yin Zheng, Yu-Jin Zhang, Hugo Larochelle category:cs.CV cs.LG stat.ML  published:2013-05-23 summary:Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to perform scene recognition and annotation. Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for document modeling. In this work, we show how to successfully apply and extend this model to the context of visual scene modeling. Specifically, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the hidden topic features by incorporating label information into the training objective of the model. We also describe how to leverage information about the spatial position of the visual words and how to embed additional image annotations, so as to simultaneously perform image classification and annotation. We test our model on the Scene15, LabelMe and UIUC-Sports datasets and show that it compares favorably to other topic models such as the supervised variant of LDA. version:1
arxiv-1305-5160 | A novel automatic thresholding segmentation method with local adaptive thresholds | http://arxiv.org/abs/1305.5160 | id:1305.5160 author:Bo Xiao, Yuefeng Jing, Yonghong Guan category:cs.CV  published:2013-05-22 summary:A novel method for segmenting bright objects from dark background for grayscale image is proposed. The concept of this method can be stated simply as: to pick out the local-thinnest bands on the grayscale grade-map. It turns out to be a threshold-based method with local adaptive thresholds, where each local threshold is determined by requiring the average normal-direction gradient on the object boundary to be local minimal. The method is highly automatic and the segmentation mimics a man's natural expectation even the object boundaries are fuzzy. version:1
arxiv-1208-2015 | Sharp analysis of low-rank kernel matrix approximations | http://arxiv.org/abs/1208.2015 | id:1208.2015 author:Francis Bach category:cs.LG math.ST stat.TH  published:2012-08-09 summary:We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations n, i.e., O(n^2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of running time complexities to O(p^2 n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this paper, we show that in the context of kernel ridge regression, for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms, for any given problem instance, and not only for worst-case situations. version:3
arxiv-1305-5078 | A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings | http://arxiv.org/abs/1305.5078 | id:1305.5078 author:Alicja A. Wieczorkowska, Miron B. Kursa category:cs.LG cs.IR cs.SD  published:2013-05-22 summary:In this paper, we first apply random ferns for classification of real music recordings of a jazz band. No initial segmentation of audio data is assumed, i.e., no onset, offset, nor pitch data are needed. The notion of random ferns is described in the paper, to familiarize the reader with this classification algorithm, which was introduced quite recently and applied so far in image recognition tasks. The performance of random ferns is compared with random forests for the same data. The results of experiments are presented in the paper, and conclusions are drawn. version:1
arxiv-1305-3794 | Evolution of Covariance Functions for Gaussian Process Regression using Genetic Programming | http://arxiv.org/abs/1305.3794 | id:1305.3794 author:Gabriel Kronberger, Michael Kommenda category:cs.NE cs.LG stat.ML  published:2013-05-16 summary:In this contribution we describe an approach to evolve composite covariance functions for Gaussian processes using genetic programming. A critical aspect of Gaussian processes and similar kernel-based models such as SVM is, that the covariance function should be adapted to the modeled data. Frequently, the squared exponential covariance function is used as a default. However, this can lead to a misspecified model, which does not fit the data well. In the proposed approach we use a grammar for the composition of covariance functions and genetic programming to search over the space of sentences that can be derived from the grammar. We tested the proposed approach on synthetic data from two-dimensional test functions, and on the Mauna Loa CO2 time series. The results show, that our approach is feasible, finding covariance functions that perform much better than a default covariance function. For the CO2 data set a composite covariance function is found, that matches the performance of a hand-tuned covariance function. version:2
arxiv-1305-5017 | PAWL-Forced Simulated Tempering | http://arxiv.org/abs/1305.5017 | id:1305.5017 author:Luke Bornn category:stat.CO stat.ML  published:2013-05-22 summary:In this short note, we show how the parallel adaptive Wang-Landau (PAWL) algorithm of Bornn et al. (2013) can be used to automate and improve simulated tempering algorithms. While Wang-Landau and other stochastic approximation methods have frequently been applied within the simulated tempering framework, this note demonstrates through a simple example the additional improvements brought about by parallelization, adaptive proposals and automated bin splitting. version:1
arxiv-1305-4947 | Improving NSGA-II with an Adaptive Mutation Operator | http://arxiv.org/abs/1305.4947 | id:1305.4947 author:Arthur Carvalho, Aluizio F. R. Araujo category:cs.NE  published:2013-05-21 summary:The performance of a Multiobjective Evolutionary Algorithm (MOEA) is crucially dependent on the parameter setting of the operators. The most desired control of such parameters presents the characteristic of adaptiveness, i.e., the capacity of changing the value of the parameter, in distinct stages of the evolutionary process, using feedbacks from the search for determining the direction and/or magnitude of changing. Given the great popularity of the algorithm NSGA-II, the objective of this research is to create adaptive controls for each parameter existing in this MOEA. With these controls, we expect to improve even more the performance of the algorithm. In this work, we propose an adaptive mutation operator that has an adaptive control which uses information about the diversity of candidate solutions for controlling the magnitude of the mutation. A number of experiments considering different problems suggest that this mutation operator improves the ability of the NSGA-II for reaching the Pareto optimal Front and for getting a better diversity among the final solutions. version:1
arxiv-1305-4893 | Out-of-sample Extension for Latent Position Graphs | http://arxiv.org/abs/1305.4893 | id:1305.4893 author:Minh Tang, Youngser Park, Carey E. Priebe category:stat.ML  published:2013-05-21 summary:We consider the problem of vertex classification for graphs constructed from the latent position model. It was shown previously that the approach of embedding the graphs into some Euclidean space followed by classification in that space can yields a universally consistent vertex classifier. However, a major technical difficulty of the approach arises when classifying unlabeled out-of-sample vertices without including them in the embedding stage. In this paper, we studied the out-of-sample extension for the graph embedding step and its impact on the subsequent inference tasks. We show that, under the latent position graph model and for sufficiently large $n$, the mapping of the out-of-sample vertices is close to its true latent position. We then demonstrate that successful inference for the out-of-sample vertices is possible. version:1
