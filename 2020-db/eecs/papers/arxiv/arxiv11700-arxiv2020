arxiv-1508-07266 | Understanding Editing Behaviors in Multilingual Wikipedia | http://arxiv.org/abs/1508.07266 | id:1508.07266 author:Suin Kim, Sungjoon Park, Scott A. Hale, Sooyoung Kim, Jeongmin Byun, Alice Oh category:cs.SI cs.CL cs.CY  published:2015-08-28 summary:Multilingualism is common offline, but we have a more limited understanding of the ways multilingualism is displayed online and the roles that multilinguals play in the spread of content between speakers of different languages. We take a computational approach to studying multilingualism using one of the largest user-generated content platforms, Wikipedia. We study multilingualism by collecting and analyzing a large dataset of the content written by multilingual editors of the English, German, and Spanish editions of Wikipedia. This dataset contains over two million paragraphs edited by over 15,000 multilingual users from July 8 to August 9, 2013. We analyze these multilingual editors in terms of their engagement, interests, and language proficiency in their primary and non-primary (secondary) languages and find that the English edition of Wikipedia displays different dynamics from the Spanish and German editions. Users primarily editing the Spanish and German editions make more complex edits than users who edit these editions as a second language. In contrast, users editing the English edition as a second language make edits that are just as complex as the edits by users who primarily edit the English edition. In this way, English serves a special role bringing together content written by multilinguals from many language editions. Nonetheless, language remains a formidable hurdle to the spread of content: we find evidence for a complexity barrier whereby editors are less likely to edit complex content in a second language. In addition, we find that multilinguals are less engaged and show lower levels of language proficiency in their second languages. We also examine the topical interests of multilingual editors and find that there is no significant difference between primary and non-primary editors in each language. version:1
arxiv-1508-07243 | Bilevel parameter learning for higher-order total variation regularisation models | http://arxiv.org/abs/1508.07243 | id:1508.07243 author:J. C. De los Reyes, C. -B. Schönlieb, T. Valkonen category:math.OC cs.CV  published:2015-08-28 summary:We consider a bilevel optimisation approach for parameter learning in higher-order total variation image reconstruction models. Apart from the least squares cost functional, naturally used in bilevel learning, we propose and analyse an alternative cost, based on a Huber regularised TV-seminorm. Differentiability properties of the solution operator are verified and a first-order optimality system is derived. Based on the adjoint information, a quasi-Newton algorithm is proposed for the numerical solution of the bilevel problems. Numerical experiments are carried out to show the suitability of our approach and the improved performance of the new cost functional. Thanks to the bilevel optimisation framework, also a detailed comparison between TGV$^2$ and ICTV is carried out, showing the advantages and shortcomings of both regularisers, depending on the structure of the processed images and their noise level. version:1
arxiv-1507-03194 | A Review of Nonnegative Matrix Factorization Methods for Clustering | http://arxiv.org/abs/1507.03194 | id:1507.03194 author:Ali Caner Türkmen category:stat.ML cs.LG cs.NA  published:2015-07-12 summary:Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank matrix approximation technique, and has enjoyed a wide area of applications. Although NMF does not seem related to the clustering problem at first, it was shown that they are closely linked. In this report, we provide a gentle introduction to clustering and NMF before reviewing the theoretical relationship between them. We then explore several NMF variants, namely Sparse NMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along with their clustering interpretations. version:2
arxiv-1508-07148 | Discrete Hashing with Deep Neural Network | http://arxiv.org/abs/1508.07148 | id:1508.07148 author:Thanh-Toan Do, Anh-Zung Doan, Ngai-Man Cheung category:cs.CV  published:2015-08-28 summary:This paper addresses the problem of learning binary hash codes for large scale image search by proposing a novel hashing method based on deep neural network. The advantage of our deep model over previous deep model used in hashing is that our model contains necessary criteria for producing good codes such as similarity preserving, balance and independence. Another advantage of our method is that instead of relaxing the binary constraint of codes during the learning process as most previous works, in this paper, by introducing the auxiliary variable, we reformulate the optimization into two sub-optimization steps allowing us to efficiently solve binary constraints without any relaxation. The proposed method is also extended to the supervised hashing by leveraging the label information such that the learned binary codes preserve the pairwise label of inputs. The experimental results on three benchmark datasets show the proposed methods outperform state-of-the-art hashing methods. version:1
arxiv-1409-8403 | Evaluation of Output Embeddings for Fine-Grained Image Classification | http://arxiv.org/abs/1409.8403 | id:1409.8403 author:Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, Bernt Schiele category:cs.CV  published:2014-09-30 summary:Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results. version:2
arxiv-1508-02593 | Type-Constrained Representation Learning in Knowledge Graphs | http://arxiv.org/abs/1508.02593 | id:1508.02593 author:Denis Krompaß, Stephan Baier, Volker Tresp category:cs.AI cs.LG  published:2015-08-11 summary:Large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics, as in search or question answering systems. Latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs, showing promising results in tasks related to knowledge graph completion and cleaning. Besides storing facts about the world, schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships. In this work, we study how type-constraints can generally support the statistical modeling with latent variable models. More precisely, we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches. Our experimental results show that prior knowledge on relation-types significantly improves these models up to 77% in link-prediction tasks. The achieved improvements are especially prominent when a low model complexity is enforced, a crucial requirement when these models are applied to very large datasets. Unfortunately, type-constraints are neither always available nor always complete e.g., they can become fuzzy when entities lack proper typing. We show that in these cases, it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data. version:2
arxiv-1508-07130 | Parallel Dither and Dropout for Regularising Deep Neural Networks | http://arxiv.org/abs/1508.07130 | id:1508.07130 author:Andrew J. R. Simpson category:cs.LG cs.NE 68Txx  published:2015-08-28 summary:Effective regularisation during training can mean the difference between success and failure for deep neural networks. Recently, dither has been suggested as alternative to dropout for regularisation during batch-averaged stochastic gradient descent (SGD). In this article, we show that these methods fail without batch averaging and we introduce a new, parallel regularisation method that may be used without batch averaging. Our results for parallel-regularised non-batch-SGD are substantially better than what is possible with batch-SGD. Furthermore, our results demonstrate that dither and dropout are complimentary. version:1
arxiv-1508-07103 | Regularized Kernel Recursive Least Square Algoirthm | http://arxiv.org/abs/1508.07103 | id:1508.07103 author:Songlin Zhao category:cs.LG stat.ML  published:2015-08-28 summary:In most adaptive signal processing applications, system linearity is assumed and adaptive linear filters are thus used. The traditional class of supervised adaptive filters rely on error-correction learning for their adaptive capability. The kernel method is a powerful nonparametric modeling tool for pattern analysis and statistical signal processing. Through a nonlinear mapping, kernel methods transform the data into a set of points in a Reproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast convergence rate in stationary scenario. However the good performance is obtained at a cost of high computation complexity. Sparsification in kernel methods is know to related to less computational complexity and memory consumption. version:1
arxiv-1508-07096 | Partitioning Large Scale Deep Belief Networks Using Dropout | http://arxiv.org/abs/1508.07096 | id:1508.07096 author:Yanping Huang, Sai Zhang category:stat.ML cs.LG cs.NE  published:2015-08-28 summary:Deep learning methods have shown great promise in many practical applications, ranging from speech recognition, visual object recognition, to text processing. However, most of the current deep learning methods suffer from scalability problems for large-scale applications, forcing researchers or users to focus on small-scale problems with fewer parameters. In this paper, we consider a well-known machine learning model, deep belief networks (DBNs) that have yielded impressive classification performance on a large number of benchmark machine learning tasks. To scale up DBN, we propose an approach that can use the computing clusters in a distributed environment to train large models, while the dense matrix computations within a single machine are sped up using graphics processors (GPU). When training a DBN, each machine randomly drops out a portion of neurons in each hidden layer, for each training case, making the remaining neurons only learn to detect features that are generally helpful for producing the correct answer. Within our approach, we have developed four methods to combine outcomes from each machine to form a unified model. Our preliminary experiment on the mnst handwritten digit database demonstrates that our approach outperforms the state of the art test error rate. version:1
arxiv-1507-06527 | Deep Recurrent Q-Learning for Partially Observable MDPs | http://arxiv.org/abs/1507.06527 | id:1507.06527 author:Matthew Hausknecht, Peter Stone category:cs.LG  published:2015-07-23 summary:Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes. version:3
arxiv-1412-0696 | Understanding confounding effects in linguistic coordination: an information-theoretic approach | http://arxiv.org/abs/1412.0696 | id:1412.0696 author:Shuyang Gao, Greg Ver Steeg, Aram Galstyan category:cs.CL cs.IT cs.SI math.IT physics.data-an  published:2014-12-01 summary:We suggest an information-theoretic approach for measuring stylistic coordination in dialogues. The proposed measure has a simple predictive interpretation and can account for various confounding factors through proper conditioning. We revisit some of the previous studies that reported strong signatures of stylistic accommodation, and find that a significant part of the observed coordination can be attributed to a simple confounding effect - length coordination. Specifically, longer utterances tend to be followed by longer responses, which gives rise to spurious correlations in the other stylistic features. We propose a test to distinguish correlations in length due to contextual factors (topic of conversation, user verbosity, etc.) and turn-by-turn coordination. We also suggest a test to identify whether stylistic coordination persists even after accounting for length coordination and contextual factors. version:2
arxiv-1508-06936 | Validation of neural spike sorting algorithms without ground-truth information | http://arxiv.org/abs/1508.06936 | id:1508.06936 author:Alex H. Barnett, Jeremy F. Magland, Leslie F. Greengard category:q-bio.NC cs.CV 94A12  92C55  62M10  published:2015-08-27 summary:We describe a suite of validation metrics that assess the credibility of a given automatic spike sorting algorithm applied to a given electrophysiological recording, when ground-truth is unavailable. By rerunning the spike sorter two or more times, the metrics measure stability under various perturbations consistent with variations in the data itself, making no assumptions about the noise model, nor about the internal workings of the sorting algorithm. Such stability is a prerequisite for reproducibility of results. We illustrate the metrics on standard sorting algorithms for both in vivo and ex vivo recordings. We believe that such metrics could reduce the significant human labor currently spent on validation, and should form an essential part of large-scale automated spike sorting and systematic benchmarking of algorithms. version:1
arxiv-1508-06901 | Compressive Sensing via Low-Rank Gaussian Mixture Models | http://arxiv.org/abs/1508.06901 | id:1508.06901 author:Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford category:stat.ML cs.LG stat.AP  published:2015-08-27 summary:We develop a new compressive sensing (CS) inversion algorithm by utilizing the Gaussian mixture model (GMM). While the compressive sensing is performed globally on the entire image as implemented in our lensless camera, a low-rank GMM is imposed on the local image patches. This low-rank GMM is derived via eigenvalue thresholding of the GMM trained on the projection of the measurement data, thus learned {\em in situ}. The GMM and the projection of the measurement data are updated iteratively during the reconstruction. Our GMM algorithm degrades to the piecewise linear estimator (PLE) if each patch is represented by a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also developed for CS inversion, constituting an additional contribution of this paper. Extensive results on both simulation data and real data captured by the lensless camera demonstrate the efficacy of the proposed algorithm. Furthermore, we compare the CS reconstruction results using our algorithm with the JPEG compression. Simulation results demonstrate that when limited bandwidth is available (a small number of measurements), our algorithm can achieve comparable results as JPEG. version:1
arxiv-1508-06853 | Shopper Analytics: a customer activity recognition system using a distributed RGB-D camera network | http://arxiv.org/abs/1508.06853 | id:1508.06853 author:Daniele Liciotti, Marco Contigiani, Emanuele Frontoni, Adriano Mancini, Primo Zingaretti, Valerio Placidi category:cs.CV  published:2015-08-27 summary:The aim of this paper is to present an integrated system consisted of a RGB-D camera and a software able to monitor shoppers in intelligent retail environments. We propose an innovative low cost smart system that can understand the shoppers' behavior and, in particular, their interactions with the products in the shelves, with the aim to develop an automatic RGB-D technique for video analysis. The system of cameras detects the presence of people and univocally identifies them. Through the depth frames, the system detects the interactions of the shoppers with the products on the shelf and determines if a product is picked up or if the product is taken and then put back and finally, if there is not contact with the products. The system is low cost and easy to install, and experimental results demonstrated that its performances are satisfactory also in real environments. version:1
arxiv-1508-06845 | Encrypted statistical machine learning: new privacy preserving methods | http://arxiv.org/abs/1508.06845 | id:1508.06845 author:Louis J. M. Aslett, Pedro M. Esperança, Chris C. Holmes category:stat.ML cs.CR cs.LG stat.ME  published:2015-08-27 summary:We present two new statistical machine learning methods designed to learn on fully homomorphic encrypted (FHE) data. The introduction of FHE schemes following Gentry (2009) opens up the prospect of privacy preserving statistical machine learning analysis and modelling of encrypted data without compromising security constraints. We propose tailored algorithms for applying extremely random forests, involving a new cryptographic stochastic fraction estimator, and na\"{i}ve Bayes, involving a semi-parametric model for the class decision boundary, and show how they can be used to learn and predict from encrypted data. We demonstrate that these techniques perform competitively on a variety of classification data sets and provide detailed information about the computational practicalities of these and other FHE methods. version:1
arxiv-1508-06802 | Introducing Elitist Black-Box Models: When Does Elitist Selection Weaken the Performance of Evolutionary Algorithms? | http://arxiv.org/abs/1508.06802 | id:1508.06802 author:Carola Doerr, Johannes Lengler category:cs.NE cs.DS  published:2015-08-27 summary:Black-box complexity theory provides lower bounds for the runtime of black-box optimizers like evolutionary algorithms and serves as an inspiration for the design of new genetic algorithms. Several black-box models covering different classes of algorithms exist, each highlighting a different aspect of the algorithms under considerations. In this work we add to the existing black-box notions a new \emph{elitist black-box model}, in which algorithms are required to base all decisions solely on (a fixed number of) the best search points sampled so far. Our model combines features of the ranking-based and the memory-restricted black-box models with elitist selection. We provide several examples for which the elitist black-box complexity is exponentially larger than that the respective complexities in all previous black-box models, thus showing that the elitist black-box complexity can be much closer to the runtime of typical evolutionary algorithms. We also introduce the concept of $p$-Monte Carlo black-box complexity, which measures the time it takes to optimize a problem with failure probability at most $p$. Even for small~$p$, the $p$-Monte Carlo black-box complexity of a function class $\mathcal F$ can be smaller by an exponential factor than its typically regarded Las Vegas complexity (which measures the \emph{expected} time it takes to optimize $\mathcal F$). version:1
arxiv-1508-06446 | Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric Admixture Modeling | http://arxiv.org/abs/1508.06446 | id:1508.06446 author:Lavanya Sita Tekumalla, Priyanka Agrawal, Indrajit Bhattacharya category:stat.ML cs.LG  published:2015-08-26 summary:Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixture modeling, where the number of mixture components grows with the number of data items. The Hierarchical Dirichlet Process (HDP), is an extension of DP for grouped data, often used for non-parametric topic modeling, where each group is a mixture over shared mixture densities. The Nested Dirichlet Process (nDP), on the other hand, is an extension of the DP for learning group level distributions from data, simultaneously clustering the groups. It allows group level distributions to be shared across groups in a non-parametric setting, leading to a non-parametric mixture of mixtures. The nCRF extends the nDP for multilevel non-parametric mixture modeling, enabling modeling topic hierarchies. However, the nDP and nCRF do not allow sharing of distributions as required in many applications, motivating the need for multi-level non-parametric admixture modeling. We address this gap by proposing multi-level nested HDPs (nHDP) where the base distribution of the HDP is itself a HDP at each level thereby leading to admixtures of admixtures at each level. Because of couplings between various HDP levels, scaling up is naturally a challenge during inference. We propose a multi-level nested Chinese Restaurant Franchise (nCRF) representation for the nested HDP, with which we outline an inference algorithm based on Gibbs Sampling. We evaluate our model with the two level nHDP for non-parametric entity topic modeling where an inner HDP creates a countably infinite set of topic mixtures and associates them with author entities, while an outer HDP associates documents with these author entities. In our experiments on two real world research corpora, the nHDP is able to generalize significantly better than existing models and detect missing author entities with a reasonable level of accuracy. version:2
arxiv-1512-00035 | A biologically constrained model of the whole basal ganglia addressing the paradoxes of connections and selection | http://arxiv.org/abs/1512.00035 | id:1512.00035 author:Jean Liénard, Benoît Girard category:q-bio.NC cs.NE cs.RO  published:2015-08-27 summary:The basal ganglia nuclei form a complex network of nuclei often assumed to perform selection, yet their individual roles and how they influence each other is still largely unclear. In particular, the ties between the external and internal parts of the globus pallidus are paradoxical, as anatomical data suggest a potent inhibitory projection between them while electrophys-iological recordings indicate that they have similar activities. Here we introduce a theoretical study that reconciles both views on the intra-pallidal projection, by providing a plausible characterization of the relationship between the external and internal globus pallidus. Specifically, we developed a mean-field model of the whole basal ganglia, whose parameterization is optimized to respect best a collection of numerous anatomical and electrophysiological data. We first obtained models respecting all our constraints, hence anatomical and electrophysiological data on the intrapallidal projection are globally consistent. This model furthermore predicts that both aforementioned views about the intra-pallidal projection may be reconciled when this projection is weakly inhibitory, thus making it possible to support similar neural activity in both nuclei and for the entire basal ganglia to select between actions. Second, we predicts that afferent projections are substantially unbalanced towards the external segment, as it receives the strongest excitation from STN and the weakest inhibition from the striatum. Finally, our study strongly suggest that the intrapallidal connection pattern is not focused but diffuse, as this latter pattern is more efficient for the overall selection performed in the basal ganglia. version:1
arxiv-1508-06374 | A fully data-driven method to identify (correlated) changes in diachronic corpora | http://arxiv.org/abs/1508.06374 | id:1508.06374 author:Alexander Koplenig category:cs.CL cs.IR stat.AP  published:2015-08-26 summary:In this paper, a method for measuring synchronic corpus (dis-)similarity put forward by Kilgarriff (2001) is adapted and extended to identify trends and correlated changes in diachronic text data, using the Corpus of Historical American English (Davies 2010a) and the Google Ngram Corpora (Michel et al. 2010a). This paper shows that this fully data-driven method, which extracts word types that have undergone the most pronounced change in frequency in a given period of time, is computationally very cheap and that it allows interpretations of diachronic trends that are both intuitively plausible and motivated from the perspective of information theory. Furthermore, it demonstrates that the method is able to identify correlated linguistic changes and diachronic shifts that can be linked to historical events. Finally, it can help to improve diachronic POS tagging and complement existing NLP approaches. This indicates that the approach can facilitate an improved understanding of diachronic processes in language change. version:2
arxiv-1508-06728 | A Comparative Analysis of Retrieval Techniques In Content Based Image Retrieval | http://arxiv.org/abs/1508.06728 | id:1508.06728 author:Mohini P. Sardey, G. K. Kharate category:cs.CV 68  published:2015-08-27 summary:Basic group of visual techniques such as color, shape, texture are used in Content Based Image Retrievals (CBIR) to retrieve query image or subregion of image to find similar images in image database. To improve query result, relevance feedback is used many times in CBIR to help user to express their preference and improve query results.In this paper, a new approach for image retrieval is proposed which is based on the features such as Color Histogram, Eigen Values and Match Point. Images from various types of database are first identified by using edge detection techniques.Once the image is identified, then the image is searched in the particular database, then all related images are displayed. This will save the retrieval time. Further to retrieve the precise query image, any of the three techniques are used and comparison is done w.r.t. average retrieval time. Eigen value technique found to be the best as compared with other two techniques version:1
arxiv-1508-06725 | Image Type Water Meter Character Recognition Based on Embedded DSP | http://arxiv.org/abs/1508.06725 | id:1508.06725 author:Ying Liu, Yan-bin Han, Yu-lin Zhang category:cs.CV  published:2015-08-27 summary:In the paper, we combined DSP processor with image processing algorithm and studied the method of water meter character recognition. We collected water meter image through camera at a fixed angle, and the projection method is used to recognize those digital images. The experiment results show that the method can recognize the meter characters accurately and artificial meter reading is replaced by automatic digital recognition, which improves working efficiency. version:1
arxiv-1508-06717 | Online Anomaly Detection via Class-Imbalance Learning | http://arxiv.org/abs/1508.06717 | id:1508.06717 author:Chandresh Kumar Maurya, Durga Toshniwal, Gopalan Vijendran Venkoparao category:cs.LG  published:2015-08-27 summary:Anomaly detection is an important task in many real world applications such as fraud detection, suspicious activity detection, health care monitoring etc. In this paper, we tackle this problem from supervised learning perspective in online learning setting. We maximize well known \emph{Gmean} metric for class-imbalance learning in online learning framework. Specifically, we show that maximizing \emph{Gmean} is equivalent to minimizing a convex surrogate loss function and based on that we propose novel online learning algorithm for anomaly detection. We then show, by extensive experiments, that the performance of the proposed algorithm with respect to $sum$ metric is as good as a recently proposed Cost-Sensitive Online Classification(CSOC) algorithm for class-imbalance learning over various benchmarked data sets while keeping running time close to the perception algorithm. Our another conclusion is that other competitive online algorithms do not perform consistently over data sets of varying size. This shows the potential applicability of our proposed approach. version:1
arxiv-1508-06708 | Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation | http://arxiv.org/abs/1508.06708 | id:1508.06708 author:Sijin Li, Weichen Zhang, Antoni B. Chan category:cs.CV  published:2015-08-27 summary:This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks. We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration. version:1
arxiv-1508-06705 | Using Genetic Algorithms to Benchmark the Cloud | http://arxiv.org/abs/1508.06705 | id:1508.06705 author:Jeff Kinnison, Sekou L. Remy category:cs.DC cs.NE cs.PF  published:2015-08-27 summary:This paper presents a novel application of Genetic Algorithms(GAs) to quantify the performance of Platform as a Service (PaaS), a cloud service model that plays a critical role in both industry and academia. While Cloud benchmarks are not new, in this novel concept, the authors use a GA to take advantage of the elasticity in Cloud services in a graceful manner that was not previously possible. Using Google App Engine, Heroku, and Python Anywhere with three distinct classes of client computers running our GA codebase, we quantified the completion time for application of the GA to search for the parameters of controllers for dynamical systems. Our results show statistically significant differences in PaaS performance by vendor, and also that the performance of the PaaS performance is dependent upon the client that uses it. Results also show the effectiveness of our GA in determining the level of service of PaaS providers, and for determining if the level of service of one PaaS vendor is repeatable with another. Such a concept could then increase the appeal of PaaS Cloud services by making them more financially appealing. version:1
arxiv-1508-06669 | Component-Enhanced Chinese Character Embeddings | http://arxiv.org/abs/1508.06669 | id:1508.06669 author:Yanran Li, Wenjie Li, Fei Sun, Sujian Li category:cs.CL  published:2015-08-26 summary:Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. version:1
arxiv-1506-03487 | From Paraphrase Database to Compositional Paraphrase Model and Back | http://arxiv.org/abs/1506.03487 | id:1506.03487 author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, Dan Roth category:cs.CL  published:2015-06-10 summary:The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates. However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage. We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB's internal scores while simultaneously improving its coverage. They allow for learning phrase embeddings as well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks. version:2
arxiv-1505-07765 | Automatic Relevance Determination For Deep Generative Models | http://arxiv.org/abs/1505.07765 | id:1505.07765 author:Theofanis Karaletsos, Gunnar Rätsch category:stat.ML  published:2015-05-28 summary:A recurring problem when building probabilistic latent variable models is regularization and model selection, for instance, the choice of the dimensionality of the latent space. In the context of belief networks with latent variables, this problem has been adressed with Automatic Relevance Determination (ARD) employing Monte Carlo inference. We present a variational inference approach to ARD for Deep Generative Models using doubly stochastic variational inference to provide fast and scalable learning. We show empirical results on a standard dataset illustrating the effects of contracting the latent space automatically. We show that the resulting latent representations are significantly more compact without loss of expressive power of the learned models. version:3
arxiv-1508-06586 | Financial Market Modeling with Quantum Neural Networks | http://arxiv.org/abs/1508.06586 | id:1508.06586 author:Carlos Pedro Gonçalves category:q-fin.CP cs.NE physics.soc-ph q-fin.GN  published:2015-08-26 summary:Econophysics has developed as a research field that applies the formalism of Statistical Mechanics and Quantum Mechanics to address Economics and Finance problems. The branch of Econophysics that applies of Quantum Theory to Economics and Finance is called Quantum Econophysics. In Finance, Quantum Econophysics' contributions have ranged from option pricing to market dynamics modeling, behavioral finance and applications of Game Theory, integrating the empirical finding, from human decision analysis, that shows that nonlinear update rules in probabilities, leading to non-additive decision weights, can be computationally approached from quantum computation, with resulting quantum interference terms explaining the non-additive probabilities. The current work draws on these results to introduce new tools from Quantum Artificial Intelligence, namely Quantum Artificial Neural Networks as a way to build and simulate financial market models with adaptive selection of trading rules, leading to turbulence and excess kurtosis in the returns distributions for a wide range of parameters. version:1
arxiv-1508-01745 | Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems | http://arxiv.org/abs/1508.01745 | id:1508.01745 author:Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL  published:2015-08-07 summary:Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems. version:2
arxiv-1508-06574 | A review of homomorphic encryption and software tools for encrypted statistical machine learning | http://arxiv.org/abs/1508.06574 | id:1508.06574 author:Louis J. M. Aslett, Pedro M. Esperança, Chris C. Holmes category:stat.ML cs.CR cs.LG  published:2015-08-26 summary:Recent advances in cryptography promise to enable secure statistical computation on encrypted data, whereby a limited set of operations can be carried out without the need to first decrypt. We review these homomorphic encryption schemes in a manner accessible to statisticians and machine learners, focusing on pertinent limitations inherent in the current state of the art. These limitations restrict the kind of statistics and machine learning algorithms which can be implemented and we review those which have been successfully applied in the literature. Finally, we document a high performance R package implementing a recent homomorphic scheme in a general framework. version:1
arxiv-1508-06535 | Deep Convolutional Neural Networks for Smile Recognition | http://arxiv.org/abs/1508.06535 | id:1508.06535 author:Patrick O. Glauner category:cs.CV cs.LG cs.NE  published:2015-08-26 summary:This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared. version:1
arxiv-1508-06491 | Alignment-based compositional semantics for instruction following | http://arxiv.org/abs/1508.06491 | id:1508.06491 author:Jacob Andreas, Dan Klein category:cs.CL  published:2015-08-26 summary:This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the model's flexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results. version:1
arxiv-1502-01199 | A Multiple-Expert Binarization Framework for Multispectral Images | http://arxiv.org/abs/1502.01199 | id:1502.01199 author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV  published:2015-02-04 summary:In this work, a multiple-expert binarization framework for multispectral images is proposed. The framework is based on a constrained subspace selection limited to the spectral bands combined with state-of-the-art gray-level binarization methods. The framework uses a binarization wrapper to enhance the performance of the gray-level binarization. Nonlinear preprocessing of the individual spectral bands is used to enhance the textual information. An evolutionary optimizer is considered to obtain the optimal and some suboptimal 3-band subspaces from which an ensemble of experts is then formed. The framework is applied to a ground truth multispectral dataset with promising results. In addition, a generalization to the cross-validation approach is developed that not only evaluates generalizability of the framework, it also provides a practical instance of the selected experts that could be then applied to unseen inputs despite the small size of the given ground truth dataset. version:6
arxiv-1508-06483 | Population Synthesis via k-Nearest Neighbor Crossover Kernel | http://arxiv.org/abs/1508.06483 | id:1508.06483 author:Naoki Hamada, Katsumi Homma, Hiroyuki Higuchi, Hideyuki Kikuchi category:cs.NE  published:2015-08-26 summary:The recent development of multi-agent simulations brings about a need for population synthesis. It is a task of reconstructing the entire population from a sampling survey of limited size (1% or so), supplying the initial conditions from which simulations begin. This paper presents a new kernel density estimator for this task. Our method is an analogue of the classical Breiman-Meisel-Purcell estimator, but employs novel techniques that harness the huge degree of freedom which is required to model high-dimensional nonlinearly correlated datasets: the crossover kernel, the k-nearest neighbor restriction of the kernel construction set and the bagging of kernels. The performance as a statistical estimator is examined through real and synthetic datasets. We provide an "optimization-free" parameter selection rule for our method, a theory of how our method works and a computational cost analysis. To demonstrate the usefulness as a population synthesizer, our method is applied to a household synthesis task for an urban micro-simulator. version:1
arxiv-1508-06477 | Greedy methods, randomization approaches and multi-arm bandit algorithms for efficient sparsity-constrained optimization | http://arxiv.org/abs/1508.06477 | id:1508.06477 author:A. Rakotomamonjy, S. Koço, L. Ralaivola category:cs.LG  published:2015-08-26 summary:Several sparsity-constrained algorithms such as Orthogonal Matching Pursuit or the Frank-Wolfe algorithm with sparsity constraints work by iteratively selecting a novel atom to add to the current non-zero set of variables. This selection step is usually performed by computing the gradient and then by looking for the gradient component with maximal absolute entry. This step can be computationally expensive especially for large-scale and high-dimensional data. In this work, we aim at accelerating these sparsity-constrained optimization algorithms by exploiting the key observation that, for these algorithms to work, one only needs the coordinate of the gradient's top entry. Hence, we introduce algorithms based on greedy methods and randomization approaches that aim at cheaply estimating the gradient and its top entry. Another of our contribution is to cast the problem of finding the best gradient entry as a best arm identification in a multi-armed bandit problem. Owing to this novel insight, we are able to provide a bandit-based algorithm that directly estimates the top entry in a very efficient way. Theoretical results stating that the resulting inexact Frank-Wolfe or Orthogonal Matching Pursuit algorithms act, with high probability, similarly to their exact versions are also given. We have carried out several experiments showing that the greedy deterministic and the bandit approaches we propose can achieve an acceleration of an order of magnitude while being as efficient as the exact gradient when used in algorithms such as version:1
arxiv-1508-04826 | Dither is Better than Dropout for Regularising Deep Neural Networks | http://arxiv.org/abs/1508.04826 | id:1508.04826 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-08-19 summary:Regularisation of deep neural networks (DNN) during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a more effective regulariser. version:2
arxiv-1508-06451 | Crossings as a side effect of dependency lengths | http://arxiv.org/abs/1508.06451 | id:1508.06451 author:Ramon Ferrer-i-Cancho, Carlos Gómez-Rodríguez category:cs.CL cs.SI physics.soc-ph  published:2015-08-26 summary:The syntactic structure of sentences exhibits a striking regularity: dependencies tend to not cross when drawn above the sentence. Here we investigate two competing hypotheses for the origins of non-crossing dependencies. The traditional hypothesis is that the low frequency of dependency crossings arises from an independent principle of syntax that reduces crossings practically to zero. An alternative to this view is the hypothesis that crossings are a side effect of dependency lengths. According to this view, sentences with shorter dependency lengths should tend to have fewer crossings. We recast the traditional view as a null hypothesis where one of the variables, i.e. the number of crossings, is mean independent of the other, i.e. the sum of dependency lengths. The alternative view is then a positive correlation between these two variables. In spite of the rough estimation of dependency crossings that this sum provides, we are able to reject the traditional view in the majority of languages considered. The alternative hypothesis can lead to a more parsimonious theory of syntax. version:1
arxiv-1508-06388 | Gaussian Mixture Models with Component Means Constrained in Pre-selected Subspaces | http://arxiv.org/abs/1508.06388 | id:1508.06388 author:Mu Qiao, Jia Li category:stat.ML cs.LG  published:2015-08-26 summary:We investigate a Gaussian mixture model (GMM) with component means constrained in a pre-selected subspace. Applications to classification and clustering are explored. An EM-type estimation algorithm is derived. We prove that the subspace containing the component means of a GMM with a common covariance matrix also contains the modes of the density and the class means. This motivates us to find a subspace by applying weighted principal component analysis to the modes of a kernel density and the class means. To circumvent the difficulty of deciding the kernel bandwidth, we acquire multiple subspaces from the kernel densities based on a sequence of bandwidths. The GMM constrained by each subspace is estimated; and the model yielding the maximum likelihood is chosen. A dimension reduction property is proved in the sense of being informative for classification or clustering. Experiments on real and simulated data sets are conducted to examine several ways of determining the subspace and to compare with the reduced rank mixture discriminant analysis (MDA). Our new method with the simple technique of spanning the subspace only by class means often outperforms the reduced rank MDA when the subspace dimension is very low, making it particularly appealing for visualization. version:1
arxiv-1508-06336 | SPRIGHT: A Fast and Robust Framework for Sparse Walsh-Hadamard Transform | http://arxiv.org/abs/1508.06336 | id:1508.06336 author:Xiao Li, Joseph K. Bradley, Sameer Pawar, Kannan Ramchandran category:cs.IT cs.LG math.IT  published:2015-08-26 summary:We consider the problem of computing the Walsh-Hadamard Transform (WHT) of some $N$-length input vector in the presence of noise, where the $N$-point Walsh spectrum is $K$-sparse with $K = {O}(N^{\delta})$ scaling sub-linearly in the input dimension $N$ for some $0<\delta<1$. Over the past decade, there has been a resurgence in research related to the computation of Discrete Fourier Transform (DFT) for some length-$N$ input signal that has a $K$-sparse Fourier spectrum. In particular, through a sparse-graph code design, our earlier work on the Fast Fourier Aliasing-based Sparse Transform (FFAST) algorithm computes the $K$-sparse DFT in time ${O}(K\log K)$ by taking ${O}(K)$ noiseless samples. Inspired by the coding-theoretic design framework, Scheibler et al. proposed the Sparse Fast Hadamard Transform (SparseFHT) algorithm that elegantly computes the $K$-sparse WHT in the absence of noise using ${O}(K\log N)$ samples in time ${O}(K\log^2 N)$. However, the SparseFHT algorithm explicitly exploits the noiseless nature of the problem, and is not equipped to deal with scenarios where the observations are corrupted by noise. Therefore, a question of critical interest is whether this coding-theoretic framework can be made robust to noise. Further, if the answer is yes, what is the extra price that needs to be paid for being robust to noise? In this paper, we show, quite interestingly, that there is {\it no extra price} that needs to be paid for being robust to noise other than a constant factor. In other words, we can maintain the same sample complexity ${O}(K\log N)$ and the computational complexity ${O}(K\log^2 N)$ as those of the noiseless case, using our SParse Robust Iterative Graph-based Hadamard Transform (SPRIGHT) algorithm. version:1
arxiv-1311-2542 | Toward a unified theory of sparse dimensionality reduction in Euclidean space | http://arxiv.org/abs/1311.2542 | id:1311.2542 author:Jean Bourgain, Sjoerd Dirksen, Jelani Nelson category:cs.DS cs.CG cs.IT math.IT math.PR stat.ML  published:2013-11-11 summary:Let $\Phi\in\mathbb{R}^{m\times n}$ be a sparse Johnson-Lindenstrauss transform [KN14] with $s$ non-zeroes per column. For a subset $T$ of the unit sphere, $\varepsilon\in(0,1/2)$ given, we study settings for $m,s$ required to ensure $$ \mathop{\mathbb{E}}_\Phi \sup_{x\in T} \left \ \Phi x\ _2^2 - 1 \right < \varepsilon , $$ i.e. so that $\Phi$ preserves the norm of every $x\in T$ simultaneously and multiplicatively up to $1+\varepsilon$. We introduce a new complexity parameter, which depends on the geometry of $T$, and show that it suffices to choose $s$ and $m$ such that this parameter is small. Our result is a sparse analog of Gordon's theorem, which was concerned with a dense $\Phi$ having i.i.d. Gaussian entries. We qualitatively unify several results related to the Johnson-Lindenstrauss lemma, subspace embeddings, and Fourier-based restricted isometries. Our work also implies new results in using the sparse Johnson-Lindenstrauss transform in numerical linear algebra, classical and model-based compressed sensing, manifold learning, and constrained least squares problems such as the Lasso. version:4
arxiv-1508-06264 | Multiple kernel multivariate performance learning using cutting plane algorithm | http://arxiv.org/abs/1508.06264 | id:1508.06264 author:Jingbin Wang, Haoxiang Wang, Yihua Zhou, Nancy McDonald category:cs.LG cs.CV  published:2015-08-25 summary:In this paper, we propose a multi-kernel classifier learning algorithm to optimize a given nonlinear and nonsmoonth multivariate classifier performance measure. Moreover, to solve the problem of kernel function selection and kernel parameter tuning, we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels. The learning of the classifier parameter and the kernel weight are unified in a single objective function considering to minimize the upper boundary of the given multivariate performance measure. The objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm by using cutting plane algorithm. The developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems. The experiment results show the proposed algorithm outperforms the competing methods. version:1
arxiv-1406-5986 | A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares | http://arxiv.org/abs/1406.5986 | id:1406.5986 author:Garvesh Raskutti, Michael Mahoney category:stat.ML  published:2014-06-23 summary:We consider statistical as well as algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. For a LS problem with input data $(X, Y) \in \mathbb{R}^{n \times p} \times \mathbb{R}^n$, sketching algorithms use a sketching matrix, $S\in\mathbb{R}^{r \times n}$ with $r \ll n$. Then, rather than solving the LS problem using the full data $(X,Y)$, sketching algorithms solve the LS problem using only the sketched data $(SX, SY)$. Prior work has typically adopted an algorithmic perspective, in that it has made no statistical assumptions on the input $X$ and $Y$, and instead it has been assumed that the data $(X,Y)$ are fixed and worst-case (WC). Prior results show that, when using sketching matrices such as random projections and leverage-score sampling algorithms, with $p < r \ll n$, the WC error is the same as solving the original problem, up to a small constant. From a statistical perspective, we typically consider the mean-squared error performance of randomized sketching algorithms, when data $(X, Y)$ are generated according to a statistical model $Y = X \beta + \epsilon$, where $\epsilon$ is a noise process. We provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling sketching algorithms. Among other results, we show that the RE can be upper bounded when $p < r \ll n$ while the PE typically requires the sample size $r$ to be substantially larger. Lower bounds developed in subsequent results show that our upper bounds on PE can not be improved. version:2
arxiv-1509-01199 | Inferring Passenger Type from Commuter Eigentravel Matrices | http://arxiv.org/abs/1509.01199 | id:1509.01199 author:Erika Fille Legara, Christopher Monterola category:physics.soc-ph cs.CY physics.data-an stat.AP stat.ML  published:2015-08-25 summary:A sufficient knowledge of the demographics of a commuting public is essential in formulating and implementing more targeted transportation policies, as commuters exhibit different ways of traveling. With the advent of the Automated Fare Collection system (AFC), probing the travel patterns of commuters has become less invasive and more accessible. Consequently, numerous transport studies related to human mobility have shown that these observed patterns allow one to pair individuals with locations and/or activities at certain times of the day. However, classifying commuters using their travel signatures is yet to be thoroughly examined. Here, we contribute to the literature by demonstrating a procedure to characterize passenger types (Adult, Child/Student, and Senior Citizen) based on their three-month travel patterns taken from a smart fare card system. We first establish a method to construct distinct commuter matrices, which we refer to as eigentravel matrices, that capture the characteristic travel routines of individuals. From the eigentravel matrices, we build classification models that predict the type of passengers traveling. Among the models explored, the gradient boosting method (GBM) gives the best prediction accuracy at 76%, which is 84% better than the minimum model accuracy (41%) required vis-\`a-vis the proportional chance criterion. In addition, we find that travel features generated during weekdays have greater predictive power than those on weekends. This work should not only be useful for transport planners, but for market researchers as well. With the awareness of which commuter types are traveling, ads, service announcements, and surveys, among others, can be made more targeted spatiotemporally. Finally, our framework should be effective in creating synthetic populations for use in real-world simulations that involve a metropolitan's public transport system. version:1
arxiv-1505-02890 | Sparse 3D convolutional neural networks | http://arxiv.org/abs/1505.02890 | id:1505.02890 author:Ben Graham category:cs.CV  published:2015-05-12 summary:We have implemented a convolutional neural network designed for processing sparse three-dimensional input data. The world we live in is three dimensional so there are a large number of potential applications including 3D object recognition and analysis of space-time objects. In the quest for efficiency, we experiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice. version:2
arxiv-1508-06171 | BREN: Body Reflection Essence-Neuter Model for Separation of Reflection Components | http://arxiv.org/abs/1508.06171 | id:1508.06171 author:Changsoo Je, Hyung-Min Park category:cs.CV cs.GR physics.optics  published:2015-08-25 summary:We propose a novel reflection color model consisting of body essence and (mixed) neuter, and present an effective method for separating dichromatic reflection components using a single image. Body essence is an entity invariant to interface reflection, and has two degrees of freedom unlike hue and maximum chromaticity. As a result, the proposed method is insensitive to noise and proper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red, green, and blue), contrary to the maximum chromaticity-based methods. Interface reflection is separated by using a Gaussian function, which removes a critical thresholding problem. Furthermore, the method does not require any region segmentation. Experimental results show the efficacy of the proposed model and method. version:1
arxiv-1508-06161 | Robot Language Learning, Generation, and Comprehension | http://arxiv.org/abs/1508.06161 | id:1508.06161 author:Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, Jeffrey Mark Siskind category:cs.RO cs.AI cs.CL cs.HC cs.LG  published:2015-08-25 summary:We present a unified framework which supports grounding natural-language semantics in robotic driving. This framework supports acquisition (learning grounded meanings of nouns and prepositions from human annotation of robotic driving paths), generation (using such acquired meanings to generate sentential description of new robotic driving paths), and comprehension (using such acquired meanings to support automated driving to accomplish navigational goals specified in natural language). We evaluate the performance of these three tasks by having independent human judges rate the semantic fidelity of the sentences associated with paths, achieving overall average correctness of 94.6% and overall average completeness of 85.6%. version:1
arxiv-1508-02496 | A Practical Guide to CNNs and Fisher Vectors for Image Instance Retrieval | http://arxiv.org/abs/1508.02496 | id:1508.02496 author:Vijay Chandrasekhar, Jie Lin, Olivier Morère, Hanlin Goh, Antoine Veillard category:cs.CV cs.IR  published:2015-08-11 summary:With deep learning becoming the dominant approach in computer vision, the use of representations extracted from Convolutional Neural Nets (CNNs) is quickly gaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global image descriptors for image instance retrieval. While the good performance of CNNs for image classification are unambiguously recognised, which of the two has the upper hand in the image retrieval context is not entirely clear yet. In this work, we propose a comprehensive study that systematically evaluates FVs and CNNs for image retrieval. The first part compares the performances of FVs and CNNs on multiple publicly available data sets. We investigate a number of details specific to each method. For FVs, we compare sparse descriptors based on interest point detectors with dense single-scale and multi-scale variants. For CNNs, we focus on understanding the impact of depth, architecture and training data on retrieval results. Our study shows that no descriptor is systematically better than the other and that performance gains can usually be obtained by using both types together. The second part of the study focuses on the impact of geometrical transformations such as rotations and scale changes. FVs based on interest point detectors are intrinsically resilient to such transformations while CNNs do not have a built-in mechanism to ensure such invariance. We show that performance of CNNs can quickly degrade in presence of rotations while they are far less affected by changes in scale. We then propose a number of ways to incorporate the required invariances in the CNN pipeline. Overall, our work is intended as a reference guide offering practically useful and simply implementable guidelines to anyone looking for state-of-the-art global descriptors best suited to their specific image instance retrieval problem. version:3
arxiv-1508-06095 | OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based Neural Training | http://arxiv.org/abs/1508.06095 | id:1508.06095 author:Rossella Cancelliere, Mario Gai, Patrick Gallinari, Luca Rubini category:cs.NE cs.LG stat.ML  published:2015-08-25 summary:In this paper we consider the training of single hidden layer neural networks by pseudoinversion, which, in spite of its popularity, is sometimes affected by numerical instability issues. Regularization is known to be effective in such cases, so that we introduce, in the framework of Tikhonov regularization, a matricial reformulation of the problem which allows us to use the condition number as a diagnostic tool for identification of instability. By imposing well-conditioning requirements on the relevant matrices, our theoretical analysis allows the identification of an optimal value for the regularization parameter from the standpoint of stability. We compare with the value derived by cross-validation for overfitting control and optimisation of the generalization performance. We test our method for both regression and classification tasks. The proposed method is quite effective in terms of predictivity, often with some improvement on performance with respect to the reference cases considered. This approach, due to analytical determination of the regularization parameter, dramatically reduces the computational load required by many other techniques. version:1
arxiv-1508-06092 | An analysis of numerical issues in neural training by pseudoinversion | http://arxiv.org/abs/1508.06092 | id:1508.06092 author:R. Cancelliere, R. Deluca, M. Gai, P. Gallinari, L. Rubini category:cs.LG cs.NE  published:2015-08-25 summary:Some novel strategies have recently been proposed for single hidden layer neural network training that set randomly the weights from input to hidden layer, while weights from hidden to output layer are analytically determined by pseudoinversion. These techniques are gaining popularity in spite of their known numerical issues when singular and/or almost singular matrices are involved. In this paper we discuss a critical use of Singular Value Analysis for identification of these drawbacks and we propose an original use of regularisation to determine the output weights, based on the concept of critical hidden layer size. This approach also allows to limit the training computational effort. Besides, we introduce a novel technique which relies an effective determination of input weights to the hidden layer dimension. This approach is tested for both regression and classification tasks, resulting in a significant performance improvement with respect to alternative methods. version:1
arxiv-1508-06091 | AUC Optimisation and Collaborative Filtering | http://arxiv.org/abs/1508.06091 | id:1508.06091 author:Charanpal Dhanjal, Romaric Gaudel, Stephan Clemencon category:stat.ML cs.LG  published:2015-08-25 summary:In recommendation systems, one is interested in the ranking of the predicted items as opposed to other losses such as the mean squared error. Although a variety of ways to evaluate rankings exist in the literature, here we focus on the Area Under the ROC Curve (AUC) as it widely used and has a strong theoretical underpinning. In practical recommendation, only items at the top of the ranked list are presented to the users. With this in mind, we propose a class of objective functions over matrix factorisations which primarily represent a smooth surrogate for the real AUC, and in a special case we show how to prioritise the top of the list. The objectives are differentiable and optimised through a carefully designed stochastic gradient-descent-based algorithm which scales linearly with the size of the data. In the special case of square loss we show how to improve computational complexity by leveraging previously computed measures. To understand theoretically the underlying matrix factorisation approaches we study both the consistency of the loss functions with respect to AUC, and generalisation using Rademacher theory. The resulting generalisation analysis gives strong motivation for the optimisation under study. Finally, we provide computation results as to the efficacy of the proposed method using synthetic and real data. version:1
arxiv-1508-06044 | Visualizing NLP annotations for Crowdsourcing | http://arxiv.org/abs/1508.06044 | id:1508.06044 author:Hanchuan Li, Haichen Shen, Shengliang Xu, Congle Zhang category:cs.CL  published:2015-08-25 summary:Visualizing NLP annotation is useful for the collection of training data for the statistical NLP approaches. Existing toolkits either provide limited visual aid, or introduce comprehensive operators to realize sophisticated linguistic rules. Workers must be well trained to use them. Their audience thus can hardly be scaled to large amounts of non-expert crowdsourced workers. In this paper, we present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to annotate two general categories of NLP problems: clustering and parsing. Workers can finish the tasks with simplified operators in an interactive interface, and fix errors conveniently. User studies show our toolkit is very friendly to NLP non-experts, and allow them to produce high quality labels for several sophisticated problems. We release our source code and toolkit to spur future research. version:1
arxiv-1508-06034 | Better Summarization Evaluation with Word Embeddings for ROUGE | http://arxiv.org/abs/1508.06034 | id:1508.06034 author:Jun-Ping Ng, Viktoria Abrecht category:cs.CL cs.IR  published:2015-08-25 summary:ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients. version:1
arxiv-1508-03790 | Depth-Gated LSTM | http://arxiv.org/abs/1508.03790 | id:1508.03790 author:Kaisheng Yao, Trevor Cohn, Katerina Vylomova, Kevin Duh, Chris Dyer category:cs.NE cs.CL  published:2015-08-16 summary:In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances. version:4
arxiv-1508-06013 | ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution | http://arxiv.org/abs/1508.06013 | id:1508.06013 author:Zeinab Bahmani, Leopoldo Bertossi, Nikolaos Vasiloglou category:cs.DB cs.AI cs.LG  published:2015-08-25 summary:Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called matching dependencies (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating three components of ER: (a) Classifiers for duplicate/non-duplicate record pairs built using machine learning (ML) techniques, (b) MDs for supporting both the blocking phase of ML and the merge itself; and (c) The use of the declarative language LogiQL -an extended form of Datalog supported by the LogicBlox platform- for data processing, and the specification and enforcement of MDs. version:1
arxiv-1508-06010 | Wavelet subspace decomposition of thermal infrared images for defect detection in artworks | http://arxiv.org/abs/1508.06010 | id:1508.06010 author:Muhammad Zubair Ahmad, Amir Ali Khan, Sihem Mezghani, Eric Perrin, Kamel Mouhoubi, Jean-Luc Bodnar, Valeriu Vrabie category:cs.CV  published:2015-08-25 summary:Monitoring the health of ancient artworks requires adequate prudence because of the sensitive nature of these materials. Classical techniques for identifying the development of faults rely on acoustic testing. These techniques, being invasive, may result in causing permanent damage to the material, especially if the material is inspected periodically. Non destructive testing has been carried out for different materials since long. In this regard, non-invasive systems were developed based on infrared thermometry principle to identify the faults in artworks. The test artwork is heated and the thermal response of the different layers is captured with the help of a thermal infrared camera. However, prolonged heating risks overheating and thus causing damage to artworks and an alternate approach is to use pseudo-random binary sequence excitations. The faults in the artwork, though, cannot be detected on the captured images, especially if their strength is weak. The weaker faults are either masked by the stronger ones, by the pictorial layer of the artwork or by the non-uniform heating. This work addresses the detection and localization of the faults through a wavelet based subspace decomposition scheme. The proposed scheme, on one hand, allows to remove the background while, on the other hand, removes the undesired high frequency noise. It is shown that the detection parameter is proportional to the diameter and the depth of the fault. A criterion is proposed to select the optimal wavelet basis along with suitable level selection for wavelet decomposition and reconstruction. The proposed approach is tested on a laboratory developed test sample with known fault locations and dimensions as well as real artworks. A comparison with a previously reported method demonstrates the efficacy of the proposed approach for fault detection in artworks. version:1
arxiv-1508-05995 | An algorithm for Left Atrial Thrombi detection using Transesophageal Echocardiography | http://arxiv.org/abs/1508.05995 | id:1508.05995 author:Jianrui Ding, Min Xian, H. D. Cheng, Yang Li, Fei Xu, Yingtao Zhang category:cs.CV  published:2015-08-24 summary:Transesophageal echocardiography (TEE) is widely used to detect left atrium (LA)/left atrial appendage (LAA) thrombi. In this paper, the local binary pattern variance (LBPV) features are extracted from region of interest (ROI). And the dynamic features are formed by using the information of its neighbor frames in the sequence. The sequence is viewed as a bag, and the images in the sequence are considered as the instances. Multiple-instance learning (MIL) method is employed to solve the LAA thrombi detection. The experimental results show that the proposed method can achieve better performance than that by using other methods. version:1
arxiv-1504-00657 | Eliciting Disease Data from Wikipedia Articles | http://arxiv.org/abs/1504.00657 | id:1504.00657 author:Geoffrey Fairchild, Lalindra De Silva, Sara Y. Del Valle, Alberto M. Segre category:cs.IR cs.CL cs.SI q-bio.PE  published:2015-04-02 summary:Traditional disease surveillance systems suffer from several disadvantages, including reporting lags and antiquated technology, that have caused a movement towards internet-based disease surveillance systems. Internet systems are particularly attractive for disease outbreaks because they can provide data in near real-time and can be verified by individuals around the globe. However, most existing systems have focused on disease monitoring and do not provide a data repository for policy makers or researchers. In order to fill this gap, we analyzed Wikipedia article content. We demonstrate how a named-entity recognizer can be trained to tag case counts, death counts, and hospitalization counts in the article narrative that achieves an F1 score of 0.753. We also show, using the 2014 West African Ebola virus disease epidemic article as a case study, that there are detailed time series data that are consistently updated that closely align with ground truth data. We argue that Wikipedia can be used to create the first community-driven open-source emerging disease detection, monitoring, and repository system. version:4
arxiv-1503-02533 | A Smoothed Dual Approach for Variational Wasserstein Problems | http://arxiv.org/abs/1503.02533 | id:1503.02533 author:Marco Cuturi, Gabriel Peyré category:stat.ML math.OC  published:2015-03-09 summary:Variational problems that involve Wasserstein distances have been recently proposed to summarize and learn from probability measures. Despite being conceptually simple, such problems are computationally challenging because they involve minimizing over quantities (Wasserstein distances) that are themselves hard to compute. We show that the dual formulation of Wasserstein variational problems introduced recently by Carlier et al. (2014) can be regularized using an entropic smoothing, which leads to smooth, differentiable, convex optimization problems that are simpler to implement and numerically more stable. We illustrate the versatility of this approach by applying it to the computation of Wasserstein barycenters and gradient flows of spacial regularization functionals. version:2
arxiv-1503-02193 | Label optimal regret bounds for online local learning | http://arxiv.org/abs/1503.02193 | id:1503.02193 author:Pranjal Awasthi, Moses Charikar, Kevin A. Lai, Andrej Risteski category:cs.LG  published:2015-03-07 summary:We resolve an open question from (Christiano, 2014b) posed in COLT'14 regarding the optimal dependency of the regret achievable for online local learning on the size of the label set. In this framework the algorithm is shown a pair of items at each step, chosen from a set of $n$ items. The learner then predicts a label for each item, from a label set of size $L$ and receives a real valued payoff. This is a natural framework which captures many interesting scenarios such as collaborative filtering, online gambling, and online max cut among others. (Christiano, 2014a) designed an efficient online learning algorithm for this problem achieving a regret of $O(\sqrt{nL^3T})$, where $T$ is the number of rounds. Information theoretically, one can achieve a regret of $O(\sqrt{n \log L T})$. One of the main open questions left in this framework concerns closing the above gap. In this work, we provide a complete answer to the question above via two main results. We show, via a tighter analysis, that the semi-definite programming based algorithm of (Christiano, 2014a), in fact achieves a regret of $O(\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely, we show that a polynomial time algorithm for online local learning with lower regret would imply a polynomial time algorithm for the planted clique problem which is widely believed to be hard. We prove a similar hardness result under a related conjecture concerning planted dense subgraphs that we put forth. Unlike planted clique, the planted dense subgraph problem does not have any known quasi-polynomial time algorithms. Computational lower bounds for online learning are relatively rare, and we hope that the ideas developed in this work will lead to lower bounds for other online learning scenarios as well. version:2
arxiv-1508-05913 | Another Look at DWD: Thrifty Algorithm and Bayes Risk Consistency in RKHS | http://arxiv.org/abs/1508.05913 | id:1508.05913 author:Boxiang Wang, Hui Zou category:stat.ML  published:2015-08-24 summary:Distance weighted discrimination (DWD) is a margin-based classifier with an interesting geometric motivation. DWD was originally proposed as a superior alternative to the support vector machine (SVM), however DWD is yet to be popular compared with the SVM. The main reasons are twofold. First, the state-of-the-art algorithm for solving DWD is based on the second-order-cone programming (SOCP), while the SVM is a quadratic programming problem which is much more efficient to solve. Second, the current statistical theory of DWD mainly focuses on the linear DWD for the high-dimension-low-sample-size setting and data-piling, while the learning theory for the SVM mainly focuses on the Bayes risk consistency of the kernel SVM. In fact, the Bayes risk consistency of DWD is presented as an open problem in the original DWD paper. In this work, we advance the current understanding of DWD from both computational and theoretical perspectives. We propose a novel efficient algorithm for solving DWD, and our algorithm can be several hundred times faster than the existing state-of-the-art algorithm based on the SOCP. In addition, our algorithm can handle the generalized DWD, while the SOCP algorithm only works well for a special DWD but not the generalized DWD. Furthermore, we consider a natural kernel DWD in a reproducing kernel Hilbert space and then establish the Bayes risk consistency of the kernel DWD. We compare DWD and the SVM on several benchmark data sets and show that the two have comparable classification accuracy, but DWD equipped with our new algorithm can be much faster to compute than the SVM. version:1
arxiv-1508-05902 | A Framework for Comparing Groups of Documents | http://arxiv.org/abs/1508.05902 | id:1508.05902 author:Arun S. Maiya category:cs.CL cs.SI I.2.7  published:2015-08-24 summary:We present a general framework for comparing multiple groups of documents. A bipartite graph model is proposed where document groups are represented as one node set and the comparison criteria are represented as the other node set. Using this model, we present basic algorithms to extract insights into similarities and differences among the document groups. Finally, we demonstrate the versatility of our framework through an analysis of NSF funding programs for basic research. version:1
arxiv-1508-05879 | Optical images-based edge detection in Synthetic Aperture Radar images | http://arxiv.org/abs/1508.05879 | id:1508.05879 author:Gilberto P. Silva Junior, Alejandro C. Frery, Sandra Sandri, Humberto Bustince, Edurne Barrenechea, Cédric Marco-Detchart category:cs.CV  published:2015-08-24 summary:We address the issue of adapting optical images-based edge detection techniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery. We modify the gravitational edge detection technique (inspired by the Law of Universal Gravity) proposed by Lopez-Molina et al, using the non-standard neighbourhood configuration proposed by Fu et al, to reduce the speckle noise in polarimetric SAR imagery. We compare the modified and unmodified versions of the gravitational edge detection technique with the well-established one proposed by Canny, as well as with a recent multiscale fuzzy-based technique proposed by Lopez-Molina et Alejandro We also address the issues of aggregation of gray level images before and after edge detection and of filtering. All techniques addressed here are applied to a mosaic built using class distributions obtained from a real scene, as well as to the true PolSAR image; the mosaic results are assessed using Baddeley's Delta Metric. Our experiments show that modifying the gravitational edge detection technique with a non-standard neighbourhood configuration produces better results than the original technique, as well as the other techniques used for comparison. The experiments show that adapting edge detection methods from Computational Intelligence for use in PolSAR imagery is a new field worthy of exploration. version:1
arxiv-1506-04472 | A Survey of Multithreading Image Analysis | http://arxiv.org/abs/1506.04472 | id:1506.04472 author:Elham Sagheb Hossein Pour category:cs.CV  published:2015-06-15 summary:Digital image analysis has made a big advance in many areas of enterprise applications including medicine, industry, and entertainment by assisting the extraction of semantic information from digital images. However, its large computational complexity has been a trouble to most real-time developments. While image analysis in general has been studied for a log period in computer science community, the use of multithreading strategy as the most efficient improving computational capacity technique has been limited so far. In this survey an attempt is made to explain the current knowledge and so far progresses in incorporating image analysis with multithreading approaches. The present work also provides insights and tendencies for the possible future enhancement of multithreading image analysis. version:5
arxiv-1508-05873 | Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for Stationary Gaussian Inputs and Slow Learning | http://arxiv.org/abs/1508.05873 | id:1508.05873 author:Jingen Ni, Jian Yang, Jie Chen, Cédric Richard, José Carlos M. Bermudez category:cs.NA cs.LG  published:2015-08-24 summary:Some system identification problems impose nonnegativity constraints on the parameters to estimate due to inherent physical characteristics of the unknown system. The nonnegative least-mean-square (NNLMS) algorithm and its variants allow to address this problem in an online manner. A nonnegative least mean fourth (NNLMF) algorithm has been recently proposed to improve the performance of these algorithms in cases where the measurement noise is not Gaussian. This paper provides a first theoretical analysis of the stochastic behavior of the NNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation results illustrate the accuracy of the proposed analysis. version:1
arxiv-1508-05817 | Echoes of Persuasion: The Effect of Euphony in Persuasive Communication | http://arxiv.org/abs/1508.05817 | id:1508.05817 author:Marco Guerini, Gözde Özbal, Carlo Strapparava category:cs.CL cs.CY cs.SI  published:2015-08-24 summary:While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and non-persuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion. version:1
arxiv-1508-05803 | Searching for significant patterns in stratified data | http://arxiv.org/abs/1508.05803 | id:1508.05803 author:Felipe Llinares-Lopez, Laetitia Papaxanthos, Dean Bodenham, Karsten Borgwardt category:stat.ML cs.LG  published:2015-08-24 summary:Significant pattern mining, the problem of finding itemsets that are significantly enriched in one class of objects, is statistically challenging, as the large space of candidate patterns leads to an enormous multiple testing problem. Recently, the concept of testability was proposed as one approach to correct for multiple testing in pattern mining while retaining statistical power. Still, these strategies based on testability do not allow one to condition the test of significance on the observed covariates, which severely limits its utility in biomedical applications. Here we propose a strategy and an efficient algorithm to perform significant pattern mining in the presence of categorical covariates with K states. version:1
arxiv-1508-05752 | An evolutionary approach to the identification of Cellular Automata based on partial observations | http://arxiv.org/abs/1508.05752 | id:1508.05752 author:Witold Bołt, Jan M. Baetens, Bernard De Baets category:cs.NE nlin.CG  published:2015-08-24 summary:In this paper we consider the identification problem of Cellular Automata (CAs). The problem is defined and solved in the context of partial observations with time gaps of unknown length, i.e. pre-recorded, partial configurations of the system at certain, unknown time steps. A solution method based on a modified variant of a Genetic Algorithm (GA) is proposed and illustrated with brief experimental results. version:1
arxiv-1505-05004 | An Experimental Comparison of Hybrid Algorithms for Bayesian Network Structure Learning | http://arxiv.org/abs/1505.05004 | id:1505.05004 author:Maxime Gasse, Alex Aussem, Haytham Elghazel category:stat.ML cs.AI cs.LG  published:2015-05-19 summary:We present a novel hybrid algorithm for Bayesian network structure learning, called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. It is based on a subroutine called HPC, that combines ideas from incremental and divide-and-conquer constraint-based methods to learn the parents and children of a target variable. We conduct an experimental comparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning, on several benchmarks with various data sizes. Our extensive experiments show that H2PC outperforms MMHC both in terms of goodness of fit to new data and in terms of the quality of the network structure itself, which is closer to the true dependence structure of the data. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available. version:2
arxiv-1508-05056 | Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual Sentiment Prediction | http://arxiv.org/abs/1508.05056 | id:1508.05056 author:Victor Campos, Amaia Salvador, Brendan Jou, Xavier Giró-i-Nieto category:cs.MM cs.CV I.2.10; H.1.2  published:2015-08-20 summary:Visual media are powerful means of expressing emotions and sentiments. The constant generation of new content in social networks highlights the need of automated visual sentiment analysis tools. While Convolutional Neural Networks (CNNs) have established a new state-of-the-art in several vision problems, their application to the task of sentiment analysis is mostly unexplored and there are few studies regarding how to design CNNs for this purpose. In this work, we study the suitability of fine-tuning a CNN for visual sentiment prediction as well as explore performance boosting techniques within this deep learning setting. Finally, we provide a deep-dive analysis into a benchmark, state-of-the-art network architecture to gain insight about how to design patterns for CNNs on the task of visual sentiment prediction. version:2
arxiv-1508-05711 | Fast Asynchronous Parallel Stochastic Gradient Decent | http://arxiv.org/abs/1508.05711 | id:1508.05711 author:Shen-Yi Zhao, Wu-Jun Li category:stat.ML cs.LG  published:2015-08-24 summary:Stochastic gradient descent~(SGD) and its variants have become more and more popular in machine learning due to their efficiency and effectiveness. To handle large-scale problems, researchers have recently proposed several parallel SGD methods for multicore systems. However, existing parallel SGD methods cannot achieve satisfactory performance in real applications. In this paper, we propose a fast asynchronous parallel SGD method, called AsySVRG, by designing an asynchronous strategy to parallelize the recently proposed SGD variant called stochastic variance reduced gradient~(SVRG). Both theoretical and empirical results show that AsySVRG can outperform existing state-of-the-art parallel SGD methods like Hogwild! in terms of convergence rate and computation cost. version:1
arxiv-1508-05704 | Iterative Thresholded Bi-Histogram Equalization for Medical Image Enhancement | http://arxiv.org/abs/1508.05704 | id:1508.05704 author:Muhammad Ali Qadar, Yan Zhaowen, Li Hua category:cs.CV  published:2015-08-24 summary:Enhancement of human vision to get an insight to information content is of vital importance. The traditional histogram equalization methods have been suffering from amplified contrast with the addition of artifacts and a surprising unnatural visibility of the processed images. In order to overcome these drawbacks, this paper proposes interative, mean, and multi-threshold selection criterion with plateau limits, which consist of histogram segmentation, clipping and transformation modules. The histogram partition consists of multiple thresholding processes that divide the histogram into two parts, whereas the clipping process nicely enhances the contrast by having a check on the rate of enhancement that could be tuned. Histogram equalization to each segmented sub-histogram provides the output image with preserved brightness and enhanced contrast. Results of the present study showed that the proposed method efficiently handles the noise amplification. Further, it also preserves the brightness by retaining natural look of targeted image. version:1
arxiv-1501-04505 | Robust Visual Tracking via Convolutional Networks | http://arxiv.org/abs/1501.04505 | id:1501.04505 author:Kaihua Zhang, Qingshan Liu, Yi Wu, Ming-Hsuan Yang category:cs.CV  published:2015-01-19 summary:Deep networks have been successfully applied to visual tracking by learning a generic representation offline from numerous training images. However the offline training is time-consuming and the learned generic representation may be less discriminative for tracking specific objects. In this paper we present that, even without offline training with a large amount of auxiliary data, simple two-layer convolutional networks can be powerful enough to develop a robust representation for visual tracking. In the first frame, we employ the k-means algorithm to extract a set of normalized patches from the target region as fixed filters, which integrate a series of adaptive contextual filters surrounding the target to define a set of feature maps in the subsequent frames. These maps measure similarities between each filter and the useful local intensity patterns across the target, thereby encoding its local structural information. Furthermore, all the maps form together a global representation, which is built on mid-level features, thereby remaining close to image-level information, and hence the inner geometric layout of the target is also well preserved. A simple soft shrinkage method with an adaptive threshold is employed to de-noise the global representation, resulting in a robust sparse representation. The representation is updated via a simple and effective online strategy, allowing it to robustly adapt to target appearance variations. Our convolution networks have surprisingly lightweight structure, yet perform favorably against several state-of-the-art methods on the CVPR2013 tracking benchmark dataset with 50 challenging videos. version:2
arxiv-1508-05683 | Morphometry-Based Longitudinal Neurodegeneration Simulation with MR Imaging | http://arxiv.org/abs/1508.05683 | id:1508.05683 author:Siqi Liu, Sidong Liu, Sonia Pujol, Ron Kikinis, Dagan Feng, Michael Fulham, Weidong Cai category:cs.CV  published:2015-08-24 summary:We present a longitudinal MR simulation framework which simulates the future neurodegenerative progression by outputting the predicted follow-up MR image and the voxel-based morphometry (VBM) map. This framework expects the patients to have at least 2 historical MR images available. The longitudinal and cross-sectional VBM maps are extracted to measure the affinity between the target subject and the template subjects collected for simulation. Then the follow-up simulation is performed by resampling the latest available target MR image with a weighted sum of non-linear transformations derived from the best-matched templates. The leave-one-out strategy was used to compare different simulation methods. Compared to the state-of-the-art voxel-based method, our proposed morphometry-based simulation achieves better accuracy in most cases. version:1
arxiv-1408-2764 | Convex Calibration Dimension for Multiclass Loss Matrices | http://arxiv.org/abs/1408.2764 | id:1408.2764 author:Harish G. Ramaswamy, Shivani Agarwal category:cs.LG stat.ML  published:2014-08-12 summary:We study consistency properties of surrogate loss functions for general multiclass learning problems, defined by a general multiclass loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be calibrated with respect to a loss matrix in this setting. We then introduce the notion of convex calibration dimension of a multiclass loss matrix, which measures the smallest `size' of a prediction space in which it is possible to design a convex surrogate that is calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, we apply our framework to study various subset ranking losses, and use the convex calibration dimension as a tool to show both the existence and non-existence of various types of convex calibrated surrogates for these losses. Our results strengthen recent results of Duchi et al. (2010) and Calauzenes et al. (2012) on the non-existence of certain types of convex calibrated surrogates in subset ranking. We anticipate the convex calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems. version:2
arxiv-1508-05608 | The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms | http://arxiv.org/abs/1508.05608 | id:1508.05608 author:Yahel David, Nahum Shimkin category:stat.ML cs.AI cs.LG  published:2015-08-23 summary:We consider the Max $K$-Armed Bandit problem, where a learning agent is faced with several sources (arms) of items (rewards), and interested in finding the best item overall. At each time step the agent chooses an arm, and obtains a random real valued reward. The rewards of each arm are assumed to be i.i.d., with an unknown probability distribution that generally differs among the arms. Under the PAC framework, we provide lower bounds on the sample complexity of any $(\epsilon,\delta)$-correct algorithm, and propose algorithms that attain this bound up to logarithmic factors. We compare the performance of this multi-arm algorithms to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage. Interestingly, when the maximal rewards of the arms happen to be similar, the latter approach may provide better performance. version:1
arxiv-1508-03411 | Emphatic TD Bellman Operator is a Contraction | http://arxiv.org/abs/1508.03411 | id:1508.03411 author:Assaf Hallak, Aviv Tamar, Shie Mannor category:stat.ML cs.LG  published:2015-08-14 summary:Recently, \citet{SuttonMW15} introduced the emphatic temporal differences (ETD) algorithm for off-policy evaluation in Markov decision processes. In this short note, we show that the projected fixed-point equation that underlies ETD involves a contraction operator, with a $\sqrt{\gamma}$-contraction modulus (where $\gamma$ is the discount factor). This allows us to provide error bounds on the approximation error of ETD. To our knowledge, these are the first error bounds for an off-policy evaluation algorithm under general target and behavior policies. version:2
arxiv-1509-01220 | Light Efficient Flutter Shutter | http://arxiv.org/abs/1509.01220 | id:1509.01220 author:Moshe Ben-Ezra category:cs.GR cs.CV  published:2015-08-23 summary:Flutter shutter is a technique in which the exposure is chopped into segments and light is only integrated part of the time. By carefully selecting the chopping sequence it is possible to better condition the data for reconstruction problems such as motion deblurring, focal sweeping, and compressed sensing. The partial exposure trades better conditioning for less energy. In problems such as motion deblurring the available energy is what caused the problem in the first place (as strong illumination allows short exposure thus eliminates motion blur). It is still beneficial because the benefit from the better conditioning outweighs the cost in energy. This documents is focused on light efficient flutter shutter that provides better conditioning and better energy utilization than conventional flutter shutter. version:1
arxiv-1506-02275 | Confounds and Consequences in Geotagged Twitter Data | http://arxiv.org/abs/1506.02275 | id:1506.02275 author:Umashanthi Pavalanathan, Jacob Eisenstein category:cs.CL  published:2015-06-07 summary:Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location field in each profile. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and text-based geolocation. GPS-tagging and self-reported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40. version:2
arxiv-1412-6334 | Leveraging Monolingual Data for Crosslingual Compositional Word Representations | http://arxiv.org/abs/1412.6334 | id:1412.6334 author:Hubert Soyer, Pontus Stenetorp, Akiko Aizawa category:cs.CL  published:2014-12-19 summary:In this work, we present a novel neural network based architecture for inducing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the word-level representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely semantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classification task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for the English to German and German to English sub-tasks respectively. The former advances the state of the art by 0.9% points of accuracy, the latter is an absolute improvement upon the previous state of the art by 7.7% points of accuracy and an improvement of 33.0% in error reduction. version:4
arxiv-1508-05514 | Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence | http://arxiv.org/abs/1508.05514 | id:1508.05514 author:Tohid Ardeshiri, Umut Orguner, Emre Özkan category:stat.ML cs.CV cs.LG cs.RO cs.SY  published:2015-08-22 summary:We propose a greedy mixture reduction algorithm which is capable of pruning mixture components as well as merging them based on the Kullback-Leibler divergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD based method since it is not restricted to merging operations. The capability of pruning (in addition to merging) gives the algorithm the ability of preserving the peaks of the original mixture during the reduction. Analytical approximations are derived to circumvent the computational intractability of the KLD which results in a computationally efficient method. The proposed algorithm is compared with Runnalls' and Williams' methods in two numerical examples, using both simulated and real world data. The results indicate that the performance and computational complexity of the proposed approach make it an efficient alternative to existing mixture reduction methods. version:1
arxiv-1508-05508 | Towards Neural Network-based Reasoning | http://arxiv.org/abs/1508.05508 | id:1508.05508 author:Baolin Peng, Zhengdong Lu, Hang Li, Kam-Fai Wong category:cs.AI cs.CL cs.LG cs.NE  published:2015-08-22 summary:We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%. version:1
arxiv-1407-5155 | Sparse and spurious: dictionary learning with noise and outliers | http://arxiv.org/abs/1407.5155 | id:1407.5155 author:Rémi Gribonval, Rodolphe Jenatton, Francis Bach category:cs.LG stat.ML  published:2014-07-19 summary:A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries, noisy signals, and possible outliers, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations. version:4
arxiv-1503-06567 | On some provably correct cases of variational inference for topic models | http://arxiv.org/abs/1503.06567 | id:1503.06567 author:Pranjal Awasthi, Andrej Risteski category:cs.LG cs.DS stat.ML  published:2015-03-23 summary:Variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. It's closely related to Expectation Maximization (EM), and is applied when exact EM is computationally infeasible. Despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. In this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. More specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012c). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003). It is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient. While our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest. version:2
arxiv-1508-05495 | Bayesian Hypothesis Testing for Block Sparse Signal Recovery | http://arxiv.org/abs/1508.05495 | id:1508.05495 author:Mehdi Korki, Hadi Zayyani, Jingxin Zhang category:stat.ML cs.IT math.IT  published:2015-08-22 summary:This letter presents a novel Block Bayesian Hypothesis Testing Algorithm (Block-BHTA) for reconstructing block sparse signals with unknown block structures. The Block-BHTA comprises the detection and recovery of the supports, and the estimation of the amplitudes of the block sparse signal. The support detection and recovery is performed using a Bayesian hypothesis testing. Then, based on the detected and reconstructed supports, the nonzero amplitudes are estimated by linear MMSE. The effectiveness of Block-BHTA is demonstrated by numerical experiments. version:1
arxiv-1405-1958 | A Self-Adaptive Network Protection System | http://arxiv.org/abs/1405.1958 | id:1405.1958 author:Mohamed Hassan category:cs.NE cs.AI cs.CR  published:2014-05-08 summary:In this treatise we aim to build a hybrid network automated (self-adaptive) security threats discovery and prevention system; by using unconventional techniques and methods, including fuzzy logic and biological inspired algorithms under the context of soft computing. version:2
arxiv-1408-3526 | Parallel software implementation of recursive multidimensional digital filters for point-target detection in cluttered infrared scenes | http://arxiv.org/abs/1408.3526 | id:1408.3526 author:Hugh L. Kennedy category:cs.CV  published:2014-08-15 summary:A technique for the enhancement of point targets in clutter is described. The local 3-D spectrum at each pixel is estimated recursively. An optical flow-field for the textured background is then generated using the 3-D autocorrelation function and the local velocity estimates are used to apply high-pass velocity-selective spatiotemporal filters, with finite impulse responses (FIRs), to subtract the background clutter signal, leaving the foreground target signal, plus noise. Parallel software implementations using a multicore central processing unit (CPU) and a graphical processing unit (GPU) are investigated. version:4
arxiv-1410-7074 | Random Sampling in an Age of Automation: Minimizing Expenditures through Balanced Collection and Annotation | http://arxiv.org/abs/1410.7074 | id:1410.7074 author:Oscar Beijbom category:cs.CY cs.LG stat.ME  published:2014-10-26 summary:Methods for automated collection and annotation are changing the cost-structures of sampling surveys for a wide range of applications. Digital samples in the form of images or audio recordings can be collected rapidly, and annotated by computer programs or crowd workers. We consider the problem of estimating a population mean under these new cost-structures, and propose a Hybrid-Offset sampling design. This design utilizes two annotators: a primary, which is accurate but costly (e.g. a human expert) and an auxiliary which is noisy but cheap (e.g. a computer program), in order to minimize total sampling expenditures. Our analysis gives necessary conditions for the Hybrid-Offset design and specifies optimal sample sizes for both annotators. Simulations on data from a coral reef survey program indicate that the Hybrid-Offset design outperforms several alternative sampling designs. In particular, sampling expenditures are reduced 50% compared to the Conventional design currently deployed by the coral ecologists. version:4
arxiv-1508-05326 | A large annotated corpus for learning natural language inference | http://arxiv.org/abs/1508.05326 | id:1508.05326 author:Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning category:cs.CL  published:2015-08-21 summary:Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. version:1
arxiv-1508-05306 | Exemplar Based Deep Discriminative and Shareable Feature Learning for Scene Image Classification | http://arxiv.org/abs/1508.05306 | id:1508.05306 author:Zhen Zuo, Gang Wang, Bing Shuai, Lifan Zhao, Qingxiong Yang category:cs.CV  published:2015-08-21 summary:In order to encode the class correlation and class specific information in image representation, we propose a new local feature learning approach named Deep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to hierarchically learn feature transformation filter banks to transform raw pixel image patches to features. The learned filter banks are expected to: (1) encode common visual patterns of a flexible number of categories; (2) encode discriminative information; and (3) hierarchically extract patterns at different visual levels. Particularly, in each single layer of DDSFL, shareable filters are jointly learned for classes which share the similar patterns. Discriminative power of the filters is achieved by enforcing the features from the same category to be close, while features from different categories to be far away from each other. Furthermore, we also propose two exemplar selection methods to iteratively select training data for more efficient and effective learning. Based on the experimental results, DDSFL can achieve very promising performance, and it also shows great complementary effect to the state-of-the-art Caffe features. version:1
arxiv-1307-4891 | Robust Subspace Clustering via Thresholding | http://arxiv.org/abs/1307.4891 | id:1307.4891 author:Reinhard Heckel, Helmut Bölcskei category:stat.ML cs.IT cs.LG math.IT  published:2013-07-18 summary:The problem of clustering noisy and incompletely observed high-dimensional data points into a union of low-dimensional subspaces and a set of outliers is considered. The number of subspaces, their dimensions, and their orientations are assumed unknown. We propose a simple low-complexity subspace clustering algorithm, which applies spectral clustering to an adjacency matrix obtained by thresholding the correlations between data points. In other words, the adjacency matrix is constructed from the nearest neighbors of each data point in spherical distance. A statistical performance analysis shows that the algorithm exhibits robustness to additive noise and succeeds even when the subspaces intersect. Specifically, our results reveal an explicit tradeoff between the affinity of the subspaces and the tolerable noise level. We furthermore prove that the algorithm succeeds even when the data points are incompletely observed with the number of missing entries allowed to be (up to a log-factor) linear in the ambient dimension. We also propose a simple scheme that provably detects outliers, and we present numerical results on real and synthetic data. version:4
arxiv-1508-05249 | Representation of Quasi-Monotone Functionals by Families of Separating Hyperplanes | http://arxiv.org/abs/1508.05249 | id:1508.05249 author:Ingo Steinwart category:math.OC math.FA math.ST stat.ML stat.TH  published:2015-08-21 summary:We characterize when the level sets of a continuous quasi-monotone functional defined on a suitable convex subset of a normed space can be uniquely represented by a family of bounded continuous functionals. Furthermore, we investigate how regularly these functionals depend on the parameterizing level. Finally, we show how this question relates to the recent problem of property elicitation that simultaneously attracted interest in machine learning, statistical evaluation of forecasts, and finance. version:1
arxiv-1509-03221 | Recurrent Neural Network Based Modeling of Gene Regulatory Network Using Bat Algorithm | http://arxiv.org/abs/1509.03221 | id:1509.03221 author:Sudip Mandal, Goutam Saha, Rajat K. Pal category:cs.AI cs.NE  published:2015-08-21 summary:Correct inference of genetic regulations inside a cell is one of the greatest challenges in post genomic era for the biologist and researchers. Several intelligent techniques and models were already proposed to identify the regulatory relations among genes from the biological database like time series microarray data. Recurrent Neural Network (RNN) is one of the most popular and simple approach to model the dynamics as well as to infer correct dependencies among genes. In this paper, Bat Algorithm (BA) was applied to optimize the model parameters of RNN model of Gene Regulatory Network (GRN). Initially the proposed method is tested against small artificial network without any noise and the efficiency was observed in term of number of iteration, number of population and BA optimization parameters. The model was also validated in presence of different level of random noise for the small artificial network and that proved its ability to infer the correct inferences in presence of noise like real world dataset. In the next phase of this research, BA based RNN is applied to real world benchmark time series microarray dataset of E. Coli. The results shown that it can able to identify the maximum true positive regulation but also include some false positive regulations. Therefore, BA is very suitable for identifying biological plausible GRN with the help RNN model version:1
arxiv-1503-04549 | High-dimensional quadratic classifiers in non-sparse settings | http://arxiv.org/abs/1503.04549 | id:1503.04549 author:Makoto Aoshima, Kazuyoshi Yata category:stat.ML math.ST stat.TH  published:2015-03-16 summary:We consider high-dimensional quadratic classifiers in non-sparse settings. The target of classification rules is not Bayes error rates in the context. The classifier based on the Mahalanobis distance does not always give a preferable performance even if the populations are normal distributions having known covariance matrices. The quadratic classifiers proposed in this paper draw information about heterogeneity effectively through both the differences of expanding mean vectors and covariance matrices. We show that they hold a consistency property in which misclassification rates tend to zero as the dimension goes to infinity under non-sparse settings. We verify that they are asymptotically distributed as a normal distribution under certain conditions. We also propose a quadratic classifier after feature selection by using both the differences of mean vectors and covariance matrices. Finally, we discuss performances of the classifiers in actual data analyses. The proposed classifiers achieve highly accurate classification with very low computational costs. version:2
arxiv-1508-04535 | Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification | http://arxiv.org/abs/1508.04535 | id:1508.04535 author:Ruimao Zhang, Liang Lin, Rui Zhang, Wangmeng Zuo, Lei Zhang category:cs.CV  published:2015-08-19 summary:Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval . Conventional methods often study these two steps separately, e.g., learning hash functions from a predefined hand-crafted feature space. Meanwhile, the bit lengths of output hashing codes are preset in most previous methods, neglecting the significance level of different bits and restricting their practical flexibility. To address these issues, we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. Specifically, we organize the training images into a batch of triplet samples, each sample containing two images with the same label and one with a different label. With these triplet samples, we maximize the margin between matched pairs and mismatched pairs in the Hamming space. In addition, a regularization term is introduced to enforce the adjacency consistency, i.e., images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion, where discriminative image features and hash functions are simultaneously optimized. Furthermore, each bit of our hashing codes is unequally weighted so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths. version:2
arxiv-1508-05170 | Adaptive Online Learning | http://arxiv.org/abs/1508.05170 | id:1508.05170 author:Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML  published:2015-08-21 summary:We propose a general framework for studying adaptive regret bounds in the online learning framework, including model selection bounds and data-dependent bounds. Given a data- or model-dependent bound we ask, "Does there exist some algorithm achieving this bound?" We show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved. In particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. A cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes. Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second-order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem that holds for countably infinite sets. version:1
arxiv-1508-05383 | On Monotonicity of the Optimal Transmission Policy in Cross-layer Adaptive m-QAM Modulation | http://arxiv.org/abs/1508.05383 | id:1508.05383 author:Ni Ding, Parastoo Sadeghi, Rodney A. Kennedy category:stat.ML cs.IT math.IT  published:2015-08-21 summary:This paper considers a cross-layer adaptive modulation system that is modeled as a Markov decision process (MDP). We study how to utilize the monotonicity of the optimal transmission policy to relieve the computational complexity of dynamic programming (DP). In this system, a scheduler controls the bit rate of the m-quadrature amplitude modulation (m-QAM) in order to minimize the long-term losses incurred by the queue overflow in the data link layer and the transmission power consumption in the physical layer. The work is done in two steps. Firstly, we observe the L-natural-convexity and submodularity of DP to prove that the optimal policy is always nondecreasing in queue occupancy/state and derive the sufficient condition for it to be nondecreasing in both queue and channel states. We also show that, due to the L-natural-convexity of DP, the variation of the optimal policy in queue state is restricted by a bounded marginal effect: The increment of the optimal policy between adjacent queue states is no greater than one. Secondly, we use the monotonicity results to present two low complexity algorithms: monotonic policy iteration (MPI) based on L-natural-convexity and discrete simultaneous perturbation stochastic approximation (DSPSA). We run experiments to show that the time complexity of MPI based on L-natural-convexity is much lower than that of DP and the conventional MPI that is based on submodularity and DSPSA is able to adaptively track the optimal policy when the system parameters change. version:1
arxiv-1506-04500 | Circle-based Eye Center Localization (CECL) | http://arxiv.org/abs/1506.04500 | id:1506.04500 author:Yustinus Eko Soelistio, Eric Postma, Alfons Maes category:cs.CV  published:2015-06-15 summary:We propose an improved eye center localization method based on the Hough transform, called Circle-based Eye Center Localization (CECL) that is simple, robust, and achieves accuracy on a par with typically more complex state-of-the-art methods. The CECL method relies on color and shape cues that distinguish the iris from other facial structures. The accuracy of the CECL method is demonstrated through a comparison with 15 state-of-the-art eye center localization methods against five error thresholds, as reported in the literature. The CECL method achieved an accuracy of 80.8% to 99.4% and ranked first for 2 of the 5 thresholds. It is concluded that the CECL method offers an attractive alternative to existing methods for automatic eye center localization. version:2
arxiv-1508-05163 | Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method | http://arxiv.org/abs/1508.05163 | id:1508.05163 author:Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra category:cs.CL cs.IR  published:2015-08-21 summary:Text mining can be applied to many fields. One of the application is using text mining in digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems. version:1
arxiv-1508-05051 | Auto-Sizing Neural Networks: With Applications to n-gram Language Models | http://arxiv.org/abs/1508.05051 | id:1508.05051 author:Kenton Murray, David Chiang category:cs.CL  published:2015-08-20 summary:Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through $\ell_{\infty,1}$ and $\ell_{2,1}$ regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions. version:1
arxiv-1508-05046 | Improving Image Restoration with Soft-Rounding | http://arxiv.org/abs/1508.05046 | id:1508.05046 author:Xing Mei, Honggang Qi, Bao-Gang Hu, Siwei Lyu category:cs.CV  published:2015-08-20 summary:Several important classes of images such as text, barcode and pattern images have the property that pixels can only take a distinct subset of values. This knowledge can benefit the restoration of such images, but it has not been widely considered in current restoration methods. In this work, we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework. We introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them. When incorporated into the regularized least squares restoration framework, this regularizer leads to a simple and efficient step that resembles and extends the rounding operation, which we term as soft-rounding. We apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values. Experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures (PSNR and SSIM). Furthermore, we show that this regularizer can also benefit the restoration of general natural images. version:1
arxiv-1412-2291 | Adjusted least squares fitting of algebraic hypersurfaces | http://arxiv.org/abs/1412.2291 | id:1412.2291 author:Konstantin Usevich, Ivan Markovsky category:stat.CO cs.CG cs.CV math.NA  published:2014-12-06 summary:We consider the problem of fitting a set of points in Euclidean space by an algebraic hypersurface. We assume that points on a true hypersurface, described by a polynomial equation, are corrupted by zero mean independent Gaussian noise, and we estimate the coefficients of the true polynomial equation. The adjusted least squares estimator accounts for the bias present in the ordinary least squares estimator. The adjusted least squares estimator is based on constructing a quasi-Hankel matrix, which is a bias-corrected matrix of moments. For the case of unknown noise variance, the estimator is defined as a solution of a polynomial eigenvalue problem. In this paper, we present new results on invariance properties of the adjusted least squares estimator and an improved algorithm for computing the estimator for an arbitrary set of monomials in the polynomial equation. version:2
arxiv-1508-05028 | Using User Generated Online Photos to Estimate and Monitor Air Pollution in Major Cities | http://arxiv.org/abs/1508.05028 | id:1508.05028 author:Yuncheng Li, Jifei Huang, Jiebo Luo category:cs.CV  published:2015-08-20 summary:With the rapid development of economy in China over the past decade, air pollution has become an increasingly serious problem in major cities and caused grave public health concerns in China. Recently, a number of studies have dealt with air quality and air pollution. Among them, some attempt to predict and monitor the air quality from different sources of information, ranging from deployed physical sensors to social media. These methods are either too expensive or unreliable, prompting us to search for a novel and effective way to sense the air quality. In this study, we propose to employ the state of the art in computer vision techniques to analyze photos that can be easily acquired from online social media. Next, we establish the correlation between the haze level computed directly from photos with the official PM 2.5 record of the taken city at the taken time. Our experiments based on both synthetic and real photos have shown the promise of this image-based approach to estimating and monitoring air pollution. version:1
arxiv-1508-05003 | AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization | http://arxiv.org/abs/1508.05003 | id:1508.05003 author:Suvrit Sra, Adams Wei Yu, Mu Li, Alexander J. Smola category:stat.ML cs.LG math.OC  published:2015-08-20 summary:We study distributed stochastic convex optimization under the delayed gradient model where the server nodes perform parameter updates, while the worker nodes compute stochastic gradients. We discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time. Therefore, we allow the parameter updates to be sensitive to the actual delays experienced, rather than to worst-case bounds on the maximum delay. This sensitivity leads to larger stepsizes, that can help gain rapid initial convergence without having to wait too long for slower machines, while maintaining the same asymptotic complexity. We obtain encouraging improvements to overall convergence for distributed experiments on real datasets with up to billions of examples and features. version:1
arxiv-1508-04999 | A Deep Bag-of-Features Model for Music Auto-Tagging | http://arxiv.org/abs/1508.04999 | id:1508.04999 author:Juhan Nam, Jorge Herrera, Kyogu Lee category:cs.LG cs.SD stat.ML  published:2015-08-20 summary:Feature learning and deep learning have drawn great attention in recent years as a way of transforming input data into more effective representations using learning algorithms. Such interest has grown up in the area of music information retrieval (MIR) as well, particularly in music classification tasks such as auto-tagging. While a number of promising results have been shown, it is not well understood what acoustic sense the learned feature representations have and how they are associated with semantic meaning of music. In this paper, we attempt to demystify the learned audio features using a bag-of-features model with two learning stages. The first stage learns to project local acoustic patterns of musical signals onto a high-dimensional sparse space in an unsupervised manner and summarizes an audio track as a bag-of-features. The second stage maps the bag-of-features to semantic tags using deep neural networks in a supervised manner. For the first stage, we focus on analyzing the learned local audio features by quantitatively measuring the acoustic properties and interpreting the statistics in semantic context. For the second stage, we examine training choices and tuning parameters for the neural networks and show how the deep representations of bag-of-features become more discriminative. Through this analysis, we not only provide better understanding of learned local audio features but also show the effectiveness of the deep bag-of-features model in the music auto-tagging task. version:1
arxiv-1508-04981 | High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging | http://arxiv.org/abs/1508.04981 | id:1508.04981 author:Changsoo Je, Sang Wook Lee, Rae-Hong Park category:cs.CV cs.GR physics.optics I.2.10; I.4.8  published:2015-08-20 summary:For structured-light range imaging, color stripes can be used for increasing the number of distinguishable light patterns compared to binary BW stripes. Therefore, an appropriate use of color patterns can reduce the number of light projections and range imaging is achievable in single video frame or in "one shot". On the other hand, the reliability and range resolution attainable from color stripes is generally lower than those from multiply projected binary BW patterns since color contrast is affected by object color reflectance and ambient light. This paper presents new methods for selecting stripe colors and designing multiple-stripe patterns for "one-shot" and "two-shot" imaging. We show that maximizing color contrast between the stripes in one-shot imaging reduces the ambiguities resulting from colored object surfaces and limitations in sensor/projector resolution. Two-shot imaging adds an extra video frame and maximizes the color contrast between the first and second video frames to diminish the ambiguities even further. Experimental results demonstrate the effectiveness of the presented one-shot and two-shot color-stripe imaging schemes. version:1
arxiv-1508-04955 | Introducing Geometry in Active Learning for Image Segmentation | http://arxiv.org/abs/1508.04955 | id:1508.04955 author:Ksenia Konyushkova, Raphael Sznitman, Pascal Fua category:cs.CV  published:2015-08-20 summary:We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3D image volumes. To this end, we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images. We evaluated our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images. Comparing our approach against several accepted baselines demonstrates a marked performance increase. version:1
arxiv-1508-04912 | The ABACOC Algorithm: a Novel Approach for Nonparametric Classification of Data Streams | http://arxiv.org/abs/1508.04912 | id:1508.04912 author:Rocco De Rosa, Francesco Orabona, Nicolò Cesa-Bianchi category:stat.ML cs.LG  published:2015-08-20 summary:Stream mining poses unique challenges to machine learning: predictive models are required to be scalable, incrementally trainable, must remain bounded in size (even when the data stream is arbitrarily long), and be nonparametric in order to achieve high accuracy even in complex and dynamic environments. Moreover, the learning system must be parameterless ---traditional tuning methods are problematic in streaming settings--- and avoid requiring prior knowledge of the number of distinct class labels occurring in the stream. In this paper, we introduce a new algorithmic approach for nonparametric learning in data streams. Our approach addresses all above mentioned challenges by learning a model that covers the input space using simple local classifiers. The distribution of these classifiers dynamically adapts to the local (unknown) complexity of the classification problem, thus achieving a good balance between model complexity and predictive accuracy. We design four variants of our approach of increasing adaptivity. By means of an extensive empirical evaluation against standard nonparametric baselines, we show state-of-the-art results in terms of accuracy versus model size. For the variant that imposes a strict bound on the model size, we show better performance against all other methods measured at the same model size value. Our empirical analysis is complemented by a theoretical performance guarantee which does not rely on any stochastic assumption on the source generating the stream. version:1
arxiv-1508-04909 | Histogram of gradients of Time-Frequency Representations for Audio scene detection | http://arxiv.org/abs/1508.04909 | id:1508.04909 author:Alain Rakotomamonjy, Gilles Gasso category:cs.SD cs.LG  published:2015-08-20 summary:This paper addresses the problem of audio scenes classification and contributes to the state of the art by proposing a novel feature. We build this feature by considering histogram of gradients (HOG) of time-frequency representation of an audio scene. Contrarily to classical audio features like MFCC, we make the hypothesis that histogram of gradients are able to encode some relevant informations in a time-frequency {representation:} namely, the local direction of variation (in time and frequency) of the signal spectral power. In addition, in order to gain more invariance and robustness, histogram of gradients are locally pooled. We have evaluated the relevance of {the novel feature} by comparing its performances with state-of-the-art competitors, on several datasets, including a novel one that we provide, as part of our contribution. This dataset, that we make publicly available, involves $19$ classes and contains about $900$ minutes of audio scene recording. We thus believe that it may be the next standard dataset for evaluating audio scene classification algorithms. Our comparison results clearly show that our HOG-based features outperform its competitors version:1
arxiv-1508-04906 | Semi-supervised Learning with Regularized Laplacian | http://arxiv.org/abs/1508.04906 | id:1508.04906 author:Konstantin Avrachenkov, Pavel Chebotarev, Alexey Mishenin category:cs.LG  published:2015-08-20 summary:We study a semi-supervised learning method based on the similarity graph and RegularizedLaplacian. We give convenient optimization formulation of the Regularized Laplacian method and establishits various properties. In particular, we show that the kernel of the methodcan be interpreted in terms of discrete and continuous time random walks and possesses several importantproperties of proximity measures. Both optimization and linear algebra methods can be used for efficientcomputation of the classification functions. We demonstrate on numerical examples that theRegularized Laplacian method is competitive with respect to the other state of the art semi-supervisedlearning methods. version:1
arxiv-1508-04904 | Review and Perspective for Distance Based Trajectory Clustering | http://arxiv.org/abs/1508.04904 | id:1508.04904 author:Philippe Besse, Brendan Guillouet, Jean-Michel Loubes, Royer François category:stat.ML cs.LG stat.AP  published:2015-08-20 summary:In this paper we tackle the issue of clustering trajectories of geolocalized observations. Using clustering technics based on the choice of a distance between the observations, we first provide a comprehensive review of the different distances used in the literature to compare trajectories. Then based on the limitations of these methods, we introduce a new distance : Symmetrized Segment-Path Distance (SSPD). We finally compare this new distance to the others according to their corresponding clustering results obtained using both hierarchical clustering and affinity propagation methods. version:1
arxiv-1506-00196 | Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion | http://arxiv.org/abs/1506.00196 | id:1506.00196 author:Kaisheng Yao, Geoffrey Zweig category:cs.CL  published:2015-05-31 summary:Sequence-to-sequence translation methods based on generation with a side-conditioned language model have recently shown promising results in several tasks. In machine translation, models conditioned on source side words have been used to produce target-language text, and in image captioning, models conditioned images have been used to generate caption text. Past work with this approach has focused on large vocabulary tasks, and measured quality in terms of BLEU. In this paper, we explore the applicability of such models to the qualitatively different grapheme-to-phoneme task. Here, the input and output side vocabularies are small, plain n-gram models do well, and credit is only given when the output is exactly correct. We find that the simple side-conditioned generation approach is able to rival the state-of-the-art, and we are able to significantly advance the stat-of-the-art with bi-directional long short-term memory (LSTM) neural networks that use the same alignment information that is used in conventional approaches. version:3
arxiv-1508-04887 | Multi-criteria Similarity-based Anomaly Detection using Pareto Depth Analysis | http://arxiv.org/abs/1508.04887 | id:1508.04887 author:Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, Alfred O. Hero III category:cs.CV cs.LG stat.ML  published:2015-08-20 summary:We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. Similarity-based anomaly detection algorithms detect abnormally large amounts of similarity or dissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between a test sample and the training samples. In many application domains there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such cases, multiple dissimilarity measures can be defined, including non-metric measures, and one can test for anomalies by scalarizing using a non-negative linear combination of them. If the relative importance of the different dissimilarity measures are not known in advance, as in many anomaly detection applications, the anomaly detection algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we propose a method for similarity-based anomaly detection using a novel multi-criteria dissimilarity measure, the Pareto depth. The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach is provably better than using linear combinations of the criteria and shows superior performance on experiments with synthetic and real data sets. version:1
arxiv-1508-04843 | Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection | http://arxiv.org/abs/1508.04843 | id:1508.04843 author:Kisuk Lee, Aleksandar Zlateski, Ashwin Vishwanathan, H. Sebastian Seung category:cs.CV  published:2015-08-20 summary:Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem. version:1
arxiv-1508-01211 | Listen, Attend and Spell | http://arxiv.org/abs/1508.01211 | id:1508.01211 author:William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals category:cs.CL cs.LG cs.NE stat.ML  published:2015-08-05 summary:We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%. version:2
arxiv-1507-07909 | Offline Handwritten Signature Verification - Literature Review | http://arxiv.org/abs/1507.07909 | id:1507.07909 author:Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira category:cs.CV stat.ML I.5.4  published:2015-07-28 summary:The area of Handwritten Signature Verification has been broadly researched in the last decades and still remains as an open research problem. This report focuses on offline signature verification, characterized by the usage of static (scanned) images of signatures, where the objective is to discriminate if a given signature is genuine (produced by the claimed individual), or a forgery (produced by an impostor). We present an overview of how the problem has been handled by several researchers in the past few decades and the recent advancements in the field. version:2
arxiv-1508-04785 | Who are the Devils Wearing Prada in New York City? | http://arxiv.org/abs/1508.04785 | id:1508.04785 author:KuanTing Chen, Kezhen Chen, Peizhong Cong, Winston H. Hsu, Jiebo Luo category:cs.CV cs.CY  published:2015-08-19 summary:Fashion is a perpetual topic in human social life, and the mass has the penchant to emulate what large city residents and celebrities wear. Undeniably, New York City is such a bellwether large city with all kinds of fashion leadership. Consequently, to study what the fashion trends are during this year, it is very helpful to learn the fashion trends of New York City. Discovering fashion trends in New York City could boost many applications such as clothing recommendation and advertising. Does the fashion trend in the New York Fashion Show actually influence the clothing styles on the public? To answer this question, we design a novel system that consists of three major components: (1) constructing a large dataset from the New York Fashion Shows and New York street chic in order to understand the likely clothing fashion trends in New York, (2) utilizing a learning-based approach to discover fashion attributes as the representative characteristics of fashion trends, and (3) comparing the analysis results from the New York Fashion Shows and street-chic images to verify whether the fashion shows have actual influence on the people in New York City. Through the preliminary experiments over a large clothing dataset, we demonstrate the effectiveness of our proposed system, and obtain useful insights on fashion trends and fashion influence. version:1
arxiv-1508-04757 | Time Series Clustering via Community Detection in Networks | http://arxiv.org/abs/1508.04757 | id:1508.04757 author:Leonardo N. Ferreira, Liang Zhao category:stat.ML cs.LG cs.SI  published:2015-08-19 summary:In this paper, we propose a technique for time series clustering using community detection in complex networks. Firstly, we present a method to transform a set of time series into a network using different distance functions, where each time series is represented by a vertex and the most similar ones are connected. Then, we apply community detection algorithms to identify groups of strongly connected vertices (called a community) and, consequently, identify time series clusters. Still in this paper, we make a comprehensive analysis on the influence of various combinations of time series distance functions, network generation methods and community detection techniques on clustering results. Experimental study shows that the proposed network-based approach achieves better results than various classic or up-to-date clustering techniques under consideration. Statistical tests confirm that the proposed method outperforms some classic clustering algorithms, such as $k$-medoids, diana, median-linkage and centroid-linkage in various data sets. Interestingly, the proposed method can effectively detect shape patterns presented in time series due to the topological structure of the underlying network constructed in the clustering process. At the same time, other techniques fail to identify such patterns. Moreover, the proposed method is robust enough to group time series presenting similar pattern but with time shifts and/or amplitude variations. In summary, the main point of the proposed method is the transformation of time series from time-space domain to topological domain. Therefore, we hope that our approach contributes not only for time series clustering, but also for general time series analysis tasks. version:1
arxiv-1508-04734 | Fault Diagnosis of Helical Gear Box using Large Margin K-Nearest Neighbors Classifier using Sound Signals | http://arxiv.org/abs/1508.04734 | id:1508.04734 author:M. Amarnath, S. Arunav, Hemantha Kumar, V. Sugumaran, G. S Raghvendra category:cs.LG  published:2015-08-19 summary:Gear drives are one of the most widely used transmission system in many machinery. Sound signals of a rotating machine contain the dynamic information about its health conditions. Not much information available in the literature reporting suitability of sound signals for fault diagnosis applications. Maximum numbers of literature are based on FFT (Fast Fourier Transform) analysis and have its own limitations with non-stationary signals like the ones from gears. In this paper, attempt has been made in using sound signals acquired from gears in good and simulated faulty conditions for the purpose of fault diagnosis through a machine learning approach. The descriptive statistical features were extracted from the acquired sound signals and the predominant features were selected using J48 decision tree technique. The selected features were then used for classification using Large Margin K-nearest neighbor approach. The paper also discusses the effect of various parameters on classification accuracy. version:1
arxiv-1508-04586 | Saliency maps on image hierarchies | http://arxiv.org/abs/1508.04586 | id:1508.04586 author:Verónica Vilaplana category:cs.CV  published:2015-08-19 summary:In this paper we propose two saliency models for salient object segmentation based on a hierarchical image segmentation, a tree-like structure that represents regions at different scales from the details to the whole image (e.g. gPb-UCM, BPT). The first model is based on a hierarchy of image partitions. The saliency at each level is computed on a region basis, taking into account the contrast between regions. The maps obtained for the different partitions are then integrated into a final saliency map. The second model directly works on the structure created by the segmentation algorithm, computing saliency at each node and integrating these cues in a straightforward manner into a single saliency map. We show that the proposed models produce high quality saliency maps. Objective evaluation demonstrates that the two methods achieve state-of-the-art performance in several benchmark datasets. version:1
arxiv-1508-04582 | Learning to Predict Independent of Span | http://arxiv.org/abs/1508.04582 | id:1508.04582 author:Hado van Hasselt, Richard S. Sutton category:cs.LG  published:2015-08-19 summary:We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the `how' to the `why'. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning. version:1
arxiv-1508-04562 | Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections | http://arxiv.org/abs/1508.04562 | id:1508.04562 author:Jingwei Zhang, Aaron Gerow, Jaan Altosaar, James Evans, Richard Jean So category:cs.CL cs.IR  published:2015-08-19 summary:Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics. To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR. version:1
arxiv-1508-04559 | Introduction to Cross-Entropy Clustering The R Package CEC | http://arxiv.org/abs/1508.04559 | id:1508.04559 author:Jacek Tabor, Przemysław Spurek, Konrad Kamieniecki, Marek Śmieja, Krzysztof Misztal category:cs.LG stat.ME stat.ML  published:2015-08-19 summary:The R Package CEC performs clustering based on the cross-entropy clustering (CEC) method, which was recently developed with the use of information theory. The main advantage of CEC is that it combines the speed and simplicity of $k$-means with the ability to use various Gaussian mixture models and reduce unnecessary clusters. In this work we present a practical tutorial to CEC based on the R Package CEC. Functions are provided to encompass the whole process of clustering. version:1
arxiv-1508-04556 | Spatio-temporal Spike and Slab Priors for Multiple Measurement Vector Problems | http://arxiv.org/abs/1508.04556 | id:1508.04556 author:Michael Riis Andersen, Ole Winther, Lars Kai Hansen category:stat.ML  published:2015-08-19 summary:We are interested in solving the multiple measurement vector (MMV) problem for instances, where the underlying sparsity pattern exhibit spatio-temporal structure motivated by the electroencephalogram (EEG) source localization problem. We propose a probabilistic model that takes this structure into account by generalizing the structured spike and slab prior and the associated Expectation Propagation inference scheme. Based on numerical experiments, we demonstrate the viability of the model and the approximate inference scheme. version:1
arxiv-1508-04554 | Mining Brain Networks using Multiple Side Views for Neurological Disorder Identification | http://arxiv.org/abs/1508.04554 | id:1508.04554 author:Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu, Ann B. Ragin category:cs.LG cs.CV cs.CY stat.AP stat.ML  published:2015-08-19 summary:Mining discriminative subgraph patterns from graph data has attracted great interest in recent years. It has a wide variety of applications in disease diagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the graph representation alone. However, in many real-world applications, the side information is available along with the graph data. For example, for neurological disorder identification, in addition to the brain networks derived from neuroimaging data, hundreds of clinical, immunologic, serologic and cognitive measures may also be documented for each subject. These measures compose multiple side views encoding a tremendous amount of supplemental information for diagnostic purposes, yet are often ignored. In this paper, we study the problem of discriminative subgraph selection using multiple side views and propose a novel solution to find an optimal set of subgraph features for graph classification by exploring a plurality of side views. We derive a feature evaluation criterion, named gSide, to estimate the usefulness of subgraph patterns based upon side views. Then we develop a branch-and-bound algorithm, called gMSV, to efficiently search for optimal subgraph features by integrating the subgraph mining process and the procedure of discriminative feature selection. Empirical studies on graph classification tasks for neurological disorders using brain networks demonstrate that subgraph patterns selected by the multi-side-view guided subgraph selection approach can effectively boost graph classification performances and are relevant to disease diagnosis. version:1
arxiv-1508-04546 | Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images | http://arxiv.org/abs/1508.04546 | id:1508.04546 author:Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang, Stefan Gumhold, Carsten Rother category:cs.CV 65-XX  published:2015-08-19 summary:Analysis-by-synthesis has been a successful approach for many tasks in computer vision, such as 6D pose estimation of an object in an RGB-D image which is the topic of this work. The idea is to compare the observation with the output of a forward process, such as a rendered image of the object of interest in a particular pose. Due to occlusion or complicated sensor noise, it can be difficult to perform this comparison in a meaningful way. We propose an approach that "learns to compare", while taking these difficulties into account. This is done by describing the posterior density of a particular object pose with a convolutional neural network (CNN) that compares an observed and rendered image. The network is trained with the maximum likelihood paradigm. We observe empirically that the CNN does not specialize to the geometry or appearance of specific objects, and it can be used with objects of vastly different shapes and appearances, and in different backgrounds. Compared to state-of-the-art, we demonstrate a significant improvement on two different datasets which include a total of eleven objects, cluttered background, and heavy occlusion. version:1
arxiv-1506-01094 | Traversing Knowledge Graphs in Vector Space | http://arxiv.org/abs/1506.01094 | id:1506.01094 author:Kelvin Guu, John Miller, Percy Liang category:cs.CL cs.AI cs.DB stat.ML  published:2015-06-03 summary:Path queries on a knowledge graph can be used to answer compositional questions such as "What languages are spoken by people living in Lisbon?". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new "compositional" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results. version:2
arxiv-1506-06833 | A Survey of Current Datasets for Vision and Language Research | http://arxiv.org/abs/1506.06833 | id:1506.06833 author:Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao, Huang, Lucy Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell category:cs.CL cs.AI cs.CV cs.GL  published:2015-06-23 summary:Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. version:2
arxiv-1508-04525 | Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles | http://arxiv.org/abs/1508.04525 | id:1508.04525 author:Wei Zhang, Yang Yu, Osho Gupta, Judith Gelernter category:cs.CL cs.LG D.3.3  published:2015-08-19 summary:Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases. We collected and annotated data set by querying commercial web searches API with such spatiotemporal expressions as were missed by state-of-the- art parsers. Due to the high cost of sentence annotation, active learning was used to label training data, and a new strategy was designed to better select training examples to reduce labeling cost. For the learning algorithm, we applied an average perceptron trained Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create an ensemble, with the output phrase selected by voting. Our ensemble model was tested on a range of sequential labeling tasks, and has shown competitive performance. Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that 'memorizes' the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities. version:1
arxiv-1508-04515 | Exploring Metaphorical Senses and Word Representations for Identifying Metonyms | http://arxiv.org/abs/1508.04515 | id:1508.04515 author:Wei Zhang, Judith Gelernter category:cs.CL I.2.7  published:2015-08-19 summary:A metonym is a word with a figurative meaning, similar to a metaphor. Because metonyms are closely related to metaphors, we apply features that are used successfully for metaphor recognition to the task of detecting metonyms. On the ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system achieved 86.45% accuracy on the location metonyms. Our code can be found on GitHub. version:1
arxiv-1508-04486 | A Dictionary Learning Approach for Factorial Gaussian Models | http://arxiv.org/abs/1508.04486 | id:1508.04486 author:Y. Cem Subakan, Johannes Traa, Paris Smaragdis, Noah Stein category:cs.LG stat.ML  published:2015-08-18 summary:In this paper, we develop a parameter estimation method for factorially parametrized models such as Factorial Gaussian Mixture Model and Factorial Hidden Markov Model. Our contributions are two-fold. First, we show that the emission matrix of the standard Factorial Model is unidentifiable even if the true assignment matrix is known. Secondly, we address the issue of identifiability by making a one component sharing assumption and derive a parameter learning algorithm for this case. Our approach is based on a dictionary learning problem of the form $X = O R$, where the goal is to learn the dictionary $O$ given the data matrix $X$. We argue that due to the specific structure of the activation matrix $R$ in the shared component factorial mixture model, and an incoherence assumption on the shared component, it is possible to extract the columns of the $O$ matrix without the need for alternating between the estimation of $O$ and $R$. version:1
arxiv-1508-04467 | Robust Subspace Clustering via Smoothed Rank Approximation | http://arxiv.org/abs/1508.04467 | id:1508.04467 author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.IT cs.LG cs.NA math.IT stat.ML  published:2015-08-18 summary:Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms. version:1
arxiv-1508-04409 | ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R | http://arxiv.org/abs/1508.04409 | id:1508.04409 author:Marvin N. Wright, Andreas Ziegler category:stat.ML stat.CO  published:2015-08-18 summary:We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study. version:1
arxiv-1508-03285 | Hash Function Learning via Codewords | http://arxiv.org/abs/1508.03285 | id:1508.03285 author:Yinjie Huang, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2015-08-13 summary:In this paper we introduce a novel hash learning framework that has two main distinguishing features, when compared to past approaches. First, it utilizes codewords in the Hamming space as ancillary means to accomplish its hash learning task. These codewords, which are inferred from the data, attempt to capture similarity aspects of the data's hash codes. Secondly and more importantly, the same framework is capable of addressing supervised, unsupervised and, even, semi-supervised hash learning tasks in a natural manner. A series of comparative experiments focused on content-based image retrieval highlights its performance advantages. version:2
arxiv-1508-04389 | A Deep Pyramid Deformable Part Model for Face Detection | http://arxiv.org/abs/1508.04389 | id:1508.04389 author:Rajeev Ranjan, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2015-08-18 summary:We present a face detection algorithm based on Deformable Part Models and deep pyramidal features. The proposed method called DP2MFD is able to detect faces of various sizes and poses in unconstrained conditions. It reduces the gap in training and testing of DPM on deep features by adding a normalization layer to the deep convolutional neural network (CNN). Extensive experiments on four publicly available unconstrained face detection datasets show that our method is able to capture the meaningful structure of faces and performs significantly better than many competitive face detection algorithms. version:1
arxiv-1508-04333 | ESDF: Ensemble Selection using Diversity and Frequency | http://arxiv.org/abs/1508.04333 | id:1508.04333 author:Shouvick Mondal, Arko Banerjee category:cs.LG 62H30  published:2015-08-18 summary:Recently ensemble selection for consensus clustering has emerged as a research problem in Machine Intelligence. Normally consensus clustering algorithms take into account the entire ensemble of clustering, where there is a tendency of generating a very large size ensemble before computing its consensus. One can avoid considering the entire ensemble and can judiciously select few partitions in the ensemble without compromising on the quality of the consensus. This may result in an efficient consensus computation technique and may save unnecessary computational overheads. The ensemble selection problem addresses this issue of consensus clustering. In this paper, we propose an efficient method of ensemble selection for a large ensemble. We prioritize the partitions in the ensemble based on diversity and frequency. Our method selects top K of the partitions in order of priority, where K is decided by the user. We observe that considering jointly the diversity and frequency helps in identifying few representative partitions whose consensus is qualitatively better than the consensus of the entire ensemble. Experimental analysis on a large number of datasets shows our method gives better results than earlier ensemble selection methods. version:1
arxiv-1508-04326 | Cascade Learning by Optimally Partitioning | http://arxiv.org/abs/1508.04326 | id:1508.04326 author:Yanwei Pang, Jiale Cao, Xuelong Li category:cs.CV cs.LG  published:2015-08-18 summary:Cascaded AdaBoost classifier is a well-known efficient object detection algorithm. The cascade structure has many parameters to be determined. Most of existing cascade learning algorithms are designed by assigning detection rate and false positive rate to each stage either dynamically or statically. Their objective functions are not directly related to minimum computation cost. These algorithms are not guaranteed to have optimal solution in the sense of minimizing computation cost. On the assumption that a strong classifier is given, in this paper we propose an optimal cascade learning algorithm (we call it iCascade) which iteratively partitions the strong classifiers into two parts until predefined number of stages are generated. iCascade searches the optimal number ri of weak classifiers of each stage i by directly minimizing the computation cost of the cascade. Theorems are provided to guarantee the existence of the unique optimal solution. Theorems are also given for the proposed efficient algorithm of searching optimal parameters ri. Once a new stage is added, the parameter ri for each stage decreases gradually as iteration proceeds, which we call decreasing phenomenon. Moreover, with the goal of minimizing computation cost, we develop an effective algorithm for setting the optimal threshold of each stage classifier. In addition, we prove in theory why more new weak classifiers are required compared to the last stage. Experimental results on face detection demonstrate the effectiveness and efficiency of the proposed algorithm. version:1
arxiv-1508-02844 | What is Holding Back Convnets for Detection? | http://arxiv.org/abs/1508.02844 | id:1508.02844 author:Bojan Pepik, Rodrigo Benenson, Tobias Ritschel, Bernt Schiele category:cs.CV  published:2015-08-12 summary:Convolutional neural networks have recently shown excellent results in general object detection and many other tasks. Albeit very effective, they involve many user-defined design choices. In this paper we want to better understand these choices by inspecting two key aspects "what did the network learn?", and "what can the network learn?". We exploit new annotations (Pascal3D+), to enable a new empirical analysis of the R-CNN detector. Despite common belief, our results indicate that existing state-of-the-art convnet architectures are not invariant to various appearance factors. In fact, all considered networks have similar weak points which cannot be mitigated by simply increasing the training data (architectural changes are needed). We show that overall performance can improve when using image renderings for data augmentation. We report the best known results on the Pascal3D+ detection and view-point estimation tasks. version:2
arxiv-1508-04319 | Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo | http://arxiv.org/abs/1508.04319 | id:1508.04319 author:Markus Heinonen, Henrik Mannerström, Juho Rousu, Samuel Kaski, Harri Lähdesmäki category:stat.ML  published:2015-08-18 summary:We present a novel approach for fully non-stationary Gaussian process regression (GPR), where all three key parameters -- noise variance, signal variance and lengthscale -- can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. We propose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with model gradients. We also learn the MAP solution from the posterior by gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the nonstationary GPR is shown to be necessary for modeling realistic input-dependent dynamics, while it performs comparably to conventional stationary or previous non-stationary GPR models otherwise. version:1
arxiv-1508-04306 | Deep clustering: Discriminative embeddings for segmentation and separation | http://arxiv.org/abs/1508.04306 | id:1508.04306 author:John R. Hershey, Zhuo Chen, Jonathan Le Roux, Shinji Watanabe category:cs.NE cs.LG stat.ML  published:2015-08-18 summary:We address the problem of acoustic source separation in a deep learning framework we call "deep clustering." Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be "decoded" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains. version:1
arxiv-1507-06821 | Multimodal Deep Learning for Robust RGB-D Object Recognition | http://arxiv.org/abs/1507.06821 | id:1507.06821 author:Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello, Martin Riedmiller, Wolfram Burgard category:cs.CV cs.LG cs.NE cs.RO  published:2015-07-24 summary:Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings. version:2
arxiv-1508-03391 | Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems | http://arxiv.org/abs/1508.03391 | id:1508.03391 author:Pei-Hao Su, David Vandyke, Milica Gasic, Nikola Mrksic, Tsung-Hsien Wen, Steve Young category:cs.LG cs.CL  published:2015-08-14 summary:Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these RNNs are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user's goal. version:2
arxiv-1508-04271 | Probabilistic Modelling of Morphologically Rich Languages | http://arxiv.org/abs/1508.04271 | id:1508.04271 author:Jan A. Botha category:cs.CL I.2.7; I.2.6  published:2015-08-18 summary:This thesis investigates how the sub-structure of words can be accounted for in probabilistic models of language. Such models play an important role in natural language processing tasks such as translation or speech recognition, but often rely on the simplistic assumption that words are opaque symbols. This assumption does not fit morphologically complex language well, where words can have rich internal structure and sub-word elements are shared across distinct word forms. Our approach is to encode basic notions of morphology into the assumptions of three different types of language models, with the intention that leveraging shared sub-word structure can improve model performance and help overcome data sparsity that arises from morphological processes. In the context of n-gram language modelling, we formulate a new Bayesian model that relies on the decomposition of compound words to attain better smoothing, and we develop a new distributed language model that learns vector representations of morphemes and leverages them to link together morphologically related words. In both cases, we show that accounting for word sub-structure improves the models' intrinsic performance and provides benefits when applied to other tasks, including machine translation. We then shift the focus beyond the modelling of word sequences and consider models that automatically learn what the sub-word elements of a given language are, given an unannotated list of words. We formulate a novel model that can learn discontiguous morphemes in addition to the more conventional contiguous morphemes that most previous models are limited to. This approach is demonstrated on Semitic languages, and we find that modelling discontiguous sub-word structures leads to improvements in the task of segmenting words into their contiguous morphemes. version:1
arxiv-1508-04238 | Preprint ARPPS Augmented Reality Pipeline Prospect System | http://arxiv.org/abs/1508.04238 | id:1508.04238 author:Xiaolei Zhang, Yong Han, DongSheng Hao, Zhihan Lv category:cs.CV  published:2015-08-18 summary:This is the preprint version of our paper on ICONIP. Outdoor augmented reality geographic information system (ARGIS) is the hot application of augmented reality over recent years. This paper concludes the key solutions of ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS), and respectively realizes the machine vision based pipeline prospect system (MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the MVBPPS's realization, this paper studies the neural network based 3D features matching method. version:1
arxiv-1412-5808 | Minimizing the Number of Matching Queries for Object Retrieval | http://arxiv.org/abs/1412.5808 | id:1412.5808 author:Johannes Niedermayer, Peer Kröger category:cs.CV  published:2014-12-18 summary:To increase the computational efficiency of interest-point based object retrieval, researchers have put remarkable research efforts into improving the efficiency of kNN-based feature matching, pursuing to match thousands of features against a database within fractions of a second. However, due to the high-dimensional nature of image features that reduces the effectivity of index structures (curse of dimensionality), due to the vast amount of features stored in image databases (images are often represented by up to several thousand features), this ultimate goal demanded to trade query runtimes for query precision. In this paper we address an approach complementary to indexing in order to improve the runtimes of retrieval by querying only the most promising keypoint descriptors, as this affects matching runtimes linearly and can therefore lead to increased efficiency. As this reduction of kNN queries reduces the number of tentative correspondences, a loss of query precision is minimized by an additional image-level correspondence generation stage with a computational performance independent of the underlying indexing structure. We evaluate such an adaption of the standard recognition pipeline on a variety of datasets using both SIFT and state-of-the-art binary descriptors. Our results suggest that decreasing the number of queried descriptors does not necessarily imply a reduction in the result quality as long as alternative ways of increasing query recall (by thoroughly selecting k) and MAP (using image-level correspondence generation) are considered. version:3
arxiv-1508-04224 | Image tag completion by local learning | http://arxiv.org/abs/1508.04224 | id:1508.04224 author:Jingyan Wang, Yihua Zhou, Haoxiang Wang, Xiaohong Yang, Feng Yang, Austin Peterson category:cs.CV  published:2015-08-18 summary:The problem of tag completion is to learn the missing tags of an image. In this paper, we propose to learn a tag scoring vector for each image by local linear learning. A local linear function is used in the neighborhood of each image to predict the tag scoring vectors of its neighboring images. We construct a unified objective function for the learning of both tag scoring vectors and local linear function parame- ters. In the objective, we impose the learned tag scoring vectors to be consistent with the known associations to the tags of each image, and also minimize the prediction error of each local linear function, while reducing the complexity of each local function. The objective function is optimized by an alternate optimization strategy and gradient descent methods in an iterative algorithm. We compare the proposed algorithm against different state-of-the-art tag completion methods, and the results show its advantages. version:1
arxiv-1507-00019 | Representing data by sparse combination of contextual data points for classification | http://arxiv.org/abs/1507.00019 | id:1507.00019 author:Jingyan Wang, Yihua Zhou, Ming Yin, Shaochang Chen, Benjamin Edwards category:cs.CV  published:2015-06-30 summary:In this paper, we study the problem of using contextual da- ta points of a data point for its classification problem. We propose to represent a data point as the sparse linear reconstruction of its context, and learn the sparse context to gather with a linear classifier in a su- pervised way to increase its discriminative ability. We proposed a novel formulation for context learning, by modeling the learning of context reconstruction coefficients and classifier in a unified objective. In this objective, the reconstruction error is minimized and the coefficient spar- sity is encouraged. Moreover, the hinge loss of the classifier is minimized and the complexity of the classifier is reduced. This objective is opti- mized by an alternative strategy in an iterative algorithm. Experiments on three benchmark data set show its advantage over state-of-the-art context-based data representation and classification methods. version:2
arxiv-1503-00185 | When Are Tree Structures Necessary for Deep Learning of Representations? | http://arxiv.org/abs/1503.00185 | id:1503.00185 author:Jiwei Li, Minh-Thang Luong, Dan Jurafsky, Eudard Hovy category:cs.AI cs.CL  published:2015-02-28 summary:Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. But there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper we benchmark {\bf recursive} neural models against sequential {\bf recurrent} neural models (simple recurrent and LSTM models), enforcing apples-to-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answer-phrases; (3) discourse parsing; (4) semantic relation extraction (e.g., {\em component-whole} between nouns). Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require associating headwords across a long distance, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. version:5
arxiv-1508-04221 | Supervised learning of sparse context reconstruction coefficients for data representation and classification | http://arxiv.org/abs/1508.04221 | id:1508.04221 author:Xuejie Liu, Jingbin Wang, Ming Yin, Benjamin Edwards, Peijuan Xu category:cs.LG cs.CV  published:2015-08-18 summary:Context of data points, which is usually defined as the other data points in a data set, has been found to play important roles in data representation and classification. In this paper, we study the problem of using context of a data point for its classification problem. Our work is inspired by the observation that actually only very few data points are critical in the context of a data point for its representation and classification. We propose to represent a data point as the sparse linear combination of its context, and learn the sparse context in a supervised way to increase its discriminative ability. To this end, we proposed a novel formulation for context learning, by modeling the learning of context parameter and classifier in a unified objective, and optimizing it with an alternative strategy in an iterative algorithm. Experiments on three benchmark data set show its advantage over state-of-the-art context-based data representation and classification methods. version:1
arxiv-1502-05134 | Supervised cross-modal factor analysis for multiple modal data classification | http://arxiv.org/abs/1502.05134 | id:1502.05134 author:Jingbin Wang, Yihua Zhou, Kanghong Duan, Jim Jing-Yan Wang, Halima Bensmail category:cs.LG  published:2015-02-18 summary:In this paper we study the problem of learning from multiple modal data for purpose of document classification. In this problem, each document is composed two different modals of data, i.e., an image and a text. Cross-modal factor analysis (CFA) has been proposed to project the two different modals of data to a shared data space, so that the classification of a image or a text can be performed directly in this space. A disadvantage of CFA is that it has ignored the supervision information. In this paper, we improve CFA by incorporating the supervision information to represent and classify both image and text modals of documents. We project both image and text data to a shared data space by factor analysis, and then train a class label predictor in the shared space to use the class label information. The factor analysis parameter and the predictor parameter are learned jointly by solving one single objective function. With this objective function, we minimize the distance between the projections of image and text of the same document, and the classification error of the projection measured by hinge loss function. The objective function is optimized by an alternate optimization strategy in an iterative algorithm. Experiments in two different multiple modal document data sets show the advantage of the proposed algorithm over other CFA methods. version:2
arxiv-1508-04211 | Scalable Bayesian Non-Negative Tensor Factorization for Massive Count Data | http://arxiv.org/abs/1508.04211 | id:1508.04211 author:Changwei Hu, Piyush Rai, Changyou Chen, Matthew Harding, Lawrence Carin category:stat.ML cs.LG  published:2015-08-18 summary:We present a Bayesian non-negative tensor factorization model for count-valued tensor data, and develop scalable inference algorithms (both batch and online) for dealing with massive tensors. Our generative model can handle overdispersed counts as well as infer the rank of the decomposition. Moreover, leveraging a reparameterization of the Poisson distribution as a multinomial facilitates conjugacy in the model and enables simple and efficient Gibbs sampling and variational Bayes (VB) inference updates, with a computational cost that only depends on the number of nonzeros in the tensor. The model also provides a nice interpretability for the factors; in our model, each factor corresponds to a "topic". We develop a set of online inference algorithms that allow further scaling up the model to massive tensors, for which batch inference methods may be infeasible. We apply our framework on diverse real-world applications, such as \emph{multiway} topic modeling on a scientific publications database, analyzing a political science data set, and analyzing a massive household transactions data set. version:1
arxiv-1508-04210 | Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors | http://arxiv.org/abs/1508.04210 | id:1508.04210 author:Changwei Hu, Piyush Rai, Lawrence Carin category:stat.ML cs.LG  published:2015-08-18 summary:We present a scalable Bayesian model for low-rank factorization of massive tensors with binary observations. The proposed model has the following key properties: (1) in contrast to the models based on the logistic or probit likelihood, using a zero-truncated Poisson likelihood for binary data allows our model to scale up in the number of \emph{ones} in the tensor, which is especially appealing for massive but sparse binary tensors; (2) side-information in form of binary pairwise relationships (e.g., an adjacency network) between objects in any tensor mode can also be leveraged, which can be especially useful in "cold-start" settings; and (3) the model admits simple Bayesian inference via batch, as well as \emph{online} MCMC; the latter allows scaling up even for \emph{dense} binary data (i.e., when the number of ones in the tensor/network is also massive). In addition, non-negative factor matrices in our model provide easy interpretability, and the tensor rank can be inferred from the data. We evaluate our model on several large-scale real-world binary tensors, achieving excellent computational scalability, and also demonstrate its usefulness in leveraging side-information provided in form of mode-network(s). version:1
arxiv-1508-04112 | Molding CNNs for text: non-linear, non-consecutive convolutions | http://arxiv.org/abs/1508.04112 | id:1508.04112 author:Tao Lei, Regina Barzilay, Tommi Jaakkola category:cs.CL cs.AI  published:2015-08-17 summary:The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task. version:2
arxiv-1508-04198 | Low Rank Representation on Riemannian Manifold of Square Root Densities | http://arxiv.org/abs/1508.04198 | id:1508.04198 author:Yifan Fu, Junbin Gao, Xia Hong, David Tien category:cs.CV  published:2015-08-18 summary:In this paper, we present a novel low rank representation (LRR) algorithm for data lying on the manifold of square root densities. Unlike traditional LRR methods which rely on the assumption that the data points are vectors in the Euclidean space, our new algorithm is designed to incorporate the intrinsic geometric structure and geodesic distance of the manifold. Experiments on several computer vision datasets showcase its noise robustness and superior performance on classification and subspace clustering compared to other state-of-the-art approaches. version:1
arxiv-1508-04190 | Action Recognition based on Subdivision-Fusion Model | http://arxiv.org/abs/1508.04190 | id:1508.04190 author:Hao Zongbo, Lu Linlin, Zhang Qianni, Wu Jie, Izquierdo Ebroul, Yang Juanyu, Zhao Jun category:cs.CV  published:2015-08-18 summary:This paper proposes a novel Subdivision-Fusion Model (SFM) to recognize human actions. In most action recognition tasks, overlapping feature distribution is a common problem leading to overfitting. In the subdivision stage of the proposed SFM, samples in each category are clustered. Then, such samples are grouped into multiple more concentrated subcategories. Boundaries for the subcategories are easier to find and as consequence overfitting is avoided. In the subsequent fusion stage, the multi-subcategories classification results are converted back to the original category recognition problem. Two methods to determine the number of clusters are provided. The proposed model has been thoroughly tested with four popular datasets. In the Hollywood2 dataset, an accuracy of 79.4% is achieved, outperforming the state-of-the-art accuracy of 64.3%. The performance on the YouTube Action dataset has been improved from 75.8% to 82.5%, while considerably improvements are also observed on the KTH and UCF50 datasets. version:1
arxiv-1412-7839 | Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big, Distributed Data | http://arxiv.org/abs/1412.7839 | id:1412.7839 author:Haroon Raja, Waheed U. Bajwa category:cs.LG cs.IT math.IT stat.ML  published:2014-12-25 summary:This paper studies the problem of data-adaptive representations for big, distributed data. It is assumed that a number of geographically-distributed, interconnected sites have massive local data and they are interested in collaboratively learning a low-dimensional geometric structure underlying these data. In contrast to previous works on subspace-based data representations, this paper focuses on the geometric structure of a union of subspaces (UoS). In this regard, it proposes a distributed algorithm---termed cloud K-SVD---for collaborative learning of a UoS structure underlying distributed data of interest. The goal of cloud K-SVD is to learn a common overcomplete dictionary at each individual site such that every sample in the distributed data can be represented through a small number of atoms of the learned dictionary. Cloud K-SVD accomplishes this goal without requiring exchange of individual samples between sites. This makes it suitable for applications where sharing of raw data is discouraged due to either privacy concerns or large volumes of data. This paper also provides an analysis of cloud K-SVD that gives insights into its properties as well as deviations of the dictionaries learned at individual sites from a centralized solution in terms of different measures of local/global data and topology of interconnections. Finally, the paper numerically illustrates the efficacy of cloud K-SVD on real and synthetic distributed data. version:2
arxiv-1508-04123 | Evaluating Classifiers in Detecting 419 Scams in Bilingual Cybercriminal Communities | http://arxiv.org/abs/1508.04123 | id:1508.04123 author:Alex V. Mbaziira, Ehab Abozinadah, James H. Jones Jr category:cs.SI cs.CY cs.LG  published:2015-08-17 summary:Incidents of organized cybercrime are rising because of criminals are reaping high financial rewards while incurring low costs to commit crime. As the digital landscape broadens to accommodate more internet-enabled devices and technologies like social media, more cybercriminals who are not native English speakers are invading cyberspace to cash in on quick exploits. In this paper we evaluate the performance of three machine learning classifiers in detecting 419 scams in a bilingual Nigerian cybercriminal community. We use three popular classifiers in text processing namely: Na\"ive Bayes, k-nearest neighbors (IBK) and Support Vector Machines (SVM). The preliminary results on a real world dataset reveal the SVM significantly outperforms Na\"ive Bayes and IBK at 95% confidence level. version:1
arxiv-1505-00199 | Theory of Optimizing Pseudolinear Performance Measures: Application to F-measure | http://arxiv.org/abs/1505.00199 | id:1505.00199 author:Shameem A Puthiya Parambath, Nicolas Usunier, Yves Grandvalet category:cs.LG  published:2015-05-01 summary:Non-linear performance measures are widely used for the evaluation of learning algorithms. For example, $F$-measure is a commonly used performance measure for classification problems in machine learning and information retrieval community. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes $F$-measure, \emph{Jaccard Index}, among many others. We establish that many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the $F$-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on pseudo-linear measures, which are asymptotic in nature. We also establish the multi-objective nature of the $F$-score maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various $F$-measure optimization tasks. version:3
arxiv-1506-02328 | EventNet: A Large Scale Structured Concept Library for Complex Event Detection in Video | http://arxiv.org/abs/1506.02328 | id:1506.02328 author:Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu, Shih-Fu Chang category:cs.CV  published:2015-06-08 summary:Event-specific concepts are the semantic concepts designed for the events of interest, which can be used as a mid-level representation of complex events in videos. Existing methods only focus on defining event-specific concepts for a small number of predefined events, but cannot handle novel unseen events. This motivates us to build a large scale event-specific concept library that covers as many real-world events and their concepts as possible. Specifically, we choose WikiHow, an online forum containing a large number of how-to articles on human daily life events. We perform a coarse-to-fine event discovery process and discover 500 events from WikiHow articles. Then we use each event name as query to search YouTube and discover event-specific concepts from the tags of returned videos. After an automatic filter process, we end up with 95,321 videos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model on the 95,321 videos over the 500 events, and use the model to extract deep learning feature from video content. With the learned deep learning feature, we train 4,490 binary SVM classifiers as the event-specific concept library. The concepts and events are further organized in a hierarchical structure defined by WikiHow, and the resultant concept library is called EventNet. Finally, the EventNet concept library is used to generate concept based representation of event videos. To the best of our knowledge, EventNet represents the first video event ontology that organizes events and their concepts into a semantic structure. It offers great potential for event retrieval and browsing. Extensive experiments over the zero-shot event retrieval task when no training samples are available show that the EventNet concept library consistently and significantly outperforms the state-of-the-art (such as the 20K ImageNet concepts trained with CNN) by a large margin up to 207%. version:2
arxiv-1508-04065 | A Deep Learning Approach to Structured Signal Recovery | http://arxiv.org/abs/1508.04065 | id:1508.04065 author:Ali Mousavi, Ankit B. Patel, Richard G. Baraniuk category:cs.LG stat.ML  published:2015-08-17 summary:In this paper, we develop a new framework for sensing and recovering structured signals. In contrast to compressive sensing (CS) systems that employ linear measurements, sparse representations, and computationally complex convex/greedy algorithms, we introduce a deep learning framework that supports both linear and mildly nonlinear measurements, that learns a structured representation from training data, and that efficiently computes a signal estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an unsupervised feature learner. SDA enables us to capture statistical dependencies between the different elements of certain signals and improve signal recovery performance as compared to the CS approach. version:1
arxiv-1508-04035 | A Generative Model for Multi-Dialect Representation | http://arxiv.org/abs/1508.04035 | id:1508.04035 author:Emmanuel N. Osegi category:cs.CV cs.LG stat.ML  published:2015-08-17 summary:In the era of deep learning several unsupervised models have been developed to capture the key features in unlabeled handwritten data. Popular among them is the Restricted Boltzmann Machines RBM. However, due to the novelty in handwritten multidialect data, the RBM may fail to generate an efficient representation. In this paper we propose a generative model, the Mode Synthesizing Machine MSM for on-line representation of real life handwritten multidialect language data. The MSM takes advantage of the hierarchical representation of the modes of a data distribution using a two-point error update to learn a sequence of representative multidialects in a generative way. Experiments were performed to evaluate the performance of the MSM over the RBM with the former attaining much lower error values than the latter on both independent and mixed data set. version:1
arxiv-1506-09107 | A complex network approach to stylometry | http://arxiv.org/abs/1506.09107 | id:1506.09107 author:Diego R. Amancio category:cs.CL  published:2015-06-30 summary:Statistical methods have been widely employed to study the fundamental properties of language. In recent years, methods from complex and dynamical systems proved useful to create several language models. Despite the large amount of studies devoted to represent texts with physical models, only a limited number of studies have shown how the properties of the underlying physical systems can be employed to improve the performance of natural language processing tasks. In this paper, I address this problem by devising complex networks methods that are able to improve the performance of current statistical methods. Using a fuzzy classification strategy, I show that the topological properties extracted from texts complement the traditional textual description. In several cases, the performance obtained with hybrid approaches outperformed the results obtained when only traditional or networked methods were used. Because the proposed model is generic, the framework devised here could be straightforwardly used to study similar textual applications where the topology plays a pivotal role in the description of the interacting agents. version:2
arxiv-1508-02103 | Lifted Representation of Relational Causal Models Revisited: Implications for Reasoning and Structure Learning | http://arxiv.org/abs/1508.02103 | id:1508.02103 author:Sanghack Lee, Vasant Honavar category:cs.AI cs.LG  published:2015-08-10 summary:Maier et al. (2010) introduced the relational causal model (RCM) for representing and inferring causal relationships in relational data. A lifted representation, called abstract ground graph (AGG), plays a central role in reasoning with and learning of RCM. The correctness of the algorithm proposed by Maier et al. (2013a) for learning RCM from data relies on the soundness and completeness of AGG for relational d-separation to reduce the learning of an RCM to learning of an AGG. We revisit the definition of AGG and show that AGG, as defined in Maier et al. (2013b), does not correctly abstract all ground graphs. We revise the definition of AGG to ensure that it correctly abstracts all ground graphs. We further show that AGG representation is not complete for relational d-separation, that is, there can exist conditional independence relations in an RCM that are not entailed by AGG. A careful examination of the relationship between the lack of completeness of AGG for relational d-separation and faithfulness conditions suggests that weaker notions of completeness, namely adjacency faithfulness and orientation faithfulness between an RCM and its AGG, can be used to learn an RCM from data. version:2
arxiv-1503-01993 | Tomographic Image Reconstruction using Training images | http://arxiv.org/abs/1503.01993 | id:1503.01993 author:Sara Soltani, Martin S. Andersen, Per Christian Hansen category:cs.CV math.NA 65F22  65K10  published:2015-03-06 summary:We describe and examine an algorithm for tomographic image reconstruction where prior knowledge about the solution is available in the form of training images. We first construct a nonnegative dictionary based on prototype elements from the training images; this problem is formulated as a regularized non-negative matrix factorization. Incorporating the dictionary as a prior in a convex reconstruction problem, we then find an approximate solution with a sparse representation in the dictionary. The dictionary is applied to non-overlapping patches of the image, which reduces the computational complexity compared to other algorithms. Computational experiments clarify the choice and interplay of the model parameters and the regularization parameters, and we show that in few-projection low-dose settings our algorithm is competitive with total variation regularization and tends to include more texture and more correct edges. version:2
arxiv-1508-03953 | Sense Beyond Expressions: Cuteness | http://arxiv.org/abs/1508.03953 | id:1508.03953 author:Kang Wang, Tam V. Nguyen, Jiashi Feng, Jose Sepulveda category:cs.CV  published:2015-08-17 summary:With the development of Internet culture, cuteness has become a popular concept. Many people are curious about what factors making a person look cute. However, there is rare research to answer this interesting question. In this work, we construct a dataset of personal images with comprehensively annotated cuteness scores and facial attributes to investigate this high-level concept in depth. Based on this dataset, through an automatic attributes mining process, we find several critical attributes determining the cuteness of a person. We also develop a novel Continuous Latent Support Vector Machine (C-LSVM) method to predict the cuteness score of one person given only his image. Extensive evaluations validate the effectiveness of the proposed method for cuteness prediction. version:1
arxiv-1508-03928 | LCNN: Low-level Feature Embedded CNN for Salient Object Detection | http://arxiv.org/abs/1508.03928 | id:1508.03928 author:Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price category:cs.CV  published:2015-08-17 summary:In this paper, we propose a novel deep neural network framework embedded with low-level features (LCNN) for salient object detection in complex images. We utilise the advantage of convolutional neural networks to automatically learn the high-level features that capture the structured information and semantic context in the image. In order to better adapt a CNN model into the saliency task, we redesign the network architecture based on the small-scale datasets. Several low-level features are extracted, which can effectively capture contrast and spatial information in the salient regions, and incorporated to compensate with the learned high-level features at the output of the last fully connected layer. The concatenated feature vector is further fed into a hinge-loss SVM detector in a joint discriminative learning manner and the final saliency score of each region within the bounding box is obtained by the linear combination of the detector's weights. Experiments on three challenging benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be effective and superior than most low-level oriented state-of-the-arts in terms of P-R curves, F-measure and mean absolute errors. version:1
arxiv-1508-03906 | Using a Machine Learning Approach to Implement and Evaluate Product Line Features | http://arxiv.org/abs/1508.03906 | id:1508.03906 author:Davide Bacciu, Stefania Gnesi, Laura Semini category:cs.SE cs.LG  published:2015-08-17 summary:Bike-sharing systems are a means of smart transportation in urban environments with the benefit of a positive impact on urban mobility. In this paper we are interested in studying and modeling the behavior of features that permit the end user to access, with her/his web browser, the status of the Bike-Sharing system. In particular, we address features able to make a prediction on the system state. We propose to use a machine learning approach to analyze usage patterns and learn computational models of such features from logs of system usage. On the one hand, machine learning methodologies provide a powerful and general means to implement a wide choice of predictive features. On the other hand, trained machine learning models are provided with a measure of predictive performance that can be used as a metric to assess the cost-performance trade-off of the feature. This provides a principled way to assess the runtime behavior of different components before putting them into operation. version:1
arxiv-1209-3909 | Network Routing Optimization Using Swarm Intelligence | http://arxiv.org/abs/1209.3909 | id:1209.3909 author:Mohamed A. El Galil category:cs.NE cs.DM  published:2012-09-18 summary:The aim of this paper is to highlight and explore a traditional problem, which is the minimum spanning tree, and finding the shortest-path in network routing, by using Swarm Intelligence. This work to be considered as an investigation topic with combination between operations research, discrete mathematics, and evolutionary computing aiming to solve one of networking problems. version:2
arxiv-1508-03856 | Two-stage Cascaded Classifier for Purchase Prediction | http://arxiv.org/abs/1508.03856 | id:1508.03856 author:Sheikh Muhammad Sarwar, Mahamudul Hasan, Dmitry I. Ignatov category:cs.IR cs.LG  published:2015-08-16 summary:In this paper we describe our machine learning solution for the RecSys Challenge, 2015. We have proposed a time efficient two-stage cascaded classifier for the prediction of buy sessions and purchased items within such sessions. Based on the model, several interesting features found, and formation of our own test bed, we have achieved a reasonable score. Usage of Random Forests helps us to cope with the effect of the multiplicity of good models depending on varying subsets of features in the purchased items prediction and, in its turn, boosting is used as a suitable technique to overcome severe class imbalance of the buy-session prediction. version:1
arxiv-1508-03854 | Online Representation Learning in Recurrent Neural Language Models | http://arxiv.org/abs/1508.03854 | id:1508.03854 author:Marek Rei category:cs.CL cs.LG cs.NE  published:2015-08-16 summary:We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. version:1
arxiv-1507-02205 | Talking to the crowd: What do people react to in online discussions? | http://arxiv.org/abs/1507.02205 | id:1507.02205 author:Aaron Jaech, Victoria Zayats, Hao Fang, Mari Ostendorf, Hannaneh Hajishirzi category:cs.CL cs.SI  published:2015-07-08 summary:This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger. A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments. Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest. version:2
arxiv-1507-02045 | What Your Username Says About You | http://arxiv.org/abs/1507.02045 | id:1507.02045 author:Aaron Jaech, Mari Ostendorf category:cs.CL  published:2015-07-08 summary:Usernames are ubiquitous on the Internet, and they are often suggestive of user demographics. This work looks at the degree to which gender and language can be inferred from a username alone by making use of unsupervised morphology induction to decompose usernames into sub-units. Experimental results on the two tasks demonstrate the effectiveness of the proposed morphological features compared to a character n-gram baseline. version:2
arxiv-1508-03846 | Schema Independent Relational Learning | http://arxiv.org/abs/1508.03846 | id:1508.03846 author:Jose Picado, Arash Termehchy, Alan Fern category:cs.DB cs.AI cs.LG cs.LO  published:2015-08-16 summary:Learning novel and interesting concepts and relations from relational databases is an important problem with many applications in database systems and machine learning. Relational learning algorithms generally leverage the properties of the database schema to find the definition of the target concept in terms of the existing relations in the database. Nevertheless, it is well established that the same data set may be represented under different schemas for various reasons, such as efficiency, data quality, and usability. Unfortunately, many current learning algorithms tend to vary quite substantially over the choice of schema, both in terms of learning accuracy and efficiency, which complicates their off-the-shelf application. In this paper, we formalize the property of schema independence of relational learning algorithms, and study both the theoretical and empirical dependence of existing algorithms on the common class of vertical (de)composition schema transformations. We study both sample-based learning algorithms, which learn from sets of labeled examples, and query-based algorithms, which learn by asking queries to a user. For sample-based algorithms we consider the two main algorithm classes: top-down and bottom-up. We prove that practical top-down algorithms are generally not schema independent, while, in contrast, two bottom-up algorithms Golem and ProGolem are schema independent with some modifications. For query-based learning algorithms we show that the vertical (de)composition transformations influence their learning efficiency. We support the theoretical results with an empirical study that demonstrates the schema dependence/independence of several algorithms on existing benchmark data sets under natural vertical (de)compositions. version:1
arxiv-1508-03826 | A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution | http://arxiv.org/abs/1508.03826 | id:1508.03826 author:Shaohua Li, Jun Zhu, Chunyan Miao category:cs.CL cs.LG stat.ML  published:2015-08-16 summary:Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods. version:1
arxiv-1508-03755 | Beat-Event Detection in Action Movie Franchises | http://arxiv.org/abs/1508.03755 | id:1508.03755 author:Danila Potapov, Matthijs Douze, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid category:cs.CV  published:2015-08-15 summary:While important advances were recently made towards temporally localizing and recognizing specific human actions or activities in videos, efficient detection and classification of long video chunks belonging to semantically defined categories such as "pursuit" or "romance" remains challenging.We introduce a new dataset, Action Movie Franchises, consisting of a collection of Hollywood action movie franchises. We define 11 non-exclusive semantic categories - called beat-categories - that are broad enough to cover most of the movie footage. The corresponding beat-events are annotated as groups of video shots, possibly overlapping.We propose an approach for localizing beat-events based on classifying shots into beat-categories and learning the temporal constraints between shots. We show that temporal constraints significantly improve the classification performance. We set up an evaluation protocol for beat-event localization as well as for shot classification, depending on whether movies from the same franchise are present or not in the training data. version:1
arxiv-1503-04596 | Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network | http://arxiv.org/abs/1503.04596 | id:1503.04596 author:Mark D. McDonnell, Tony Vladusich category:cs.NE cs.CV cs.LG  published:2015-03-16 summary:We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few tunable parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods. version:3
arxiv-1407-4981 | Statistical Inference of Intractable Generative Models via Classification | http://arxiv.org/abs/1407.4981 | id:1407.4981 author:Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander category:stat.CO stat.ME stat.ML  published:2014-07-18 summary:Increasingly complex generative models are being used across the disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to perform likelihood-based statistical inference. We consider here a likelihood-free framework where inference is done by identifying parameter values which generate simulated data adequately resembling the observed data. A major difficulty is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. version:2
arxiv-1506-00275 | Diversity in Spectral Learning for Natural Language Parsing | http://arxiv.org/abs/1506.00275 | id:1506.00275 author:Shashi Narayan, Shay B. Cohen category:cs.CL  published:2015-05-31 summary:We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the $F_1$ score of 90.18, and for German we achieve the $F_1$ score of 83.38. version:2
arxiv-1508-03721 | A Comparative Study on Regularization Strategies for Embedding-based Neural Networks | http://arxiv.org/abs/1508.03721 | id:1508.03721 author:Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin category:cs.CL cs.LG  published:2015-08-15 summary:This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. version:1
arxiv-1508-03720 | Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path | http://arxiv.org/abs/1508.03720 | id:1508.03720 author:Xu Yan, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, Zhi Jin category:cs.CL cs.LG  published:2015-08-15 summary:Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an $F_1$-score of 83.7\%, higher than competing methods in the literature. version:1
arxiv-1506-00379 | Modeling Relation Paths for Representation Learning of Knowledge Bases | http://arxiv.org/abs/1506.00379 | id:1506.00379 author:Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, Song Liu category:cs.CL  published:2015-06-01 summary:Representation learning of knowledge bases (KBs) aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. version:2
arxiv-1508-03712 | Towards an Axiomatic Approach to Hierarchical Clustering of Measures | http://arxiv.org/abs/1508.03712 | id:1508.03712 author:Philipp Thomann, Ingo Steinwart, Nico Schmid category:stat.ML cs.LG math.ST stat.ME stat.TH  published:2015-08-15 summary:We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density. version:1
arxiv-1508-03710 | A Novel Approach For Finger Vein Verification Based on Self-Taught Learning | http://arxiv.org/abs/1508.03710 | id:1508.03710 author:Mohsen Fayyaz, Masoud PourReza, Mohammad Hajizadeh Saffar, Mohammad Sabokrou, Mahmood Fathy category:cs.CV  published:2015-08-15 summary:In this paper, we propose a method for user Finger Vein Authentication (FVA) as a biometric system. Using the discriminative features for classifying theses finger veins is one of the main tips that make difference in related works, Thus we propose to learn a set of representative features, based on autoencoders. We model the user finger vein using a Gaussian distribution. Experimental results show that our algorithm perform like a state-of-the-art on SDUMLA-HMT benchmark. version:1
arxiv-1508-03106 | Neyman-Pearson Classification under High-Dimensional Settings | http://arxiv.org/abs/1508.03106 | id:1508.03106 author:Anqi Zhao, Yang Feng, Lie Wang, Xin Tong category:stat.ML  published:2015-08-13 summary:Most existing binary classification methods target on the optimization of the overall classification risk and may fail to serve some real-world applications such as cancer diagnosis, where users are more concerned with the risk of misclassifying one specific class than the other. Neyman-Pearson (NP) paradigm was introduced in this context as a novel statistical framework for handling asymmetric type I/II error priorities. It seeks classifiers with a minimal type II error and a constrained type I error under a user specified level. This article is the first attempt to construct classifiers with guaranteed theoretical performance under the NP paradigm in high-dimensional settings. Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to construct NP-type classifiers for Naive Bayes models. The proposed classifiers satisfy the NP oracle inequalities, which are natural NP paradigm counterparts of the oracle inequalities in classical binary classification. Besides their desirable theoretical properties, we also demonstrated their numerical advantages in prioritized error control via both simulation and real data studies. version:2
arxiv-1508-03666 | Unbounded Bayesian Optimization via Regularization | http://arxiv.org/abs/1508.03666 | id:1508.03666 author:Bobak Shahriari, Alexandre Bouchard-Côté, Nando de Freitas category:stat.ML  published:2015-08-14 summary:Bayesian optimization has recently emerged as a popular and efficient tool for global optimization and hyperparameter tuning. Currently, the established Bayesian optimization practice requires a user-defined bounding box which is assumed to contain the optimizer. However, when little is known about the probed objective function, it can be difficult to prescribe such bounds. In this work we modify the standard Bayesian optimization framework in a principled way to allow automatic resizing of the search space. We introduce two alternative methods and compare them on two common synthetic benchmarking test functions as well as the tasks of tuning the stochastic gradient descent optimizer of a multi-layered perceptron and a convolutional neural network on MNIST. version:1
arxiv-1505-07752 | A new Semi-Markov model based clustering for a Clustering-Scheduling Integrated framework for Patient Flow Modeling and Optimization | http://arxiv.org/abs/1505.07752 | id:1505.07752 author:Chitta Ranjan, Kamran Paynabar, Jonathan E. Helm category:stat.ME stat.AP stat.ML  published:2015-05-28 summary:The ability to accurately forecast and control inpatient census, and thereby workloads, is a critical and longstanding problem in hospital management. The majority of current literature focuses on optimal scheduling of elective inpatients, but largely ignores the process of accurate estimation of the trajectory of patients throughout the treatment and recovery process. The result is that current elective scheduling models are optimizing based on inaccurate input data. In this paper, we develop a Clustering and Scheduling Integrated (CSI) approach to capture patient flows through a network of hospital services. CSI functions by clustering patients into groups based on similarity of trajectory using a Semi-Markov model-based clustering scheme, as opposed to clustering by admit type or condition as in previous literature. The methodology is validated by simulation and then applied to real patient data from a partner hospital where we see it outperforms current methods. Further, we demonstrate that optimization methods achieve significantly better results on key hospital performance measure under CSI, compared with traditional estimation approaches, increasing elective admissions by 97% and utilization by 22% compared to 30% and 8% using traditional estimation techniques. From a theoretical standpoint, the SMM-clustering is a novel approach applicable to any temporal-spatial stochastic data that is prevalent in many industries. version:3
arxiv-1508-03601 | Is Stack Overflow Overflowing With Questions and Tags | http://arxiv.org/abs/1508.03601 | id:1508.03601 author:Ranjitha R. K., Sanjay Singh category:cs.SI cs.CL H.5.3; I.2.6; I.2.7  published:2015-08-14 summary:Programming question and answer (Q & A) websites, such as Quora, Stack Overflow, and Yahoo! Answer etc. helps us to understand the programming concepts easily and quickly in a way that has been tested and applied by many software developers. Stack Overflow is one of the most frequently used programming Q\&A website where the questions and answers posted are presently analyzed manually, which requires a huge amount of time and resource. To save the effort, we present a topic modeling based technique to analyze the words of the original texts to discover the themes that run through them. We also propose a method to automate the process of reviewing the quality of questions on Stack Overflow dataset in order to avoid ballooning the stack overflow with insignificant questions. The proposed method also recommends the appropriate tags for the new post, which averts the creation of unnecessary tags on Stack Overflow. version:1
arxiv-1406-1305 | Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets | http://arxiv.org/abs/1406.1305 | id:1406.1305 author:Dan Garber, Elad Hazan category:math.OC cs.LG  published:2014-06-05 summary:The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of $\frac{1}{t^2}$. This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order $\frac{1}{t}$, and known to be tight. We show that various balls induced by $\ell_p$ norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fast-rate results for the FW method follow easily from our analysis. version:2
arxiv-1301-4666 | A Linearly Convergent Conditional Gradient Algorithm with Applications to Online and Stochastic Optimization | http://arxiv.org/abs/1301.4666 | id:1301.4666 author:Dan Garber, Elad Hazan category:cs.LG math.OC stat.ML  published:2013-01-20 summary:Linear optimization is many times algorithmically simpler than non-linear convex optimization. Linear optimization over matroid polytopes, matching polytopes and path polytopes are example of problems for which we have simple and efficient combinatorial algorithms, but whose non-linear convex counterpart is harder and admits significantly less efficient algorithms. This motivates the computational model of convex optimization, including the offline, online and stochastic settings, using a linear optimization oracle. In this computational model we give several new results that improve over the previous state-of-the-art. Our main result is a novel conditional gradient algorithm for smooth and strongly convex optimization over polyhedral sets that performs only a single linear optimization step over the domain on each iteration and enjoys a linear convergence rate. This gives an exponential improvement in convergence rate over previous results. Based on this new conditional gradient algorithm we give the first algorithms for online convex optimization over polyhedral sets that perform only a single linear optimization step over the domain while having optimal regret guarantees, answering an open question of Kalai and Vempala, and Hazan and Kale. Our online algorithms also imply conditional gradient algorithms for non-smooth and stochastic convex optimization with the same convergence rates as projected (sub)gradient methods. version:6
arxiv-1405-4969 | Sparsity Based Methods for Overparameterized Variational Problems | http://arxiv.org/abs/1405.4969 | id:1405.4969 author:Raja Giryes, Michael Elad, Alfred M. Bruckstein category:cs.CV stat.ML  published:2014-05-20 summary:Two complementary approaches have been extensively used in signal and image processing leading to novel results, the sparse representation methodology and the variational strategy. Recently, a new sparsity based model has been proposed, the cosparse analysis framework, which may potentially help in bridging sparse approximation based methods to the traditional total-variation minimization. Based on this, we introduce a sparsity based framework for solving overparameterized variational problems. The latter has been used to improve the estimation of optical flow and also for general denoising of signals and images. However, the recovery of the space varying parameters involved was not adequately addressed by traditional variational methods. We first demonstrate the efficiency of the new framework for one dimensional signals in recovering a piecewise linear and polynomial function. Then, we illustrate how the new technique can be used for denoising and segmentation of images. version:5
arxiv-1508-03498 | Lensless Compressive Imaging | http://arxiv.org/abs/1508.03498 | id:1508.03498 author:Xin Yuan, Hong Jiang, Gang Huang, Paul Wilford category:cs.CV stat.AP stat.ME  published:2015-08-14 summary:We develop a lensless compressive imaging architecture, which consists of an aperture assembly and a single sensor, without using any lens. An anytime algorithm is proposed to reconstruct images from the compressive measurements; the algorithm produces a sequence of solutions that monotonically converge to the true signal (thus, anytime). The algorithm is developed based on the sparsity of local overlapping patches (in the transformation domain) and state-of-the-art results have been obtained. Experiments on real data demonstrate that encouraging results are obtained by measuring about 10% (of the image pixels) compressive measurements. The reconstruction results of the proposed algorithm are compared with the JPEG compression (based on file sizes) and the reconstructed image quality is close to the JPEG compression, in particular at a high compression rate. version:1
arxiv-1312-7559 | A model selection approach for clustering a multinomial sequence with non-negative factorization | http://arxiv.org/abs/1312.7559 | id:1312.7559 author:Nam H. Lee, Runze Tang, Carey E. Priebe, Michael Rosen category:stat.ML  published:2013-12-29 summary:We consider a problem of clustering a sequence of multinomial observations by way of a model selection criterion. We propose a form of a penalty term for the model selection procedure. Our approach subsumes both the conventional AIC and BIC criteria but also extends the conventional criteria in a way that it can be applicable also to a sequence of sparse multinomial observations, where even within a same cluster, the number of multinomial trials may be different for different observations. In addition, as a preliminary estimation step to maximum likelihood estimation, and more generally, to maximum $L_{q}$ estimation, we propose to use reduced rank projection in combination with non-negative factorization. We motivate our approach by showing that our model selection criterion and preliminary estimation step yield consistent estimates under simplifying assumptions. We also illustrate our approach through numerical experiments using real and simulated data. version:7
arxiv-1509-03660 | Oracle MCG: A first peek into COCO Detection Challenges | http://arxiv.org/abs/1509.03660 | id:1509.03660 author:Jordi Pont-Tuset, Pablo Arbeláez, Luc Van Gool category:cs.CV  published:2015-08-14 summary:The recently presented COCO detection challenge will most probably be the reference benchmark in object detection in the next years. COCO is two orders of magnitude larger than Pascal and has four times the number of categories; so in all likelihood researchers will be faced with a number of new challenges. At this point, without any finished round of the competition, it is difficult for researchers to put their techniques in context, or in other words, to know how good their results are. In order to give a little context, this note evaluates a hypothetical object detector consisting in an oracle picking the best object proposal from a state-of-the-art technique. This oracle achieves a AP=0.292 in segmented objects and AP=0.317 in bounding boxes, showing that indeed the database is challenging, given that this value is the best one can expect if working on object proposals without refinement. version:1
arxiv-1506-05532 | A Spatial Layout and Scale Invariant Feature Representation for Indoor Scene Classification | http://arxiv.org/abs/1506.05532 | id:1506.05532 author:Munawar Hayat, Salman H. Khan, Mohammed Bennamoun, Senjian An category:cs.CV  published:2015-06-18 summary:Unlike standard object classification, where the image to be classified contains one or multiple instances of the same object, indoor scene classification is quite different since the image consists of multiple distinct objects. Further, these objects can be of varying sizes and are present across numerous spatial locations in different layouts. For automatic indoor scene categorization, large scale spatial layout deformations and scale variations are therefore two major challenges and the design of rich feature descriptors which are robust to these challenges is still an open problem. This paper introduces a new learnable feature descriptor called "spatial layout and scale invariant convolutional activations" to deal with these challenges. For this purpose, a new Convolutional Neural Network architecture is designed which incorporates a novel 'Spatially Unstructured' layer to introduce robustness against spatial layout deformations. To achieve scale invariance, we present a pyramidal image representation. For feasible training of the proposed network for images of indoor scenes, the paper proposes a new methodology which efficiently adapts a trained network model (on a large scale data) for our task with only a limited amount of available training data. Compared with existing state of the art, the proposed approach achieves a relative performance improvement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8, Graz-02 and NYU datasets respectively. version:2
arxiv-1508-03395 | Information-theoretic Bounds on Matrix Completion under Union of Subspaces Model | http://arxiv.org/abs/1508.03395 | id:1508.03395 author:Vaneet Aggarwal, Shuchin Aeron category:cs.IT math.IT stat.ML  published:2015-08-14 summary:In this short note we extend some of the recent results on matrix completion under the assumption that the columns of the matrix can be grouped (clustered) into subspaces (not necessarily disjoint or independent). This model deviates from the typical assumption prevalent in the literature dealing with compression and recovery for big-data applications. The results have a direct bearing on the problem of subspace clustering under missing or incomplete information. version:1
arxiv-1508-03386 | Learning from Real Users: Rating Dialogue Success with Neural Networks for Reinforcement Learning in Spoken Dialogue Systems | http://arxiv.org/abs/1508.03386 | id:1508.03386 author:Pei-Hao Su, David Vandyke, Milica Gasic, Dongho Kim, Nikola Mrksic, Tsung-Hsien Wen, Steve Young category:cs.LG cs.CL  published:2015-08-13 summary:To train a statistical spoken dialogue system (SDS) it is essential that an accurate method for measuring task success is available. To date training has relied on presenting a task to either simulated or paid users and inferring the dialogue's success by observing whether this presented task was achieved or not. Our aim however is to be able to learn from real users acting under their own volition, in which case it is non-trivial to rate the success as any prior knowledge of the task is simply unavailable. User feedback may be utilised but has been found to be inconsistent. Hence, here we present two neural network models that evaluate a sequence of turn-level features to rate the success of a dialogue. Importantly these models make no use of any prior knowledge of the user's task. The models are trained on dialogues generated by a simulated user and the best model is then used to train a policy on-line which is shown to perform at least as well as a baseline system using prior knowledge of the user's task. We note that the models should also be of interest for evaluating SDS and for monitoring a dialogue in rule-based SDS. version:1
arxiv-1508-03329 | Multi-Task Learning with Group-Specific Feature Space Sharing | http://arxiv.org/abs/1508.03329 | id:1508.03329 author:Niloofar Yousefi, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2015-08-13 summary:When faced with learning a set of inter-related tasks from a limited amount of usable data, learning each task independently may lead to poor generalization performance. Multi-Task Learning (MTL) exploits the latent relations between tasks and overcomes data scarcity limitations by co-learning all these tasks simultaneously to offer improved performance. We propose a novel Multi-Task Multiple Kernel Learning framework based on Support Vector Machines for binary classification tasks. By considering pair-wise task affinity in terms of similarity between a pair's respective feature spaces, the new framework, compared to other similar MTL approaches, offers a high degree of flexibility in determining how similar feature spaces should be, as well as which pairs of tasks should share a common feature space in order to benefit overall performance. The associated optimization problem is solved via a block coordinate descent, which employs a consensus-form Alternating Direction Method of Multipliers algorithm to optimize the Multiple Kernel Learning weights and, hence, to determine task affinities. Empirical evaluation on seven data sets exhibits a statistically significant improvement of our framework's results compared to the ones of several other Clustered Multi-Task Learning methods. version:1
arxiv-1508-03276 | Talking about the Moving Image: A Declarative Model for Image Schema Based Embodied Perception Grounding and Language Generation | http://arxiv.org/abs/1508.03276 | id:1508.03276 author:Jakob Suchan, Mehul Bhatt, Harshita Jhavar category:cs.AI cs.CL cs.CV cs.HC  published:2015-08-13 summary:We present a general theory and corresponding declarative model for the embodied grounding and natural language based analytical summarisation of dynamic visuo-spatial imagery. The declarative model ---ecompassing spatio-linguistic abstractions, image schemas, and a spatio-temporal feature based language generator--- is modularly implemented within Constraint Logic Programming (CLP). The implemented model is such that primitives of the theory, e.g., pertaining to space and motion, image schemata, are available as first-class objects with `deep semantics' suited for inference and query. We demonstrate the model with select examples broadly motivated by areas such as film, design, geography, smart environments where analytical natural language based externalisations of the moving image are central from the viewpoint of human interaction, evidence-based qualitative analysis, and sensemaking. Keywords: moving image, visual semantics and embodiment, visuo-spatial cognition and computation, cognitive vision, computational models of narrative, declarative spatial reasoning version:1
arxiv-1508-03269 | A New Approach to an Old Problem: The Reconstruction of a Go Game through a Series of Photographs | http://arxiv.org/abs/1508.03269 | id:1508.03269 author:Mario Corsolini, Andrea Carta category:cs.CV  published:2015-08-13 summary:Given a series of photographs taken during a Go game, we describe the techniques we successfully employ for pinpointing the grid lines of the Go board and for tracking their small movements between consecutive photographs; then we discuss how to approximate the location and orientation of the observer's point of view, in order to compensate for projection effects. Finally we describe the different criteria that jointly form the algorithm for stones' detection, thus enabling us to automatically reconstruct the whole move sequence. version:1
arxiv-1503-01444 | Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications | http://arxiv.org/abs/1503.01444 | id:1503.01444 author:Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So Kweon category:cs.CV cs.AI  published:2015-03-04 summary:Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values, which implicitly encourages the target rank constraint. Our experimental analyses show that, when the number of samples is deficient, our approach leads to a higher success rate than conventional rank minimization, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, motion edge detection, photometric stereo, image alignment and recovery, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method. version:2
arxiv-1508-02354 | Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning | http://arxiv.org/abs/1508.02354 | id:1508.02354 author:Jianpeng Cheng, Dimitri Kartsaklis category:cs.CL cs.AI cs.NE  published:2015-08-10 summary:Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range. version:2
arxiv-1505-05401 | A Max-Sum algorithm for training discrete neural networks | http://arxiv.org/abs/1505.05401 | id:1505.05401 author:Carlo Baldassi, Alfredo Braunstein category:cond-mat.dis-nn cs.LG cs.NE  published:2015-05-20 summary:We present an efficient learning algorithm for the problem of training neural networks with discrete synapses, a well-known hard (NP-complete) discrete optimization problem. The algorithm is a variant of the so-called Max-Sum (MS) algorithm. In particular, we show how, for bounded integer weights with $q$ distinct states and independent concave a priori distribution (e.g. $l_{1}$ regularization), the algorithm's time complexity can be made to scale as $O\left(N\log N\right)$ per node update, thus putting it on par with alternative schemes, such as Belief Propagation (BP), without resorting to approximations. Two special cases are of particular interest: binary synapses $W\in\{-1,1\}$ and ternary synapses $W\in\{-1,0,1\}$ with $l_{0}$ regularization. The algorithm we present performs as well as BP on binary perceptron learning problems, and may be better suited to address the problem on fully-connected two-layer networks, since inherent symmetries in two layer networks are naturally broken using the MS approach. version:2
arxiv-1508-03174 | Logical N-AND Gate on a Molecular Turing Machine | http://arxiv.org/abs/1508.03174 | id:1508.03174 author:Victor Hernandez-Urbina category:cs.ET cs.NE  published:2015-08-13 summary:In Boolean algebra, it is known that the logical function that corresponds to the negation of the conjunction --NAND-- is universal in the sense that any other logical function can be built based on it. This property makes it essential to modern digital electronics and computer processor design. Here, we design a molecular Turing machine that computes the NAND function over binary strings of arbitrary length. For this purpose, we will perform a mathematical abstraction of the kind of operations that can be done over a double-stranded DNA molecule, as well as presenting a molecular encoding of the input symbols for such a machine. version:1
arxiv-1508-03170 | Generation of Multimedia Artifacts: An Extractive Summarization-based Approach | http://arxiv.org/abs/1508.03170 | id:1508.03170 author:Paulo Figueiredo, Marta Aparício, David Martins de Matos, Ricardo Ribeiro category:cs.AI cs.CL cs.MM I.2.7  published:2015-08-13 summary:We explore methods for content selection and address the issue of coherence in the context of the generation of multimedia artifacts. We use audio and video to present two case studies: generation of film tributes, and lecture-driven science talks. For content selection, we use centrality-based and diversity-based summarization, along with topic analysis. To establish coherence, we use the emotional content of music, for film tributes, and ensure topic similarity between lectures and documentaries, for science talks. Composition techniques for the production of multimedia artifacts are addressed as a means of organizing content, in order to improve coherence. We discuss our results considering the above aspects. version:1
arxiv-1508-03130 | Probabilistic Dependency Networks for Prediction and Diagnostics | http://arxiv.org/abs/1508.03130 | id:1508.03130 author:Narayanan U. Edakunni, Aditi Raghunathan, Abhishek Tripathi, John Handley, Fredric Roulland category:cs.LG  published:2015-08-13 summary:Research in transportation frequently involve modelling and predicting attributes of events that occur at regular intervals. The event could be arrival of a bus at a bus stop, the volume of a traffic at a particular point, the demand at a particular bus stop etc. In this work, we propose a specific implementation of probabilistic graphical models to learn the probabilistic dependency between the events that occur in a network. A dependency graph is built from the past observed instances of the event and we use the graph to understand the causal effects of some events on others in the system. The dependency graph is also used to predict the attributes of future events and is shown to have a good prediction accuracy compared to the state of the art. version:1
arxiv-1508-03117 | Optimized Projections for Compressed Sensing via Direct Mutual Coherence Minimization | http://arxiv.org/abs/1508.03117 | id:1508.03117 author:Zhouchen Lin, Canyi Lu, Huan Li category:cs.IT cs.LG math.IT  published:2015-08-13 summary:Compressed Sensing (CS) is a novel technique for simultaneous signal sampling and compression based on the existence of a sparse representation of signal and a projected dictionary $\PP\D$, where $\PP\in\mathbb{R}^{m\times d}$ is the projection matrix and $\D\in\mathbb{R}^{d\times n}$ is the dictionary. To exactly recover the signal with a small number of measurements $m$, the projected dictionary $\PP\D$ is expected to be of low mutual coherence. Several previous methods attempt to find the projection $\PP$ such that the mutual coherence of $\PP\D$ can be as low as possible. However, they do not minimize the mutual coherence directly and thus their methods are far from optimal. Also the solvers they used lack of the convergence guarantee and thus there has no guarantee on the quality of their obtained solutions. This work aims to address these issues. We propose to find an optimal projection by minimizing the mutual coherence of $\PP\D$ directly. This leads to a nonconvex nonsmooth minimization problem. We then approximate it by smoothing and solve it by alternate minimization. We further prove the convergence of our algorithm. To the best of our knowledge, this is the first work which directly minimizes the mutual coherence of the projected dictionary with a convergence guarantee. Numerical experiments demonstrate that the proposed method can recover sparse signals better than existing methods. version:1
arxiv-1508-03649 | Borobudur was Built Algorithmically | http://arxiv.org/abs/1508.03649 | id:1508.03649 author:Hokky Situngkir category:cs.CY cs.CV cs.GR  published:2015-08-13 summary:The self-similarity of Indonesian Borobudur Temple is observed through the dimensionality of stupa that is hypothetically closely related to whole architectural body. Fractal dimension is calculated by using the cube counting method and found that the dimension is 2.325, which is laid between the two-dimensional plane and three dimensional space. The applied fractal geometry and self-similarity of the building is emerged as the building process implement the metric rules, since there is no universal metric standard known in ancient traditional Javanese culture thus the architecture is not based on final master plan. The paper also proposes how the hypothetical algorithmic architecture might be applied computationally in order to see some experimental generations of similar building. The paper ends with some conjectures for further challenge and insights related to fractal geometry in Javanese traditional cultural heritages. version:1
arxiv-1506-02582 | On Convergence of Emphatic Temporal-Difference Learning | http://arxiv.org/abs/1506.02582 | id:1506.02582 author:Huizhen Yu category:cs.LG 90C40  62L20  68W40  published:2015-06-08 summary:We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD($\lambda$) and ELSTD($\lambda$). We prove, under general off-policy conditions, the convergence in $L^1$ for ELSTD($\lambda$) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD($\lambda$) also converges under off-policy training for $\lambda$ sufficiently large. version:2
arxiv-1508-02986 | From Cutting Planes Algorithms to Compression Schemes and Active Learning | http://arxiv.org/abs/1508.02986 | id:1508.02986 author:Liva Ralaivola, Ugo Louche category:cs.LG  published:2015-08-12 summary:Cutting-plane methods are well-studied localization(and optimization) algorithms. We show that they provide a natural framework to perform machinelearning ---and not just to solve optimization problems posed by machinelearning--- in addition to their intended optimization use. In particular, theyallow one to learn sparse classifiers and provide good compression schemes.Moreover, we show that very little effort is required to turn them intoeffective active learning methods. This last property provides a generic way todesign a whole family of active learning algorithms from existing passivemethods. We present numerical simulations testifying of the relevance ofcutting-plane methods for passive and active learning tasks. version:1
arxiv-1508-02977 | A massively parallel multi-level approach to a domain decomposition method for the optical flow estimation with varying illumination | http://arxiv.org/abs/1508.02977 | id:1508.02977 author:Diane Gilliocq-Hirtz, Zakaria Belhachmi category:cs.CV  published:2015-08-12 summary:We consider a variational method to solve the optical flow problem with varying illumination. We apply an adaptive control of the regularization parameter which allows us to preserve the edges and fine features of the computed flow. To reduce the complexity of the estimation for high resolution images and the time of computations, we implement a multi-level parallel approach based on the domain decomposition with the Schwarz overlapping method. The second level of parallelism uses the massively parallel solver MUMPS. We perform some numerical simulations to show the efficiency of our approach and to validate it on classical and real-world image sequences. version:1
arxiv-1508-02959 | Mountain Peak Detection in Online Social Media | http://arxiv.org/abs/1508.02959 | id:1508.02959 author:Roman Fedorov category:cs.CV cs.MM  published:2015-08-12 summary:We present a system for the classification of mountain panoramas from user-generated photographs followed by identification and extraction of mountain peaks from those panoramas. We have developed an automatic technique that, given as input a geo-tagged photograph, estimates its FOV (Field Of View) and the direction of the camera using a matching algorithm on the photograph edge maps and a rendered view of the mountain silhouettes that should be seen from the observer's point of view. The extraction algorithm then identifies the mountain peaks present in the photograph and their profiles. We discuss possible applications in social fields such as photograph peak tagging on social portals, augmented reality on mobile devices when viewing a mountain panorama, and generation of collective intelligence systems (such as environmental models) from massive social media collections (e.g. snow water availability maps based on mountain peak states extracted from photograph hosting services). version:1
arxiv-1507-01581 | Joint Calibration for Semantic Segmentation | http://arxiv.org/abs/1507.01581 | id:1507.01581 author:Holger Caesar, Jasper Uijlings, Vittorio Ferrari category:cs.CV 68T45  published:2015-07-06 summary:Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [18] dataset in both the fully and weakly supervised setting by a considerably margin (+6% and +10%, respectively). version:4
arxiv-1508-02925 | RCR: Robust Compound Regression for Robust Estimation of Errors-in-Variables Model | http://arxiv.org/abs/1508.02925 | id:1508.02925 author:Hao Han, Wei Zhu category:stat.ME math.ST stat.ML stat.TH  published:2015-08-12 summary:The errors-in-variables (EIV) regression model, being more realistic by accounting for measurement errors in both the dependent and the independent variables, is widely adopted in applied sciences. The traditional EIV model estimators, however, can be highly biased by outliers and other departures from the underlying assumptions. In this paper, we develop a novel nonparametric regression approach - the robust compound regression (RCR) analysis method for the robust estimation of EIV models. We first introduce a robust and efficient estimator called least sine squares (LSS). Taking full advantage of both the new LSS method and the compound regression analysis method developed in our own group, we subsequently propose the RCR approach as a generalization of those two, which provides a robust counterpart of the entire class of the maximum likelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping. Technically, our approach gives users the flexibility to select from a class of RCR estimates the optimal one with a predefined regression efficiency criterion satisfied. Simulation studies and real-life examples are provided to illustrate the effectiveness of the RCR approach. version:1
arxiv-1508-02905 | Bayesian Dropout | http://arxiv.org/abs/1508.02905 | id:1508.02905 author:Tue Herlau, Morten Mørup, Mikkel N. Schmidt category:stat.ML  published:2015-08-12 summary:Dropout has recently emerged as a powerful and simple method for training neural networks preventing co-adaptation by stochastically omitting neurons. Dropout is currently not grounded in explicit modelling assumptions which so far has precluded its adoption in Bayesian modelling. Using Bayesian entropic reasoning we show that dropout can be interpreted as optimal inference under constraints. We demonstrate this on an analytically tractable regression model providing a Bayesian interpretation of its mechanism for regularizing and preventing co-adaptation as well as its connection to other Bayesian techniques. We also discuss two general approximate techniques for applying Bayesian dropout for general models, one based on an analytical approximation and the other on stochastic variational techniques. These techniques are then applied to a Baysian logistic regression problem and are shown to improve performance as the model become more misspecified. Our framework roots dropout as a theoretically justified and practical tool for statistical modelling allowing Bayesians to tap into the benefits of dropout training. version:1
arxiv-1508-02865 | Maximum Entropy Vector Kernels for MIMO system identification | http://arxiv.org/abs/1508.02865 | id:1508.02865 author:Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso category:cs.SY stat.ML  published:2015-08-12 summary:Recent contributions have framed linear system identification as a nonparametric regularized inverse problem, which in some situations have proved to be advantageous w.r.t classical parametric methods. Typical formulations exploit an $\ell_2$-type regularization which accounts for the stability and smoothness of the impulse response to be estimated. In this paper, adopting Maximum Entropy arguments, we derive a new type of $\ell_2$-regularization which results in a vector-valued kernel; our aim is to introduce regularization on the block Hankel matrix built with Markov coefficients, thus controlling the complexity of the identified model, measured by its McMillan degree. As a special case we recover the standard nuclear norm penalty. Combining this Hankel-based regularization with the standard $\ell_2$-type regularization adopted in previous literature we design a kernel which, at the same time, encodes stability, smoothness and low McMillan degree. In contrast with previous literature on reweighed nuclear norm penalties, our kernel is described by a small number of hyper-prameters, which are iteratively updated through marginal likelihood maximization. To this purpose we also adapt a Scaled Gradient Projection (SGP) algorithm which is proved to be significantly computationally cheaper than other first and second order off-the-shelf optimization methods. The effectiveness of the identification technique we propose is confirmed by several Monte-Carlo studies. version:1
arxiv-1508-02849 | Manifold regularization in structured output space for semi-supervised structured output prediction | http://arxiv.org/abs/1508.02849 | id:1508.02849 author:Fei Jiang, Lili Jia, Xiaobao Sheng, Riley LeMieux category:cs.LG cs.CV  published:2015-08-12 summary:Structured output prediction aims to learn a predictor to predict a structured output from a input data vector. The structured outputs include vector, tree, sequence, etc. We usually assume that we have a training set of input-output pairs to train the predictor. However, in many real-world appli- cations, it is difficult to obtain the output for a input, thus for many training input data points, the structured outputs are missing. In this paper, we dis- cuss how to learn from a training set composed of some input-output pairs, and some input data points without outputs. This problem is called semi- supervised structured output prediction. We propose a novel method for this problem by constructing a nearest neighbor graph from the input space to present the manifold structure, and using it to regularize the structured out- put space directly. We define a slack structured output for each training data point, and proposed to predict it by learning a structured output predictor. The learning of both slack structured outputs and the predictor are unified within one single minimization problem. In this problem, we propose to mini- mize the structured loss between the slack structured outputs of neighboring data points, and the prediction error measured by the structured loss. The problem is optimized by an iterative algorithm. Experiment results over three benchmark data sets show its advantage. version:1
arxiv-1508-02848 | Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration | http://arxiv.org/abs/1508.02848 | id:1508.02848 author:Yunjin Chen, Thomas Pock category:cs.CV  published:2015-08-12 summary:Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework to obtain simple but effective models for various image restoration problems. The proposed approach is based on the concept of nonlinear reaction diffusion, but we extend conventional nonlinear reaction diffusion models by highly parametrized linear filters as well as highly parametrized influence functions. In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are learned from training data through a loss based approach. We call this approach TNRD -- Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets with respect to the tested applications. Our trained models retain the structural simplicity of diffusion models and take only a small number of steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast. version:1
arxiv-1501-06721 | Massively-concurrent Agent-based Evolutionary Computing | http://arxiv.org/abs/1501.06721 | id:1501.06721 author:D. Krzywicki, W. Turek, A. Byrski, M. Kisiel-Dorohinicki category:cs.MA cs.NE  published:2015-01-27 summary:The fusion of the multi-agent paradigm with evolutionary computation yielded promising results in many optimization problems. Evolutionary multi-agent system (EMAS) are more similar to biological evolution than classical evolutionary algorithms. However, technological limitations prevented the use of fully asynchronous agents in previous EMAS implementations. In this paper we present a new algorithm for agent-based evolutionary computations. The individuals are represented as fully autonomous and asynchronous agents. An efficient implementation of this algorithm was possible through the use of modern technologies based on functional languages (namely Erlang and Scala), which natively support lightweight processes and asynchronous communication. Our experiments show that such an asynchronous approach is both faster and more efficient in solving common optimization problems. version:2
arxiv-1508-02823 | Learning to Hire Teams | http://arxiv.org/abs/1508.02823 | id:1508.02823 author:Adish Singla, Eric Horvitz, Pushmeet Kohli, Andreas Krause category:cs.HC cs.CY cs.LG  published:2015-08-12 summary:Crowdsourcing and human computation has been employed in increasingly sophisticated projects that require the solution of a heterogeneous set of tasks. We explore the challenge of building or hiring an effective team, for performing tasks required for such projects on an ongoing basis, from an available pool of applicants or workers who have bid for the tasks. The recruiter needs to learn workers' skills and expertise by performing online tests and interviews, and would like to minimize the amount of budget or time spent in this process before committing to hiring the team. How can one optimally spend budget to learn the expertise of workers as part of recruiting a team? How can one exploit the similarities among tasks as well as underlying social ties or commonalities among the workers for faster learning? We tackle these decision-theoretic challenges by casting them as an instance of online learning for best action selection. We present algorithms with PAC bounds on the required budget to hire a near-optimal team with high confidence. Furthermore, we consider an embedding of the tasks and workers in an underlying graph that may arise from task similarities or social ties, and that can provide additional side-observations for faster learning. We then quantify the improvement in the bounds that we can achieve depending on the characteristic properties of this graph structure. We evaluate our methodology on simulated problem instances as well as on real-world crowdsourcing data collected from the oDesk platform. Our methodology and results present an interesting direction of research to tackle the challenges faced by a recruiter for contract-based crowdsourcing. version:1
arxiv-1503-02108 | Maximum a Posteriori Adaptation of Network Parameters in Deep Models | http://arxiv.org/abs/1503.02108 | id:1503.02108 author:Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jiadong Wu, Chin-Hui Lee category:cs.LG cs.CL cs.NE  published:2015-03-06 summary:We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones. version:2
arxiv-1508-02792 | Possible Mechanisms for Neural Reconfigurability and their Implications | http://arxiv.org/abs/1508.02792 | id:1508.02792 author:Thomas M. Breuel category:cs.NE q-bio.NC K.3.2  published:2015-08-12 summary:The paper introduces a biologically and evolutionarily plausible neural architecture that allows a single group of neurons, or an entire cortical pathway, to be dynamically reconfigured to perform multiple, potentially very different computations. The paper shows that reconfigurability can account for the observed stochastic and distributed coding behavior of neurons and provides a parsimonious explanation for timing phenomena in psychophysical experiments. It also shows that reconfigurable pathways correspond to classes of statistical classifiers that include decision lists, decision trees, and hierarchical Bayesian methods. Implications for the interpretation of neurophysiological and psychophysical results are discussed, and future experiments for testing the reconfigurability hypothesis are explored. version:1
arxiv-1508-02790 | On the Convergence of SGD Training of Neural Networks | http://arxiv.org/abs/1508.02790 | id:1508.02790 author:Thomas M. Breuel category:cs.NE cs.LG K.3.2  published:2015-08-12 summary:Neural networks are usually trained by some form of stochastic gradient descent (SGD)). A number of strategies are in common use intended to improve SGD optimization, such as learning rate schedules, momentum, and batching. These are motivated by ideas about the occurrence of local minima at different scales, valleys, and other phenomena in the objective function. Empirical results presented here suggest that these phenomena are not significant factors in SGD optimization of MLP-related objective functions, and that the behavior of stochastic gradient descent in these problems is better described as the simultaneous convergence at different rates of many, largely non-interacting subproblems version:1
arxiv-1508-02788 | The Effects of Hyperparameters on SGD Training of Neural Networks | http://arxiv.org/abs/1508.02788 | id:1508.02788 author:Thomas M. Breuel category:cs.NE cs.LG K.3.2  published:2015-08-12 summary:The performance of neural network classifiers is determined by a number of hyperparameters, including learning rate, batch size, and depth. A number of attempts have been made to explore these parameters in the literature, and at times, to develop methods for optimizing them. However, exploration of parameter spaces has often been limited. In this note, I report the results of large scale experiments exploring these different parameters and their interactions. version:1
arxiv-1508-02774 | Benchmarking of LSTM Networks | http://arxiv.org/abs/1508.02774 | id:1508.02774 author:Thomas M. Breuel category:cs.NE K.3.2  published:2015-08-11 summary:LSTM (Long Short-Term Memory) recurrent neural networks have been highly successful in a number of application areas. This technical report describes the use of the MNIST and UW3 databases for benchmarking LSTM networks and explores the effect of di?erent architectural and hyperparameter choices on performance. Significant ?ndings include: (1) LSTM performance depends smoothly on learning rates, (2) batching and momentum has no significant effect on performance, (3) softmax training outperforms least square training, (4) peephole units are not useful, (5) the standard non-linearities (tanh and sigmoid) perform best, (6) bidirectional training combined with CTC performs better than other methods. version:1
arxiv-1508-00509 | Maintaining prediction quality under the condition of a growing knowledge space | http://arxiv.org/abs/1508.00509 | id:1508.00509 author:Christoph Jahnz category:cs.AI cs.LG  published:2015-08-03 summary:Intelligence can be understood as an agent's ability to predict its environment's dynamic by a level of precision which allows it to effectively foresee opportunities and threats. Under the assumption that such intelligence relies on a knowledge space any effective reasoning would benefit from a maximum portion of useful and a minimum portion of misleading knowledge fragments. It begs the question of how the quality of such knowledge space can be kept high as the amount of knowledge keeps growing. This article proposes a mathematical model to describe general principles of how quality of a growing knowledge space evolves depending on error rate, error propagation and countermeasures. There is also shown to which extend the quality of a knowledge space collapses as removal of low quality knowledge fragments occurs too slowly for a given knowledge space's growth rate. version:2
arxiv-1508-02681 | Artificial Prediction Markets for Online Prediction of Continuous Variables-A Preliminary Report | http://arxiv.org/abs/1508.02681 | id:1508.02681 author:Fatemeh Jahedpari, Marina De Vos, Sattar Hashemi, Benjamin Hirsch, Julian Padget category:cs.AI cs.LG  published:2015-08-11 summary:We propose the Artificial Continuous Prediction Market (ACPM) as a means to predict a continuous real value, by integrating a range of data sources and aggregating the results of different machine learning (ML) algorithms. ACPM adapts the concept of the (physical) prediction market to address the prediction of real values instead of discrete events. Each ACPM participant has a data source, a ML algorithm and a local decision-making procedure that determines what to bid on what value. The contributions of ACPM are: (i) adaptation to changes in data quality by the use of learning in: (a) the market, which weights each market participant to adjust the influence of each on the market prediction and (b) the participants, which use a Q-learning based trading strategy to incorporate the market prediction into their subsequent predictions, (ii) resilience to a changing population of low- and high-performing participants. We demonstrate the effectiveness of ACPM by application to an influenza-like illnesses data set, showing ACPM out-performs a range of well-known regression models and is resilient to variation in data source quality. version:1
arxiv-1505-01728 | Integrating K-means with Quadratic Programming Feature Selection | http://arxiv.org/abs/1505.01728 | id:1505.01728 author:Yamuna Prasad, K. K. Biswas category:cs.CV cs.LG  published:2015-05-07 summary:Several data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets. In this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy. version:2
arxiv-1508-00657 | Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs | http://arxiv.org/abs/1508.00657 | id:1508.00657 author:Miguel Ballesteros, Chris Dyer, Noah A. Smith category:cs.CL  published:2015-08-04 summary:We present extensions to a continuous-state dependency parsing method that makes it applicable to morphologically rich languages. Starting with a high-performance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words. version:2
arxiv-1503-04337 | Communication-efficient sparse regression: a one-shot approach | http://arxiv.org/abs/1503.04337 | id:1503.04337 author:Jason D. Lee, Yuekai Sun, Qiang Liu, Jonathan E. Taylor category:stat.ML cs.LG  published:2015-03-14 summary:We devise a one-shot approach to distributed sparse regression in the high-dimensional setting. The key idea is to average "debiased" or "desparsified" lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines. We also extend the approach to generalized linear models. version:3
arxiv-1412-8464 | Alternating Minimization Algorithm with Automatic Relevance Determination for Transmission Tomography under Poisson Noise | http://arxiv.org/abs/1412.8464 | id:1412.8464 author:Yan Kaganovsky, Shaobo Han, Soysal Degirmenci, David G. Politte, David J. Brady, Joseph A. O'Sullivan, Lawrence Carin category:math.NA stat.ML  published:2014-12-29 summary:We propose a globally convergent alternating minimization (AM) algorithm for image reconstruction in transmission tomography, which extends automatic relevance determination (ARD) to Poisson noise models with Beer's law. The algorithm promotes solutions that are sparse in the pixel/voxel-differences domain by introducing additional latent variables, one for each pixel/voxel, and then learning these variables from the data using a hierarchical Bayesian model. Importantly, the proposed AM algorithm is free of any tuning parameters with image quality comparable to standard penalized likelihood methods. Our algorithm exploits optimization transfer principles which reduce the problem into parallel 1D optimization tasks (one for each pixel/voxel), making the algorithm feasible for large-scale problems. This approach considerably reduces the computational bottleneck of ARD associated with the posterior variances. Positivity constraints inherent in transmission tomography problems are also enforced. We demonstrate the performance of the proposed algorithm for x-ray computed tomography using synthetic and real-world datasets. The algorithm is shown to have much better performance than prior ARD algorithms based on approximate Gaussian noise models, even for high photon flux. version:2
arxiv-1504-06667 | Handling oversampling in dynamic networks using link prediction | http://arxiv.org/abs/1504.06667 | id:1504.06667 author:Benjamin Fish, Rajmonda S. Caceres category:cs.SI cs.LG physics.soc-ph  published:2015-04-24 summary:Oversampling is a common characteristic of data representing dynamic networks. It introduces noise into representations of dynamic networks, but there has been little work so far to compensate for it. Oversampling can affect the quality of many important algorithmic problems on dynamic networks, including link prediction. Link prediction seeks to predict edges that will be added to the network given previous snapshots. We show that not only does oversampling affect the quality of link prediction, but that we can use link prediction to recover from the effects of oversampling. We also introduce a novel generative model of noise in dynamic networks that represents oversampling. We demonstrate the results of our approach on both synthetic and real-world data. version:2
arxiv-1508-02606 | InAR:Inverse Augmented Reality | http://arxiv.org/abs/1508.02606 | id:1508.02606 author:Hao Hu, Hainan Cui category:cs.CV  published:2015-08-11 summary:Augmented reality is the art to seamlessly fuse virtual objects into real ones. In this short note, we address the opposite problem, the inverse augmented reality, that is, given a perfectly augmented reality scene where human is unable to distinguish real objects from virtual ones, how the machine could help do the job. We show by structure from motion (SFM), a simple 3D reconstruction technique from images in computer vision, the real and virtual objects can be easily separated in the reconstructed 3D scene. version:1
arxiv-1408-3967 | Learning Deep Representation for Face Alignment with Auxiliary Attributes | http://arxiv.org/abs/1408.3967 | id:1408.3967 author:Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.LG  published:2014-08-18 summary:In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model. version:4
arxiv-1508-01887 | Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning in Hierarchy | http://arxiv.org/abs/1508.01887 | id:1508.01887 author:Zhanglin Peng, Ya Li, Zhaoquan Cai, Liang Lin category:cs.CV cs.LG cs.NE  published:2015-08-08 summary:This work investigates how the traditional image classification pipelines can be extended into a deep architecture, inspired by recent successes of deep neural networks. We propose a deep boosting framework based on layer-by-layer joint feature boosting and dictionary learning. In each layer, we construct a dictionary of filters by combining the filters from the lower layer, and iteratively optimize the image representation with a joint discriminative-generative formulation, i.e. minimization of empirical classification error plus regularization of analysis image generation over training images. For optimization, we perform two iterating steps: i) to minimize the classification error, select the most discriminative features using the gentle adaboost algorithm; ii) according to the feature selection, update the filters to minimize the regularization on analysis image representation using the gradient descent method. Once the optimization is converged, we learn the higher layer representation in the same way. Our model delivers several distinct advantages. First, our layer-wise optimization provides the potential to build very deep architectures. Second, the generated image representation is compact and meaningful. In several visual recognition tasks, our framework outperforms existing state-of-the-art approaches. version:2
arxiv-1508-02521 | Topology Control of wireless sensor network using Quantum Inspired Genetic algorithm | http://arxiv.org/abs/1508.02521 | id:1508.02521 author:Sajid Ullah, Mussarat Wahid category:cs.NE cs.NI  published:2015-08-11 summary:In this work, an evolving Linked Quantum register has been introduced, which are group vector of binary pair of genes, which in its local proximity represent those nodes that will have high connectivity and keep the energy consumption at low, and which are taken into account for topology control. The register works in higher dimension. Here order-2 Quantum inspired genetic algorithm has been used and also higher order can be used to achieve greater versatility in topology control of nodes. Numerical result has been obtained, analysis is done as how the result has previously been obtained with Quantum genetic algorithm and results are compared too. For future work, factor is hinted which would exploit the algorithm to work in more computational intensive problem. version:1
arxiv-1508-02505 | Simulating Brain Reaction to Methamphetamine Regarding Consumer Personality | http://arxiv.org/abs/1508.02505 | id:1508.02505 author:Maryam Keyvanara, Seyed Amirhassan Monadjemi category:cs.NE q-bio.NC  published:2015-08-11 summary:Addiction, as a nervous disease, can be analysed using mathematical modelling and computer simulations. In this paper, we use an existing mathematical model to predict and simulate human brain response to the consumption of a single dose of methamphetamine. The model is implemented and coded in Matlab. Three types of personalities including introverts, ambiverts and extroverts are studied. The parameters of the mathematical model are calibrated and optimized, according to psychological theories, using a real coded genetic algorithm. The simulations show significant correlation between people response to methamphetamine abuse and their personality. They also show that one of the causes of tendency to stimulants roots in consumers personality traits. The results can be used as a tool for reducing attitude towards addiction. version:1
arxiv-1508-01951 | Crowd Access Path Optimization: Diversity Matters | http://arxiv.org/abs/1508.01951 | id:1508.01951 author:Besmira Nushi, Adish Singla, Anja Gruenheid, Erfan Zamanian, Andreas Krause, Donald Kossmann category:cs.LG cs.DB H.1.2; I.2.6; H.2.5  published:2015-08-08 summary:Quality assurance is one the most important challenges in crowdsourcing. Assigning tasks to several workers to increase quality through redundant answers can be expensive if asking homogeneous sources. This limitation has been overlooked by current crowdsourcing platforms resulting therefore in costly solutions. In order to achieve desirable cost-quality tradeoffs it is essential to apply efficient crowd access optimization techniques. Our work argues that optimization needs to be aware of diversity and correlation of information within groups of individuals so that crowdsourcing redundancy can be adequately planned beforehand. Based on this intuitive idea, we introduce the Access Path Model (APM), a novel crowd model that leverages the notion of access paths as an alternative way of retrieving information. APM aggregates answers ensuring high quality and meaningful confidence. Moreover, we devise a greedy optimization algorithm for this model that finds a provably good approximate plan to access the crowd. We evaluate our approach on three crowdsourced datasets that illustrate various aspects of the problem. Our results show that the Access Path Model combined with greedy optimization is cost-efficient and practical to overcome common difficulties in large-scale crowdsourcing like data sparsity and anonymity. version:2
arxiv-1508-02445 | Removing Biases from Trainable MT Metrics by Using Self-Training | http://arxiv.org/abs/1508.02445 | id:1508.02445 author:Miloš Stanojević category:cs.CL  published:2015-08-10 summary:Most trainable machine translation (MT) metrics train their weights on human judgments of state-of-the-art MT systems outputs. This makes trainable metrics biases in many ways. One of them is preferring longer translations. These biased metrics when used for tuning are evaluating different types of translations -- n-best lists of translations with very diverse quality. Systems tuned with these metrics tend to produce overly long translations that are preferred by the metric but not by humans. This is usually solved by manually tweaking metric's weights to equally value recall and precision. Our solution is more general: (1) it does not address only the recall bias but also all other biases that might be present in the data and (2) it does not require any knowledge of the types of features used which is useful in cases when manual tuning of metric's weights is not possible. This is accomplished by self-training on unlabeled n-best lists by using metric that was initially trained on standard human judgments. One way of looking at this is as domain adaptation from the domain of state-of-the-art MT translations to diverse n-best list translations. version:1
arxiv-1508-02428 | FactorBase: SQL for Learning A Multi-Relational Graphical Model | http://arxiv.org/abs/1508.02428 | id:1508.02428 author:Oliver Schulte, Zhensong Qian category:cs.DB cs.LG H.2.8; H.2.4  published:2015-08-10 summary:We describe FactorBase, a new SQL-based framework that leverages a relational database management system to support multi-relational model discovery. A multi-relational statistical model provides an integrated analysis of the heterogeneous and interdependent data resources in the database. We adopt the BayesStore design philosophy: statistical models are stored and managed as first-class citizens inside a database. Whereas previous systems like BayesStore support multi-relational inference, FactorBase supports multi-relational learning. A case study on six benchmark databases evaluates how our system supports a challenging machine learning application, namely learning a first-order Bayesian network model for an entire database. Model learning in this setting has to examine a large number of potential statistical associations across data tables. Our implementation shows how the SQL constructs in FactorBase facilitate the fast, modular, and reliable development of highly scalable model learning systems. version:1
arxiv-1507-03111 | Kernel Methods for Linear Discrete-Time Equations | http://arxiv.org/abs/1507.03111 | id:1507.03111 author:Fritz Colonius, Boumediene Hamzi category:math.DS math.OC math.ST stat.ML stat.TH  published:2015-07-11 summary:Methods from learning theory are used in the state space of linear dynamical and control systems in order to estimate the system matrices. An application to stabilization via algebraic Riccati equations is included. The approach is illustrated via a series of numerical examples. version:2
arxiv-1508-02405 | Gait Assessment for Multiple Sclerosis Patients Using Microsoft Kinect | http://arxiv.org/abs/1508.02405 | id:1508.02405 author:Farnood Gholami, Daria A. Trojan, Jozsef Kovecses, Wassim M. Haddad, Behnood Gholami category:cs.CV  published:2015-08-10 summary:Gait analysis of patients with neurological disorders, including multiple sclerosis (MS), is important for rehabilitation and treatment. The Mircrosoft Kinect sensor, which was developed for motion recognition in gaming applications, is an ideal candidate for an inexpensive system providing the capability for human gait analysis. In this research, we develop a framework to quantify the gait abnormality of MS patients using a Kinect for Windows camera. In addition to the previously introduced gait indices, a novel set of MS gait indices based on the concept of dynamic time warping is introduced. The newly introduced indices can characterize a patient's gait pattern as a whole and quantify a subject's gait distance from the healthy population. We will investigate the correlation of gait indices with the multiple sclerosis walking scale (MSWS) and the clinical ambulation score. This work establishes the feasibility of using the Kinect sensor for clinical gait assessment for MS patients. version:1
arxiv-1508-02375 | Approximation-Aware Dependency Parsing by Belief Propagation | http://arxiv.org/abs/1508.02375 | id:1508.02375 author:Matthew R. Gormley, Mark Dredze, Jason Eisner category:cs.CL cs.LG  published:2015-08-10 summary:We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood. version:1
arxiv-1508-02373 | Training Conditional Random Fields with Natural Gradient Descent | http://arxiv.org/abs/1508.02373 | id:1508.02373 author:Yuan Cao category:cs.LG  published:2015-08-10 summary:We propose a novel parameter estimation procedure that works efficiently for conditional random fields (CRF). This algorithm is an extension to the maximum likelihood estimation (MLE), using loss functions defined by Bregman divergences which measure the proximity between the model expectation and the empirical mean of the feature vectors. This leads to a flexible training framework from which multiple update strategies can be derived using natural gradient descent (NGD). We carefully choose the convex function inducing the Bregman divergence so that the types of updates are reduced, while making the optimization procedure more effective by transforming the gradients of the log-likelihood loss function. The derived algorithms are very simple and can be easily implemented on top of the existing stochastic gradient descent (SGD) optimization procedure, yet it is very effective as illustrated by experimental results. version:1
arxiv-1508-02344 | Local Algorithms for Block Models with Side Information | http://arxiv.org/abs/1508.02344 | id:1508.02344 author:Elchanan Mossel, Jiaming Xu category:stat.ML cs.CC cs.DC math.PR  published:2015-08-10 summary:There has been a recent interest in understanding the power of local algorithms for optimization and inference problems on sparse graphs. Gamarnik and Sudan (2014) showed that local algorithms are weaker than global algorithms for finding large independent sets in sparse random regular graphs. Montanari (2015) showed that local algorithms are suboptimal for finding a community with high connectivity in the sparse Erd\H{o}s-R\'enyi random graphs. For the symmetric planted partition problem (also named community detection for the block models) on sparse graphs, a simple observation is that local algorithms cannot have non-trivial performance. In this work we consider the effect of side information on local algorithms for community detection under the binary symmetric stochastic block model. In the block model with side information each of the $n$ vertices is labeled $+$ or $-$ independently and uniformly at random; each pair of vertices is connected independently with probability $a/n$ if both of them have the same label or $b/n$ otherwise. The goal is to estimate the underlying vertex labeling given 1) the graph structure and 2) side information in the form of a vertex labeling positively correlated with the true one. Assuming that the ratio between in and out degree $a/b$ is $\Theta(1)$ and the average degree $ (a+b) / 2 = n^{o(1)}$, we characterize three different regimes under which a local algorithm, namely, belief propagation run on the local neighborhoods, maximizes the expected fraction of vertices labeled correctly. Thus, in contrast to the case of symmetric block models without side information, we show that local algorithms can achieve optimal performance for the block model with side information. version:1
arxiv-1508-02297 | Measuring Word Significance using Distributed Representations of Words | http://arxiv.org/abs/1508.02297 | id:1508.02297 author:Adriaan M. J. Schakel, Benjamin J. Wilson category:cs.CL  published:2015-08-10 summary:Distributed representations of words as real-valued vectors in a relatively low-dimensional space aim at extracting syntactic and semantic features from large text corpora. A recently introduced neural network, named word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic information in the direction of the word vectors. In this brief report, it is proposed to use the length of the vectors, together with the term frequency, as measure of word significance in a corpus. Experimental evidence using a domain-specific corpus of abstracts is presented to support this proposal. A useful visualization technique for text corpora emerges, where words are mapped onto a two-dimensional plane and automatically ranked by significance. version:1
arxiv-1508-02285 | Adapting Phrase-based Machine Translation to Normalise Medical Terms in Social Media Messages | http://arxiv.org/abs/1508.02285 | id:1508.02285 author:Nut Limsopatham, Nigel Collier category:cs.CL  published:2015-08-10 summary:Previous studies have shown that health reports in social media, such as DailyStrength and Twitter, have potential for monitoring health conditions (e.g. adverse drug reactions, infectious diseases) in particular communities. However, in order for a machine to understand and make inferences on these health conditions, the ability to recognise when laymen's terms refer to a particular medical concept (i.e.\ text normalisation) is required. To achieve this, we propose to adapt an existing phrase-based machine translation (MT) technique and a vector representation of words to map between a social media phrase and a medical concept. We evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions. Our experimental results show that the combination of a phrase-based MT technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55%. version:1
arxiv-1508-02268 | Dropout Training for SVMs with Data Augmentation | http://arxiv.org/abs/1508.02268 | id:1508.02268 author:Ning Chen, Jun Zhu, Jianfei Chen, Ting Chen category:cs.LG  published:2015-08-10 summary:Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for both linear SVMs and the nonlinear extension with latent representation learning. For linear SVMs, to deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights are analytically updated. For nonlinear latent SVMs, we consider learning one layer of latent representations in SVMs and extend the data augmentation technique in conjunction with first-order Taylor-expansion to deal with the intractable expected non-smooth hinge loss and the nonlinearity of latent representations. Finally, we apply the similar data augmentation ideas to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions, and we further develop a non-linear extension of logistic regression by incorporating one layer of latent representations. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMs further improve the prediction performance on several image datasets. version:1
arxiv-1508-02246 | Feature Learning for Interaction Activity Recognition in RGBD Videos | http://arxiv.org/abs/1508.02246 | id:1508.02246 author:Ngu Nguyen category:cs.CV  published:2015-08-10 summary:This paper proposes a human activity recognition method which is based on features learned from 3D video data without incorporating domain knowledge. The experiments on data collected by RGBD cameras produce results outperforming other techniques. Our feature encoding method follows the bag-of-visual-word model, then we use a SVM classifier to recognise the activities. We do not use skeleton or tracking information and the same technique is applied on color and depth data. version:1
arxiv-1508-02225 | Improve the Evaluation of Translation Fluency by Using Entropy of Matched Sub-segments | http://arxiv.org/abs/1508.02225 | id:1508.02225 author:Hui Yu, Xiaofeng Wu, Wenbin Jiang, Qun Liu, Shouxun Lin category:cs.CL  published:2015-08-10 summary:The widely-used automatic evaluation metrics cannot adequately reflect the fluency of the translations. The n-gram-based metrics, like BLEU, limit the maximum length of matched fragments to n and cannot catch the matched fragments longer than n, so they can only reflect the fluency indirectly. METEOR, which is not limited by n-gram, uses the number of matched chunks but it does not consider the length of each chunk. In this paper, we propose an entropy-based method, which can sufficiently reflect the fluency of translations through the distribution of matched words. This method can easily combine with the widely-used automatic evaluation metrics to improve the evaluation of fluency. Experiments show that the correlations of BLEU and METEOR are improved on sentence level after combining with the entropy-based method on WMT 2010 and WMT 2012. version:1
arxiv-1508-02186 | Model-based SIR for dimension reduction | http://arxiv.org/abs/1508.02186 | id:1508.02186 author:Luca Scrucca category:stat.ME stat.ML  published:2015-08-10 summary:A new dimension reduction method based on Gaussian finite mixtures is proposed as an extension to sliced inverse regression (SIR). The model-based SIR (MSIR) approach allows the main limitation of SIR to be overcome, i.e., failure in the presence of regression symmetric relationships, without the need to impose further assumptions. Extensive numerical studies are presented to compare the new method with some of most popular dimension reduction methods, such as SIR, sliced average variance estimation, principal Hessian direction, and directional regression. MSIR appears sufficiently flexible to accommodate various regression functions, and its performance is comparable with or better, particularly as sample size grows, than other available methods. Lastly, MSIR is illustrated with two real data examples about ozone concentration regression, and hand-written digit classification. version:1
arxiv-1508-02171 | Automatic Extraction of the Passing Strategies of Soccer Teams | http://arxiv.org/abs/1508.02171 | id:1508.02171 author:Laszlo Gyarmati, Xavier Anguera category:cs.CV stat.ML  published:2015-08-10 summary:Technology offers new ways to measure the locations of the players and of the ball in sports. This translates to the trajectories the ball takes on the field as a result of the tactics the team applies. The challenge professionals in soccer are facing is to take the reverse path: given the trajectories of the ball is it possible to infer the underlying strategy/tactic of a team? We propose a method based on Dynamic Time Warping to reveal the tactics of a team through the analysis of repeating series of events. Based on the analysis of an entire season, we derive insights such as passing strategies for maintaining ball possession or counter attacks, and passing styles with a focus on the team or on the capabilities of the individual players. version:1
arxiv-1508-02142 | Feature-based Decipherment for Large Vocabulary Machine Translation | http://arxiv.org/abs/1508.02142 | id:1508.02142 author:Iftekhar Naim, Daniel Gildea category:cs.CL  published:2015-08-10 summary:Orthographic similarities across languages provide a strong signal for probabilistic decipherment, especially for closely related language pairs. The existing decipherment models, however, are not well-suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via MCMC sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence scales to large vocabularies and outperforms the existing generative decipherment models by exploiting the orthographic features. version:1
arxiv-1508-02131 | Learning Structural Kernels for Natural Language Processing | http://arxiv.org/abs/1508.02131 | id:1508.02131 author:Daniel Beck, Trevor Cohn, Christian Hardmeier, Lucia Specia category:cs.CL cs.LG  published:2015-08-10 summary:Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods. version:1
arxiv-1412-6808 | Learning the nonlinear geometry of high-dimensional data: Models and algorithms | http://arxiv.org/abs/1412.6808 | id:1412.6808 author:Tong Wu, Waheed U. Bajwa category:stat.ML cs.CV cs.LG  published:2014-12-21 summary:Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing "related" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering. version:2
arxiv-1404-4812 | Beyond Bell's Theorem II: Scenarios with arbitrary causal structure | http://arxiv.org/abs/1404.4812 | id:1404.4812 author:Tobias Fritz category:quant-ph stat.ML  published:2014-04-18 summary:It has recently been found that Bell scenarios are only a small subclass of interesting setups for studying the non-classical features of quantum theory within spacetime. We find that it is possible to talk about classical correlations, quantum correlations and other kinds of correlations on any directed acyclic graph, and this captures various extensions of Bell scenarios which have been considered in the literature. From a conceptual point of view, the main feature of our approach is its high level of unification: while the notions of source, choice of setting and measurement play all seemingly different roles in a Bell scenario, our formalism shows that they are all instances of the same concept of "event". Our work can also be understood as a contribution to the subject of causal inference with latent variables. Among other things, we introduce hidden Bayesian networks as a generalization of hidden Markov models. version:2
arxiv-1508-02091 | Image Representations and New Domains in Neural Image Captioning | http://arxiv.org/abs/1508.02091 | id:1508.02091 author:Jack Hessel, Nicolas Savva, Michael J. Wilber category:cs.CL cs.CV  published:2015-08-09 summary:We examine the possibility that recent promising results in automatic caption generation are due primarily to language models. By varying image representation quality produced by a convolutional neural network, we find that a state-of-the-art neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations. We replicate this result in a new, fine-grained, transfer learned captioning domain, consisting of 66K recipe image/title pairs. We also provide some experiments regarding the appropriateness of datasets for automatic captioning, and find that having multiple captions per image is beneficial, but not an absolute requirement. version:1
arxiv-1508-02064 | Sensitivity study using machine learning algorithms on simulated r-mode gravitational wave signals from newborn neutron stars | http://arxiv.org/abs/1508.02064 | id:1508.02064 author:Antonis Mytidis, Athanasios A. Panagopoulos, Orestis P. Panagopoulos, Bernard Whiting category:astro-ph.IM cs.LG  published:2015-08-09 summary:This is a follow-up sensitivity study on r-mode gravitational wave signals from newborn neutron stars illustrating the applicability of machine learning algorithms for the detection of long-lived gravitational-wave transients. In this sensitivity study we examine three machine learning algorithms (MLAs): artificial neural networks (ANNs), support vector machines (SVMs) and constrained subspace classifiers (CSCs). The objective of this study is to compare the detection efficiency that MLAs can achieve with the efficiency of conventional detection algorithms discussed in an earlier paper. Comparisons are made using 2 distinct r-mode waveforms. For the training of the MLAs we assumed that some information about the distance to the source is given so that the training was performed over distance ranges not wider than half an order of magnitude. The results of this study suggest that machine learning algorithms are suitable for the detection of long-lived gravitational-wave transients and that when assuming knowledge of the distance to the source, MLAs are at least as efficient as conventional methods. version:1
arxiv-1508-01996 | An Automatic Machine Translation Evaluation Metric Based on Dependency Parsing Model | http://arxiv.org/abs/1508.01996 | id:1508.01996 author:Hui Yu, Xiaofeng Wu, Wenbin Jiang, Qun Liu, ShouXun Lin category:cs.CL  published:2015-08-09 summary:Most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference. These sub-structures are defined by human and can't express all the information in the trees because of the limited length of sub-structures. In addition, the overlapped parts between these sub-structures are computed repeatedly. To avoid these problems, we propose a novel automatic evaluation metric based on dependency parsing model, with no need to define sub-structures by human. First, we train a dependency parsing model by the reference dependency tree. Then we generate the hypothesis dependency tree and the corresponding probability by the dependency parsing model. The quality of the hypothesis can be judged by this probability. In order to obtain the lexicon similarity, we also introduce the unigram F-score to the new metric. Experiment results show that the new metric gets the state-of-the-art performance on system level, and is comparable with METEOR on sentence level. version:1
arxiv-1508-01993 | Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures | http://arxiv.org/abs/1508.01993 | id:1508.01993 author:Ralph Fehrer, Stefan Feuerriegel category:stat.ML cs.CL cs.LG  published:2015-08-09 summary:Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%. version:1
arxiv-1412-8293 | Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels | http://arxiv.org/abs/1412.8293 | id:1412.8293 author:Haim Avron, Vikas Sindhwani, Jiyan Yang, Michael Mahoney category:stat.ML cs.LG math.NA stat.CO  published:2014-12-29 summary:We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem. version:2
arxiv-1508-01991 | Bidirectional LSTM-CRF Models for Sequence Tagging | http://arxiv.org/abs/1508.01991 | id:1508.01991 author:Zhiheng Huang, Wei Xu, Kai Yu category:cs.CL  published:2015-08-09 summary:In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations. version:1
arxiv-1502-05577 | Adaptive system optimization using random directions stochastic approximation | http://arxiv.org/abs/1502.05577 | id:1502.05577 author:Prashanth L. A., Shalabh Bhatnagar, Michael Fu, Steve Marcus category:math.OC cs.LG  published:2015-02-19 summary:We present novel algorithms for simulation optimization using random directions stochastic approximation (RDSA). These include first-order (gradient) as well as second-order (Newton) schemes. We incorporate both continuous-valued as well as discrete-valued perturbations into both our algorithms. The former are chosen to be independent and identically distributed (i.i.d.) symmetric, uniformly distributed random variables (r.v.), while the latter are i.i.d., asymmetric, Bernoulli r.v.s. Our Newton algorithm, with a novel Hessian estimation scheme, requires N-dimensional perturbations and three loss measurements per iteration, whereas the simultaneous perturbation Newton search algorithm of [1] requires 2N-dimensional perturbations and four loss measurements per iteration. We prove the unbiasedness of both gradient and Hessian estimates and asymptotic (strong) convergence for both first-order and second-order schemes. We also provide asymptotic normality results, which in particular establish that the asymmetric Bernoulli variant of Newton RDSA method is better than 2SPSA of [1]. Numerical experiments are used to validate the theoretical results. version:2
arxiv-1405-2881 | Consistency of random forests | http://arxiv.org/abs/1405.2881 | id:1405.2881 author:Erwan Scornet, Gérard Biau, Jean-Philippe Vert category:math.ST stat.ML stat.TH  published:2014-05-12 summary:Random forests are a learning algorithm proposed by Breiman [Mach. Learn. 45 (2001) 5--32] that combines several randomized decision trees and aggregates their predictions by averaging. Despite its wide usage and outstanding practical performance, little is known about the mathematical properties of the procedure. This disparity between theory and practice originates in the difficulty to simultaneously analyze both the randomization process and the highly data-dependent tree structure. In the present paper, we take a step forward in forest exploration by proving a consistency result for Breiman's [Mach. Learn. 45 (2001) 5--32] original algorithm in the context of additive regression models. Our analysis also sheds an interesting light on how random forests can nicely adapt to sparsity. 1. Introduction. Random forests are an ensemble learning method for classification and regression that constructs a number of randomized decision trees during the training phase and predicts by averaging the results. Since its publication in the seminal paper of Breiman (2001), the procedure has become a major data analysis tool, that performs well in practice in comparison with many standard methods. What has greatly contributed to the popularity of forests is the fact that they can be applied to a wide range of prediction problems and have few parameters to tune. Aside from being simple to use, the method is generally recognized for its accuracy and its ability to deal with small sample sizes, high-dimensional feature spaces and complex data structures. The random forest methodology has been successfully involved in many practical problems, including air quality prediction (winning code of the EMC data science global hackathon in 2012, see http://www.kaggle.com/c/dsg-hackathon), chemoinformatics [Svetnik et al. (2003)], ecology [Prasad, Iverson and Liaw (2006), Cutler et al. (2007)], 3D version:4
arxiv-1508-01928 | A variational approach to the consistency of spectral clustering | http://arxiv.org/abs/1508.01928 | id:1508.01928 author:Nicolás García Trillos, Dejan Slepčev category:math.ST cs.LG stat.ML stat.TH  published:2015-08-08 summary:This paper establishes the consistency of spectral approaches to data clustering. We consider clustering of point clouds obtained as samples of a ground-truth measure. A graph representing the point cloud is obtained by assigning weights to edges based on the distance between the points they connect. We investigate the spectral convergence of both unnormalized and normalized graph Laplacians towards the appropriate operators in the continuum domain. We obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the spectral convergence to hold. We also show that the discrete clusters obtained via spectral clustering converge towards a continuum partition of the ground truth measure. Such continuum partition minimizes a functional describing the continuum analogue of the graph-based spectral partitioning. Our approach, based on variational convergence, is general and flexible. version:1
arxiv-1602-07573 | A straightforward method to assess motion blur for different types of displays | http://arxiv.org/abs/1602.07573 | id:1602.07573 author:Fuhao Chen, Jun Chen, Feng Huang category:cs.CV  published:2015-08-08 summary:A simulation method based on the liquid crystal response and the human visual system is suitable to characterize motion blur for LCDs but not other display types. We propose a more straightforward and widely applicable method to quantify motion blur based on the width of the moving object. We thus compare various types of displays objectively. A perceptual experiment was conducted to validate the proposed method. We test varying motion velocities for nine commercial displays. We compare the three motion blur evaluation methods (simulation, human perception, and our method) using z-scores. Our comparisons indicate that our method accurately characterizes motion blur for various display types. version:1
arxiv-1404-6334 | Input anticipating critical reservoirs show power law forgetting of unexpected input events | http://arxiv.org/abs/1404.6334 | id:1404.6334 author:Norbert Michael Mayer category:cs.NE  published:2014-04-25 summary:Usually, reservoir computing shows an exponential memory decay. This paper investigates under which circumstances echo state networks can show a power law forgetting. That means traces of earlier events can be found in the reservoir for very long time spans. Such a setting requires critical connectivity exactly at the limit of what is permissible according the echo state condition. However, for general matrices the limit cannot be determined exactly from theory. In addition, the behavior of the network is strongly influenced by the input flow. Results are presented that use certain types of restricted recurrent connectivity and anticipation learning with regard to the input, where indeed power law forgetting can be achieved. version:5
arxiv-1407-4916 | Extensions of stability selection using subsamples of observations and covariates | http://arxiv.org/abs/1407.4916 | id:1407.4916 author:Andre Beinrucker, Ürün Dogan, Gilles Blanchard category:stat.ME stat.CO stat.ML  published:2014-07-18 summary:We introduce extensions of stability selection, a method to stabilise variable selection methods introduced by Meinshausen and B\"uhlmann (J R Stat Soc 72:417-473, 2010). We propose to apply a base selection method repeatedly to random observation subsamples and covariate subsets under scrutiny, and to select covariates based on their selection frequency. We analyse the effects and benefits of these extensions. Our analysis generalizes the theoretical results of Meinshausen and B\"uhlmann (J R Stat Soc 72:417-473, 2010) from the case of half-samples to subsamples of arbitrary size. We study, in a theoretical manner, the effect of taking random covariate subsets using a simplified score model. Finally we validate these extensions on numerical experiments on both synthetic and real datasets, and compare the obtained results in detail to the original stability selection method. version:3
arxiv-1508-01859 | Simulation of optical flow and fuzzy based obstacle avoidance system for mobile robots | http://arxiv.org/abs/1508.01859 | id:1508.01859 author:G. D. Illeperuma, D. U. J. Sonnadara category:cs.CV cs.RO  published:2015-08-08 summary:Honey bees use optical flow to avoid obstacles effectively. In this research work similar methodology was tested on a simulated mobile robot. Simulation framework was based on VRML and Simulink in a 3D world. Optical flow vectors were calculated from a video scene captured by a virtual camera which was used as inputs to a fuzzy logic controller. Fuzzy logic controller decided the locomotion of the robot. Different fuzzy logic rules were evaluated. The robot was able to navigate through complex static and dynamic environments effectively, avoiding obstacles on its path. version:1
arxiv-1508-01819 | Spectral Clustering and Block Models: A Review And A New Algorithm | http://arxiv.org/abs/1508.01819 | id:1508.01819 author:Sharmodeep Bhattacharyya, Peter J. Bickel category:math.ST cs.SI stat.ML stat.TH  published:2015-08-07 summary:We focus on spectral clustering of unlabeled graphs and review some results on clustering methods which achieve weak or strong consistent identification in data generated by such models. We also present a new algorithm which appears to perform optimally both theoretically using asymptotic theory and empirically. version:1
arxiv-1508-01786 | Mimicry Is Presidential: Linguistic Style Matching in Presidential Debates and Improved Polling Numbers | http://arxiv.org/abs/1508.01786 | id:1508.01786 author:Daniel M. Romero, Roderick I. Swaab, Brian Uzzi, Adam D. Galinsky category:cs.CL cs.SI  published:2015-08-07 summary:The current research used the contexts of U.S. presidential debates and negotiations to examine whether matching the linguistic style of an opponent in a two-party exchange affects the reactions of third-party observers. Building off communication accommodation theory (CAT), interaction alignment theory (IAT), and processing fluency, we propose that language style matching (LSM) will improve subsequent third-party evaluations because matching an opponent's linguistic style reflects greater perspective taking and will make one's arguments easier to process. In contrast, research on status inferences predicts that LSM will negatively impact third-party evaluations because LSM implies followership. We conduct two studies to test these competing hypotheses. Study 1 analyzed transcripts of U.S. presidential debates between 1976 and 2012 and found that candidates who matched their opponent's linguistic style increased their standing in the polls. Study 2 demonstrated a causal relationship between LSM and third-party observer evaluations using negotiation transcripts. version:1
arxiv-1508-01755 | Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking | http://arxiv.org/abs/1508.01755 | id:1508.01755 author:Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young category:cs.CL  published:2015-08-07 summary:The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add significantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predefined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems. version:1
arxiv-1507-08571 | Agglomerative clustering and collectiveness measure via exponent generating function | http://arxiv.org/abs/1507.08571 | id:1507.08571 author:Wei-Ya Ren, Shuo-Hao Li, Qiang Guo, Guo-Hui Li, Jun Zhang category:cs.CV cs.GR  published:2015-07-30 summary:The key in agglomerative clustering is to define the affinity measure between two sets. A novel agglomerative clustering method is proposed by utilizing the path integral to define the affinity measure. Firstly, the path integral descriptor of an edge, a node and a set is computed by path integral and exponent generating function. Then, the affinity measure between two sets is obtained by path integral descriptor of sets. Several good properties of the path integral descriptor is proposed in this paper. In addition, we give the physical interpretation of the proposed path integral descriptor of a set. The proposed path integral descriptor of a set can be regard as the collectiveness measure of a set, which can be a moving system such as human crowd, sheep herd and so on. Self-driven particle (SDP) model is used to test the ability of the proposed method in measuring collectiveness. version:2
arxiv-1508-01718 | Study of Phonemes Confusions in Hierarchical Automatic Phoneme Recognition System | http://arxiv.org/abs/1508.01718 | id:1508.01718 author:Rimah Amami, Noureddine Ellouze category:cs.CL  published:2015-08-07 summary:In this paper, we have analyzed the impact of confusions on the robustness of phoneme recognitions system. The confusions are detected at the pronunciation and the confusions matrices of the phoneme recognizer. The confusions show that some similarities between phonemes at the pronunciation affect significantly the recognition rates. This paper proposes to understand those confusions in order to improve the performance of the phoneme recognition system by isolating the problematic phonemes. Confusion analysis leads to build a new hierarchical recognizer using new phoneme distribution and the information from the confusion matrices. This new hierarchical phoneme recognition system shows significant improvements of the recognition rates on TIMIT database. version:1
arxiv-1508-01713 | Dimension reduction for model-based clustering | http://arxiv.org/abs/1508.01713 | id:1508.01713 author:Luca Scrucca category:stat.ME stat.ML  published:2015-08-07 summary:We introduce a dimension reduction method for visualizing the clustering structure obtained from a finite mixture of Gaussian densities. Information on the dimension reduction subspace is obtained from the variation on group means and, depending on the estimated mixture model, on the variation on group covariances. The proposed method aims at reducing the dimensionality by identifying a set of linear combinations, ordered by importance as quantified by the associated eigenvalues, of the original features which capture most of the cluster structure contained in the data. Observations may then be projected onto such a reduced subspace, thus providing summary plots which help to visualize the clustering structure. These plots can be particularly appealing in the case of high-dimensional data and noisy structure. The new constructed variables capture most of the clustering information available in the data, and they can be further reduced to improve clustering performance. We illustrate the approach on both simulated and real data sets. version:1
arxiv-1508-01667 | Places205-VGGNet Models for Scene Recognition | http://arxiv.org/abs/1508.01667 | id:1508.01667 author:Limin Wang, Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV  published:2015-08-07 summary:VGGNets have turned out to be effective for object recognition in still images. However, it is unable to yield good performance by directly adapting the VGGNet models trained on the ImageNet dataset for scene recognition. This report describes our implementation of training the VGGNets on the large-scale Places205 dataset. Specifically, we train three VGGNet models, namely VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe toolbox with high computational efficiency. We verify the performance of trained Places205-VGGNet models on three datasets: MIT67, SUN397, and Places205. Our trained models achieve the state-of-the-art performance on these datasets and are made public available. version:1
arxiv-1508-01609 | The Contribution of Internal and Model Variabilities to the Uncertainty in CMIP5 Decadal Climate Predictions | http://arxiv.org/abs/1508.01609 | id:1508.01609 author:Ehud Strobach, Golan Bel category:physics.ao-ph physics.data-an stat.ML  published:2015-08-07 summary:Decadal climate predictions, which are initialized with observed conditions, are characterized by two main sources of uncertainties--internal and model variabilities. Using an ensemble of climate model simulations from the CMIP5 decadal experiments, we quantified the total uncertainty associated with these predictions and the relative importance of each source. Annual and monthly averages of the surface temperature and wind components were considered. We show that different definitions of the anomaly results in different conclusions regarding the variance of the ensemble members. However, some features of the uncertainty are common to all the measures we considered. We found that over decadal time scales, there is no considerable increase in the uncertainty with time. The model variability is more sensitive to the annual cycle than the internal variability. This, in turn, results in a maximal uncertainty during the winter in the northern hemisphere. The uncertainty of the surface temperature prediction is dominated by the model variability, whereas the uncertainty of the wind components is determined by both sources. Analysis of the spatial distribution of the uncertainty reveals that the surface temperature has higher variability over land and in high latitudes, whereas the surface zonal wind has higher variability over the ocean. The relative importance of the internal and model variabilities depends on the averaging period, the definition of the anomaly, and the location. These findings suggest that several methods should be combined in order to assess future climate prediction uncertainties and that weighting schemes of the ensemble members may reduce the uncertainties. version:1
arxiv-1508-01596 | Sublinear Partition Estimation | http://arxiv.org/abs/1508.01596 | id:1508.01596 author:Pushpendre Rastogi, Benjamin Van Durme category:stat.ML cs.LG  published:2015-08-07 summary:The output scores of a neural network classifier are converted to probabilities via normalizing over the scores of all competing categories. Computing this partition function, $Z$, is then linear in the number of categories, which is problematic as real-world problem sets continue to grow in categorical types, such as in visual object recognition or discriminative language modeling. We propose three approaches for sublinear estimation of the partition function, based on approximate nearest neighbor search and kernel feature maps and compare the performance of the proposed approaches empirically. version:1
arxiv-1508-01571 | A Mood-based Genre Classification of Television Content | http://arxiv.org/abs/1508.01571 | id:1508.01571 author:Humberto Corona, Michael P. O'Mahony category:cs.IR cs.CL H.3.3  published:2015-08-06 summary:The classification of television content helps users organise and navigate through the large list of channels and programs now available. In this paper, we address the problem of television content classification by exploiting text information extracted from program transcriptions. We present an analysis which adapts a model for sentiment that has been widely and successfully applied in other fields such as music or blog posts. We use a real-world dataset obtained from the Boxfish API to compare the performance of classifiers trained on a number of different feature sets. Our experiments show that, over a large collection of television content, program genres can be represented in a three-dimensional space of valence, arousal and dominance, and that promising classification results can be achieved using features based on this representation. This finding supports the use of the proposed representation of television content as a feature space for similarity computation and recommendation generation. version:1
arxiv-1507-07094 | Compressed Sensing without Sparsity Assumptions | http://arxiv.org/abs/1507.07094 | id:1507.07094 author:Miles E. Lopes category:cs.IT math.IT math.ST stat.ME stat.ML stat.TH  published:2015-07-25 summary:The theory of Compressed Sensing asserts that an unknown signal $x\in\mathbb{R}^p$ can be accurately recovered from an underdetermined set of $n$ linear measurements with $n\ll p$, provided that $x$ is sufficiently sparse. However, in applications, the degree of sparsity $\ x\ _0$ is typically unknown, and the problem of directly estimating $\ x\ _0$ has been a longstanding gap between theory and practice. A closely related issue is that $\ x\ _0$ is a highly idealized measure of sparsity, and for real signals with entries not exactly equal to 0, the value $\ x\ _0=p$ is not a useful description of compressibility. In our previous conference paper that examined these problems, Lopes 2013, we considered an alternative measure of "soft" sparsity, $\ x\ _1^2/\ x\ _2^2$, and designed a procedure to estimate $\ x\ _1^2/\ x\ _2^2$ that does not rely on sparsity assumptions. The present work offers a new deconvolution-based method for estimating unknown sparsity, which has wider applicability and sharper theoretical guarantees. Whereas our earlier work was limited to estimating the quantity $\ x\ _1^2/\ x\ _2^2$, the current paper introduces a family of entropy-based sparsity measures $s_q(x):=\big(\frac{\ x\ _q}{\ x\ _1}\big)^{\frac{q}{1-q}}$ parameterized by $q\in[0,\infty]$. Two other main advantages of the new approach are that it handles measurement noise with infinite variance, and that it yields confidence intervals for $s_q(x)$ with asymptotically exact coverage probability (whereas our previous intervals were conservative). In addition to confidence intervals, we also analyze several other aspects of our proposed estimator $\hat{s}_q(x)$ and show that randomized measurements are an essential aspect of our procedure. version:2
arxiv-1508-01551 | A Knowledge Gradient Policy for Sequencing Experiments to Identify the Structure of RNA Molecules Using a Sparse Additive Belief Model | http://arxiv.org/abs/1508.01551 | id:1508.01551 author:Yan Li, Kristofer G. Reyes, Jorge Vazquez-Anderson, Yingfei Wang, Lydia M. Contreras, Warren B. Powell category:math.OC stat.AP stat.ML  published:2015-08-06 summary:We present a sparse knowledge gradient (SpKG) algorithm for adaptively selecting the targeted regions within a large RNA molecule to identify which regions are most amenable to interactions with other molecules. Experimentally, such regions can be inferred from fluorescence measurements obtained by binding a complementary probe with fluorescence markers to the targeted regions. We use a biophysical model which shows that the fluorescence ratio under the log scale has a sparse linear relationship with the coefficients describing the accessibility of each nucleotide, since not all sites are accessible (due to the folding of the molecule). The SpKG algorithm uniquely combines the Bayesian ranking and selection problem with the frequentist $\ell_1$ regularized regression approach Lasso. We use this algorithm to identify the sparsity pattern of the linear model as well as sequentially decide the best regions to test before experimental budget is exhausted. Besides, we also develop two other new algorithms: batch SpKG algorithm, which generates more suggestions sequentially to run parallel experiments; and batch SpKG with a procedure which we call length mutagenesis. It dynamically adds in new alternatives, in the form of types of probes, are created by inserting, deleting or mutating nucleotides within existing probes. In simulation, we demonstrate these algorithms on the Group I intron (a mid-size RNA molecule), showing that they efficiently learn the correct sparsity pattern, identify the most accessible region, and outperform several other policies. version:1
arxiv-1508-01549 | Theoretical and Empirical Analysis of a Parallel Boosting Algorithm | http://arxiv.org/abs/1508.01549 | id:1508.01549 author:Uday Kamath, Carlotta Domeniconi, Kenneth De Jong category:cs.LG cs.DC  published:2015-08-06 summary:Many real-world problems involve massive amounts of data. Under these circumstances learning algorithms often become prohibitively expensive, making scalability a pressing issue to be addressed. A common approach is to perform sampling to reduce the size of the dataset and enable efficient learning. Alternatively, one customizes learning algorithms to achieve scalability. In either case, the key challenge is to obtain algorithmic efficiency without compromising the quality of the results. In this paper we discuss a meta-learning algorithm (PSBML) which combines features of parallel algorithms with concepts from ensemble and boosting methodologies to achieve the desired scalability property. We present both theoretical and empirical analyses which show that PSBML preserves a critical property of boosting, specifically, convergence to a distribution centered around the margin. We then present additional empirical analyses showing that this meta-level algorithm provides a general and effective framework that can be used in combination with a variety of learning classifiers. We perform extensive experiments to investigate the tradeoff achieved between scalability and accuracy, and robustness to noise, on both synthetic and real-world data. These empirical results corroborate our theoretical analysis, and demonstrate the potential of PSBML in achieving scalability without sacrificing accuracy. version:1
arxiv-1508-01534 | Nonlinear Metric Learning for kNN and SVMs through Geometric Transformations | http://arxiv.org/abs/1508.01534 | id:1508.01534 author:Bibo Shi, Jundong Liu category:cs.LG cs.CV  published:2015-08-06 summary:In recent years, research efforts to extend linear metric learning models to handle nonlinear structures have attracted great interests. In this paper, we propose a novel nonlinear solution through the utilization of deformable geometric models to learn spatially varying metrics, and apply the strategy to boost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS) are chosen as the geometric model due to their remarkable versatility and representation power in accounting for high-order deformations. By transforming the input space through TPS, we can pull same-class neighbors closer while pushing different-class points farther away in kNN, as well as make the input data points more linearly separable in SVMs. Improvements in the performance of kNN classification are demonstrated through experiments on synthetic and real world datasets, with comparisons made with several state-of-the-art metric learning solutions. Our SVM-based models also achieve significant improvements over traditional linear and kernel SVMs with the same datasets. version:1
arxiv-1508-01476 | Hyponymy extraction of domain ontology concept based on ccrfs and hierarchy clustering | http://arxiv.org/abs/1508.01476 | id:1508.01476 author:Qiang Zhan, Chunhong Wang category:cs.CL  published:2015-08-06 summary:Concept hierarchy is the backbone of ontology, and the concept hierarchy acquisition has been a hot topic in the field of ontology learning. this paper proposes a hyponymy extraction method of domain ontology concept based on cascaded conditional random field(CCRFs) and hierarchy clustering. It takes free text as extracting object, adopts CCRFs identifying the domain concepts. First the low layer of CCRFs is used to identify simple domain concept, then the results are sent to the high layer, in which the nesting concepts are recognized. Next we adopt hierarchy clustering to identify the hyponymy relation between domain ontology concepts. The experimental results demonstrate the proposed method is efficient. version:1
arxiv-1508-01158 | Socially Constrained Structural Learning for Groups Detection in Crowd | http://arxiv.org/abs/1508.01158 | id:1508.01158 author:Francesco Solera, Simone Calderara, Rita Cucchiara category:cs.CV  published:2015-08-05 summary:Modern crowd theories agree that collective behavior is the result of the underlying interactions among small groups of individuals. In this work, we propose a novel algorithm for detecting social groups in crowds by means of a Correlation Clustering procedure on people trajectories. The affinity between crowd members is learned through an online formulation of the Structural SVM framework and a set of specifically designed features characterizing both their physical and social identity, inspired by Proxemic theory, Granger causality, DTW and Heat-maps. To adhere to sociological observations, we introduce a loss function (G-MITRE) able to deal with the complexity of evaluating group detection performances. We show our algorithm achieves state-of-the-art results when relying on both ground truth trajectories and tracklets previously extracted by available detector/tracker systems. version:2
arxiv-1508-01447 | Using Linguistic Analysis to Translate Arabic Natural Language Queries to SPARQL | http://arxiv.org/abs/1508.01447 | id:1508.01447 author:Iyad AlAgha category:cs.CL cs.AI cs.DB  published:2015-08-06 summary:The logic-based machine-understandable framework of the Semantic Web often challenges naive users when they try to query ontology-based knowledge bases. Existing research efforts have approached this problem by introducing Natural Language (NL) interfaces to ontologies. These NL interfaces have the ability to construct SPARQL queries based on NL user queries. However, most efforts were restricted to queries expressed in English, and they often benefited from the advancement of English NLP tools. However, little research has been done to support querying the Arabic content on the Semantic Web by using NL queries. This paper presents a domain-independent approach to translate Arabic NL queries to SPARQL by leveraging linguistic analysis. Based on a special consideration on Noun Phrases (NPs), our approach uses a language parser to extract NPs and the relations from Arabic parse trees and match them to the underlying ontology. It then utilizes knowledge in the ontology to group NPs into triple-based representations. A SPARQL query is finally generated by extracting targets and modifiers, and interpreting them into SPARQL. The interpretation of advanced semantic features including negation, conjunctive and disjunctive modifiers is also supported. The approach was evaluated by using two datasets consisting of OWL test data and queries, and the obtained results have confirmed its feasibility to translate Arabic NL queries to SPARQL. version:1
arxiv-1508-01420 | Privacy-Preserving Multi-Document Summarization | http://arxiv.org/abs/1508.01420 | id:1508.01420 author:Luís Marujo, José Portêlo, Wang Ling, David Martins de Matos, João P. Neto, Anatole Gershman, Jaime Carbonell, Isabel Trancoso, Bhiksha Raj category:cs.IR cs.CL cs.CR H.3; I.2.7; K.4.1  published:2015-08-06 summary:State-of-the-art extractive multi-document summarization systems are usually designed without any concern about privacy issues, meaning that all documents are open to third parties. In this paper we propose a privacy-preserving approach to multi-document summarization. Our approach enables other parties to obtain summaries without learning anything else about the original documents' content. We use a hashing scheme known as Secure Binary Embeddings to convert documents representation containing key phrases and bag-of-words into bit strings, allowing the computation of approximate distances, instead of exact ones. Our experiments indicate that our system yields similar results to its non-private counterpart on standard multi-document evaluation datasets. version:1
arxiv-1507-08271 | A Gauss-Newton Method for Markov Decision Processes | http://arxiv.org/abs/1507.08271 | id:1507.08271 author:Thomas Furmston, Guy Lever category:cs.AI cs.LG stat.ML  published:2015-07-29 summary:Approximate Newton methods are a standard optimization tool which aim to maintain the benefits of Newton's method, such as a fast rate of convergence, whilst alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov Decision Processes (MDPs). We first analyse the structure of the Hessian of the objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton Methods for MDPs. Like the Gauss-Newton method for non-linear least squares, these methods involve approximating the Hessian by ignoring certain terms in the Hessian which are difficult to estimate. The approximate Hessians possess desirable properties, such as negative definiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space, and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss-Newton algorithm is closely related to both the EM-algorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains. version:4
arxiv-1508-01349 | Automatic classification of bengali sentences based on sense definitions present in bengali wordnet | http://arxiv.org/abs/1508.01349 | id:1508.01349 author:Alok Ranjan Pal, Diganta Saha, Niladri Sekhar Dash category:cs.CL  published:2015-08-06 summary:Based on the sense definition of words available in the Bengali WordNet, an attempt is made to classify the Bengali sentences automatically into different groups in accordance with their underlying senses. The input sentences are collected from 50 different categories of the Bengali text corpus developed in the TDIL project of the Govt. of India, while information about the different senses of particular ambiguous lexical item is collected from Bengali WordNet. In an experimental basis we have used Naive Bayes probabilistic model as a useful classifier of sentences. We have applied the algorithm over 1747 sentences that contain a particular Bengali lexical item which, because of its ambiguous nature, is able to trigger different senses that render sentences in different meanings. In our experiment we have achieved around 84% accurate result on the sense classification over the total input sentences. We have analyzed those residual sentences that did not comply with our experiment and did affect the results to note that in many cases, wrong syntactic structures and less semantic information are the main hurdles in semantic classification of sentences. The applicational relevance of this study is attested in automatic text classification, machine learning, information extraction, and word sense disambiguation. version:1
arxiv-1508-01346 | Word sense disambiguation: a survey | http://arxiv.org/abs/1508.01346 | id:1508.01346 author:Alok Ranjan Pal, Diganta Saha category:cs.CL  published:2015-08-06 summary:In this paper, we made a survey on Word Sense Disambiguation (WSD). Near about in all major languages around the world, research in WSD has been conducted upto different extents. In this paper, we have gone through a survey regarding the different approaches adopted in different research works, the State of the Art in the performance in this domain, recent works in different Indian languages and finally a survey in Bengali language. We have made a survey on different competitions in this field and the bench mark results, obtained from those competitions. version:1
arxiv-1508-01340 | Universal Approximation of Edge Density in Large Graphs | http://arxiv.org/abs/1508.01340 | id:1508.01340 author:Marc Boullé category:cs.SI cs.DB stat.ML H.2.8; I.5.3; G.3  published:2015-08-06 summary:In this paper, we present a novel way to summarize the structure of large graphs, based on non-parametric estimation of edge density in directed multigraphs. Following coclustering approach, we use a clustering of the vertices, with a piecewise constant estimation of the density of the edges across the clusters, and address the problem of automatically and reliably inferring the number of clusters, which is the granularity of the coclustering. We use a model selection technique with data-dependent prior and obtain an exact evaluation criterion for the posterior probability of edge density estimation models. We demonstrate, both theoretically and empirically, that our data-dependent modeling technique is consistent, resilient to noise, valid non asymptotically and asymptotically behaves as an universal approximator of the true edge density in directed multigraphs. We evaluate our method using artificial graphs and present its practical interest on real world graphs. The method is both robust and scalable. It is able to extract insightful patterns in the unsupervised learning setting and to provide state of the art accuracy when used as a preparation step for supervised learning. version:1
arxiv-1508-01321 | On Gobbledygook and Mood of the Philippine Senate: An Exploratory Study on the Readability and Sentiment of Selected Philippine Senators' Microposts | http://arxiv.org/abs/1508.01321 | id:1508.01321 author:Fatima M. Moncada, Jaderick P. Pabico category:cs.CL cs.CY  published:2015-08-06 summary:This paper presents the findings of a readability assessment and sentiment analysis of selected six Philippine senators' microposts over the popular Twitter microblog. Using the Simple Measure of Gobbledygook (SMOG), tweets of Senators Cayetano, Defensor-Santiago, Pangilinan, Marcos, Guingona, and Escudero were assessed. A sentiment analysis was also done to determine the polarity of the senators' respective microposts. Results showed that on the average, the six senators are tweeting at an eight to ten SMOG level. This means that, at least a sixth grader will be able to understand the senators' tweets. Moreover, their tweets are mostly neutral and their sentiments vary in unison at some period of time. This could mean that a senator's tweet sentiment is affected by specific Philippine-based events. version:1
arxiv-1508-01310 | The study of cuckoo optimization algorithm for production planning problem | http://arxiv.org/abs/1508.01310 | id:1508.01310 author:Afsane Akbarzadeh, Elham Shadkam category:math.OC cs.NE  published:2015-08-06 summary:Constrained Nonlinear programming problems are hard problems, and one of the most widely used and common problems for production planning problem to optimize. In this study, one of the mathematical models of production planning is survey and the problem solved by cuckoo algorithm. Cuckoo Algorithm is efficient method to solve continues non linear problem. Moreover, mentioned models of production planning solved with Genetic algorithm and Lingo software and the results will compared. The Cuckoo Algorithm is suitable choice for optimization in convergence of solution version:1
arxiv-1508-01308 | Collaborative Total Variation: A General Framework for Vectorial TV Models | http://arxiv.org/abs/1508.01308 | id:1508.01308 author:Joan Duran, Michael Moeller, Catalina Sbert, Daniel Cremers category:cs.CV math.HO math.NA math.OC  published:2015-08-06 summary:Even after over two decades, the total variation (TV) remains one of the most popular regularizations for image processing problems and has sparked a tremendous amount of research, particularly to move from scalar to vector-valued functions. In this paper, we consider the gradient of a color image as a three dimensional matrix or tensor with dimensions corresponding to the spatial extend, the differences to other pixels, and the spectral channels. The smoothness of this tensor is then measured by taking different norms along the different dimensions. Depending on the type of these norms one obtains very different properties of the regularization, leading to novel models for color images. We call this class of regularizations collaborative total variation (CTV). On the theoretical side, we characterize the dual norm, the subdifferential and the proximal mapping of the proposed regularizers. We further prove, with the help of the generalized concept of singular vectors, that an $\ell^{\infty}$ channel coupling makes the most prior assumptions and has the greatest potential to reduce color artifacts. Our practical contributions consist of an extensive experimental section where we compare the performance of a large number of collaborative TV methods for inverse problems like denoising, deblurring and inpainting. version:1
arxiv-1401-5311 | Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face Recognition | http://arxiv.org/abs/1401.5311 | id:1401.5311 author:Changxing Ding, Jonghyun Choi, Dacheng Tao, Larry S. Davis category:cs.CV  published:2014-01-21 summary:To perform unconstrained face recognition robust to variations in illumination, pose and expression, this paper presents a new scheme to extract "Multi-Directional Multi-Level Dual-Cross Patterns" (MDML-DCPs) from face images. Specifically, the MDMLDCPs scheme exploits the first derivative of Gaussian operator to reduce the impact of differences in illumination and then computes the DCP feature at both the holistic and component levels. DCP is a novel face image descriptor inspired by the unique textural structure of human faces. It is computationally efficient and only doubles the cost of computing local binary patterns, yet is extremely robust to pose and expression variations. MDML-DCPs comprehensively yet efficiently encodes the invariant characteristics of a face image from multiple levels into patterns that are highly discriminative of inter-personal differences but robust to intra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC 2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local descriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face identification and face verification tasks. More impressively, the best performance is achieved on the challenging LFW and FRGC 2.0 databases by deploying MDML-DCPs in a simple recognition scheme. version:2
arxiv-1508-01248 | Sparse Pseudo-input Local Kriging for Large Non-stationary Spatial Datasets with Exogenous Variables | http://arxiv.org/abs/1508.01248 | id:1508.01248 author:Babak Farmanesh, Arash Pourhabib category:stat.ML  published:2015-08-05 summary:Gaussian process (GP) regression is a powerful tool for building predictive models for spatial systems. However, it does not scale efficiently for large datasets. Particularly, for high-dimensional spatial datasets, i.e., spatial datasets that contain exogenous variables, the performance of GP regression further deteriorates. This paper presents the Sparse Pseudo-input Local Kriging (SPLK) which approximates the full GP for spatial datasets with exogenous variables. SPLK employs orthogonal cuts which decompose the domain into smaller subdomains and then applies a sparse approximation of the full GP in each subdomain. We obtain the continuity of the global predictor by imposing continuity constraints on the boundaries of the neighboring subdomains. The domain decomposition scheme applies independent covariance structures in each region, and as a result, SPLK captures heterogeneous covariance structures. SPLK achieves computational efficiency by utilizing sparse approximation in each subdomain which enables SPLK to accommodate large subdomains that contain many data points and possess a homogenous covariance structure. We Apply the proposed method to real and simulated datasets. We conclude that the combination of orthogonal cuts and sparse approximation makes the proposed method an efficient algorithm for high-dimensional large spatial datasets. version:1
arxiv-1508-01240 | Non-isometric Curve to Surface Matching with Incomplete Data for Functional Calibration | http://arxiv.org/abs/1508.01240 | id:1508.01240 author:Arash Pourhabib, Balabhaskar Balasundaram category:stat.ML  published:2015-08-05 summary:Calibration refers to the process of adjusting features of a computational model that are not observed in the physical process so that the model matches the real process. We propose a framework for calibration when the unobserved features, i.e. calibration parameters, do not assume a single value, but are functionally dependent on other inputs. We demonstrate that this problem is curve to surface matching where the matched curve does not possess the same length as the original curve. Therefore, we perform non-isometric matching of a curve to a surface. Since in practical applications we do not observe a continuous curve but a sample of data points, we use a graph-theoretic approach to solve this matching of incomplete data. We define a graph structure in which the nodes are selected from the incomplete surface and the weights of the edges are decided based on the response values of the curve and surface. We show that the problem of non-isometric incomplete curve to surface matching is a shortest path problem in a directed acyclic graph. We apply the proposed method, graph-theoretic non-isometric matching, to real and synthetic data and demonstrate that the proposed method improves the prediction accuracy in functional calibration. version:1
arxiv-1508-01176 | HFirst: A Temporal Approach to Object Recognition | http://arxiv.org/abs/1508.01176 | id:1508.01176 author:Garrick Orchard, Cedric Meyer, Ralph Etienne-Cummings, Christoph Posch, Nitish Thakor, Ryad Benosman category:cs.CV  published:2015-08-05 summary:This paper introduces a spiking hierarchical model for object recognition which utilizes the precise timing information inherently present in the output of biologically inspired asynchronous Address Event Representation (AER) vision sensors. The asynchronous nature of these systems frees computation and communication from the rigid predetermined timing enforced by system clocks in conventional systems. Freedom from rigid timing constraints opens the possibility of using true timing to our advantage in computation. We show not only how timing can be used in object recognition, but also how it can in fact simplify computation. Specifically, we rely on a simple temporal-winner-take-all rather than more computationally intensive synchronous operations typically used in biologically inspired neural networks for object recognition. This approach to visual computation represents a major paradigm shift from conventional clocked systems and can find application in other sensory modalities and computational tasks. We showcase effectiveness of the approach by achieving the highest reported accuracy to date (97.5\%$\pm$3.5\%) for a previously published four class card pip recognition task and an accuracy of 84.9\%$\pm$1.9\% for a new more difficult 36 class character recognition task. version:1
arxiv-1503-02727 | Video Compressive Sensing for Spatial Multiplexing Cameras using Motion-Flow Models | http://arxiv.org/abs/1503.02727 | id:1503.02727 author:Aswin C. Sankaranarayanan, Lina Xu, Christoph Studer, Yun Li, Kevin Kelly, Richard G. Baraniuk category:cs.CV  published:2015-03-09 summary:Spatial multiplexing cameras (SMCs) acquire a (typically static) scene through a series of coded projections using a spatial light modulator (e.g., a digital micro-mirror device) and a few optical sensors. This approach finds use in imaging applications where full-frame sensors are either too expensive (e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC systems reconstruct static scenes using techniques from compressive sensing (CS). For videos, however, existing acquisition and recovery methods deliver poor quality. In this paper, we propose the CS multi-scale video (CS-MUVI) sensing and recovery framework for high-quality video acquisition and recovery using SMCs. Our framework features novel sensing matrices that enable the efficient computation of a low-resolution video preview, while enabling high-resolution video recovery using convex optimization. To further improve the quality of the reconstructed videos, we extract optical-flow estimates from the low-resolution previews and impose them as constraints in the recovery procedure. We demonstrate the efficacy of our CS-MUVI framework for a host of synthetic and real measured SMC video data, and we show that high-quality videos can be recovered at roughly $60\times$ compression. version:2
arxiv-1508-01128 | Partitioned Shape Modeling with On-the-Fly Sparse Appearance Learning for Anterior Visual Pathway Segmentation | http://arxiv.org/abs/1508.01128 | id:1508.01128 author:Awais Mansoor, Juan J. Cerrolaza, Robert A. Avery, Marius G. Linguraru category:cs.CV  published:2015-08-05 summary:MRI quantification of cranial nerves such as anterior visual pathway (AVP) in MRI is challenging due to their thin small size, structural variation along its path, and adjacent anatomic structures. Segmentation of pathologically abnormal optic nerve (e.g. optic nerve glioma) poses additional challenges due to changes in its shape at unpredictable locations. In this work, we propose a partitioned joint statistical shape model approach with sparse appearance learning for the segmentation of healthy and pathological AVP. Our main contributions are: (1) optimally partitioned statistical shape models for the AVP based on regional shape variations for greater local flexibility of statistical shape model; (2) refinement model to accommodate pathological regions as well as areas of subtle variation by training the model on-the-fly using the initial segmentation obtained in (1); (3) hierarchical deformable framework to incorporate scale information in partitioned shape and appearance models. Our method, entitled PAScAL (PArtitioned Shape and Appearance Learning), was evaluated on 21 MRI scans (15 healthy + 6 glioma cases) from pediatric patients (ages 2-17). The experimental results show that the proposed localized shape and sparse appearance-based learning approach significantly outperforms segmentation approaches in the analysis of pathological data. version:1
arxiv-1508-01108 | Evaluating color texture descriptors under large variations of controlled lighting conditions | http://arxiv.org/abs/1508.01108 | id:1508.01108 author:Claudio Cusano, Paolo Napoletano, Raimondo Schettini category:cs.CV  published:2015-08-05 summary:The recognition of color texture under varying lighting conditions is still an open issue. Several features have been proposed for this purpose, ranging from traditional statistical descriptors to features extracted with neural networks. Still, it is not completely clear under what circumstances a feature performs better than the others. In this paper we report an extensive comparison of old and new texture features, with and without a color normalization step, with a particular focus on how they are affected by small and large variation in the lighting conditions. The evaluation is performed on a new texture database including 68 samples of raw food acquired under 46 conditions that present single and combined variations of light color, direction and intensity. The database allows to systematically investigate the robustness of texture descriptors across a large range of variations of imaging conditions. version:1
arxiv-1508-01084 | Deep Convolutional Networks are Hierarchical Kernel Machines | http://arxiv.org/abs/1508.01084 | id:1508.01084 author:Fabio Anselmi, Lorenzo Rosasco, Cheston Tan, Tomaso Poggio category:cs.LG cs.NE  published:2015-08-05 summary:In i-theory a typical layer of a hierarchical architecture consists of HW modules pooling the dot products of the inputs to the layer with the transformations of a few templates under a group. Such layers include as special cases the convolutional layers of Deep Convolutional Networks (DCNs) as well as the non-convolutional layers (when the group contains only the identity). Rectifying nonlinearities -- which are used by present-day DCNs -- are one of the several nonlinearities admitted by i-theory for the HW module. We discuss here the equivalence between group averages of linear combinations of rectifying nonlinearities and an associated kernel. This property implies that present-day DCNs can be exactly equivalent to a hierarchy of kernel machines with pooling and non-pooling layers. Finally, we describe a conjecture for theoretically understanding hierarchies of such modules. A main consequence of the conjecture is that hierarchies of trained HW modules minimize memory requirements while computing a selective and invariant representation. version:1
