arxiv-1608-05605 | Using Distributed Representations to Disambiguate Biomedical and Clinical Concepts | http://arxiv.org/abs/1608.05605 | id:1608.05605 author:Stéphan Tulkens, Simon Šuster, Walter Daelemans category:cs.CL  published:2016-08-19 summary:In this paper, we report a knowledge-based method for Word Sense Disambiguation in the domains of biomedical and clinical text. We combine word representations created on large corpora with a small number of definitions from the UMLS to create concept representations, which we then compare to representations of the context of ambiguous terms. Using no relational information, we obtain comparable performance to previous approaches on the MSH-WSD dataset, which is a well-known dataset in the biomedical domain. Additionally, our method is fast and easy to set up and extend to other domains. Supplementary materials, including source code, can be found at https: //github.com/clips/yarn version:1
arxiv-1608-05604 | Modeling Human Reading with Neural Attention | http://arxiv.org/abs/1608.05604 | id:1608.05604 author:Michael Hahn, Frank Keller category:cs.CL  published:2016-08-19 summary:When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g.,~using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading. version:1
arxiv-1608-05578 | Haploid-Diploid Algorithms | http://arxiv.org/abs/1608.05578 | id:1608.05578 author:Larry Bull category:cs.NE q-bio.PE  published:2016-08-19 summary:This short paper uses the recently presented idea that the fundamental haploid-diploid lifecycle of all eukaryote organisms exploits a rudimentary form of the Baldwin effect. The general approach presented here differs from all previous known work using diploid representations within evolutionary computation. The role of recombination is also changed from that previously considered in both natural and artificial evolution under the new theory. Using the NK model of fitness landscapes and the RBNK model of gene regulatory networks it is here shown that varying landscape ruggedness varies the benefit of a haploid-diploid approach in comparison to the traditional haploid representation in both cases. version:1
arxiv-1608-05571 | Learning Spatially Regularized Correlation Filters for Visual Tracking | http://arxiv.org/abs/1608.05571 | id:1608.05571 author:Martin Danelljan, Gustav Häger, Fahad Shahbaz Khan, Michael Felsberg category:cs.CV  published:2016-08-19 summary:Robust and accurate visual tracking is one of the most challenging computer vision problems. Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial. Recently, discriminatively learned correlation filters (DCF) have been successfully applied to address this problem for tracking. These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood. However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model. We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is introduced in the learning to penalize correlation filter coefficients depending on their spatial location. Our SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples. We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF. Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014. Our approach achieves state-of-the-art results on all four datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 8.2% respectively, in mean overlap precision, compared to the best existing trackers. version:1
arxiv-1608-05562 | Rigid Slice-To-Volume Medical Image Registration through Markov Random Fields | http://arxiv.org/abs/1608.05562 | id:1608.05562 author:Roque Porchetto, Franco Stramana, Nikos Paragios, Enzo Ferrante category:cs.CV  published:2016-08-19 summary:Rigid slice-to-volume registration is a challenging task, which finds application in medical imaging problems like image fusion for image guided surgeries and motion correction for volume reconstruction. It is usually formulated as an optimization problem and solved using standard continuous methods. In this paper, we discuss how this task be formulated as a discrete labeling problem on a graph. Inspired by previous works on discrete estimation of linear transformations using Markov Random Fields (MRFs), we model it using a pairwise MRF, where the nodes are associated to the rigid parameters, and the edges encode the relation between the variables. We compare the performance of the proposed method to a continuous formulation optimized using simplex, and we discuss how it can be used to further improve the accuracy of our approach. Promising results are obtained using a monomodal dataset composed of magnetic resonance images (MRI) of a beating heart. version:1
arxiv-1608-05560 | Iterative Views Agreement: An Iterative Low-Rank based Structured Optimization Method to Multi-View Spectral Clustering | http://arxiv.org/abs/1608.05560 | id:1608.05560 author:Yang Wang, Wenjie Zhang, Lin Wu, Xuemin Lin, Meng Fang, Shirui Pan category:cs.LG stat.ML  published:2016-08-19 summary:Multi-view spectral clustering, which aims at yielding an agreement or consensus data objects grouping across multi-views with their graph laplacian matrices, is a fundamental clustering problem. Among the existing methods, Low-Rank Representation (LRR) based method is quite superior in terms of its effectiveness, intuitiveness and robustness to noise corruptions. However, it aggressively tries to learn a common low-dimensional subspace for multi-view data, while inattentively ignoring the local manifold structure in each view, which is critically important to the spectral clustering; worse still, the low-rank minimization is enforced to achieve the data correlation consensus among all views, failing to flexibly preserve the local manifold structure for each view. In this paper, 1) we propose a multi-graph laplacian regularized LRR with each graph laplacian corresponding to one view to characterize its local manifold structure. 2) Instead of directly enforcing the low-rank minimization among all views for correlation consensus, we separately impose low-rank constraint on each view, coupled with a mutual structural consensus constraint, where it is able to not only well preserve the local manifold structure but also serve as a constraint for that from other views, which iteratively makes the views more agreeable. Extensive experiments on real-world multi-view data sets demonstrate its superiority. version:1
arxiv-1608-05554 | Learning to Start for Sequence to Sequence Architecture | http://arxiv.org/abs/1608.05554 | id:1608.05554 author:Qingfu Zhu, Weinan Zhang, Lianqiang Zhou, Ting Liu category:cs.CL  published:2016-08-19 summary:The sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences. It typically consists of two parts: an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder's output and the last generated word. However, it faces to the cold start problem when generating the first word as there is no previous word to refer. Existing work mainly use a special start symbol </s>to generate the first word. An obvious drawback of these work is that there is not a learnable relationship between words and the start symbol. Furthermore, it may lead to the error accumulation for decoding when the first word is incorrectly generated. In this paper, we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol. Experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations. version:1
arxiv-1608-05528 | Automatic Selection of Context Configurations for Improved (and Fast) Class-Specific Word Representations | http://arxiv.org/abs/1608.05528 | id:1608.05528 author:Ivan Vulić, Roy Schwartz, Ari Rappoport, Roi Reichart, Anna Korhonen category:cs.CL  published:2016-08-19 summary:Recent work has demonstrated that state-of-the-art word embedding models require different context types to produce high-quality representations for different word classes such as adjectives (A), verbs (V), and nouns (N). This paper is concerned with identifying contexts useful for learning A/V/N-specific representations. We introduce a simple yet effective framework for selecting class-specific context configurations that yield improved representations for each class. We propose an automatic A* style selection algorithm that effectively searches only a fraction of the large configuration space. The results on predicting similarity scores for the A, V, and N subsets of the benchmarking SimLex-999 evaluation set indicate that our method is useful for each class: the improvements are 6% (A), 6% (V), and 5% (N) over the best previously proposed context type for each class. At the same time, the model trains on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in much shorter training time. version:1
arxiv-1608-05518 | On the Existence of a Projective Reconstruction | http://arxiv.org/abs/1608.05518 | id:1608.05518 author:Hon-Leung Lee category:cs.CV  published:2016-08-19 summary:In this note we study the connection between the existence of a projective reconstruction and the existence of a fundamental matrix satisfying the epipolar constraints. version:1
arxiv-1608-05513 | Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network | http://arxiv.org/abs/1608.05513 | id:1608.05513 author:Shraddha Deshmukh, Sagar Gandhi, Pratap Sanap, Vivek Kulkarni category:cs.AI cs.NE  published:2016-08-19 summary:Recently, a multi-level fuzzy min max neural network (MLF) was proposed, which improves the classification accuracy by handling an overlapped region (area of confusion) with the help of a tree structure. In this brief, an extension of MLF is proposed which defines a new boundary region, where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent. A methodology to classify patterns more accurately is presented. Our work enhances the testing procedure by means of data centroids. We exhibit an illustrative example, clearly highlighting the advantage of our approach. Results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate. version:1
arxiv-1608-05512 | Critical Points for Two-view Triangulation | http://arxiv.org/abs/1608.05512 | id:1608.05512 author:Hon-Leung Lee category:math.OC cs.CV math.AG  published:2016-08-19 summary:Two-view triangulation is a problem of minimizing a quadratic polynomial under an equality constraint. We derive a polynomial that encodes the local minimizers of this problem using the theory of Lagrange multipliers. This offers a simpler derivation of the critical points that are given in Hartley-Sturm [6]. version:1
arxiv-1608-04738 | An Efficient Character-Level Neural Machine Translation | http://arxiv.org/abs/1608.04738 | id:1608.04738 author:Shenjian Zhao, Zhihua Zhang category:cs.CL stat.ML  published:2016-08-16 summary:Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems on the task of English-to-French translation. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose an efficient architecture to train a deep character-level neural machine translation by introducing a decimator and an interpolator. The decimator is used to sample the source sequence before encoding while the interpolator is used to resample after decoding. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is much faster and more memory-efficient in training than conventional character-based models. More interestingly, our model is able to translate the misspelled word like human beings. version:2
arxiv-1608-03665 | Learning Structured Sparsity in Deep Neural Networks | http://arxiv.org/abs/1608.03665 | id:1608.03665 author:Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li category:cs.NE cs.LG stat.ML I.2.6; I.5.1  published:2016-08-12 summary:High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in https://github.com/wenwei202/caffe/tree/scnn. version:3
arxiv-1608-04471 | Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm | http://arxiv.org/abs/1608.04471 | id:1608.04471 author:Qiang Liu, Dilin Wang category:stat.ML cs.LG  published:2016-08-16 summary:We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest. version:2
arxiv-1608-05493 | Network Volume Anomaly Detection and Identification in Large-scale Networks based on Online Time-structured Traffic Tensor Tracking | http://arxiv.org/abs/1608.05493 | id:1608.05493 author:Hiroyuki Kasai, Wolfgang Kellerer, Martin Kleinsteuber category:cs.NI stat.ML  published:2016-08-19 summary:This paper addresses network anomography, that is, the problem of inferring network-level anomalies from indirect link measurements. This problem is cast as a low-rank subspace tracking problem for normal flows under incomplete observations, and an outlier detection problem for abnormal flows. Since traffic data is large-scale time-structured data accompanied with noise and outliers under partial observations, an efficient modeling method is essential. To this end, this paper proposes an online subspace tracking of a Hankelized time-structured traffic tensor for normal flows based on the Candecomp/PARAFAC decomposition exploiting the recursive least squares (RLS) algorithm. We estimate abnormal flows as outlier sparse flows via sparsity maximization in the underlying under-constrained linear-inverse problem. A major advantage is that our algorithm estimates normal flows by low-dimensional matrices with time-directional features as well as the spatial correlation of multiple links without using the past observed measurements and the past model parameters. Extensive numerical evaluations show that the proposed algorithm achieves faster convergence per iteration of model approximation, and better volume anomaly detection performance compared to state-of-the-art algorithms. version:1
arxiv-1608-05461 | We Can "See" You via Wi-Fi - An Overview and Beyond | http://arxiv.org/abs/1608.05461 | id:1608.05461 author:Jen-Yin Chang, Kuan-Ying Lee, Yu-Lin Wei, Kate Ching-Ju Lin, Winston Hsu category:cs.CV  published:2016-08-19 summary:Recently, Wi-Fi has caught tremendous attention for its ubiquity, and, motivated by Wi-Fi's low cost and privacy preservation, researchers have been putting lots of investigation into its potential on action recognition and even identification. In this paper, we offer an comprehensive overview on these two topics in Wi-Fi. Also, through looking at these two topics from an unprecedented perspective, we could achieve generality instead of designing specific ad-hoc features for each scenario. Observing the great resemblance of Channel State Information (CSI, a fine-grained information captured from the received Wi-Fi signal) to texture, we proposed a brand-new framework based on computer vision methods. To minimize the effect of location dependency embedded in CSI, we propose a novel de-noising method based on Singular Value Decomposition (SVD) to eliminate the background energy and effectively extract the channel information of signals reflected by human bodies. From the experiments conducted, we demonstrate the feasibility and efficacy of the proposed methods. Also, we conclude factors that would affect the performance and highlight a few promising issues that require further deliberation. version:1
arxiv-1608-05457 | Who did What: A Large-Scale Person-Centered Cloze Dataset | http://arxiv.org/abs/1608.05457 | id:1608.05457 author:Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, David McAllester category:cs.CL  published:2016-08-19 summary:We have constructed a new "Who-did-What" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community. version:1
arxiv-1608-05442 | Semantic Understanding of Scenes through the ADE20K Dataset | http://arxiv.org/abs/1608.05442 | id:1608.05442 author:Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba category:cs.CV  published:2016-08-18 summary:Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects. version:1
arxiv-1608-05426 | Reconsidering Cross-lingual Word Embeddings | http://arxiv.org/abs/1608.05426 | id:1608.05426 author:Omer Levy, Anders Søgaard, Yoav Goldberg category:cs.CL  published:2016-08-18 summary:While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remains vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, is an appealing approach for substantially improving crosslingual word embeddings. version:1
arxiv-1608-05401 | Distributed Optimization of Convex Sum of Non-Convex Functions | http://arxiv.org/abs/1608.05401 | id:1608.05401 author:Shripad Gade, Nitin H. Vaidya category:cs.DC cs.LG math.OC  published:2016-08-18 summary:We present a distributed solution to optimizing a convex function composed of several non-convex functions. Each non-convex function is privately stored with an agent while the agents communicate with neighbors to form a network. We show that coupled consensus and projected gradient descent algorithm proposed in [1] can optimize convex sum of non-convex functions under an additional assumption on gradient Lipschitzness. We further discuss the applications of this analysis in improving privacy in distributed optimization. version:1
arxiv-1608-05374 | DNN-based Speech Synthesis for Indian Languages from ASCII text | http://arxiv.org/abs/1608.05374 | id:1608.05374 author:Srikanth Ronanki, Siva Reddy, Bajibabu Bollepalli, Simon King category:cs.CL  published:2016-08-18 summary:Text-to-Speech synthesis in Indian languages has a seen lot of progress over the decade partly due to the annual Blizzard challenges. These systems assume the text to be written in Devanagari or Dravidian scripts which are nearly phonemic orthography scripts. However, the most common form of computer interaction among Indians is ASCII written transliterated text. Such text is generally noisy with many variations in spelling for the same word. In this paper we evaluate three approaches to synthesize speech from such noisy ASCII text: a naive Uni-Grapheme approach, a Multi-Grapheme approach, and a supervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the ASCII text to a phonetic script, and then learn a Deep Neural Network to synthesize speech from that. We train and test our models on Blizzard Challenge datasets that were transliterated to ASCII using crowdsourcing. Our experiments on Hindi, Tamil and Telugu demonstrate that our models generate speech of competetive quality from ASCII text compared to the speech synthesized from the native scripts. All the accompanying transliterated datasets are released for public access. version:1
arxiv-1608-05347 | Probabilistic Data Analysis with Probabilistic Programming | http://arxiv.org/abs/1608.05347 | id:1608.05347 author:Feras Saad, Vikash Mansinghka category:cs.AI cs.LG stat.ML  published:2016-08-18 summary:Probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries. version:1
arxiv-1608-05343 | Decoupled Neural Interfaces using Synthetic Gradients | http://arxiv.org/abs/1608.05343 | id:1608.05343 author:Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu category:cs.LG  published:2016-08-18 summary:Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation. version:1
arxiv-1608-05339 | Photo Filter Recommendation by Category-Aware Aesthetic Learning | http://arxiv.org/abs/1608.05339 | id:1608.05339 author:Wei-Tse Sun, Ting-Hsuan Chao, Yin-Hsi Kuo, Winston H. Hsu category:cs.CV  published:2016-08-18 summary:Nowadays, social media have become popular platforms for the public to share photos. To make photos more visually appealing, most social media provide filters by which users can change the appearance of their photos without domain knowledge. However, due to the growing number of filter types, it becomes a major issue for users to choose the best filter type instantly. For this purpose, learning image aesthetics takes an important role in image quality ranking problems. In these years, several works have declared that Convolutional Neural Networks outperform traditional methods in image aesthetic categorization, which classifies images into high or low quality. In this paper, we propose a novel method for image aesthetic learning and investigate the effect of filtered images. Instead of binarizing image quality, we adjust the CNN architectures and design a pairwise ranking loss function to learn the aesthetic response for images. By utilizing pairwise image comparison, the models embed aesthetic responses in the hidden layers. Moreover, based on our pilot study, we observe that users usually prefer different filters for various image categories. To improve the aesthetic ranking, we further integrate the image category into our proposed aesthetic-oriented models. Meanwhile, to the best of our knowledge, there is no public dataset for aesthetics judgement with filtered images. We create a new dataset called Filter Aesthetic Comparison Dataset (FACD). The dataset contains around 28,000 filtered images and 42,240 reliable image pairs with aesthetic comparison annotations using Amazon Mechanical Turk. It is the first dataset containing filtered images and the user preference labels. The experimental results show that our method which learns aesthetic ranking by category-aware pairwise learning outperforms the traditional aesthetic classification methods. version:1
arxiv-1608-05277 | Some problems with a Scotsman and a Russian: Caveats on Bayesian and hidden-Markov models (v2.3) | http://arxiv.org/abs/1608.05277 | id:1608.05277 author:Lambert Schomaker category:cs.LG I.2.6  published:2016-08-18 summary:This paper describes a number of fundamental and practical problems in the application of hidden-Markov models and Bayes when applied to cursive-script recognition. Several problems, however, will have an effect in other application areas. The most fundamental problem is the propagation of error in the product of probabilities. This is a common and pervasive problem which deserves more attention. On the basis of Monte Carlo modeling, tables for the expected relative error are given. It seems that it is distributed according to a continuous Poisson distribution over log probabilities. A second essential problem is related to the appropriateness of the Markov assumption. Basic tests will reveal whether a problem requires modeling of the stochastics of seriality, at all. Examples are given of lexical encodings which cover 95-99% classification accuracy of a lexicon, with removed sequence information, for several European languages. Finally, a summary of results on a non- Bayes, non-Markov method in handwriting recognition are presented, with very acceptable results and minimal modeling or training requirements using nearest-mean classification. The suggestion is to keep considering the design of features and exploiting the power and convenience of nearest-neighbour search on contemporary computing platforms. version:1
arxiv-1608-05275 | A Tight Convex Upper Bound on the Likelihood of a Finite Mixture | http://arxiv.org/abs/1608.05275 | id:1608.05275 author:Elad Mezuman, Yair Weiss category:cs.LG stat.ML  published:2016-08-18 summary:The likelihood function of a finite mixture model is a non-convex function with multiple local maxima and commonly used iterative algorithms such as EM will converge to different solutions depending on initial conditions. In this paper we ask: is it possible to assess how far we are from the global maximum of the likelihood? Since the likelihood of a finite mixture model can grow unboundedly by centering a Gaussian on a single datapoint and shrinking the covariance, we constrain the problem by assuming that the parameters of the individual models are members of a large discrete set (e.g. estimating a mixture of two Gaussians where the means and variances of both Gaussians are members of a set of a million possible means and variances). For this setting we show that a simple upper bound on the likelihood can be computed using convex optimization and we analyze conditions under which the bound is guaranteed to be tight. This bound can then be used to assess the quality of solutions found by EM (where the final result is projected on the discrete set) or any other mixture estimation algorithm. For any dataset our method allows us to find a finite mixture model together with a dataset-specific bound on how far the likelihood of this mixture is from the global optimum of the likelihood version:1
arxiv-1608-05267 | Spatial, Structural and Temporal Feature Learning for Human Interaction Prediction | http://arxiv.org/abs/1608.05267 | id:1608.05267 author:Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, Ferdous Sohel category:cs.CV  published:2016-08-18 summary:Human interaction prediction, i.e., the recognition of an ongoing interaction activity before it is completely executed, has a wide range of applications such as human-robot interaction and the prevention of dangerous events. Due to the large variations in appearance and the evolution of scenes, interaction prediction at an early stage is a challenging task. In this paper, a novel structural feature is exploited as a complement together with the spatial and temporal information to improve the performance of interaction prediction. The proposed structural feature is captured by Long Short Term Memory (LSTM) networks, which process the global and local features associated to each frame and each optical flow image. A new ranking score fusion method is then introduced to combine the spatial, temporal and structural models. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods for human interaction prediction on the BIT-Interaction, the TV Human Interaction and the UT-Interaction datasets. version:1
arxiv-1608-05258 | Parameter Learning for Log-supermodular Distributions | http://arxiv.org/abs/1608.05258 | id:1608.05258 author:Tatiana Shpakova, Francis Bach category:stat.ML cs.LG  published:2016-08-18 summary:We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on "perturb-and-MAP" ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data. version:1
arxiv-1608-05246 | How Image Degradations Affect Deep CNN-based Face Recognition? | http://arxiv.org/abs/1608.05246 | id:1608.05246 author:Samil Karahan, Merve Kilinc Yildirim, Kadir Kirtac, Ferhat Sukru Rende, Gultekin Butun, Hazim Kemal Ekenel category:cs.CV  published:2016-08-18 summary:Face recognition approaches that are based on deep convolutional neural networks (CNN) have been dominating the field. The performance improvements they have provided in the so called in-the-wild datasets are significant, however, their performance under image quality degradations have not been assessed, yet. This is particularly important, since in real-world face recognition applications, images may contain various kinds of degradations due to motion blur, noise, compression artifacts, color distortions, and occlusion. In this work, we have addressed this problem and analyzed the influence of these image degradations on the performance of deep CNN-based face recognition approaches using the standard LFW closed-set identification protocol. We have evaluated three popular deep CNN models, namely, the AlexNet, VGG-Face, and GoogLeNet. Results have indicated that blur, noise, and occlusion cause a significant decrease in performance, while deep CNN models are found to be robust to distortions, such as color distortions and change in color balance. version:1
arxiv-1608-05243 | Multilingual Modal Sense Classification using a Convolutional Neural Network | http://arxiv.org/abs/1608.05243 | id:1608.05243 author:Ana Marasović, Anette Frank category:cs.CL  published:2016-08-18 summary:Modal sense classification (MSC) is a special WSD task that depends on the meaning of the proposition in the modal's scope. We explore a CNN architecture for classifying modal sense in English and German. We show that CNNs are superior to manually designed feature-based classifiers and a standard NN classifier. We analyze the feature maps learned by the CNN and identify known and previously unattested linguistic features. We benchmark the CNN on a standard WSD task, where it compares favorably to models using sense-disambiguated target vectors. version:1
arxiv-1608-05225 | Active Learning for Approximation of Expensive Functions with Normal Distributed Output Uncertainty | http://arxiv.org/abs/1608.05225 | id:1608.05225 author:Joachim van der Herten, Ivo Couckuyt, Dirk Deschrijver, Tom Dhaene category:cs.LG stat.ML  published:2016-08-18 summary:When approximating a black-box function, sampling with active learning focussing on regions with non-linear responses tends to improve accuracy. We present the FLOLA-Voronoi method introduced previously for deterministic responses, and theoretically derive the impact of output uncertainty. The algorithm automatically puts more emphasis on exploration to provide more information to the models. version:1
arxiv-1608-05209 | Efficient Multi-Frequency Phase Unwrapping using Kernel Density Estimation | http://arxiv.org/abs/1608.05209 | id:1608.05209 author:Felix Järemo Lawin, Per-Erik Forssén, Hannes Ovrén category:cs.CV  published:2016-08-18 summary:In this paper we introduce an efficient method to unwrap multi-frequency phase estimates for time-of-flight ranging. The algorithm generates multiple depth hypotheses and uses a spatial kernel density estimate (KDE) to rank them. The confidence produced by the KDE is also an effective means to detect outliers. We also introduce a new closed-form expression for phase noise prediction, that better fits real data. The method is applied to depth decoding for the Kinect v2 sensor, and compared to the Microsoft Kinect SDK and to the open source driver libfreenect2. The intended Kinect v2 use case is scenes with less than 8m range, and for such cases we observe consistent improvements, while maintaining real-time performance. When extending the depth range to the maximal value of 8.75m, we get about 52% more valid measurements than libfreenect2. The effect is that the sensor can now be used in large depth scenes, where it was previously not a good choice. Code and supplementary material are available at http://www.cvl.isy.liu.se/research/datasets/kinect2-dataset. version:1
arxiv-1608-05204 | Refining Geometry from Depth Sensors using IR Shading Images | http://arxiv.org/abs/1608.05204 | id:1608.05204 author:Gyeongmin Choe, Jaesik Park, Yu-Wing Tai, In So Kweon category:cs.CV  published:2016-08-18 summary:We propose a method to refine geometry of 3D meshes from a consumer level depth camera, e.g. Kinect, by exploiting shading cues captured from an infrared (IR) camera. A major benefit to using an IR camera instead of an RGB camera is that the IR images captured are narrow band images that filter out most undesired ambient light, which makes our system robust against natural indoor illumination. Moreover, for many natural objects with colorful textures in the visible spectrum, the subjects appear to have a uniform albedo in the IR spectrum. Based on our analyses on the IR projector light of the Kinect, we define a near light source IR shading model that describes the captured intensity as a function of surface normals, albedo, lighting direction, and distance between light source and surface points. To resolve the ambiguity in our model between the normals and distances, we utilize an initial 3D mesh from the Kinect fusion and multi-view information to reliably estimate surface details that were not captured and reconstructed by the Kinect fusion. Our approach directly operates on the mesh model for geometry refinement. We ran experiments on our algorithm for geometries captured by both the Kinect I and Kinect II, as the depth acquisition in Kinect I is based on a structured-light technique and that of the Kinect II is based on a time-of-flight (ToF) technology. The effectiveness of our approach is demonstrated through several challenging real-world examples. We have also performed a user study to evaluate the quality of the mesh models before and after our refinements. version:1
arxiv-1608-05203 | Seeing with Humans: Gaze-Assisted Neural Image Captioning | http://arxiv.org/abs/1608.05203 | id:1608.05203 author:Yusuke Sugano, Andreas Bulling category:cs.CV  published:2016-08-18 summary:Gaze reflects how humans process visual scenes and is therefore increasingly used in computer vision systems. Previous works demonstrated the potential of gaze for object-centric tasks, such as object localization and recognition, but it remains unclear if gaze can also be beneficial for scene-centric tasks, such as image captioning. We present a new perspective on gaze-assisted image captioning by studying the interplay between human gaze and the attention mechanism of deep neural networks. Using a public large-scale gaze dataset, we first assess the relationship between state-of-the-art object and scene recognition models, bottom-up visual saliency, and human gaze. We then propose a novel split attention model for image captioning. Our model integrates human gaze information into an attention-based long short-term memory architecture, and allows the algorithm to allocate attention selectively to both fixated and non-fixated image regions. Through evaluation on the COCO/SALICON datasets we show that our method improves image captioning performance and that gaze can complement machine attention for semantic scene understanding tasks. version:1
arxiv-1608-04187 | Occlusion-Model Guided Anti-Occlusion Depth Estimation in Light Field | http://arxiv.org/abs/1608.04187 | id:1608.04187 author:Hao Zhu, Qing Wang, Jingyi Yu category:cs.CV I.5.4  published:2016-08-15 summary:Occlusion is one of the most challenging problems in depth estimation. Previous work has modeled the single-occluder occlusion in light field and get good results, however it is still difficult to obtain accurate depth for multi-occluder occlusion. In this paper, we explore the multi-occluder occlusion model in light field, and derive the occluder-consistency between the spatial and angular space which is used as a guidance to select the un-occluded views for each candidate occlusion point. Then an anti-occlusion energy function is built to regularize depth map. The experimental results on public light field datasets have demonstrated the advantages of the proposed algorithm compared with other state-of-the-art light field depth estimation algorithms, especially in multi-occluder areas. version:2
arxiv-1608-05186 | Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs | http://arxiv.org/abs/1608.05186 | id:1608.05186 author:Youbao Tang, Xiangqian Wu category:cs.CV  published:2016-08-18 summary:This paper proposes a novel saliency detection method by combining region-level saliency estimation and pixel-level saliency prediction with CNNs (denoted as CRPSD). For pixel-level saliency prediction, a fully convolutional neural network (called pixel-level CNN) is constructed by modifying the VGGNet architecture to perform multi-scale feature learning, based on which an image-to-image prediction is conducted to accomplish the pixel-level saliency detection. For region-level saliency estimation, an adaptive superpixel based region generation technique is first designed to partition an image into regions, based on which the region-level saliency is estimated by using a CNN model (called region-level CNN). The pixel-level and region-level saliencies are fused to form the final salient map by using another CNN (called fusion CNN). And the pixel-level CNN and fusion CNN are jointly learned. Extensive quantitative and qualitative experiments on four public benchmark datasets demonstrate that the proposed method greatly outperforms the state-of-the-art saliency detection approaches. version:1
arxiv-1608-05182 | A Bayesian Nonparametic Approach for Estimating Individualized Treatment-Response Curves | http://arxiv.org/abs/1608.05182 | id:1608.05182 author:Yanbo Xu, Yanxun Xu, Suchi Saria category:cs.LG stat.ML  published:2016-08-18 summary:We study the problem of estimating the continuous response over time of interventions from observational time series---a retrospective dataset where the policy by which the data are generated are unknown to the learner. We are motivated by applications where response varies by individuals and therefore, estimating responses at the individual-level are valuable for personalizing decision-making. We refer to this as the problem of estimating individualized treatment response (ITR) curves. In statistics, G-computation formula has been commonly used for estimating treatment responses from observational data containing sequential treatment assignments. However, past studies have focused predominantly on obtaining point-in-time estimates at the population level. We leverage G-computation formula and develop a novel method based on Bayesian nonparametrics (BNP) that can flexibly model functional data and provide posterior inference over the treatment response curves both at the individual and population level. On a challenging dataset containing time series from patients admitted to a hospital, we estimate treatment responses for treatments used in managing kidney function and show that the resulting fits are more accurate than alternative approaches. Accurate methods for obtaining ITRs from observational data can dramatically accelerate the pace at which personalized treatment plans become possible. version:1
arxiv-1608-05180 | A Holistic Approach for Data-Driven Object Cutout | http://arxiv.org/abs/1608.05180 | id:1608.05180 author:Huayong Xu, Yangyan Li, Wenzheng Chen, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen category:cs.CV  published:2016-08-18 summary:Object cutout is a fundamental operation for image editing and manipulation, yet it is extremely challenging to automate it in real-world images, which typically contain considerable background clutter. In contrast to existing cutout methods, which are based mainly on low-level image analysis, we propose a more holistic approach, which considers the entire shape of the object of interest by leveraging higher-level image analysis and learnt global shape priors. Specifically, we leverage a deep neural network (DNN) trained for objects of a particular class (chairs) for realizing this mechanism. Given a rectangular image region, the DNN outputs a probability map (P-map) that indicates for each pixel inside the rectangle how likely it is to be contained inside an object from the class of interest. We show that the resulting P-maps may be used to evaluate how likely a rectangle proposal is to contain an instance of the class, and further process good proposals to produce an accurate object cutout mask. This amounts to an automatic end-to-end pipeline for catergory-specific object cutout. We evaluate our approach on segmentation benchmark datasets, and show that it significantly outperforms the state-of-the-art on them. version:1
arxiv-1608-05177 | Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection | http://arxiv.org/abs/1608.05177 | id:1608.05177 author:Youbao Tang, Xiangqian Wu, Wei Bu category:cs.CV  published:2016-08-18 summary:This paper proposes a novel saliency detection method by developing a deeply-supervised recurrent convolutional neural network (DSRCNN), which performs a full image-to-image saliency prediction. For saliency detection, the local, global, and contextual information of salient objects is important to obtain a high quality salient map. To achieve this goal, the DSRCNN is designed based on VGGNet-16. Firstly, the recurrent connections are incorporated into each convolutional layer, which can make the model more powerful for learning the contextual information. Secondly, side-output layers are added to conduct the deeply-supervised operation, which can make the model learn more discriminative and robust features by effecting the intermediate layers. Finally, all of the side-outputs are fused to integrate the local and global information to get the final saliency detection results. Therefore, the DSRCNN combines the advantages of recurrent convolutional neural networks and deeply-supervised nets. The DSRCNN model is tested on five benchmark datasets, and experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art saliency detection approaches on all test datasets. version:1
arxiv-1608-05167 | AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification | http://arxiv.org/abs/1608.05167 | id:1608.05167 author:Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang category:cs.CV  published:2016-08-18 summary:Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in remote sensing area and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing datasets for aerial scene classification like UC-Merced dataset and WHU-RS19 are with relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image Dataset (AID): a large-scale dataset for aerial scene classification. The goal of AID is to advance the state-of-the-arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than ten thousands aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely-used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark. version:1
arxiv-1608-04170 | Every Filter Extracts A Specific Texture In Convolutional Neural Networks | http://arxiv.org/abs/1608.04170 | id:1608.04170 author:Zhiqiang Xia, Ce Zhu, Zhengtao Wang, Qi Guo, Yipeng Liu category:cs.CV  published:2016-08-15 summary:Many works have concentrated on visualizing and understanding the inner mechanism of convolutional neural networks (CNNs) by generating images that activate some specific neurons, which is called deep visualization. However, it is still unclear what the filters extract from images intuitively. In this paper, we propose a modified code inversion algorithm, called feature map inversion, to understand the function of filter of interest in CNNs. We reveal that every filter extracts a specific texture. The texture from higher layer contains more colours and more intricate structures. We also demonstrate that style of images could be a combination of these texture primitives. Two methods are proposed to reallocate energy distribution of feature maps randomly and purposefully. Then, we inverse the modified code and generate images of diverse styles. With these results, we provide an explanation about why Gram matrix of feature maps \cite{Gatys_2016_CVPR} could represent image style. version:2
arxiv-1608-05159 | Multi-stage Object Detection with Group Recursive Learning | http://arxiv.org/abs/1608.05159 | id:1608.05159 author:Jianan Li, Xiaodan Liang, Jianshu Li, Tingfa Xu, Jiashi Feng, Shuicheng Yan category:cs.CV  published:2016-08-18 summary:Most of existing detection pipelines treat object proposals independently and predict bounding box locations and classification scores over them separately. However, the important semantic and spatial layout correlations among proposals are often ignored, which are actually useful for more accurate object detection. In this work, we propose a new EM-like group recursive learning approach to iteratively refine object proposals by incorporating such context of surrounding proposals and provide an optimal spatial configuration of object detections. In addition, we propose to incorporate the weakly-supervised object segmentation cues and region-based object detection into a multi-stage architecture in order to fully exploit the learned segmentation features for better object detection in an end-to-end way. The proposed architecture consists of three cascaded networks which respectively learn to perform weakly-supervised object segmentation, object proposal generation and recursive detection refinement. Combining the group recursive learning and the multi-stage architecture provides competitive mAPs of 78.6% and 74.9% on the PASCAL VOC2007 and VOC2012 datasets respectively, which outperforms many well-established baselines [10] [20] significantly. version:1
arxiv-1608-04647 | Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets | http://arxiv.org/abs/1608.04647 | id:1608.04647 author:Michael J. Anderson, Mihai Capotă, Javier S. Turek, Xia Zhu, Theodore L. Willke, Yida Wang, Po-Hsuan Chen, Jeremy R. Manning, Peter J. Ramadge, Kenneth A. Norman category:stat.ML cs.DC cs.LG 68W15 I.2  published:2016-08-16 summary:The scale of functional magnetic resonance image data is rapidly increasing as large multi-subject datasets are becoming widely available and high-resolution scanners are adopted. The inherent low-dimensionality of the information in this data has led neuroscientists to consider factor analysis methods to extract and analyze the underlying brain activity. In this work, we consider two recent multi-subject factor analysis methods: the Shared Response Model and Hierarchical Topographic Factor Analysis. We perform analytical, algorithmic, and code optimization to enable multi-node parallel implementations to scale. Single-node improvements result in 99x and 1812x speedups on these two methods, and enables the processing of larger datasets. Our distributed implementations show strong scaling of 3.3x and 5.5x respectively with 20 nodes on real datasets. We also demonstrate weak scaling on a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768 cores. version:2
arxiv-1608-03045 | Combinatorial Inference for Graphical Models | http://arxiv.org/abs/1608.03045 | id:1608.03045 author:Matey Neykov, Junwei Lu, Han Liu category:math.ST stat.ML stat.TH  published:2016-08-10 summary:We propose a new family of combinatorial inference problems for graphical models. Unlike classical statistical inference where the main interest is point estimation or parameter testing, combinatorial inference aims at testing the global structure of the underlying graph. Examples include testing the graph connectivity, the presence of a cycle of certain size, or the maximum degree of the graph. To begin with, we develop a unified theory for the fundamental limits of a large family of combinatorial inference problems. We propose new concepts including structural packing and buffer entropies to characterize how the complexity of combinatorial graph structures impacts the corresponding minimax lower bounds. On the other hand, we propose a family of novel and practical structural testing algorithms to match the lower bounds. We provide thorough numerical results on both synthetic graphical models and brain networks to illustrate the usefulness of these proposed methods. version:2
arxiv-1608-05152 | Conditional Sparse Linear Regression | http://arxiv.org/abs/1608.05152 | id:1608.05152 author:Brendan Juba category:cs.LG cs.DS stat.ML  published:2016-08-18 summary:Machine learning and statistics typically focus on building models that capture the vast majority of the data, possibly ignoring a small subset of data as "noise" or "outliers." By contrast, here we consider the problem of jointly identifying a significant (but perhaps small) segment of a population in which there is a highly sparse linear regression fit, together with the coefficients for the linear fit. We contend that such tasks are of interest both because the models themselves may be able to achieve better predictions in such special cases, but also because they may aid our understanding of the data. We give algorithms for such problems under the sup norm, when this unknown segment of the population is described by a k-DNF condition and the regression fit is s-sparse for constant k and s. For the variants of this problem when the regression fit is not so sparse or using expected error, we also give a preliminary algorithm and highlight the question as a challenge for future work. version:1
arxiv-1608-05148 | Full Resolution Image Compression with Recurrent Neural Networks | http://arxiv.org/abs/1608.05148 | id:1608.05148 author:George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell category:cs.CV  published:2016-08-18 summary:This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding. version:1
arxiv-1608-05138 | Hybrid CPU-GPU Framework for Network Motifs | http://arxiv.org/abs/1608.05138 | id:1608.05138 author:Ryan A. Rossi, Rong Zhou category:cs.DC cs.SI stat.ML H.2.8; I.2.6; G.1.0  published:2016-08-18 summary:Massively parallel architectures such as the GPU are becoming increasingly important due to the recent proliferation of data. In this paper, we propose a key class of hybrid parallel graphlet algorithms that leverages multiple CPUs and GPUs simultaneously for computing k-vertex induced subgraph statistics (called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we also investigate single GPU methods (using multiple cores) and multi-GPU methods that leverage all available GPUs simultaneously for computing induced subgraph statistics. Both methods leverage GPU devices only, whereas the hybrid multi-core CPU-GPU framework leverages all available multi-core CPUs and multiple GPUs for computing graphlets in large networks. Compared to recent approaches, our methods are orders of magnitude faster, while also more cost effective enjoying superior performance per capita and per watt. In particular, the methods are up to 300 times faster than the recent state-of-the-art method. To the best of our knowledge, this is the first work to leverage multiple CPUs and GPUs simultaneously for computing induced subgraph statistics. version:1
arxiv-1608-05137 | IM2CAD | http://arxiv.org/abs/1608.05137 | id:1608.05137 author:Hamid Izadinia, Qi Shan, Steven M. Seitz category:cs.CV  published:2016-08-18 summary:Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database. We present a completely automatic system to address this IM2CAD problem that produces high quality results on challenging imagery from real estate web sites. Our approach iteratively optimizes the placement and scale of objects in the room to best match scene renderings to the input photo, used image comparison metrics trained using deep convolutional neural nets. By operating jointly on the full scene at once, we account for inter-object occlusions. version:1
arxiv-1608-05129 | SlangSD: Building and Using a Sentiment Dictionary of Slang Words for Short-Text Sentiment Classification | http://arxiv.org/abs/1608.05129 | id:1608.05129 author:Liang Wu, Fred Morstatter, Huan Liu category:cs.CL  published:2016-08-17 summary:Sentiment in social media is increasingly considered as an important resource for customer segmentation, market understanding, and tackling other socio-economic issues. However, sentiment in social media is difficult to measure since user-generated content is usually short and informal. Although many traditional sentiment analysis methods have been proposed, identifying slang sentiment words remains untackled. One of the reasons is that slang sentiment words are not available in existing dictionaries or sentiment lexicons. To this end, we propose to build the first sentiment dictionary of slang words to aid sentiment analysis of social media content. It is laborious and time-consuming to collect and label the sentiment polarity of a comprehensive list of slang words. We present an approach to leverage web resources to construct an extensive Slang Sentiment word Dictionary (SlangSD) that is easy to maintain and extend. SlangSD is publicly available for research purposes. We empirically show the advantages of using SlangSD, the newly-built slang sentiment word dictionary for sentiment classification, and provide examples demonstrating its ease of use with an existing sentiment system. version:1
arxiv-1608-05127 | A Bayesian Network approach to County-Level Corn Yield Prediction using historical data and expert knowledge | http://arxiv.org/abs/1608.05127 | id:1608.05127 author:Vikas Chawla, Hsiang Sing Naik, Adedotun Akintayo, Dermot Hayes, Patrick Schnable, Baskar Ganapathysubramanian, Soumik Sarkar category:cs.LG stat.AP stat.ML  published:2016-08-17 summary:Crop yield forecasting is the methodology of predicting crop yields prior to harvest. The availability of accurate yield prediction frameworks have enormous implications from multiple standpoints, including impact on the crop commodity futures markets, formulation of agricultural policy, as well as crop insurance rating. The focus of this work is to construct a corn yield predictor at the county scale. Corn yield (forecasting) depends on a complex, interconnected set of variables that include economic, agricultural, management and meteorological factors. Conventional forecasting is either knowledge-based computer programs (that simulate plant-weather-soil-management interactions) coupled with targeted surveys or statistical model based. The former is limited by the need for painstaking calibration, while the latter is limited to univariate analysis or similar simplifying assumptions that fail to capture the complex interdependencies affecting yield. In this paper, we propose a data-driven approach that is "gray box" i.e. that seamlessly utilizes expert knowledge in constructing a statistical network model for corn yield forecasting. Our multivariate gray box model is developed on Bayesian network analysis to build a Directed Acyclic Graph (DAG) between predictors and yield. Starting from a complete graph connecting various carefully chosen variables and yield, expert knowledge is used to prune or strengthen edges connecting variables. Subsequently the structure (connectivity and edge weights) of the DAG that maximizes the likelihood of observing the training data is identified via optimization. We curated an extensive set of historical data (1948-2012) for each of the 99 counties in Iowa as data to train the model. version:1
arxiv-1608-05109 | Optimal Management of Naturally Regenerating Uneven-aged Forests | http://arxiv.org/abs/1608.05109 | id:1608.05109 author:Ankur Sinha, Janne Rämö, Pekka Malo, Markku Kallio, Olli Tahvonen category:math.OC cs.NE G.1.6; I.2.8  published:2016-08-17 summary:A shift from even-aged forest management to uneven-aged management practices leads to a problem rather different from the existing straightforward practice that follows a rotation cycle of artificial regeneration, thinning of inferior trees and a clearcut. A lack of realistic models and methods suggesting how to manage uneven-aged stands in a way that is economically viable and ecologically sustainable creates difficulties in adopting this new management practice. To tackle this problem, we make a two-fold contribution in this paper. The first contribution is the proposal of an algorithm that is able to handle a realistic uneven-aged stand management model that is otherwise computationally tedious and intractable. The model considered in this paper is an empirically estimated size-structured ecological model for uneven-aged spruce forests. The second contribution is on the sensitivity analysis of the forest model with respect to a number of important parameters. The analysis provides us an insight into the behavior of the uneven-aged forest model. version:1
arxiv-1608-05105 | Evolutionary Approaches to Optimization Problems in Chimera Topologies | http://arxiv.org/abs/1608.05105 | id:1608.05105 author:Roberto Santana, Zheng Zhu, Helmut G. Katzgraber category:cond-mat.dis-nn cs.NE quant-ph  published:2016-08-17 summary:Chimera graphs define the topology of one of the first commercially available quantum computers. A variety of optimization problems have been mapped to this topology to evaluate the behavior of quantum enhanced optimization heuristics in relation to other optimizers, being able to efficiently solve problems classically to use them as benchmarks for quantum machines. In this paper we investigate for the first time the use of Evolutionary Algorithms (EAs) on Ising spin glass instances defined on the Chimera topology. Three genetic algorithms (GAs) and three estimation of distribution algorithms (EDAs) are evaluated over $1000$ hard instances of the Ising spin glass constructed from Sidon sets. We focus on determining whether the information about the topology of the graph can be used to improve the results of EAs and on identifying the characteristics of the Ising instances that influence the success rate of GAs and EDAs. version:1
arxiv-1608-05104 | Scene Labeling Through Knowledge-Based Rules Employing Constrained Integer Linear Programing | http://arxiv.org/abs/1608.05104 | id:1608.05104 author:Nasim Souly, Mubarak Shah category:cs.CV  published:2016-08-17 summary:Scene labeling task is to segment the image into meaningful regions and categorize them into classes of objects which comprised the image. Commonly used methods typically find the local features for each segment and label them using classifiers. Afterward, labeling is smoothed in order to make sure that neighboring regions receive similar labels. However, they ignore expressive and non-local dependencies among regions due to expensive training and inference. In this paper, we propose to use high-level knowledge regarding rules in the inference to incorporate dependencies among regions in the image to improve scores of classification. Towards this aim, we extract these rules from data and transform them into constraints for Integer Programming to optimize the structured problem of assigning labels to super-pixels (consequently pixels) of an image. In addition, we propose to use soft-constraints in some scenarios, allowing violating the constraint by imposing a penalty, to make the model more flexible. We assessed our approach on three datasets and obtained promising results. version:1
arxiv-1608-05081 | Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks \& Replay Buffer Spiking | http://arxiv.org/abs/1608.05081 | id:1608.05081 author:Zachary C. Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, Li Deng category:cs.LG cs.NE stat.ML  published:2016-08-17 summary:When rewards are sparse and efficient exploration essential, deep Q-learning with $\epsilon$-greedy exploration tends to fail. This poses problems for otherwise promising domains such as task-oriented dialog systems, where the primary reward signal, indicating successful completion, typically occurs only at the end of each episode but depends on the entire sequence of utterances. A poor agent encounters such successful dialogs rarely, and a random agent may never stumble upon a successful outcome in reasonable time. We present two techniques that significantly improve the efficiency of exploration for deep Q-learning agents in dialog systems. First, we demonstrate that exploration by Thompson sampling, using Monte Carlo samples from a Bayes-by-Backprop neural network, yields marked improvement over standard DQNs with Boltzmann or $\epsilon$-greedy exploration. Second, we show that spiking the replay buffer with a small number of successes, as are easy to harvest for dialog tasks, can make Q-learning feasible when it might otherwise fail catastrophically. version:1
arxiv-1608-05045 | Large Angle based Skeleton Extraction for 3D Animation | http://arxiv.org/abs/1608.05045 | id:1608.05045 author:Hugo Martin, Raphael Fernandez, Yong Khoo category:cs.CV  published:2016-08-17 summary:In this paper, we present a solution for arbitrary 3D character deformation by investigating rotation angle of decomposition and preserving the mesh topology structure. In computer graphics, skeleton extraction and skeleton-driven animation is an active areas and gains increasing interests from researchers. The accuracy is critical for realistic animation and related applications. There have been extensive studies on skeleton based 3D deformation. However for the scenarios of large angle rotation of different body parts, it has been relatively less addressed by the state-of-the-art, which often yield unsatisfactory results. Besides 3D animation problems, we also notice for many 3D skeleton detection or tracking applications from a video or depth streams, large angle rotation is also a critical factor in the regression accuracy and robustness. We introduced a distortion metric function to quantify the surface curviness before and after deformation, which is a major clue for large angle rotation detection. The intensive experimental results show that our method is suitable for 3D modeling, animation, skeleton based tracking applications. version:1
arxiv-1608-05044 | Simulation of an Optional Strategy in the Prisoner's Dilemma in Spatial and Non-spatial Environments | http://arxiv.org/abs/1608.05044 | id:1608.05044 author:Marcos Cardinot, Maud Gibbons, Colm O'Riordan, Josephine Griffith category:cs.GT cs.MA cs.NE nlin.AO  published:2016-08-17 summary:This paper presents research comparing the effects of different environments on the outcome of an extended Prisoner's Dilemma, in which agents have the option to abstain from playing the game. We consider three different pure strategies: cooperation, defection and abstinence. We adopt an evolutionary game theoretic approach and consider two different environments: the first which imposes no spatial constraints and the second in which agents are placed on a lattice grid. We analyse the performance of the three strategies as we vary the loner's payoff in both structured and unstructured environments. Furthermore we also present the results of simulations which identify scenarios in which cooperative clusters of agents emerge and persist in both environments. version:1
arxiv-1608-05014 | The Roles of Path-based and Distributional Information in Recognizing Lexical Semantic Relations | http://arxiv.org/abs/1608.05014 | id:1608.05014 author:Vered Shwartz, Ido Dagan category:cs.CL  published:2016-08-17 summary:Recognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former's contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source. version:1
arxiv-1608-04983 | Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models for Reverberant Speech Recognition | http://arxiv.org/abs/1608.04983 | id:1608.04983 author:Jeehye Lee, Myungin Lee, Joon-Hyuk Chang category:cs.CL  published:2016-08-17 summary:Distant speech recognition is a challenge, particularly due to the corruption of speech signals by reverberation caused by large distances between the speaker and microphone. In order to cope with a wide range of reverberations in real-world situations, we present novel approaches for acoustic modeling including an ensemble of deep neural networks (DNNs) and an ensemble of jointly trained DNNs. First, multiple DNNs are established, each of which corresponds to a different reverberation time 60 (RT60) in a setup step. Also, each model in the ensemble of DNN acoustic models is further jointly trained, including both feature mapping and acoustic modeling, where the feature mapping is designed for the dereverberation as a front-end. In a testing phase, the two most likely DNNs are chosen from the DNN ensemble using maximum a posteriori (MAP) probabilities, computed in an online fashion by using maximum likelihood (ML)-based blind RT60 estimation and then the posterior probability outputs from two DNNs are combined using the ML-based weights as a simple average. Extensive experiments demonstrate that the proposed approach leads to substantial improvements in speech recognition accuracy over the conventional DNN baseline systems under diverse reverberant conditions. version:1
arxiv-1608-04980 | Mollifying Networks | http://arxiv.org/abs/1608.04980 | id:1608.04980 author:Caglar Gulcehre, Marcin Moczulski, Francesco Visin, Yoshua Bengio category:cs.LG cs.NE  published:2016-08-17 summary:The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed -- or \textit{mollified} -- objective function that gradually has a more non-convex energy landscape during the training. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship with recent works on continuation methods for neural networks and mollifiers. version:1
arxiv-1608-04972 | A Three Spatial Dimension Wave Latent Force Model for Describing Excitation Sources and Electric Potentials Produced by Deep Brain Stimulation | http://arxiv.org/abs/1608.04972 | id:1608.04972 author:Pablo A. Alvarado, Mauricio A. Álvarez, Álvaro A. Orozco category:q-bio.NC stat.ML  published:2016-08-17 summary:Deep brain stimulation (DBS) is a surgical treatment for Parkinson's Disease. Static models based on quasi-static approximation are common approaches for DBS modeling. While this simplification has been validated for bioelectric sources, its application to rapid stimulation pulses, which contain more high-frequency power, may not be appropriate, as DBS therapeutic results depend on stimulus parameters such as frequency and pulse width, which are related to time variations of the electric field. We propose an alternative hybrid approach based on probabilistic models and differential equations, by using Gaussian processes and wave equation. Our model avoids quasi-static approximation, moreover, it is able to describe dynamic behavior of DBS. Therefore, the proposed model may be used to obtain a more realistic phenomenon description. The proposed model can also solve inverse problems, i.e. to recover the corresponding source of excitation, given electric potential distribution. The electric potential produced by a time-varying source was predicted using proposed model. For static sources, the electric potential produced by different electrode configurations were modeled. Four different sources of excitation were recovered by solving the inverse problem. We compare our outcomes with the electric potential obtained by solving Poisson's equation using the Finite Element Method (FEM). Our approach is able to take into account time variations of the source and the produced field. Also, inverse problem can be addressed using the proposed model. The electric potential calculated with the proposed model is close to the potential obtained by solving Poisson's equation using FEM. version:1
arxiv-1608-03226 | Drift Analysis and Evolutionary Algorithms Revisited | http://arxiv.org/abs/1608.03226 | id:1608.03226 author:Johannes Lengler, Angelika Steger category:math.CO cs.NE math.PR G.3  published:2016-08-10 summary:One of the easiest randomized greedy optimization algorithms is the following evolutionary algorithm which aims at maximizing a boolean function $f:\{0,1\}^n \to {\mathbb R}$. The algorithm starts with a random search point $\xi \in \{0,1\}^n$, and in each round it flips each bit of $\xi$ with probability $c/n$ independently at random, where $c>0$ is a fixed constant. The thus created offspring $\xi'$ replaces $\xi$ if and only if $f(\xi') > f(\xi)$. The analysis of the runtime of this simple algorithm on monotone and on linear functions turned out to be highly non-trivial. In this paper we review known results and provide new and self-contained proofs of partly stronger results. version:2
arxiv-1608-05404 | Multi-Person Tracking by Multicut and Deep Matching | http://arxiv.org/abs/1608.05404 | id:1608.05404 author:Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Bernt Schiele category:cs.CV  published:2016-08-17 summary:In [1], we proposed a graph-based formulation that links and clusters person hypotheses over time by solving a minimum cost subgraph multicut problem. In this paper, we modify and extend [1] in three ways: 1) We introduce a novel local pairwise feature based on local appearance matching that is robust to partial occlusion and camera motion. 2) We perform extensive experiments to compare different pairwise potentials and to analyze the robustness of the tracking formulation. 3) We consider a plain multicut problem and remove outlying clusters from its solution. This allows us to employ an efficient primal feasible optimization algorithm that is not applicable to the subgraph multicut problem of [1]. Unlike the branch-and-cut algorithm used there, this efficient algorithm used here is applicable to long videos and many detections. Together with the novel feature, it eliminates the need for the intermediate tracklet representation of [1]. We demonstrate the effectiveness of our overall approach on the MOT16 benchmark [2], achieving state-of-art performance. version:1
arxiv-1608-04961 | Clustering Mixed Datasets Using Homogeneity Analysis with Applications to Big Data | http://arxiv.org/abs/1608.04961 | id:1608.04961 author:Rajiv Sambasivan category:stat.ML  published:2016-08-17 summary:Clustering datasets with a mix of continuous and categorical attributes is encountered routinely by data analysts. This work presents a method to clustering such datasets using Homogeneity Analysis. An Optimal Euclidean representation of mixed datasets is obtained using Homogeneity Analysis. This representation is then clustered. The relevant aspects of the theory from Homogeneity Analysis used to determine a numerical representation of the categorical attributes is presented. An illustration of the method to real world data sets, including a very large dataset, is provided. version:1
arxiv-1608-04959 | Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation | http://arxiv.org/abs/1608.04959 | id:1608.04959 author:Rakshith Shetty, Jorma Laaksonen category:cs.CV  published:2016-08-17 summary:We present our submission to the Microsoft Video to Language Challenge of generating short captions describing videos in the challenge dataset. Our model is based on the encoder--decoder pipeline, popular in image and video captioning systems. We propose to utilize two different kinds of video features, one to capture the video content in terms of objects and attributes, and the other to capture the motion and action information. Using these diverse features we train models specializing in two separate input sub-domains. We then train an evaluator model which is used to pick the best caption from the pool of candidates generated by these domain expert models. We argue that this approach is better suited for the current video captioning task, compared to using a single model, due to the diversity in the dataset. Efficacy of our method is proven by the fact that it was rated best in MSR Video to Language Challenge, as per human evaluation. Additionally, we were ranked second in the automatic evaluation metrics based table. version:1
arxiv-1608-03983 | SGDR: Stochastic Gradient Descent with Restarts | http://arxiv.org/abs/1608.03983 | id:1608.03983 author:Ilya Loshchilov, Frank Hutter category:cs.LG cs.NE math.OC  published:2016-08-13 summary:Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on CIFAR-10 and CIFAR-100 datasets where we demonstrate new state-of-the-art results below 4\% and 19\%, respectively. Our source code is available at https://github.com/loshchil/SGDR. version:2
arxiv-1608-04171 | Power Series Classification: A Hybrid of LSTM and a Novel Advancing Dynamic Time Warping | http://arxiv.org/abs/1608.04171 | id:1608.04171 author:Yuanlong Li, Han Hu, Yonggang Wen, Jun Zhang category:cs.NE cs.LG  published:2016-08-15 summary:As many applications organize data into temporal sequences, the problem of time series data classification has been widely studied. Recent studies show that the 1-nearest neighbor with dynamic time warping (1NN-DTW) and the long short term memory (LSTM) neural network can achieve a better performance than other machine learning algorithms. In this paper, we build a novel time series classification algorithm hybridizing 1NN-DTW and LSTM, and apply it to a practical data center power monitoring problem. Firstly, we define a new distance measurement for the 1NN-DTW classifier, termed as Advancing Dynamic Time Warping (ADTW), which is non-commutative and non-dynamic programming. Secondly, we hybridize the 1NN-ADTW and LSTM together. In particular, a series of auxiliary test samples generated by the linear combination of the original test sample and its nearest neighbor with ADTW are utilized to detect which classifier to trust in the hybrid algorithm. Finally, using the power consumption data from a real data center, we show that the proposed ADTW can improve the classification accuracy from about 84\% to 89\%. Furthermore, with the hybrid algorithm, the accuracy can be further improved and we achieve an accuracy up to about 92\%. Our research can inspire more studies on non-commutative distance measurement and the hybrid of the deep learning models with other traditional models. version:2
arxiv-1608-04929 | Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes | http://arxiv.org/abs/1608.04929 | id:1608.04929 author:K J Prabuchandran, Tejas Bodas, Theja Tulabandhula category:cs.LG  published:2016-08-17 summary:A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations. version:1
arxiv-1608-04917 | Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities | http://arxiv.org/abs/1608.04917 | id:1608.04917 author:Darko Cherepnalkoski, Andreas Karpf, Igor Mozetic, Miha Grcar category:cs.CL cs.CY cs.SI  published:2016-08-17 summary:We study the cohesion within and the coalitions between political groups in the Eighth European Parliament (2014--2019) by analyzing two entirely different aspects of the behavior of the Members of the European Parliament (MEPs) in the policy-making processes. On one hand, we analyze their co-voting patterns and, on the other, their retweeting behavior. We make use of two diverse datasets in the analysis. The first one is the roll-call vote dataset, where cohesion is regarded as the tendency to co-vote within a group, and a coalition is formed when the members of several groups exhibit a high degree of co-voting agreement on a subject. The second dataset comes from Twitter; it captures the retweeting (i.e., endorsing) behavior of the MEPs and implies cohesion (retweets within the same group) and coalitions (retweets between groups) from a completely different perspective. We employ two different methodologies to analyze the cohesion and coalitions. The first one is based on Krippendorff's Alpha reliability, used to measure the agreement between raters in data-analysis scenarios, and the second one is based on Exponential Random Graph Models, often used in social-network analysis. We give general insights into the cohesion of political groups in the European Parliament, explore whether coalitions are formed in the same way for different policy areas, and examine to what degree the retweeting behavior of MEPs corresponds to their co-voting patterns. A novel and interesting aspect of our work is the relationship between the co-voting and retweeting patterns. version:1
arxiv-1608-04914 | Geometry-aware Similarity Learning on SPD Manifolds for Visual Recognition | http://arxiv.org/abs/1608.04914 | id:1608.04914 author:Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc Van Gool, Xilin Chen category:cs.CV  published:2016-08-17 summary:Symmetric Positive Definite (SPD) matrices have been widely used for data representation in many visual recognition tasks. The success mainly attributes to learning discriminative SPD matrices with encoding the Riemannian geometry of the underlying SPD manifold. In this paper, we propose a geometry-aware SPD similarity learning (SPDSL) framework to learn discriminative SPD features by directly pursuing manifold-manifold transformation matrix of column full-rank. Specifically, by exploiting the Riemannian geometry of the manifold of fixed-rank Positive Semidefinite (PSD) matrices, we present a new solution to reduce optimizing over the space of column full-rank transformation matrices to optimizing on the PSD manifold which has a well-established Riemannian structure. Under this solution, we exploit a new supervised SPD similarity learning technique to learn the transformation by regressing the similarities of selected SPD data pairs to their ground-truth similarities on the target SPD manifold. To optimize the proposed objective function, we further derive an algorithm on the PSD manifold. Evaluations on three visual classification tasks show the advantages of the proposed approach over the existing SPD-based discriminant learning methods. version:1
arxiv-1608-04902 | Globally Variance-Constrained Sparse Representation for Image Set Compression | http://arxiv.org/abs/1608.04902 | id:1608.04902 author:Xiang Zhang, Jiarui Sun, Siwei Ma, Zhouchen Lin, Jian Zhang, Shiqi Wang, Wen Gao category:cs.CV cs.MM  published:2016-08-17 summary:Sparse representation presents an efficient approach to approximately recover a signal by the linear composition of a few bases from a learnt dictionary, based on which various successful applications have been observed. However, in the scenario of data compression, its efficiency and popularity are hindered due to the extra overhead for encoding the sparse coefficients. Therefore, how to establish an accurate rate model in sparse coding and dictionary learning becomes meaningful, which has been not fully exploited in the context of sparse representation. According to the Shannon entropy inequality, the variance of data source bounds its entropy, which can reflect the actual coding bits. Hence, in this work a Globally Variance-Constrained Sparse Representation (GVCSR) model is proposed, where a variance-constrained rate model is introduced in the optimization process. Specifically, we employ the Alternating Direction Method of Multipliers (ADMM) to solve the non-convex optimization problem for sparse coding and dictionary learning, both of which have shown state-of-the-art performance in image representation. Furthermore, we investigate the potential of GVCSR in practical image set compression, where a common dictionary is trained by several key images to represent the whole image set. Experimental results have demonstrated significant performance improvements against the most popular image codecs including JPEG and JPEG2000. version:1
arxiv-1608-04872 | Hard Clusters Maximize Mutual Information | http://arxiv.org/abs/1608.04872 | id:1608.04872 author:Bernhard C. Geiger, Rana Ali Amjad category:cs.IT cs.IR cs.LG math.IT  published:2016-08-17 summary:In this paper, we investigate mutual information as a cost function for clustering, and show in which cases hard, i.e., deterministic, clusters are optimal. Using convexity properties of mutual information, we show that certain formulations of the information bottleneck problem are solved by hard clusters. Similarly, hard clusters are optimal for the information-theoretic co-clustering problem that deals with simultaneous clustering of two dependent data sets. If both data sets have to be clustered using the same cluster assignment, hard clusters are not optimal in general. We point at interesting and practically relevant special cases of this so-called pairwise clustering problem, for which we can either prove or have evidence that hard clusters are optimal. Our results thus show that one can relax the otherwise combinatorial hard clustering problem to a real-valued optimization problem with the same global optimum. version:1
arxiv-1608-04868 | Towards Music Captioning: Generating Music Playlist Descriptions | http://arxiv.org/abs/1608.04868 | id:1608.04868 author:Keunwoo Choi, George Fazekas, Mark Sandler category:cs.MM cs.AI cs.CL  published:2016-08-17 summary:Descriptions are often provided along with recommendations to help users' discovery. Recommending automatically generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural language processing are adopted to utilise the information of each track. version:1
arxiv-1608-04846 | A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation | http://arxiv.org/abs/1608.04846 | id:1608.04846 author:Po-Hsuan Chen, Xia Zhu, Hejia Zhang, Javier S. Turek, Janice Chen, Theodore L. Willke, Uri Hasson, Peter J. Ramadge category:stat.ML cs.AI cs.CV cs.LG  published:2016-08-17 summary:Finding the most effective way to aggregate multi-subject fMRI data is a long-standing and challenging problem. It is of increasing interest in contemporary fMRI studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects. Recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the brain. We examine two ways to combine the ideas of a factor model and a searchlight based analysis to aggregate multi-subject fMRI data while preserving spatial locality. We first do this directly by combining a recent factor method known as a shared response model with searchlight analysis. Then we design a multi-view convolutional autoencoder for the same task. Both approaches preserve spatial locality and have competitive or better performance compared with standard searchlight analysis and the shared response model applied across the whole brain. We also report a system design to handle the computational challenge of training the convolutional autoencoder. version:1
arxiv-1608-04845 | Lecture Notes on Spectral Graph Methods | http://arxiv.org/abs/1608.04845 | id:1608.04845 author:Michael W. Mahoney category:cs.DS stat.ML  published:2016-08-17 summary:These are lecture notes that are based on the lectures from a class I taught on the topic of Spectral Graph Methods at UC Berkeley during the Spring 2015 semester. version:1
arxiv-1608-04839 | Dynamic Collaborative Filtering with Compound Poisson Factorization | http://arxiv.org/abs/1608.04839 | id:1608.04839 author:Ghassen Jerfel, Mehmet E. Basbug, Barbara E. Engelhardt category:cs.LG cs.AI stat.ML  published:2016-08-17 summary:Model-based collaborative filtering analyzes user-item interactions to infer latent factors that represent user preferences and item characteristics in order to predict future interactions. Most collaborative filtering algorithms assume that these latent factors are static, although it has been shown that user preferences and item perceptions drift over time. In this paper, we propose a conjugate and numerically stable dynamic matrix factorization (DCPF) based on compound Poisson matrix factorization that models the smoothly drifting latent factors using Gamma-Markov chains. We propose a numerically stable Gamma chain construction, and then present a stochastic variational inference approach to estimate the parameters of our model. We apply our model to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF achieves a higher predictive accuracy than state-of-the-art static and dynamic factorization models. version:1
arxiv-1608-04830 | Outlier Detection on Mixed-Type Data: An Energy-based Approach | http://arxiv.org/abs/1608.04830 | id:1608.04830 author:Kien Do, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG  published:2016-08-17 summary:Outlier detection amounts to finding data points that differ significantly from the norm. Classic outlier detection methods are largely designed for single data type such as continuous or discrete. However, real world data is increasingly heterogeneous, where a data point can have both discrete and continuous attributes. Handling mixed-type data in a disciplined way remains a great challenge. In this paper, we propose a new unsupervised outlier detection method for mixed-type data based on Mixed-variate Restricted Boltzmann Machine (Mv.RBM). The Mv.RBM is a principled probabilistic method that models data density. We propose to use \emph{free-energy} derived from Mv.RBM as outlier score to detect outliers as those data points lying in low density regions. The method is fast to learn and compute, is scalable to massive datasets. At the same time, the outlier score is identical to data negative log-density up-to an additive constant. We evaluate the proposed method on synthetic and real-world datasets and demonstrate that (a) a proper handling mixed-types is necessary in outlier detection, and (b) free-energy of Mv.RBM is a powerful and efficient outlier scoring method, which is highly competitive against state-of-the-arts. version:1
arxiv-1608-04808 | Learning Latent Local Conversation Modes for Predicting Community Endorsement in Online Discussions | http://arxiv.org/abs/1608.04808 | id:1608.04808 author:Hao Fang, Hao Cheng, Mari Ostendorf category:cs.SI cs.CL  published:2016-08-16 summary:Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy. version:1
arxiv-1608-04802 | Large-scale Learning With Global Non-Decomposable Objectives | http://arxiv.org/abs/1608.04802 | id:1608.04802 author:Elad ET. Eban, Mariano Schain, Ariel Gordon, Rif A. Saurous, Gal Elidan category:stat.ML cs.LG  published:2016-08-16 summary:Modern retrieval systems are often driven by an underlying machine learning model. The goal of such systems is to identify and possibly rank the few most relevant items for a given query or context. Thus, the objective we would like to optimize in such scenarios is typically a globaln on-decomposable one such as the area under the precision-recall curve, the $F_\beta$ score, precision at fixed recall, etc. In practice, due to the scalability limitations of existing approaches for optimizing such objectives, large-scale systems are trained to maximize classification accuracy, in the hope that performance as measured via the true objective will also be favorable. In this work we present a unified framework that, using straightforward building block bounds, allows for highly scalable optimization of a wide range of ranking-based objectives. We demonstrate the advantage of our approach on several real-life retrieval problems that are significantly larger than those considered in the literature, while achieving substantial improvement in performance over the accuracy-objective baseline. version:1
arxiv-1608-04789 | Modelling Student Behavior using Granular Large Scale Action Data from a MOOC | http://arxiv.org/abs/1608.04789 | id:1608.04789 author:Steven Tang, Joshua C. Peterson, Zachary A. Pardos category:cs.CY cs.LG  published:2016-08-16 summary:Digital learning environments generate a precise record of the actions learners take as they interact with learning materials and complete exercises towards comprehension. With this high quantity of sequential data comes the potential to apply time series models to learn about underlying behavioral patterns and trends that characterize successful learning based on the granular record of student actions. There exist several methods for looking at longitudinal, sequential data like those recorded from learning environments. In the field of language modelling, traditional n-gram techniques and modern recurrent neural network (RNN) approaches have been applied to algorithmically find structure in language and predict the next word given the previous words in the sentence or paragraph as input. In this paper, we draw an analogy to this work by treating student sequences of resource views and interactions in a MOOC as the inputs and predicting students' next interaction as outputs. In this study, we train only on students who received a certificate of completion. In doing so, the model could potentially be used for recommendation of sequences eventually leading to success, as opposed to perpetuating unproductive behavior. Given that the MOOC used in our study had over 3,500 unique resources, predicting the exact resource that a student will interact with next might appear to be a difficult classification problem. We find that simply following the syllabus (built-in structure of the course) gives on average 23% accuracy in making this prediction, followed by the n-gram method with 70.4%, and RNN based methods with 72.2%. This research lays the ground work for recommendation in a MOOC and other digital learning environments where high volumes of sequential data exist. version:1
arxiv-1608-04783 | Application of multiview techniques to NHANES dataset | http://arxiv.org/abs/1608.04783 | id:1608.04783 author:Aileme Omogbai category:cs.LG stat.ML  published:2016-08-16 summary:Disease prediction or classification using health datasets involve using well-known predictors associated with the disease as features for the models. This study considers multiple data components of an individual's health, using the relationship between variables to generate features that may improve the performance of disease classification models. In order to capture information from different aspects of the data, this project uses a multiview learning approach, using Canonical Correlation Analysis (CCA), a technique that finds projections with maximum correlations between two data views. Data categories collected from the NHANES survey (1999-2014) are used as views to learn the multiview representations. The usefulness of the representations is demonstrated by applying them as features in a Diabetes classification task. version:1
arxiv-1608-04773 | Faster Principal Component Regression via Optimal Polynomial Approximation to sgn(x) | http://arxiv.org/abs/1608.04773 | id:1608.04773 author:Zeyuan Allen-Zhu, Yuanzhi Li category:stat.ML cs.DS cs.LG math.NA math.OC  published:2016-08-16 summary:We solve principle component regression (PCR) by providing an efficient algorithm to project any vector onto the subspace formed by the top principle components of a matrix. Our algorithm does not require any explicit construction of the top principle components, and therefore is suitable for large-scale PCR instances. Specifically, to project onto the subspace formed by principle components with eigenvalues above a threshold $\lambda$ and with a multiplicative accuracy $(1\pm \gamma) \lambda$, our algorithm requires $\tilde{O}(\gamma^{-1})$ black-box calls of ridge regression. In contrast, previous result requires $\tilde{O}(\gamma^{-2})$ such calls. We obtain this result by designing a degree-optimal polynomial approximation of the sign function. version:1
arxiv-1608-04759 | Faster Sublinear Algorithms using Conditional Sampling | http://arxiv.org/abs/1608.04759 | id:1608.04759 author:Themistoklis Gouleakis, Christos Tzamos, Manolis Zampetakis category:cs.DS cs.LG  published:2016-08-16 summary:A conditional sampling oracle for a probability distribution D returns samples from the conditional distribution of D restricted to a specified subset of the domain. A recent line of work (Chakraborty et al. 2013 and Cannone et al. 2014) has shown that having access to such a conditional sampling oracle requires only polylogarithmic or even constant number of samples to solve distribution testing problems like identity and uniformity. This significantly improves over the standard sampling model where polynomially many samples are necessary. Inspired by these results, we introduce a computational model based on conditional sampling to develop sublinear algorithms with exponentially faster runtimes compared to standard sublinear algorithms. We focus on geometric optimization problems over points in high dimensional Euclidean space. Access to these points is provided via a conditional sampling oracle that takes as input a succinct representation of a subset of the domain and outputs a uniformly random point in that subset. We study two well studied problems: k-means clustering and estimating the weight of the minimum spanning tree. In contrast to prior algorithms for the classic model, our algorithms have time, space and sample complexity that is polynomial in the dimension and polylogarithmic in the number of points. Finally, we comment on the applicability of the model and compare with existing ones like streaming, parallel and distributed computational models. version:1
arxiv-1608-03793 | Applying Deep Learning to Basketball Trajectories | http://arxiv.org/abs/1608.03793 | id:1608.03793 author:Rajiv Shah, Rob Romijnders category:cs.NE cs.CV cs.LG  published:2016-08-12 summary:One of the emerging trends for sports analytics is the growing use of player and ball tracking data. A parallel development is deep learning predictive approaches that use vast quantities of data with less reliance on feature engineering. This paper applies recurrent neural networks in the form of sequence modeling to predict whether a three-point shot is successful. The models are capable of learning the trajectory of a basketball without any knowledge of physics. For comparison, a baseline static machine learning model with a full set of features, such as angle and velocity, in addition to the positional data is also tested. Using a dataset of over 20,000 three pointers from NBA SportVu data, the models based simply on sequential positional data outperform a static feature rich machine learning model in predicting whether a three-point shot is successful. This suggests deep learning models may offer an improvement to traditional feature based machine learning methods for tracking data. version:2
arxiv-1608-04700 | A Data-Driven Approach to Estimating the Number of Clusters in Hierarchical Clustering | http://arxiv.org/abs/1608.04700 | id:1608.04700 author:Antoine Zambelli category:q-bio.QM cs.LG stat.ME 62-07  published:2016-08-16 summary:We propose two new methods for estimating the number of clusters in a hierarchical clustering framework in the hopes of creating a fully automated process with no human intervention. The methods are completely data-driven and require no input from the researcher, and as such are fully automated. They are quite easy to implement and not computationally intensive in the least. We analyze performance on several simulated data sets and the Biobase Gene Expression Set, comparing our methods to the established Gap statistic and Elbow methods and outperforming both in multi-cluster scenarios. version:1
arxiv-1608-04695 | Parameterized Principal Component Analysis | http://arxiv.org/abs/1608.04695 | id:1608.04695 author:Ajay Gupta, Adrian Barbu category:cs.CV  published:2016-08-16 summary:When modeling multivariate data, one might have an extra parameter of contextual information that could be used to treat some observations as more similar to others. For example, images of faces can vary by yaw rotation, and one would expect a left profile face to be more similar to a left-semiprofile face than to frontal face. We introduce a novel method, parameterized principal component analysis (PPCA) that can model data with linear variation like principal component analysis (PCA), but can also take advantage of this parameter of contextual information like yaw rotation. Like PCA, PPCA models an observation using a mean vector and the product of observation-specific coefficients and basis vectors. Unlike PCA, PPCA treats the elements of the mean vector and basis vectors as smooth, piecewise linear functions of the contextual parameter. PPCA is fit by a penalized optimization that penalizes potential models which have overly large differences between corresponding mean or basis vector elements for similar parameter values. The penalty ensures that each observation's projection will share information with observations that have similar parameter values, but not with observations that have dissimilar parameter values. We tested PPCA on artificial data based on known, smooth functions of an added parameter, as well as on three real datasets with different types of parameters. We compared PPCA to independent principal component analysis (IPCA), which groups observations by their parameter values and projects each group using principal component analysis with no sharing of information for different groups. PPCA recovers the known functions with less error and projects the datasets' test set observations with consistently less reconstruction error than IPCA does. PPCA's performance is particularly strong, relative to IPCA, when there are limited training data. version:1
arxiv-1608-04689 | A Shallow High-Order Parametric Approach to Data Visualization and Compression | http://arxiv.org/abs/1608.04689 | id:1608.04689 author:Martin Renqiang Min, Hongyu Guo, Dongjin Song category:cs.AI cs.LG stat.ML  published:2016-08-16 summary:Explicit high-order feature interactions efficiently capture essential structural knowledge about the data of interest and have been used for constructing generative models. We present a supervised discriminative High-Order Parametric Embedding (HOPE) approach to data visualization and compression. Compared to deep embedding models with complicated deep architectures, HOPE generates more effective high-order feature mapping through an embarrassingly simple shallow model. Furthermore, two approaches to generating a small number of exemplars conveying high-order interactions to represent large-scale data sets are proposed. These exemplars in combination with the feature mapping learned by HOPE effectively capture essential data variations. Moreover, through HOPE, these exemplars are employed to increase the computational efficiency of kNN classification for fast information retrieval by thousands of times. For classification in two-dimensional embedding space on MNIST and USPS datasets, our shallow method HOPE with simple Sigmoid transformations significantly outperforms state-of-the-art supervised deep embedding models based on deep neural networks, and even achieved historically low test error rate of 0.65% in two-dimensional space on MNIST, which demonstrates the representational efficiency and power of supervised shallow models with high-order feature interactions. version:1
arxiv-1608-04674 | Shape Constrained Tensor Decompositions using Sparse Representations in Over-Complete Libraries | http://arxiv.org/abs/1608.04674 | id:1608.04674 author:Bethany Lusch, Eric C. Chi, J. Nathan Kutz category:stat.ML cs.LG stat.ME  published:2016-08-16 summary:We consider $N$-way data arrays and low-rank tensor factorizations where the time mode is coded as a sparse linear combination of temporal elements from an over-complete library. Our method, Shape Constrained Tensor Decomposition (SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which produces $r$-rank approximations of data tensors via outer products of vectors in each dimension of the data. By constraining the vector in the temporal dimension to known analytic forms which are selected from a large set of candidate functions, more readily interpretable decompositions are achieved and analytic time dependencies discovered. The SCTD method circumvents traditional {\em flattening} techniques where an $N$-way array is reshaped into a matrix in order to perform a singular value decomposition. A clear advantage of the SCTD algorithm is its ability to extract transient and intermittent phenomena which is often difficult for SVD-based methods. We motivate the SCTD method using several intuitively appealing results before applying it on a number of high-dimensional, real-world data sets in order to illustrate the efficiency of the algorithm in extracting interpretable spatio-temporal modes. With the rise of data-driven discovery methods, the decomposition proposed provides a viable technique for analyzing multitudes of data in a more comprehensible fashion. version:1
arxiv-1608-04667 | Medical image denoising using convolutional denoising autoencoders | http://arxiv.org/abs/1608.04667 | id:1608.04667 author:Lovedeep Gondara category:cs.CV stat.ML  published:2016-08-16 summary:Image denoising is important in medical image analysis. Different algorithms have been proposed in last three decades with varying denoising performances. More recently deep learning methods have shown great promise in image denoising and have outperformed all conventional methods. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images with small training sample size and heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye. version:1
arxiv-1608-04644 | Towards Evaluating the Robustness of Neural Networks | http://arxiv.org/abs/1608.04644 | id:1608.04644 author:Nicholas Carlini, David Wagner category:cs.CR cs.CV  published:2016-08-16 summary:We consider how to measure the robustness of a neural network against adversarial examples. We introduce three new attack algorithms, tailored to three different distance metrics, to find adversarial examples: given an image x and a target class, we can find a new image x' that is similar to x but classified differently. We show that our attacks are significantly more powerful than previously published attacks: in particular, they find adversarial examples that are between 2 and 10 times closer. Then, we study defensive distillation, a recently proposed approach which increases the robustness of neural networks. Our attacks succeed with probability 200 higher than previous attacks against defensive distillation and effectively break defensive distillation, showing that it provides little added security. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples. version:1
arxiv-1608-04642 | Temporally Consistent Motion Segmentation from RGB-D Video | http://arxiv.org/abs/1608.04642 | id:1608.04642 author:Peter Bertholet, Alexandru-Eugen Ichim, Matthias Zwicker category:cs.CV 68T45 I.4.8  published:2016-08-16 summary:We present a method for temporally consistent motion segmentation from RGB-D videos assuming a piecewise rigid motion model. We formulate global energies over entire RGB-D sequences in terms of the segmentation of each frame into a number of objects, and the rigid motion of each object through the sequence. We develop a novel initialization procedure that clusters feature tracks obtained from the RGB data by leveraging the depth information. We minimize the energy using a coordinate descent approach that includes novel techniques to assemble object motion hypotheses. A main benefit of our approach is that it enables us to fuse consistently labeled object segments from all RGB-D frames of an input sequence into individual 3D object reconstructions. version:1
arxiv-1608-04636 | Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition | http://arxiv.org/abs/1608.04636 | id:1608.04636 author:Hamed Karimi, Julie Nutini, Mark Schmidt category:cs.LG math.OC stat.CO stat.ML 65K10 G.1.6; I.2.6  published:2016-08-16 summary:In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods. version:1
arxiv-1608-04631 | Neural versus Phrase-Based Machine Translation Quality: a Case Study | http://arxiv.org/abs/1608.04631 | id:1608.04631 author:Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, Marcello Federico category:cs.CL  published:2016-08-16 summary:Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved. version:1
arxiv-1608-05001 | An image compression and encryption scheme based on deep learning | http://arxiv.org/abs/1608.05001 | id:1608.05001 author:Fei Hu, Changjiu Pu, Haowei Gao, Mengzi Tang, Li Li category:cs.CV cs.LG cs.MM  published:2016-08-16 summary:Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has multi layers that project the vector representation of input data into a lower vector space. These projection vectors are dense representations of the input data. As a result, SAE can be used for image compression. Using chaotic logistic map, the compression ones can further be encrypted. In this study, an application of image compression and encryption is suggested using SAE and chaotic logistic map. Experiments show that this application is feasible and effective. It can be used for image transmission and image protection on internet simultaneously. version:1
arxiv-1608-04622 | Training Echo State Networks with Regularization through Dimensionality Reduction | http://arxiv.org/abs/1608.04622 | id:1608.04622 author:Sigurd Løkse, Filippo Maria Bianchi, Robert Jenssen category:cs.NE cs.LG  published:2016-08-16 summary:In this paper we introduce a new framework to train an Echo State Network to predict real valued time-series. The method consists in projecting the output of the internal layer of the network on a space with lower dimensionality, before training the output layer to learn the target task. Notably, we enforce a regularization constraint that leads to better generalization capabilities. We evaluate the performances of our approach on several benchmark tests, using different techniques to train the readout of the network, achieving superior predictive performance when using the proposed framework. Finally, we provide an insight on the effectiveness of the implemented mechanics through a visualization of the trajectory in the phase space and relying on the methodologies of nonlinear time-series analysis. By applying our method on well known chaotic systems, we provide evidence that the lower dimensional embedding retains the dynamical properties of the underlying system better than the full-dimensional internal states of the network. version:1
arxiv-1608-04615 | Scalable Modeling of Multivariate Longitudinal Data for Prediction of Chronic Kidney Disease Progression | http://arxiv.org/abs/1608.04615 | id:1608.04615 author:Joseph Futoma, Mark Sendak, C. Blake Cameron, Katherine Heller category:stat.ML stat.AP stat.ME  published:2016-08-16 summary:Prediction of the future trajectory of a disease is an important challenge for personalized medicine and population health management. However, many complex chronic diseases exhibit large degrees of heterogeneity, and furthermore there is not always a single readily available biomarker to quantify disease severity. Even when such a clinical variable exists, there are often additional related biomarkers routinely measured for patients that may better inform the predictions of their future disease state. To this end, we propose a novel probabilistic generative model for multivariate longitudinal data that captures dependencies between multivariate trajectories. We use a Gaussian process based regression model for each individual trajectory, and build off ideas from latent class models to induce dependence between their mean functions. We fit our method using a scalable variational inference algorithm to a large dataset of longitudinal electronic patient health records, and find that it improves dynamic predictions compared to a recent state of the art method. Our local accountable care organization then uses the model predictions during chart reviews of high risk patients with chronic kidney disease. version:1
arxiv-1608-04585 | Conformalized density- and distance-based anomaly detection in time-series data | http://arxiv.org/abs/1608.04585 | id:1608.04585 author:Evgeny Burnaev, Vladislav Ishimtsev category:stat.AP cs.LG stat.ML  published:2016-08-16 summary:Anomalies (unusual patterns) in time-series data give essential, and often actionable information in critical situations. Examples can be found in such fields as healthcare, intrusion detection, finance, security and flight safety. In this paper we propose new conformalized density- and distance-based anomaly detection algorithms for a one-dimensional time-series data. The algorithms use a combination of a feature extraction method, an approach to assess a score whether a new observation differs significantly from a previously observed data, and a probabilistic interpretation of this score based on the conformal paradigm. version:1
arxiv-1608-04581 | A novel transfer learning method based on common space mapping and weighted domain matching | http://arxiv.org/abs/1608.04581 | id:1608.04581 author:Ru-Ze Liang, Wei Xie, Weizhi Li, Hongqi Wang, Jim Jing-Yan Wang, Lisa Taylor category:cs.LG stat.ML  published:2016-08-16 summary:In this paper, we propose a novel learning framework for the problem of domain transfer learning. We map the data of two domains to one single common space, and learn a classifier in this common space. Then we adapt the common classifier to the two domains by adding two adaptive functions to it respectively. In the common space, the target domain data points are weighted and matched to the target domain in term of distributions. The weighting terms of source domain data points and the target domain classification responses are also regularized by the local reconstruction coefficients. The novel transfer learning framework is evaluated over some benchmark cross-domain data sets, and it outperforms the existing state-of-the-art transfer learning methods. version:1
arxiv-1608-04314 | Weakly Supervised Object Localization Using Size Estimates | http://arxiv.org/abs/1608.04314 | id:1608.04314 author:Miaojing Shi, Vittorio Ferrari category:cs.CV  published:2016-08-15 summary:We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques. version:2
arxiv-1608-04550 | Fast Calculation of the Knowledge Gradient for Optimization of Deterministic Engineering Simulations | http://arxiv.org/abs/1608.04550 | id:1608.04550 author:Joachim van der Herten, Ivo Couckuyt, Dirk Deschrijver, Tom Dhaene category:cs.CE cs.LG stat.ML  published:2016-08-16 summary:A novel efficient method for computing the Knowledge-Gradient policy for Continuous Parameters (KGCP) for deterministic optimization is derived. The differences with Expected Improvement (EI), a popular choice for Bayesian optimization of deterministic engineering simulations, are explored. Both policies and the Upper Confidence Bound (UCB) policy are compared on a number of benchmark functions including a problem from structural dynamics. It is empirically shown that KGCP has similar performance as the EI policy for many problems, but has better convergence properties for complex (multi-modal) optimization problems as it emphasizes more on exploration when the model is confident about the shape of optimal regions. In addition, the relationship between Maximum Likelihood Estimation (MLE) and slice sampling for estimation of the hyperparameters of the underlying models, and the complexity of the problem at hand, is studied. version:1
arxiv-1608-04245 | The Bayesian Low-Rank Determinantal Point Process Mixture Model | http://arxiv.org/abs/1608.04245 | id:1608.04245 author:Mike Gartrell, Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG  published:2016-08-15 summary:Determinantal point processes (DPPs) are an elegant model for encoding probabilities over subsets, such as shopping baskets, of a ground set, such as an item catalog. They are useful for a number of machine learning tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. Recent work has shown that using a low-rank factorization of this kernel provides remarkable scalability improvements that open the door to training on large-scale datasets and computing online recommendations, both of which are infeasible with standard DPP models that use a full-rank kernel. In this paper we present a low-rank DPP mixture model that allows us to represent the latent structure present in observed subsets as a mixture of a number of component low-rank DPPs, where each component DPP is responsible for representing a portion of the observed data. The mixture model allows us to effectively address the capacity constraints of the low-rank DPP model. We present an efficient and scalable Markov Chain Monte Carlo (MCMC) learning algorithm for our model that uses Gibbs sampling and stochastic gradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several real-world product recommendation datasets, we show that our low-rank DPP mixture model provides substantially better predictive performance than is possible with a single low-rank or full-rank DPP, and significantly better performance than several other competing recommendation methods in many cases. version:2
arxiv-1608-04540 | Dopamine modulation of prefrontal delay activity-reverberatory activity and sharpness of tuning curves | http://arxiv.org/abs/1608.04540 | id:1608.04540 author:Gabriele Scheler, Jean-Marc Fellous category:q-bio.NC cs.NE q-bio.SC  published:2016-08-16 summary:Recent electrophysiological experiments have shown that dopamine (D1) modulation of pyramidal cells in prefrontal cortex reduces spike frequency adaptation and enhances NMDA transmission. Using four models, from multicompartmental to integrate and fire, we examine the effects of these modulations on sustained (delay) activity in a reverberatory network. We find that D1 modulation may enable robust network bistability yielding selective reverberation among cells that code for a particular item or location. We further show that the tuning curve of such cells is sharpened, and that signal-to-noise ratio is increased. We postulate that D1 modulation affects the tuning of "memory fields" and yield efficient distributed dynamic representations. version:1
arxiv-1608-04517 | Image Restoration using Group Sparse Representation via Weighted Nuclear Norm Minimization | http://arxiv.org/abs/1608.04517 | id:1608.04517 author:Zhiyuan Zha, Xinggan Zhang, Xin Liu, Ziheng Zhou, Jingang Shi, Shouren Lan, Yang Chen, Yechao Bai, Qiong Wang, Lan Tang category:cs.CV  published:2016-08-16 summary:As the matrix formed by nonlocal similar patches in a natural image is of a low rank, the nuclear norm minimization (NNM) has been widely studied for image processing. Since the singular values have clear meanings and should be treated differently, NNM regularizes each of them equally, which often restricts its capability and flexibility. Recent advances have suggested that the weighted nuclear norm minimization (WNNM) has shown great potential in different image restoration studies, where singular values are assigned different value. However, it still lacks a mathematical derivation why the weighted nuclear norm is more appropriate than the nuclear norm. In this paper, we proposed a new scheme for image restoration using group sparse representation via weighted nuclear norm minimization (GSR-WNNM). We show mathematically the advantage of WNNM, from a group sparse representation perspective, where GSR offers a powerful mechanism of combining local sparsity and nonlocal self-similarity of images simultaneously in a unified framework. Then, an effective dictionary for each group is learned from the reconstructed image itself rather a large number of natural image dataset, ensuring a low computational complexity. Moreover, to further improve the computational efficiency of the proposed method, we have developed an implementation of fast convergence via the alternating direction method of multipliers (ADMM). Experimental results have shown that the proposed GSR-WNNM method significantly outperforms the state-of-the-art methods both quantitatively and qualitatively. version:1
arxiv-1608-04236 | Generative and Discriminative Voxel Modeling with Convolutional Neural Networks | http://arxiv.org/abs/1608.04236 | id:1608.04236 author:Andrew Brock, Theodore Lim, J. M. Ritchie, Nick Weston category:cs.CV cs.HC cs.LG stat.ML  published:2016-08-15 summary:When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification. version:2
arxiv-1608-04509 | Unconstrained Two-parallel-plane Model for Focused Plenoptic Cameras Calibration | http://arxiv.org/abs/1608.04509 | id:1608.04509 author:Chunping Zhang, Zhe Ji, Qing Wang category:cs.CV I.5.4  published:2016-08-16 summary:The plenoptic camera can capture both angular and spatial information of the rays, enabling 3D reconstruction by single exposure. The geometry of the recovered scene structure is affected by the calibration of the plenoptic camera significantly. In this paper, we propose a novel unconstrained two-parallel-plane (TPP) model with 7 parameters to describe a 4D light field. By reconstructing scene points from ray-ray association, a 3D projective transformation is deduced to establish the relationship between the scene structure and the TPP parameters. Based on the transformation, we simplify the focused plenoptic camera as a TPP model and calibrate its intrinsic parameters. Our calibration method includes a close-form solution and a nonlinear optimization by minimizing re-projection error. Experiments on both simulated data and real scene data verify the performance of the calibration on the focused plenoptic camera. version:1
arxiv-1608-04493 | Dynamic Network Surgery for Efficient DNNs | http://arxiv.org/abs/1608.04493 | id:1608.04493 author:Yiwen Guo, Anbang Yao, Yurong Chen category:cs.NE cs.CV cs.LG  published:2016-08-16 summary:Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\bm{108}\times$ and $\bm{17.7}\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code will be made publicly available. version:1
arxiv-1608-04489 | SenTion: A framework for Sensing Facial Expressions | http://arxiv.org/abs/1608.04489 | id:1608.04489 author:Rahul Islam, Karan Ahuja, Sandip Karmakar, Ferdous Barbhuiya category:cs.CV  published:2016-08-16 summary:Facial expressions are an integral part of human cognition and communication, and can be applied in various real life applications. A vital precursor to accurate expression recognition is feature extraction. In this paper, we propose SenTion: A framework for sensing facial expressions. We propose a novel person independent and scale invariant method of extracting Inter Vector Angles (IVA) as geometric features, which proves to be robust and reliable across databases. SenTion employs a novel framework of combining geometric (IVA's) and appearance based features (Histogram of Gradients) to create a hybrid model, that achieves state of the art recognition accuracy. We evaluate the performance of SenTion on two famous face expression data set, namely: CK+ and JAFFE; and subsequently evaluate the viability of facial expression systems by a user study. Extensive experiments showed that SenTion framework yielded dramatic improvements in facial expression recognition and could be employed in real-world applications with low resolution imaging and minimal computational resources in real-time, achieving 15-18 fps on a 2.4 GHz CPU with no GPU. version:1
arxiv-1608-04485 | Authorship clustering using multi-headed recurrent neural networks | http://arxiv.org/abs/1608.04485 | id:1608.04485 author:Douglas Bagnall category:cs.CL  published:2016-08-16 summary:A recurrent neural network that has been trained to separately model the language of several documents by unknown authors is used to measure similarity between the documents. It is able to find clues of common authorship even when the documents are very short and about disparate topics. While it is easy to make statistically significant predictions regarding authorship, it is difficult to group documents into definite clusters with high accuracy. version:1
arxiv-1608-04481 | Lecture Notes on Randomized Linear Algebra | http://arxiv.org/abs/1608.04481 | id:1608.04481 author:Michael W. Mahoney category:cs.DS stat.ML  published:2016-08-16 summary:These are lecture notes that are based on the lectures from a class I taught on the topic of Randomized Linear Algebra (RLA) at UC Berkeley during the Fall 2013 semester. version:1
arxiv-1608-04478 | A Geometrical Approach to Topic Model Estimation | http://arxiv.org/abs/1608.04478 | id:1608.04478 author:Zheng Tracy Ke category:stat.ME cs.LG stat.ML  published:2016-08-16 summary:In the probabilistic topic models, the quantity of interest---a low-rank matrix consisting of topic vectors---is hidden in the text corpus matrix, masked by noise, and the Singular Value Decomposition (SVD) is a potentially useful tool for learning such a low-rank matrix. However, the connection between this low-rank matrix and the singular vectors of the text corpus matrix are usually complicated and hard to spell out, so how to use SVD for learning topic models faces challenges. In this paper, we overcome the challenge by revealing a surprising insight: there is a low-dimensional simplex structure which can be viewed as a bridge between the low-rank matrix of interest and the SVD of the text corpus matrix, and allows us to conveniently reconstruct the former using the latter. Such an insight motivates a new SVD approach to learning topic models, which we analyze with delicate random matrix theory and derive the rate of convergence. We support our methods and theory numerically, using both simulated data and real data. version:1
arxiv-1608-04468 | Unbiased Learning-to-Rank with Biased Feedback | http://arxiv.org/abs/1608.04468 | id:1608.04468 author:Thorsten Joachims, Adith Swaminathan, Tobias Schnabel category:cs.IR cs.LG  published:2016-08-16 summary:Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, user centric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a Propensity-Weighted Ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. In contrast to most conventional approaches to de-bias the data using click models, this allows training of ranking functions even in settings where queries do not repeat. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model misspecification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance. version:1
arxiv-1608-04465 | Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees | http://arxiv.org/abs/1608.04465 | id:1608.04465 author:Ehsan Shareghi, Matthias Petri, Gholamreza Haffari, Trevor Cohn category:cs.CL  published:2016-08-16 summary:Efficient methods for storing and querying are critical for scaling high-order n-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500x, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). version:1
arxiv-1609-01982 | Uniform Transformation of Non-Separable Probability Distributions | http://arxiv.org/abs/1609.01982 | id:1609.01982 author:Eric Kee category:cs.NE  published:2016-08-16 summary:A theoretical framework is developed to describe the transformation that distributes probability density functions uniformly over space. In one dimension, the cumulative distribution can be used, but does not generalize to higher dimensions, or non-separable distributions. A potential function is shown to link probability density functions to their transformation, and to generalize the cumulative. A numerical method is developed to compute the potential, and examples are shown in two dimensions. version:1
arxiv-1608-04434 | Natural Language Processing using Hadoop and KOSHIK | http://arxiv.org/abs/1608.04434 | id:1608.04434 author:Emre Erturk, Hong Shi category:cs.CL  published:2016-08-15 summary:Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP. This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data. Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performance. version:1
arxiv-1608-04428 | TerpreT: A Probabilistic Programming Language for Program Induction | http://arxiv.org/abs/1608.04428 | id:1608.04428 author:Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow category:cs.LG cs.AI cs.NE  published:2016-08-15 summary:We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community. Our key contribution is the proposal of TerpreT, a domain-specific language for expressing program synthesis problems. TerpreT is similar to a probabilistic programming language: a model is composed of a specification of a program representation (declarations of random variables) and an interpreter describing how programs map inputs to outputs (a model connecting unknowns to observations). The inference task is to observe a set of input-output examples and infer the underlying program. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing like-to-like comparisons between different approaches to inference. From a single TerpreT specification we automatically perform inference using four different back-ends. These are based on gradient descent, linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. We illustrate the value of TerpreT by developing several interpreter models and performing an empirical comparison between alternative inference algorithms. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. We conclude with suggestions for the machine learning community to make progress on program synthesis. version:1
arxiv-1608-04414 | Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back | http://arxiv.org/abs/1608.04414 | id:1608.04414 author:Vitaly Feldman category:cs.LG stat.ML  published:2016-08-15 summary:In stochastic convex optimization the goal is to minimize a convex function $F(x) \doteq {\mathbf E}_{{\mathbf f}\sim D}[{\mathbf f}(x)]$ over a convex set $\cal K \subset {\mathbb R}^d$ where $D$ is some unknown distribution and each $f(\cdot)$ in the support of $D$ is convex over $\cal K$. The optimization is commonly based on i.i.d.~samples $f^1,f^2,\ldots,f^n$ from $D$. A standard approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \doteq \frac{1}{n}\sum_{i\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\cal K$. We demonstrate that in the standard $\ell_p/\ell_q$ setting of Lipschitz-bounded functions over a $\cal K$ of bounded radius, ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\Omega(\log d)$ dependence proved for $\ell_2/\ell_2$ setting by Shalev-Shwartz et al. (2009). In stark contrast, these problems can be solved using dimension-independent number of samples for $\ell_2/\ell_2$ setting and $\log d$ dependence for $\ell_1/\ell_\infty$ setting using other approaches. We also demonstrate that for a more general class of range-bounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2. version:1
arxiv-1608-01745 | Play and Learn: Using Video Games to Train Computer Vision Models | http://arxiv.org/abs/1608.01745 | id:1608.01745 author:Alireza Shafaei, James J. Little, Mark Schmidt category:cs.CV  published:2016-08-05 summary:Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision. version:2
arxiv-1608-04374 | A Geometric Framework for Convolutional Neural Networks | http://arxiv.org/abs/1608.04374 | id:1608.04374 author:Anthony L. Caterini, Dong Eui Chang category:stat.ML cs.AI cs.NE I.5.1; I.2.6  published:2016-08-15 summary:In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach can be applied to other network structures and provides a basis on which to create new networks. version:1
arxiv-1608-04369 | Star-galaxy Classification Using Deep Convolutional Neural Networks | http://arxiv.org/abs/1608.04369 | id:1608.04369 author:Edward J. Kim, Robert J. Brunner category:astro-ph.IM astro-ph.CO astro-ph.GA cs.CV  published:2016-08-15 summary:Most existing star-galaxy classifiers use the reduced summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering. version:1
arxiv-1608-04363 | Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification | http://arxiv.org/abs/1608.04363 | id:1608.04363 author:Justin Salamon, Juan Pablo Bello category:cs.SD cs.CV cs.LG cs.NE  published:2016-08-15 summary:The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a "shallow" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation. version:1
arxiv-1608-04348 | Anomaly detection and classification for streaming data using partial differential equations | http://arxiv.org/abs/1608.04348 | id:1608.04348 author:Bilal Abbasi, Jeff Calder, Adam M. Oberman category:cs.LG cs.CV cs.DB I.5; G.3; H.2.8  published:2016-08-15 summary:Nondominated sorting, or Pareto Depth Analysis (PDA), is widely used in multi-objective optimization and has recently found important applications in multi-criteria anomaly detection. We propose in this paper a fast real-time streaming version of the PDA algorithm for anomaly detection and classification that exploits the computational advantages of partial differential equation (PDE) continuum limits. We prove convergence rates for the continuum approximations and present the results of numerical experiments. version:1
arxiv-1608-04342 | Intrinsic Light Fields | http://arxiv.org/abs/1608.04342 | id:1608.04342 author:Elena Garces, Jose I. Echevarria, Wen Zhang, Hongzhi Wu, Kun Zhou, Diego Gutierrez category:cs.CV  published:2016-08-15 summary:We present the first method to automatically decompose a light field into its intrinsic shading and albedo components. Contrary to previous work targeted to 2D single images and videos, a light field is a 4D structure that captures non-integrated incoming radiance over a discrete angular domain. This higher dimensionality of the problem renders previous state-of-the-art algorithms impractical either due to their cost of processing a single 2D slice, or their inability to enforce proper coherence in additional dimensions. We propose a new decomposition algorithm that jointly optimizes the whole light field data for proper angular coherency. For efficiency, we extend Retinex theory, working on the gradient domain where new albedo and occlusion terms are introduced. Results show our method provides 4D intrinsic decompositions difficult to achieve with previous state-of-the-art algorithms. version:1
arxiv-1608-04339 | Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition | http://arxiv.org/abs/1608.04339 | id:1608.04339 author:Yi Zhu, Shawn Newsam category:cs.CV  published:2016-08-15 summary:This paper performs the first investigation into depth for large-scale human action recognition in video where the depth cues are estimated from the videos themselves. We develop a new framework called depth2action and experiment thoroughly into how best to incorporate the depth information. We introduce spatio-temporal depth normalization (STDN) to enforce temporal consistency in our estimated depth sequences. We also propose modified depth motion maps (MDMM) to capture the subtle temporal changes in depth. These two components significantly improve the action recognition performance. We evaluate our depth2action framework on three large-scale action recognition video benchmarks. Our model achieves state-of-the-art performance when combined with appearance and motion information thus demonstrating that depth2action is indeed complementary to existing approaches. version:1
arxiv-1608-04337 | Factorized Convolutional Neural Networks | http://arxiv.org/abs/1608.04337 | id:1608.04337 author:Min Wang, Baoyuan Liu, Hassan Foroosh category:cs.CV  published:2016-08-15 summary:Deep convolutional neural networks achieve better than human level visual recognition accuracy, at the cost of high computational complexity. We propose to factorize the convolutional layers to improve their efficiency. In traditional convolutional layers, the 3D convolution can be considered as performing in-channel spatial convolution and linear channel projection simultaneously, leading to highly redundant computation. By unravelling them apart, the proposed layer only involves single in-channel convolution and linear channel projection. When stacking such layers together, we achieves similar accuracy with significantly less computation. Additionally, we propose a topological connection framework between the input channels and output channels that further improves the layer's efficiency. Our experiments demonstrate that the proposed method remarkably outperforms the standard convolutional layer with regard to accuracy/complexity ratio. Our model achieves accuracy of GoogLeNet while consuming 3.4 times less computation. version:1
arxiv-1608-04331 | Consistency constraints for overlapping data clustering | http://arxiv.org/abs/1608.04331 | id:1608.04331 author:Jared Culbertson, Dan P. Guralnik, Jakob Hansen, Peter F. Stiller category:cs.LG stat.ML 51K05  68P01 H.3.3  published:2016-08-15 summary:We examine overlapping clustering schemes with functorial constraints, in the spirit of Carlsson--Memoli. This avoids issues arising from the chaining required by partition-based methods. Our principal result shows that any clustering functor is naturally constrained to refine single-linkage clusters and be refined by maximal-linkage clusters. We work in the context of metric spaces with non-expansive maps, which is appropriate for modeling data processing which does not increase information content. version:1
arxiv-1608-04320 | Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated | http://arxiv.org/abs/1608.04320 | id:1608.04320 author:Namrata Vaswani, Han Guo category:cs.LG cs.IT math.IT  published:2016-08-15 summary:Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for decades. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often referred to as "data-dependent noise". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, called cluster-EVD, and argue that it reduces the sample complexity of EVD in certain regimes. version:1
arxiv-1608-04307 | Transitive Hashing Network for Heterogeneous Multimedia Retrieval | http://arxiv.org/abs/1608.04307 | id:1608.04307 author:Zhangjie Cao, Mingsheng Long, Qiang Yang category:cs.CV  published:2016-08-15 summary:Hashing has been widely applied to large-scale multimedia retrieval due to the storage and retrieval efficiency. Cross-modal hashing enables efficient retrieval from database of one modality in response to a query of another modality. Existing work on cross-modal hashing assumes heterogeneous relationship across modalities for hash function learning. In this paper, we relax the strong assumption by only requiring such heterogeneous relationship in an auxiliary dataset different from the query/database domain. We craft a hybrid deep architecture to simultaneously learn the cross-modal correlation from the auxiliary dataset, and align the dataset distributions between the auxiliary dataset and the query/database domain, which generates transitive hash codes for heterogeneous multimedia retrieval. Extensive experiments exhibit that the proposed approach yields state of the art multimedia retrieval performance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA. version:1
arxiv-1608-04290 | Robust Volume Minimization-Based Matrix Factorization for Remote Sensing and Document Clustering | http://arxiv.org/abs/1608.04290 | id:1608.04290 author:Xiao Fu, Kejun Huang, Bo Yang, Wing-Kin Ma, Nicholas D. Sidiropoulos category:stat.ML  published:2016-08-15 summary:This paper considers \emph{volume minimization} (VolMin)-based structured matrix factorization (SMF). VolMin is a factorization criterion that decomposes a given data matrix into a basis matrix times a structured coefficient matrix via finding the minimum-volume simplex that encloses all the columns of the data matrix. Recent work showed that VolMin guarantees the identifiability of the factor matrices under mild conditions that are realistic in a wide variety of applications. This paper focuses on both theoretical and practical aspects of VolMin. On the theory side, exact equivalence of two independently developed sufficient conditions for VolMin identifiability is proven here, thereby providing a more comprehensive understanding of this aspect of VolMin. On the algorithm side, computational complexity and sensitivity to outliers are two key challenges associated with real-world applications of VolMin. These are addressed here via a new VolMin algorithm that handles volume regularization in a computationally simple way, and automatically detects and {iteratively downweights} outliers, simultaneously. Simulations and real-data experiments using a remotely sensed hyperspectral image and the Reuters document corpus are employed to showcase the effectiveness of the proposed algorithm. version:1
arxiv-1608-04274 | Visual place recognition using landmark distribution descriptors | http://arxiv.org/abs/1608.04274 | id:1608.04274 author:Pilailuck Panphattarasap, Andrew Calway category:cs.CV  published:2016-08-15 summary:Recent work by Suenderhauf et al. [1] demonstrated improved visual place recognition using proposal regions coupled with features from convolutional neural networks (CNN) to match landmarks between views. In this work we extend the approach by introducing descriptors built from landmark features which also encode the spatial distribution of the landmarks within a view. Matching descriptors then enforces consistency of the relative positions of landmarks between views. This has a significant impact on performance. For example, in experiments on 10 image-pair datasets, each consisting of 200 urban locations with significant differences in viewing positions and conditions, we recorded average precision of around 70% (at 100% recall), compared with 58% obtained using whole image CNN features and 50% for the method in [1]. version:1
arxiv-1608-03644 | Deep GDashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks | http://arxiv.org/abs/1608.03644 | id:1608.03644 author:Jack Lanchantin, Ritambhara Singh, Beilun Wang, Yanjun Qi category:cs.LG cs.CV cs.NE  published:2016-08-12 summary:Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Genomic Dashboard (Deep GDashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output values, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them. version:2
arxiv-1608-04267 | Detecting Vanishing Points in Natural Scenes with Application in Photo Composition Analysis | http://arxiv.org/abs/1608.04267 | id:1608.04267 author:Zihan Zhou, Farshid Farhat, James Z. Wang category:cs.CV cs.MM  published:2016-08-15 summary:Linear perspective is widely used in landscape photography to create the impression of depth on a 2D photo. Automated understanding of the use of linear perspective in landscape photography has a number of real-world applications, including aesthetics assessment, image retrieval, and on-site feedback for photo composition. We address this problem by detecting vanishing points and the associated line structures in photos. However, natural landscape scenes pose great technical challenges because there are often inadequate number of strong edges converging to the vanishing points. To overcome this difficulty, we propose a novel vanishing point detection method that exploits global structures in the scene via contour detection. We show that our method significantly outperforms state-of-the-art methods on a public ground truth landscape image dataset that we have created. Based on the detection results, we further demonstrate how our approach to linear perspective understanding can be used to provide on-site guidance to amateur photographers on their work through a novel viewpoint-specific image retrieval system. version:1
arxiv-1608-04233 | A Riemannian Network for SPD Matrix Learning | http://arxiv.org/abs/1608.04233 | id:1608.04233 author:Zhiwu Huang, Luc Van Gool category:cs.CV  published:2016-08-15 summary:Symmetric Positive Definite (SPD) matrix learning methods have become popular in many image and video processing tasks, thanks to their ability to learn appropriate statistical representations while respecting the Riemannian geometry of the underlying SPD manifold. In this paper we build a Riemannian network to open up a new direction of SPD matrix non-linear learning in a deep architecture. The built network generalizes the Euclidean network paradigm to non-Euclidean SPD manifolds. In particular, we devise bilinear mapping layers to transform input SPD matrices into more desirable SPD matrices, exploit eigenvalue rectification layers to introduce the non-linearity with a non-linear function on the new SPD matrices, and design eigenvalue logarithm layers to perform Log-Euclidean Riemannian computing on the resulting SPD matrices for regular output layers. For training the deep network, we propose a Riemannian matrix backpropagation by exploiting a variant of stochastic gradient descent on Stiefel manifolds where the network weights reside on. We show through experiments that the proposed SPD network can be simply trained and outperform existing SPD matrix learning and state-of-the-art methods in three typical visual classification tasks. version:1
arxiv-1608-04224 | Generating Synthetic Data for Text Recognition | http://arxiv.org/abs/1608.04224 | id:1608.04224 author:Praveen Krishnan, C. V. Jawahar category:cs.CV  published:2016-08-15 summary:Generating synthetic images is an art which emulates the natural process of image generation in a closest possible manner. In this work, we exploit such a framework for data generation in handwritten domain. We render synthetic data using open source fonts and incorporate data augmentation schemes. As part of this work, we release 9M synthetic handwritten word image corpus which could be useful for training deep network architectures and advancing the performance in handwritten word spotting and recognition tasks. version:1
arxiv-1608-04219 | Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition With Groebner Bases | http://arxiv.org/abs/1608.04219 | id:1608.04219 author:Zongyan Huang, Matthew England, James H. Davenport, Lawrence C. Paulson category:cs.SC cs.LG 68W30  68T05 I.2.6; I.1.0  published:2016-08-15 summary:Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. However, it can be expensive, with worst case complexity doubly exponential in the size of the input. Hence it is important to formulate the problem in the best manner for the CAD algorithm. One possibility is to precondition the input polynomials using Groebner Basis (GB) theory. Previous experiments have shown that while this can often be very beneficial to the CAD algorithm, for some problems it can significantly worsen the CAD performance. In the present paper we investigate whether machine learning, specifically a support vector machine (SVM), may be used to identify those CAD problems which benefit from GB preconditioning. We run experiments with over 1000 problems (many times larger than previous studies) and find that the machine learned choice does better than the human-made heuristic. version:1
arxiv-1608-04207 | Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks | http://arxiv.org/abs/1608.04207 | id:1608.04207 author:Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg category:cs.CL  published:2016-08-15 summary:There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations. version:1
arxiv-1608-04200 | Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video | http://arxiv.org/abs/1608.04200 | id:1608.04200 author:Zhiwu Huang, Ruiping Wang, Shiguang Shan, Luc Van Gool, Xilin Chen category:cs.CV  published:2016-08-15 summary:Riemannian manifolds have been widely employed for video representations in visual classification tasks including video-based face recognition. The success mainly derives from learning a discriminant Riemannian metric which encodes the non-linear geometry of the underlying Riemannian manifold. In this paper, we propose a novel metric learning framework to learn a distance metric across a Euclidean space and a Riemannian manifold to fuse the average appearance and pattern variation of faces within one video. The proposed metric learning framework can handle three typical tasks of video-based face recognition: Video-to-Still, Still-to-Video and Video-to-Video settings. To accomplish this new framework, by exploiting typical Riemannian geometries for kernel embedding, we map the source Euclidean space and Riemannian manifold into a common Euclidean subspace, each through a corresponding high-dimensional Reproducing Kernel Hilbert Space (RKHS). With this mapping, the problem of learning a cross-view metric between the two source heterogeneous spaces can be expressed as learning a single-view Euclidean distance metric in the target common Euclidean space. By learning information on heterogeneous data with the shared label, the discriminant metric in the common space improves face recognition from videos. Extensive experiments on four challenging video face databases demonstrate that the proposed framework has a clear advantage over the state-of-the-art methods in the three classical video-based face recognition tasks. version:1
arxiv-1608-04188 | Face Alignment In-the-Wild: A Survey | http://arxiv.org/abs/1608.04188 | id:1608.04188 author:Xin Jin, Xiaoyang Tan category:cs.CV  published:2016-08-15 summary:Over the last two decades, face alignment or localizing fiducial facial points has received increasing attention owing to its comprehensive applications in automatic face analysis. However, such a task has proven extremely challenging in unconstrained environments due to many confounding factors, such as pose, occlusions, expression and illumination. While numerous techniques have been developed to address these challenges, this problem is still far away from being solved. In this survey, we present an up-to-date critical review of the existing literatures on face alignment, focusing on those methods addressing overall difficulties and challenges of this topic under uncontrolled conditions. Specifically, we categorize existing face alignment techniques, present detailed descriptions of the prominent algorithms within each category, and discuss their advantages and disadvantages. Furthermore, we organize special discussions on the practical aspects of face alignment in-the-wild, towards the development of a robust face alignment system. In addition, we show performance statistics of the state of the art, and conclude this paper with several promising directions for future research. version:1
arxiv-1608-04670 | Attribute Extraction from Product Titles in eCommerce | http://arxiv.org/abs/1608.04670 | id:1608.04670 author:Ajinkya More category:cs.CL cs.IR  published:2016-08-15 summary:This paper presents a named entity extraction system for detecting attributes in product titles of eCommerce retailers like Walmart. The absence of syntactic structure in such short pieces of text makes extracting attribute values a challenging problem. We find that combining sequence labeling algorithms such as Conditional Random Fields and Structured Perceptron with a curated normalization scheme produces an effective system for the task of extracting product attribute values from titles. To keep the discussion concrete, we will illustrate the mechanics of the system from the point of view of a particular attribute - brand. We also discuss the importance of an attribute extraction system in the context of retail websites with large product catalogs, compare our approach to other potential approaches to this problem and end the paper with a discussion of the performance of our system for extracting attributes. version:1
arxiv-1608-04147 | Numerically Grounded Language Models for Semantic Error Correction | http://arxiv.org/abs/1608.04147 | id:1608.04147 author:Georgios P. Spithourakis, Isabelle Augenstein, Sebastian Riedel category:cs.CL cs.NE  published:2016-08-14 summary:Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements. version:1
arxiv-1608-04123 | The Spectral Condition Number Plot for Regularization Parameter Determination | http://arxiv.org/abs/1608.04123 | id:1608.04123 author:Carel F. W. Peeters, Mark A. van de Wiel, Wessel N. van Wieringen category:stat.CO stat.ML  published:2016-08-14 summary:Many modern statistical applications ask for the estimation of a covariance (or precision) matrix in settings where the number of variables is larger than the number of observations. There exists a broad class of ridge-type estimators that employs regularization to cope with the subsequent singularity of the sample covariance matrix. These estimators depend on a penalty parameter and choosing its value can be hard, in terms of being computationally unfeasible or tenable only for a restricted set of ridge-type estimators. Here we introduce a simple graphical tool, the spectral condition number plot, for informed heuristic penalty parameter selection. The proposed tool is computationally friendly and can be employed for the full class of ridge-type covariance (precision) estimators. version:1
arxiv-1608-04117 | The Importance of Skip Connections in Biomedical Image Segmentation | http://arxiv.org/abs/1608.04117 | id:1608.04117 author:Michal Drozdzal, Eugene Vorontsov, Gabriel Chartrand, Samuel Kadoury, Chris Pal category:cs.CV  published:2016-08-14 summary:In this paper, we study the influence of both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. In standard FCNs, only long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling. We extend FCNs by adding short skip connections, that are similar to the ones introduced in residual networks, in order to build very deep FCNs (of hundreds of layers). A review of the gradient flow confirms that for a very deep FCN it is beneficial to have both long and short skip connections. Finally, we show that a very deep FCN can achieve near-to-state-of-the-art results on the EM dataset without any further post-processing. version:1
arxiv-1608-04767 | Proceedings of the LexSem+Logics Workshop 2016 | http://arxiv.org/abs/1608.04767 | id:1608.04767 author:Steven Neale, Valeria de Paiva, Arantxa Otegi, Alexandre Rademaker category:cs.CL  published:2016-08-14 summary:Lexical semantics continues to play an important role in driving research directions in NLP, with the recognition and understanding of context becoming increasingly important in delivering successful outcomes in NLP tasks. Besides traditional processing areas such as word sense and named entity disambiguation, the creation and maintenance of dictionaries, annotated corpora and resources have become cornerstones of lexical semantics research and produced a wealth of contextual information that NLP processes can exploit. New efforts both to link and construct from scratch such information - as Linked Open Data or by way of formal tools coming from logic, ontologies and automated reasoning - have increased the interoperability and accessibility of resources for lexical and computational semantics, even in those languages for which they have previously been limited. LexSem+Logics 2016 combines the 1st Workshop on Lexical Semantics for Lesser-Resources Languages and the 3rd Workshop on Logics and Ontologies. The accepted papers in our program covered topics across these two areas, including: the encoding of plurals in Wordnets, the creation of a thesaurus from multiple sources based on semantic similarity metrics, and the use of cross-lingual treebanks and annotations for universal part-of-speech tagging. We also welcomed talks from two distinguished speakers: on Portuguese lexical knowledge bases (different approaches, results and their application in NLP tasks) and on new strategies for open information extraction (the capture of verb-based propositions from massive text corpora). version:1
arxiv-1608-04109 | Depth and depth-based classification with R-package ddalpha | http://arxiv.org/abs/1608.04109 | id:1608.04109 author:Oleksii Pokotylo, Pavlo Mozharovskyi, Rainer Dyckerhoff category:stat.CO stat.ML  published:2016-08-14 summary:Following the seminal idea of Tukey, data depth is a function that measures how close an arbitrary point of the space is located to an implicitly defined center of a data cloud. Having undergone theoretical and computational developments, it is now employed in numerous applications with classification being the most popular one. The R-package ddalpha is a software directed to fuse experience of the applicant with recent achievements in the area of data depth and depth-based classification. ddalpha provides an implementation for exact and approximate computation of most reasonable and widely applied notions of data depth. These can be further used in the depth-based multivariate and functional classifiers implemented in the package, where the $DD\alpha$-procedure is in the main focus. The package is expandable with user-defined custom depth methods and separators. The implemented functions for depth visualization and the built-in benchmark procedures may also serve to provide insights into the geometry of the data and the quality of pattern recognition. version:1
arxiv-1608-04105 | Machine Learning with Memristors via Thermodynamic RAM | http://arxiv.org/abs/1608.04105 | id:1608.04105 author:Timothy W. Molter, M. Alexander Nugent category:cs.ET cs.NE  published:2016-08-14 summary:Thermodynamic RAM (kT-RAM) is a neuromemristive co-processor design based on the theory of AHaH Computing and implemented via CMOS and memristors. The co-processor is a 2-D array of differential memristor pairs (synapses) that can be selectively coupled together (neurons) via the digital bit addressing of the underlying CMOS RAM circuitry. The chip is designed to plug into existing digital computers and be interacted with via a simple instruction set. Anti-Hebbian and Hebbian (AHaH) computing forms the theoretical framework from which a nature-inspired type of computing architecture is built where, unlike von Neumann architectures, memory and processor are physically combined for synaptic operations. Through exploitation of AHaH attractor states, memristor-based circuits converge to attractor basins that represents machine learning solutions such as unsupervised feature learning, supervised classification and anomaly detection. Because kT-RAM eliminates the need to shuttle bits back and forth between memory and processor and can operate at very low voltage levels, it can significantly surpass CPU, GPU, and FPGA performance for synaptic integration and learning operations. Here, we present a memristor technology developed for use in kT-RAM, in particular bi-directional incremental adaptation of conductance via short low-voltage 1.0 V, 1.0 microsecond pulses. version:1
arxiv-1608-04089 | Viewpoint and Topic Modeling of Current Events | http://arxiv.org/abs/1608.04089 | id:1608.04089 author:Kerry Zhang, Jussi Karlgren, Cheng Zhang, Jens Lagergren category:cs.CL cs.IR stat.ML  published:2016-08-14 summary:There are multiple sides to every story, and while statistical topic models have been highly successful at topically summarizing the stories in corpora of text documents, they do not explicitly address the issue of learning the different sides, the viewpoints, expressed in the documents. In this paper, we show how these viewpoints can be learned completely unsupervised and represented in a human interpretable form. We use a novel approach of applying CorrLDA2 for this purpose, which learns topic-viewpoint relations that can be used to form groups of topics, where each group represents a viewpoint. A corpus of documents about the Israeli-Palestinian conflict is then used to demonstrate how a Palestinian and an Israeli viewpoint can be learned. By leveraging the magnitudes and signs of the feature weights of a linear SVM, we introduce a principled method to evaluate associations between topics and viewpoints. With this, we demonstrate, both quantitatively and qualitatively, that the learned topic groups are contextually coherent, and form consistently correct topic-viewpoint associations. version:1
arxiv-1608-04080 | Dynamic Hand Gesture Recognition for Wearable Devices with Low Complexity Recurrent Neural Networks | http://arxiv.org/abs/1608.04080 | id:1608.04080 author:Sungho Shin, Wonyong Sung category:cs.CV cs.LG  published:2016-08-14 summary:Gesture recognition is a very essential technology for many wearable devices. While previous algorithms are mostly based on statistical methods including the hidden Markov model, we develop two dynamic hand gesture recognition techniques using low complexity recurrent neural network (RNN) algorithms. One is based on video signal and employs a combined structure of a convolutional neural network (CNN) and an RNN. The other uses accelerometer data and only requires an RNN. Fixed-point optimization that quantizes most of the weights into two bits is conducted to optimize the amount of memory size for weight storage and reduce the power consumption in hardware and software based implementations. version:1
arxiv-1608-04077 | Generative Transfer Learning between Recurrent Neural Networks | http://arxiv.org/abs/1608.04077 | id:1608.04077 author:Sungho Shin, Kyuyeon Hwang, Wonyong Sung category:cs.LG  published:2016-08-14 summary:Training a neural network demands a large amount of labeled data. Keeping the data after the training may not be allowed because of legal or privacy reasons. In this study, we train a new RNN, called a student network, using a previously developed RNN, the teacher network, without using the original data. The teacher network is used for generating a data for training the student network. In order to generate a long sequence of data that does not repeat, a random number assisted output label selection method is employed. The softmax output of the teacher RNN is used as for the soft target when training a student network. The performance evaluation is conducted using a character-level language model. The experimental results show that the proposed method yields good performance approaching that of the original data based training. This work not only gives insight to knowledge transfer between RNNs but also can be useful when the original training data is not available. version:1
arxiv-1608-04064 | About Pyramid Structure in Convolutional Neural Networks | http://arxiv.org/abs/1608.04064 | id:1608.04064 author:Ihsan Ullah, Alfredo Petrosino category:cs.CV  published:2016-08-14 summary:Deep convolutional neural networks (CNN) brought revolution without any doubt to various challenging tasks, mainly in computer vision. However, their model designing still requires attention to reduce number of learnable parameters, with no meaningful reduction in performance. In this paper we investigate to what extend CNN may take advantage of pyramid structure typical of biological neurons. A generalized statement over convolutional layers from input till fully connected layer is introduced that helps further in understanding and designing a successful deep network. It reduces ambiguity, number of parameters, and their size on disk without degrading overall accuracy. Performance are shown on state-of-the-art models for MNIST, Cifar-10, Cifar-100, and ImageNet-12 datasets. Despite more than 80% reduction in parameters for Caffe_LENET, challenging results are obtained. Further, despite 10-20% reduction in training data along with 10-40% reduction in parameters for AlexNet model and its variations, competitive results are achieved when compared to similar well-engineered deeper architectures. version:1
arxiv-1608-04063 | Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest Neighbor Classification | http://arxiv.org/abs/1608.04063 | id:1608.04063 author:Hyun-Chul Kim category:cs.LG stat.ML  published:2016-08-14 summary:The $k$-nearest neighbor classification method ($k$-NNC) is one of the simplest nonparametric classification methods. The mutual $k$-NN classification method (M$k$NNC) is a variant of $k$-NNC based on mutual neighborship. We propose another variant of $k$-NNC, the symmetric $k$-NN classification method (S$k$NNC) based on both mutual neighborship and one-sided neighborship. The performance of M$k$NNC and S$k$NNC depends on the parameter $k$ as the one of $k$-NNC does. We propose the ways how M$k$NN and S$k$NN classification can be performed based on Bayesian mutual and symmetric $k$-NN regression methods with the selection schemes for the parameter $k$. Bayesian mutual and symmetric $k$-NN regression methods are based on Gaussian process models, and it turns out that they can do M$k$NN and S$k$NN classification with new encodings of target values (class labels). The simulation results show that the proposed methods are better than or comparable to $k$-NNC, M$k$NNC and S$k$NNC with the parameter $k$ selected by the leave-one-out cross validation method not only for an artificial data set but also for real world data sets. version:1
arxiv-1608-04062 | Stacked Approximated Regression Machine: A Simple Deep Learning Approach | http://arxiv.org/abs/1608.04062 | id:1608.04062 author:Zhangyang Wang, Shiyu Chang, Qing Ling, Shuai Huang, Xia Hu, Honghui Shi, Thomas S. Huang category:cs.LG cs.CV  published:2016-08-14 summary:This paper proposes the Stacked Approximated Regression Machine (SARM), a novel, simple yet powerful deep learning (DL) baseline. We start by discussing the relationship between regularized regression models and feed-forward networks, with emphasis on the non-negative sparse coding and convolutional sparse coding models. We demonstrate how these models are naturally converted into a unified feed-forward network structure, which coincides with popular DL components. SARM is constructed by stacking multiple unfolded and truncated regression models. Compared to the PCANet, whose feature extraction layers are completely linear, SARM naturally introduces non-linearities, by embedding sparsity regularization. The parameters of SARM are easily obtained, by solving a series of light-weight problems, e.g., PCA or KSVD. Extensive experiments are conducted, which show that SARM outperforms the existing simple deep baseline, PCANet, and is on par with many state-of-the-art deep models, but with much lower computational loads. version:1
arxiv-1608-04059 | Scout-It: Interior tomography using modified scout acquisition | http://arxiv.org/abs/1608.04059 | id:1608.04059 author:Kriti Sen Sharma category:q-bio.QM cs.CV  published:2016-08-14 summary:Global scout views have been previously used to reduce interior reconstruction artifacts in high-resolution micro-CT and C-arm systems. However these methods cannot be directly used in the all-important domain of clinical CT. This is because when the CT scan is truncated, the scout views are also truncated. However many cases of truncation in clinical CT involve partial truncation, where the anterio-posterior (AP) scout is truncated, but the medio-lateral (ML) scout is non-truncated. In this paper, we show that in such cases of partially truncated CT scans, a modified configuration may be used to acquire non-truncated AP scout view, and ultimately allow for highly accurate interior reconstruction. version:1
arxiv-1608-04051 | SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation | http://arxiv.org/abs/1608.04051 | id:1608.04051 author:Ting Liu, Miaomiao Zhang, Mehran Javanmardi, Nisha Ramesh, Tolga Tasdizen category:cs.CV  published:2016-08-14 summary:Region-based methods have proven necessary for improving segmentation accuracy of neuronal structures in electron microscopy (EM) images. Most region-based segmentation methods use a scoring function to determine region merging. Such functions are usually learned with supervised algorithms that demand considerable ground truth data, which are costly to collect. We propose a semi-supervised approach that reduces this demand. Based on a merge tree structure, we develop a differentiable unsupervised loss term that enforces consistent predictions from the learned function. We then propose a Bayesian model that combines the supervised and the unsupervised information for probabilistic learning. The experimental results on three EM data sets demonstrate that by using a subset of only 3% to 7% of the entire ground truth data, our approach consistently performs close to the state-of-the-art supervised method with the full labeled data set, and significantly outperforms the supervised method with the same labeled subset. version:1
arxiv-1608-04048 | Ultra High-Dimensional Nonlinear Feature Selection for Big Biological Data | http://arxiv.org/abs/1608.04048 | id:1608.04048 author:Makoto Yamada, Jiliang Tang, Jose Lugo-Martinez, Ermin Hodzic, Raunak Shrestha, Avishek Saha, Hua Ouyang, Dawei Yin, Hiroshi Mamitsuka, Cenk Sahinalp, Predrag Radivojac, Filippo Menczer, Yi Chang category:stat.ML  published:2016-08-14 summary:Machine learning methods are used to discover complex nonlinear relationships in biological and medical data. However, sophisticated learning models are computationally unfeasible for data with millions of features. Here we introduce the first feature selection method for nonlinear learning problems that can scale up to large, ultra-high dimensional biological data. More specifically, we scale up the novel Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) to handle millions of features with tens of thousand samples. The proposed method is guaranteed to find an optimal subset of maximally predictive features with minimal redundancy, yielding higher predictive power and improved interpretability. Its effectiveness is demonstrated through applications to classify phenotypes based on module expression in human prostate cancer patients and to detect enzymes among protein structures. We achieve high accuracy with as few as 20 out of one million features --- a dimensionality reduction of 99.998%. Our algorithm can be implemented on commodity cloud computing platforms. The dramatic reduction of features may lead to the ubiquitous deployment of sophisticated prediction models in mobile health care applications. version:1
arxiv-1608-04045 | Branching Gaussian Processes with Applications to Spatiotemporal Reconstruction of 3D Trees | http://arxiv.org/abs/1608.04045 | id:1608.04045 author:Kyle Simek, Ravishankar Palanivelu, Kobus Barnard category:cs.CV  published:2016-08-14 summary:We propose a robust method for estimating dynamic 3D curvilinear branching structure from monocular images. While 3D reconstruction from images has been widely studied, estimating thin structure has received less attention. This problem becomes more challenging in the presence of camera error, scene motion, and a constraint that curves are attached in a branching structure. We propose a new general-purpose prior, a branching Gaussian processes (BGP), that models spatial smoothness and temporal dynamics of curves while enforcing attachment between them. We apply this prior to fit 3D trees directly to image data, using an efficient scheme for approximate inference based on expectation propagation. The BGP prior's Gaussian form allows us to approximately marginalize over 3D trees with a given model structure, enabling principled comparison between tree models with varying complexity. We test our approach on a novel multi-view dataset depicting plants with known 3D structures and topologies undergoing small nonrigid motion. Our method outperforms a state-of-the-art 3D reconstruction method designed for non-moving thin structure. We evaluate under several common measures, and we propose a new measure for reconstructions of branching multi-part 3D scenes under motion. version:1
arxiv-1608-04042 | Can Peripheral Representations Improve Clutter Metrics on Complex Scenes? | http://arxiv.org/abs/1608.04042 | id:1608.04042 author:Arturo Deza, Miguel P. Eckstein category:cs.CV cs.AI cs.HC  published:2016-08-14 summary:Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. A toolbox for creating peripheral architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and Artificial Systems will be made available. version:1
arxiv-1608-04037 | An approach to dealing with missing values in heterogeneous data using k-nearest neighbors | http://arxiv.org/abs/1608.04037 | id:1608.04037 author:Davi E. N. Frossard, Igor O. Nunes, Renato A. Krohling category:cs.LG cs.IR stat.ML  published:2016-08-13 summary:Techniques such as clusterization, neural networks and decision making usually rely on algorithms that are not well suited to deal with missing values. However, real world data frequently contains such cases. The simplest solution is to either substitute them by a best guess value or completely disregard the missing values. Unfortunately, both approaches can lead to biased results. In this paper, we propose a technique for dealing with missing values in heterogeneous data using imputation based on the k-nearest neighbors algorithm. It can handle real (which we refer to as crisp henceforward), interval and fuzzy data. The effectiveness of the algorithm is tested on several datasets and the numerical results are promising. version:1
arxiv-1608-04020 | Undecidability of the Lambek calculus with subexponentials and bracket modalities | http://arxiv.org/abs/1608.04020 | id:1608.04020 author:Max Kanovich, Stepan Kuznetsov, Andre Scedrov category:math.LO cs.CL 03B47  published:2016-08-13 summary:Morrill and Valentin in the paper "Computational coverage of TLG: Nonlinearity" considered two extensions of the Lambek calculus with so-called "exponential" modalities. These calculi serve as a basis for describing fragments of natural language via categorial grammars. In this paper we show undecidability of derivability problems in these two calculi. On the other hand, restricted fragments of them are decidable and therefore can be used in practice. version:1
arxiv-1608-03995 | Analysis of Morphology in Topic Modeling | http://arxiv.org/abs/1608.03995 | id:1608.03995 author:Chandler May, Ryan Cotterell, Benjamin Van Durme category:cs.CL  published:2016-08-13 summary:Topic models make strong assumptions about their data. In particular, different words are implicitly assumed to have different meanings: topic models are often used as human-interpretable dimensionality reductions and a proliferation of words with identical meanings would undermine the utility of the top-$m$ word list representation of a topic. Though a number of authors have added preprocessing steps such as lemmatization to better accommodate these assumptions, the effects of such data massaging have not been publicly studied. We make first steps toward elucidating the role of morphology in topic modeling by testing the effect of lemmatization on the interpretability of a latent Dirichlet allocation (LDA) model. Using a word intrusion evaluation, we quantitatively demonstrate that lemmatization provides a significant benefit to the interpretability of a model learned on Wikipedia articles in a morphologically rich language. version:1
arxiv-1608-03981 | Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising | http://arxiv.org/abs/1608.03981 | id:1608.03981 author:Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang category:cs.CV  published:2016-08-13 summary:Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks such as Gaussian denoising, single image super-resolution and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing. version:1
arxiv-1608-03974 | Recurrent Fully Convolutional Neural Networks for Multi-slice MRI Cardiac Segmentation | http://arxiv.org/abs/1608.03974 | id:1608.03974 author:Rudra P K Poudel, Pablo Lamata, Giovanni Montana category:stat.ML cs.CV cs.LG  published:2016-08-13 summary:In cardiac magnetic resonance imaging, fully-automatic segmentation of the heart enables precise structural and functional measurements to be taken, e.g. from short-axis MR images of the left-ventricle. In this work we propose a recurrent fully-convolutional network (RFCN) that learns image representations from the full stack of 2D slices and has the ability to leverage inter-slice spatial dependences through internal memory units. RFCN combines anatomical detection and segmentation into a single architecture that is trained end-to-end thus significantly reducing computational time, simplifying the segmentation pipeline, and potentially enabling real-time applications. We report on an investigation of RFCN using two datasets, including the publicly available MICCAI 2009 Challenge dataset. Comparisons have been carried out between fully convolutional networks and deep restricted Boltzmann machines, including a recurrent version that leverages inter-slice spatial correlation. Our studies suggest that RFCN produces state-of-the-art results and can substantially improve the delineation of contours near the apex of the heart. version:1
arxiv-1608-03938 | Determining Health Utilities through Data Mining of Social Media | http://arxiv.org/abs/1608.03938 | id:1608.03938 author:Christopher Thompson, Josh Introne, Clint Young category:cs.CL cs.AI cs.CY cs.SI  published:2016-08-13 summary:'Health utilities' measure patient preferences for perfect health compared to specific unhealthy states, such as asthma, a fractured hip, or colon cancer. When integrated over time, these estimations are called quality adjusted life years (QALYs). Until now, characterizing health utilities (HUs) required detailed patient interviews or written surveys. While reliable and specific, this data remained costly due to efforts to locate, enlist and coordinate participants. Thus the scope, context and temporality of diseases examined has remained limited. Now that more than a billion people use social media, we propose a novel strategy: use natural language processing to analyze public online conversations for signals of the severity of medical conditions and correlate these to known HUs using machine learning. In this work, we filter a dataset that originally contained 2 billion tweets for relevant content on 60 diseases. Using this data, our algorithm successfully distinguished mild from severe diseases, which had previously been categorized only by traditional techniques. This represents progress towards two related applications: first, predicting HUs where such information is nonexistent; and second, (where rich HU data already exists) estimating temporal or geographic patterns of disease severity through data mining. version:1
arxiv-1608-03933 | Improved dynamic regret for non-degeneracy functions | http://arxiv.org/abs/1608.03933 | id:1608.03933 author:Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, Zhi-Hua Zhou category:cs.LG  published:2016-08-13 summary:Recently, there has been a growing research interest in the analysis of dynamic regret, which measures the performance of an online learner against a sequence of local minimizers. By exploiting the strong convexity, previous studies have shown that the dynamic regret can be upper bounded by the path-length of the comparator sequence. In this paper, we illustrate that the dynamic regret can be further improved by allowing the learner to query the gradient of the function multiple times, and meanwhile the strong convexity can be weakened to other non-degeneracy conditions. Specifically, we introduce the squared path-length, which could be much smaller than the path-length, as a new regularity of the comparator sequence. When multiple gradients are accessible to the learner, we first demonstrate that the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. We then extend our theoretical guarantee to functions that are semi-strongly convex or self-concordant. To the best of our knowledge, this is the first time the semi-strong convexity and the self-concordance are utilized to tighten the dynamic regret. version:1
arxiv-1608-03932 | Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning | http://arxiv.org/abs/1608.03932 | id:1608.03932 author:Keze Wang, Shengfu Zhai, Hui Cheng, Xiaodan Liang, Liang Lin category:cs.CV  published:2016-08-13 summary:Human pose estimation (i.e., locating the body parts / joints of a person) is a fundamental problem in human-computer interaction and multimedia applications. Significant progress has been made based on the development of depth sensors, i.e., accessible human pose prediction from still depth images [32]. However, most of the existing approaches to this problem involve several components/models that are independently designed and optimized, leading to suboptimal performances. In this paper, we propose a novel inference-embedded multi-task learning framework for predicting human pose from still depth images, which is implemented with a deep architecture of neural networks. Specifically, we handle two cascaded tasks: i) generating the heat (confidence) maps of body parts via a fully convolutional network (FCN); ii) seeking the optimal configuration of body parts based on the detected body part proposals via an inference built-in MatchNet [10], which measures the appearance and geometric kinematic compatibility of body parts and embodies the dynamic programming inference as an extra network layer. These two tasks are jointly optimized. Our extensive experiments show that the proposed deep model significantly improves the accuracy of human pose estimation over other several state-of-the-art methods or SDKs. We also release a large-scale dataset for comparison, which includes 100K depth images under challenging scenarios. version:1
arxiv-1608-03928 | Hybrid Jacobian and Gauss-Seidel proximal block coordinate update methods for linearly constrained convex programming | http://arxiv.org/abs/1608.03928 | id:1608.03928 author:Yangyang Xu category:math.OC math.NA stat.ML  published:2016-08-13 summary:Recent years have witnessed the rapid development of block coordinate update (BCU) methods, which are particularly suitable for problems involving large-sized data and/or variables. In optimization, BCU first appears as the coordinate descent method that works well for smooth problems or those with separable nonsmooth terms and/or separable constraints. As nonseparable constraints exist, BCU can be applied under primal-dual settings. In the literature, it has been shown that for weakly convex problems with nonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may fail to converge and that with fully Jacobian rule can converge sublinearly. However, empirically the method with Jacobian update is usually slower than that with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid Jacobian and Gauss-Seidel BCU method for solving linearly constrained multi-block structured convex programming, where the objective may have a nonseparable quadratic term and separable nonsmooth terms. At each primal block variable update, the method approximates the augmented Lagrangian function at an affine combination of the previous two iterates, and the affinely mixing matrix with desired nice properties can be chosen through solving a semidefinite programming. We show that the hybrid method enjoys the theoretical convergence guarantee as Jacobian BCU. In addition, we numerically demonstrate that the method can perform as well as Gauss-Seidel method and better than a recently proposed randomized primal-dual BCU method. version:1
arxiv-1608-03914 | When was that made? | http://arxiv.org/abs/1608.03914 | id:1608.03914 author:Sirion Vittayakorn, Alexander C. Berg, Tamara L. Berg category:cs.CV  published:2016-08-12 summary:In this paper, we explore deep learning methods for estimating when objects were made. Automatic methods for this task could potentially be useful for historians, collectors, or any individual interested in estimating when their artifact was created. Direct applications include large-scale data organization or retrieval. Toward this goal, we utilize features from existing deep networks and also fine-tune new networks for temporal estimation. In addition, we create two new datasets of 67,771 dated clothing items from Flickr and museum collections. Our method outperforms both a color-based baseline and previous state of the art methods for temporal estimation. We also provide several analyses of what our networks have learned, and demonstrate applications to identifying temporal inspiration in fashion collections. version:1
arxiv-1608-03907 | Temporal Registration in In-Utero Volumetric MRI Time Series | http://arxiv.org/abs/1608.03907 | id:1608.03907 author:Ruizhi Liao, Esra Turk, Miaomiao Zhang, Jie Luo, Ellen Grant, Elfar Adalsteinsson, Polina Golland category:cs.CV  published:2016-08-12 summary:We present a robust method to correct for motion and deformations for in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI requires robust alignment across time in the presence of substantial and unpredictable motion. We make a Markov assumption on the nature of deformations to take advantage of the temporal structure in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We demonstrate the utility of the temporal model by showing that its use improves the accuracy of the segmentation propagation through temporal registration. Our results suggest that the proposed model captures accurately the temporal dynamics of deformations in in-utero MRI time series. version:1
arxiv-1608-03902 | Rapid Classification of Crisis-Related Data on Social Networks using Convolutional Neural Networks | http://arxiv.org/abs/1608.03902 | id:1608.03902 author:Dat Tien Nguyen, Kamela Ali Al Mannai, Shafiq Joty, Hassan Sajjad, Muhammad Imran, Prasenjit Mitra category:cs.CL cs.LG cs.SI  published:2016-08-12 summary:The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The Scarcity of labeled data, particularly in the early hours of a crisis, delays the machine learning process. The current state-of-the-art classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for binary and multi-class tweet classification task. We show that neural network based models do not require any feature engineering and perform better than state-of-the-art methods. In the early hours of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results. version:1
arxiv-1608-03866 | Distributed Optimization for Client-Server Architecture with Negative Gradient Weights | http://arxiv.org/abs/1608.03866 | id:1608.03866 author:Shripad Gade, Nitin H. Vaidya category:cs.DC cs.LG math.OC  published:2016-08-12 summary:Availability of both massive datasets and computing resources have made machine learning and predictive analytics extremely pervasive. In this work we present a synchronous algorithm and architecture for distributed optimization motivated by privacy requirements posed by applications in machine learning. We present an algorithm for the recently proposed multi-parameter-server architecture. We consider a group of parameter servers that learn a model based on randomized gradients received from clients. Clients are computational entities with private datasets (inducing a private objective function), that evaluate and upload randomized gradients to the parameter servers. The parameter servers perform model updates based on received gradients and share the model parameters with other servers. We prove that the proposed algorithm can optimize the overall objective function for a very general architecture involving $C$ clients connected to $S$ parameter servers in an arbitrary time varying topology and the parameter servers forming a connected network. version:1
arxiv-1607-07405 | gvnn: Neural Network Library for Geometric Computer Vision | http://arxiv.org/abs/1607.07405 | id:1607.07405 author:Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John McCormac, Andrew Davison category:cs.CV cs.LG  published:2016-07-25 summary:We introduce gvnn, a neural network library in Torch aimed towards bridging the gap between classic geometric computer vision and deep learning. Inspired by the recent success of Spatial Transformer Networks, we propose several new layers which are often used as parametric transformations on the data in geometric computer vision. These layers can be inserted within a neural network much in the spirit of the original spatial transformers and allow backpropagation to enable end-to-end learning of a network involving any domain knowledge in geometric computer vision. This opens up applications in learning invariance to 3D geometric transformation for place recognition, end-to-end visual odometry, depth estimation and unsupervised learning through warping with a parametric transformation for image reconstruction error. version:3
arxiv-1608-03832 | On Minimal Accuracy Algorithm Selection in Computer Vision and Intelligent Systems | http://arxiv.org/abs/1608.03832 | id:1608.03832 author:Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama category:cs.CV  published:2016-08-12 summary:In this paper we discuss certain theoretical properties of algorithm selection approach to image processing and to intelligent system in general. We analyze the theoretical limits of algorithm selection with respect to the algorithm selection accuracy. We show the theoretical formulation of a crisp bound on the algorithm selector precision guaranteeing to always obtain better than the best available algorithm result. version:1
arxiv-1608-03819 | DeepDiary: Automatic Caption Generation for Lifelogging Image Streams | http://arxiv.org/abs/1608.03819 | id:1608.03819 author:Chenyou Fan, David J. Crandall category:cs.CV  published:2016-08-12 summary:Lifelogging cameras capture everyday life from a first-person perspective, but generate so much data that it is hard for users to browse and organize their image collections effectively. In this paper, we propose to use automatic image captioning algorithms to generate textual representations of these collections. We develop and explore novel techniques based on deep learning to generate captions for both individual images and image streams, using temporal consistency constraints to create summaries that are both more compact and less noisy. We evaluate our techniques with quantitative and qualitative results, and apply captioning to an image retrieval application for finding potentially private images. Our results suggest that our automatic captioning algorithms, while imperfect, may work well enough to help users manage lifelogging photo collections. version:1
arxiv-1608-03817 | Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages | http://arxiv.org/abs/1608.03817 | id:1608.03817 author:Yin Cheng Ng, Pawel Chilinski, Ricardo Silva category:stat.ML  published:2016-08-12 summary:Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs. version:1
arxiv-1608-03811 | Content-based image retrieval tutorial | http://arxiv.org/abs/1608.03811 | id:1608.03811 author:Joani Mitro category:stat.ML cs.IR cs.LG  published:2016-08-12 summary:This paper functions as a tutorial for individuals interested to enter the field of information retrieval but wouldn't know where to begin from. It describes two fundamental yet efficient image retrieval techniques, the first being k - nearest neighbors (knn) and the second support vector machines(svm). The goal is to provide the reader with both the theoretical and practical aspects in order to acquire a better understanding. Along with this tutorial we have also developed the equivalent software1 using the MATLAB environment in order to illustrate the techniques, so that the reader can have a hands-on experience. version:1
arxiv-1608-03803 | Redefining part-of-speech classes with distributional semantic models | http://arxiv.org/abs/1608.03803 | id:1608.03803 author:Andrey Kutuzov, Erik Velldal, Lilja Øvrelid category:cs.CL  published:2016-08-12 summary:This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. Our work targets the Universal PoS tag set, which is currently actively being used for annotation of a range of languages. We experiment with training classifiers for predicting PoS tags for words based on their embeddings. The results show that the information about PoS affiliation contained in the distributional vectors allows us to discover groups of words with distributional patterns that differ from other words of the same part of speech. This data often reveals hidden inconsistencies of the annotation process or guidelines. At the same time, it supports the notion of `soft' or `graded' part of speech affiliations. Finally, we show that information about PoS is distributed among dozens of vector components, not limited to only one or two features. version:1
arxiv-1608-03785 | Compositional Distributional Cognition | http://arxiv.org/abs/1608.03785 | id:1608.03785 author:Yaared Al-Mehairi, Bob Coecke, Martha Lewis category:cs.AI cs.CL math.CT  published:2016-08-12 summary:We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of [32] within the categorical compositional semantics (CatCo) of [13], forming a model of categorical compositional cognition (CatCog). This resolves intrinsic problems with ICS such as the fact that representations inhabit an unbounded space and that sentences with differing tree structures cannot be directly compared. We do so in a way that makes the most of the grammatical structure available, in contrast to strategies like circular convolution. Using the CatCo model also allows us to make use of tools developed for CatCo such as the representation of ambiguity and logical reasoning via density matrices, structural meanings for words such as relative pronouns, and addressing over- and under-extension, all of which are present in cognitive processes. Moreover the CatCog framework is sufficiently flexible to allow for entirely different representations of meaning, such as conceptual spaces. Interestingly, since the CatCo model was largely inspired by categorical quantum mechanics, so is CatCog. version:1
arxiv-1608-03767 | Measuring the State of the Art of Automated Pathway Curation Using Graph Algorithms - A Case Study of the mTOR Pathway | http://arxiv.org/abs/1608.03767 | id:1608.03767 author:Michael Spranger, Sucheendra K. Palaniappan, Samik Ghosh category:cs.CL q-bio.MN  published:2016-08-12 summary:This paper evaluates the difference between human pathway curation and current NLP systems. We propose graph analysis methods for quantifying the gap between human curated pathway maps and the output of state-of-the-art automatic NLP systems. Evaluation is performed on the popular mTOR pathway. Based on analyzing where current systems perform well and where they fail, we identify possible avenues for progress. version:1
arxiv-1608-03764 | Extracting Biological Pathway Models From NLP Event Representations | http://arxiv.org/abs/1608.03764 | id:1608.03764 author:Michael Spranger, Sucheendra K. Palaniappan, Samik Ghosh category:cs.CL q-bio.MN  published:2016-08-12 summary:This paper describes an an open-source software system for the automatic conversion of NLP event representations to system biology structured data interchange formats such as SBML and BioPAX. It is part of a larger effort to make results of the NLP community available for system biology pathway modelers. version:1
arxiv-1608-03757 | Student's t Distribution based Estimation of Distribution Algorithms for Derivative-free Global Optimization | http://arxiv.org/abs/1608.03757 | id:1608.03757 author:Bin Liu, Shi Cheng, Yuhui Shi category:cs.NE  published:2016-08-12 summary:In this paper, we are concerned with a branch of evolutionary algorithms termed estimation of distribution (EDA), which has been successfully used to tackle derivative-free global optimization problems. For existent EDA algorithms, it is a common practice to use a Gaussian distribution or a mixture of Gaussian components to represent the statistical property of available promising solutions found so far. Observing that the Student's t distribution has heavier and longer tails than the Gaussian, which may be beneficial for exploring the solution space, we propose a novel EDA algorithm termed ESTDA, in which the Student's t distribution, rather than Gaussian, is employed. To address hard multimodal and deceptive problems, we extend ESTDA further by substituting a single Student's t distribution with a mixture of Student's t distributions. The resulting algorithm is named as estimation of mixture of Student's t distribution algorithm (EMSTDA). Both ESTDA and EMSTDA are evaluated through extensive and in-depth numerical experiments using over a dozen of benchmark objective functions. Empirical results demonstrate that the proposed algorithms provide remarkably better performance than their Gaussian counterparts. version:1
arxiv-1608-03748 | Self-paced Learning for Weakly Supervised Evidence Discovery in Multimedia Event Search | http://arxiv.org/abs/1608.03748 | id:1608.03748 author:Mengyi Liu, Lu Jiang, Shiguang Shan, Alexander G. Hauptmann category:cs.CV  published:2016-08-12 summary:Multimedia event detection has been receiving increasing attention in recent years. Besides recognizing an event, the discovery of evidences (which is refered to as "recounting") is also crucial for user to better understand the searching result. Due to the difficulty of evidence annotation, only limited supervision of event labels are available for training a recounting model. To deal with the problem, we propose a weakly supervised evidence discovery method based on self-paced learning framework, which follows a learning process from easy "evidences" to gradually more complex ones, and simultaneously exploit more and more positive evidence samples from numerous weakly annotated video segments. Moreover, to evaluate our method quantitatively, we also propose two metrics, \textit{PctOverlap} and \textit{F1-score}, for measuring the performance of evidence localization specifically. The experiments are conducted on a subset of TRECVID MED dataset and demonstrate the promising results obtained by our method. version:1
arxiv-1608-03720 | Speech Signal Analysis for the Estimation of Heart Rates Under Different Emotional States | http://arxiv.org/abs/1608.03720 | id:1608.03720 author:Aibek Ryskaliyev, Sanzhar Askaruly, Alex Pappachen James category:cs.SD cs.CV  published:2016-08-12 summary:A non-invasive method for the monitoring of heart activity can help to reduce the deaths caused by heart disorders such as stroke, arrhythmia and heart attack. The human voice can be considered as a biometric data that can be used for estimation of heart rate. In this paper, we propose a method for estimating the heart rate from human speech dynamically using voice signal analysis and by the development of an empirical linear predictor model. The correlation between the voice signal and heart rate are established by classifiers and prediction of the heart rates with or without emotions are done using linear models. The prediction accuracy was tested using the data collected from 15 subjects, it is about 4050 samples of speech signals and corresponding electrocardiogram samples. The proposed approach can use for early non-invasive detection of heart rate changes that can be correlated to an emotional state of the individual and also can be used as a tool for diagnosis of heart conditions in real-time situations. version:1
arxiv-1608-03714 | Unsupervised feature learning from finite data by message passing: discontinuous versus continuous phase transition | http://arxiv.org/abs/1608.03714 | id:1608.03714 author:Haiping Huang, Taro Toyoizumi category:cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC  published:2016-08-12 summary:Unsupervised neural network learning extracts hidden features from unlabeled training data. This is used as a pretraining step for further supervised learning in deep networks. Hence, understanding unsupervised learning is of fundamental importance. Here, we study the unsupervised learning from a finite number of data, based on the restricted Boltzmann machine learning. Our study inspires an efficient message passing algorithm to infer the hidden feature, and estimate the entropy of candidate features consistent with the data. Our analysis reveals that the learning requires only a few data if the feature is salient and extensively many if the feature is weak. Moreover, the entropy of candidate features monotonically decreases with data size and becomes negative (i.e., entropy crisis) before the message passing becomes unstable, suggesting a discontinuous phase transition. In terms of convergence time of the message passing algorithm, the unsupervised learning exhibits an easy-hard-easy phenomenon as the training data size increases. The hard region may be related to the discontinuous phase transition. All these properties are reproduced in an approximate Hopfield model, with an exception that the entropy crisis is absent (continuous phase transition). This key difference is also confirmed in a handwritten digits dataset. This study deepens our understanding of unsupervised learning from a finite number of data, and may provide insights into its role in training deep networks. version:1
arxiv-1608-01431 | An efficient iterative thresholding method for image segmentation | http://arxiv.org/abs/1608.01431 | id:1608.01431 author:Dong Wang, Haohan Li, Xiaoyu Wei, Xiaoping Wang category:cs.CV math.NA  published:2016-08-04 summary:We proposed an efficient iterative thresholding method for multi-phase image segmentation. The algorithm is based on minimizing piecewise constant Mumford-Shah functional in which the contour length (or perimeter) is approximated by a non-local multi-phase energy. The minimization problem is solved by an iterative method. Each iteration consists of computing simple convolutions followed by a thresholding step. The algorithm is easy to implement and has the optimal complexity $O(N \log N)$ per iteration. We also show that the iterative algorithm has the total energy decaying property. We present some numerical results to show the efficiency of our method. version:2
arxiv-1608-03694 | Density Matching Reward Learning | http://arxiv.org/abs/1608.03694 | id:1608.03694 author:Sungjoon Choi, Kyungjae Lee, Andy Park, Songhwai Oh category:cs.RO cs.LG  published:2016-08-12 summary:In this paper, we focus on the problem of inferring the underlying reward function of an expert given demonstrations, which is often referred to as inverse reinforcement learning (IRL). In particular, we propose a model-free density-based IRL algorithm, named density matching reward learning (DMRL), which does not require model dynamics. The performance of DMRL is analyzed theoretically and the sample complexity is derived. Furthermore, the proposed DMRL is extended to handle nonlinear IRL problems by assuming that the reward function is in the reproducing kernel Hilbert space (RKHS) and kernel DMRL (KDMRL) is proposed. The parameters for KDMRL can be computed analytically, which greatly reduces the computation time. The performance of KDMRL is extensively evaluated in two sets of experiments: grid world and track driving experiments. In grid world experiments, the proposed KDMRL method is compared with both model-based and model-free IRL methods and shows superior performance on a nonlinear reward setting and competitive performance on a linear reward setting in terms of expected value di?fferences. Then we move on to more realistic experiments of learning diff?erent driving styles for autonomous navigation in complex and dynamic tracks using KDMRL and receding horizon control. version:1
arxiv-1608-03647 | Learning with Value-Ramp | http://arxiv.org/abs/1608.03647 | id:1608.03647 author:Tom J. Ameloot, Jan Van den Bussche category:cs.LG  published:2016-08-12 summary:We study a learning principle based on the intuition of forming ramps. The agent tries to follow an increasing sequence of values until the agent meets a peak of reward. The resulting Value-Ramp algorithm is natural, easy to configure, and has a robust implementation with natural numbers. version:1
arxiv-1608-03667 | Reasoning and Algorithm Selection Augmented Symbolic Segmentation | http://arxiv.org/abs/1608.03667 | id:1608.03667 author:Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama category:cs.CV  published:2016-08-12 summary:In this paper we present an alternative method to symbolic segmentation: we approach symbolic segmentation as an algorithm selection problem. That is, let there be a set A of available algorithms for symbolic segmentation, a set of input features $F$, a set of image attribute $\mathbb{A}$ and a selection mechanism $S(F,\mathbb{A},A)$ that selects on a case by case basis the best algorithm. The semantic segmentation is then an optimization process that combines best component segments from multiple results into a single optimal result. The experiments compare three different algorithm selection mechanisms using three selected semantic segmentation algorithms. The results show that using the current state of art algorithms and relatively low accuracy of algorithm selection the accuracy of the semantic segmentation can be improved by 2\%. version:1
arxiv-1608-03658 | Deep Hashing: A Joint Approach for Image Signature Learning | http://arxiv.org/abs/1608.03658 | id:1608.03658 author:Yadong Mu, Zhu Liu category:cs.CV  published:2016-08-12 summary:Similarity-based image hashing represents crucial technique for visual data storage reduction and expedited image search. Conventional hashing schemes typically feed hand-crafted features into hash functions, which separates the procedures of feature extraction and hash function learning. In this paper, we propose a novel algorithm that concurrently performs feature engineering and non-linear supervised hashing function learning. Our technical contributions in this paper are two-folds: 1) deep network optimization is often achieved by gradient propagation, which critically requires a smooth objective function. The discrete nature of hash codes makes them not amenable for gradient-based optimization. To address this issue, we propose an exponentiated hashing loss function and its bilinear smooth approximation. Effective gradient calculation and propagation are thereby enabled; 2) pre-training is an important trick in supervised deep learning. The impact of pre-training on the hash code quality has never been discussed in current deep hashing literature. We propose a pre-training scheme inspired by recent advance in deep network based image classification, and experimentally demonstrate its effectiveness. Comprehensive quantitative evaluations are conducted on several widely-used image benchmarks. On all benchmarks, our proposed deep hashing algorithm outperforms all state-of-the-art competitors by significant margins. In particular, our algorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy with only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In comparison, the best accuracies obtained on CIFAR10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively. version:1
arxiv-1608-03643 | Beyond Spectral: Tight Bounds for Planted Gaussians | http://arxiv.org/abs/1608.03643 | id:1608.03643 author:Ravi Kannan, Santosh Vempala category:cs.LG cs.DS stat.ML  published:2016-08-12 summary:Given an $n\times n$ matrix with i.i.d. N(0,1) entries except for a hidden n^{0.5-\delta} x n^{0.5-\delta} submatrix where the entries are N(0,\sigma_1^2) ("planted Gaussian"), we give a polynomial-time algorithm to recover the hidden part for some \delta >0, provided \sigma_1^2>2. We also show a matching lower bound: there is no polynomial time Statistical Query algorithm for detecting a hidden part when \sigma_1^2\in (0,2], unless \delta=0. We present two algorithms for the upper bound, with different behaviors close to the threshold. The lower bound as well as the stronger upper bound depend on the chi-squared distance of the two distributions. We give extensions to the setting when the hidden part has entries from N(\mu,\sigma_1^2). version:1
arxiv-1608-03639 | Faster Training of Very Deep Networks Via p-Norm Gates | http://arxiv.org/abs/1608.03639 | id:1608.03639 author:Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG cs.NE  published:2016-08-11 summary:A major contributing factor to the recent advances in deep neural networks is structural units that let sensory information and gradients to propagate easily. Gating is one such structure that acts as a flow control. Gates are employed in many recent state-of-the-art recurrent models such as LSTM and GRU, and feedforward models such as Residual Nets and Highway Networks. This enables learning in very deep networks with hundred layers and helps achieve record-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP (e.g., machine translation with GRU). However, there is limited work in analysing the role of gating in the learning process. In this paper, we propose a flexible $p$-norm gating scheme, which allows user-controllable flow and as a consequence, improve the learning speed. This scheme subsumes other existing gating schemes, including those in GRU, Highway Networks and Residual Nets as special cases. Experiments on large sequence and vector datasets demonstrate that the proposed gating scheme helps improve the learning speed significantly without extra overhead. version:1
arxiv-1608-03630 | Distributed-memory large deformation diffeomorphic 3D image registration | http://arxiv.org/abs/1608.03630 | id:1608.03630 author:Andreas Mang, Amir Gholami, George Biros category:cs.DC cs.CV math.OC  published:2016-08-11 summary:We present a parallel distributed-memory algorithm for large deformation diffeomorphic registration of volumetric images that produces large isochoric deformations (locally volume preserving). Image registration is a key technology in medical image analysis. Our algorithm uses a partial differential equation constrained optimal control formulation. Finding the optimal deformation map requires the solution of a highly nonlinear problem that involves pseudo-differential operators, biharmonic operators, and pure advection operators both forward and back- ward in time. A key issue is the time to solution, which poses the demand for efficient optimization methods as well as an effective utilization of high performance computing resources. To address this problem we use a preconditioned, inexact, Gauss-Newton- Krylov solver. Our algorithm integrates several components: a spectral discretization in space, a semi-Lagrangian formulation in time, analytic adjoints, different regularization functionals (including volume-preserving ones), a spectral preconditioner, a highly optimized distributed Fast Fourier Transform, and a cubic interpolation scheme for the semi-Lagrangian time-stepping. We demonstrate the scalability of our algorithm on images with resolution of up to $1024^3$ on the "Maverick" and "Stampede" systems at the Texas Advanced Computing Center (TACC). The critical problem in the medical imaging application domain is strong scaling, that is, solving registration problems of a moderate size of $256^3$---a typical resolution for medical images. We are able to solve the registration problem for images of this size in less than five seconds on 64 x86 nodes of TACC's "Maverick" system. version:1
arxiv-1608-03617 | Automatic detection of moving objects in video surveillance | http://arxiv.org/abs/1608.03617 | id:1608.03617 author:Larbi Guezouli, Hanane Belhani category:cs.CV  published:2016-08-11 summary:This work is in the field of video surveillance including motion detection. The video surveillance is one of essential techniques for automatic video analysis to extract crucial information or relevant scenes in video surveillance systems. The aim of our work is to propose solutions for the automatic detection of moving objects in real time with a surveillance camera. The detected objects are objects that have some geometric shape (circle, ellipse, square, and rectangle). version:1
arxiv-1608-03609 | Clockwork Convnets for Video Semantic Segmentation | http://arxiv.org/abs/1608.03609 | id:1608.03609 author:Evan Shelhamer, Kate Rakelly, Judy Hoffman, Trevor Darrell category:cs.CV  published:2016-08-11 summary:Recent years have seen tremendous progress in still-image segmentation; however the na\"ive application of these state-of-the-art algorithms to every video frame requires considerable computation and ignores the temporal continuity inherent in video. We propose a video recognition framework that relies on two key observations: 1) while pixels may change rapidly from frame to frame, the semantic content of a scene evolves more slowly, and 2) execution can be viewed as an aspect of architecture, yielding purpose-fit computation schedules for networks. We define a novel family of "clockwork" convnets driven by fixed or adaptive clock signals that schedule the processing of different layers at different update rates according to their semantic stability. We design a pipeline schedule to reduce latency for real-time recognition and a fixed-rate schedule to reduce overall computation. Finally, we extend clockwork scheduling to adaptive video processing by incorporating data-driven clocks that can be tuned on unlabeled video. The accuracy and efficiency of clockwork convnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video datasets. version:1
arxiv-1608-03585 | Warm Starting Bayesian Optimization | http://arxiv.org/abs/1608.03585 | id:1608.03585 author:Matthias Poloczek, Jialei Wang, Peter I. Frazier category:stat.ML cs.LG stat.AP  published:2016-08-11 summary:We develop a framework for warm-starting Bayesian optimization, that reduces the solution time required to solve an optimization problem that is one in a sequence of related problems. This is useful when optimizing the output of a stochastic simulator that fails to provide derivative information, for which Bayesian optimization methods are well-suited. Solving sequences of related optimization problems arises when making several business decisions using one optimization model and input data collected over different time periods or markets. While many gradient-based methods can be warm started by initiating optimization at the solution to the previous problem, this warm start approach does not apply to Bayesian optimization methods, which carry a full metamodel of the objective function from iteration to iteration. Our approach builds a joint statistical model of the entire collection of related objective functions, and uses a value of information calculation to recommend points to evaluate. version:1
arxiv-1608-03542 | WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia | http://arxiv.org/abs/1608.03542 | id:1608.03542 author:Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, David Berthelot category:cs.CL  published:2016-08-11 summary:We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%. version:1
arxiv-1608-03530 | Semi-Supervised Prediction of Gene Regulatory Networks Using Machine Learning Algorithms | http://arxiv.org/abs/1608.03530 | id:1608.03530 author:Nihir Patel, Jason T. L. Wang category:cs.LG q-bio.QM stat.ML  published:2016-08-11 summary:Use of computational methods to predict gene regulatory networks (GRNs) from gene expression data is a challenging task. Many studies have been conducted using unsupervised methods to fulfill the task; however, such methods usually yield low prediction accuracies due to the lack of training data. In this article, we propose semi-supervised methods for GRN prediction by utilizing two machine learning algorithms, namely support vector machines (SVM) and random forests (RF). The semi-supervised methods make use of unlabeled data for training. We investigate inductive and transductive learning approaches, both of which adopt an iterative procedure to obtain reliable negative training data from the unlabeled data. We then apply our semi-supervised methods to gene expression data of Escherichia coli and Saccharomyces cerevisiae, and evaluate the performance of our methods using the expression data. Our analysis indicated that the transductive learning approach outperformed the inductive learning approach for both organisms. However, there was no conclusive difference identified in the performance of SVM and RF. Experimental results also showed that the proposed semi-supervised methods performed better than existing supervised methods for both organisms. version:1
arxiv-1608-03487 | A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates | http://arxiv.org/abs/1608.03487 | id:1608.03487 author:Tianbao Yang, Qihang Lin, Lijun Zhang category:math.OC stat.ML  published:2016-08-11 summary:This paper focuses on convex constrained optimization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates. We first present a general theory of optimization with only one projection. Its application to smooth optimization with only one projection yields $O(1/\epsilon)$ iteration complexity, which can be further reduced under strong convexity and improves over the $O(1/\epsilon^2)$ iteration complexity established before for non-smooth optimization. Then we introduce the local error bound condition and develop faster convergent algorithms for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we achieve a convergence rate of $\widetilde O(1/\epsilon^{2(1-\theta)})$ for non-smooth optimization and $\widetilde O(1/\epsilon^{1-\theta})$ for smooth optimization, where $\theta\in(0,1]$ is a constant in the local error bound condition. An experiment on solving the constrained $\ell_1$ minimization problem in compressive sensing demonstrates that the proposed algorithm achieve significant speed-up. version:1
arxiv-1608-03474 | Learning Dynamic Hierarchical Models for Anytime Scene Labeling | http://arxiv.org/abs/1608.03474 | id:1608.03474 author:Buyu Liu, Xuming He category:cs.CV  published:2016-08-11 summary:With increasing demand for efficient image and video analysis, test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications. We propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction. In particular, our approach incorporates the cost of feature computation and model inference, and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models. We formulate this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space. A high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism. We demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets, which achieves $90\%$ of the state-of-the-art performances by using $15\%$ of their overall costs. version:1
arxiv-1608-03462 | Multi-View Product Image Search Using ConvNets Features | http://arxiv.org/abs/1608.03462 | id:1608.03462 author:Muhammet Bastan, Ozgur Yilmaz category:cs.CV cs.MM  published:2016-08-11 summary:Multi-view queries on a multi-view product image database with bag-of-visual words (BoWs) have been shown to improve the average precision significantly compared to traditional single view queries on single view databases. In this paper, we investigate the performance of deep convolutional neural networks (ConvNets) on multi-view product image search and compare to the classical BoWs. We used a simplified version of VGG network to train and extract global ConvNets image representations for retrieval. We performed experiments on the publicly available Multi-View Object Image Dataset (MVOD 5K) and concluded that (1) multi-view queries with ConvNets representations perform significantly better than single view queries, (2) ConvNets perform much better than BoWs and have room for further improvement. version:1
arxiv-1608-03448 | Sex, drugs, and violence | http://arxiv.org/abs/1608.03448 | id:1608.03448 author:Stefania Raimondo, Frank Rudzicz category:cs.CL  published:2016-08-11 summary:Automatically detecting inappropriate content can be a difficult NLP task, requiring understanding context and innuendo, not just identifying specific keywords. Due to the large quantity of online user-generated content, automatic detection is becoming increasingly necessary. We take a largely unsupervised approach using a large corpus of narratives from a community-based self-publishing website and a small segment of crowd-sourced annotations. We explore topic modelling using latent Dirichlet allocation (and a variation), and use these to regress appropriateness ratings, effectively automating rating for suitability. The results suggest that certain topics inferred may be useful in detecting latent inappropriateness -- yielding recall up to 96% and low regression errors. version:1
arxiv-1608-03440 | Recurrent Neural Networks to Enhance Satellite Image Classification Maps | http://arxiv.org/abs/1608.03440 | id:1608.03440 author:Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat, Pierre Alliez category:cs.CV  published:2016-08-11 summary:The automatic pixelwise labeling of satellite images is of paramount importance in remote sensing. Convolutional neural networks represent a competitive means to learn the contextual features required to distinguish an object class from the rest. However, spatial precision is usually lost in the process, leading to coarse classification maps that do not accurately outline the objects. While specific enhancement algorithms have been designed in the literature to improve such coarse neural network outputs, it requires a decision-making process to choose and tune the right enhancement algorithm. Instead, we aim at learning the enhancement algorithm itself. We consider the class of partial differential equations, see them as iterative enhancement processes, and observe that they can be expressed as recurrent neural networks. Consequently, we train a recurrent neural network from manually labeled data for our enhancement task. In a series of experiments we show that our network effectively learns an iterative process that significantly improves the quality of satellite image classification maps. version:1
arxiv-1608-03410 | Solving Visual Madlibs with Multiple Cues | http://arxiv.org/abs/1608.03410 | id:1608.03410 author:Tatiana Tommasi, Arun Mallya, Bryan Plummer, Svetlana Lazebnik, Alexander C. Berg, Tamara L. Berg category:cs.CV  published:2016-08-11 summary:This paper focuses on answering fill-in-the-blank style multiple choice questions from the Visual Madlibs dataset. Previous approaches to Visual Question Answering (VQA) have mainly used generic image features from networks trained on the ImageNet dataset, despite the wide scope of questions. In contrast, our approach employs features derived from networks trained for specialized tasks of scene classification, person activity prediction, and person and object attribute prediction. We also present a method for selecting sub-regions of an image that are relevant for evaluating the appropriateness of a putative answer. Visual features are computed both from the whole image and from local regions, while sentences are mapped to a common space using a simple normalized canonical correlation analysis (CCA) model. Our results show a significant improvement over the previous state of the art, and indicate that answering different question types benefits from examining a variety of image cues and carefully choosing informative image sub-regions. version:1
arxiv-1608-03396 | A machine learning method for the large-scale evaluation of urban visual environment | http://arxiv.org/abs/1608.03396 | id:1608.03396 author:Lun Liu, Hui Wang, Chunyang Wu category:cs.CV cs.CY cs.HC  published:2016-08-11 summary:Given the size of modern cities in the urbanising age, it is beyond the perceptual capacity of most people to develop a good knowledge about the beauty and ugliness of the city at every street corner. Correspondingly, for planners, it is also difficult to accurately answer questions like 'where are the worst-looking places in the city that regeneration should give first consideration', or 'in the fast urbanising cities, how is the city appearance changing', etc. To address this issue, we here present a computer vision method for the large-scale and automatic evaluation of the urban visual environment, by leveraging state-of-the-art machine learning techniques and the wide-coverage street view images. From the various factors that are at work, we choose two key features, the visual quality of street facade and the continuity of street wall, as the starting point of this line of analysis. In order to test the validity of this method, we further compare the machine ratings with ratings collected on site from 752 passers-by on fifty-six locations. We show that the machine learning model can produce a good estimation of people's real visual experience, and it holds much potential for various tasks in terms of urban design evaluation, culture identification, etc. version:1
arxiv-1608-03374 | Automatic text extraction and character segmentation using maximally stable extremal regions | http://arxiv.org/abs/1608.03374 | id:1608.03374 author:Nitigya Sambyal, Pawanesh Abrol category:cs.CV  published:2016-08-11 summary:Text detection and segmentation is an important prerequisite for many content based image analysis tasks. The paper proposes a novel text extraction and character segmentation algorithm using Maximally Stable Extremal Regions as basic letter candidates. These regions are then subjected to thresholding and thereafter various connected components are determined to identify separate characters. The algorithm is tested along a set of various JPEG, PNG and BMP images over four different character sets; English, Russian, Hindi and Urdu. The algorithm gives good results for English and Russian character set; however character segmentation in Urdu and Hindi language is not much accurate. The algorithm is simple, efficient, involves no overhead as required in training and gives good results for even low quality images. The paper also proposes various challenges in text extraction and segmentation for multilingual inputs. version:1
arxiv-1608-03369 | Enabling My Robot To Play Pictionary : Recurrent Neural Networks For Sketch Recognition | http://arxiv.org/abs/1608.03369 | id:1608.03369 author:Ravi Kiran Sarvadevabhatla, Jogendra Kundu, Babu R. Venkatesh category:cs.CV  published:2016-08-11 summary:Freehand sketching is an inherently sequential process. Yet, most approaches for hand-drawn sketch recognition either ignore this sequential aspect or exploit it in an ad-hoc manner. In our work, we propose a recurrent neural network architecture for sketch object recognition which exploits the long-term sequential and structural regularities in stroke data in a scalable manner. Specifically, we introduce a Gated Recurrent Unit based framework which leverages deep sketch features and weighted per-timestep loss to achieve state-of-the-art results on a large database of freehand object sketches across a large number of object categories. The inherently online nature of our framework is especially suited for on-the-fly recognition of objects as they are being drawn. Thus, our framework can enable interesting applications such as camera-equipped robots playing the popular party game Pictionary with human players and generating sparsified yet recognizable sketches of objects. version:1
arxiv-1608-03344 | Multi-source Hierarchical Prediction Consolidation | http://arxiv.org/abs/1608.03344 | id:1608.03344 author:Chenwei Zhang, Sihong Xie, Yaliang Li, Jing Gao, Wei Fan, Philip S. Yu category:cs.DB cs.LG  published:2016-08-11 summary:In big data applications such as healthcare data mining, due to privacy concerns, it is necessary to collect predictions from multiple information sources for the same instance, with raw features being discarded or withheld when aggregating multiple predictions. Besides, crowd-sourced labels need to be aggregated to estimate the ground truth of the data. Because of the imperfect predictive models or human crowdsourcing workers, noisy and conflicting information is ubiquitous and inevitable. Although state-of-the-art aggregation methods have been proposed to handle label spaces with flat structures, as the label space is becoming more and more complicated, aggregation under a label hierarchical structure becomes necessary but has been largely ignored. These label hierarchies can be quite informative as they are usually created by domain experts to make sense of highly complex label correlations for many real-world cases like protein functionality interactions or disease relationships. We propose a novel multi-source hierarchical prediction consolidation method to effectively exploits the complicated hierarchical label structures to resolve the noisy and conflicting information that inherently originates from multiple imperfect sources. We formulate the problem as an optimization problem with a closed-form solution. The proposed method captures the smoothness overall information sources as well as penalizing any consolidation result that violates the constraints derived from the label hierarchy. The hierarchical instance similarity, as well as the consolidation result, are inferred in a totally unsupervised, iterative fashion. Experimental results on both synthetic and real-world datasets show the effectiveness of the proposed method over existing alternatives. version:1
arxiv-1608-03339 | Distributed Learning with Regularized Least Squares | http://arxiv.org/abs/1608.03339 | id:1608.03339 author:Shaobo Lin, Xin Guo, Dingxuan Zhou category:cs.LG stat.ML 68T05  94A20  41A35 F.2.3  published:2016-08-11 summary:We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds in expectation in both the $L^2$-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our error bounds are sharp and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in the literature. version:1
arxiv-1608-03333 | Temporal Learning and Sequence Modeling for a Job Recommender System | http://arxiv.org/abs/1608.03333 | id:1608.03333 author:Kuan Liu, Xing Shi, Anoop Kumar, Linhong Zhu, Prem Natarajan category:cs.LG stat.ML  published:2016-08-11 summary:We present our solution to the job recommendation task for RecSys Challenge 2016. The main contribution of our work is to combine temporal learning with sequence modeling to capture complex user-item activity patterns to improve job recommendations. First, we propose a time-based ranking model applied to historical observations and a hybrid matrix factorization over time re-weighted interactions. Second, we exploit sequence properties in user-items activities and develop a RNN-based recommendation model. Our solution achieved 5$^{th}$ place in the challenge among more than 100 participants. Notably, the strong performance of our RNN approach shows a promising new direction in employing sequence modeling for recommendation systems. version:1
arxiv-1608-03308 | Approximate search with quantized sparse representations | http://arxiv.org/abs/1608.03308 | id:1608.03308 author:Himalaya Jain, Patrick Pérez, Rémi Gribonval, Joaquin Zepeda, Hervé Jégou category:cs.CV  published:2016-08-10 summary:This paper tackles the task of storing a large collection of vectors, such as visual descriptors, and of searching in it. To this end, we propose to approximate database vectors by constrained sparse coding, where possible atom weights are restricted to belong to a finite subset. This formulation encompasses, as particular cases, previous state-of-the-art methods such as product or residual quantization. As opposed to traditional sparse coding methods, quantized sparse coding includes memory usage as a design constraint, thereby allowing us to index a large collection such as the BIGANN billion-sized benchmark. Our experiments, carried out on standard benchmarks, show that our formulation leads to competitive solutions when considering different trade-offs between learning/coding time, index size and search quality. version:1
arxiv-1608-03287 | Deep vs. shallow networks : An approximation theory perspective | http://arxiv.org/abs/1608.03287 | id:1608.03287 author:Hrushikesh Mhaskar, Tomaso Poggio category:cs.LG math.FA  published:2016-08-10 summary:The paper briefy reviews several recent results on hierarchical architectures for learning from examples, that may formally explain the conditions under which Deep Convolutional Neural Networks perform much better in function approximation problems than shallow, one-hidden layer architectures. The paper announces new results for a non-smooth activation function - the ReLU function - used in present-day neural networks, as well as for the Gaussian networks. We propose a new definition of relative dimension to encapsulate different notions of sparsity of a function class that can possibly be exploited by deep networks but not by shallow ones to drastically reduce the complexity required for approximation and learning. version:1
arxiv-1608-00272 | Modeling Context in Referring Expressions | http://arxiv.org/abs/1608.00272 | id:1608.00272 author:Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L. Berg category:cs.CV cs.CL  published:2016-07-31 summary:Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension. version:3
arxiv-1608-03248 | Combinations of Adaptive Filters with Coefficients Feedback | http://arxiv.org/abs/1608.03248 | id:1608.03248 author:Luiz F. O. Chamon, Cassio G. Lopes category:math.OC cs.IT cs.LG math.IT  published:2016-08-10 summary:Parallel combinations of adaptive filters have been effectively used to improve the performance of adaptive algorithms and address typical trade-offs, such as the one between convergence rate and steady-state error. In these combinations, the component filters are usually run independently and then combined, which leads to a well known convergence stagnation effect. Conditional transfers of coefficients between filters were introduced in an attempt to handle this issue. This work introduces a more natural way of accelerating convergence to steady-state by cyclically feeding back the overall coefficients to all component filters. Besides coping with convergence stagnation, this new topology allows several adaptive algorithms (e.g., mixed norm, data reusing, and variable step size) to be posed as combinations of simple adaptive filters, bridging an important conceptual gap. Steady-state and tracking analysis accounting for a myriad of component filters are derived for combinations with and without feedback. Transient analyses of the typical convex and affine supervisors are extended to general activation functions and applied to combinations with cyclic coefficients feedback. Numerical examples are provided to illustrate how coefficients feedback can improve the performance of several existing parallel combinations at a small additional computational cost. version:1
arxiv-1608-03240 | Fractional Calculus In Image Processing: A Review | http://arxiv.org/abs/1608.03240 | id:1608.03240 author:Qi Yang, Dali Chen, Tiebiao Zhao, YangQuan Chen category:cs.CV 26A33  published:2016-08-10 summary:Over the last decade, it has been demonstrated that many systems in science and engineering can be modeled more accurately by fractional-order than integer-order derivatives, and many methods are developed to solve the problem of fractional systems. Due to the extra free parameter order, fractional-order based methods provide additional degree of freedom in optimization performance. Not surprisingly, many fractional-order based methods have been used in image processing field. Herein recent studies are reviewed in ten sub-fields, which include image enhancement, image denoising, image edge detection, image segmentation, image registration, image recognition, image fusion, image encryption, image compression and image restoration. In sum, it is well proved that as a fundamental mathematic tool, fractional-order derivative shows great success in image processing. version:1
arxiv-1608-03235 | Gaze2Segment: A Pilot Study for Integrating Eye-Tracking Technology into Medical Image Segmentation | http://arxiv.org/abs/1608.03235 | id:1608.03235 author:Naji Khosravan, Haydar Celik, Baris Turkbey, Ruida Cheng, Evan McCreedy, Matthew McAuliffe, Sandra Bednarova, Elizabeth Jones, Xinjian Chen, Peter L. Choyke, Bradford J. Wood, Ulas Bagci category:cs.CV  published:2016-08-10 summary:This study introduced a novel system, called Gaze2Segment, integrating biological and computer vision techniques to support radiologists' reading experience with an automatic image segmentation task. During diagnostic assessment of lung CT scans, the radiologists' gaze information were used to create a visual attention map. This map was then combined with a computer-derived saliency map, extracted from the gray-scale CT images. The visual attention map was used as an input for indicating roughly the location of a object of interest. With computer-derived saliency information, on the other hand, we aimed at finding foreground and background cues for the object of interest. At the final step, these cues were used to initiate a seed-based delineation process. Segmentation accuracy of the proposed Gaze2Segment was found to be 86% with dice similarity coefficient and 1.45 mm with Hausdorff distance. To the best of our knowledge, Gaze2Segment is the first true integration of eye-tracking technology into a medical image segmentation task without the need for any further user-interaction. version:1
arxiv-1608-03217 | DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns | http://arxiv.org/abs/1608.03217 | id:1608.03217 author:Ali Diba, Ali Mohammad Pazandeh, Hamed Pirsiavash, Luc Van Gool category:cs.CV  published:2016-08-10 summary:The recognition of human actions and the determination of human attributes are two tasks that call for fine-grained classification. Indeed, often rather small and inconspicuous objects and features have to be detected to tell their classes apart. In order to deal with this challenge, we propose a novel convolutional neural network that mines mid-level image patches that are sufficiently dedicated to resolve the corresponding subtleties. In particular, we train a newly de- signed CNN (DeepPattern) that learns discriminative patch groups. There are two innovative aspects to this. On the one hand we pay attention to contextual information in an origi- nal fashion. On the other hand, we let an iteration of feature learning and patch clustering purify the set of dedicated patches that we use. We validate our method for action clas- sification on two challenging datasets: PASCAL VOC 2012 Action and Stanford 40 Actions, and for attribute recogni- tion we use the Berkeley Attributes of People dataset. Our discriminative mid-level mining CNN obtains state-of-the- art results on these datasets, without a need for annotations about parts and poses. version:1
arxiv-1608-03192 | Growing Graphs with Hyperedge Replacement Graph Grammars | http://arxiv.org/abs/1608.03192 | id:1608.03192 author:Salvador Aguiñaga, Rodrigo Palacios, David Chiang, Tim Weninger category:cs.SI cs.CL  published:2016-08-10 summary:Discovering the underlying structures present in large real world graphs is a fundamental scientific problem. In this paper we show that a graph's clique tree can be used to extract a hyperedge replacement grammar. If we store an ordering from the extraction process, the extracted graph grammar is guaranteed to generate an isomorphic copy of the original graph. Or, a stochastic application of the graph grammar rules can be used to quickly create random graphs. In experiments on large real world networks, we show that random graphs, generated from extracted graph grammars, exhibit a wide range of properties that are very similar to the original graphs. In addition to graph properties like degree or eigenvector centrality, what a graph "looks like" ultimately depends on small details in local graph substructures that are difficult to define at a global level. We show that our generative graph model is able to preserve these local substructures when generating new graphs and performs well on new and difficult tests of model robustness. version:1
arxiv-1608-03123 | Escaping Local Optima using Crossover with Emergent or Reinforced Diversity | http://arxiv.org/abs/1608.03123 | id:1608.03123 author:Duc-Cuong Dang, Tobias Friedrich, Timo Kötzing, Martin S. Krejca, Per Kristian Lehre, Pietro S. Oliveto, Dirk Sudholt, Andrew M. Sutton category:cs.NE cs.CC q-bio.PE  published:2016-08-10 summary:Population diversity is essential for avoiding premature convergence in Genetic Algorithms (GAs) and for the effective use of crossover. Yet the dynamics of how diversity emerges in populations are not well understood. We use rigorous runtime analysis to gain insight into population dynamics and GA performance for the ($\mu$+1) GA and the $\text{Jump}_k$ test function. We show that the interplay of crossover and mutation may serve as a catalyst leading to a sudden burst of diversity. This leads to improvements of the expected optimisation time of order $\Omega(n/\log n)$ compared to mutation-only algorithms like (1+1) EA. Moreover, increasing the mutation rate by an arbitrarily small constant factor can facilitate the generation of diversity, leading to speedups of order $\Omega(n)$. We also compare seven commonly used diversity mechanisms and evaluate their impact on runtime bounds for the ($\mu$+1) GA. All previous results in this context only hold for unrealistically low crossover probability $p_c=O(k/n)$, while we give analyses for the setting of constant $p_c < 1$ in all but one case. For the typical case of constant $k > 2$ and constant $p_c$, we can compare the resulting expected runtimes for different diversity mechanisms assuming an optimal choice of $\mu$: $O(n^{k-1})$ for duplicate elimination/minim., $O(n^2\log n)$ for maximising the convex hull, $O(n\log n)$ for deterministic crowding (assuming $p_c = k/n$), $O(n\log n)$ for maximising Hamming distance, $O(n\log n)$ for fitness sharing, $O(n\log n)$ for single-receiver island model. This proves a sizeable advantage of all variants of the ($\mu$+1) GA compared to (1+1) EA, which requires time $\Theta(n^k)$. Experiments complement our theoretical findings and further highlight the benefits of crossover and diversity on $\text{Jump}_k$. version:1
arxiv-1608-03100 | Estimation from Indirect Supervision with Linear Moments | http://arxiv.org/abs/1608.03100 | id:1608.03100 author:Aditi Raghunathan, Roy Frostig, John Duchi, Percy Liang category:stat.ML cs.LG  published:2016-08-10 summary:In structured prediction problems where we have indirect supervision of the output, maximum marginal likelihood faces two computational obstacles: non-convexity of the objective and intractability of even a single gradient computation. In this paper, we bypass both obstacles for a class of what we call linear indirectly-supervised problems. Our approach is simple: we solve a linear system to estimate sufficient statistics of the model, which we then use to estimate parameters via convex optimization. We analyze the statistical properties of our approach and show empirically that it is effective in two settings: learning with local privacy constraints and learning from low-cost count-based annotations. version:1
arxiv-1608-03075 | 3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information | http://arxiv.org/abs/1608.03075 | id:1608.03075 author:Sungheon Park, Jihye Hwang, Nojun Kwak category:cs.CV  published:2016-08-10 summary:While there has been a success in 2D human pose estimation with convolutional neural networks (CNNs), 3D human pose estimation has not been thoroughly studied. In this paper, we tackle the 3D human pose estimation task with end-to-end learning using CNNs. Relative 3D positions between one joint and the other joints are learned via CNNs. The proposed method improves the performance of CNN with two novel ideas. First, we added 2D pose information to estimate a 3D pose from an image by concatenating 2D pose estimation result with the features from an image. Second, we have found that more accurate 3D poses are obtained by combining information on relative positions with respect to multiple joints, instead of just one root joint. Experimental results show that the proposed method achieves comparable performance to the state-of-the-art methods on Human 3.6m dataset. version:1
arxiv-1608-03066 | Object Detection, Tracking, and Motion Segmentation for Object-level Video Segmentation | http://arxiv.org/abs/1608.03066 | id:1608.03066 author:Benjamin Drayer, Thomas Brox category:cs.CV  published:2016-08-10 summary:We present an approach for object segmentation in videos that combines frame-level object detection with concepts from object tracking and motion segmentation. The approach extracts temporally consistent object tubes based on an off-the-shelf detector. Besides the class label for each tube, this provides a location prior that is independent of motion. For the final video segmentation, we combine this information with motion cues. The method overcomes the typical problems of weakly supervised/unsupervised video segmentation, such as scenes with no motion, dominant camera motion, and objects that move as a unit. In contrast to most tracking methods, it provides an accurate, temporally consistent segmentation of each object. We report results on four video segmentation datasets: YouTube Objects, SegTrackv2, egoMotion, and FBMS. version:1
arxiv-1608-03065 | An assessment of orthographic similarity measures for several African languages | http://arxiv.org/abs/1608.03065 | id:1608.03065 author:C. Maria Keet category:cs.CL I.2.7  published:2016-08-10 summary:Natural Language Interfaces and tools such as spellcheckers and Web search in one's own language are known to be useful in ICT-mediated communication. Most languages in Southern Africa are under-resourced, however. Therefore, it would be very useful if both the generic and the few language-specific NLP tools could be reused or easily adapted across languages. This depends on the notion, and extent, of similarity between the languages. We assess this from the angle of orthography and corpora. Twelve versions of the Universal Declaration of Human Rights (UDHR) are examined, showing clusters of languages, and which are thus more or less amenable to cross-language adaptation of NLP tools, which do not match with Guthrie zones. To examine the generalisability of these results, we zoom in on isiZulu both quantitatively and qualitatively with four other corpora and texts in different genres. The results show that the UDHR is a typical text document orthographically. The results also provide insight into usability of typical measures such as lexical diversity and genre, and that the same statistic may mean different things in different documents. While NLTK for Python could be used for basic analyses of text, it, and similar NLP tools, will need considerable customization. version:1
arxiv-1608-02833 | Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator | http://arxiv.org/abs/1608.02833 | id:1608.02833 author:Mundher Al-Shabi, Wooi Ping Cheah, Tee Connie category:cs.CV cs.AI cs.HC  published:2016-08-09 summary:Recognizing facial expression has remained a challenging task in computer vision. Deriving an effective facial expression recognition is an important step for successful human-computer interaction systems. This paper describes a novel approach towards facial expression recognition task. It is motivated by the success of Convolutional Neural Networks (CNN) on face recognition problems. Unlike other works, we focus on getting good accuracy results while requiring only a small sample data to train the model by merging the CNN and SIFT features. The proposed classification model is an aggregation of multiple deep convolutional neural networks and a hybrid CNN-SIFT classifiers. The goal of using SIFT features is to increase the performance on small data as SIFT does not require large training data to generate useful features. The model has been tested on FER-2013, CK+ and SFEW 2.0 datasets. The results showed how CNN-SIFT feature improve the accuracy when added as a voting member in an ensemble classifier. It generates state-of-art results on FER-2013 and CK+ datasets, where it achieved 73.58% on FER-2013 and 99.35% on CK+. version:2
arxiv-1608-03049 | Fashion Landmark Detection in the Wild | http://arxiv.org/abs/1608.03049 | id:1608.03049 author:Ziwei Liu, Sijie Yan, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2016-08-10 summary:Visual fashion analysis has attracted many attentions in the recent years. Previous work represented clothing regions by either bounding boxes or human joints. This work presents fashion landmark detection or fashion alignment, which is to predict the positions of functional key points defined on the fashion items, such as the corners of neckline, hemline, and cuff. To encourage future studies, we introduce a fashion landmark dataset with over 120K images, where each image is labeled with eight landmarks. With this dataset, we study fashion alignment by cascading multiple convolutional neural networks in three stages. These stages gradually improve the accuracies of landmark predictions. Extensive experiments demonstrate the effectiveness of the proposed method, as well as its generalization ability to pose estimation. Fashion landmark is also compared to clothing bounding boxes and human joints in two applications, fashion attribute prediction and clothes retrieval, showing that fashion landmark is a more discriminative representation to understand fashion images. version:1
arxiv-1608-03030 | Hierarchical Character-Word Models for Language Identification | http://arxiv.org/abs/1608.03030 | id:1608.03030 author:Aaron Jaech, George Mulcaire, Shobhit Hathi, Mari Ostendorf, Noah A. Smith category:cs.CL  published:2016-08-10 summary:Social media messages' brevity and unconventional spelling pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong base- lines, and can also reveal code-switching. version:1
arxiv-1608-03023 | Stochastic Rank-1 Bandits | http://arxiv.org/abs/1608.03023 | id:1608.03023 author:Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade, Zheng Wen category:cs.LG stat.ML  published:2016-08-10 summary:We propose stochastic rank-$1$ bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their payoffs as a reward. The main challenge of the problem is that the learning agent does not observe the payoffs of the individual arms, only their product. The payoffs of the row and column arms are stochastic, and independent of each other. We propose a computationally-efficient algorithm for solving our problem, Rank1Elim, and derive a $O((K + L) (1 / \Delta) \log n)$ upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the number of columns, and $\Delta$ is the minimum gap in the row and column payoffs. To the best of our knowledge, this is the first bandit algorithm for stochastic rank-$1$ matrix factorization whose regret is linear in $K + L$, $1 / \Delta$, and $\log n$. We evaluate Rank1Elim on a synthetic problem and show that its regret scales as suggested by our upper bound. We also compare it to UCB1, and show significant improvements as $K$ and $L$ increase. version:1
arxiv-1608-03022 | Dynamic Principal Component Analysis: Identifying the Relationship between Multiple Air Pollutants | http://arxiv.org/abs/1608.03022 | id:1608.03022 author:Oleg Melnikov, Loren H. Raun, Katherine B. Ensor category:stat.AP stat.ML  published:2016-08-10 summary:The dynamic nature of air quality chemistry and transport makes it difficult to identify the mixture of air pollutants for a region. In this study of air quality in the Houston metropolitan area we apply dynamic principal component analysis (DPCA) to a normalized multivariate time series of daily concentration measurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009 through December 31, 2011 for each of the 24 hours in a day. The resulting dynamic components are examined by hour across days for the 3 year period. Diurnal and seasonal patterns are revealed underlining times when DPCA performs best and two principal components (PCs) explain most variability in the multivariate series. DPCA is shown to be superior to static principal component analysis (PCA) in discovery of linear relations among transformed pollutant measurements. DPCA captures the time-dependent correlation structure of the underlying pollutants recorded at up to 34 monitoring sites in the region. In winter mornings the first principal component (PC1) (mainly CO and NO2) explains up to 70% of variability. Augmenting with the second principal component (PC2) (mainly driven by SO2) the explained variability rises to 90%. In the afternoon, O3 gains prominence in the second principal component. The seasonal profile of PCs' contribution to variance loses its distinction in the afternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability in ambient air data. DPCA provides a strategy for identifying the changing air quality profile for the region studied. version:1
arxiv-1608-03016 | Mining Fashion Outfit Composition Using An End-to-End Deep Learning Approach on Set Data | http://arxiv.org/abs/1608.03016 | id:1608.03016 author:Yuncheng Li, LiangLiang Cao, Jiang Zhu, Jiebo Luo category:cs.MM cs.LG  published:2016-08-10 summary:Fashion composition involves deep understanding of fashion standards while incorporating creativity for choosing multiple fashion items (e.g., Jewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality fashion compositions are usually designed by fashion experts and followed by large audiences. In this paper, we aim to employ a machine learning strategy to compose fashion compositions by learning directly from the fashion websites. We propose an end-to-end system to learn a fashion item embedding that helps disentangle the factors contributing to fashion popularity, such as instance aesthetics and set compatibility. Our learning system consists of 1) deep convolutional network embedding of fashion images, 2) title embedding, and 3) category embedding. To leverage the multimodal information, we develop a multiple-layer perceptron module with different pooling strategies to predict the set popularity. For our experiments, we have collected a large-scale fashion set from the fashion website Polyvore. Although fashion composition is a rather challenging task, the performance of our system is quite encouraging: we have achieved an AUC of 85\% for the fashion set popularity prediction task on the Polyvore fashion set. version:1
arxiv-1608-03000 | Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge | http://arxiv.org/abs/1608.03000 | id:1608.03000 author:Nicholas Locascio, Karthik Narasimhan, Eduardo DeLeon, Nate Kushman, Regina Barzilay category:cs.CL cs.AI  published:2016-08-09 summary:This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models. version:1
arxiv-1608-02996 | Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders | http://arxiv.org/abs/1608.02996 | id:1608.02996 author:Antonio Valerio Miceli Barone category:cs.CL cs.LG cs.NE  published:2016-08-09 summary:Current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text, aligned at word, sentence or at least document level. We hypothesize however, that different natural languages share enough semantic structure that it should be possible, in principle, to learn compatible vector representations just by analyzing the monolingual distribution of words. In order to evaluate this hypothesis, we propose a scheme to map word vectors trained on a source language to vectors semantically compatible with word vectors trained on a target language using an adversarial autoencoder. We present preliminary qualitative results and discuss possible future developments of this technique, such as applications to cross-lingual sentence representations. version:1
arxiv-1608-02989 | Deep Convolutional Neural Networks for Microscopy-Based Point of Care Diagnostics | http://arxiv.org/abs/1608.02989 | id:1608.02989 author:John A. Quinn, Rose Nakasi, Pius K. B. Mugagga, Patrick Byanyima, William Lubega, Alfred Andama category:cs.CV  published:2016-08-09 summary:Point of care diagnostics using microscopy and computer vision methods have been applied to a number of practical problems, and are particularly relevant to low-income, high disease burden areas. However, this is subject to the limitations in sensitivity and specificity of the computer vision methods used. In general, deep learning has recently revolutionised the field of computer vision, in some cases surpassing human performance for other object recognition tasks. In this paper, we evaluate the performance of deep convolutional neural networks on three different microscopy tasks: diagnosis of malaria in thick blood smears, tuberculosis in sputum samples, and intestinal parasite eggs in stool samples. In all cases accuracy is very high and substantially better than an alternative approach more representative of traditional medical imaging techniques. version:1
arxiv-1608-02257 | Robust High-Dimensional Linear Regression | http://arxiv.org/abs/1608.02257 | id:1608.02257 author:Chang Liu, Bo Li, Yevgeniy Vorobeychik, Alina Oprea category:cs.LG cs.CR stat.ML  published:2016-08-07 summary:The effectiveness of supervised learning techniques has made them ubiquitous in research and practice. In high-dimensional settings, supervised learning commonly relies on dimensionality reduction to improve performance and identify the most important factors in predicting outcomes. However, the economic importance of learning has made it a natural target for adversarial manipulation of training data, which we term poisoning attacks. Prior approaches to dealing with robust supervised learning rely on strong assumptions about the nature of the feature matrix, such as feature independence and sub-Gaussian noise with low variance. We propose an integrated method for robust regression that relaxes these assumptions, assuming only that the feature matrix can be well approximated by a low-rank matrix. Our techniques integrate improved robust low-rank matrix approximation and robust principle component regression, and yield strong performance guarantees. Moreover, we experimentally show that our methods significantly outperform state of the art both in running time and prediction error. version:2
arxiv-1608-02971 | Neuroevolution-Based Inverse Reinforcement Learning | http://arxiv.org/abs/1608.02971 | id:1608.02971 author:Karan K. Budhraja, Tim Oates category:cs.NE cs.AI cs.LG  published:2016-08-09 summary:The problem of Learning from Demonstration is targeted at learning to perform tasks based on observed examples. One approach to Learning from Demonstration is Inverse Reinforcement Learning, in which actions are observed to infer rewards. This work combines a feature based state evaluation approach to Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying neural networks based on their performance on a given task. Neural networks are used to learn from a demonstrated expert policy and are evolved to generate a policy similar to the demonstration. The algorithm is discussed and evaluated against competitive feature-based Inverse Reinforcement Learning approaches. At the cost of execution time, neural networks allow for non-linear combinations of features in state evaluations. These valuations may correspond to state value or state reward. This results in better correspondence to observed examples as opposed to using linear combinations. This work also extends existing work on Bayesian Non-Parametric Feature Construction for Inverse Reinforcement Learning by using non-linear combinations of intermediate data to improve performance. The algorithm is observed to be specifically suitable for a linearly solvable non-deterministic Markov Decision Processes in which multiple rewards are sparsely scattered in state space. A conclusive performance hierarchy between evaluated algorithms is presented. version:1
arxiv-1608-02927 | Temporal Attention Model for Neural Machine Translation | http://arxiv.org/abs/1608.02927 | id:1608.02927 author:Baskaran Sankaran, Haitao Mi, Yaser Al-Onaizan, Abe Ittycheriah category:cs.CL  published:2016-08-09 summary:Attention-based Neural Machine Translation (NMT) models suffer from attention deficiency issues as has been observed in recent research. We propose a novel mechanism to address some of these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal memory, as the decoder generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles. version:1
arxiv-1608-02908 | Residual Networks of Residual Networks: Multilevel Residual Networks | http://arxiv.org/abs/1608.02908 | id:1608.02908 author:Ke Zhang, Miao Sun, Tony X. Han, Xingfang Yuan, Liru Guo, Tao Liu category:cs.CV  published:2016-08-09 summary:Residual networks family with hundreds or even thousands of layers dominate major image recognition tasks, but building a network by simply stacking residual blocks inevitably limits its optimization ability. This paper proposes a novel residual-network architecture, Residual networks of Residual networks (RoR), to dig the optimization ability of residual networks. RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping, in particular, adding level-wise shortcut connections upon original residual networks, to promote the learning capability of residual networks. More importantly, RoR can be applied to various kinds of residual networks (Pre-ResNets and WRN) and significantly boost their performance. Our experiments demonstrate the effectiveness and versatility of RoR, where it achieves the best performance in all residual-network-like structures. Our RoR-3-WRN58-4 models achieve new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59% respectively. These results outperform 1001-layer Pre-ResNets by 18.4% on CIFAR-10 and 13.1% on CIFAR-100. version:1
arxiv-1608-02904 | A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter | http://arxiv.org/abs/1608.02904 | id:1608.02904 author:Jeniya Tabassum, Alan Ritter, Wei Xu category:cs.IR cs.CL  published:2016-08-09 summary:We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from the reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems. version:1
arxiv-1608-02902 | Linear Regression with an Unknown Permutation: Statistical and Computational Limits | http://arxiv.org/abs/1608.02902 | id:1608.02902 author:Ashwin Pananjady, Martin J. Wainwright, Thomas A. Courtade category:math.ST cs.IT math.IT stat.ML stat.TH  published:2016-08-09 summary:Consider a noisy linear observation model with an unknown permutation, based on observing $y = \Pi^* A x^* + w$, where $x^* \in \mathbb{R}^d$ is an unknown vector, $\Pi^*$ is an unknown $n \times n$ permutation matrix, and $w \in \mathbb{R}^n$ is additive Gaussian noise. We analyze the problem of permutation recovery in a random design setting in which the entries of the matrix $A$ are drawn i.i.d. from a standard Gaussian distribution, and establish sharp conditions on the SNR, sample size $n$, and dimension $d$ under which $\Pi^*$ is exactly and approximately recoverable. On the computational front, we show that the maximum likelihood estimate of $\Pi^*$ is NP-hard to compute, while also providing a polynomial time algorithm when $d =1$. version:1
arxiv-1608-02861 | Classification with the pot-pot plot | http://arxiv.org/abs/1608.02861 | id:1608.02861 author:Oleksii Pokotylo, Karl Mosler category:stat.ML cs.LG 62H30  62G07  published:2016-08-09 summary:We propose a procedure for supervised classification that is based on potential functions. The potential of a class is defined as a kernel density estimate multiplied by the class's prior probability. The method transforms the data to a potential-potential (pot-pot) plot, where each data point is mapped to a vector of potentials. Separation of the classes, as well as classification of new data points, is performed on this plot. For this, either the $\alpha$-procedure ($\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed. For data that are generated from continuous distributions, these classifiers prove to be strongly Bayes-consistent. The potentials depend on the kernel and its bandwidth used in the density estimate. We investigate several variants of bandwidth selection, including joint and separate pre-scaling and a bandwidth regression approach. The new method is applied to benchmark data from the literature, including simulated data sets as well as 50 sets of real data. It compares favorably to known classification methods such as LDA, QDA, max kernel density estimates, $k$-NN, and $DD$-plot classification using depth functions. version:1
arxiv-1608-01198 | Ensemble-driven support vector clustering: From ensemble learning to automatic parameter estimation | http://arxiv.org/abs/1608.01198 | id:1608.01198 author:Dong Huang, Chang-Dong Wang, Jian-Huang Lai, Yun Liang, Shan Bian, Yu Chen category:cs.LG  published:2016-08-03 summary:Support vector clustering (SVC) is a versatile clustering technique that is able to identify clusters of arbitrary shapes by exploiting the kernel trick. However, one hurdle that restricts the application of SVC lies in its sensitivity to the kernel parameter and the trade-off parameter. Although many extensions of SVC have been developed, to the best of our knowledge, there is still no algorithm that is able to effectively estimate the two crucial parameters in SVC without supervision. In this paper, we propose a novel support vector clustering approach termed ensemble-driven support vector clustering (EDSVC), which for the first time tackles the automatic parameter estimation problem for SVC based on ensemble learning, and is capable of producing robust clustering results in a purely unsupervised manner. Experimental results on multiple real-world datasets demonstrate the effectiveness of our approach. version:2
arxiv-1608-02824 | Camera Pose Estimation from Lines using Plücker Coordinates | http://arxiv.org/abs/1608.02824 | id:1608.02824 author:Bronislav Přibyl, Pavel Zemčík, Martin Čadík category:cs.CV 68T45 I.4.8; I.4.1  published:2016-08-09 summary:Correspondences between 3D lines and their 2D images captured by a camera are often used to determine position and orientation of the camera in space. In this work, we propose a novel algebraic algorithm to estimate the camera pose. We parameterize 3D lines using Pl\"ucker coordinates that allow linear projection of the lines into the image. A line projection matrix is estimated using Linear Least Squares and the camera pose is then extracted from the matrix. An algebraic approach to handle mismatched line correspondences is also included. The proposed algorithm is an order of magnitude faster yet comparably accurate and robust to the state-of-the-art, it does not require initialization, and it yields only one solution. The described method requires at least 9 lines and is particularly suitable for scenarios with 25 and more lines, as also shown in the results. version:1
arxiv-1608-02784 | Canonical Correlation Inference for Mapping Abstract Scenes to Text | http://arxiv.org/abs/1608.02784 | id:1608.02784 author:Helen Jiang, Nikos Papasarantopoulos, Shay B. Cohen category:cs.CL  published:2016-08-09 summary:We describe a technique for structured prediction, based on canonical correlation analysis. Our learning algorithm finds two projections for the input and the output spaces that aim at projecting a given input and its correct output into points close to each other. We demonstrate our technique on a language-vision problem, namely the problem of giving a textual description to an "abstract scene". version:1
arxiv-1608-02778 | Deep Convolution Networks for Compression Artifacts Reduction | http://arxiv.org/abs/1608.02778 | id:1608.02778 author:Ke Yu, Chao Dong, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2016-08-09 summary:Lossy compression introduces complex compression artifacts, particularly blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restore sharpened images that are accompanied with ringing effects. Inspired by the success of deep convolutional networks (DCN) on superresolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. To meet the speed requirement of real-world applications, we further accelerate the proposed baseline model by layer decomposition and joint use of large-stride convolutional and deconvolutional layers. This also leads to a more general CNN framework that has a close relationship with the conventional Multi-Layer Perceptron (MLP). Finally, the modified network achieves a speed up of 7.5 times with almost no performance loss compared to the baseline model. We also demonstrate that a deeper model can be effectively trained with features learned in a shallow network. Following a similar "easy to hard" idea, we systematically investigate three practical transfer settings and show the effectiveness of transfer learning in low-level vision problems. Our method shows superior performance than the state-of-the-art methods both on benchmark datasets and a real-world use case. version:1
arxiv-1608-02755 | Convolutional Oriented Boundaries | http://arxiv.org/abs/1608.02755 | id:1608.02755 author:Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez, Luc Van Gool category:cs.CV  published:2016-08-09 summary:We present Convolutional Oriented Boundaries (COB), which produces multiscale oriented contours and region hierarchies starting from generic image classification Convolutional Neural Networks (CNNs). COB is computationally efficient, because it requires a single CNN forward pass for contour detection and it uses a novel sparse boundary representation for hierarchical segmentation; it gives a significant leap in performance over the state-of-the-art, and it generalizes very well to unseen categories and datasets. Particularly, we show that learning to estimate not only contour strength but also orientation provides more accurate results. We perform extensive experiments on BSDS, PASCAL Context, PASCAL Segmentation, and MS-COCO, showing that COB provides state-of-the-art contours, region hierarchies, and object proposals in all datasets. version:1
arxiv-1608-02732 | On Lower Bounds for Regret in Reinforcement Learning | http://arxiv.org/abs/1608.02732 | id:1608.02732 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG  published:2016-08-09 summary:This is a brief technical note to clarify the state of lower bounds on regret for reinforcement learning. In particular, this paper: - Reproduces a lower bound on regret for reinforcement learning, similar to the result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010). - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett and Tewari 2009) does not hold using the standard techniques without further work. We suggest that this result should instead be considered a conjecture as it has no rigorous proof. - Suggests that the conjectured lower bound given by (Bartlett and Tewari 2009) is incorrect and, in fact, it is possible to improve the scaling of the upper bound to match the weaker lower bounds presented in this paper. We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work. version:1
arxiv-1608-02731 | Posterior Sampling for Reinforcement Learning Without Episodes | http://arxiv.org/abs/1608.02731 | id:1608.02731 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG  published:2016-08-09 summary:This is a brief technical note to clarify some of the issues with applying the application of the algorithm posterior sampling for reinforcement learning (PSRL) in environments without fixed episodes. In particular, this paper aims to: - Review some of results which have been proven for finite horizon MDPs (Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic structure (Gopalan et al 2014). - Review similar results for optimistic algorithms in infinite horizon problems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and Szepesvari 2011), with particular attention to the dynamic episode growth. - Highlight the delicate technical issue which has led to a fault in the proof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We present an explicit counterexample to this style of argument. Therefore, we suggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead considered a conjecture, as it has no rigorous proof. - Present pragmatic approaches to apply PSRL in infinite horizon problems. We conjecture that, under some additional assumptions, it will be possible to obtain bounds $O( \sqrt{T} )$ even without episodic reset. We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work. version:1
arxiv-1608-02728 | OnionNet: Sharing Features in Cascaded Deep Classifiers | http://arxiv.org/abs/1608.02728 | id:1608.02728 author:Martin Simonovsky, Nikos Komodakis category:cs.CV cs.LG cs.NE  published:2016-08-09 summary:The focus of our work is speeding up evaluation of deep neural networks in retrieval scenarios, where conventional architectures may spend too much time on negative examples. We propose to replace a monolithic network with our novel cascade of feature-sharing deep classifiers, called OnionNet, where subsequent stages may add both new layers as well as new feature channels to the previous ones. Importantly, intermediate feature maps are shared among classifiers, preventing them from the necessity of being recomputed. To accomplish this, the model is trained end-to-end in a principled way under a joint loss. We validate our approach in theory and on a synthetic benchmark. As a result demonstrated in three applications (patch matching, object detection, and image retrieval), our cascade can operate significantly faster than both monolithic networks and traditional cascades without sharing at the cost of marginal decrease in precision. version:1
arxiv-1608-02717 | Mean Box Pooling: A Rich Image Representation and Output Embedding for the Visual Madlibs Task | http://arxiv.org/abs/1608.02717 | id:1608.02717 author:Ashkan Mokarian, Mateusz Malinowski, Mario Fritz category:cs.CV cs.AI cs.CL cs.LG  published:2016-08-09 summary:We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs. version:1
arxiv-1608-02715 | A deep language model for software code | http://arxiv.org/abs/1608.02715 | id:1608.02715 author:Hoa Khanh Dam, Truyen Tran, Trang Pham category:cs.SE stat.ML  published:2016-08-09 summary:Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process. version:1
arxiv-1608-02702 | Steerable Principal Components for Space-Frequency Localized Images | http://arxiv.org/abs/1608.02702 | id:1608.02702 author:Boris Landa, Yoel Shkolnisky category:cs.CV cs.NA  published:2016-08-09 summary:This paper describes a fast and accurate method for obtaining steerable principal components from a large dataset of images, assuming the images are well localized in space and frequency. The obtained steerable principal components are optimal for expanding the images in the dataset and all of their rotations. The method relies upon first expanding the images using a series of two-dimensional Prolate Spheroidal Wave Functions (PSWFs), where the expansion coefficients are evaluated using a specially designed numerical integration scheme. Then, the expansion coefficients are used to construct a rotationally-invariant covariance matrix which admits a block-diagonal structure, and the eigen-decomposition of its blocks provides us with the desired steerable principal components. The proposed method is shown to be faster then existing methods, while providing appropriate error bounds which guarantee its accuracy. version:1
arxiv-1608-00869 | SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity | http://arxiv.org/abs/1608.00869 | id:1608.00869 author:Daniela Gerz, Ivan Vulić, Felix Hill, Roi Reichart, Anna Korhonen category:cs.CL  published:2016-08-02 summary:Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning. version:3
arxiv-1608-02693 | Deeply Semantic Inductive Spatio-Temporal Learning | http://arxiv.org/abs/1608.02693 | id:1608.02693 author:Jakob Suchan, Mehul Bhatt, Carl Schultz category:cs.AI cs.CV cs.LG cs.LO  published:2016-08-09 summary:We present an inductive spatio-temporal learning framework rooted in inductive logic programming. With an emphasis on visuo-spatial language, logic, and cognition, the framework supports learning with relational spatio-temporal features identifiable in a range of domains involving the processing and interpretation of dynamic visuo-spatial imagery. We present a prototypical system, and an example application in the domain of computing for visual arts and computational cognitive science. version:1
arxiv-1608-02689 | Multi-task Multi-domain Representation Learning for Sequence Tagging | http://arxiv.org/abs/1608.02689 | id:1608.02689 author:Nanyun Peng, Mark Dredze category:cs.CL cs.LG  published:2016-08-09 summary:Representation learning with deep models have demonstrated success in a range of NLP. In this paper we consider its use in a multi-task multi-domain setting for sequence tagging by proposing a unified framework for learning across tasks and domains. Our model learns robust representations that yield better performance in this setting. We use shared CRFs and domain projections to allow the model to learn domain specific representations that can feed a single task specific CRF. We evaluate our model on two tasks -- Chinese word segmentation and named entity recognition -- and two domains -- news and social media -- and achieve state-of-the-art results for both social media tasks. version:1
arxiv-1608-02680 | A Factorization Approach to Inertial Affine Structure from Motion | http://arxiv.org/abs/1608.02680 | id:1608.02680 author:Roberto Tron category:cs.CV cs.RO  published:2016-08-09 summary:We consider the problem of reconstructing a 3-D scene from a moving camera with high frame rate using the affine projection model. This problem is traditionally known as Affine Structure from Motion (Affine SfM), and can be solved using an elegant low-rank factorization formulation. In this paper, we assume that an accelerometer and gyro are rigidly mounted with the camera, so that synchronized linear acceleration and angular velocity measurements are available together with the image measurements. We extend the standard Affine SfM algorithm to integrate these measurements through the use of image derivatives. version:1
arxiv-1608-02676 | End-to-End Localization and Ranking for Relative Attributes | http://arxiv.org/abs/1608.02676 | id:1608.02676 author:Krishna Kumar Singh, Yong Jae Lee category:cs.CV  published:2016-08-09 summary:We propose an end-to-end deep convolutional network to simultaneously localize and rank relative visual attributes, given only weakly-supervised pairwise image comparisons. Unlike previous methods, our network jointly learns the attribute's features, localization, and ranker. The localization module of our network discovers the most informative image region for the attribute, which is then used by the ranking module to learn a ranking model of the attribute. Our end-to-end framework also significantly speeds up processing and is much faster than previous methods. We show state-of-the-art ranking results on various relative attribute datasets, and our qualitative localization results clearly demonstrate our network's ability to learn meaningful image patches. version:1
arxiv-1608-02554 | Sparse recovery via Orthogonal Least-Squares under presence of Noise | http://arxiv.org/abs/1608.02554 | id:1608.02554 author:Abolfazl Hashemi, Haris Vikalo category:stat.ML cs.IT math.IT  published:2016-08-08 summary:We consider the Orthogonal Least-Squares (OLS) algorithm for the recovery of a $m$-dimensional $k$-sparse signal from a low number of noisy linear measurements. The Exact Recovery Condition (ERC) in bounded noisy scenario is established for OLS under certain condition on nonzero elements of the signal. The new result also improves the existing guarantees for Orthogonal Matching Pursuit (OMP) algorithm. In addition, This framework is employed to provide probabilistic guarantees for the case that the coefficient matrix is drawn at random according to Gaussian or Bernoulli distribution where we exploit some concentration properties. It is shown that under certain conditions, OLS recovers the true support in $k$ iterations with high probability. This in turn demonstrates that ${\cal O}\left(k\log m\right)$ measurements is sufficient for exact recovery of sparse signals via OLS. version:1
arxiv-1608-02549 | Sampling Requirements and Accelerated Schemes for Sparse Linear Regression with Orthogonal Least-Squares | http://arxiv.org/abs/1608.02549 | id:1608.02549 author:Abolfazl Hashemi, Haris Vikalo category:stat.ML cs.IT math.IT  published:2016-08-08 summary:The Orthogonal Least Squares (OLS) algorithm sequentially selects columns of the coefficient matrix to greedily find an approximate sparse solution to an underdetermined system of linear equations. Previous work on the analysis of OLS has been limited; in particular, there exist no guarantees on the performance of OLS for sparse linear regression from random measurements. In this paper, the problem of inferring a sparse vector from random linear combinations of its components using OLS is studied. For the noiseless scenario, it is shown that when the entries of a coefficient matrix are samples from a Gaussian or a Bernoulli distribution, OLS with high probability recovers a $k$-sparse $m$-dimensional sparse vector using ${\cal O}\left(k\log m\right)$ measurements. Similar result is established for the bounded-noise scenario where an additional condition on the smallest nonzero element of the unknown vector is required. Moreover, generalizations that reduce computational complexity of OLS and thus extend its practical feasibility are proposed. The generalized OLS algorithm is empirically shown to outperform broadly used existing algorithms in terms of accuracy, running time, or both. version:1
arxiv-1608-02546 | A Stackelberg Game Perspective on the Conflict Between Machine Learning and Data Obfuscation | http://arxiv.org/abs/1608.02546 | id:1608.02546 author:Jeffrey Pawlick, Quanyan Zhu category:cs.GT cs.CR cs.LG  published:2016-08-08 summary:Data is the new oil; this refrain is repeated extensively in the age of internet tracking, machine learning, and data analytics. As data collection becomes more personal and pervasive, however, public pressure is mounting for privacy protection. In this atmosphere, developers have created applications to add noise to user attributes visible to tracking algorithms. This creates a strategic interaction between trackers and users when incentives to maintain privacy and improve accuracy are misaligned. In this paper, we conceptualize this conflict through an N+1-player, augmented Stackelberg game. First a machine learner declares a privacy protection level, and then users respond by choosing their own perturbation amounts. We use the general frameworks of differential privacy and empirical risk minimization to quantify the utility components due to privacy and accuracy, respectively. In equilibrium, each user perturbs her data independently, which leads to a high net loss in accuracy. To remedy this scenario, we show that the learner improves his utility by proactively perturbing the data himself. While other work in this area has studied privacy markets and mechanism design for truthful reporting of user information, we take a different viewpoint by considering both user and learner perturbation. version:1
arxiv-1608-02519 | Topic Modelling and Event Identification from Twitter Textual Data | http://arxiv.org/abs/1608.02519 | id:1608.02519 author:Marina Sokolova, Kanyi Huang, Stan Matwin, Joshua Ramisch, Vera Sazonova, Renee Black, Chris Orwa, Sidney Ochieng, Nanjira Sambuli category:cs.SI cs.CL  published:2016-08-08 summary:The tremendous growth of social media content on the Internet has inspired the development of the text analytics to understand and solve real-life problems. Leveraging statistical topic modelling helps researchers and practitioners in better comprehension of textual content as well as provides useful information for further analysis. Statistical topic modelling becomes especially important when we work with large volumes of dynamic text, e.g., Facebook or Twitter datasets. In this study, we summarize the message content of four data sets of Twitter messages relating to challenging social events in Kenya. We use Latent Dirichlet Allocation (LDA) topic modelling to analyze the content. Our study uses two evaluation measures, Normalized Mutual Information (NMI) and topic coherence analysis, to select the best LDA models. The obtained LDA results show that the tool can be effectively used to extract discussion topics and summarize them for further manual analysis version:1
arxiv-1608-02485 | Boosting as a kernel-based method | http://arxiv.org/abs/1608.02485 | id:1608.02485 author:Aleksandr Y. Aravkin, Giulio Bottegal, Gianluigi Pillonetto category:stat.ML  published:2016-08-08 summary:Boosting is an important learning technique for classification and regression. In this paper, we show a connection between boosting and kernel based methods, highlighting both theoretical and practical applications. In the context of $\ell_2$ boosting, we start with a weak linear learner defined by a kernel $K$. By applying boosting, we show that the resulting method is equivalent to an estimation procedure hinging on a special {\it boosting kernel} derived from $K$. The boosting kernel depends on $K$ as well as on the regression matrix, noise variance, and hyperparameters. Through this connection, the number of boosting iterations can be modeled as a continuous hyperparameter, and fit along with other parameters using standard techniques. The boosting kernel is then generalized to an entire new class of boosting approaches exploiting general weak learners, including those based on the $\ell_1$, hinge and Vapnik losses. The approach allows fast hyperparameter tuning for a general class of boosting kernels, and has a wide range of applications. We illustrate some of these applications with numerical examples on synthetic and real data. version:1
arxiv-1608-02484 | Interpolated Discretized Embedding of Single Vectors and Vector Pairs for Classification, Metric Learning and Distance Approximation | http://arxiv.org/abs/1608.02484 | id:1608.02484 author:Ofir Pele, Yakir Ben-Aliz category:cs.LG  published:2016-08-08 summary:We propose a new embedding method for a single vector and for a pair of vectors. This embedding method enables: a) efficient classification and regression of functions of single vectors; b) efficient approximation of distance functions; and c) non-Euclidean, semimetric learning. To the best of our knowledge, this is the first work that enables learning any general, non-Euclidean, semimetrics. That is, our method is a universal semimetric learning and approximation method that can approximate any distance function with as high accuracy as needed with or without semimetric constraints. The project homepage including code is at: http://www.ariel.ac.il/sites/ofirpele/ID version:1
arxiv-1607-07819 | Uniform Approximation by Neural Networks Activated by First and Second Order Ridge Splines | http://arxiv.org/abs/1607.07819 | id:1607.07819 author:Jason M. Klusowski, Andrew R. Barron category:stat.ML math.ST stat.TH 62M45  41A15  published:2016-07-26 summary:We establish sup-norm error bounds for functions that are approximated by linear combinations of first and second order ridge splines and show that these bounds are near-optimal. version:2
arxiv-1608-03532 | QPass: a Merit-based Evaluation of Soccer Passes | http://arxiv.org/abs/1608.03532 | id:1608.03532 author:Laszlo Gyarmati, Rade Stanojevic category:cs.AI stat.AP stat.ML H.4  published:2016-08-08 summary:Quantitative analysis of soccer players' passing ability focuses on descriptive statistics without considering the players' real contribution to the passing and ball possession strategy of their team. Which player is able to help the build-up of an attack, or to maintain the possession of the ball? We introduce a novel methodology called QPass to answer questions like these quantitatively. Based on the analysis of an entire season, we rank the players based on the intrinsic value of their passes using QPass. We derive an album of pass trajectories for different gaming styles. Our methodology reveals a quite counterintuitive paradigm: losing the ball possession could lead to better chances to win a game. version:1
arxiv-1608-02388 | Database of handwritten Arabic mathematical formulas images | http://arxiv.org/abs/1608.02388 | id:1608.02388 author:Ibtissem Hadj Ali, Mohammed Ali Mahjoub category:cs.CV  published:2016-08-08 summary:Although publicly available, ground-truthed database have proven useful for training, evaluating, and comparing recognition systems in many domains, the availability of such database for handwritten Arabic mathematical formula recognition in particular, is currently quite poor. In this paper, we present a new public database that contains mathematical expressions available in their off-line handwritten form. Here, we describe the different steps that allowed us to acquire this database, from the creation of the mathematical expression corpora to the transcription of the collected data. Currently, the dataset contains 4 238 off-line handwritten mathematical expressions written by 66 writers and 20 300 handwritten isolated symbol images. The ground truth is also provided for the handwritten expressions as XML files with the number of symbols, and the MATHML structure. version:1
arxiv-1608-02385 | Comparative study and enhancement of Camera Tampering Detection algorithms | http://arxiv.org/abs/1608.02385 | id:1608.02385 author:Mabrouka Hagui, Mohamed Ali Mahjoub, Ahmed Boukhris category:cs.CV  published:2016-08-08 summary:Recently the use of video surveillance systems is widely increasing. Different places are equipped by camera surveillances such as hospitals, schools, airports, museums and military places in order to ensure the safety and security of the persons and their property. Therefore it becomes significant to guarantee the proper working of these systems. Intelligent video surveillance systems equipped by sophisticated digital camera can analyze video information s and automatically detect doubtful actions. The camera tampering detection algorithms may indicate that accidental or suspicious activities have occurred and that causes the abnormality works of the video surveillance. Camera Tampering Detection uses several techniques based on image processing and computer vision. In this paper, comparative study of performance of three algorithms that can detect abnormal disturbance for video surveillance is presented. version:1
arxiv-1608-02373 | A combined Approach Based on Fuzzy Classification and Contextual Region Growing to Image Segmentation | http://arxiv.org/abs/1608.02373 | id:1608.02373 author:Mahaman Sani Chaibou, Karim Kalti, Bassel Soulaiman, Mohamed Ali Mahjoub category:cs.CV  published:2016-08-08 summary:We present in this paper an image segmentation approach that combines a fuzzy semantic region classification and a context based region-growing. Input image is first over-segmented. Then, prior domain knowledge is used to perform a fuzzy classification of these regions to provide a fuzzy semantic labeling. This allows the proposed approach to operate at high level instead of using low-level features and consequently to remedy to the problem of the semantic gap. Each over-segmented region is represented by a vector giving its corresponding membership degrees to the different thematic labels and the whole image is therefore represented by a Regions Partition Matrix. The segmentation is achieved on this matrix instead of the image pixels through two main phases: focusing and propagation. The focusing aims at selecting seeds regions from which information propagation will be performed. Thepropagation phase allows to spread toward others regions and using fuzzy contextual information the needed knowledge ensuring the semantic segmentation. An application of the proposed approach on mammograms shows promising results version:1
arxiv-1608-02367 | Learning Joint Representations of Videos and Sentences with Web Image Search | http://arxiv.org/abs/1608.02367 | id:1608.02367 author:Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, Naokazu Yokoya category:cs.CV  published:2016-08-08 summary:Our objective is video retrieval based on natural language queries. In addition, we consider the analogous problem of retrieving sentences or generating descriptions given an input video. Recent work has addressed the problem by embedding visual and textual inputs into a common space where semantic similarities correlate to distances. We also adopt the embedding approach, and make the following contributions: First, we utilize web image search in sentence embedding process to disambiguate fine-grained visual concepts. Second, we propose embedding models for sentence, image, and video inputs whose parameters are learned simultaneously. Finally, we show how the proposed model can be applied to description generation. Overall, we observe a clear improvement over the state-of-the-art methods in the video and sentence retrieval tasks. In description generation, the performance level is comparable to the current state-of-the-art, although our embeddings were trained for the retrieval tasks. version:1
arxiv-1608-02341 | Towards Representation Learning with Tractable Probabilistic Models | http://arxiv.org/abs/1608.02341 | id:1608.02341 author:Antonio Vergari, Nicola Di Mauro, Floriana Esposito category:cs.LG cs.AI stat.ML  published:2016-08-08 summary:Probabilistic models learned as density estimators can be exploited in representation learning beside being toolboxes used to answer inference queries only. However, how to extract useful representations highly depends on the particular model involved. We argue that tractable inference, i.e. inference that can be computed in polynomial time, can enable general schemes to extract features from black box models. We plan to investigate how Tractable Probabilistic Models (TPMs) can be exploited to generate embeddings by random query evaluations. We devise two experimental designs to assess and compare different TPMs as feature extractors in an unsupervised representation learning framework. We show some experimental results on standard image datasets by applying such a method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings. version:1
arxiv-1608-02318 | Discriminatively Trained Latent Ordinal Model for Video Classification | http://arxiv.org/abs/1608.02318 | id:1608.02318 author:Karan Sikka, Gaurav Sharma category:cs.CV  published:2016-08-08 summary:We study the problem of video classification for facial analysis and human action recognition. We propose a novel weakly supervised learning method that models the video as a sequence of automatically mined, discriminative sub-events (eg. onset and offset phase for "smile", running and jumping for "highjump"). The proposed model is inspired by the recent works on Multiple Instance Learning and latent SVM/HCRF -- it extends such frameworks to model the ordinal aspect in the videos, approximately. We obtain consistent improvements over relevant competitive baselines on four challenging and publicly available video based facial analysis datasets for prediction of expression, clinical pain and intent in dyadic conversations and on three challenging human action datasets. We also validate the method with qualitative results and show that they largely support the intuitions behind the method. version:1
arxiv-1608-02315 | Blankets Joint Posterior score for learning irregular Markov network structures | http://arxiv.org/abs/1608.02315 | id:1608.02315 author:Federico Schlüter, Yanela Strappa, Facundo Bromberg, Diego H. Milone category:cs.AI stat.ML 68Q32  published:2016-08-08 summary:Markov networks are extensively used to model complex sequential, spatial, and relational interactions in a wide range of fields. By learning the structure of independences of a domain, more accurate joint probability distributions can be obtained for inference tasks or, more directly, for interpreting the most significant relations among the variables. However, the performance of current available methods for learning the structure is heavily dependent on the choice of two factors: the structure representation, and the approach for learning such representation. This work follows the probabilistic maximum-a-posteriori approach for learning undirected graph structures, which has gained interest recently. Thus, the Blankets Joint Posterior score is designed for computing the posterior probability of structures given data. In particular, the score proposed can improve the learning process when the solution structure is irregular (that is, when there exists an imbalance in the number of edges over the nodes), which is a property present in many real-world networks. The approximation proposed computes the joint posterior distribution from the collection of Markov blankets of the structure. Essentially, a series of conditional distributions are calculated by using, information about other Markov blankets in the network as evidence. Our experimental results demonstrate that the proposed score has better sample complexity for learning irregular structures, when compared to state-of-the-art scores. By considering optimization with greedy hill-climbing search, we prove for several study cases that our score identifies structures with fewer errors than competitors. version:1
arxiv-1608-02307 | SANTIAGO: Spine Association for Neuron Topology Improvement and Graph Optimization | http://arxiv.org/abs/1608.02307 | id:1608.02307 author:William Gray Roncal, Colin Lea, Akira Baruah, Gregory D. Hager category:cs.CV q-bio.QM  published:2016-08-08 summary:Developing automated and semi-automated solutions for reconstructing wiring diagrams of the brain from electron micrographs is important for advancing the field of connectomics. While the ultimate goal is to generate a graph of neuron connectivity, most prior automated methods have focused on volume segmentation rather than explicit graph estimation. In these approaches, one of the key, commonly occurring error modes is dendritic shaft-spine fragmentation. We posit that directly addressing this problem of connection identification may provide critical insight into estimating more accurate brain graphs. To this end, we develop a network-centric approach motivated by biological priors image grammars. We build a computer vision pipeline to reconnect fragmented spines to their parent dendrites using both fully-automated and semi-automated approaches. Our experiments show we can learn valid connections despite uncertain segmentation paths. We curate the first known reference dataset for analyzing the performance of various spine-shaft algorithms and demonstrate promising results that recover many previously lost connections. Our automated approach improves the local subgraph score by more than four times and the full graph score by 60 percent. These data, results, and evaluation tools are all available to the broader scientific community. This reframing of the connectomics problem illustrates a semantic, biologically inspired solution to remedy a major problem with neuron tracking. version:1
arxiv-1608-02301 | Uncovering Voice Misuse Using Symbolic Mismatch | http://arxiv.org/abs/1608.02301 | id:1608.02301 author:Marzyeh Ghassemi, Zeeshan Syed, Daryush D. Mehta, Jarrad H. Van Stan, Robert E. Hillman, John V. Guttag category:cs.LG  published:2016-08-08 summary:Voice disorders affect an estimated 14 million working-aged Americans, and many more worldwide. We present the first large scale study of vocal misuse based on long-term ambulatory data collected by an accelerometer placed on the neck. We investigate an unsupervised data mining approach to uncovering latent information about voice misuse. We segment signals from over 253 days of data from 22 subjects into over a hundred million single glottal pulses (closures of the vocal folds), cluster segments into symbols, and use symbolic mismatch to uncover differences between patients and matched controls, and between patients pre- and post-treatment. Our results show significant behavioral differences between patients and controls, as well as between some pre- and post-treatment patients. Our proposed approach provides an objective basis for helping diagnose behavioral voice disorders, and is a first step towards a more data-driven understanding of the impact of voice therapy. version:1
arxiv-1608-02292 | Online Adaptation of Deep Architectures with Reinforcement Learning | http://arxiv.org/abs/1608.02292 | id:1608.02292 author:Thushan Ganegedara, Lionel Ott, Fabio Ramos category:cs.LG cs.NE  published:2016-08-08 summary:Online learning has become crucial to many problems in machine learning. As more data is collected sequentially, quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning. However, adaptation to changes in the data distribution (also known as covariate shift) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data. In this paper, we propose an online stacked Denoising Autoencoder whose structure is adapted through reinforcement learning. Our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence. Different actions, such as Pool, Increment and Merge are available to modify the structure of the network. As we observe through a series of experiments, our approach is more responsive, robust, and principled than its counterparts for non-stationary as well as stationary data distributions. Experimental results indicate that our algorithm performs better at preserving gained prior knowledge and responding to changes in the data distribution. version:1
arxiv-1608-02289 | Detecting Sarcasm in Multimodal Social Platforms | http://arxiv.org/abs/1608.02289 | id:1608.02289 author:Rossano Schifanella, Paloma de Juan, Joel Tetreault, Liangliang Cao category:cs.CV cs.CL cs.MM  published:2016-08-08 summary:Sarcasm is a peculiar form of sentiment expression, where the surface sentiment differs from the implied sentiment. The detection of sarcasm in social media platforms has been applied in the past mainly to textual utterances where lexical indicators (such as interjections and intensifiers), linguistic markers, and contextual information (such as user profiles, or past conversations) were used to detect the sarcastic tone. However, modern social media platforms allow to create multimodal messages where audiovisual content is integrated with the text, making the analysis of a mode in isolation partial. In our work, we first study the relationship between the textual and visual aspects in multimodal posts from three major social media platforms, i.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to quantify the extent to which images are perceived as necessary by human annotators. Moreover, we propose two different computational frameworks to detect sarcasm that integrate the textual and visual modalities. The first approach exploits visual semantics trained on an external dataset, and concatenates the semantics features with state-of-the-art textual features. The second method adapts a visual neural network initialized with parameters trained on ImageNet to multimodal sarcastic posts. Results show the positive effect of combining modalities for the detection of sarcasm across platforms and methods. version:1
arxiv-1608-02280 | Statistical Guarantees for Estimating the Centers of a Two-component Gaussian Mixture by EM | http://arxiv.org/abs/1608.02280 | id:1608.02280 author:Jason M. Klusowski, W. D. Brinda category:stat.ML 62F10  62F15  68W40  published:2016-08-07 summary:Recently, a general method for analyzing the statistical accuracy of the EM algorithm has been developed and applied to some simple latent variable models [Balakrishnan et al. 2016]. In that method, the basin of attraction for valid initialization is required to be a ball around the truth. Using Stein's Lemma, we extend these results in the case of estimating the centers of a two-component Gaussian mixture in $d$ dimensions. In particular, we significantly expand the basin of attraction to be the intersection of a half space and a ball around the origin. If the signal-to-noise ratio is at least a constant multiple of $ \sqrt{d\log d} $, we show that a random initialization strategy is feasible. version:1
arxiv-1608-02272 | Incorporation of Speech Duration Information in Score Fusion of Speaker Recognition Systems | http://arxiv.org/abs/1608.02272 | id:1608.02272 author:Ali Khodabakhsh, Seyyed Saeed Sarfjoo, Umut Uludag, Osman Soyyigit, Cenk Demiroglu category:cs.SD cs.CL  published:2016-08-07 summary:In recent years identity-vector (i-vector) based speaker verification (SV) systems have become very successful. Nevertheless, environmental noise and speech duration variability still have a significant effect on degrading the performance of these systems. In many real-life applications, duration of recordings are very short; as a result, extracted i-vectors cannot reliably represent the attributes of the speaker. Here, we investigate the effect of speech duration on the performance of three state-of-the-art speaker recognition systems. In addition, using a variety of available score fusion methods, we investigate the effect of score fusion for those speaker verification techniques to benefit from the performance difference of different methods under different enrollment and test speech duration conditions. This technique performed significantly better than the baseline score fusion methods. version:1
arxiv-1608-02255 | Spontaneous Facial Micro-Expression Recognition using Discriminative Spatiotemporal Local Binary Pattern with an Improved Integral Projection | http://arxiv.org/abs/1608.02255 | id:1608.02255 author:Xiaohua Huang, Sujing Wang, Xin Liu, Guoying Zhao, Xiaoyi Feng, Matti Pietikainen category:cs.CV  published:2016-08-07 summary:Recently, there are increasing interests in inferring mirco-expression from facial image sequences. Due to subtle facial movement of micro-expressions, feature extraction has become an important and critical issue for spontaneous facial micro-expression recognition. Recent works usually used spatiotemporal local binary pattern for micro-expression analysis. However, the commonly used spatiotemporal local binary pattern considers dynamic texture information to represent face images while misses the shape attribute of face images. On the other hand, their works extracted the spatiotemporal features from the global face regions, which ignore the discriminative information between two micro-expression classes. The above-mentioned problems seriously limit the application of spatiotemporal local binary pattern on micro-expression recognition. In this paper, we propose a discriminative spatiotemporal local binary pattern based on an improved integral projection to resolve the problems of spatiotemporal local binary pattern for micro-expression recognition. Firstly, we develop an improved integral projection for preserving the shape attribute of micro-expressions. Furthermore, an improved integral projection is incorporated with local binary pattern operators across spatial and temporal domains. Specifically, we extract the novel spatiotemporal features incorporating shape attributes into spatiotemporal texture features. For increasing the discrimination of micro-expressions, we propose a new feature selection based on Laplacian method to extract the discriminative information for facial micro-expression recognition. Intensive experiments are conducted on three availably published micro-expression databases. We compare our method with the state-of-the-art algorithms. Experimental results demonstrate that our proposed method achieves promising performance for micro-expression recognition. version:1
arxiv-1608-02254 | Reconciling Lambek's restriction, cut-elimination, and substitution in the presence of exponential modalities | http://arxiv.org/abs/1608.02254 | id:1608.02254 author:Max Kanovich, Stepan Kuznetsov, Andre Scedrov category:math.LO cs.CL 03B47  published:2016-08-07 summary:The Lambek calculus can be considered as a version of non-commutative intuitionistic linear logic. One of the interesting features of the Lambek calculus is the so-called "Lambek's restriction," that is, the antecedent of any provable sequent should be non-empty. In this paper we discuss ways of extending the Lambek calculus with the linear logic exponential modality while keeping Lambek's restriction. Interestingly enough, we show that for any system equipped with a reasonable exponential modality the following holds: if the system enjoys cut elimination and substitution to the full extent, then the system necessarily violates Lambek's restriction. Nevertheless, we show that two of the three conditions can be implemented. Namely, we design a system with Lambek's restriction and cut elimination and another system with Lambek's restriction and substitution. For both calculi we prove that they are undecidable, even if we take only one of the two divisions provided by the Lambek calculus. The system with cut elimination and substitution and without Lambek's restriction is folklore and known to be undecidable. version:1
arxiv-1608-02236 | Bootstrapping Face Detection with Hard Negative Examples | http://arxiv.org/abs/1608.02236 | id:1608.02236 author:Shaohua Wan, Zhijun Chen, Tao Zhang, Bo Zhang, Kong-kat Wong category:cs.CV  published:2016-08-07 summary:Recently significant performance improvement in face detection was made possible by deeply trained convolutional networks. In this report, a novel approach for training state-of-the-art face detector is described. The key is to exploit the idea of hard negative mining and iteratively update the Faster R-CNN based face detector with the hard negatives harvested from a large set of background examples. We demonstrate that our face detector outperforms state-of-the-art detectors on the FDDB dataset, which is the de facto standard for evaluating face detection algorithms. version:1
arxiv-1608-02229 | Towards the Self-constructive Brain: emergence of adaptive behavior | http://arxiv.org/abs/1608.02229 | id:1608.02229 author:Fernando Corbacho category:cs.NE cs.AI q-bio.NC  published:2016-08-07 summary:Adaptive behavior is mainly the result of adaptive brains. We go a step beyond and claim that the brain does not only adapt to its surrounding reality but rather, it builds itself up to constructs its own reality. That is, rather than just trying to passively understand its environment, the brain is the architect of its own reality in an active process where its internal models of the external world frame how its new interactions with the environment are assimilated. These internal models represent relevant predictive patterns of interaction all over the different brain structures: perceptual, sensorimotor, motor, etc. The emergence of adaptive behavior arises from this self-constructive nature of the brain, based on the following principles of organization: self-experimental, self- growing, and self-repairing. Self-experimental, since to ensure survival, the self-constructive brain (SCB) is an active machine capable of performing experiments of its own interactions with the environment by mental simulation. Self-growing, since it dynamically and incrementally constructs internal structures in order to build a model of the world as it gathers statistics from its interactions with the environment. Self-repairing, since to survive the SCB must also be robust and capable of finding ways to repair parts of previously working structures and hence re-construct a previous relevant pattern of activity. version:1
arxiv-1608-02214 | Robsut Wrod Reocginiton via semi-Character Recurrent Neural Network | http://arxiv.org/abs/1608.02214 | id:1608.02214 author:Keisuke Sakaguchi, Kevin Duh, Matt Post, Benjamin Van Durme category:cs.CL  published:2016-08-07 summary:The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated a robust word processing mechanism in humans, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model. version:1
arxiv-1608-02209 | Bayesian Learning of Dynamic Multilayer Networks | http://arxiv.org/abs/1608.02209 | id:1608.02209 author:Daniele Durante, Nabanita Mukherjee, Rebecca C. Steorts category:stat.ML stat.ME  published:2016-08-07 summary:A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents new challenges. In this paper, we focus on time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction. version:1
arxiv-1608-02201 | Residual CNDS | http://arxiv.org/abs/1608.02201 | id:1608.02201 author:Hussein A. Al-Barazanchi, Hussam Qassim, Abhishek Verma category:cs.CV  published:2016-08-07 summary:Convolutional Neural networks nowadays are of tremendous importance for any image classification system. One of the most investigated methods to increase the accuracy of CNN is by increasing the depth of CNN. Increasing the depth by stacking more layers also increases the difficulty of training besides making it computationally expensive. Some research found that adding auxiliary forks after intermediate layers increases the accuracy. Specifying which intermediate layer shoud have the fork just addressed recently. Where a simple rule were used to detect the position of intermediate layers that needs the auxiliary supervision fork. This technique known as convolutional neural networks with deep supervision (CNDS). This technique enhanced the accuracy of classification over the straight forward CNN used on the MIT places dataset and ImageNet. In the other side, Residual Learning is another technique emerged recently to ease the training of very deep CNN. Residual Learning framwork changed the learning of layers from unreferenced functions to learning residual function with regard to the layer's input. Residual Learning achieved state of arts results on ImageNet 2015 and COCO competitions. In this paper, we study the effect of adding residual connections to CNDS network. Our experiments results show increasing of accuracy over using CNDS only. version:1
arxiv-1608-02198 | A General Characterization of the Statistical Query Complexity | http://arxiv.org/abs/1608.02198 | id:1608.02198 author:Vitaly Feldman category:cs.LG cs.CC stat.ML  published:2016-08-07 summary:Statistical query (SQ) algorithms are algorithms that have access to an {\em SQ oracle} for the input distribution $D$. Given a query function $\phi:X \rightarrow [-1,1]$, the oracle returns an estimate of ${\bf E}_{ x\sim D}[\phi(x)]$ within some tolerance $\tau_\phi$. Such algorithms capture a broad spectrum of algorithmic approaches used in theory and practice. In this work we give a sharp characterization of the complexity of solving general problems over distributions using SQ algorithms. Our characterization is based on a relatively simple notion of statistical dimension. Such characterizations were investigated over the past 20 years in learning theory but prior characterizations are restricted to distribution-specific PAC learning. In contrast, our characterization applies to general search and decision problems including those where the input distribution can be any distribution over an exponentially large domain. Our characterization is also the first to precisely characterize the necessary tolerance of queries. This is crucial in the range of more recent applications of SQ algorithms. As an application of our techniques, we answer the following question that was open: is the SQ complexity of distribution-independent learning upper-bounded by the maximum over all distributions of the SQ complexity of distribution-specific PAC learning? Our results also demonstrate a separation between efficient learning from examples in the presence of random noise and SQ learning. This improves on the separation of Blum et al. (2003) and fully resolves an open problem of Kearns (1998). Finally, we demonstrate implications of our characterization to algorithms that are subject to memory, communication and local differential privacy constraints. version:1
arxiv-1608-02195 | Automating Political Bias Prediction | http://arxiv.org/abs/1608.02195 | id:1608.02195 author:Felix Biessmann category:cs.SI cs.CL  published:2016-08-07 summary:Every day media generate large amounts of text. An unbiased view on media reports requires an understanding of the political bias of media content. Assistive technology for estimating the political bias of texts can be helpful in this context. This study proposes a simple statistical learning approach to predict political bias from text. Standard text features extracted from speeches and manifestos of political parties are used to predict political bias in terms of political party affiliation and in terms of political views. Results indicate that political bias can be predicted with above chance accuracy. Mistakes of the model can be interpreted with respect to changes of policies of political actors. Two approaches are presented to make the results more interpretable: a) discriminative text features are related to the political orientation of a party and b) sentiment features of texts are correlated with a measure of political power. Political power appears to be strongly correlated with positive sentiment of a text. To highlight some potential use cases a web application shows how the model can be used for texts for which the political bias is not clear such as news articles. version:1
arxiv-1608-02192 | Playing for Data: Ground Truth from Computer Games | http://arxiv.org/abs/1608.02192 | id:1608.02192 author:Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun category:cs.CV I.4.8  published:2016-08-07 summary:Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set. version:1
arxiv-1608-02165 | ShapeFit and ShapeKick for Robust, Scalable Structure from Motion | http://arxiv.org/abs/1608.02165 | id:1608.02165 author:Thomas Goldstein, Paul Hand, Choongbum Lee, Vladislav Voroninski, Stefano Soatto category:cs.CV cs.AI math.NA math.OC 68T45 I.2.10; I.4  published:2016-08-07 summary:We introduce a new method for location recovery from pair-wise directions that leverages an efficient convex program that comes with exact recovery guarantees, even in the presence of adversarial outliers. When pairwise directions represent scaled relative positions between pairs of views (estimated for instance with epipolar geometry) our method can be used for location recovery, that is the determination of relative pose up to a single unknown scale. For this task, our method yields performance comparable to the state-of-the-art with an order of magnitude speed-up. Our proposed numerical framework is flexible in that it accommodates other approaches to location recovery and can be used to speed up other methods. These properties are demonstrated by extensively testing against state-of-the-art methods for location recovery on 13 large, irregular collections of images of real scenes in addition to simulated data with ground truth. version:1
arxiv-1608-02164 | Adapting Deep Network Features to Capture Psychological Representations | http://arxiv.org/abs/1608.02164 | id:1608.02164 author:Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths category:cs.CV cs.AI cs.NE  published:2016-08-06 summary:Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments. version:1
arxiv-1608-02158 | Deep Survival Analysis | http://arxiv.org/abs/1608.02158 | id:1608.02158 author:Rajesh Ranganath, Adler Perotte, Noémie Elhadad, David Blei category:stat.ML cs.AI stat.ME  published:2016-08-06 summary:The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk. version:1
arxiv-1608-01137 | Cascaded Continuous Regression for Real-time Incremental Face Tracking | http://arxiv.org/abs/1608.01137 | id:1608.01137 author:Enrique Sánchez-Lozano, Brais Martinez, Georgios Tzimiropoulos, Michel Valstar category:cs.CV  published:2016-08-03 summary:This paper introduces a novel real-time algorithm for facial landmark tracking. Compared to detection, tracking has both additional challenges and opportunities. Arguably the most important aspect in this domain is updating a tracker's models as tracking progresses, also known as incremental (face) tracking. While this should result in more accurate localisation, how to do this online and in real time without causing a tracker to drift is still an important open research question. We address this question in the cascaded regression framework, the state-of-the-art approach for facial landmark localisation. Because incremental learning for cascaded regression is costly, we propose a much more efficient yet equally accurate alternative using continuous regression. More specifically, we first propose cascaded continuous regression (CCR) and show its accuracy is equivalent to the Supervised Descent Method. We then derive the incremental learning updates for CCR (iCCR) and show that it is an order of magnitude faster than standard incremental learning for cascaded regression, bringing the time required for the update from seconds down to a fraction of a second, thus enabling real-time tracking. Finally, we evaluate iCCR and show the importance of incremental learning in achieving state-of-the-art performance. Code for our iCCR is available from http://www.cs.nott.ac.uk/~psxes1 version:2
arxiv-1608-01282 | A Multivariate Hawkes Process with Gaps in Observations | http://arxiv.org/abs/1608.01282 | id:1608.01282 author:Triet M Le category:stat.ML cs.IT math.DS math.IT math.NA  published:2016-08-03 summary:Given a collection of entities (or nodes) in a network and our intermittent observations of activities from each entity, an important problem is to learn the hidden edges depicting directional relationships among these entities. Here, we study causal relationships (excitations) that are realized by a multivariate Hawkes process. The multivariate Hawkes process (MHP) and its variations (spatial-temporal point processes) have been used to study contagion in earthquakes, crimes, neural spiking activities, the stock and foreign exchange markets, etc. In this paper, we consider the case with intermittent observations (and hence gaps.) We propose a variational problem for detecting sparsely hidden relationships with a multivariate Hawkes process that takes into account the gaps from each entity (MHPG). We bypass the problem of dealing with a large amount of missing events by introducing a small number of unknown boundary conditions. In the case where our observations are sparse (e.g. from 10% to 30%), we show through numerical simulations that robust recovery with MHPG is still possible even if the lengths of the observed intervals are small but they are chosen accordingly. In these cases, the proposed MHPG outperforms the classical MHP in parameter estimations. The numerical results also show that the knowledge of gaps is very crucial in discovering the underlying patterns and hidden relationships. version:2
arxiv-1608-02153 | OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus | http://arxiv.org/abs/1608.02153 | id:1608.02153 author:U. Springmann, A. Lüdeling category:cs.CL cs.DL  published:2016-08-06 summary:This article describes the results of a case study to apply Optical Character Recognition (OCR) to scanned images of books printed between 1487 and 1870 by training the OCR engine OCRopus (Breuel et al. 2013) on the RIDGES herbal text corpus (Odebrecht et al., submitted). The resulting machine-readable text has character accuracies (percentage of correctly recognized characters) from 94% to more than 99% for even the earliest printed books, which were thought to be inaccessible by OCR methods until recently. Training specific OCR models was possible because the necessary "ground truth" has been available as error-corrected diplomatic transcriptions. The OCR results have been evaluated for accuracy against the ground truth of unseen test sets. Furthermore, mixed OCR models trained on a subset of books have been tested for their predictive power on page images of other books in the corpus, mostly yielding character accuracies well above 90%. It therefore seems possible to construct generalized models covering a range of fonts that can be applied to a wide variety of historical printings. A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies. Using this method, diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription. The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means, which is a prerequisite for the mass conversion of scanned books. version:1
arxiv-1608-02146 | Leveraging Union of Subspace Structure to Improve Constrained Clustering | http://arxiv.org/abs/1608.02146 | id:1608.02146 author:John Lipor, Laura Balzano category:cs.LG cs.CV  published:2016-08-06 summary:Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present an algorithm for active query selection that allows us to leverage the union of subspace structure assumed in subspace clustering. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. This procedure can be used after any subspace clustering algorithm that outputs an affinity matrix and is capable of driving the clustering error down more quickly than other state-of-the-art active query algorithms on datasets with subspace structure. We demonstrate the effectiveness of our algorithm on several benchmark datasets, and with a modest number of queries we see significant gains in clustering performance. version:1
arxiv-1608-02128 | Spoofing 2D Face Detection: Machines See People Who Aren't There | http://arxiv.org/abs/1608.02128 | id:1608.02128 author:Michael McCoyd, David Wagner category:cs.CR cs.CV cs.LG  published:2016-08-06 summary:Machine learning is increasingly used to make sense of the physical world yet may suffer from adversarial manipulation. We examine the Viola-Jones 2D face detection algorithm to study whether images can be created that humans do not notice as faces yet the algorithm detects as faces. We show that it is possible to construct images that Viola-Jones recognizes as containing faces yet no human would consider a face. Moreover, we show that it is possible to construct images that fool facial detection even when they are printed and then photographed. version:1
arxiv-1608-02126 | How Much Did it Rain? Predicting Real Rainfall Totals Based on Radar Data | http://arxiv.org/abs/1608.02126 | id:1608.02126 author:Adam Lesnikowski category:cs.LG  published:2016-08-06 summary:We applied a variety of parametric and non-parametric machine learning models to predict the probability distribution of rainfall based on 1M training examples over a single year across several U.S. states. Our top performing model based on a squared loss objective was a cross-validated parametric k-nearest-neighbor predictor that took about six days to compute, and was competitive in a world-wide competition. version:1
arxiv-1608-02117 | HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment | http://arxiv.org/abs/1608.02117 | id:1608.02117 author:Ivan Vulić, Daniela Gerz, Douwe Kiela, Felix Hill, Anna Korhonen category:cs.CL  published:2016-08-06 summary:We introduce HyperLex - a dataset and evaluation resource that quantifies the extent of of the semantic category membership and lexical entailment (LE) relation between 2,616 concept pairs. Cognitive psychology research has established that category/class membership, and hence LE, is computed in human semantic memory as a gradual rather than binary relation. Nevertheless, most NLP research, and existing large-scale invetories of concept category membership (WordNet, DBPedia, etc.) treat category membership and LE as binary. To address this, we asked hundreds of native English speakers to indicate strength of category membership between a diverse range of concept pairs on a crowdsourcing platform. Our results confirm that category membership and LE are indeed more gradual than binary. We then compare these human judgements with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems. version:1
arxiv-1608-03544 | Online Context-Dependent Clustering in Recommendations based on Exploration-Exploitation Algorithms | http://arxiv.org/abs/1608.03544 | id:1608.03544 author:Shuai Li, Claudio Gentile, Alexandros Karatzoglou, Giovanni Zappella category:cs.LG cs.AI cs.IR stat.ML  published:2016-08-06 summary:We investigate two context-dependent clustering techniques for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithms dynamically group users based on the items under consideration and, possibly, group items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on extensive real-world datasets, showing scalability and increased prediction performance over state-of-the-art methods for clustering bandits. For one of the two algorithms we also give a regret analysis within a standard linear stochastic noise setting. version:1
arxiv-1608-02888 | Effective Data Mining Technique for Classification Cancers via Mutations in Gene using Neural Network | http://arxiv.org/abs/1608.02888 | id:1608.02888 author:Ayad Ghany Ismaeel, Dina Yousif Mikhail category:cs.LG  published:2016-08-06 summary:The prediction plays the important role in detecting efficient protection and therapy of cancer. The prediction of mutations in gene needs a diagnostic and classification, which is based on the whole database (big dataset), to reach sufficient accuracy results. Since the tumor suppressor P53 is approximately about fifty percentage of all human tumors because mutations that occur in the TP53 gene into the cells. So, this paper is applied on tumor p53, where the problem is there are several primitive databases (excel database) contain datasets of TP53 gene with its tumor protein p53, these databases are rich datasets that cover all mutations and cause diseases (cancers). But these Data Bases cannot reach to predict and diagnosis cancers, i.e. the big datasets have not efficient Data Mining method, which can predict, diagnosis the mutation, and classify the cancer of patient. The goal of this paper to reach a Data Mining technique, that employs neural network, which bases on the big datasets. Also, offers friendly predictions, flexible, and effective classified cancers, in order to overcome the previous techniques drawbacks. This proposed technique is done by using two approaches, first, bioinformatics techniques by using BLAST, CLUSTALW, etc, in order to know if there are malignant mutations or not. The second, data mining by using neural network; it is selected (12) out of (53) TP53 gene database fields. To clarify, one of these 12 fields (gene location field) did not exists in TP53 gene database; therefore, it is added to the database of TP53 gene in training and testing back propagation algorithm, in order to classify specifically the types of cancers. Feed Forward Back Propagation supports this Data Mining method with data training rate (1) and Mean Square Error (MSE) (0.00000000000001). This effective technique allows in a quick, accurate and easy way to classify the type of cancer. version:1
arxiv-1608-02097 | Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken Language Understanding | http://arxiv.org/abs/1608.02097 | id:1608.02097 author:Su Zhu, Kai Yu category:cs.CL  published:2016-08-06 summary:This paper investigates the framework of encoder-decoder with attention for sequence labelling based Spoken Language Understanding. We introduce BLSTM-LSTM as the encoder-decoder model to fully utilize the power of deep learning. In the sequence labelling task, the input and output sequences are aligned word by word, while the attention mechanism can't provide the exact alignment. To address the limitations of attention mechanism in the sequence labelling task, we propose a novel focus mechanism. Experiments on the standard ATIS dataset showed that BLSTM-LSTM with focus mechanism defined the new state-of-the-art by outperforming standard BLSTM and attention based encoder-decoder. Further experiments also showed that the proposed model is more robust to speech recognition errors. version:1
arxiv-1608-02094 | Desiderata for Vector-Space Word Representations | http://arxiv.org/abs/1608.02094 | id:1608.02094 author:Leon Derczynski category:cs.CL  published:2016-08-06 summary:A plethora of vector-space representations for words is currently available, which is growing. These consist of fixed-length vectors containing real values, which represent a word. The result is a representation upon which the power of many conventional information processing and data mining techniques can be brought to bear, as long as the representations are designed with some forethought and fit certain constraints. This paper details desiderata for the design of vector space representations of words. version:1
arxiv-1608-02076 | Bi-directional Attention with Agreement for Dependency Parsing | http://arxiv.org/abs/1608.02076 | id:1608.02076 author:Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao, Li Deng category:cs.CL cs.AI cs.LG  published:2016-08-06 summary:We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 7 languages. version:1
arxiv-1608-02071 | Transferring Knowledge from Text to Predict Disease Onset | http://arxiv.org/abs/1608.02071 | id:1608.02071 author:Yun Liu, Kun-Ta Chuang, Fu-Wen Liang, Huey-Jen Su, Collin M. Stultz, John V. Guttag category:cs.LG cs.CL  published:2016-08-06 summary:In many domains such as medicine, training data is in short supply. In such cases, external knowledge is often helpful in building predictive models. We propose a novel method to incorporate publicly available domain expertise to build accurate models. Specifically, we use word2vec models trained on a domain-specific corpus to estimate the relevance of each feature's text description to the prediction problem. We use these relevance estimates to rescale the features, causing more important features to experience weaker regularization. We apply our method to predict the onset of five chronic diseases in the next five years in two genders and two age groups. Our rescaling approach improves the accuracy of the model, particularly when there are few positive examples. Furthermore, our method selects 60% fewer features, easing interpretation by physicians. Our method is applicable to other domains where feature and outcome descriptions are available. version:1
arxiv-1608-02060 | Weighted diffusion LMP algorithm for distributed estimation in non-uniform noise conditions | http://arxiv.org/abs/1608.02060 | id:1608.02060 author:H. Zayyani, M. Korki category:stat.ML cs.IT math.IT  published:2016-08-06 summary:This letter presents an improved version of diffusion least mean ppower (LMP) algorithm for distributed estimation. Instead of sum of mean square errors, a weighted sum of mean square error is defined as the cost function for global and local cost functions of a network of sensors. The weight coefficients are updated by a simple steepest-descent recursion to minimize the error signal of the global and local adaptive algorithm. Simulation results show the advantages of the proposed weighted diffusion LMP over the diffusion LMP algorithm specially in the non-uniform noise conditions in a sensor network. version:1
arxiv-1608-02059 | Signs in time: Encoding human motion as a temporal image | http://arxiv.org/abs/1608.02059 | id:1608.02059 author:Joon Son Chung, Andrew Zisserman category:cs.CV  published:2016-08-06 summary:The goal of this work is to recognise and localise short temporal signals in image time series, where strong supervision is not available for training. To this end we propose an image encoding that concisely represents human motion in a video sequence in a form that is suitable for learning with a ConvNet. The encoding reduces the pose information from an image to a single column, dramatically diminishing the input requirements for the network, but retaining the essential information for recognition. The encoding is applied to the task of recognizing and localizing signed gestures in British Sign Language (BSL) videos. We demonstrate that using the proposed encoding, signs as short as 10 frames duration can be learnt from clips lasting hundreds of frames using only weak (clip level) supervision and with considerable label noise. version:1
arxiv-1609-02082 | An improved uncertainty decoding scheme with weighted samples for DNN-HMM hybrid systems | http://arxiv.org/abs/1609.02082 | id:1609.02082 author:Christian Huemmer, Ramón Fernández Astudillo, Walter Kellermann category:cs.LG cs.CL cs.SD  published:2016-08-04 summary:In this paper, we advance a recently-proposed uncertainty decoding scheme for DNN-HMM (deep neural network - hidden Markov model) hybrid systems. This numerical sampling concept averages DNN outputs produced by a finite set of feature samples (drawn from a probabilistic distortion model) to approximate the posterior likelihoods of the context-dependent HMM states. As main innovation, we propose a weighted DNN-output averaging based on a minimum classification error criterion and apply it to a probabilistic distortion model for spatial diffuseness features. The experimental evaluation is performed on the 8-channel REVERB Challenge task using a DNN-HMM hybrid system with multichannel front-end signal enhancement. We show that the recognition accuracy of the DNN-HMM hybrid system improves by incorporating uncertainty decoding based on random sampling and that the proposed weighted DNN-output averaging further reduces the word error rate scores. version:1
