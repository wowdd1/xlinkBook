arxiv-1311-0219 | Joint Estimation of Multiple Graphical Models from High Dimensional Time Series | http://arxiv.org/abs/1311.0219 | id:1311.0219 author:Huitong Qiu, Fang Han, Han Liu, Brian Caffo category:stat.ML  published:2013-11-01 summary:In this manuscript we consider the problem of jointly estimating multiple graphical models in high dimensions. We assume that the data are collected from n subjects, each of which consists of T possibly dependent observations. The graphical models of subjects vary, but are assumed to change smoothly corresponding to a measure of closeness between subjects. We propose a kernel based method for jointly estimating all graphical models. Theoretically, under a double asymptotic framework, where both (T,n) and the dimension d can increase, we provide the explicit rate of convergence in parameter estimation. It characterizes the strength one can borrow across different individuals and impact of data dependence on parameter estimation. Empirically, experiments on both synthetic and real resting state functional magnetic resonance imaging (rs-fMRI) data illustrate the effectiveness of the proposed method. version:2
arxiv-1410-2046 | Bayesian tracking and parameter learning for non-linear multiple target tracking models | http://arxiv.org/abs/1410.2046 | id:1410.2046 author:Lan Jiang, Sumeetpal S. Singh, Sinan Yıldırım category:stat.AP stat.CO stat.ML  published:2014-10-08 summary:We propose a new Bayesian tracking and parameter learning algorithm for non-linear non-Gaussian multiple target tracking (MTT) models. We design a Markov chain Monte Carlo (MCMC) algorithm to sample from the posterior distribution of the target states, birth and death times, and association of observations to targets, which constitutes the solution to the tracking problem, as well as the model parameters. In the numerical section, we present performance comparisons with several competing techniques and demonstrate significant performance improvements in all cases. version:1
arxiv-1410-2045 | Supervised learning Methods for Bangla Web Document Categorization | http://arxiv.org/abs/1410.2045 | id:1410.2045 author:Ashis Kumar Mandal, Rikta Sen category:cs.CL cs.LG  published:2014-10-08 summary:This paper explores the use of machine learning approaches, or more specifically, four supervised learning Methods, namely Decision Tree(C 4.5), K-Nearest Neighbour (KNN), Na\"ive Bays (NB), and Support Vector Machine (SVM) for categorization of Bangla web documents. This is a task of automatically sorting a set of documents into categories from a predefined set. Whereas a wide range of methods have been applied to English text categorization, relatively few studies have been conducted on Bangla language text categorization. Hence, we attempt to analyze the efficiency of those four methods for categorization of Bangla documents. In order to validate, Bangla corpus from various websites has been developed and used as examples for the experiment. For Bangla, empirical results support that all four methods produce satisfactory performance with SVM attaining good result in terms of high dimensional and relatively noisy document feature vectors. version:1
arxiv-1404-7828 | Deep Learning in Neural Networks: An Overview | http://arxiv.org/abs/1404.7828 | id:1404.7828 author:Juergen Schmidhuber category:cs.NE cs.LG  published:2014-04-30 summary:In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks. version:4
arxiv-1407-0749 | Projecting Ising Model Parameters for Fast Mixing | http://arxiv.org/abs/1407.0749 | id:1407.0749 author:Justin Domke, Xianghang Liu category:cs.LG stat.ML  published:2014-07-03 summary:Inference in general Ising models is difficult, due to high treewidth making tree-based algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling. version:2
arxiv-1410-1940 | GLAD: Group Anomaly Detection in Social Media Analysis- Extended Abstract | http://arxiv.org/abs/1410.1940 | id:1410.1940 author:Qi, Yu, Xinran He, Yan Liu category:cs.LG cs.SI H.2.8  published:2014-10-07 summary:Traditional anomaly detection on social media mostly focuses on individual point anomalies while anomalous phenomena usually occur in groups. Therefore it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this paper, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD takes both pair-wise and point-wise data as input, automatically infers the groups and detects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further generalize GLAD to its dynamic extension d-GLAD. We conduct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies. version:1
arxiv-1409-1257 | Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation | http://arxiv.org/abs/1409.1257 | id:1409.1257 author:Jean Pouget-Abadie, Dzmitry Bahdanau, Bart van Merrienboer, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML  published:2014-09-03 summary:The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences. version:2
arxiv-1409-1259 | On the Properties of Neural Machine Translation: Encoder-Decoder Approaches | http://arxiv.org/abs/1409.1259 | id:1409.1259 author:Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio category:cs.CL stat.ML  published:2014-09-03 summary:Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically. version:2
arxiv-1405-7764 | Generalization Bounds for Learning with Linear, Polygonal, Quadratic and Conic Side Knowledge | http://arxiv.org/abs/1405.7764 | id:1405.7764 author:Theja Tulabandhula, Cynthia Rudin category:stat.ML cs.LG  published:2014-05-30 summary:In this paper, we consider a supervised learning setting where side knowledge is provided about the labels of unlabeled examples. The side knowledge has the effect of reducing the hypothesis space, leading to tighter generalization bounds, and thus possibly better generalization. We consider several types of side knowledge, the first leading to linear and polygonal constraints on the hypothesis space, the second leading to quadratic constraints, and the last leading to conic constraints. We show how different types of domain knowledge can lead directly to these kinds of side knowledge. We prove bounds on complexity measures of the hypothesis space for quadratic and conic side knowledge, and show that these bounds are tight in a specific sense for the quadratic case. version:3
arxiv-1406-0281 | On Classification with Bags, Groups and Sets | http://arxiv.org/abs/1406.0281 | id:1406.0281 author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.CV cs.LG  published:2014-06-02 summary:Many classification problems can be difficult to formulate directly in terms of the traditional supervised setting, where both training and test samples are individual feature vectors. There are cases in which samples are better described by sets of feature vectors, that labels are only available for sets rather than individual samples, or, if individual labels are available, that these are not independent. To better deal with such problems, several extensions of supervised learning have been proposed, where either training and/or test objects are sets of feature vectors. However, having been proposed rather independently of each other, their mutual similarities and differences have hitherto not been mapped out. In this work, we provide an overview of such learning scenarios, propose a taxonomy to illustrate the relationships between them, and discuss directions for further research in these areas. version:2
arxiv-1410-1699 | Mumford-Shah and Potts Regularization for Manifold-Valued Data with Applications to DTI and Q-Ball Imaging | http://arxiv.org/abs/1410.1699 | id:1410.1699 author:Andreas Weinmann, Laurent Demaret, Martin Storath category:math.NA cs.CV math.OC physics.med-ph  published:2014-10-07 summary:Mumford-Shah and Potts functionals are powerful variational models for regularization which are widely used in signal and image processing; typical applications are edge-preserving denoising and segmentation. Being both non-smooth and non-convex, they are computationally challenging even for scalar data. For manifold-valued data, the problem becomes even more involved since typical features of vector spaces are not available. In this paper, we propose algorithms for Mumford-Shah and for Potts regularization of manifold-valued signals and images. For the univariate problems, we derive solvers based on dynamic programming combined with (convex) optimization techniques for manifold-valued data. For the class of Cartan-Hadamard manifolds (which includes the data space in diffusion tensor imaging), we show that our algorithms compute global minimizers for any starting point. For the multivariate Mumford-Shah and Potts problems (for image regularization) we propose a splitting into suitable subproblems which we can solve exactly using the techniques developed for the corresponding univariate problems. Our method does not require any a priori restrictions on the edge set and we do not have to discretize the data space. We apply our method to diffusion tensor imaging (DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentation of the corpus callosum. version:1
arxiv-1412-6069 | Annotation as a New Paradigm in Research Archiving | http://arxiv.org/abs/1412.6069 | id:1412.6069 author:Dirk Roorda, Charles van den Heuvel category:cs.DL cs.CL  published:2014-10-07 summary:We outline a paradigm to preserve results of digital scholarship, whether they are query results, feature values, or topic assignments. This paradigm is characterized by using annotations as multifunctional carriers and making them portable. The testing grounds we have chosen are two significant enterprises, one in the history of science, and one in Hebrew scholarship. The first one (CKCC) focuses on the results of a project where a Dutch consortium of universities, research institutes, and cultural heritage institutions experimented for 4 years with language techniques and topic modeling methods with the aim to analyze the emergence of scholarly debates. The data: a complex set of about 20.000 letters. The second one (DTHB) is a multi-year effort to express the linguistic features of the Hebrew bible in a text database, which is still growing in detail and sophistication. Versions of this database are packaged in commercial bible study software. We state that the results of these forms of scholarship require new knowledge management and archive practices. Only when researchers can build efficiently on each other's (intermediate) results, they can achieve the aggregations of quality data by which new questions can be answered, and hidden patterns visualized. Archives are required to find a balance between preserving authoritative versions of sources and supporting collaborative efforts in digital scholarship. Annotations are promising vehicles for preserving and reusing research results. Keywords annotation, portability, archiving, queries, features, topics, keywords, Republic of Letters, Hebrew text databases. version:1
arxiv-1401-6413 | Predicting Nearly As Well As the Optimal Twice Differentiable Regressor | http://arxiv.org/abs/1401.6413 | id:1401.6413 author:N. Denizcan Vanli, Muhammed O. Sayin, Suleyman S. Kozat category:cs.LG stat.ML  published:2014-01-23 summary:We study nonlinear regression of real valued data in an individual sequence manner, where we provide results that are guaranteed to hold without any statistical assumptions. We address the convergence and undertraining issues of conventional nonlinear regression methods and introduce an algorithm that elegantly mitigates these issues via an incremental hierarchical structure, (i.e., via an incremental decision tree). Particularly, we present a piecewise linear (or nonlinear) regression algorithm that partitions the regressor space in a data driven manner and learns a linear model at each region. Unlike the conventional approaches, our algorithm gradually increases the number of disjoint partitions on the regressor space in a sequential manner according to the observed data. Through this data driven approach, our algorithm sequentially and asymptotically achieves the performance of the optimal twice differentiable regression function for any data sequence with an unknown and arbitrary length. The computational complexity of the introduced algorithm is only logarithmic in the data length under certain regularity conditions. We provide the explicit description of the algorithm and demonstrate the significant gains for the well-known benchmark real data sets and chaotic signals. version:2
arxiv-1410-0640 | Term-Weighting Learning via Genetic Programming for Text Classification | http://arxiv.org/abs/1410.0640 | id:1410.0640 author:Hugo Jair Escalante, Mauricio A. García-Limón, Alicia Morales-Reyes, Mario Graff, Manuel Montes-y-Gómez, Eduardo F. Morales category:cs.NE cs.LG 68T50  68T10  published:2014-10-02 summary:This paper describes a novel approach to learning term-weighting schemes (TWSs) in the context of text classification. In text mining a TWS determines the way in which documents will be represented in a vector space model, before applying a classifier. Whereas acceptable performance has been obtained with standard TWSs (e.g., Boolean and term-frequency schemes), the definition of TWSs has been traditionally an art. Further, it is still a difficult task to determine what is the best TWS for a particular problem and it is not clear yet, whether better schemes, than those currently available, can be generated by combining known TWS. We propose in this article a genetic program that aims at learning effective TWSs that can improve the performance of current schemes in text classification. The genetic program learns how to combine a set of basic units to give rise to discriminative TWSs. We report an extensive experimental study comprising data sets from thematic and non-thematic text classification as well as from image classification. Our study shows the validity of the proposed method; in fact, we show that TWSs learned with the genetic program outperform traditional schemes and other TWSs proposed in recent works. Further, we show that TWSs learned from a specific domain can be effectively used for other tasks. version:3
arxiv-1410-1462 | Top Rank Optimization in Linear Time | http://arxiv.org/abs/1410.1462 | id:1410.1462 author:Nan Li, Rong Jin, Zhi-Hua Zhou category:cs.LG cs.AI cs.IR  published:2014-10-06 summary:Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the ranking loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster. version:1
arxiv-1410-2188 | An Aerial Image Recognition Framework using Discrimination and Redundancy Quality Measure | http://arxiv.org/abs/1410.2188 | id:1410.2188 author:Yuxin Hu, Luming Zhang category:cs.CV  published:2014-10-06 summary:Aerial image categorization plays an indispensable role in remote sensing and artificial intelligence. In this paper, we propose a new aerial image categorization framework, focusing on organizing the local patches of each aerial image into multiple discriminative subgraphs. The subgraphs reflect both the geometric property and the color distribution of an aerial image. First, each aerial image is decomposed into a collection of regions in terms of their color intensities. Thereby region connected graph (RCG), which models the connection between the spatial neighboring regions, is constructed to encode the spatial context of an aerial image. Second, a subgraph mining technique is adopted to discover the frequent structures in the RCGs constructed from the training aerial images. Thereafter, a set of refined structures are selected among the frequent ones toward being highly discriminative and low redundant. Lastly, given a new aerial image, its sub-RCGs corresponding to the refined structures are extracted. They are further quantized into a discriminative vector for SVM classification. Thorough experimental results validate the e?ectiveness of the proposed method. In addition, the visualized mined subgraphs show that the discriminative topologies of each aerial image are discovered. version:1
arxiv-1412-6154 | Effective persistent homology of digital images | http://arxiv.org/abs/1412.6154 | id:1412.6154 author:Ana Romero, Julio Rubio, Francis Sergeraert category:cs.CV  published:2014-10-06 summary:In this paper, three Computational Topology methods (namely effective homology, persistent homology and discrete vector fields) are mixed together to produce algorithms for homological digital image processing. The algorithms have been implemented as extensions of the Kenzo system and have shown a good performance when applied on some actual images extracted from a public dataset. version:1
arxiv-1410-1267 | Memristive Threshold Logic Circuit Design of Fast Moving Object Detection | http://arxiv.org/abs/1410.1267 | id:1410.1267 author:Akshay Kumar Maan, Dinesh Sasi Kumar, Sherin Sugathan, Alex Pappachen James category:cs.CV cs.AR cs.ET  published:2014-10-06 summary:Real-time detection of moving objects involves memorisation of features in the template image and their comparison with those in the test image. At high sampling rates, such techniques face the problems of high algorithmic complexity and component delays. We present a new resistive switching based threshold logic cell which encodes the pixels of a template image. The cell comprises a voltage divider circuit that programs the resistances of the memristors arranged in a single node threshold logic network and the output is encoded as a binary value using a CMOS inverter gate. When a test image is applied to the template-programmed cell, a mismatch in the respective pixels is seen as a change in the output voltage of the cell. The proposed cell when compared with CMOS equivalent implementation shows improved performance in area, leakage power, power dissipation and delay. version:1
arxiv-1402-0330 | Sequential Monte Carlo for Graphical Models | http://arxiv.org/abs/1402.0330 | id:1402.0330 author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.ME stat.ML  published:2014-02-03 summary:We propose a new framework for how to use sequential Monte Carlo (SMC) algorithms for inference in probabilistic graphical models (PGM). Via a sequential decomposition of the PGM we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces. By targeting these auxiliary distributions using SMC we are able to approximate the full joint distribution defined by the PGM. One of the key merits of the SMC sampler is that it provides an unbiased estimate of the partition function of the model. We also show how it can be used within a particle Markov chain Monte Carlo framework in order to construct high-dimensional block-sampling algorithms for general PGMs. version:4
arxiv-1410-0117 | Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape Estimation from Monocular Image Sequences | http://arxiv.org/abs/1410.0117 | id:1410.0117 author:Atul Kanaujia category:cs.CV  published:2014-10-01 summary:Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused on acquiring behavioral information of the targets and their activities. Continuous evolution of intelligence being gathered of the human centric activities has put increased focus on the humans, especially inferring their innate characteristics - size, shapes and physiology. These bio-signatures extracted from the surveillance sensors can be used to deduce age, ethnicity, gender and actions, and further characterize human actions in unseen scenarios. However, recovery of pose and shape of humans in such monocular videos is inherently an ill-posed problem, marked by frequent depth and view based ambiguities due to self-occlusion, foreshortening and misalignment. The likelihood function often yields a highly multimodal posterior that is difficult to propagate even using the most advanced particle filtering(PF) algorithms. Motivated by the recent success of the discriminative approaches to efficiently predict 3D poses directly from the 2D images, we present several principled approaches to integrate predictive cues using learned regression models to sustain multimodality of the posterior during tracking. Additionally, these learned priors can be actively adapted to the test data using a likelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D human shape model to each frame independently for inferring anthropometric bio-signatures. The proposed system is fully automated, robust to noisy test data and has ability to swiftly recover from tracking failures even after confronting with significant errors. We evaluate the system on a large number of monocular human motion sequences. version:2
arxiv-1410-1174 | Learning Topology and Dynamics of Large Recurrent Neural Networks | http://arxiv.org/abs/1410.1174 | id:1410.1174 author:Yiyuan She, Yuejia He, Dapeng Wu category:stat.ML stat.CO  published:2014-10-05 summary:Large-scale recurrent networks have drawn increasing attention recently because of their capabilities in modeling a large variety of real-world phenomena and physical mechanisms. This paper studies how to identify all authentic connections and estimate system parameters of a recurrent network, given a sequence of node observations. This task becomes extremely challenging in modern network applications, because the available observations are usually very noisy and limited, and the associated dynamical system is strongly nonlinear. By formulating the problem as multivariate sparse sigmoidal regression, we develop simple-to-implement network learning algorithms, with rigorous convergence guarantee in theory, for a variety of sparsity-promoting penalty forms. A quantile variant of progressive recurrent network screening is proposed for efficient computation and allows for direct cardinality control of network topology in estimation. Moreover, we investigate recurrent network stability conditions in Lyapunov's sense, and integrate such stability constraints into sparse network learning. Experiments show excellent performance of the proposed algorithms in network topology identification and forecasting. version:1
arxiv-1409-6155 | 1-HKUST: Object Detection in ILSVRC 2014 | http://arxiv.org/abs/1409.6155 | id:1409.6155 author:Cewu Lu, Hao Chen, Qifeng Chen, Hei Law, Yao Xiao, Chi-Keung Tang category:cs.CV  published:2014-09-22 summary:The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one of the most important big data challenges to date. We participated in the object detection track of ILSVRC 2014 and received the fourth place among the 38 teams. We introduce in our object detection system a number of novel techniques in localization and recognition. For localization, initial candidate proposals are generated using selective search, and a novel bounding boxes regression method is used for better object localization. For recognition, to represent a candidate proposal, we adopt three features, namely, RCNN feature, IFV feature, and DPM feature. Given these features, category-specific combination functions are learned to improve the object recognition rate. In addition, object context in the form of background priors and object interaction priors are learned and applied in our system. Our ILSVRC 2014 results are reported alongside with the results of other participating teams. version:3
arxiv-1410-1151 | Training Algorithm for Neuro-Fuzzy Network Based on Singular Spectrum Analysis | http://arxiv.org/abs/1410.1151 | id:1410.1151 author:Yulia S. Maslennikova, Vladimir V. Bochkarev category:cs.NE stat.ME 62M45  62M10  68T05 I.5.1  published:2014-10-05 summary:In this article, we propose a combination of an noise-reduction algorithm based on Singular Spectrum Analysis (SSA) and a standard feedforward neural prediction model. Basically, the proposed algorithm consists of two different steps: data preprocessing based on the SSA filtering method and step-by-step training procedure in which we use a simple feedforward multilayer neural network with backpropagation learning. The proposed noise-reduction procedure successfully removes most of the noise. That increases long-term predictability of the processed dataset comparison with the raw dataset. The method was applied to predict the International sunspot number RZ time series. The results show that our combined technique has better performances than those offered by the same network directly applied to raw dataset. version:1
arxiv-1410-1135 | Corpora Preparation and Stopword List Generation for Arabic data in Social Network | http://arxiv.org/abs/1410.1135 | id:1410.1135 author:Walaa Medhat, Ahmed H. Yousef, Hoda Korashy category:cs.CL  published:2014-10-05 summary:This paper proposes a methodology to prepare corpora in Arabic language from online social network (OSN) and review site for Sentiment Analysis (SA) task. The paper also proposes a methodology for generating a stopword list from the prepared corpora. The aim of the paper is to investigate the effect of removing stopwords on the SA task. The problem is that the stopwords lists generated before were on Modern Standard Arabic (MSA) which is not the common language used in OSN. We have generated a stopword list of Egyptian dialect and a corpus-based list to be used with the OSN corpora. We compare the efficiency of text classification when using the generated lists along with previously generated lists of MSA and combining the Egyptian dialect list with the MSA list. The text classification was performed using Na\"ive Bayes and Decision Tree classifiers and two feature selection approaches, unigrams and bigram. The experiments show that the general lists containing the Egyptian dialects words give better performance than using lists of MSA stopwords only. version:1
arxiv-1405-6676 | Statistique et Big Data Analytics; Volumétrie, L'Attaque des Clones | http://arxiv.org/abs/1405.6676 | id:1405.6676 author:Philippe Besse, Nathalie Villa-Vialaneix category:stat.OT cs.LG math.ST stat.TH  published:2014-05-26 summary:This article assumes acquired the skills and expertise of a statistician in unsupervised (NMF, k-means, SVD) and supervised learning (regression, CART, random forest). What skills and knowledge do a statistician must acquire to reach the "Volume" scale of big data? After a quick overview of the different strategies available and especially of those imposed by Hadoop, the algorithms of some available learning methods are outlined in order to understand how they are adapted to the strong stresses of the Map-Reduce functionalities version:2
arxiv-1307-5438 | Towards Distribution-Free Multi-Armed Bandits with Combinatorial Strategies | http://arxiv.org/abs/1307.5438 | id:1307.5438 author:Xiang-yang Li, Shaojie Tang, Yaqin Zhou category:cs.LG  published:2013-07-20 summary:In this paper we study a generalized version of classical multi-armed bandits (MABs) problem by allowing for arbitrary constraints on constituent bandits at each decision point. The motivation of this study comes from many situations that involve repeatedly making choices subject to arbitrary constraints in an uncertain environment: for instance, regularly deciding which advertisements to display online in order to gain high click-through-rate without knowing user preferences, or what route to drive home each day under uncertain weather and traffic conditions. Assume that there are $K$ unknown random variables (RVs), i.e., arms, each evolving as an \emph{i.i.d} stochastic process over time. At each decision epoch, we select a strategy, i.e., a subset of RVs, subject to arbitrary constraints on constituent RVs. We then gain a reward that is a linear combination of observations on selected RVs. The performance of prior results for this problem heavily depends on the distribution of strategies generated by corresponding learning policy. For example, if the reward-difference between the best and second best strategy approaches zero, prior result may lead to arbitrarily large regret. Meanwhile, when there are exponential number of possible strategies at each decision point, naive extension of a prior distribution-free policy would cause poor performance in terms of regret, computation and space complexity. To this end, we propose an efficient Distribution-Free Learning (DFL) policy that achieves zero regret, regardless of the probability distribution of the resultant strategies. Our learning policy has both $O(K)$ time complexity and $O(K)$ space complexity. In successive generations, we show that even if finding the optimal strategy at each decision point is NP-hard, our policy still allows for approximated solutions while retaining near zero-regret. version:3
arxiv-1410-2149 | Language-based Examples in the Statistics Classroom | http://arxiv.org/abs/1410.2149 | id:1410.2149 author:Roger Bilisoly category:cs.CL  published:2014-10-05 summary:Statistics pedagogy values using a variety of examples. Thanks to text resources on the Web, and since statistical packages have the ability to analyze string data, it is now easy to use language-based examples in a statistics class. Three such examples are discussed here. First, many types of wordplay (e.g., crosswords and hangman) involve finding words with letters that satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs of words often appear together more frequently than chance. For example, in the Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.) Third, a pangram contains all the letters of the alphabet at least once. These are searched for in Charles Dickens' A Christmas Carol, and their lengths are compared to the expected value given by the unequal probability coupon collector's problem as well as simulations. version:1
arxiv-1410-1090 | Explain Images with Multimodal Recurrent Neural Networks | http://arxiv.org/abs/1410.1090 | id:1410.1090 author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille category:cs.CV cs.CL cs.LG  published:2014-10-04 summary:In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. version:1
arxiv-1410-1080 | Generating abbreviations using Google Books library | http://arxiv.org/abs/1410.1080 | id:1410.1080 author:Valery D. Solovyev, Vladimir V. Bochkarev category:cs.CL stat.AP 91F20  62P25 I.2.7; J.5  published:2014-10-04 summary:The article describes the original method of creating a dictionary of abbreviations based on the Google Books Ngram Corpus. The dictionary of abbreviations is designed for Russian, yet as its methodology is universal it can be applied to any language. The dictionary can be used to define the function of the period during text segmentation in various applied systems of text processing. The article describes difficulties encountered in the process of its construction as well as the ways to overcome them. A model of evaluating a probability of first and second type errors (extraction accuracy and fullness) is constructed. Certain statistical data for the use of abbreviations are provided. version:1
arxiv-1410-1068 | Gamma Processes, Stick-Breaking, and Variational Inference | http://arxiv.org/abs/1410.1068 | id:1410.1068 author:Anirban Roychowdhury, Brian Kulis category:stat.ML cs.LG  published:2014-10-04 summary:While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithms on nonnegative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors. version:1
arxiv-1410-1037 | Facial Feature Point Detection: A Comprehensive Survey | http://arxiv.org/abs/1410.1037 | id:1410.1037 author:Nannan Wang, Xinbo Gao, Dacheng Tao, Xuelong Li category:cs.CV  published:2014-10-04 summary:This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods can be categorized into the following four groups: constrained local model (CLM)-based, active appearance model (AAM)-based, regression-based, and other methods. CLM-based methods consist of a shape model and a number of local experts, each of which is utilized to detect a facial feature point. AAM-based methods fit a shape model to an image by minimizing texture synthesis errors. Regression-based methods directly learn a mapping function from facial image appearance to facial feature points. Besides the above three major categories of methods, there are also minor categories of methods which we classify into other methods: graphical model-based methods, joint face alignment methods, independent facial feature point detectors, and deep learning-based methods. Though significant progress has been made, facial feature point detection is limited in its success by wild and real-world conditions: variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provide us a holistic understanding and deep insight into facial feature point detection, which also motivates us to explore promising future directions. version:1
arxiv-1410-0996 | Minimax Analysis of Active Learning | http://arxiv.org/abs/1410.0996 | id:1410.0996 author:Steve Hanneke, Liu Yang category:cs.LG math.ST stat.ML stat.TH  published:2014-10-03 summary:This work establishes distribution-free upper and lower bounds on the minimax label complexity of active learning with general hypothesis classes, under various noise models. The results reveal a number of surprising facts. In particular, under the noise model of Tsybakov (2004), the minimax label complexity of active learning with a VC class is always asymptotically smaller than that of passive learning, and is typically significantly smaller than the best previously-published upper bounds in the active learning literature. In high-noise regimes, it turns out that all active learning problems of a given VC dimension have roughly the same minimax label complexity, which contrasts with well-known results for bounded noise. In low-noise regimes, we find that the label complexity is well-characterized by a simple combinatorial complexity measure we call the star number. Interestingly, we find that almost all of the complexity measures previously explored in the active learning literature have worst-case values exactly equal to the star number. We also propose new active learning strategies that nearly achieve these minimax label complexities. version:1
arxiv-1410-0948 | Contributions of natural ventilation on thermal performance of alternative floor plan designs | http://arxiv.org/abs/1410.0948 | id:1410.0948 author:Eugénio Rodrigues, Adélio R. Gaspar, Álvaro Gomes, Manuel Gameiro da Silva category:cs.NE cs.SE D.2.2; G.1.6  published:2014-10-03 summary:During the earliest phase of architectural design process, practitioners after analyzing the client's design program, legal requirements, topographic constraints, and preferences synthesize these requirements into architectural floor plan drawings. Design decisions taken in this phase may significantly contribute to the building performance. On account of this reason, it is important to estimate and compare alternative solutions, when it is still manageable to change the building design. The authors have been developing a prototype tool to assist architects during this initial design phase. It is made up of two algorithms. The first algorithm generates alternative floor plans according to the architect's preferences and requirements, and the client's design program. It consists in one evolutionary strategy approach enhanced with local search technique to allocate rooms on several levels in the two-dimensional space. The second algorithm evaluates, ranks, and optimizes those floor plans according to thermal performance criteria. The prototype tool is coupled with dynamic simulation program, which estimates the thermal behavior of each solution. A sequential variable optimization is used to change several geometric values of different architectural elements in the floor plans to explore the improvement potential. In the present communication, the two algorithms are used in an iterative process to generate and optimize the thermal performance of alternative floor plans. In the building simulation specifications of EnergyPlus program, the airflow network model has been used in order to adequately model the air infiltration and the airflows through indoor spaces. A case study of a single-family house with three rooms in a single level is presented. version:1
arxiv-1407-0733 | Cortical spatio-temporal dimensionality reduction for visual grouping | http://arxiv.org/abs/1407.0733 | id:1407.0733 author:Giacomo Cocci, Davide Barbieri, Giovanna Citti, Alessandro Sarti category:cs.CV cs.NE q-bio.NC stat.ML  published:2014-07-02 summary:The visual systems of many mammals, including humans, is able to integrate the geometric information of visual stimuli and to perform cognitive tasks already at the first stages of the cortical processing. This is thought to be the result of a combination of mechanisms, which include feature extraction at single cell level and geometric processing by means of cells connectivity. We present a geometric model of such connectivities in the space of detected features associated to spatio-temporal visual stimuli, and show how they can be used to obtain low-level object segmentation. The main idea is that of defining a spectral clustering procedure with anisotropic affinities over datasets consisting of embeddings of the visual stimuli into higher dimensional spaces. Neural plausibility of the proposed arguments will be discussed. version:2
arxiv-1410-0908 | Probit Normal Correlated Topic Models | http://arxiv.org/abs/1410.0908 | id:1410.0908 author:Xingchen Yu, Ernest Fokoue category:stat.ML cs.IR cs.LG 62H25  62H30  published:2014-10-03 summary:The logistic normal distribution has recently been adapted via the transformation of multivariate Gaus- sian variables to model the topical distribution of documents in the presence of correlations among topics. In this paper, we propose a probit normal alternative approach to modelling correlated topical structures. Our use of the probit model in the context of topic discovery is novel, as many authors have so far con- centrated solely of the logistic model partly due to the formidable inefficiency of the multinomial probit model even in the case of very small topical spaces. We herein circumvent the inefficiency of multinomial probit estimation by using an adaptation of the diagonal orthant multinomial probit in the topic models context, resulting in the ability of our topic modelling scheme to handle corpuses with a large number of latent topics. An additional and very important benefit of our method lies in the fact that unlike with the logistic normal model whose non-conjugacy leads to the need for sophisticated sampling schemes, our ap- proach exploits the natural conjugacy inherent in the auxiliary formulation of the probit model to achieve greater simplicity. The application of our proposed scheme to a well known Associated Press corpus not only helps discover a large number of meaningful topics but also reveals the capturing of compellingly intuitive correlations among certain topics. Besides, our proposed approach lends itself to even further scalability thanks to various existing high performance algorithms and architectures capable of handling millions of documents. version:1
arxiv-1403-0736 | Fast Prediction with SVM Models Containing RBF Kernels | http://arxiv.org/abs/1403.0736 | id:1403.0736 author:Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor category:stat.ML cs.LG G.3; I.2.6; I.5.1  published:2014-03-04 summary:We present an approximation scheme for support vector machine models that use an RBF kernel. A second-order Maclaurin series approximation is used for exponentials of inner products between support vectors and test instances. The approximation is applicable to all kernel methods featuring sums of kernel evaluations and makes no assumptions regarding data normalization. The prediction speed of approximated models no longer relates to the amount of support vectors but is quadratic in terms of the number of input dimensions. If the number of input dimensions is small compared to the amount of support vectors, the approximated model is significantly faster in prediction and has a smaller memory footprint. An optimized C++ implementation was made to assess the gain in prediction speed in a set of practical tests. We additionally provide a method to verify the approximation accuracy, prior to training models or during run-time, to ensure the loss in accuracy remains acceptable and within known bounds. version:3
arxiv-1410-0868 | Group Orbit Optimization: A Unified Approach to Data Normalization | http://arxiv.org/abs/1410.0868 | id:1410.0868 author:Shuchang Zhou, Zhihua Zhang, Xiaobing Feng category:cs.NA cs.CV math.NA 15-02 G.1.3; I.5.4  published:2014-10-03 summary:In this paper we propose and study an optimization problem over a matrix group orbit that we call \emph{Group Orbit Optimization} (GOO). We prove that GOO can be used to induce matrix decomposition techniques such as singular value decomposition (SVD), LU decomposition, QR decomposition, Schur decomposition and Cholesky decomposition, etc. This gives rise to a unified framework for matrix decomposition and allows us to bridge these matrix decomposition methods. Moreover, we generalize GOO for tensor decomposition. As a concrete application of GOO, we devise a new data decomposition method over a special linear group to normalize point cloud data. Experiment results show that our normalization method is able to obtain recovery well from distortions like shearing, rotation and squeezing. version:1
arxiv-1410-0860 | Individualized Rank Aggregation using Nuclear Norm Regularization | http://arxiv.org/abs/1410.0860 | id:1410.0860 author:Yu Lu, Sahand N. Negahban category:stat.ML  published:2014-10-03 summary:In recent years rank aggregation has received significant attention from the machine learning community. The goal of such a problem is to combine the (partially revealed) preferences over objects of a large population into a single, relatively consistent ordering of those objects. However, in many cases, we might not want a single ranking and instead opt for individual rankings. We study a version of the problem known as collaborative ranking. In this problem we assume that individual users provide us with pairwise preferences (for example purchasing one item over another). From those preferences we wish to obtain rankings on items that the users have not had an opportunity to explore. The results here have a very interesting connection to the standard matrix completion problem. We provide a theoretical justification for a nuclear norm regularized optimization procedure, and provide high-dimensional scaling results that show how the error in estimating user preferences behaves as the number of observations increase. version:1
arxiv-1410-0818 | Feature Learning from Incomplete EEG with Denoising Autoencoder | http://arxiv.org/abs/1410.0818 | id:1410.0818 author:Junhua Li, Zbigniew Struzik, Liqing Zhang, Andrzej Cichocki category:cs.CV q-bio.NC  published:2014-10-03 summary:An alternative pathway for the human brain to communicate with the outside world is by means of a brain computer interface (BCI). A BCI can decode electroencephalogram (EEG) signals of brain activities, and then send a command or an intent to an external interactive device, such as a wheelchair. The effectiveness of the BCI depends on the performance in decoding the EEG. Usually, the EEG is contaminated by different kinds of artefacts (e.g., electromyogram (EMG), background activity), which leads to a low decoding performance. A number of filtering methods can be utilized to remove or weaken the effects of artefacts, but they generally fail when the EEG contains extreme artefacts. In such cases, the most common approach is to discard the whole data segment containing extreme artefacts. This causes the fatal drawback that the BCI cannot output decoding results during that time. In order to solve this problem, we employ the Lomb-Scargle periodogram to estimate the spectral power from incomplete EEG (after removing only parts contaminated by artefacts), and Denoising Autoencoder (DAE) for learning. The proposed method is evaluated with motor imagery EEG data. The results show that our method can successfully decode incomplete EEG to good effect. version:1
arxiv-1410-2191 | Learning manifold to regularize nonnegative matrix factorization | http://arxiv.org/abs/1410.2191 | id:1410.2191 author:Jim Jing-Yan Wang, Xin Gao category:cs.LG  published:2014-10-03 summary:Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularize nonegative matrix factorization (NMF) for data representation problems. NMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowrank nonnegative matrices, has been a popular method for data representation due to its ability to explore the latent part-based structure of data. Recent study shows that lots of data distributions have manifold structures, and we should respect the manifold structure when the data are represented. Recently, manifold regularized NMF used a nearest neighbor graph to regulate the learning of factorization parameter matrices and has shown its advantage over traditional NMF methods for data representation problems. However, how to construct an optimal graph to present the manifold prop- erly remains a difficultproblem due to the graph modelselection, noisy features, and nonlinear distributed data. In this chapter, we introduce three effective methods to solve these problems of graph construction for manifold regularized NMF. Multiple graph learning is proposed to solve the problem of graph model selection, adaptive graph learning via feature selection is proposed to solve the problem of constructing a graph from noisy features, while multi-kernel learning-based graph construction is used to solve the problem of learning a graph from nonlinearly distributed data. version:1
arxiv-1312-7077 | Language Modeling with Power Low Rank Ensembles | http://arxiv.org/abs/1312.7077 | id:1312.7077 author:Ankur P. Parikh, Avneesh Saluja, Chris Dyer, Eric P. Xing category:cs.CL cs.LG stat.ML  published:2013-12-26 summary:We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can be understood as a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task. version:2
arxiv-1410-0555 | Linear State-Space Model with Time-Varying Dynamics | http://arxiv.org/abs/1410.0555 | id:1410.0555 author:Jaakko Luttinen, Tapani Raiko, Alexander Ilin category:stat.ML  published:2014-10-02 summary:This paper introduces a linear state-space model with time-varying dynamics. The time dependency is obtained by forming the state dynamics matrix as a time-varying linear combination of a set of matrices. The time dependency of the weights in the linear combination is modelled by another linear Gaussian dynamical model allowing the model to learn how the dynamics of the process changes. Previous approaches have used switching models which have a small set of possible state dynamics matrices and the model selects one of those matrices at each time, thus jumping between them. Our model forms the dynamics as a linear combination and the changes can be smooth and more continuous. The model is motivated by physical processes which are described by linear partial differential equations whose parameters vary in time. An example of such a process could be a temperature field whose evolution is driven by a varying wind direction. The posterior inference is performed using variational Bayesian approximation. The experiments on stochastic advection-diffusion processes and real-world weather processes show that the model with time-varying dynamics can outperform previously introduced approaches. version:2
arxiv-1410-0741 | Generalized Laguerre Reduction of the Volterra Kernel for Practical Identification of Nonlinear Dynamic Systems | http://arxiv.org/abs/1410.0741 | id:1410.0741 author:Brett W. Israelsen, Dale A. Smith category:cs.LG  published:2014-10-03 summary:The Volterra series can be used to model a large subset of nonlinear, dynamic systems. A major drawback is the number of coefficients required model such systems. In order to reduce the number of required coefficients, Laguerre polynomials are used to estimate the Volterra kernels. Existing literature proposes algorithms for a fixed number of Volterra kernels, and Laguerre series. This paper presents a novel algorithm for generalized calculation of the finite order Volterra-Laguerre (VL) series for a MIMO system. An example addresses the utility of the algorithm in practical application. version:1
arxiv-1409-2552 | Sparse Additive Model using Symmetric Nonnegative Definite Smoothers | http://arxiv.org/abs/1409.2552 | id:1409.2552 author:Yan Li category:stat.ML cs.LG  published:2014-09-08 summary:We introduce a new algorithm, called adaptive sparse backfitting algorithm, for solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric, non-negative definite smoothers. Unlike the previous sparse backfitting algorithm, our method is essentially a block coordinate descent algorithm that guarantees to converge to the optimal solution. It bridges the gap between the population backfitting algorithm and that of the data version. We also prove variable selection consistency under suitable conditions. Numerical studies on both synthesis and real data are conducted to show that adaptive sparse backfitting algorithm outperforms previous sparse backfitting algorithm in fitting and predicting high dimensional nonparametric models. version:3
arxiv-1407-3897 | Bayesian Network Structure Learning Using Quantum Annealing | http://arxiv.org/abs/1407.3897 | id:1407.3897 author:Bryan O'Gorman, Alejandro Perdomo-Ortiz, Ryan Babbush, Alan Aspuru-Guzik, Vadim Smelyanskiy category:quant-ph cs.LG  published:2014-07-15 summary:We introduce a method for the problem of learning the structure of a Bayesian network using the quantum adiabatic algorithm. We do so by introducing an efficient reformulation of a standard posterior-probability scoring function on graphs as a pseudo-Boolean function, which is equivalent to a system of 2-body Ising spins, as well as suitable penalty terms for enforcing the constraints necessary for the reformulation; our proposed method requires $\mathcal O(n^2)$ qubits for $n$ Bayesian network variables. Furthermore, we prove lower bounds on the necessary weighting of these penalty terms. The logical structure resulting from the mapping has the appealing property that it is instance-independent for a given number of Bayesian network variables, as well as being independent of the number of data cases. version:2
arxiv-1410-0630 | Deep Directed Generative Autoencoders | http://arxiv.org/abs/1410.0630 | id:1410.0630 author:Sherjil Ozair, Yoshua Bengio category:stat.ML cs.LG cs.NE  published:2014-10-02 summary:For discrete data, the likelihood $P(x)$ can be rewritten exactly and parametrized into $P(X = x) = P(X = x H = f(x)) P(H = f(x))$ if $P(X H)$ has enough capacity to put no probability mass on any $x'$ for which $f(x')\neq f(x)$, where $f(\cdot)$ is a deterministic discrete function. The log of the first factor gives rise to the log-likelihood reconstruction error of an autoencoder with $f(\cdot)$ as the encoder and $P(X H)$ as the (probabilistic) decoder. The log of the second term can be seen as a regularizer on the encoded activations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder can be represented by a deep neural network and trained to maximize the average of the optimal log-likelihood $\log p(x)$. The objective is to learn an encoder $f(\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than $X$ itself, estimated by $P(H)$. This "flattens the manifold" or concentrates probability mass in a smaller number of (relevant) dimensions over which the distribution factorizes. Generating samples from the model is straightforward using ancestral sampling. One challenge is that regular back-propagation cannot be used to obtain the gradient on the parameters of the encoder, but we find that using the straight-through estimator works well here. We also find that although optimizing a single level of such architecture may be difficult, much better results can be obtained by pre-training and stacking them, gradually transforming the data distribution into one that is more easily captured by a simple parametric model. version:1
arxiv-1402-1973 | Dictionary learning for fast classification based on soft-thresholding | http://arxiv.org/abs/1402.1973 | id:1402.1973 author:Alhussein Fawzi, Mike Davies, Pascal Frossard category:cs.CV cs.LG stat.ML  published:2014-02-09 summary:Classifiers based on sparse representations have recently been shown to provide excellent results in many visual recognition and classification tasks. However, the high cost of computing sparse representations at test time is a major obstacle that limits the applicability of these methods in large-scale problems, or in scenarios where computational power is restricted. We consider in this paper a simple yet efficient alternative to sparse coding for feature extraction. We study a classification scheme that applies the soft-thresholding nonlinear mapping in a dictionary, followed by a linear classifier. A novel supervised dictionary learning algorithm tailored for this low complexity classification architecture is proposed. The dictionary learning problem, which jointly learns the dictionary and linear classifier, is cast as a difference of convex (DC) program and solved efficiently with an iterative DC solver. We conduct experiments on several datasets, and show that our learning algorithm that leverages the structure of the classification problem outperforms generic learning procedures. Our simple classifier based on soft-thresholding also competes with the recent sparse coding classifiers, when the dictionary is learned appropriately. The adopted classification scheme further requires less computational time at the testing stage, compared to other classifiers. The proposed scheme shows the potential of the adequately trained soft-thresholding mapping for classification and paves the way towards the development of very efficient classification methods for vision problems. version:2
arxiv-1410-0602 | A probabilistic evolutionary optimization approach to compute quasiparticle braids | http://arxiv.org/abs/1410.0602 | id:1410.0602 author:Roberto Santana, Ross B. McDonald, Helmut G. Katzgraber category:quant-ph cs.NE  published:2014-10-02 summary:Topological quantum computing is an alternative framework for avoiding the quantum decoherence problem in quantum computation. The problem of executing a gate in this framework can be posed as the problem of braiding quasiparticles. Because these are not Abelian, the problem can be reduced to finding an optimal product of braid generators where the optimality is defined in terms of the gate approximation and the braid's length. In this paper we propose the use of different variants of estimation of distribution algorithms to deal with the problem. Furthermore, we investigate how the regularities of the braid optimization problem can be translated into statistical regularities by means of the Boltzmann distribution. We show that our best algorithm is able to produce many solutions that approximates the target gate with an accuracy in the order of $10^{-6}$, and have lengths up to 9 times shorter than those expected from braids of the same accuracy obtained with other methods. version:1
arxiv-1410-0576 | Mapping Energy Landscapes of Non-Convex Learning Problems | http://arxiv.org/abs/1410.0576 | id:1410.0576 author:Maria Pavlovskaia, Kewei Tu, Song-Chun Zhu category:stat.ML cs.LG  published:2014-10-02 summary:In many statistical learning problems, the target functions to be optimized are highly non-convex in various model spaces and thus are difficult to analyze. In this paper, we compute \emph{Energy Landscape Maps} (ELMs) which characterize and visualize an energy function with a tree structure, in which each leaf node represents a local minimum and each non-leaf node represents the barrier between adjacent energy basins. The ELM also associates each node with the estimated probability mass and volume for the corresponding energy basin. We construct ELMs by adopting the generalized Wang-Landau algorithm and multi-domain sampler that simulates a Markov chain traversing the model space by dynamically reweighting the energy function. We construct ELMs in the model space for two classic statistical learning problems: i) clustering with Gaussian mixture models or Bernoulli templates; and ii) bi-clustering. We propose a way to measure the difficulties (or complexity) of these learning problems and study how various conditions affect the landscape complexity, such as separability of the clusters, the number of examples, and the level of supervision; and we also visualize the behaviors of different algorithms, such as K-mean, EM, two-step EM and Swendsen-Wang cuts, in the energy landscapes. version:1
arxiv-1410-1784 | Stochastic Discriminative EM | http://arxiv.org/abs/1410.1784 | id:1410.1784 author:Andres R. Masegosa category:cs.LG  published:2014-10-02 summary:Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional log-likelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions. version:1
arxiv-1410-0510 | Deep Sequential Neural Network | http://arxiv.org/abs/1410.0510 | id:1410.0510 author:Ludovic Denoyer, Patrick Gallinari category:cs.LG cs.NE  published:2014-10-02 summary:Neural Networks sequentially build high-level features through their successive layers. We propose here a new neural network model where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting model is structured according to a DAG like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations, like in classical multilayer networks, this model allows us for learning a set of local transformations. It is thus able to process data with different characteristics through specific sequences of such local transformations, increasing the expression power of this model w.r.t a classical multilayered network. The learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques. Experiments on different datasets show the relevance of this approach. version:1
arxiv-1410-0507 | Generating functionals for computational intelligence: the Fisher information as an objective function for self-limiting Hebbian learning rules | http://arxiv.org/abs/1410.0507 | id:1410.0507 author:Rodrigo Echeveste, Claudius Gros category:q-bio.NC cond-mat.dis-nn cs.NE  published:2014-10-02 summary:Generating functionals may guide the evolution of a dynamical system and constitute a possible route for handling the complexity of neural networks as relevant for computational intelligence. We propose and explore a new objective function, which allows to obtain plasticity rules for the afferent synaptic weights. The adaption rules are Hebbian, self-limiting, and result from the minimization of the Fisher information with respect to the synaptic flux. We perform a series of simulations examining the behavior of the new learning rules in various circumstances. The vector of synaptic weights aligns with the principal direction of input activities, whenever one is present. A linear discrimination is performed when there are two or more principal directions; directions having bimodal firing-rate distributions, being characterized by a negative excess kurtosis, are preferred. We find robust performance and full homeostatic adaption of the synaptic weights results as a by-product of the synaptic flux minimization. This self-limiting behavior allows for stable online learning for arbitrary durations. The neuron acquires new information when the statistics of input activities is changed at a certain point of the simulation, showing however, a distinct resilience to unlearn previously acquired knowledge. Learning is fast when starting with randomly drawn synaptic weights and substantially slower when the synaptic weights are already fully adapted. version:1
arxiv-1410-0478 | Recognition of Handwritten Bangla Basic Characters and Digits using Convex Hull based Feature Set | http://arxiv.org/abs/1410.0478 | id:1410.0478 author:Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri category:cs.CV  published:2014-10-02 summary:In dealing with the problem of recognition of handwritten character patterns of varying shapes and sizes, selection of a proper feature set is important to achieve high recognition performance. The current research aims to evaluate the performance of the convex hull based feature set, i.e. 125 features in all computed over different bays attributes of the convex hull of a pattern, for effective recognition of isolated handwritten Bangla basic characters and digits. On experimentation with a database of 10000 samples, the maximum recognition rate of 76.86% is observed for handwritten Bangla characters. For Bangla numerals the maximum success rate of 99.45%. is achieved on a database of 12000 sample. The current work validates the usefulness of a new kind of feature set for recognition of handwritten Bangla basic characters and numerals. version:1
arxiv-1410-0446 | Identification of Dynamic functional brain network states Through Tensor Decomposition | http://arxiv.org/abs/1410.0446 | id:1410.0446 author:Arash Golibagh Mahyari, Selin Aviyente category:cs.NE q-bio.NC  published:2014-10-02 summary:With the advances in high resolution neuroimaging, there has been a growing interest in the detection of functional brain connectivity. Complex network theory has been proposed as an attractive mathematical representation of functional brain networks. However, most of the current studies of functional brain networks have focused on the computation of graph theoretic indices for static networks, i.e. long-time averages of connectivity networks. It is well-known that functional connectivity is a dynamic process and the construction and reorganization of the networks is key to understanding human cognition. Therefore, there is a growing need to track dynamic functional brain networks and identify time intervals over which the network is quasi-stationary. In this paper, we present a tensor decomposition based method to identify temporally invariant 'network states' and find a common topographic representation for each state. The proposed methods are applied to electroencephalogram (EEG) data during the study of error-related negativity (ERN). version:1
arxiv-1410-0440 | Scalable Nonlinear Learning with Adaptive Polynomial Expansions | http://arxiv.org/abs/1410.0440 | id:1410.0440 author:Alekh Agarwal, Alina Beygelzimer, Daniel Hsu, John Langford, Matus Telgarsky category:cs.LG stat.ML  published:2014-10-02 summary:Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines. version:1
arxiv-1405-4047 | Methods and Models for Interpretable Linear Classification | http://arxiv.org/abs/1405.4047 | id:1405.4047 author:Berk Ustun, Cynthia Rudin category:stat.ME cs.LG stat.ML  published:2014-05-16 summary:We present an integer programming framework to build accurate and interpretable discrete linear classification models. Unlike existing approaches, our framework is designed to provide practitioners with the control and flexibility they need to tailor accurate and interpretable models for a domain of choice. To this end, our framework can produce models that are fully optimized for accuracy, by minimizing the 0--1 classification loss, and that address multiple aspects of interpretability, by incorporating a range of discrete constraints and penalty functions. We use our framework to produce models that are difficult to create with existing methods, such as scoring systems and M-of-N rule tables. In addition, we propose specially designed optimization methods to improve the scalability of our framework through decomposition and data reduction. We show that discrete linear classifiers can attain the training accuracy of any other linear classifier, and provide an Occam's Razor type argument as to why the use of small discrete coefficients can provide better generalization. We demonstrate the performance and flexibility of our framework through numerical experiments and a case study in which we construct a highly tailored clinical tool for sleep apnea diagnosis. version:2
arxiv-1410-0389 | Learning to Transfer Privileged Information | http://arxiv.org/abs/1410.0389 | id:1410.0389 author:Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert category:cs.CV stat.ML  published:2014-10-01 summary:We introduce a learning framework called learning using privileged information (LUPI) to the computer vision field. We focus on the prototypical computer vision problem of teaching computers to recognize objects in images. We want the computers to be able to learn faster at the expense of providing extra information during training time. As additional information about the image data, we look at several scenarios that have been studied in computer vision before: attributes, bounding boxes and image tags. The information is privileged as it is available at training time but not at test time. We explore two maximum-margin techniques that are able to make use of this additional source of information, for binary and multiclass object classification. We interpret these methods as learning easiness and hardness of the objects in the privileged space and then transferring this knowledge to train a better classifier in the original space. We provide a thorough analysis and comparison of information transfer from privileged to the original data spaces for both LUPI methods. Our experiments show that incorporating privileged information can improve the classification accuracy. Finally, we conduct user studies to understand which samples are easy and which are hard for human learning, and explore how this information is related to easy and hard samples when learning a classifier. version:1
arxiv-1410-0334 | Domain adaptation of weighted majority votes via perturbed variation-based self-labeling | http://arxiv.org/abs/1410.0334 | id:1410.0334 author:Emilie Morvant category:stat.ML cs.LG  published:2014-10-01 summary:In machine learning, the domain adaptation problem arrives when the test (target) and the train (source) data are generated from different distributions. A key applied issue is thus the design of algorithms able to generalize on a new distribution, for which we have no label information. We focus on learning classification models defined as a weighted majority vote over a set of real-val ued functions. In this context, Germain et al. (2013) have shown that a measure of disagreement between these functions is crucial to control. The core of this measure is a theoretical bound--the C-bound (Lacasse et al., 2007)--which involves the disagreement and leads to a well performing majority vote learning algorithm in usual non-adaptative supervised setting: MinCq. In this work, we propose a framework to extend MinCq to a domain adaptation scenario. This procedure takes advantage of the recent perturbed variation divergence between distributions proposed by Harel and Mannor (2012). Justified by a theoretical bound on the target risk of the vote, we provide to MinCq a target sample labeled thanks to a perturbed variation-based self-labeling focused on the regions where the source and target marginals appear similar. We also study the influence of our self-labeling, from which we deduce an original process for tuning the hyperparameters. Finally, our framework called PV-MinCq shows very promising results on a rotation and translation synthetic problem. version:1
arxiv-1409-5403 | Deformable Part Models are Convolutional Neural Networks | http://arxiv.org/abs/1409.5403 | id:1409.5403 author:Ross Girshick, Forrest Iandola, Trevor Darrell, Jitendra Malik category:cs.CV  published:2014-09-18 summary:Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are "black-box" non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a novel synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent (and at times novel) CNN layer. From this perspective, it becomes natural to replace the standard image features used in DPM with a learned feature extractor. We call the resulting model DeepPyramid DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly outperforms DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running an order of magnitude faster. version:2
arxiv-1410-0316 | Using social network graph analysis for interest detection | http://arxiv.org/abs/1410.0316 | id:1410.0316 author:Brian Lee Yung Rowe category:cs.SI cs.CL physics.soc-ph  published:2014-10-01 summary:A person's interests exist as an internal state and are difficult to define. Since only external actions are observable, a proxy must be used that represents someone's interests. Techniques like collaborative filtering, behavioral targeting, and hashtag analysis implicitly model an individual's interests. I argue that these models are limited to shallow, temporary interests, which do not reflect people's deeper interests or passions. I propose an alternative model of interests that takes advantage of a user's social graph. The basic principle is that people only follow those that interest them, so the social graph is an effective and robust proxy for people's interests. version:1
arxiv-1410-0286 | LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible | http://arxiv.org/abs/1410.0286 | id:1410.0286 author:Dirk Roorda, Gino Kalkman, Martijn Naaijer, Andreas van Cranenburgh category:cs.CL  published:2014-10-01 summary:The Linguistic Annotation Framework (LAF) provides a general, extensible stand-off markup system for corpora. This paper discusses LAF-Fabric, a new tool to analyse LAF resources in general with an extension to process the Hebrew Bible in particular. We first walk through the history of the Hebrew Bible as text database in decennium-wide steps. Then we describe how LAF-Fabric may serve as an analysis tool for this corpus. Finally, we describe three analytic projects/workflows that benefit from the new LAF representation: 1) the study of linguistic variation: extract cooccurrence data of common nouns between the books of the Bible (Martijn Naaijer); 2) the study of the grammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman); 3) construction of a parser of classical Hebrew by Data Oriented Parsing: generate tree structures from the database (Andreas van Cranenburgh). version:1
arxiv-1410-0243 | Pattern Encoding on the Poincare Sphere | http://arxiv.org/abs/1410.0243 | id:1410.0243 author:Aleksandra Pizurica category:cs.CV  published:2014-10-01 summary:This paper presents a convenient graphical tool for encoding visual patterns (such as image patches and image atoms) as point constellations in a space spanned by perceptual features and with a clear geometrical interpretation. General theory and a practical pattern encoding scheme are presented, inspired by encoding polarization states of a light wave on the Poincare sphere. This new pattern encoding scheme can be useful for many applications in image processing and computer vision. Here, three possible applications are illustrated, in clustering perceptually similar patterns, visualizing properties of learned dictionaries of image atoms and generating new dictionaries of image atoms from spherical codes. version:1
arxiv-1410-0162 | Reservoir Computing using Cellular Automata | http://arxiv.org/abs/1410.0162 | id:1410.0162 author:Ozgur Yilmaz category:cs.NE  published:2014-10-01 summary:We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long short-term memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efficient compared to state-of-the-art approaches. version:1
arxiv-1410-0123 | Deep Tempering | http://arxiv.org/abs/1410.0123 | id:1410.0123 author:Guillaume Desjardins, Heng Luo, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML  published:2014-10-01 summary:Restricted Boltzmann Machines (RBMs) are one of the fundamental building blocks of deep learning. Approximate maximum likelihood training of RBMs typically necessitates sampling from these models. In many training scenarios, computationally efficient Gibbs sampling procedures are crippled by poor mixing. In this work we propose a novel method of sampling from Boltzmann machines that demonstrates a computationally efficient way to promote mixing. Our approach leverages an under-appreciated property of deep generative models such as the Deep Belief Network (DBN), where Gibbs sampling from deeper levels of the latent variable hierarchy results in dramatically increased ergodicity. Our approach is thus to train an auxiliary latent hierarchical model, based on the DBN. When used in conjunction with parallel-tempering, the method is asymptotically guaranteed to simulate samples from the target RBM. Experimental results confirm the effectiveness of this sampling strategy in the context of RBM training. version:1
arxiv-1410-0095 | Riemannian Multi-Manifold Modeling | http://arxiv.org/abs/1410.0095 | id:1410.0095 author:Xu Wang, Konstantinos Slavakis, Gilad Lerman category:stat.ML cs.CV cs.LG  published:2014-10-01 summary:This paper advocates a novel framework for segmenting a dataset in a Riemannian manifold $M$ into clusters lying around low-dimensional submanifolds of $M$. Important examples of $M$, for which the proposed clustering algorithm is computationally efficient, are the sphere, the set of positive definite matrices, and the Grassmannian. The clustering problem with these examples of $M$ is already useful for numerous application domains such as action identification in video sequences, dynamic texture clustering, brain fiber segmentation in medical imaging, and clustering of deformed images. The proposed clustering algorithm constructs a data-affinity matrix by thoroughly exploiting the intrinsic geometry and then applies spectral clustering. The intrinsic local geometry is encoded by local sparse coding and more importantly by directional information of local tangent spaces and geodesics. Theoretical guarantees are established for a simplified variant of the algorithm even when the clusters intersect. To avoid complication, these guarantees assume that the underlying submanifolds are geodesic. Extensive validation on synthetic and real data demonstrates the resiliency of the proposed method against deviations from the theoretical model as well as its superior performance over state-of-the-art techniques. version:1
arxiv-1409-8630 | Unsupervised Bump Hunting Using Principal Components | http://arxiv.org/abs/1409.8630 | id:1409.8630 author:Daniel A Díaz-Pachón, Jean-Eudes Dazard, J. Sunil Rao category:stat.ML 65C60  published:2014-09-30 summary:Principal Components Analysis is a widely used technique for dimension reduction and characterization of variability in multivariate populations. Our interest lies in studying when and why the rotation to principal components can be used effectively within a response-predictor set relationship in the context of mode hunting. Specifically focusing on the Patient Rule Induction Method (PRIM), we first develop a fast version of this algorithm (fastPRIM) under normality which facilitates the theoretical studies to follow. Using basic geometrical arguments, we then demonstrate how the PC rotation of the predictor space alone can in fact generate improved mode estimators. Simulation results are used to illustrate our findings. version:1
arxiv-1401-2529 | A Study of Image Analysis with Tangent Distance | http://arxiv.org/abs/1401.2529 | id:1401.2529 author:Elif Vural, Pascal Frossard category:cs.CV  published:2014-01-11 summary:The computation of the geometric transformation between a reference and a target image, known as registration or alignment, corresponds to the projection of the target image onto the transformation manifold of the reference image (the set of images generated by its geometric transformations). It, however, often takes a nontrivial form such that the exact computation of projections on the manifold is difficult. The tangent distance method is an effective algorithm to solve this problem by exploiting a linear approximation of the manifold. As theoretical studies about the tangent distance algorithm have been largely overlooked, we present in this work a detailed performance analysis of this useful algorithm, which can eventually help its implementation. We consider a popular image registration setting using a multiscale pyramid of lowpass filtered versions of the (possibly noisy) reference and target images, which is particularly useful for recovering large transformations. We first show that the alignment error has a nonmonotonic variation with the filter size, due to the opposing effects of filtering on both manifold nonlinearity and image noise. We then study the convergence of the multiscale tangent distance method to the optimal solution. We finally examine the performance of the tangent distance method in image classification applications. Our theoretical findings are confirmed by experiments on image transformation models involving translations, rotations and scalings. Our study is the first detailed study of the tangent distance algorithm that leads to a better understanding of its efficacy and to the proper selection of its design parameters. version:3
arxiv-1409-8606 | Distributed Detection : Finite-time Analysis and Impact of Network Topology | http://arxiv.org/abs/1409.8606 | id:1409.8606 author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI stat.ML  published:2014-09-30 summary:This paper addresses the problem of distributed detection in multi-agent networks. Agents receive private signals about an unknown state of the world. The underlying state is globally identifiable, yet informative signals may be dispersed throughout the network. Using an optimization-based framework, we develop an iterative local strategy for updating individual beliefs. In contrast to the existing literature which focuses on asymptotic learning, we provide a finite-time analysis. Furthermore, we introduce a Kullback-Leibler cost to compare the efficiency of the algorithm to its centralized counterpart. Our bounds on the cost are expressed in terms of network size, spectral gap, centrality of each agent and relative entropy of agents' signal structures. A key observation is that distributing more informative signals to central agents results in a faster learning rate. Furthermore, optimizing the weights, we can speed up learning by improving the spectral gap. We also quantify the effect of link failures on learning speed in symmetric networks. We finally provide numerical simulations which verify our theoretical results. version:1
arxiv-1409-8576 | Data Imputation through the Identification of Local Anomalies | http://arxiv.org/abs/1409.8576 | id:1409.8576 author:Huseyin Ozkan, Ozgun S. Pelvan, Suleyman S. Kozat category:cs.LG stat.ML  published:2014-09-30 summary:We introduce a comprehensive and statistical framework in a model free setting for a complete treatment of localized data corruptions due to severe noise sources, e.g., an occluder in the case of a visual recording. Within this framework, we propose i) a novel algorithm to efficiently separate, i.e., detect and localize, possible corruptions from a given suspicious data instance and ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. As a generalization to Euclidean distance, we also propose a novel distance measure, which is based on the ranked deviations among the data attributes and empirically shown to be superior in separating the corruptions. Our algorithm first splits the suspicious instance into parts through a binary partitioning tree in the space of data attributes and iteratively tests those parts to detect local anomalies using the nominal statistics extracted from an uncorrupted (clean) reference data set. Once each part is labeled as anomalous vs normal, the corresponding binary patterns over this tree that characterize corruptions are identified and the affected attributes are imputed. Under a certain conditional independency structure assumed for the binary patterns, we analytically show that the false alarm rate of the introduced algorithm in detecting the corruptions is independent of the data and can be directly set without any parameter tuning. The proposed framework is tested over several well-known machine learning data sets with synthetically generated corruptions; and experimentally shown to produce remarkable improvements in terms of classification purposes with strong corruption separation capabilities. Our experiments also indicate that the proposed algorithms outperform the typical approaches and are robust to varying training phase conditions. version:1
arxiv-1409-8211 | Efficient multivariate sequence classification | http://arxiv.org/abs/1409.8211 | id:1409.8211 author:Pavel P. Kuksa category:cs.LG  published:2014-09-29 summary:Kernel-based approaches for sequence classification have been successfully applied to a variety of domains, including the text categorization, image classification, speech analysis, biological sequence analysis, time series and music classification, where they show some of the most accurate results. Typical kernel functions for sequences in these domains (e.g., bag-of-words, mismatch, or subsequence kernels) are restricted to {\em discrete univariate} (i.e. one-dimensional) string data, such as sequences of words in the text analysis, codeword sequences in the image analysis, or nucleotide or amino acid sequences in the DNA and protein sequence analysis. However, original sequence data are often of real-valued multivariate nature, i.e. are not univariate and discrete as required by typical $k$-mer based sequence kernel functions. In this work, we consider the problem of the {\em multivariate} sequence classification such as classification of multivariate music sequences, or multidimensional protein sequence representations. To this end, we extend {\em univariate} kernel functions typically used in sequence analysis and propose efficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a direct feature quantization (DFQ) of each sequence dimension in the original {\em real-valued} multivariate sequences and (2) applying novel multivariate discrete kernel measures on these multivariate discrete DFQ sequence representations to more accurately capture similarity relationships among sequences and improve classification performance. Experiments using the proposed MVDFQ-SK kernel method show excellent classification performance on three challenging music classification tasks as well as protein sequence classification with significant 25-40% improvements over univariate kernel methods and existing state-of-the-art sequence classification methods. version:2
arxiv-1409-8558 | A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis | http://arxiv.org/abs/1409.8558 | id:1409.8558 author:Prasanna Kumar Muthukumar, Alan W. Black category:cs.CL cs.LG cs.NE  published:2014-09-30 summary:Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis. We create an invertible, low-dimensional, noise-robust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is then unwrapped and used as the initialization for a Multi-Layer Perceptron (MLP). The MLP is fine-tuned by training it to reconstruct the input at the output layer. This MLP is then split down the middle to form encoding and decoding networks. These networks produce a parameterization of the Mel Log Spectrum that is intended to better fulfill the requirements of synthesis. Results are reported for experiments conducted using this resulting parameterization with the ClusterGen speech synthesizer. version:1
arxiv-1409-8498 | Non-Myopic Learning in Repeated Stochastic Games | http://arxiv.org/abs/1409.8498 | id:1409.8498 author:Jacob W. Crandall category:cs.GT cs.AI cs.LG  published:2014-09-30 summary:This paper addresses learning in repeated stochastic games (RSGs) played against unknown associates. Learning in RSGs is extremely challenging due to their inherently large strategy spaces. Furthermore, these games typically have multiple (often infinite) equilibria, making attempts to solve them via equilibrium analysis and rationality assumptions wholly insufficient. As such, previous learning algorithms for RSGs either learn very slowly or make extremely limiting assumptions about the game structure or associates' behaviors. In this paper, we propose and evaluate the notion of game abstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many aspects of the original game, including security and Pareto optimal Nash equilibria. We demonstrate that Gabe substantially outperforms existing algorithms in many scenarios. version:1
arxiv-1409-8485 | Predicting missing links via correlation between nodes | http://arxiv.org/abs/1409.8485 | id:1409.8485 author:Hao Liao, An Zeng, Yi-Cheng Zhang category:physics.soc-ph cs.SI physics.data-an stat.ML  published:2014-09-30 summary:As a fundamental problem in many different fields, link prediction aims to estimate the likelihood of an existing link between two nodes based on the observed information. Since this problem is related to many applications ranging from uncovering missing data to predicting the evolution of networks, link prediction has been intensively investigated recently and many methods have been proposed so far. The essential challenge of link prediction is to estimate the similarity between nodes. Most of the existing methods are based on the common neighbor index and its variants. In this paper, we propose to calculate the similarity between nodes by the correlation coefficient. This method is found to be very effective when applied to calculate similarity based on high order paths. We finally fuse the correlation-based method with the resource allocation method, and find that the combined method can substantially outperform the existing methods, especially in sparse networks. version:1
arxiv-1409-8484 | An agent-driven semantical identifier using radial basis neural networks and reinforcement learning | http://arxiv.org/abs/1409.8484 | id:1409.8484 author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cs.AI cs.CL cs.LG cs.MA C.2.1; I.2.6; I.2.7  published:2014-09-30 summary:Due to the huge availability of documents in digital form, and the deception possibility raise bound to the essence of digital documents and the way they are spread, the authorship attribution problem has constantly increased its relevance. Nowadays, authorship attribution,for both information retrieval and analysis, has gained great importance in the context of security, trust and copyright preservation. This work proposes an innovative multi-agent driven machine learning technique that has been developed for authorship attribution. By means of a preprocessing for word-grouping and time-period related analysis of the common lexicon, we determine a bias reference level for the recurrence frequency of the words within analysed texts, and then train a Radial Basis Neural Networks (RBPNN)-based classifier to identify the correct author. The main advantage of the proposed approach lies in the generality of the semantic analysis, which can be applied to different contexts and lexical domains, without requiring any modification. Moreover, the proposed system is able to incorporate an external input, meant to tune the classifier, and then self-adjust by means of continuous learning reinforcement. version:1
arxiv-1409-8428 | Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback | http://arxiv.org/abs/1409.8428 | id:1409.8428 author:Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, Ohad Shamir category:cs.LG stat.ML  published:2014-09-30 summary:We present and study a partial-information model of online learning, where a decision maker repeatedly chooses from a finite set of actions, and observes some subset of the associated losses. This naturally models several situations where the losses of different actions are related, and knowing the loss of one action provides information on the loss of other actions. Moreover, it generalizes and interpolates between the well studied full-information setting (where all losses are revealed) and the bandit setting (where only the loss of the action chosen by the player is revealed). We provide several algorithms addressing different variants of our setting, and provide tight regret bounds depending on combinatorial properties of the information feedback structure. version:1
arxiv-1406-1655 | Variational inference of latent state sequences using Recurrent Networks | http://arxiv.org/abs/1406.1655 | id:1406.1655 author:Justin Bayer, Christian Osendorfer category:stat.ML cs.LG  published:2014-06-06 summary:Recent advances in the estimation of deep directed graphical models and recurrent networks let us contribute to the removal of a blind spot in the area of probabilistc modelling of time series. The proposed methods i) can infer distributed latent state-space trajectories with nonlinear transitions, ii) scale to large data sets thanks to the use of a stochastic objective and fast, approximate inference, iii) enable the design of rich emission models which iv) will naturally lead to structured outputs. Two different paths of introducing latent state sequences are pursued, leading to the variational recurrent auto encoder (VRAE) and the variational one step predictor (VOSP). The use of independent Wiener processes as priors on the latent state sequence is a viable compromise between efficient computation of the Kullback-Leibler divergence from the variational approximation of the posterior and maintaining a reasonable belief in the dynamics. We verify our methods empirically, obtaining results close or superior to the state of the art. We also show qualitative results for denoising and missing value imputation. version:2
arxiv-1410-2488 | Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science | http://arxiv.org/abs/1410.2488 | id:1410.2488 author:Emily L. Spratt, Ahmed Elgammal category:cs.CV physics.hist-ph  published:2014-09-30 summary:In part one of the Critique of Judgment, Immanuel Kant wrote that "the judgment of taste...is not a cognitive judgment, and so not logical, but is aesthetic."\cite{Kant} While the condition of aesthetic discernment has long been the subject of philosophical discourse, the role of the arbiters of that judgment has more often been assumed than questioned. The art historian, critic, connoisseur, and curator have long held the esteemed position of the aesthetic judge, their training, instinct, and eye part of the inimitable subjective processes that Kant described as occurring upon artistic evaluation. Although the concept of intangible knowledge in regard to aesthetic theory has been much explored, little discussion has arisen in response to the development of new types of artificial intelligence as a challenge to the seemingly ineffable abilities of the human observer. This paper examines the developments in the field of computer vision analysis of paintings from canonical movements with the history of Western art and the reaction of art historians to the application of this technology in the field. Through an investigation of the ethical consequences of this innovative technology, the unquestioned authority of the art expert is challenged and the subjective nature of aesthetic judgment is brought to philosophical scrutiny once again. version:1
arxiv-1402-5792 | A Novel Scheme for Intelligent Recognition of Pornographic Images | http://arxiv.org/abs/1402.5792 | id:1402.5792 author:Seyed Mostafa Kia, Hossein Rahmani, Reza Mortezaei, Mohsen Ebrahimi Moghaddam, Amer Namazi category:cs.CV  published:2014-02-24 summary:Harmful contents are rising in internet day by day and this motivates the essence of more research in fast and reliable obscene and immoral material filtering. Pornographic image recognition is an important component in each filtering system. In this paper, a new approach for detecting pornographic images is introduced. In this approach, two new features are suggested. These two features in combination with other simple traditional features provide decent difference between porn and non-porn images. In addition, we applied fuzzy integral based information fusion to combine MLP (Multi-Layer Perceptron) and NF (Neuro-Fuzzy) outputs. To test the proposed method, performance of system was evaluated over 18354 download images from internet. The attained precision was 93% in TP and 8% in FP on training dataset, and 87% and 5.5% on test dataset. Achieved results verify the performance of proposed system versus other related works. version:3
arxiv-1402-1412 | Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial | http://arxiv.org/abs/1402.1412 | id:1402.1412 author:Yarin Gal, Mark van der Wilk category:stat.ML  published:2014-02-06 summary:In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias & Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate. version:2
arxiv-1402-1389 | Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models | http://arxiv.org/abs/1402.1389 | id:1402.1389 author:Yarin Gal, Mark van der Wilk, Carl E. Rasmussen category:stat.ML cs.LG  published:2014-02-06 summary:Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data. version:2
arxiv-1409-8327 | Bayesian and regularization approaches to multivariable linear system identification: the role of rank penalties | http://arxiv.org/abs/1409.8327 | id:1409.8327 author:Giulia Prando, Alessandro Chiuso, Gianluigi Pillonetto category:cs.SY cs.LG stat.ML  published:2014-09-29 summary:Recent developments in linear system identification have proposed the use of non-parameteric methods, relying on regularization strategies, to handle the so-called bias/variance trade-off. This paper introduces an impulse response estimator which relies on an $\ell_2$-type regularization including a rank-penalty derived using the log-det heuristic as a smooth approximation to the rank function. This allows to account for different properties of the estimated impulse response (e.g. smoothness and stability) while also penalizing high-complexity models. This also allows to account and enforce coupling between different input-output channels in MIMO systems. According to the Bayesian paradigm, the parameters defining the relative weight of the two regularization terms as well as the structure of the rank penalty are estimated optimizing the marginal likelihood. Once these hyperameters have been estimated, the impulse response estimate is available in closed form. Experiments show that the proposed method is superior to the estimator relying on the "classic" $\ell_2$-regularization alone as well as those based in atomic and nuclear norm. version:1
arxiv-1409-8309 | Arabic Spelling Correction using Supervised Learning | http://arxiv.org/abs/1409.8309 | id:1409.8309 author:Youssef Hassan, Mohamed Aly, Amir Atiya category:cs.LG cs.CL  published:2014-09-29 summary:In this work, we address the problem of spelling correction in the Arabic language utilizing the new corpus provided by QALB (Qatar Arabic Language Bank) project which is an annotated corpus of sentences with errors and their corrections. The corpus contains edit, add before, split, merge, add after, move and other error types. We are concerned with the first four error types as they contribute more than 90% of the spelling errors in the corpus. The proposed system has many models to address each error type on its own and then integrating all the models to provide an efficient and robust system that achieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58 including all the error types on the development set. Our system participated in the QALB 2014 shared task "Automatic Arabic Error Correction" and achieved an F1 score of 0.6, earning the sixth place out of nine participants. version:1
arxiv-1409-8202 | Short-Term Predictability of Photovoltaic Production over Italy | http://arxiv.org/abs/1409.8202 | id:1409.8202 author:Matteo De Felice, Marcello Petitta, Paolo M. Ruti category:cs.LG stat.AP  published:2014-09-29 summary:Photovoltaic (PV) power production increased drastically in Europe throughout the last years. About the 6% of electricity in Italy comes from PV and for an efficient management of the power grid an accurate and reliable forecasting of production would be needed. Starting from a dataset of electricity production of 65 Italian solar plants for the years 2011-2012 we investigate the possibility to forecast daily production from one to ten days of lead time without using on site measurements. Our study is divided in two parts: an assessment of the predictability of meteorological variables using weather forecasts and an analysis on the application of data-driven modelling in predicting solar power production. We calibrate a SVM model using available observations and then we force the same model with the predicted variables from weather forecasts with a lead time from one to ten days. As expected, solar power production is strongly influenced by cloudiness and clear sky, in fact we observe that while during summer we obtain a general error under the 10% (slightly lower in south Italy), during winter the error is abundantly above the 20%. version:1
arxiv-1409-8572 | Freshness-Aware Thompson Sampling | http://arxiv.org/abs/1409.8572 | id:1409.8572 author:Djallel Bouneffouf category:cs.IR cs.LG I.2  published:2014-09-29 summary:To follow the dynamicity of the user's content, researchers have recently started to model interactions between users and the Context-Aware Recommender Systems (CARS) as a bandit problem where the system needs to deal with exploration and exploitation dilemma. In this sense, we propose to study the freshness of the user's content in CARS through the bandit problem. We introduce in this paper an algorithm named Freshness-Aware Thompson Sampling (FA-TS) that manages the recommendation of fresh document according to the user's risk of the situation. The intensive evaluation and the detailed analysis of the experimental results reveals several important discoveries in the exploration/exploitation (exr/exp) behaviour. version:1
arxiv-1409-8191 | A Neural Networks Committee for the Contextual Bandit Problem | http://arxiv.org/abs/1409.8191 | id:1409.8191 author:Robin Allesiardo, Raphael Feraud, Djallel Bouneffouf category:cs.NE cs.LG I.2  published:2014-09-29 summary:This paper presents a new contextual bandit algorithm, NeuralBandit, which does not need hypothesis on stationarity of contexts and rewards. Several neural networks are trained to modelize the value of rewards knowing the context. Two variants, based on multi-experts approach, are proposed to choose online the parameters of multi-layer perceptrons. The proposed algorithms are successfully tested on a large dataset with and without stationarity of rewards. version:1
arxiv-1409-1458 | Communication-Efficient Distributed Dual Coordinate Ascent | http://arxiv.org/abs/1409.1458 | id:1409.1458 author:Martin Jaggi, Virginia Smith, Martin Takáč, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, Michael I. Jordan category:cs.LG math.OC stat.ML 90C25  68W15 G.1.6; C.1.4  published:2014-09-04 summary:Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, CoCoA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA converges to the same .001-accurate solution quality on average 25x as quickly. version:2
arxiv-1404-3581 | Random forests with random projections of the output space for high dimensional multi-label classification | http://arxiv.org/abs/1404.3581 | id:1404.3581 author:Arnaud Joly, Pierre Geurts, Louis Wehenkel category:stat.ML cs.LG  published:2014-04-14 summary:We adapt the idea of random projections applied to the output space, so as to enhance tree-based ensemble methods in the context of multi-label classification. We show how learning time complexity can be reduced without affecting computational complexity and accuracy of predictions. We also show that random output space projections may be used in order to reach different bias-variance tradeoffs, over a broad panel of benchmark problems, and that this may lead to improved accuracy while reducing significantly the computational burden of the learning stage. version:4
arxiv-1409-8152 | Controversy and Sentiment in Online News | http://arxiv.org/abs/1409.8152 | id:1409.8152 author:Yelena Mejova, Amy X. Zhang, Nicholas Diakopoulos, Carlos Castillo category:cs.CY cs.CL  published:2014-09-29 summary:How do news sources tackle controversial issues? In this work, we take a data-driven approach to understand how controversy interplays with emotional expression and biased language in the news. We begin by introducing a new dataset of controversial and non-controversial terms collected using crowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare millions of articles discussing controversial and non-controversial issues over a span of 7 months. We find that in general, when it comes to controversial issues, the use of negative affect and biased language is prevalent, while the use of strong emotion is tempered. We also observe many differences across news sources. Using these findings, we show that we can indicate to what extent an issue is controversial, by comparing it with other issues in terms of how they are portrayed across different media. version:1
arxiv-1409-8276 | A Bayesian Tensor Factorization Model via Variational Inference for Link Prediction | http://arxiv.org/abs/1409.8276 | id:1409.8276 author:Beyza Ermis, A. Taylan Cemgil category:cs.LG cs.NA stat.ML  published:2014-09-29 summary:Probabilistic approaches for tensor factorization aim to extract meaningful structure from incomplete data by postulating low rank constraints. Recently, variational Bayesian (VB) inference techniques have successfully been applied to large scale models. This paper presents full Bayesian inference via VB on both single and coupled tensor factorization models. Our method can be run even for very large models and is easily implemented. It exhibits better prediction performance than existing approaches based on maximum likelihood on several real-world datasets for missing link prediction problem. version:1
arxiv-1209-1086 | Robustness and Generalization for Metric Learning | http://arxiv.org/abs/1209.1086 | id:1209.1086 author:Aurélien Bellet, Amaury Habrard category:cs.LG stat.ML  published:2012-09-05 summary:Metric learning has attracted a lot of interest over the last decade, but the generalization ability of such methods has not been thoroughly studied. In this paper, we introduce an adaptation of the notion of algorithmic robustness (previously introduced by Xu and Mannor) that can be used to derive generalization bounds for metric learning. We further show that a weak notion of robustness is in fact a necessary and sufficient condition for a metric learning algorithm to generalize. To illustrate the applicability of the proposed framework, we derive generalization results for a large family of existing metric learning algorithms, including some sparse formulations that are not covered by previous results. version:3
arxiv-1409-8008 | CRF-based Named Entity Recognition @ICON 2013 | http://arxiv.org/abs/1409.8008 | id:1409.8008 author:Arjun Das, Utpal Garain category:cs.CL  published:2014-09-29 summary:This paper describes performance of CRF based systems for Named Entity Recognition (NER) in Indian language as a part of ICON 2013 shared task. In this task we have considered a set of language independent features for all the languages. Only for English a language specific feature, i.e. capitalization, has been added. Next the use of gazetteer is explored for Bengali, Hindi and English. The gazetteers are built from Wikipedia and other sources. Test results show that the system achieves the highest F measure of 88% for English and the lowest F measure of 69% for both Tamil and Telugu. Note that for the least performing two languages no gazetteer was used. NER in Bengali and Hindi finds accuracy (F measure) of 87% and 79%, respectively. version:1
arxiv-1409-8581 | Improving the Performance of English-Tamil Statistical Machine Translation System using Source-Side Pre-Processing | http://arxiv.org/abs/1409.8581 | id:1409.8581 author:M. Anand Kumar, V. Dhanalakshmi, K. P. Soman, V. Sharmiladevi category:cs.CL  published:2014-09-29 summary:Machine Translation is one of the major oldest and the most active research area in Natural Language Processing. Currently, Statistical Machine Translation (SMT) dominates the Machine Translation research. Statistical Machine Translation is an approach to Machine Translation which uses models to learn translation patterns directly from data, and generalize them to translate a new unseen text. The SMT approach is largely language independent, i.e. the models can be applied to any language pair. Statistical Machine Translation (SMT) attempts to generate translations using statistical methods based on bilingual text corpora. Where such corpora are available, excellent results can be attained translating similar texts, but such corpora are still not available for many language pairs. Statistical Machine Translation systems, in general, have difficulty in handling the morphology on the source or the target side especially for morphologically rich languages. Errors in morphology or syntax in the target language can have severe consequences on meaning of the sentence. They change the grammatical function of words or the understanding of the sentence through the incorrect tense information in verb. Baseline SMT also known as Phrase Based Statistical Machine Translation (PBSMT) system does not use any linguistic information and it only operates on surface word form. Recent researches shown that adding linguistic information helps to improve the accuracy of the translation with less amount of bilingual corpora. Adding linguistic information can be done using the Factored Statistical Machine Translation system through pre-processing steps. This paper investigates about how English side pre-processing is used to improve the accuracy of English-Tamil SMT system. version:1
arxiv-1402-6926 | Sequential Complexity as a Descriptor for Musical Similarity | http://arxiv.org/abs/1402.6926 | id:1402.6926 author:Peter Foster, Matthias Mauch, Simon Dixon category:cs.IR cs.LG cs.SD  published:2014-02-27 summary:We propose string compressibility as a descriptor of temporal structure in audio, for the purpose of determining musical similarity. Our descriptors are based on computing track-wise compression rates of quantised audio features, using multiple temporal resolutions and quantisation granularities. To verify that our descriptors capture musically relevant information, we incorporate our descriptors into similarity rating prediction and song year prediction tasks. We base our evaluation on a dataset of 15500 track excerpts of Western popular music, for which we obtain 7800 web-sourced pairwise similarity ratings. To assess the agreement among similarity ratings, we perform an evaluation under controlled conditions, obtaining a rank correlation of 0.33 between intersected sets of ratings. Combined with bag-of-features descriptors, we obtain performance gains of 31.1% and 10.9% for similarity rating prediction and song year prediction. For both tasks, analysis of selected descriptors reveals that representing features at multiple time scales benefits prediction accuracy. version:3
arxiv-1409-7963 | MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation | http://arxiv.org/abs/1409.7963 | id:1409.7963 author:Arjun Jain, Jonathan Tompson, Yann LeCun, Christoph Bregler category:cs.CV cs.LG cs.NE  published:2014-09-28 summary:In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems. version:1
arxiv-1409-7935 | Combining human and machine learning for morphological analysis of galaxy images | http://arxiv.org/abs/1409.7935 | id:1409.7935 author:Evan Kuminski, Joe George, John Wallin, Lior Shamir category:astro-ph.IM astro-ph.GA cs.CV cs.LG  published:2014-09-28 summary:The increasing importance of digital sky surveys collecting many millions of galaxy images has reinforced the need for robust methods that can perform morphological analysis of large galaxy image databases. Citizen science initiatives such as Galaxy Zoo showed that large datasets of galaxy images can be analyzed effectively by non-scientist volunteers, but since databases generated by robotic telescopes grow much faster than the processing power of any group of citizen scientists, it is clear that computer analysis is required. Here we propose to use citizen science data for training machine learning systems, and show experimental results demonstrating that machine learning systems can be trained with citizen science data. Our findings show that the performance of machine learning depends on the quality of the data, which can be improved by using samples that have a high degree of agreement between the citizen scientists. The source code of the method is publicly available. version:1
arxiv-1405-1005 | Comparing apples to apples in the evaluation of binary coding methods | http://arxiv.org/abs/1405.1005 | id:1405.1005 author:Mohammad Rastegari, Shobeir Fakhraei, Jonghyun Choi, David Jacobs, Larry S. Davis category:cs.CV cs.LG  published:2014-05-05 summary:We discuss methodological issues related to the evaluation of unsupervised binary code construction methods for nearest neighbor search. These issues have been widely ignored in literature. These coding methods attempt to preserve either Euclidean distance or angular (cosine) distance in the binary embedding space. We explain why when comparing a method whose goal is preserving cosine similarity to one designed for preserving Euclidean distance, the original features should be normalized by mapping them to the unit hypersphere before learning the binary mapping functions. To compare a method whose goal is to preserves Euclidean distance to one that preserves cosine similarity, the original feature data must be mapped to a higher dimension by including a bias term in binary mapping functions. These conditions ensure the fair comparison between different binary code methods for the task of nearest neighbor search. Our experiments show under these conditions the very simple methods (e.g. LSH and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and OK-means). version:2
arxiv-1407-1885 | PAINTER: a spatio-spectral image reconstruction algorithm for optical interferometry | http://arxiv.org/abs/1407.1885 | id:1407.1885 author:Antony Schutz, André Ferrari, David Mary, Férréol Soulez, Éric Thiébaut, Martin Vannier category:astro-ph.IM cs.CV  published:2014-06-29 summary:Astronomical optical interferometers sample the Fourier transform of the intensity distribution of a source at the observation wavelength. Because of rapid perturbations caused by atmospheric turbulence, the phases of the complex Fourier samples (visibilities) cannot be directly exploited. Consequently, specific image reconstruction methods have been devised in the last few decades. Modern polychromatic optical interferometric instruments are now paving the way to multiwavelength imaging. This paper is devoted to the derivation of a spatio-spectral (3D) image reconstruction algorithm, coined PAINTER (Polychromatic opticAl INTErferometric Reconstruction software). The algorithm relies on an iterative process, which alternates estimation of polychromatic images and of complex visibilities. The complex visibilities are not only estimated from squared moduli and closure phases, but also differential phases, which helps to better constrain the polychromatic reconstruction. Simulations on synthetic data illustrate the efficiency of the algorithm and in particular the relevance of injecting a differential phases model in the reconstruction. version:2
arxiv-1210-7599 | The automatic creation of concept maps from documents written using morphologically rich languages | http://arxiv.org/abs/1210.7599 | id:1210.7599 author:Krunoslav Zubrinic, Damir Kalpic, Mario Milicevic category:cs.IR cs.AI cs.CL  published:2012-10-29 summary:Concept map is a graphical tool for representing knowledge. They have been used in many different areas, including education, knowledge management, business and intelligence. Constructing of concept maps manually can be a complex task; an unskilled person may encounter difficulties in determining and positioning concepts relevant to the problem area. An application that recommends concept candidates and their position in a concept map can significantly help the user in that situation. This paper gives an overview of different approaches to automatic and semi-automatic creation of concept maps from textual and non-textual sources. The concept map mining process is defined, and one method suitable for the creation of concept maps from unstructured textual sources in highly inflected languages such as the Croatian language is described in detail. Proposed method uses statistical and data mining techniques enriched with linguistic tools. With minor adjustments, that method can also be used for concept map mining from textual sources in other morphologically rich languages. version:2
arxiv-1409-7787 | Audio Surveillance: a Systematic Review | http://arxiv.org/abs/1409.7787 | id:1409.7787 author:Marco Crocco, Marco Cristani, Andrea Trucco, Vittorio Murino category:cs.SD cs.CV cs.MM  published:2014-09-27 summary:Despite surveillance systems are becoming increasingly ubiquitous in our living environment, automated surveillance, currently based on video sensory modality and machine intelligence, lacks most of the time the robustness and reliability required in several real applications. To tackle this issue, audio sensory devices have been taken into account, both alone or in combination with video, giving birth, in the last decade, to a considerable amount of research. In this paper audio-based automated surveillance methods are organized into a comprehensive survey: a general taxonomy, inspired by the more widespread video surveillance field, is proposed in order to systematically describe the methods covering background subtraction, event classification, object tracking and situation analysis. For each of these tasks, all the significant works are reviewed, detailing their pros and cons and the context for which they have been proposed. Moreover, a specific section is devoted to audio features, discussing their expressiveness and their employment in the above described tasks. Differently, from other surveys on audio processing and analysis, the present one is specifically targeted to automated surveillance, highlighting the target applications of each described methods and providing the reader tables and schemes useful to retrieve the most suited algorithms for a specific requirement. version:1
arxiv-1409-7780 | Maximum mutual information regularized classification | http://arxiv.org/abs/1409.7780 | id:1409.7780 author:Jim Jing-Yan Wang, Yi Wang, Shiguang Zhao, Xin Gao category:cs.LG  published:2014-09-27 summary:In this paper, a novel pattern classification approach is proposed by regularizing the classifier learning to maximize mutual information between the classification response and the true class label. We argue that, with the learned classifier, the uncertainty of the true class label of a data sample should be reduced by knowing its classification response as much as possible. The reduced uncertainty is measured by the mutual information between the classification response and the true class label. To this end, when learning a linear classifier, we propose to maximize the mutual information between classification responses and true class labels of training samples, besides minimizing the classification error and reduc- ing the classifier complexity. An objective function is constructed by modeling mutual information with entropy estimation, and it is optimized by a gradi- ent descend method in an iterative algorithm. Experiments on two real world pattern classification problems show the significant improvements achieved by maximum mutual information regularization. version:1
arxiv-1409-7758 | Combating Corrupt Messages in Sparse Clustered Associative Memories | http://arxiv.org/abs/1409.7758 | id:1409.7758 author:Zhe Yao, Vincent Gripon, Michael Rabbat category:cs.NE  published:2014-09-27 summary:In this paper we analyze and extend the neural network based associative memory proposed by Gripon and Berrou. This associative memory resembles the celebrated Willshaw model with an added partite cluster structure. In the literature, two retrieving schemes have been proposed for the network dynamics, namely sum-of-sum and sum-of-max. They both offer considerably better performance than Willshaw and Hopfield networks, when comparable retrieval scenarios are considered. Former discussions and experiments concentrate on the erasure scenario, where a partial message is used as a probe to the network, in the hope of retrieving the full message. In this regard, sum-of-max outperforms sum-of-sum in terms of retrieval rate by a large margin. However, we observe that when noise and errors are present and the network is queried by a corrupt probe, sum-of-max faces a severe limitation as its stringent activation rule prevents a neuron from reviving back into play once deactivated. In this manuscript, we categorize and analyze different error scenarios so that both the erasure and the corrupt scenarios can be treated consistently. We make an amendment to the network structure to improve the retrieval rate, at the cost of an extra scalar per neuron. Afterwards, five different approaches are proposed to deal with corrupt probes. As a result, we extend the network capability, and also increase the robustness of the retrieving procedure. We then experimentally compare all these proposals and discuss pros and cons of each approach under different types of errors. Simulation results show that if carefully designed, the network is able to preserve both a high retrieval rate and a low running time simultaneously, even when queried by a corrupt probe. version:1
arxiv-1409-5763 | Active Dictionary Learning in Sparse Representation Based Classification | http://arxiv.org/abs/1409.5763 | id:1409.5763 author:Jin Xu, Haibo He, Hong Man category:cs.CV 68T05  published:2014-09-19 summary:Sparse representation, which uses dictionary atoms to reconstruct input vectors, has been studied intensively in recent years. A proper dictionary is a key for the success of sparse representation. In this paper, an active dictionary learning (ADL) method is introduced, in which classification error and reconstruction error are considered as the active learning criteria in selection of the atoms for dictionary construction. The learned dictionaries are caculated in sparse representation based classification (SRC). The classification accuracy and reconstruction error are used to evaluate the proposed dictionary learning method. The performance of the proposed dictionary learning method is compared with other methods, including unsupervised dictionary learning and whole-training-data dictionary. The experimental results based on the UCI data sets and face data set demonstrate the efficiency of the proposed method. version:2
arxiv-1409-7686 | How close are we to understanding image-based saliency? | http://arxiv.org/abs/1409.7686 | id:1409.7686 author:Matthias Kümmerer, Thomas Wallis, Matthias Bethge category:cs.CV q-bio.NC stat.AP  published:2014-09-26 summary:Within the set of the many complex factors driving gaze placement, the properities of an image that are associated with fixations under free viewing conditions have been studied extensively. There is a general impression that the field is close to understanding this particular association. Here we frame saliency models probabilistically as point processes, allowing the calculation of log-likelihoods and bringing saliency evaluation into the domain of information. We compared the information gain of state-of-the-art models to a gold standard and find that only one third of the explainable spatial information is captured. We additionally provide a principled method to show where and how models fail to capture information in the fixations. Thus, contrary to previous assertions, purely spatial saliency remains a significant challenge. version:1
arxiv-1409-7672 | Order-invariant prior specification in Bayesian factor analysis | http://arxiv.org/abs/1409.7672 | id:1409.7672 author:Dennis Leung, Mathias Drton category:stat.ME stat.ML  published:2014-09-26 summary:In (exploratory) factor analysis, the loading matrix is identified only up to orthogonal rotation. For identifiability, one thus often takes the loading matrix to be lower triangular with positive diagonal entries. In Bayesian inference, a standard practice is then to specify a prior under which the loadings are independent, the off-diagonal loadings are normally distributed, and the diagonal loadings follow a truncated normal distribution. This prior specification, however, depends in an important way on how the variables and associated rows of the loading matrix are ordered. We show how a minor modification of the approach allows one to compute with the identifiable lower triangular loading matrix but maintain invariance properties under reordering of the variables. version:1
arxiv-1409-7134 | Deconvolution of High-Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain | http://arxiv.org/abs/1409.7134 | id:1409.7134 author:Charles Zheng, Franco Pestilli, Ariel Rokem category:stat.ML  published:2014-09-25 summary:Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost, together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI). We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers. version:2
arxiv-1409-7591 | Topic Similarity Networks: Visual Analytics for Large Document Sets | http://arxiv.org/abs/1409.7591 | id:1409.7591 author:Arun S. Maiya, Robert M. Rolfe category:cs.CL cs.HC cs.IR cs.SI stat.ML I.2.6; I.2.7; H.5.2  published:2014-09-26 summary:We investigate ways in which to improve the interpretability of LDA topic models by better analyzing and visualizing their outputs. We focus on examining what we refer to as topic similarity networks: graphs in which nodes represent latent topics in text collections and links represent similarity among topics. We describe efficient and effective approaches to both building and labeling such networks. Visualizations of topic models based on these networks are shown to be a powerful means of exploring, characterizing, and summarizing large collections of unstructured text documents. They help to "tease out" non-obvious connections among different sets of documents and provide insights into how topics form larger themes. We demonstrate the efficacy and practicality of these approaches through two case studies: 1) NSF grants for basic research spanning a 14 year period and 2) the entire English portion of Wikipedia. version:1
arxiv-1409-7478 | An Analysis on Selection for High-Resolution Approximations in Many-Objective Optimization | http://arxiv.org/abs/1409.7478 | id:1409.7478 author:Hernan Aguirre, Arnaud Liefooghe, Sébastien Verel, Kiyoshi Tanaka category:cs.NE  published:2014-09-26 summary:This work studies the behavior of three elitist multi- and many-objective evolutionary algorithms generating a high-resolution approximation of the Pareto optimal set. Several search-assessment indicators are defined to trace the dynamics of survival selection and measure the ability to simultaneously keep optimal solutions and discover new ones under different population sizes, set as a fraction of the size of the Pareto optimal set. version:1
arxiv-1409-7476 | Short-term solar irradiance and irradiation forecasts via different time series techniques: A preliminary study | http://arxiv.org/abs/1409.7476 | id:1409.7476 author:Cédric Join, Cyril Voyant, Michel Fliess, Marc Muselli, Marie Laure Nivet, Christophe Paoli, Frédéric Chaxel category:cs.LG physics.ao-ph  published:2014-09-26 summary:This communication is devoted to solar irradiance and irradiation short-term forecasts, which are useful for electricity production. Several different time series approaches are employed. Our results and the corresponding numerical simulations show that techniques which do not need a large amount of historical data behave better than those which need them, especially when those data are quite noisy. version:1
arxiv-1409-7474 | Extracting man-made objects from remote sensing images via fast level set evolutions | http://arxiv.org/abs/1409.7474 | id:1409.7474 author:Zhongbin Li, Wenzhong Shi, Qunming Wang, Zelang Miao category:cs.CV 68T10  68T45 B.2.4; I.4.6; I.4.8  published:2014-09-26 summary:Object extraction from remote sensing images has long been an intensive research topic in the field of surveying and mapping. Most existing methods are devoted to handling just one type of object and little attention has been paid to improving the computational efficiency. In recent years, level set evolution (LSE) has been shown to be very promising for object extraction in the community of image processing and computer vision because it can handle topological changes automatically while achieving high accuracy. However, the application of state-of-the-art LSEs is compromised by laborious parameter tuning and expensive computation. In this paper, we proposed two fast LSEs for man-made object extraction from high spatial resolution remote sensing images. The traditional mean curvature-based regularization term is replaced by a Gaussian kernel and it is mathematically sound to do that. Thus a larger time step can be used in the numerical scheme to expedite the proposed LSEs. In contrast to existing methods, the proposed LSEs are significantly faster. Most importantly, they involve much fewer parameters while achieving better performance. The advantages of the proposed LSEs over other state-of-the-art approaches have been verified by a range of experiments. version:1
arxiv-1411-4614 | Using graph transformation algorithms to generate natural language equivalents of icons expressing medical concepts | http://arxiv.org/abs/1411.4614 | id:1411.4614 author:Pascal Vaillant, Jean-Baptiste Lamy category:cs.CL  published:2014-09-26 summary:A graphical language addresses the need to communicate medical information in a synthetic way. Medical concepts are expressed by icons conveying fast visual information about patients' current state or about the known effects of drugs. In order to increase the visual language's acceptance and usability, a natural language generation interface is currently developed. In this context, this paper describes the use of an informatics method ---graph transformation--- to prepare data consisting of concepts in an OWL-DL ontology for use in a natural language generation component. The OWL concept may be considered as a star-shaped graph with a central node. The method transforms it into a graph representing the deep semantic structure of a natural language phrase. This work may be of future use in other contexts where ontology concepts have to be mapped to half-formalized natural language expressions. version:1
arxiv-1405-5737 | Semi-supervised Spectral Clustering for Classification | http://arxiv.org/abs/1405.5737 | id:1405.5737 author:Arif Mahmood, Ajmal S. Mian category:cs.CV  published:2014-05-22 summary:We propose a Classification Via Clustering (CVC) algorithm which enables existing clustering methods to be efficiently employed in classification problems. In CVC, training and test data are co-clustered and class-cluster distributions are used to find the label of the test data. To determine an efficient number of clusters, a Semi-supervised Hierarchical Clustering (SHC) algorithm is proposed. Clusters are obtained by hierarchically applying two-way NCut by using signs of the Fiedler vector of the normalized graph Laplacian. To this end, a Direct Fiedler Vector Computation algorithm is proposed. The graph cut is based on the data structure and does not consider labels. Labels are used only to define the stopping criterion for graph cut. We propose clustering to be performed on the Grassmannian manifolds facilitating the formation of spectral ensembles. The proposed algorithm outperformed state-of-the-art image-set classification algorithms on five standard datasets. version:2
arxiv-1409-7461 | Autoencoder Trees | http://arxiv.org/abs/1409.7461 | id:1409.7461 author:Ozan İrsoy, Ethem Alpaydın category:cs.LG stat.ML  published:2014-09-26 summary:We discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees. We use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path. The encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input. Exploiting the continuity of the trees, autoencoder trees are trained with stochastic gradient descent. On handwritten digit and news data, we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons. We also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space. version:1
arxiv-1409-7458 | Beyond Maximum Likelihood: from Theory to Practice | http://arxiv.org/abs/1409.7458 | id:1409.7458 author:Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman category:stat.ME cs.DS cs.IT math.IT stat.ML  published:2014-09-26 summary:Maximum likelihood is the most widely used statistical estimation technique. Recent work by the authors introduced a general methodology for the construction of estimators for functionals in parametric models, and demonstrated improvements - both in theory and in practice - over the maximum likelihood estimator (MLE), particularly in high dimensional scenarios involving parameter dimension comparable to or larger than the number of samples. This approach to estimation, building on results from approximation theory, is shown to yield minimax rate-optimal estimators for a wide class of functionals, implementable with modest computational requirements. In a nutshell, a message of this recent work is that, for a wide class of functionals, the performance of these essentially optimal estimators with $n$ samples is comparable to that of the MLE with $n \ln n$ samples. In the present paper, we highlight the applicability of the aforementioned methodology to statistical problems beyond functional estimation, and show that it can yield substantial gains. For example, we demonstrate that for learning tree-structured graphical models, our approach achieves a significant reduction of the required data size compared with the classical Chow--Liu algorithm, which is an implementation of the MLE, to achieve the same accuracy. The key step in improving the Chow--Liu algorithm is to replace the empirical mutual information with the estimator for mutual information proposed by the authors. Further, applying the same replacement approach to classical Bayesian network classification, the resulting classifiers uniformly outperform the previous classifiers on 26 widely used datasets. version:1
arxiv-1409-7450 | Two-stage Geometric Information Guided Image Reconstruction | http://arxiv.org/abs/1409.7450 | id:1409.7450 author:Jing Qin, Weihong Guo category:math.OC cs.CV  published:2014-09-26 summary:In compressive sensing, it is challenging to reconstruct image of high quality from very few noisy linear projections. Existing methods mostly work well on piecewise constant images but not so well on piecewise smooth images such as natural images, medical images that contain a lot of details. We propose a two-stage method called GeoCS to recover images with rich geometric information from very limited amount of noisy measurements. The method adopts the shearlet transform that is mathematically proven to be optimal in sparsely representing images containing anisotropic features such as edges, corners, spikes etc. It also uses the weighted total variation (TV) sparsity with spatially variant weights to preserve sharp edges but to reduce the staircase effects of TV. Geometric information extracted from the results of stage I serves as an initial prior for stage II which alternates image reconstruction and geometric information update in a mutually beneficial way. GeoCS has been tested on incomplete spectral Fourier samples. It is applicable to other types of measurements as well. Experimental results on various complicated images show that GeoCS is efficient and generates high-quality images. version:1
arxiv-1308-2955 | Community Detection in Sparse Random Networks | http://arxiv.org/abs/1308.2955 | id:1308.2955 author:Ery Arias-Castro, Nicolas Verzelen category:math.ST stat.ML stat.TH  published:2013-08-13 summary:We consider the problem of detecting a tight community in a sparse random network. This is formalized as testing for the existence of a dense random subgraph in a random graph. Under the null hypothesis, the graph is a realization of an Erd\"os-R\'enyi graph on $N$ vertices and with connection probability $p_0$; under the alternative, there is an unknown subgraph on $n$ vertices where the connection probability is p1 > p0. In Arias-Castro and Verzelen (2012), we focused on the asymptotically dense regime where p0 is large enough that np0>(n/N)^{o(1)}. We consider here the asymptotically sparse regime where p0 is small enough that np0<(n/N)^{c0} for some c0>0. As before, we derive information theoretic lower bounds, and also establish the performance of various tests. Compared to our previous work, the arguments for the lower bounds are based on the same technology, but are substantially more technical in the details; also, the methods we study are different: besides a variant of the scan statistic, we study other statistics such as the size of the largest connected component, the number of triangles, the eigengap of the adjacency matrix, etc. Our detection bounds are sharp, except in the Poisson regime where we were not able to fully characterize the constant arising in the bound. version:2
arxiv-1409-7313 | A Deep Graph Embedding Network Model for Face Recognition | http://arxiv.org/abs/1409.7313 | id:1409.7313 author:Yufei Gan, Teng Yang, Chu He category:cs.CV  published:2014-09-25 summary:In this paper, we propose a new deep learning network "GENet", it combines the multi-layer network architec- ture and graph embedding framework. Firstly, we use simplest unsupervised learning PCA/LDA as first layer to generate the low- level feature. Secondly, many cascaded dimensionality reduction layers based on graph embedding framework are applied to GENet. Finally, a linear SVM classifier is used to classify dimension-reduced features. The experiments indicate that higher classification accuracy can be obtained by this algorithm on the CMU-PIE, ORL, Extended Yale B dataset. version:1
arxiv-1409-7307 | Image Classification with A Deep Network Model based on Compressive Sensing | http://arxiv.org/abs/1409.7307 | id:1409.7307 author:Yufei Gan, Tong Zhuo, Chu He category:cs.CV  published:2014-09-25 summary:To simplify the parameter of the deep learning network, a cascaded compressive sensing model "CSNet" is implemented for image classification. Firstly, we use cascaded compressive sensing network to learn feature from the data. Secondly, CSNet generates the feature by binary hashing and block-wise histograms. Finally, a linear SVM classifier is used to classify these features. The experiments on the MNIST dataset indicate that higher classification accuracy can be obtained by this algorithm. version:1
arxiv-1409-7386 | Performance of Stanford and Minipar Parser on Biomedical Texts | http://arxiv.org/abs/1409.7386 | id:1409.7386 author:Rushdi Shams category:cs.CL  published:2014-09-25 summary:In this paper, the performance of two dependency parsers, namely Stanford and Minipar, on biomedical texts has been reported. The performance of te parsers to assignm dependencies between two biomedical concepts that are already proved to be connected is not satisfying. Both Stanford and Minipar, being statistical parsers, fail to assign dependency relation between two connected concepts if they are distant by at least one clause. Minipar's performance, in terms of precision, recall and the F-score of the attachment score (e.g., correctly identified head in a dependency), to parse biomedical text is also measured taking the Stanford's as a gold standard. The results suggest that Minipar is not suitable yet to parse biomedical texts. In addition, a qualitative investigation reveals that the difference between working principles of the parsers also play a vital role for Minipar's degraded performance. version:1
arxiv-1409-7612 | Semi-supervised Classification for Natural Language Processing | http://arxiv.org/abs/1409.7612 | id:1409.7612 author:Rushdi Shams category:cs.CL cs.LG  published:2014-09-25 summary:Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization. version:1
arxiv-1409-7287 | Identification of jump Markov linear models using particle filters | http://arxiv.org/abs/1409.7287 | id:1409.7287 author:Andreas Svensson, Thomas B. Schön, Fredrik Lindsten category:stat.CO math.OC stat.ML  published:2014-09-25 summary:Jump Markov linear models consists of a finite number of linear state space models and a discrete variable encoding the jumps (or switches) between the different linear models. Identifying jump Markov linear models makes for a challenging problem lacking an analytical solution. We derive a new expectation maximization (EM) type algorithm that produce maximum likelihood estimates of the model parameters. Our development hinges upon recent progress in combining particle filters with Markov chain Monte Carlo methods in solving the nonlinear state smoothing problem inherent in the EM formulation. Key to our development is that we exploit a conditionally linear Gaussian substructure in the model, allowing for an efficient algorithm. version:1
arxiv-1409-7275 | The meaning-frequency law in Zipfian optimization models of communication | http://arxiv.org/abs/1409.7275 | id:1409.7275 author:Ramon Ferrer-i-Cancho category:cs.CL physics.data-an physics.soc-ph  published:2014-09-25 summary:According to Zipf's meaning-frequency law, words that are more frequent tend to have more meanings. Here it is shown that a linear dependency between the frequency of a form and its number of meanings is found in a family of models of Zipf's law for word frequencies. This is evidence for a weak version of the meaning-frequency law. Interestingly, that weak law (a) is not an inevitable of property of the assumptions of the family and (b) is found at least in the narrow regime where those models exhibit Zipf's law for word frequencies. version:1
arxiv-1409-7272 | Ctrax extensions for tracking in difficult lighting conditions | http://arxiv.org/abs/1409.7272 | id:1409.7272 author:Ulrich Stern, Chung-Hui Yang category:q-bio.QM cs.CV  published:2014-09-25 summary:The fly tracking software Ctrax by Branson et al. is popular for positional tracking of animals both within and beyond the fly community. Ctrax was not designed to handle tracking in difficult lighting conditions with strong shadows or recurring "on"/"off" changes in lighting - a condition that will likely become increasingly common due to the advent of red-shifted channelrhodopsin. We describe Ctrax extensions we developed that address this problem. The extensions enabled good tracking accuracy in three types of difficult lighting conditions in our lab. Our technique handling shadows relies on "single animal tracking"; the other techniques should be widely applicable. version:1
arxiv-1409-7619 | Generating Conceptual Metaphors from Proposition Stores | http://arxiv.org/abs/1409.7619 | id:1409.7619 author:Ekaterina Ovchinnikova, Vladimir Zaytsev, Suzanne Wertheim, Ross Israel category:cs.CL  published:2014-09-25 summary:Contemporary research on computational processing of linguistic metaphors is divided into two main branches: metaphor recognition and metaphor interpretation. We take a different line of research and present an automated method for generating conceptual metaphors from linguistic data. Given the generated conceptual metaphors, we find corresponding linguistic metaphors in corpora. In this paper, we describe our approach and its evaluation using English and Russian data. version:1
arxiv-1304-3841 | The risks of mixing dependency lengths from sequences of different length | http://arxiv.org/abs/1304.3841 | id:1304.3841 author:Ramon Ferrer-i-Cancho, Haitao Liu category:cs.CL physics.data-an  published:2013-04-13 summary:Mixing dependency lengths from sequences of different length is a common practice in language research. However, the empirical distribution of dependency lengths of sentences of the same length differs from that of sentences of varying length and the distribution of dependency lengths depends on sentence length for real sentences and also under the null hypothesis that dependencies connect vertices located in random positions of the sequence. This suggests that certain results, such as the distribution of syntactic dependency lengths mixing dependencies from sentences of varying length, could be a mere consequence of that mixing. Furthermore, differences in the global averages of dependency length (mixing lengths from sentences of varying length) for two different languages do not simply imply a priori that one language optimizes dependency lengths better than the other because those differences could be due to differences in the distribution of sentence lengths and other factors. version:2
arxiv-1409-7165 | Heterogeneous Metric Learning with Content-based Regularization for Software Artifact Retrieval | http://arxiv.org/abs/1409.7165 | id:1409.7165 author:Liang Wu, Hui Xiong, Liang Du, Bo Liu, Guandong Xu, Yong Ge, Yanjie Fu, Yuanchun Zhou, Jianhui Li category:cs.LG cs.IR cs.SE  published:2014-09-25 summary:The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin. version:1
arxiv-1409-7164 | Deep Learning Representation using Autoencoder for 3D Shape Retrieval | http://arxiv.org/abs/1409.7164 | id:1409.7164 author:Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, Xiang Bai category:cs.CV  published:2014-09-25 summary:We study the problem of how to build a deep learning representation for 3D shape. Deep learning has shown to be very effective in variety of visual applications, such as image classification and object detection. However, it has not been successfully applied to 3D shape recognition. This is because 3D shape has complex structure in 3D space and there are limited number of 3D shapes for feature learning. To address these problems, we project 3D shapes into 2D space and use autoencoder for feature learning on the 2D images. High accuracy 3D shape retrieval performance is obtained by aggregating the features learned on 2D images. In addition, we show the proposed deep learning feature is complementary to conventional local image descriptors. By combing the global deep learning representation and the local descriptor representation, our method can obtain the state-of-the-art performance on 3D shape retrieval benchmarks. version:1
arxiv-1409-5185 | Deeply-Supervised Nets | http://arxiv.org/abs/1409.5185 | id:1409.5185 author:Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu category:stat.ML cs.LG cs.NE  published:2014-09-18 summary:Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce "companion objective" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN). version:2
arxiv-1312-5465 | Learning rates of $l^q$ coefficient regularization learning with Gaussian kernel | http://arxiv.org/abs/1312.5465 | id:1312.5465 author:Shaobo Lin, Jinshan Zeng, Jian Fang, Zongben Xu category:cs.LG stat.ML 68T05 F.2.1  published:2013-12-19 summary:Regularization is a well recognized powerful strategy to improve the performance of a learning machine and $l^q$ regularization schemes with $0<q<\infty$ are central in use. It is known that different $q$ leads to different properties of the deduced estimators, say, $l^2$ regularization leads to smooth estimators while $l^1$ regularization leads to sparse estimators. Then, how does the generalization capabilities of $l^q$ regularization learning vary with $q$? In this paper, we study this problem in the framework of statistical learning theory and show that implementing $l^q$ coefficient regularization schemes in the sample dependent hypothesis space associated with Gaussian kernel can attain the same almost optimal learning rates for all $0<q<\infty$. That is, the upper and lower bounds of learning rates for $l^q$ regularization learning are asymptotically identical for all $0<q<\infty$. Our finding tentatively reveals that, in some modeling contexts, the choice of $q$ might not have a strong impact with respect to the generalization capability. From this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc.. version:3
arxiv-1409-7085 | Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach | http://arxiv.org/abs/1409.7085 | id:1409.7085 author:Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori Levin, Scott Miller, Christine Piatko category:cs.CL cs.LG stat.ML  published:2014-09-24 summary:We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality---and further demonstrates that large gains can be achieved for low-resource languages with different word order than English. version:1
arxiv-1409-7074 | Variational Pseudolikelihood for Regularized Ising Inference | http://arxiv.org/abs/1409.7074 | id:1409.7074 author:Charles K. Fisher category:cond-mat.stat-mech cs.LG stat.ML  published:2014-09-24 summary:I propose a variational approach to maximum pseudolikelihood inference of the Ising model. The variational algorithm is more computationally efficient, and does a better job predicting out-of-sample correlations than $L_2$ regularized maximum pseudolikelihood inference as well as mean field and isolated spin pair approximations with pseudocount regularization. The key to the approach is a variational energy that regularizes the inference problem by shrinking the couplings towards zero, while still allowing some large couplings to explain strong correlations. The utility of the variational pseudolikelihood approach is illustrated by training an Ising model to represent the letters A-J using samples of letters from different computer fonts. version:1
arxiv-1409-6981 | Unsupervised learning of regression mixture models with unknown number of components | http://arxiv.org/abs/1409.6981 | id:1409.6981 author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML  published:2014-09-24 summary:Regression mixture models are widely studied in statistics, machine learning and data analysis. Fitting regression mixtures is challenging and is usually performed by maximum likelihood by using the expectation-maximization (EM) algorithm. However, it is well-known that the initialization is crucial for EM. If the initialization is inappropriately performed, the EM algorithm may lead to unsatisfactory results. The EM algorithm also requires the number of clusters to be given a priori; the problem of selecting the number of mixture components requires using model selection criteria to choose one from a set of pre-estimated candidate models. We propose a new fully unsupervised algorithm to learn regression mixture models with unknown number of components. The developed unsupervised learning approach consists in a penalized maximum likelihood estimation carried out by a robust expectation-maximization (EM) algorithm for fitting polynomial, spline and B-spline regressions mixtures. The proposed learning approach is fully unsupervised: 1) it simultaneously infers the model parameters and the optimal number of the regression mixture components from the data as the learning proceeds, rather than in a two-fold scheme as in standard model-based clustering using afterward model selection criteria, and 2) it does not require accurate initialization unlike the standard EM for regression mixtures. The developed approach is applied to curve clustering problems. Numerical experiments on simulated data show that the proposed robust EM algorithm performs well and provides accurate results in terms of robustness with regard initialization and retrieving the optimal partition with the actual number of clusters. An application to real data in the framework of functional data clustering, confirms the benefit of the proposed approach for practical applications. version:1
arxiv-1309-6786 | One-class Collaborative Filtering with Random Graphs: Annotated Version | http://arxiv.org/abs/1309.6786 | id:1309.6786 author:Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG G.3  published:2013-09-26 summary:The bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. In this paper we present a novel Bayesian generative model for implicit collaborative filtering. It forms a core component of the Xbox Live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply not considering it. The latent signal is treated as an unobserved random graph connecting users with items they might have encountered. We demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples. A fine-grained comparison is done against a state of the art baseline on real world data. version:4
arxiv-1409-6838 | Recent Progress in Image Deblurring | http://arxiv.org/abs/1409.6838 | id:1409.6838 author:Ruxin Wang, Dacheng Tao category:cs.CV  published:2014-09-24 summary:This paper comprehensively reviews the recent development of image deblurring, including non-blind/blind, spatially invariant/variant deblurring techniques. Indeed, these techniques share the same objective of inferring a latent sharp image from one or several corresponding blurry images, while the blind deblurring techniques are also required to derive an accurate blur kernel. Considering the critical role of image restoration in modern imaging systems to provide high-quality images under complex environments such as motion, undesirable lighting conditions, and imperfect system components, image deblurring has attracted growing attention in recent years. From the viewpoint of how to handle the ill-posedness which is a crucial issue in deblurring tasks, existing methods can be grouped into five categories: Bayesian inference framework, variational methods, sparse representation-based methods, homography-based modeling, and region-based methods. In spite of achieving a certain level of development, image deblurring, especially the blind case, is limited in its success by complex application conditions which make the blur kernel hard to obtain and be spatially variant. We provide a holistic understanding and deep insight into image deblurring in this review. An analysis of the empirical evidence for representative methods, practical issues, as well as a discussion of promising future directions are also presented. version:1
arxiv-1409-6833 | Quantized Estimation of Gaussian Sequence Models in Euclidean Balls | http://arxiv.org/abs/1409.6833 | id:1409.6833 author:Yuancheng Zhu, John Lafferty category:math.ST stat.ML stat.TH  published:2014-09-24 summary:A central result in statistical theory is Pinsker's theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper, we present an extension to Pinsker's theorem where estimation is carried out under storage or communication constraints. In particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball, which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting. version:1
arxiv-1409-6805 | Improving Cross-domain Recommendation through Probabilistic Cluster-level Latent Factor Model--Extended Version | http://arxiv.org/abs/1409.6805 | id:1409.6805 author:Siting Ren, Sheng Gao category:cs.IR cs.LG stat.ML  published:2014-09-24 summary:Cross-domain recommendation has been proposed to transfer user behavior pattern by pooling together the rating data from multiple domains to alleviate the sparsity problem appearing in single rating domains. However, previous models only assume that multiple domains share a latent common rating pattern based on the user-item co-clustering. To capture diversities among different domains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF) model to improve the cross-domain recommendation performance. Experiments on several real world datasets demonstrate that our proposed model outperforms the state-of-the-art methods for the cross-domain recommendation task. version:1
arxiv-1409-6745 | A Concept Learning Approach to Multisensory Object Perception | http://arxiv.org/abs/1409.6745 | id:1409.6745 author:Ifeoma Nwogu, Goker Erdogan, Ilker Yildirim, Robert Jacobs category:cs.CV  published:2014-09-23 summary:This paper presents a computational model of concept learning using Bayesian inference for a grammatically structured hypothesis space, and test the model on multisensory (visual and haptics) recognition of 3D objects. The study is performed on a set of artificially generated 3D objects known as fribbles, which are complex, multipart objects with categorical structures. The goal of this work is to develop a working multisensory representational model that integrates major themes on concepts and concepts learning from the cognitive science literature. The model combines the representational power of a probabilistic generative grammar with the inferential power of Bayesian induction. version:1
arxiv-1306-3706 | Local case-control sampling: Efficient subsampling in imbalanced data sets | http://arxiv.org/abs/1306.3706 | id:1306.3706 author:William Fithian, Trevor Hastie category:stat.CO stat.ML  published:2013-06-16 summary:For classification problems with significant class imbalance, subsampling can reduce computational costs at the price of inflated variance in estimating model parameters. We propose a method for subsampling efficiently for logistic regression by adjusting the class balance locally in feature space via an accept-reject scheme. Our method generalizes standard case-control sampling, using a pilot estimate to preferentially select examples whose responses are conditionally rare given their features. The biased subsampling is corrected by a post-hoc analytic adjustment to the parameters. The method is simple and requires one parallelizable scan over the full data set. Standard case-control sampling is inconsistent under model misspecification for the population risk-minimizing coefficients $\theta^*$. By contrast, our estimator is consistent for $\theta^*$ provided that the pilot estimate is. Moreover, under correct specification and with a consistent, independent pilot estimate, our estimator has exactly twice the asymptotic variance of the full-sample MLE - even if the selected subsample comprises a miniscule fraction of the full data set, as happens when the original data are severely imbalanced. The factor of two improves to $1+\frac{1}{c}$ if we multiply the baseline acceptance probabilities by $c>1$ (and weight points with acceptance probability greater than 1), taking roughly $\frac{1+c}{2}$ times as many data points into the subsample. Experiments on simulated and real data show that our method can substantially outperform standard case-control subsampling. version:2
arxiv-1409-6448 | HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection | http://arxiv.org/abs/1409.6448 | id:1409.6448 author:Bo Han, Bo He, Tingting Sun, Mengmeng Ma, Amaury Lendasse category:cs.CV cs.LG  published:2014-09-23 summary:In this paper, we propose a novel method for fast face recognition called L1/2 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing L1/2 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gabor-feature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate. version:1
arxiv-1408-2004 | RMSE-ELM: Recursive Model based Selective Ensemble of Extreme Learning Machines for Robustness Improvement | http://arxiv.org/abs/1408.2004 | id:1408.2004 author:Bo Han, Bo He, Mengmeng Ma, Tingting Sun, Tianhong Yan, Amaury Lendasse category:cs.LG cs.NE  published:2014-08-09 summary:Extreme learning machine (ELM) as an emerging branch of shallow networks has shown its excellent generalization and fast learning speed. However, for blended data, the robustness of ELM is weak because its weights and biases of hidden nodes are set randomly. Moreover, the noisy data exert a negative effect. To solve this problem, a new framework called RMSE-ELM is proposed in this paper. It is a two-layer recursive model. In the first layer, the framework trains lots of ELMs in different groups concurrently, then employs selective ensemble to pick out an optimal set of ELMs in each group, which can be merged into a large group of ELMs called candidate pool. In the second layer, selective ensemble is recursively used on candidate pool to acquire the final ensemble. In the experiments, we apply UCI blended datasets to confirm the robustness of our new approach in two key aspects (mean square error and standard deviation). The space complexity of our method is increased to some degree, but the results have shown that RMSE-ELM significantly improves robustness with slightly computational time compared with representative methods (ELM, OP-ELM, GASEN-ELM, GASEN-BP and E-GASEN). It becomes a potential framework to solve robustness issue of ELM for high-dimensional blended data in the future. version:3
arxiv-1304-0023 | The two-dimensional Gabor function adapted to natural image statistics: An analytical model of simple-cell responses in the early visual system | http://arxiv.org/abs/1304.0023 | id:1304.0023 author:Peter Loxley category:cs.CV  published:2013-03-29 summary:The two-dimensional Gabor function is adapted to natural image statistics by learning the joint distribution of the Gabor function parameters. The joint distribution is then approximated to yield an analytical model of simple-cell receptive fields. Adapting a basis of Gabor functions is found to take an order of magnitude less computation than learning an equivalent non-parameterized basis. Derived learning rules are shown to be capable of adapting Gabor parameters to the statistics of images of man-made and natural environments. Learning is found to be most pronounced in three Gabor parameters that represent the size, aspect-ratio, and spatial frequency of the two-dimensional Gabor function. These three parameters are characterized by non-uniform marginal distributions with heavy tails -- most likely due to scale invariance in natural images -- and all three parameters are strongly correlated: resulting in a basis of multiscale Gabor functions with similar aspect-ratios, and size-dependent spatial frequencies. The Gabor orientation and phase parameters do not appear to gain anything from learning over natural images. Different tuning strategies are found by controlling learning through the Gabor parameter learning rates. Two opposing strategies include well-resolved orientation and well-resolved spatial frequency. On image reconstruction, a basis of Gabor functions with fitted marginal distributions is shown to significantly outperform a basis of Gabor functions generated from uniformly sampled parameters. An additional increase in performance results when the strong correlations are included. However, the best analytical model does not yet achieve the performance of the learned model. A comparison with estimates for biological simple cells shows that the Gabor function adapted to natural image statistics correctly predicts some key receptive field properties. version:3
arxiv-1404-4923 | Unified Structured Learning for Simultaneous Human Pose Estimation and Garment Attribute Classification | http://arxiv.org/abs/1404.4923 | id:1404.4923 author:Jie Shen, Guangcan Liu, Jia Chen, Yuqiang Fang, Jianbin Xie, Yong Yu, Shuicheng Yan category:cs.CV  published:2014-04-19 summary:In this paper, we utilize structured learning to simultaneously address two intertwined problems: human pose estimation (HPE) and garment attribute classification (GAC), which are valuable for a variety of computer vision and multimedia applications. Unlike previous works that usually handle the two problems separately, our approach aims to produce a jointly optimal estimation for both HPE and GAC via a unified inference procedure. To this end, we adopt a preprocessing step to detect potential human parts from each image (i.e., a set of "candidates") that allows us to have a manageable input space. In this way, the simultaneous inference of HPE and GAC is converted to a structured learning problem, where the inputs are the collections of candidate ensembles, the outputs are the joint labels of human parts and garment attributes, and the joint feature representation involves various cues such as pose-specific features, garment-specific features, and cross-task features that encode correlations between human parts and garment attributes. Furthermore, we explore the "strong edge" evidence around the potential human parts so as to derive more powerful representations for oriented human parts. Such evidences can be seamlessly integrated into our structured learning model as a kind of energy function, and the learning process could be performed by standard structured Support Vector Machines (SVM) algorithm. However, the joint structure of the two problems is a cyclic graph, which hinders efficient inference. To resolve this issue, we compute instead approximate optima by using an iterative procedure, where in each iteration the variables of one problem are fixed. In this way, satisfactory solutions can be efficiently computed by dynamic programming. Experimental results on two benchmark datasets show the state-of-the-art performance of our approach. version:3
arxiv-1407-1610 | Analyzing the Performance of Multilayer Neural Networks for Object Recognition | http://arxiv.org/abs/1407.1610 | id:1407.1610 author:Pulkit Agrawal, Ross Girshick, Jitendra Malik category:cs.CV cs.NE  published:2014-07-07 summary:In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems. version:2
arxiv-1409-6235 | Detecting People in Cubist Art | http://arxiv.org/abs/1409.6235 | id:1409.6235 author:Shiry Ginosar, Daniel Haas, Timothy Brown, Jitendra Malik category:cs.CV  published:2014-09-22 summary:Although the human visual system is surprisingly robust to extreme distortion when recognizing objects, most evaluations of computer object detection methods focus only on robustness to natural form deformations such as people's pose changes. To determine whether algorithms truly mirror the flexibility of human vision, they must be compared against human vision at its limits. For example, in Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization, to the point that human vision often fails to recognize them. In this paper, we evaluate existing object detection methods on these abstract renditions of objects, comparing human annotators to four state-of-the-art object detectors on a corpus of Picasso paintings. Our results demonstrate that while human perception significantly outperforms current methods, human perception and part-based models exhibit a similarly graceful degradation in object detection performance as the objects become increasingly abstract and fragmented, corroborating the theory of part-based object representation in the brain. version:1
arxiv-1409-6179 | Expectation Propagation | http://arxiv.org/abs/1409.6179 | id:1409.6179 author:Jack Raymond, Andre Manoel, Manfred Opper category:stat.ML  published:2014-09-22 summary:Variational inference is a powerful concept that underlies many iterative approximation algorithms; expectation propagation, mean-field methods and belief propagations were all central themes at the school that can be perceived from this unifying framework. The lectures of Manfred Opper introduce the archetypal example of Expectation Propagation, before establishing the connection with the other approximation methods. Corrections by expansion about the expectation propagation are then explained. Finally some advanced inference topics and applications are explored in the final sections. version:1
arxiv-1310-3099 | A Bayesian Network View on Acoustic Model-Based Techniques for Robust Speech Recognition | http://arxiv.org/abs/1310.3099 | id:1310.3099 author:Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann category:cs.LG cs.CL stat.ML  published:2013-10-11 summary:This article provides a unifying Bayesian network view on various approaches for acoustic model adaptation, missing feature, and uncertainty decoding that are well-known in the literature of robust automatic speech recognition. The representatives of these classes can often be deduced from a Bayesian network that extends the conventional hidden Markov models used in speech recognition. These extensions, in turn, can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors. By converting the observation models into a Bayesian network representation, we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to new formulations for certain approaches. The generic Bayesian perspective provided in this contribution thus highlights structural differences and similarities between the analyzed approaches. version:2
arxiv-1409-6111 | Distributed Clustering and Learning Over Networks | http://arxiv.org/abs/1409.6111 | id:1409.6111 author:Xiaochuan Zhao, Ali H. Sayed category:math.OC cs.LG cs.MA cs.SY stat.ML  published:2014-09-22 summary:Distributed processing over networks relies on in-network processing and cooperation among neighboring agents. Cooperation is beneficial when agents share a common objective. However, in many applications agents may belong to different clusters that pursue different objectives. Then, indiscriminate cooperation will lead to undesired results. In this work, we propose an adaptive clustering and learning scheme that allows agents to learn which neighbors they should cooperate with and which other neighbors they should ignore. In doing so, the resulting algorithm enables the agents to identify their clusters and to attain improved learning and estimation accuracy over networks. We carry out a detailed mean-square analysis and assess the error probabilities of Types I and II, i.e., false alarm and mis-detection, for the clustering mechanism. Among other results, we establish that these probabilities decay exponentially with the step-sizes so that the probability of correct clustering can be made arbitrarily close to one. version:1
arxiv-1408-3810 | Action Classification with Locality-constrained Linear Coding | http://arxiv.org/abs/1408.3810 | id:1408.3810 author:Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian category:cs.CV  published:2014-08-17 summary:We propose an action classification algorithm which uses Locality-constrained Linear Coding (LLC) to capture discriminative information of human body variations in each spatiotemporal subsequence of a video sequence. Our proposed method divides the input video into equally spaced overlapping spatiotemporal subsequences, each of which is decomposed into blocks and then cells. We use the Histogram of Oriented Gradient (HOG3D) feature to encode the information in each cell. We justify the use of LLC for encoding the block descriptor by demonstrating its superiority over Sparse Coding (SC). Our sequence descriptor is obtained via a logistic regression classifier with L2 regularization. We evaluate and compare our algorithm with ten state-of-the-art algorithms on five benchmark datasets. Experimental results show that, on average, our algorithm gives better accuracy than these ten algorithms. version:2
arxiv-1408-3809 | HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition | http://arxiv.org/abs/1408.3809 | id:1408.3809 author:Hossein Rahmani, Arif Mahmood, Du Q. Huynh, Ajmal Mian category:cs.CV  published:2014-08-17 summary:Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which change significantly with viewpoint. In contrast, we directly process the pointclouds and propose a new technique for action recognition which is more robust to noise, action speed and viewpoint variations. Our technique consists of a novel descriptor and keypoint detection algorithm. The proposed descriptor is extracted at a point by encoding the Histogram of Oriented Principal Components (HOPC) within an adaptive spatio-temporal support volume around that point. Based on this descriptor, we present a novel method to detect Spatio-Temporal Key-Points (STKPs) in 3D pointcloud sequences. Experimental results show that the proposed descriptor and STKP detector outperform state-of-the-art algorithms on three benchmark human activity datasets. We also introduce a new multiview public dataset and show the robustness of our proposed method to viewpoint variations. version:4
arxiv-1409-6070 | Spatially-sparse convolutional neural networks | http://arxiv.org/abs/1409.6070 | id:1409.6070 author:Benjamin Graham category:cs.CV cs.NE  published:2014-09-22 summary:Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks. Motivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%. Although pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100. version:1
arxiv-1406-5706 | On the Maximum Entropy Property of the First-Order Stable Spline Kernel and its Implications | http://arxiv.org/abs/1406.5706 | id:1406.5706 author:Francesca Paola Carli category:math.ST cs.LG stat.ML stat.TH  published:2014-06-22 summary:A new nonparametric approach for system identification has been recently proposed where the impulse response is seen as the realization of a zero--mean Gaussian process whose covariance, the so--called stable spline kernel, guarantees that the impulse response is almost surely stable. Maximum entropy properties of the stable spline kernel have been pointed out in the literature. In this paper we provide an independent proof that relies on the theory of matrix extension problems in the graphical model literature and leads to a closed form expression for the inverse of the first order stable spline kernel as well as to a new factorization in the form $UWU^\top$ with $U$ upper triangular and $W$ diagonal. Interestingly, all first--order stable spline kernels share the same factor $U$ and $W$ admits a closed form representation in terms of the kernel hyperparameter, making the factorization computationally inexpensive. Maximum likelihood properties of the stable spline kernel are also highlighted. These results can be applied both to improve the stability and to reduce the computational complexity associated with the computation of stable spline estimators. version:2
arxiv-1409-6046 | Approximation errors of online sparsification criteria | http://arxiv.org/abs/1409.6046 | id:1409.6046 author:Paul Honeine category:stat.ML cs.CV cs.IT cs.LG cs.NE math.IT  published:2014-09-21 summary:Many machine learning frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a sparsification scheme in order to address the online learning paradigm. For this purpose, several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sample that can be well represented by the already contributing samples, an operation with excessive computational complexity. Several computationally efficient sparsification criteria have been introduced in the literature, such as the distance, the coherence and the Babel criteria. In this paper, we provide a framework that connects these sparsification criteria to the issue of approximating samples, by deriving theoretical bounds on the approximation errors. Moreover, we investigate the error of approximating any feature, by proposing upper-bounds on the approximation error for each of the aforementioned sparsification criteria. Two classes of features are described in detail, the empirical mean and the principal axes in the kernel principal component analysis. version:1
arxiv-1409-6045 | Analyzing sparse dictionaries for online learning with kernels | http://arxiv.org/abs/1409.6045 | id:1409.6045 author:Paul Honeine category:stat.ML cs.CV cs.IT cs.LG math.IT  published:2014-09-21 summary:Many signal processing and machine learning methods share essentially the same linear-in-the-parameter model, with as many parameters as available samples as in kernel-based machines. Sparse approximation is essential in many disciplines, with new challenges emerging in online learning with kernels. To this end, several sparsity measures have been proposed in the literature to quantify sparse dictionaries and constructing relevant ones, the most prolific ones being the distance, the approximation, the coherence and the Babel measures. In this paper, we analyze sparse dictionaries based on these measures. By conducting an eigenvalue analysis, we show that these sparsity measures share many properties, including the linear independence condition and inducing a well-posed optimization problem. Furthermore, we prove that there exists a quasi-isometry between the parameter (i.e., dual) space and the dictionary's induced feature space. version:1
arxiv-1409-6041 | Domain Adaptive Neural Networks for Object Recognition | http://arxiv.org/abs/1409.6041 | id:1409.6041 author:Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang category:cs.CV cs.NE  published:2014-09-21 summary:We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks. version:1
arxiv-1409-6023 | A High-Level Model of Neocortical Feedback Based on an Event Window Segmentation Algorithm | http://arxiv.org/abs/1409.6023 | id:1409.6023 author:Jerry R. Van Aken category:cs.NE I.2.7  published:2014-09-21 summary:The author previously presented an event window segmentation (EWS) algorithm [5] that uses purely statistical methods to learn to recognize recurring patterns in an input stream of events. In the following discussion, the EWS algorithm is first extended to make predictions about future events. Next, this extended algorithm is used to construct a high-level, simplified model of a neocortical hierarchy. An event stream enters at the bottom of the hierarchy, and drives processing activity upward in the hierarchy. Successively higher regions in the hierarchy learn to recognize successively deeper levels of patterns in these events as they propagate from the bottom of the hierarchy. The lower levels in the hierarchy use the predictions from the levels above to strengthen their own predictions. A C++ source code listing of the model implementation and test program is included as an appendix. version:1
arxiv-1109-3745 | A KdV-like advection-dispersion equation with some remarkable properties | http://arxiv.org/abs/1109.3745 | id:1109.3745 author:Abhijit Sen, Dilip P. Ahalpara, Anantanarayanan Thyagaraja, Govind S. Krishnaswami category:nlin.PS cs.NE math.AP physics.flu-dyn  published:2011-09-17 summary:We discuss a new non-linear PDE, u_t + (2 u_xx/u) u_x = epsilon u_xxx, invariant under scaling of dependent variable and referred to here as SIdV. It is one of the simplest such translation and space-time reflection-symmetric first order advection-dispersion equations. This PDE (with dispersion coefficient unity) was discovered in a genetic programming search for equations sharing the KdV solitary wave solution. It provides a bridge between non-linear advection, diffusion and dispersion. Special cases include the mKdV and linear dispersive equations. We identify two conservation laws, though initial investigations indicate that SIdV does not follow from a polynomial Lagrangian of the KdV sort. Nevertheless, it possesses solitary and periodic travelling waves. Moreover, numerical simulations reveal recurrence properties usually associated with integrable systems. KdV and SIdV are the simplest in an infinite dimensional family of equations sharing the KdV solitary wave. SIdV and its generalizations may serve as a testing ground for numerical and analytical techniques and be a rich source for further explorations. version:3
arxiv-1409-5887 | Capturing "attrition intensifying" structural traits from didactic interaction sequences of MOOC learners | http://arxiv.org/abs/1409.5887 | id:1409.5887 author:Tanmay Sinha, Nan Li, Patrick Jermann, Pierre Dillenbourg category:cs.CY cs.LG cs.SI  published:2014-09-20 summary:This work is an attempt to discover hidden structural configurations in learning activity sequences of students in Massive Open Online Courses (MOOCs). Leveraging combined representations of video clickstream interactions and forum activities, we seek to fundamentally understand traits that are predictive of decreasing engagement over time. Grounded in the interdisciplinary field of network science, we follow a graph based approach to successfully extract indicators of active and passive MOOC participation that reflect persistence and regularity in the overall interaction footprint. Using these rich educational semantics, we focus on the problem of predicting student attrition, one of the major highlights of MOOC literature in the recent years. Our results indicate an improvement over a baseline ngram based approach in capturing "attrition intensifying" features from the learning activities that MOOC learners engage in. Implications for some compelling future research are discussed. version:1
arxiv-1306-2672 | R3MC: A Riemannian three-factor algorithm for low-rank matrix completion | http://arxiv.org/abs/1306.2672 | id:1306.2672 author:B. Mishra, R. Sepulchre category:math.OC cs.LG  published:2013-06-11 summary:We exploit the versatile framework of Riemannian optimization on quotient manifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rank matrix completion. The underlying search space of fixed-rank matrices is endowed with a novel Riemannian metric that is tailored to the least-squares cost. Numerical comparisons suggest that R3MC robustly outperforms state-of-the-art algorithms across different problem instances, especially those that combine scarcely sampled and ill-conditioned data. version:2
arxiv-1409-5834 | Tight Error Bounds for Structured Prediction | http://arxiv.org/abs/1409.5834 | id:1409.5834 author:Amir Globerson, Tim Roughgarden, David Sontag, Cafer Yildirim category:cs.LG cs.DS stat.ML  published:2014-09-19 summary:Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is typically done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. Intuitively, the more pairwise terms are used, the better the expected accuracy. However, there is currently no theoretical account of this intuition. This paper takes a significant step in this direction. We formulate the problem as classifying the vertices of a known graph $G=(V,E)$, where the vertices and edges of the graph are labelled and correlate semi-randomly with the ground truth. We show that the prospects for achieving low expected Hamming error depend on the structure of the graph $G$ in interesting ways. For example, if $G$ is a very poor expander, like a path, then large expected Hamming error is inevitable. Our main positive result shows that, for a wide class of graphs including 2D grid graphs common in machine vision applications, there is a polynomial-time algorithm with small and information-theoretically near-optimal expected error. Our results provide a first step toward a theoretical justification for the empirical success of the efficient approximate inference algorithms that are used for structured prediction in models where exact inference is intractable. version:1
arxiv-1409-5729 | Hyperspectral and Multispectral Image Fusion based on a Sparse Representation | http://arxiv.org/abs/1409.5729 | id:1409.5729 author:Qi Wei, José Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV  published:2014-09-19 summary:This paper presents a variational based approach to fusing hyperspectral and multispectral images. The fusion process is formulated as an inverse problem whose solution is the target image assumed to live in a much lower dimensional subspace. A sparse regularization term is carefully designed, relying on a decomposition of the scene on a set of dictionaries. The dictionary atoms and the corresponding supports of active coding coefficients are learned from the observed images. Then, conditionally on these dictionaries and supports, the fusion problem is solved via alternating optimization with respect to the target image (using the alternating direction method of multipliers) and the coding coefficients. Simulation results demonstrate the efficiency of the proposed algorithm when compared with the state-of-the-art fusion methods. version:1
arxiv-1409-5557 | Statistical Estimation: From Denoising to Sparse Regression and Hidden Cliques | http://arxiv.org/abs/1409.5557 | id:1409.5557 author:Eric W. Tramel, Santhosh Kumar, Andrei Giurgiu, Andrea Montanari category:cs.IT math.IT stat.ML  published:2014-09-19 summary:These notes review six lectures given by Prof. Andrea Montanari on the topic of statistical estimation for linear models. The first two lectures cover the principles of signal recovery from linear measurements in terms of minimax risk. Subsequent lectures demonstrate the application of these principles to several practical problems in science and engineering. Specifically, these topics include denoising of error-laden signals, recovery of compressively sensed signals, reconstruction of low-rank matrices, and also the discovery of hidden cliques within large networks. version:1
arxiv-1409-0685 | Effective Spectral Unmixing via Robust Representation and Learning-based Sparsity | http://arxiv.org/abs/1409.0685 | id:1409.0685 author:Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Chunhong Pan category:cs.CV  published:2014-09-02 summary:Hyperspectral unmixing (HU) plays a fundamental role in a wide range of hyperspectral applications. It is still challenging due to the common presence of outlier channels and the large solution space. To address the above two issues, we propose a novel model by emphasizing both robust representation and learning-based sparsity. Specifically, we apply the $\ell_{2,1}$-norm to measure the representation error, preventing outlier channels from dominating our objective. In this way, the side effects of outlier channels are greatly relieved. Besides, we observe that the mixed level of each pixel varies over image grids. Based on this observation, we exploit a learning-based sparsity method to simultaneously learn the HU results and a sparse guidance map. Via this guidance map, the sparsity constraint in the $\ell_{p}\!\left(\!0\!<\! p\!\leq\!1\right)$-norm is adaptively imposed according to the learnt mixed level of each pixel. Compared with state-of-the-art methods, our model is better suited to the real situation, thus expected to achieve better HU results. The resulted objective is highly non-convex and non-smooth, and so it is hard to optimize. As a profound theoretical contribution, we propose an efficient algorithm to solve it. Meanwhile, the convergence proof and the computational complexity analysis are systematically provided. Extensive evaluations verify that our method is highly promising for the HU task---it achieves very accurate guidance maps and much better HU results compared with state-of-the-art methods. version:3
arxiv-1403-4224 | Learning Negative Mixture Models by Tensor Decompositions | http://arxiv.org/abs/1403.4224 | id:1403.4224 author:Guillaume Rabusseau, François Denis category:cs.LG  published:2014-03-17 summary:This work considers the problem of estimating the parameters of negative mixture models, i.e. mixture models that possibly involve negative weights. The contributions of this paper are as follows. (i) We show that every rational probability distributions on strings, a representation which occurs naturally in spectral learning, can be computed by a negative mixture of at most two probabilistic automata (or HMMs). (ii) We propose a method to estimate the parameters of negative mixture models having a specific tensor structure in their low order observable moments. Building upon a recent paper on tensor decompositions for learning latent variable models, we extend this work to the broader setting of tensors having a symmetric decomposition with positive and negative weights. We introduce a generalization of the tensor power method for complex valued tensors, and establish theoretical convergence guarantees. (iii) We show how our approach applies to negative Gaussian mixture models, for which we provide some experiments. version:2
arxiv-1409-5502 | Using crowdsourcing system for creating site-specific statistical machine translation engine | http://arxiv.org/abs/1409.5502 | id:1409.5502 author:Alexander Kalinin, George Savchenko category:cs.CL  published:2014-09-19 summary:A crowdsourcing translation approach is an effective tool for globalization of site content, but it is also an important source of parallel linguistic data. For the given site, processed with a crowdsourcing system, a sentence-aligned corpus can be fetched, which covers a very narrow domain of terminology and language patterns - a site-specific domain. These data can be used for training and estimation of site-specific statistical machine translation engine version:1
arxiv-1206-0387 | When Does a Mixture of Products Contain a Product of Mixtures? | http://arxiv.org/abs/1206.0387 | id:1206.0387 author:Guido F. Montufar, Jason Morton category:stat.ML math.CO G.3  published:2012-06-02 summary:We derive relations between theoretical properties of restricted Boltzmann machines (RBMs), popular machine learning models which form the building blocks of deep learning models, and several natural notions from discrete mathematics and convex geometry. We give implications and equivalences relating RBM-representable probability distributions, perfectly reconstructible inputs, Hamming modes, zonotopes and zonosets, point configurations in hyperplane arrangements, linear threshold codes, and multi-covering numbers of hypercubes. As a motivating application, we prove results on the relative representational power of mixtures of product distributions and products of mixtures of pairs of product distributions (RBMs) that formally justify widely held intuitions about distributed representations. In particular, we show that a mixture of products requiring an exponentially larger number of parameters is needed to represent the probability distributions which can be obtained as products of mixtures. version:5
arxiv-1311-0686 | Particle Metropolis-Hastings using gradient and Hessian information | http://arxiv.org/abs/1311.0686 | id:1311.0686 author:Johan Dahlin, Fredrik Lindsten, Thomas B. Schön category:stat.CO stat.ML  published:2013-11-04 summary:Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference in nonlinear state space models by combining Markov chain Monte Carlo (MCMC) and particle filtering. The latter is used to estimate the intractable likelihood. In its original formulation, PMH makes use of a marginal MCMC proposal for the parameters, typically a Gaussian random walk. However, this can lead to a poor exploration of the parameter space and an inefficient use of the generated particles. We propose a number of alternative versions of PMH that incorporate gradient and Hessian information about the posterior into the proposal. This information is more or less obtained as a byproduct of the likelihood estimation. Indeed, we show how to estimate the required information using a fixed-lag particle smoother, with a computational cost growing linearly in the number of particles. We conclude that the proposed methods can: (i) decrease the length of the burn-in phase, (ii) increase the mixing of the Markov chain at the stationary phase, and (iii) make the proposal distribution scale invariant which simplifies tuning. version:4
arxiv-1409-5402 | SAME but Different: Fast and High-Quality Gibbs Parameter Estimation | http://arxiv.org/abs/1409.5402 | id:1409.5402 author:Huasha Zhao, Biye Jiang, John Canny category:cs.LG stat.ML K.3.2; D.1.3  published:2014-09-18 summary:Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference methods. SAME (State Augmentation for Marginal Estimation) \cite{Doucet99,Doucet02} is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sampling. SAME can be viewed as cooling the posterior parameter distribution and allows annealed search for the MAP parameters, often yielding very high quality (lower loss) estimates. But it does so at the expense of additional samples per iteration and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sampling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better quality. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (online Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to implement and should be applicable to many other models. version:1
arxiv-1409-5400 | Visual Landmark Recognition from Internet Photo Collections: A Large-Scale Evaluation | http://arxiv.org/abs/1409.5400 | id:1409.5400 author:Tobias Weyand, Bastian Leibe category:cs.CV  published:2014-09-18 summary:The task of a visual landmark recognition system is to identify photographed buildings or objects in query photos and to provide the user with relevant information on them. With their increasing coverage of the world's landmark buildings and objects, Internet photo collections are now being used as a source for building such systems in a fully automatic fashion. This process typically consists of three steps: clustering large amounts of images by the objects they depict; determining object names from user-provided tags; and building a robust, compact, and efficient recognition index. To this date, however, there is little empirical information on how well current approaches for those steps perform in a large-scale open-set mining and recognition task. Furthermore, there is little empirical information on how recognition performance varies for different types of landmark objects and where there is still potential for improvement. With this paper, we intend to fill these gaps. Using a dataset of 500k images from Paris, we analyze each component of the landmark recognition pipeline in order to answer the following questions: How many and what kinds of objects can be discovered automatically? How can we best use the resulting image clusters to recognize the object in a query? How can the object be efficiently represented in memory for recognition? How reliably can semantic information be extracted? And finally: What are the limiting factors in the resulting pipeline from query to semantics? We evaluate how different choices of methods and parameters for the individual pipeline steps affect overall system performance and examine their effects for different query categories such as buildings, paintings or sculptures. version:1
arxiv-1409-5391 | Fused Lasso Additive Model | http://arxiv.org/abs/1409.5391 | id:1409.5391 author:Ashley Petersen, Daniela Witten, Noah Simon category:stat.ME stat.ML  published:2014-09-18 summary:We consider the problem of predicting an outcome variable using $p$ covariates that are measured on $n$ independent observations, in the setting in which flexible and interpretable fits are desirable. We propose the fused lasso additive model (FLAM), in which each additive function is estimated to be piecewise constant with a small number of adaptively-chosen knots. FLAM is the solution to a convex optimization problem, for which a simple algorithm with guaranteed convergence to the global optimum is provided. FLAM is shown to be consistent in high dimensions, and an unbiased estimator of its degrees of freedom is proposed. We evaluate the performance of FLAM in a simulation study and on two data sets. version:1
arxiv-1409-5330 | Learning and approximation capability of orthogonal super greedy algorithm | http://arxiv.org/abs/1409.5330 | id:1409.5330 author:Jian Fang, Shaobo Lin, Zongben Xu category:cs.LG F.2.2  published:2014-09-18 summary:We consider the approximation capability of orthogonal super greedy algorithms (OSGA) and its applications in supervised learning. OSGA is concerned with selecting more than one atoms in each iteration step, which, of course, greatly reduces the computational burden when compared with the conventional orthogonal greedy algorithm (OGA). We prove that even for function classes that are not the convex hull of the dictionary, OSGA does not degrade the approximation capability of OGA provided the dictionary is incoherent. Based on this, we deduce a tight generalization error bound for OSGA learning. Our results show that in the realm of supervised learning, OSGA provides a possibility to further reduce the computational burden of OGA in the premise of maintaining its prominent generalization capability. version:1
arxiv-1409-5326 | Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX): Comparing multi-electrode recordings from simulated and biological mammalian cortical tissue | http://arxiv.org/abs/1409.5326 | id:1409.5326 author:Richard J. Tomsett, Matt Ainsworth, Alexander Thiele, Mehdi Sanayei, Xing Chen, Alwin Gieselmann, Miles A. Whittington, Mark O. Cunningham, Marcus Kaiser category:q-bio.NC cs.AI cs.NE  published:2014-09-18 summary:Local field potentials (LFPs) sampled with extracellular electrodes are frequently used as a measure of population neuronal activity. However, relating such measurements to underlying neuronal behaviour and connectivity is non-trivial. To help study this link, we developed the Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX). We first identified a reduced neuron model that retained the spatial and frequency filtering characteristics of extracellular potentials from neocortical neurons. We then developed VERTEX as an easy-to-use Matlab tool for simulating LFPs from large populations (>100 000 neurons). A VERTEX-based simulation successfully reproduced features of the LFPs from an in vitro multi-electrode array recording of macaque neocortical tissue. Our model, with virtual electrodes placed anywhere in 3D, allows direct comparisons with the in vitro recording setup. We envisage that VERTEX will stimulate experimentalists, clinicians, and computational neuroscientists to use models to understand the mechanisms underlying measured brain dynamics in health and disease. version:1
arxiv-1407-6439 | Feature Engineering for Knowledge Base Construction | http://arxiv.org/abs/1407.6439 | id:1407.6439 author:Christopher Ré, Amir Abbas Sadeghian, Zifei Shan, Jaeho Shin, Feiran Wang, Sen Wu, Ce Zhang category:cs.DB cs.CL cs.LG  published:2014-07-24 summary:Knowledge base construction (KBC) is the process of populating a knowledge base, i.e., a relational database together with inference rules, with information extracted from documents and structured sources. KBC blurs the distinction between two traditional database problems, information extraction and information integration. For the last several years, our group has been building knowledge bases with scientific collaborators. Using our approach, we have built knowledge bases that have comparable and sometimes better quality than those constructed by human volunteers. In contrast to these knowledge bases, which took experts a decade or more human years to construct, many of our projects are constructed by a single graduate student. Our approach to KBC is based on joint probabilistic inference and learning, but we do not see inference as either a panacea or a magic bullet: inference is a tool that allows us to be systematic in how we construct, debug, and improve the quality of such systems. In addition, inference allows us to construct these systems in a more loosely coupled way than traditional approaches. To support this idea, we have built the DeepDive system, which has the design goal of letting the user "think about features---not algorithms." We think of DeepDive as declarative in that one specifies what they want but not how to get it. We describe our approach with a focus on feature engineering, which we argue is an understudied problem relative to its importance to end-to-end quality. version:3
arxiv-1407-7906 | How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation | http://arxiv.org/abs/1407.7906 | id:1407.7906 author:Yoshua Bengio category:cs.LG  published:2014-07-29 summary:We propose to exploit {\em reconstruction} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations. version:3
arxiv-1409-5230 | Deep Regression for Face Alignment | http://arxiv.org/abs/1409.5230 | id:1409.5230 author:Baoguang Shi, Xiang Bai, Wenyu Liu, Jingdong Wang category:cs.CV  published:2014-09-18 summary:In this paper, we present a deep regression approach for face alignment. The deep architecture consists of a global layer and multi-stage local layers. We apply the back-propagation algorithm with the dropout strategy to jointly optimize the regression parameters. We show that the resulting deep regressor gradually and evenly approaches the true facial landmarks stage by stage, avoiding the tendency to yield over-strong early stage regressors while over-weak later stage regressors. Experimental results show that our approach achieves the state-of-the-art version:1
arxiv-1409-5188 | Fingerprint Classification Based on Depth Neural Network | http://arxiv.org/abs/1409.5188 | id:1409.5188 author:Ruxin Wang, Congying Han, Yanping Wu, Tiande Guo category:cs.CV  published:2014-09-18 summary:Fingerprint classification is an effective technique for reducing the candidate numbers of fingerprints in the stage of matching in automatic fingerprint identification system (AFIS). In recent years, deep learning is an emerging technology which has achieved great success in many fields, such as image processing, natural language processing and so on. In this paper, we only choose the orientation field as the input feature and adopt a new method (stacked sparse autoencoders) based on depth neural network for fingerprint classification. For the four-class problem, we achieve a classification of 93.1 percent using the depth network structure which has three hidden layers (with 1.8% rejection) in the NIST-DB4 database. And then we propose a novel method using two classification probabilities for fuzzy classification which can effectively enhance the accuracy of classification. By only adjusting the probability threshold, we get the accuracy of classification is 96.1% (setting threshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (setting threshold is 0.95). Using the fuzzy method, we obtain higher accuracy than other methods. version:1
arxiv-1409-5178 | Model-based Kernel Sum Rule | http://arxiv.org/abs/1409.5178 | id:1409.5178 author:Yu Nishiyama, Motonobu Kanagawa, Arthur Gretton, Kenji Fukumizu category:stat.ML stat.ME  published:2014-09-18 summary:In this study, we enrich the framework of nonparametric kernel Bayesian inference via the flexible incorporation of certain probabilistic models, such as additive Gaussian noise models. Nonparametric inference expressed in terms of kernel means, which is called kernel Bayesian inference, has been studied using basic rules such as the kernel sum rule (KSR), kernel chain rule, kernel product rule, and kernel Bayes' rule (KBR). However, the current framework used for kernel Bayesian inference deals only with nonparametric inference and it cannot allow inference when combined with probabilistic models. In this study, we introduce a novel KSR, called model-based KSR (Mb-KSR), which exploits the knowledge obtained from some probabilistic models of conditional distributions. The incorporation of Mb-KSR into nonparametric kernel Bayesian inference facilitates more flexible kernel Bayesian inference than nonparametric inference. We focus on combinations of Mb-KSR, Non-KSR, and KBR, and we propose a filtering algorithm for state space models, which combines nonparametric learning of the observation process using kernel means and additive Gaussian noise models of the transition dynamics. The idea of the Mb-KSR for additive Gaussian noise models can be extended to more general noise model cases, including a conjugate pair with a positive-definite kernel and a probabilistic model. version:1
arxiv-1409-5165 | A Method for Stopping Active Learning Based on Stabilizing Predictions and the Need for User-Adjustable Stopping | http://arxiv.org/abs/1409.5165 | id:1409.5165 author:Michael Bloodgood, K. Vijay-Shanker category:cs.LG cs.CL stat.ML  published:2014-09-17 summary:A survey of existing methods for stopping active learning (AL) reveals the needs for methods that are: more widely applicable; more aggressive in saving annotations; and more stable across changing datasets. A new method for stopping AL based on stabilizing predictions is presented that addresses these needs. Furthermore, stopping methods are required to handle a broad range of different annotation/performance tradeoff valuations. Despite this, the existing body of work is dominated by conservative methods with little (if any) attention paid to providing users with control over the behavior of stopping methods. The proposed method is shown to fill a gap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior. version:1
arxiv-1406-2984 | Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation | http://arxiv.org/abs/1406.2984 | id:1406.2984 author:Jonathan Tompson, Arjun Jain, Yann LeCun, Christoph Bregler category:cs.CV  published:2014-06-11 summary:This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques. version:2
arxiv-1409-6689 | Visual Words for Automatic Lip-Reading | http://arxiv.org/abs/1409.6689 | id:1409.6689 author:Ahmad Basheer Hassanat category:cs.CV  published:2014-09-17 summary:Lip reading is used to understand or interpret speech without hearing it, a technique especially mastered by people with hearing difficulties. The ability to lip read enables a person with a hearing impairment to communicate with others and to engage in social activities, which otherwise would be difficult. Recent advances in the fields of computer vision, pattern recognition, and signal processing has led to a growing interest in automating this challenging task of lip reading. Indeed, automating the human ability to lip read, a process referred to as visual speech recognition, could open the door for other novel applications. This thesis investigates various issues faced by an automated lip-reading system and proposes a novel "visual words" based approach to automatic lip reading. The proposed approach includes a novel automatic face localisation scheme and a lip localisation method. version:1
arxiv-1408-3944 | Down-Sampling coupled to Elastic Kernel Machines for Efficient Recognition of Isolated Gestures | http://arxiv.org/abs/1408.3944 | id:1408.3944 author:Pierre-François Marteau, Sylvie Gibet, Clement Reverdy category:cs.LG cs.HC  published:2014-08-18 summary:In the field of gestural action recognition, many studies have focused on dimensionality reduction along the spatial axis, to reduce both the variability of gestural sequences expressed in the reduced space, and the computational complexity of their processing. It is noticeable that very few of these methods have explicitly addressed the dimensionality reduction along the time axis. This is however a major issue with regard to the use of elastic distances characterized by a quadratic complexity. To partially fill this apparent gap, we present in this paper an approach based on temporal down-sampling associated to elastic kernel machine learning. We experimentally show, on two data sets that are widely referenced in the domain of human gesture recognition, and very different in terms of quality of motion capture, that it is possible to significantly reduce the number of skeleton frames while maintaining a good recognition rate. The method proves to give satisfactory results at a level currently reached by state-of-the-art methods on these data sets. The computational complexity reduction makes this approach eligible for real-time applications. version:2
arxiv-1409-5009 | Distance Shrinkage and Euclidean Embedding via Regularized Kernel Estimation | http://arxiv.org/abs/1409.5009 | id:1409.5009 author:Luwan Zhang, Grace Wahba, Ming Yuan category:stat.ML math.ST stat.ME stat.TH  published:2014-09-17 summary:Although recovering an Euclidean distance matrix from noisy observations is a common problem in practice, how well this could be done remains largely unknown. To fill in this void, we study a simple distance matrix estimate based upon the so-called regularized kernel estimate. We show that such an estimate can be characterized as simply applying a constant amount of shrinkage to all observed pairwise distances. This fact allows us to establish risk bounds for the estimate implying that the true distances can be estimated consistently in an average sense as the number of objects increases. In addition, such a characterization suggests an efficient algorithm to compute the distance matrix estimator, as an alternative to the usual second order cone programming known not to scale well for large problems. Numerical experiments and an application in visualizing the diversity of Vpu protein sequences from a recent HIV-1 study further demonstrate the practical merits of the proposed method. version:1
arxiv-1409-4988 | An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery | http://arxiv.org/abs/1409.4988 | id:1409.4988 author:Filippo Maria Bianchi, Enrico Maiorino, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.LG cs.DC cs.MA  published:2014-09-17 summary:We propose a multi-agent algorithm able to automatically discover relevant regularities in a given dataset, determining at the same time the set of configurations of the adopted parametric dissimilarity measure yielding compact and separated clusters. Each agent operates independently by performing a Markovian random walk on a suitable weighted graph representation of the input dataset. Such a weighted graph representation is induced by the specific parameter configuration of the dissimilarity measure adopted by the agent, which searches and takes decisions autonomously for one cluster at a time. Results show that the algorithm is able to discover parameter configurations that yield a consistent and interpretable collection of clusters. Moreover, we demonstrate that our algorithm shows comparable performances with other similar state-of-the-art algorithms when facing specific clustering problems. version:1
arxiv-1409-4995 | Adaptive Tag Selection for Image Annotation | http://arxiv.org/abs/1409.4995 | id:1409.4995 author:Xixi He, Xirong Li, Gang Yang, Jieping Xu, Qin Jin category:cs.CV  published:2014-09-17 summary:Not all tags are relevant to an image, and the number of relevant tags is image-dependent. Although many methods have been proposed for image auto-annotation, the question of how to determine the number of tags to be selected per image remains open. The main challenge is that for a large tag vocabulary, there is often a lack of ground truth data for acquiring optimal cutoff thresholds per tag. In contrast to previous works that pre-specify the number of tags to be selected, we propose in this paper adaptive tag selection. The key insight is to divide the vocabulary into two disjoint subsets, namely a seen set consisting of tags having ground truth available for optimizing their thresholds and a novel set consisting of tags without any ground truth. Such a division allows us to estimate how many tags shall be selected from the novel set according to the tags that have been selected from the seen set. The effectiveness of the proposed method is justified by our participation in the ImageCLEF 2014 image annotation task. On a set of 2,065 test images with ground truth available for 207 tags, the benchmark evaluation shows that compared to the popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tag selection achieves a higher F-score of 0.223. Moreover, by treating the underlying image annotation system as a black box, the new method can be used as an easy plug-in to boost the performance of existing systems. version:1
arxiv-1407-1291 | Reinforcement Learning Based Algorithm for the Maximization of EV Charging Station Revenue | http://arxiv.org/abs/1407.1291 | id:1407.1291 author:Stoyan Dimitrov, Redouane Lguensat category:cs.CE cs.LG math.OC stat.AP  published:2014-07-04 summary:This paper presents an online reinforcement learning based application which increases the revenue of one particular electric vehicles (EV) station, connected to a renewable source of energy. Moreover, the proposed application adapts to changes in the trends of the station's average number of customers and their types. Most of the parameters in the model are simulated stochastically and the algorithm used is a Q-learning algorithm. A computer simulation was implemented which demonstrates and confirms the utility of the model. version:2
arxiv-1409-4958 | Tensity Research Based on the Information of Eye Movement | http://arxiv.org/abs/1409.4958 | id:1409.4958 author:Yi Wang category:cs.RO cs.CV  published:2014-09-17 summary:User's mental state is concerned gradually, during the interaction course of human robot. As the measurement and identification method of psychological state, tension, has certain practical significance role. At presents there is no suitable method of measuring the tension. Firstly, sum up some availability of eye movement index. And then parameters extraction on eye movement characteristics of normal illumination is studied, including the location of the face, eyes location, access to the pupil diameter, the eye pupil center characteristic parameters. And with the judgment of the tension in eye images, extract exact information of gaze direction. Finally, through the experiment to prove the proposed method is effective. version:1
arxiv-1310-3609 | Scalable Verification of Markov Decision Processes | http://arxiv.org/abs/1310.3609 | id:1310.3609 author:Axel Legay, Sean Sedwards, Louis-Marie Traonouez category:cs.DS cs.DC cs.LG cs.LO  published:2013-10-14 summary:Markov decision processes (MDP) are useful to model concurrent process optimisation problems, but verifying them with numerical methods is often intractable. Existing approximative approaches do not scale well and are limited to memoryless schedulers. Here we present the basis of scalable verification for MDPSs, using an O(1) memory representation of history-dependent schedulers. We thus facilitate scalable learning techniques and the use of massively parallel verification. version:4
arxiv-1409-4936 | Ensembles of Random Sphere Cover Classifiers | http://arxiv.org/abs/1409.4936 | id:1409.4936 author:Anthony Bagnall, Reda Younsi category:cs.LG cs.AI stat.ML  published:2014-09-17 summary:We propose and evaluate alternative ensemble schemes for a new instance based learning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses instances into spheres, then bases classification on distance to spheres rather than distance to instances. The randomised nature of RSC makes it ideal for use in ensembles. We propose two ensemble methods tailored to the RSC classifier; $\alpha \beta$RSE, an ensemble based on instance resampling and $\alpha$RSSE, a subspace ensemble. We compare $\alpha \beta$RSE and $\alpha$RSSE to tree based ensembles on a set of UCI datasets and demonstrates that RSC ensembles perform significantly better than some of these ensembles, and not significantly worse than the others. We demonstrate via a case study on six gene expression data sets that $\alpha$RSSE can outperform other subspace ensemble methods on high dimensional data when used in conjunction with an attribute filter. Finally, we perform a set of Bias/Variance decomposition experiments to analyse the source of improvement in comparison to a base classifier. version:1
arxiv-1409-4928 | Statistical inference with probabilistic graphical models | http://arxiv.org/abs/1409.4928 | id:1409.4928 author:Angélique Drémeau, Christophe Schülke, Yingying Xu, Devavrat Shah category:cs.LG stat.ML  published:2014-09-17 summary:These are notes from the lecture of Devavrat Shah given at the autumn school "Statistical Physics, Optimization, Inference, and Message-Passing Algorithms", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka Zdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from Politecnico Torino. This lecture of Devavrat Shah (MIT) covers the basics of inference and learning. It explains how inference problems are represented within structures known as graphical models. The theoretical basis of the belief propagation algorithm is then explained and derived. This lecture sets the stage for generalizations and applications of message passing algorithms. version:1
arxiv-1401-5888 | Efficiently Detecting Overlapping Communities through Seeding and Semi-Supervised Learning | http://arxiv.org/abs/1401.5888 | id:1401.5888 author:Changxing Shang, Shengzhong Feng, Zhongying Zhao, Jianping Fan category:cs.SI cs.LG physics.soc-ph  published:2014-01-23 summary:Seeding then expanding is a commonly used scheme to discover overlapping communities in a network. Most seeding methods are either too complex to scale to large networks or too simple to select high-quality seeds, and the non-principled functions used by most expanding methods lead to poor performance when applied to diverse networks. This paper proposes a new method that transforms a network into a corpus where each edge is treated as a document, and all nodes of the network are treated as terms of the corpus. An effective seeding method is also proposed that selects seeds as a training set, then a principled expanding method based on semi-supervised learning is applied to classify edges. We compare our new algorithm with four other community detection algorithms on a wide range of synthetic and empirical networks. Experimental results show that the new algorithm can significantly improve clustering performance in most cases. Furthermore, the time complexity of the new algorithm is linear to the number of edges, and this low complexity makes the new algorithm scalable to large networks. version:4
arxiv-1406-6818 | Face Identification with Second-Order Pooling | http://arxiv.org/abs/1406.6818 | id:1406.6818 author:Fumin Shen, Chunhua Shen, Heng Tao Shen category:cs.CV  published:2014-06-26 summary:Automatic face recognition has received significant performance improvement by developing specialised facial image representations. On the other hand, generic object recognition has rarely been applied to the face recognition. Spatial pyramid pooling of features encoded by an over-complete dictionary has been the key component of many state-of-the-art image classification systems. Inspired by its success, in this work we develop a new face image representation method inspired by the second-order pooling in Carreira et al. [1], which was originally proposed for image segmentation. The proposed method differs from the previous methods in that, we encode the densely extracted local patches by a small-size dictionary; and the facial image signatures are obtained by pooling the second-order statistics of the encoded features. We show the importance of pooling on encoded features, which is bypassed by the original second-order pooling method to avoid the high computational cost. Equipped with a simple linear classifier, the proposed method outperforms the state-of-the-art face identification performance by large margins. For example, on the LFW databases, the proposed method performs better than the previous best by around 13% accuracy. version:2
arxiv-1406-6811 | Face Image Classification by Pooling Raw Features | http://arxiv.org/abs/1406.6811 | id:1406.6811 author:Fumin Shen, Chunhua Shen, Heng Tao Shen category:cs.CV  published:2014-06-26 summary:We propose a very simple, efficient yet surprisingly effective feature extraction method for face recognition (about 20 lines of Matlab code), which is mainly inspired by spatial pyramid pooling in generic image classification. We show that features formed by simply pooling local patches over a multi-level pyramid, coupled with a linear classifier, can significantly outperform most recent face recognition methods. The simplicity of our feature extraction procedure is demonstrated by the fact that no learning is involved (except PCA whitening). We show that, multi-level spatial pooling and dense extraction of multi-scale patches play critical roles in face image classification. The extracted facial features can capture strong structural information of individual faces with no label information being used. We also find that, pre-processing on local image patches such as contrast normalization can have an important impact on the classification accuracy. In particular, on the challenging face recognition datasets of FERET and LFW-a, our method improves previous best results by more than 10% and 20%, respectively. version:2
arxiv-1403-6355 | Continuum limit of total variation on point clouds | http://arxiv.org/abs/1403.6355 | id:1403.6355 author:Nicolás García Trillos, Dejan Slepčev category:math.ST math.AP stat.ML stat.TH  published:2014-03-25 summary:We consider point clouds obtained as random samples of a measure on a Euclidean domain. A graph representing the point cloud is obtained by assigning weights to edges based on the distance between the points they connect. Our goal is to develop mathematical tools needed to study the consistency, as the number of available data points increases, of graph-based machine learning algorithms for tasks such as clustering. In particular, we study when is the cut capacity, and more generally total variation, on these graphs a good approximation of the perimeter (total variation) in the continuum setting. We address this question in the setting of $\Gamma$-convergence. We obtain almost optimal conditions on the scaling, as number of points increases, of the size of the neighborhood over which the points are connected by an edge for the $\Gamma$-convergence to hold. Taking the limit is enabled by a transportation based metric which allows to suitably compare functionals defined on different point clouds. version:3
arxiv-1409-4842 | Going Deeper with Convolutions | http://arxiv.org/abs/1409.4842 | id:1409.4842 author:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich category:cs.CV  published:2014-09-17 summary:We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection. version:1
arxiv-1409-4835 | Taking into Account the Differences between Actively and Passively Acquired Data: The Case of Active Learning with Support Vector Machines for Imbalanced Datasets | http://arxiv.org/abs/1409.4835 | id:1409.4835 author:Michael Bloodgood, K. Vijay-Shanker category:cs.LG cs.CL stat.ML  published:2014-09-17 summary:Actively sampled data can have very different characteristics than passively sampled data. Therefore, it's promising to investigate using different inference procedures during AL than are used during passive learning (PL). This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks. The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL. version:1
arxiv-1407-7131 | Your click decides your fate: Inferring Information Processing and Attrition Behavior from MOOC Video Clickstream Interactions | http://arxiv.org/abs/1407.7131 | id:1407.7131 author:Tanmay Sinha, Patrick Jermann, Nan Li, Pierre Dillenbourg category:cs.HC cs.LG  published:2014-07-26 summary:In this work, we explore video lecture interaction in Massive Open Online Courses (MOOCs), which is central to student learning experience on these educational platforms. As a research contribution, we operationalize video lecture clickstreams of students into cognitively plausible higher level behaviors, and construct a quantitative information processing index, which can aid instructors to better understand MOOC hurdles and reason about unsatisfactory learning outcomes. Our results illustrate how such a metric inspired by cognitive psychology can help answer critical questions regarding students' engagement, their future click interactions and participation trajectories that lead to in-video & course dropouts. Implications for research and practice are discussed version:2
arxiv-1409-4747 | Anomaly Detection Based on Indicators Aggregation | http://arxiv.org/abs/1409.4747 | id:1409.4747 author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG  published:2014-09-16 summary:Automatic anomaly detection is a major issue in various areas. Beyond mere detection, the identification of the source of the problem that produced the anomaly is also essential. This is particularly the case in aircraft engine health monitoring where detecting early signs of failure (anomalies) and helping the engine owner to implement efficiently the adapted maintenance operations (fixing the source of the anomaly) are of crucial importance to reduce the costs attached to unscheduled maintenance. This paper introduces a general methodology that aims at classifying monitoring signals into normal ones and several classes of abnormal ones. The main idea is to leverage expert knowledge by generating a very large number of binary indicators. Each indicator corresponds to a fully parametrized anomaly detector built from parametric anomaly scores designed by experts. A feature selection method is used to keep only the most discriminant indicators which are used at inputs of a Naive Bayes classifier. This give an interpretable classifier based on interpretable anomaly detectors whose parameters have been optimized indirectly by the selection process. The proposed methodology is evaluated on simulated data designed to reproduce some of the anomaly types observed in real world engines. version:1
arxiv-1407-0880 | Anomaly Detection Based on Aggregation of Indicators | http://arxiv.org/abs/1407.0880 | id:1407.0880 author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG  published:2014-07-03 summary:Automatic anomaly detection is a major issue in various areas. Beyond mere detection, the identification of the origin of the problem that produced the anomaly is also essential. This paper introduces a general methodology that can assist human operators who aim at classifying monitoring signals. The main idea is to leverage expert knowledge by generating a very large number of indicators. A feature selection method is used to keep only the most discriminant indicators which are used as inputs of a Naive Bayes classifier. The parameters of the classifier have been optimized indirectly by the selection process. Simulated data designed to reproduce some of the anomaly types observed in real world engines. version:2
arxiv-1301-4976 | Supervised Classification Using Sparse Fisher's LDA | http://arxiv.org/abs/1301.4976 | id:1301.4976 author:Irina Gaynanova, James G. Booth, Martin T. Wells category:stat.ML stat.CO  published:2013-01-21 summary:It is well known that in a supervised classification setting when the number of features is smaller than the number of observations, Fisher's linear discriminant rule is asymptotically Bayes. However, there are numerous modern applications where classification is needed in the high-dimensional setting. Naive implementation of Fisher's rule in this case fails to provide good results because the sample covariance matrix is singular. Moreover, by constructing a classifier that relies on all features the interpretation of the results is challenging. Our goal is to provide robust classification that relies only on a small subset of important features and accounts for the underlying correlation structure. We apply a lasso-type penalty to the discriminant vector to ensure sparsity of the solution and use a shrinkage type estimator for the covariance matrix. The resulting optimization problem is solved using an iterative coordinate ascent algorithm. Furthermore, we analyze the effect of nonconvexity on the sparsity level of the solution and highlight the difference between the penalized and the constrained versions of the problem. The simulation results show that the proposed method performs favorably in comparison to alternatives. The method is used to classify leukemia patients based on DNA methylation features. version:2
arxiv-1409-4698 | A Mixtures-of-Experts Framework for Multi-Label Classification | http://arxiv.org/abs/1409.4698 | id:1409.4698 author:Charmgil Hong, Iyad Batal, Milos Hauskrecht category:cs.LG I.2.6  published:2014-09-16 summary:We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods. version:1
arxiv-1409-4627 | DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF Features | http://arxiv.org/abs/1409.4627 | id:1409.4627 author:Petra Budikova, Jan Botorek, Michal Batko, Pavel Zezula category:cs.IR cs.CV  published:2014-09-16 summary:This paper constitutes an extension to the report on DISA-MU team participation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as published in [3]. Specifically, we introduce a new similarity search component that was implemented into the system, report on the results achieved by utilizing this component, and analyze the influence of different similarity search parameters on the annotation quality. version:1
arxiv-1409-5079 | Predictive Capacity of Meteorological Data - Will it rain tomorrow | http://arxiv.org/abs/1409.5079 | id:1409.5079 author:Bilal Ahmed category:cs.LG  published:2014-09-16 summary:With the availability of high precision digital sensors and cheap storage medium, it is not uncommon to find large amounts of data collected on almost all measurable attributes, both in nature and man-made habitats. Weather in particular has been an area of keen interest for researchers to develop more accurate and reliable prediction models. This paper presents a set of experiments which involve the use of prevalent machine learning techniques to build models to predict the day of the week given the weather data for that particular day i.e. temperature, wind, rain etc., and test their reliability across four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The results provide a comparison of accuracy of these machine learning techniques and their reliability to predict the day of the week by analysing the weather data. We then apply the models to predict weather conditions based on the available data. version:1
arxiv-1409-4617 | The Role of Emotions in Propagating Brands in Social Networks | http://arxiv.org/abs/1409.4617 | id:1409.4617 author:Ronald Hochreiter, Christoph Waldhauser category:cs.SI cs.CL stat.ML  published:2014-09-16 summary:A key aspect of word of mouth marketing are emotions. Emotions in texts help propagating messages in conventional advertising. In word of mouth scenarios, emotions help to engage consumers and incite to propagate the message further. While the function of emotions in offline marketing in general and word of mouth marketing in particular is rather well understood, online marketing can only offer a limited view on the function of emotions. In this contribution we seek to close this gap. We therefore investigate how emotions function in social media. To do so, we collected more than 30,000 brand marketing messages from the Google+ social networking site. Using state of the art computational linguistics classifiers, we compute the sentiment of these messages. Starting out with Poisson regression-based baseline models, we seek to replicate earlier findings using this large data set. We extend upon earlier research by computing multi-level mixed effects models that compare the function of emotions across different industries. We find that while the well known notion of activating emotions propagating messages holds in general for our data as well. But there are significant differences between the observed industries. version:1
arxiv-1409-4566 | Multivariate Comparison of Classification Algorithms | http://arxiv.org/abs/1409.4566 | id:1409.4566 author:Olcay Taner Yildiz, Ethem Alpaydin category:stat.ML cs.LG  published:2014-09-16 summary:Statistical tests that compare classification algorithms are univariate and use a single performance measure, e.g., misclassification error, $F$ measure, AUC, and so on. In multivariate tests, comparison is done using multiple measures simultaneously. For example, error is the sum of false positives and false negatives and a univariate test on error cannot make a distinction between these two sources, but a 2-variate test can. Similarly, instead of combining precision and recall in $F$ measure, we can have a 2-variate test on (precision, recall). We use Hotelling's multivariate $T^2$ test for comparing two algorithms, and when we have three or more algorithms we use the multivariate analysis of variance (MANOVA) followed by pairwise post hoc tests. In our experiments, we see that multivariate tests have higher power than univariate tests, that is, they can detect differences that univariate tests cannot. We also discuss how multivariate analysis allows us to automatically extract performance measures that best distinguish the behavior of multiple algorithms. version:1
arxiv-1409-4565 | Improving files availability for BitTorrent using a diffusion model | http://arxiv.org/abs/1409.4565 | id:1409.4565 author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NI cs.NE  published:2014-09-16 summary:The BitTorrent mechanism effectively spreads file fragments by copying the rarest fragments first. We propose to apply a mathematical model for the diffusion of fragments on a P2P in order to take into account both the effects of peer distances and the changing availability of peers while time goes on. Moreover, we manage to provide a forecast on the availability of a torrent thanks to a neural network that models the behaviour of peers on the P2P system. The combination of the mathematical model and the neural network provides a solution for choosing file fragments that need to be copied first, in order to ensure their continuous availability, counteracting possible disconnections by some peers. version:1
arxiv-1409-4559 | A Combined Method Of Fractal And GLCM Features For MRI And CT Scan Images Classification | http://arxiv.org/abs/1409.4559 | id:1409.4559 author:Redouan Korchiyne, Sidi Mohamed Farssi, Abderrahmane Sbihi, Rajaa Touahni, Mustapha Tahiri Alaoui category:cs.CV  published:2014-09-16 summary:Fractal analysis has been shown to be useful in image processing for characterizing shape and gray-scale complexity. The fractal feature is a compact descriptor used to give a numerical measure of the degree of irregularity of the medical images. This descriptor property does not give ownership of the local image structure. In this paper, we present a combination of this parameter based on Box Counting with GLCM Features. This powerful combination has proved good results especially in classification of medical texture from MRI and CT Scan images of trabecular bone. This method has the potential to improve clinical diagnostics tests for osteoporosis pathologies. version:1
arxiv-1310-2273 | Semidefinite Programming Based Preconditioning for More Robust Near-Separable Nonnegative Matrix Factorization | http://arxiv.org/abs/1310.2273 | id:1310.2273 author:Nicolas Gillis, Stephen A. Vavasis category:stat.ML cs.LG math.OC  published:2013-10-08 summary:Nonnegative matrix factorization (NMF) under the separability assumption can provably be solved efficiently, even in the presence of noise, and has been shown to be a powerful technique in document classification and hyperspectral unmixing. This problem is referred to as near-separable NMF and requires that there exists a cone spanned by a small subset of the columns of the input nonnegative matrix approximately containing all columns. In this paper, we propose a preconditioning based on semidefinite programming making the input matrix well-conditioned. This in turn can improve significantly the performance of near-separable NMF algorithms which is illustrated on the popular successive projection algorithm (SPA). The new preconditioned SPA is provably more robust to noise, and outperforms SPA on several synthetic data sets. We also show how an active-set method allow us to apply the preconditioning on large-scale real-world hyperspectral images. version:2
arxiv-1409-4757 | Collapsed Variational Bayes Inference of Infinite Relational Model | http://arxiv.org/abs/1409.4757 | id:1409.4757 author:Katsuhiko Ishiguro, Issei Sato, Naonori Ueda category:cs.LG stat.ML  published:2014-09-16 summary:The Infinite Relational Model (IRM) is a probabilistic model for relational data clustering that partitions objects into clusters based on observed relationships. This paper presents Averaged CVB (ACVB) solutions for IRM, convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. We first derive ordinary CVB and CVB0 for IRM based on the lower bound maximization. CVB solutions yield deterministic iterative procedures for inferring IRM given the truncated number of clusters. Our proposal includes CVB0 updates of hyperparameters including the concentration parameter of the Dirichlet Process, which has not been studied in the literature. To make the CVB more practically useful, we further study the CVB inference in two aspects. First, we study the convergence issues and develop a convergence-guaranteed algorithm for any CVB-based inferences called ACVB, which enables automatic convergence detection and frees non-expert practitioners from difficult and costly manual monitoring of inference processes. Second, we present a few techniques for speeding up IRM inferences. In particular, we describe the linear time inference of CVB0, allowing the IRM for larger relational data uses. The ACVB solutions of IRM showed comparable or better performance compared to existing inference methods in experiments, and provide deterministic, faster, and easier convergence detection. version:1
arxiv-1407-6705 | A Robust and Efficient Method for Improving Accuracy of License Plate Characters Recognition | http://arxiv.org/abs/1407.6705 | id:1407.6705 author:Reza Azad, Hamid Reza Shayegh, Hamed Amiri category:cs.CV  published:2014-07-24 summary:License Plate Recognition (LPR) plays an important role on the traffic monitoring and parking management. A robust and efficient method for enhancing accuracy of license plate characters recognition based on K Nearest Neighbours (K-NN) classifier is presented in this paper. The system first prepares a contour form of the extracted character, then the angle and distance feature information about the character is extracted and finally K-NN classifier is used to character recognition. Angle and distance features of a character have been computed based on distribution of points on the bitmap image of character. In K-NN method, the Euclidean distance between testing point and reference points is calculated in order to find the k-nearest neighbours. We evaluated our method on the available dataset that contain 1200 sample. Using 70% samples for training, we tested our method on whole samples and obtained 99% correct recognition rate.Further, we achieved average 99.41% accuracy using three/strategy validation technique on 1200 dataset. version:2
arxiv-1409-4504 | Voting for Deceptive Opinion Spam Detection | http://arxiv.org/abs/1409.4504 | id:1409.4504 author:Tao Wang, Hua Zhu category:cs.CL cs.SI  published:2014-09-16 summary:Consumers' purchase decisions are increasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic, to deceive the readers. Existing approaches mainly focus on developing automatic supervised learning based methods to help users identify deceptive opinion spams. This work, we used the LSI and Sprinkled LSI technique to reduce the dimension for deception detection. We make our contribution to demonstrate what LSI is capturing in latent semantic space and reveal how deceptive opinions can be recognized automatically from truthful opinions. Finally, we proposed a voting scheme which integrates different approaches to further improve the classification performance. version:1
arxiv-1409-4481 | Real-time Crowd Tracking using Parameter Optimized Mixture of Motion Models | http://arxiv.org/abs/1409.4481 | id:1409.4481 author:Aniket Bera, David Wolinski, Julien Pettré, Dinesh Manocha category:cs.CV  published:2014-09-16 summary:We present a novel, real-time algorithm to track the trajectory of each pedestrian in moderately dense crowded scenes. Our formulation is based on an adaptive particle-filtering scheme that uses a combination of various multi-agent heterogeneous pedestrian simulation models. We automatically compute the optimal parameters for each of these different models based on prior tracked data and use the best model as motion prior for our particle-filter based tracking algorithm. We also use our "mixture of motion models" for adaptive particle selection and accelerate the performance of the online tracking algorithm. The motion model parameter estimation is formulated as an optimization problem, and we use an approach that solves this combinatorial optimization problem in a model independent manner and hence scalable to any multi-agent pedestrian motion model. We evaluate the performance of our approach on different crowd video datasets and highlight the improvement in accuracy over homogeneous motion models and a baseline mean-shift based tracker. In practice, our formulation can compute trajectories of tens of pedestrians on a multi-core desktop CPU in in real time and offer higher accuracy as compared to prior real time pedestrian tracking algorithms. version:1
arxiv-1409-4469 | Convolutional Networks for Image Processing by Coupled Oscillator Arrays | http://arxiv.org/abs/1409.4469 | id:1409.4469 author:Dmitri E. Nikonov, Ian A. Young, George I. Bourianoff category:nlin.PS cond-mat.dis-nn cs.CV  published:2014-09-15 summary:A coupled oscillator array is shown to approximate convolutions with Gabor filters for image processing tasks. Pixelated image fragments and filter functions are converted to voltages, differenced, and input into a corresponding array of weakly coupled Voltage Controlled Oscillators (VCOs). This is referred to as Frequency Shift Keying (FSK). Upon synchronization of the array, the common node amplitude provides a metric for the degree of match between the image fragment and the filter function. The optimal oscillator parameters for synchronization are determined and favor a moderate value of the Q-factor. version:1
arxiv-1409-4366 | The Randomized Causation Coefficient | http://arxiv.org/abs/1409.4366 | id:1409.4366 author:David Lopez-Paz, Krikamol Muandet, Benjamin Recht category:stat.ML  published:2014-09-15 summary:We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions regarding the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, and without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's "Fast Causation Coefficient Challenge" competition, with which we won the fastest code prize and ranked third in the overall leaderboard. version:1
arxiv-1409-4364 | Computational Algorithms Based on the Paninian System to Process Euphonic Conjunctions for Word Searches | http://arxiv.org/abs/1409.4364 | id:1409.4364 author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL  published:2014-09-15 summary:Searching for words in Sanskrit E-text is a problem that is accompanied by complexities introduced by features of Sanskrit such as euphonic conjunctions or sandhis. A word could occur in an E-text in a transformed form owing to the operation of rules of sandhi. Simple word search would not yield these transformed forms of the word. Further, there is no search engine in the literature that can comprehensively search for words in Sanskrit E-texts taking euphonic conjunctions into account. This work presents an optimal binary representational schema for letters of the Sanskrit alphabet along with algorithms to efficiently process the sandhi rules of Sanskrit grammar. The work further presents an algorithm that uses the sandhi processing algorithm to perform a comprehensive word search on E-text. version:1
arxiv-1409-4354 | A Binary Schema and Computational Algorithms to Process Vowel-based Euphonic Conjunctions for Word Searches | http://arxiv.org/abs/1409.4354 | id:1409.4354 author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL  published:2014-09-15 summary:Comprehensively searching for words in Sanskrit E-text is a non-trivial problem because words could change their forms in different contexts. One such context is sandhi or euphonic conjunctions, which cause a word to change owing to the presence of adjacent letters or words. The change wrought by these possible conjunctions can be so significant in Sanskrit that a simple search for the word in its given form alone can significantly reduce the success level of the search. This work presents a representational schema that represents letters in a binary format and reduces Paninian rules of euphonic conjunctions to simple bit set-unset operations. The work presents an efficient algorithm to process vowel-based sandhis using this schema. It further presents another algorithm that uses the sandhi processor to generate the possible transformed word forms of a given word to use in a comprehensive word search. version:1
arxiv-1409-4349 | On the optimality of shape and data representation in the spectral domain | http://arxiv.org/abs/1409.4349 | id:1409.4349 author:Yonathan Aflalo, Haim Brezis, Ron Kimmel category:cs.CV  published:2014-09-15 summary:A proof of the optimality of the eigenfunctions of the Laplace-Beltrami operator (LBO) in representing smooth functions on surfaces is provided and adapted to the field of applied shape and data analysis. It is based on the Courant-Fischer min-max principle adapted to our case. % The theorem we present supports the new trend in geometry processing of treating geometric structures by using their projection onto the leading eigenfunctions of the decomposition of the LBO. Utilisation of this result can be used for constructing numerically efficient algorithms to process shapes in their spectrum. We review a couple of applications as possible practical usage cases of the proposed optimality criteria. % We refer to a scale invariant metric, which is also invariant to bending of the manifold. This novel pseudo-metric allows constructing an LBO by which a scale invariant eigenspace on the surface is defined. We demonstrate the efficiency of an intermediate metric, defined as an interpolation between the scale invariant and the regular one, in representing geometric structures while capturing both coarse and fine details. Next, we review a numerical acceleration technique for classical scaling, a member of a family of flattening methods known as multidimensional scaling (MDS). There, the optimality is exploited to efficiently approximate all geodesic distances between pairs of points on a given surface, and thereby match and compare between almost isometric surfaces. Finally, we revisit the classical principal component analysis (PCA) definition by coupling its variational form with a Dirichlet energy on the data manifold. By pairing the PCA with the LBO we can handle cases that go beyond the scope defined by the observation set that is handled by regular PCA. version:1
arxiv-1407-6496 | Novel and Fast Algorithm for Extracting License Plate Location Based on Edge Analysis | http://arxiv.org/abs/1407.6496 | id:1407.6496 author:Reza Azad, Mohammad Baghdadi category:cs.CV  published:2014-07-24 summary:Nowadays in developing or developed countries, the Intelligent Transportation System (ITS) technology has attracted so much attention to itself. License Plate Recognition (LPR) systems have many applications in ITSs, such as the payment of parking fee, controlling the traffic volume, traffic data collection, etc. This paper presents a new and fast method for license plate extraction based on edge analysis. our proposed method consist of four stage, which are edge detection, non-useable edge and noise removing, edge analysis and morphology-based license plate extraction. In the result part, the proposed algorithm is applied on vehicle database and the accuracy rate reached 98%. From the experimental results it is shown that the proposed method gives fairly acceptable level of accuracy for practical license plate recognition system. version:2
arxiv-1409-4205 | Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers | http://arxiv.org/abs/1409.4205 | id:1409.4205 author:B. Conejo, N. Komodakis, S. Leprince, J. P. Avouac category:cs.CV  published:2014-09-15 summary:We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach relies on a multi-scale pruning scheme that is able to progressively reduce the solution space by use of a novel strategy based on a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. version:1
arxiv-1006-4910 | 3D Visual Tracking with Particle and Kalman Filters | http://arxiv.org/abs/1006.4910 | id:1006.4910 author:Burak Bayramli category:cs.CV  published:2010-06-25 summary:One of the most visually demonstrable and straightforward uses of filtering is in the field of Computer Vision. In this document we will try to outline the issues encountered while designing and implementing a particle and kalman filter based tracking system. version:2
arxiv-1405-2652 | Selecting Near-Optimal Approximate State Representations in Reinforcement Learning | http://arxiv.org/abs/1405.2652 | id:1405.2652 author:Ronald Ortner, Odalric-Ambrym Maillard, Daniil Ryabko category:cs.LG  published:2014-05-12 summary:We consider a reinforcement learning setting introduced in (Maillard et al., NIPS 2011) where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation. version:6
arxiv-1409-4169 | An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech Synthesis of Sanskrit Verse | http://arxiv.org/abs/1409.4169 | id:1409.4169 author:Rama N., Meenakshi Lakshmanan category:cs.CL  published:2014-09-15 summary:The rendering of Sanskrit poetry from text to speech is a problem that has not been solved before. One reason may be the complications in the language itself. We present unique algorithms based on extensive empirical analysis, to synthesize speech from a given text input of Sanskrit verses. Using a pre-recorded audio units database which is itself tremendously reduced in size compared to the colossal size that would otherwise be required, the algorithms work on producing the best possible, tunefully rendered chanting of the given verse. His would enable the visually impaired and those with reading disabilities to easily access the contents of Sanskrit verses otherwise available only in writing. version:1
arxiv-1108-0631 | Serialising the ISO SynAF Syntactic Object Model | http://arxiv.org/abs/1108.0631 | id:1108.0631 author:Laurent Romary, Amir Zeldes, Florian Zipser category:cs.CL  published:2011-08-02 summary:This paper introduces, an XML format developed to serialise the object model defined by the ISO Syntactic Annotation Framework SynAF. Based on widespread best practices we adapt a popular XML format for syntactic annotation, TigerXML, with additional features to support a variety of syntactic phenomena including constituent and dependency structures, binding, and different node types such as compounds or empty elements. We also define interfaces to other formats and standards including the Morpho-syntactic Annotation Framework MAF and the ISOCat Data Category Registry. Finally a case study of the German Treebank TueBa-D/Z is presented, showcasing the handling of constituent structures, topological fields and coreference annotation in tandem. version:3
arxiv-1409-4155 | Active Metric Learning from Relative Comparisons | http://arxiv.org/abs/1409.4155 | id:1409.4155 author:Sicheng Xiong, Rómer Rosales, Yuanli Pei, Xiaoli Z. Fern category:cs.LG  published:2014-09-15 summary:This work focuses on active learning of distance metrics from relative comparison information. A relative comparison specifies, for a data point triplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to $x_k$. Such constraints, when available, have been shown to be useful toward defining appropriate distance metrics. In real-world applications, acquiring constraints often require considerable human effort. This motivates us to study how to select and query the most useful relative comparisons to achieve effective metric learning with minimum user effort. Given an underlying class concept that is employed by the user to provide such constraints, we present an information-theoretic criterion that selects the triplet whose answer leads to the highest expected gain in information about the classes of a set of examples. Directly applying the proposed criterion requires examining $O(n^3)$ triplets with $n$ instances, which is prohibitive even for datasets of moderate size. We show that a randomized selection strategy can be used to reduce the selection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-size problems. Experiments show that the proposed method consistently outperforms two baseline policies. version:1
arxiv-1409-4139 | A feasible roadmap for developing volumetric probability atlas of localized prostate cancer | http://arxiv.org/abs/1409.4139 | id:1409.4139 author:Liang Zhao, Jianhua Xuan, Yue Wang category:q-bio.QM cs.CV  published:2014-09-15 summary:A statistical volumetric model, showing the probability map of localized prostate cancer within the host anatomical structure, has been developed from 90 optically-imaged surgical specimens. This master model permits an accurate characterization of prostate cancer distribution patterns and an atlas-informed biopsy sampling strategy. The model is constructed by mapping individual prostate models onto a site model, together with localized tumors. An accurate multi-object non-rigid warping scheme is developed based on a mixture of principal-axis registrations. We report our evaluation and pilot studies on the effectiveness of the method and its application to optimizing needle biopsy strategies. version:1
arxiv-1402-5836 | Avoiding pathologies in very deep networks | http://arxiv.org/abs/1402.5836 | id:1402.5836 author:David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani category:stat.ML cs.LG  published:2014-02-24 summary:Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes. version:2
arxiv-1107-2700 | Learning $k$-Modal Distributions via Testing | http://arxiv.org/abs/1107.2700 | id:1107.2700 author:Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio category:cs.DS cs.LG math.ST stat.TH  published:2011-07-13 summary:A $k$-modal probability distribution over the discrete domain $\{1,...,n\}$ is one whose histogram has at most $k$ "peaks" and "valleys." Such distributions are natural generalizations of monotone ($k=0$) and unimodal ($k=1$) probability distributions, which have been intensively studied in probability theory and statistics. In this paper we consider the problem of \emph{learning} (i.e., performing density estimation of) an unknown $k$-modal distribution with respect to the $L_1$ distance. The learning algorithm is given access to independent samples drawn from an unknown $k$-modal distribution $p$, and it must output a hypothesis distribution $\widehat{p}$ such that with high probability the total variation distance between $p$ and $\widehat{p}$ is at most $\epsilon.$ Our main goal is to obtain \emph{computationally efficient} algorithms for this problem that use (close to) an information-theoretically optimal number of samples. We give an efficient algorithm for this problem that runs in time $\mathrm{poly}(k,\log(n),1/\epsilon)$. For $k \leq \tilde{O}(\log n)$, the number of samples used by our algorithm is very close (within an $\tilde{O}(\log(1/\epsilon))$ factor) to being information-theoretically optimal. Prior to this work computationally efficient algorithms were known only for the cases $k=0,1$ \cite{Birge:87b,Birge:97}. A novel feature of our approach is that our learning algorithm crucially uses a new algorithm for \emph{property testing of probability distributions} as a key subroutine. The learning algorithm uses the property tester to efficiently decompose the $k$-modal distribution into $k$ (near-)monotone distributions, which are easier to learn. version:3
arxiv-1409-4095 | Cavlectometry: Towards Holistic Reconstruction of Large Mirror Objects | http://arxiv.org/abs/1409.4095 | id:1409.4095 author:Jonathan Balzer, Daniel Acevedo-Feliz, Stefano Soatto, Sebastian Höfer, Markus Hadwiger, Jürgen Beyerer category:cs.CV  published:2014-09-14 summary:We introduce a method based on the deflectometry principle for the reconstruction of specular objects exhibiting significant size and geometric complexity. A key feature of our approach is the deployment of an Automatic Virtual Environment (CAVE) as pattern generator. To unfold the full power of this extraordinary experimental setup, an optical encoding scheme is developed which accounts for the distinctive topology of the CAVE. Furthermore, we devise an algorithm for detecting the object of interest in raw deflectometric images. The segmented foreground is used for single-view reconstruction, the background for estimation of the camera pose, necessary for calibrating the sensor system. Experiments suggest a significant gain of coverage in single measurements compared to previous methods. To facilitate research on specular surface reconstruction, we will make our data set publicly available. version:1
arxiv-1409-4046 | A New Framework for Retinex based Color Image Enhancement using Particle Swarm Optimization | http://arxiv.org/abs/1409.4046 | id:1409.4046 author:M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu, V. N Manjunath Aradhya category:cs.CV 68T45 H.2.0  published:2014-09-14 summary:A new approach for tuning the parameters of MultiScale Retinex (MSR) based color image enhancement algorithm using a popular optimization method, namely, Particle Swarm Optimization (PSO) is presented in this paper. The image enhancement using MSR scheme heavily depends on parameters such as Gaussian surround space constant, number of scales, gain and offset etc. Selection of these parameters, empirically and its application to MSR scheme to produce inevitable results are the major blemishes. The method presented here results in huge savings of computation time as well as improvement in the visual quality of an image, since the PSO exploited maximizes the MSR parameters. The objective of PSO is to validate the visual quality of the enhanced image iteratively using an effective objective criterion based on entropy and edge information of an image. The PSO method of parameter optimization of MSR scheme achieves a very good quality of reconstructed images, far better than that possible with the other existing methods. Finally, the quality of the enhanced color images obtained by the proposed method are evaluated using novel metric, namely, Wavelet Energy (WE). The experimental results presented show that color images enhanced using the proposed scheme are clearer, more vivid and efficient. version:1
arxiv-1409-4044 | A new approach in machine learning | http://arxiv.org/abs/1409.4044 | id:1409.4044 author:Alain Tapp category:stat.ML cs.LG  published:2014-09-14 summary:In this technical report we presented a novel approach to machine learning. Once the new framework is presented, we will provide a simple and yet very powerful learning algorithm which will be benchmark on various dataset. The framework we proposed is based on booleen circuits; more specifically the classifier produced by our algorithm have that form. Using bits and boolean gates instead of real numbers and multiplication enable the the learning algorithm and classifier to use very efficient boolean vector operations. This enable both the learning algorithm and classifier to be extremely efficient. The accuracy of the classifier we obtain with our framework compares very favorably those produced by conventional techniques, both in terms of efficiency and accuracy. version:1
arxiv-1409-4043 | Design of Novel Algorithm and Architecture for Gaussian Based Color Image Enhancement System for Real Time Applications | http://arxiv.org/abs/1409.4043 | id:1409.4043 author:M. C. Hanumantharaju, M. Ravishankar, D. R. Rameshbabu category:cs.AR cs.CV  published:2014-09-14 summary:This paper presents the development of a new algorithm for Gaussian based color image enhancement system. The algorithm has been designed into architecture suitable for FPGA/ASIC implementation. The color image enhancement is achieved by first convolving an original image with a Gaussian kernel since Gaussian distribution is a point spread function which smoothen the image. Further, logarithm-domain processing and gain/offset corrections are employed in order to enhance and translate pixels into the display range of 0 to 255. The proposed algorithm not only provides better dynamic range compression and color rendition effect but also achieves color constancy in an image. The design exploits high degrees of pipelining and parallel processing to achieve real time performance. The design has been realized by RTL compliant Verilog coding and fits into a single FPGA with a gate count utilization of 321,804. The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148 FPGA device and is capable of processing high resolution color motion pictures of sizes of up to 1600x1200 pixels at the real time video rate of 116 frames per second. This shows that the proposed design would work for not only still images but also for high resolution video sequences. version:1
arxiv-1409-4018 | EquiNMF: Graph Regularized Multiview Nonnegative Matrix Factorization | http://arxiv.org/abs/1409.4018 | id:1409.4018 author:Daniel Hidru, Anna Goldenberg category:cs.LG cs.NA  published:2014-09-14 summary:Nonnegative matrix factorization (NMF) methods have proved to be powerful across a wide range of real-world clustering applications. Integrating multiple types of measurements for the same objects/subjects allows us to gain a deeper understanding of the data and refine the clustering. We have developed a novel Graph-reguarized multiview NMF-based method for data integration called EquiNMF. The parameters for our method are set in a completely automated data-specific unsupervised fashion, a highly desirable property in real-world applications. We performed extensive and comprehensive experiments on multiview imaging data. We show that EquiNMF consistently outperforms other single-view NMF methods used on concatenated data and multi-view NMF methods with different types of regularizations. version:1
arxiv-1409-4014 | Mining Mid-level Features for Action Recognition Based on Effective Skeleton Representation | http://arxiv.org/abs/1409.4014 | id:1409.4014 author:Pichao Wang, Wanqing Li, Philip Ogunbona, Zhimin Gao, Hanling Zhang category:cs.CV  published:2014-09-14 summary:Recently, mid-level features have shown promising performance in computer vision. Mid-level features learned by incorporating class-level information are potentially more discriminative than traditional low-level local features. In this paper, an effective method is proposed to extract mid-level features from Kinect skeletons for 3D human action recognition. Firstly, the orientations of limbs connected by two skeleton joints are computed and each orientation is encoded into one of the 27 states indicating the spatial relationship of the joints. Secondly, limbs are combined into parts and the limb's states are mapped into part states. Finally, frequent pattern mining is employed to mine the most frequent and relevant (discriminative, representative and non-redundant) states of parts in continuous several frames. These parts are referred to as Frequent Local Parts or FLPs. The FLPs allow us to build powerful bag-of-FLP-based action representation. This new representation yields state-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D. version:1
arxiv-1409-4011 | Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces | http://arxiv.org/abs/1409.4011 | id:1409.4011 author:Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, Michael A. Osborne category:stat.ML  published:2014-09-14 summary:In practical Bayesian optimization, we must often search over structures with differing numbers of parameters. For instance, we may wish to search over neural network architectures with an unknown number of layers. To relate performance data gathered for different architectures, we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels. version:1
arxiv-1409-4005 | Sparse Estimation with Strongly Correlated Variables using Ordered Weighted L1 Regularization | http://arxiv.org/abs/1409.4005 | id:1409.4005 author:Mario A. T. Figueiredo, Robert D. Nowak category:stat.ML  published:2014-09-14 summary:This paper studies ordered weighted L1 (OWL) norm regularization for sparse estimation problems with strongly correlated variables. We prove sufficient conditions for clustering based on the correlation/colinearity of variables using the OWL norm, of which the so-called OSCAR is a particular case. Our results extend previous ones for OSCAR in several ways: for the squared error loss, our conditions hold for the more general OWL norm and under weaker assumptions; we also establish clustering conditions for the absolute error loss, which is, as far as we know, a novel result. Furthermore, we characterize the statistical performance of OWL norm regularization for generative models in which certain clusters of regression variables are strongly (even perfectly) correlated, but variables in different clusters are uncorrelated. We show that if the true p-dimensional signal generating the data involves only s of the clusters, then O(s log p) samples suffice to accurately estimate the signal, regardless of the number of coefficients within the clusters. The estimation of s-sparse signals with completely independent variables requires just as many measurements. In other words, using the OWL we pay no price (in terms of the number of measurements) for the presence of strongly correlated variables. version:1
arxiv-1409-3942 | Polarity detection movie reviews in hindi language | http://arxiv.org/abs/1409.3942 | id:1409.3942 author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.CL cs.IR  published:2014-09-13 summary:Nowadays peoples are actively involved in giving comments and reviews on social networking websites and other websites like shopping websites, news websites etc. large number of people everyday share their opinion on the web, results is a large number of user data is collected .users also find it trivial task to read all the reviews and then reached into the decision. It would be better if these reviews are classified into some category so that the user finds it easier to read. Opinion Mining or Sentiment Analysis is a natural language processing task that mines information from various text forms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, user content in Hindi language is also increasing at a rapid rate on the Web. So it is very important to perform opinion mining in Hindi language as well. In this paper a Hindi language opinion mining system is proposed. The system classifies the reviews as positive, negative and neutral for Hindi language. Negation is also handled in the proposed system. Experimental results using reviews of movies show the effectiveness of the system version:1
arxiv-1409-3924 | A study on effectiveness of extreme learning machine | http://arxiv.org/abs/1409.3924 | id:1409.3924 author:Yuguang Wang, Feilong Cao, Yubo Yuan category:cs.NE cs.LG  published:2014-09-13 summary:Extreme learning machine (ELM), proposed by Huang et al., has been shown a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the input weights and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM. version:1
arxiv-1409-3913 | Concurrent Tracking of Inliers and Outliers | http://arxiv.org/abs/1409.3913 | id:1409.3913 author:Jae-Yeong Lee, Wonpil Yu category:cs.CV  published:2014-09-13 summary:In object tracking, outlier is one of primary factors which degrade performance of image-based tracking algorithms. In this respect, therefore, most of the existing methods simply discard detected outliers and pay little or no attention to employing them as an important source of information for motion estimation. We consider outliers as important as inliers for object tracking and propose a motion estimation algorithm based on concurrent tracking of inliers and outliers. Our tracker makes use of pyramidal implementation of the Lucas-Kanade tracker to estimate motion flows of inliers and outliers and final target motion is estimated robustly based on both of these information. Experimental results from challenging benchmark video sequences confirm enhanced tracking performance, showing highly stable target tracking under severe occlusion compared with state-of-the-art algorithms. The proposed algorithm runs at more than 100 frames per second even without using a hardware accelerator, which makes the proposed method more practical and portable. version:1
arxiv-1409-3912 | Parallel Distributed Block Coordinate Descent Methods based on Pairwise Comparison Oracle | http://arxiv.org/abs/1409.3912 | id:1409.3912 author:Kota Matsui, Wataru Kumagai, Takafumi Kanamori category:stat.ML cs.LG  published:2014-09-13 summary:This paper provides a block coordinate descent algorithm to solve unconstrained optimization problems. In our algorithm, computation of function values or gradients is not required. Instead, pairwise comparison of function values is used. Our algorithm consists of two steps; one is the direction estimate step and the other is the search step. Both steps require only pairwise comparison of function values, which tells us only the order of function values over two points. In the direction estimate step, a Newton type search direction is estimated. A computation method like block coordinate descent methods is used with the pairwise comparison. In the search step, a numerical solution is updated along the estimated direction. The computation in the direction estimate step can be easily parallelized, and thus, the algorithm works efficiently to find the minimizer of the objective function. Also, we show an upper bound of the convergence rate. In numerical experiments, we show that our method efficiently finds the optimal solution compared to some existing methods based on the pairwise comparison. version:1
arxiv-1212-3385 | Approximating rational Bezier curves by constrained Bezier curves of arbitrary degree | http://arxiv.org/abs/1212.3385 | id:1212.3385 author:Mao Shi, Jiansong Deng category:math.NA cs.CV  published:2012-12-14 summary:In this paper, we propose a method to obtain a constrained approximation of a rational B\'{e}zier curve by a polynomial B\'{e}zier curve. This problem is reformulated as an approximation problem between two polynomial B\'{e}zier curves based on weighted least-squares method, where weight functions $\rho(t)=\omega(t)$ and $\rho(t)=\omega(t)^{2}$ are studied respectively. The efficiency of the proposed method is tested using some examples. version:4
arxiv-1409-3906 | Structure Preserving Large Imagery Reconstruction | http://arxiv.org/abs/1409.3906 | id:1409.3906 author:Ju Shen, Jianjun Yang, Sami Taha-abusneineh, Bryson Payne, Markus Hitz category:cs.CV  published:2014-09-13 summary:With the explosive growth of web-based cameras and mobile devices, billions of photographs are uploaded to the internet. We can trivially collect a huge number of photo streams for various goals, such as image clustering, 3D scene reconstruction, and other big data applications. However, such tasks are not easy due to the fact the retrieved photos can have large variations in their view perspectives, resolutions, lighting, noises, and distortions. Fur-thermore, with the occlusion of unexpected objects like people, vehicles, it is even more challenging to find feature correspondences and reconstruct re-alistic scenes. In this paper, we propose a structure-based image completion algorithm for object removal that produces visually plausible content with consistent structure and scene texture. We use an edge matching technique to infer the potential structure of the unknown region. Driven by the estimated structure, texture synthesis is performed automatically along the estimated curves. We evaluate the proposed method on different types of images: from highly structured indoor environment to natural scenes. Our experimental results demonstrate satisfactory performance that can be potentially used for subsequent big data processing, such as image localization, object retrieval, and scene reconstruction. Our experiments show that this approach achieves favorable results that outperform existing state-of-the-art techniques. version:1
arxiv-1409-3881 | An Approach to Reducing Annotation Costs for BioNLP | http://arxiv.org/abs/1409.3881 | id:1409.3881 author:Michael Bloodgood, K. Vijay-Shanker category:cs.CL cs.LG stat.ML  published:2014-09-12 summary:There is a broad range of BioNLP tasks for which active learning (AL) can significantly reduce annotation costs and a specific AL algorithm we have developed is particularly effective in reducing annotation costs for these tasks. We have previously developed an AL algorithm called ClosestInitPA that works best with tasks that have the following characteristics: redundancy in training material, burdensome annotation costs, Support Vector Machines (SVMs) work well for the task, and imbalanced datasets (i.e. when set up as a binary classification problem, one class is substantially rarer than the other). Many BioNLP tasks have these characteristics and thus our AL algorithm is a natural approach to apply to BioNLP tasks. version:1
arxiv-1409-3854 | Linear, Deterministic, and Order-Invariant Initialization Methods for the K-Means Clustering Algorithm | http://arxiv.org/abs/1409.3854 | id:1409.3854 author:M. Emre Celebi, Hassan A. Kingravi category:cs.LG cs.CV I.5.3; H.2.8  published:2014-09-12 summary:Over the past five decades, k-means has become the clustering algorithm of choice in many application domains primarily due to its simplicity, time/space efficiency, and invariance to the ordering of the data points. Unfortunately, the algorithm's sensitivity to the initial selection of the cluster centers remains to be its most serious drawback. Numerous initialization methods have been proposed to address this drawback. Many of these methods, however, have time complexity superlinear in the number of data points, which makes them impractical for large data sets. On the other hand, linear methods are often random and/or sensitive to the ordering of the data points. These methods are generally unreliable in that the quality of their results is unpredictable. Therefore, it is common practice to perform multiple runs of such methods and take the output of the run that produces the best results. Such a practice, however, greatly increases the computational requirements of the otherwise highly efficient k-means algorithm. In this chapter, we investigate the empirical performance of six linear, deterministic (non-random), and order-invariant k-means initialization methods on a large and diverse collection of data sets from the UCI Machine Learning Repository. The results demonstrate that two relatively unknown hierarchical initialization methods due to Su and Dy outperform the remaining four methods with respect to two objective effectiveness criteria. In addition, a recent method due to Erisoglu et al. performs surprisingly poorly. version:1
arxiv-1409-3813 | Incorporating Semi-supervised Features into Discontinuous Easy-First Constituent Parsing | http://arxiv.org/abs/1409.3813 | id:1409.3813 author:Yannick Versley category:cs.CL  published:2014-09-12 summary:This paper describes adaptations for EaFi, a parser for easy-first parsing of discontinuous constituents, to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the SPMRL shared task 2014. version:1
arxiv-1409-4276 | A Fast Quartet Tree Heuristic for Hierarchical Clustering | http://arxiv.org/abs/1409.4276 | id:1409.4276 author:Rudi L. Cilibrasi, Paul M. B. Vitanyi category:cs.LG cs.CE cs.DS  published:2014-09-12 summary:The Minimum Quartet Tree Cost problem is to construct an optimal weight tree from the $3{n \choose 4}$ weighted quartet topologies on $n$ objects, where optimality means that the summed weight of the embedded quartet topologies is optimal (so it can be the case that the optimal tree embeds all quartets as nonoptimal topologies). We present a Monte Carlo heuristic, based on randomized hill climbing, for approximating the optimal weight tree, given the quartet topology weights. The method repeatedly transforms a dendrogram, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. The problem and the solution heuristic has been extensively used for general hierarchical clustering of nontree-like (non-phylogeny) data in various domains and across domains with heterogeneous data. We also present a greatly improved heuristic, reducing the running time by a factor of order a thousand to ten thousand. All this is implemented and available, as part of the CompLearn package. We compare performance and running time of the original and improved versions with those of UPGMA, BioNJ, and NJ, as implemented in the SplitsTree package on genomic data for which the latter are optimized. Keywords: Data and knowledge visualization, Pattern matching--Clustering--Algorithms/Similarity measures, Hierarchical clustering, Global optimization, Quartet tree, Randomized hill-climbing, version:1
arxiv-1409-3768 | Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection | http://arxiv.org/abs/1409.3768 | id:1409.3768 author:Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam category:stat.CO cs.LG stat.ML  published:2014-09-12 summary:Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of $\ell_1$-penalized estimation in the Gaussian framework. Though many of these inverse covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing $\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous payoffs for $\ell_1$-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof. version:1
arxiv-1409-3714 | Time-domain multiscale shape identification in electro-sensing | http://arxiv.org/abs/1409.3714 | id:1409.3714 author:Habib Ammari, Han Wang category:math.NA cs.CV  published:2014-09-12 summary:This paper presents premier and innovative time-domain multi-scale method for shape identification in electro-sensing using pulse-type signals. The method is based on transform-invariant shape descriptors computed from filtered polarization tensors at multi-scales. The proposed algorithm enjoys a remarkable noise robustness even with far-field measurements at very limited angle of view. It opens a door for pulsed imaging using echolocation and induction data. version:1
arxiv-1409-4244 | An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem | http://arxiv.org/abs/1409.4244 | id:1409.4244 author:Enrique Gabriel Baquela, Ana Carolina Olivera category:cs.NE  published:2014-09-12 summary:The lane reversal has proven to be a useful method to mitigate traffic congestion during rush hour or in case of specific events that affect high traffic volumes. In this work we propose a methodology that is placed within optimization via Simulation, by means of which a multi-objective genetic algorithm and simulations of traffic are used to determine the configuration of ideal lane reversal. version:1
arxiv-1403-3522 | An inertial forward-backward algorithm for monotone inclusions | http://arxiv.org/abs/1403.3522 | id:1403.3522 author:Dirk A. Lorenz, Thomas Pock category:cs.CV cs.NA math.NA math.OC  published:2014-03-14 summary:In this paper, we propose an inertial forward backward splitting algorithm to compute a zero of the sum of two monotone operators, with one of the two operators being co-coercive. The algorithm is inspired by the accelerated gradient method of Nesterov, but can be applied to a much larger class of problems including convex-concave saddle point problems and general monotone inclusions. We prove convergence of the algorithm in a Hilbert space setting and show that several recently proposed first-order methods can be obtained as special cases of the general algorithm. Numerical results show that the proposed algorithm converges faster than existing methods, while keeping the computational cost of each iteration basically unchanged. version:2
arxiv-1409-5671 | A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems | http://arxiv.org/abs/1409.5671 | id:1409.5671 author:Ebru Aydin Gol, Ezio Bartocci, Calin Belta category:cs.AI cs.CE cs.LG cs.LO cs.SY  published:2014-09-12 summary:We propose a technique to detect and generate patterns in a network of locally interacting dynamical systems. Central to our approach is a novel spatial superposition logic, whose semantics is defined over the quad-tree of a partitioned image. We show that formulas in this logic can be efficiently learned from positive and negative examples of several types of patterns. We also demonstrate that pattern detection, which is implemented as a model checking algorithm, performs very well for test data sets different from the learning sets. We define a quantitative semantics for the logic and integrate the model checking algorithm with particle swarm optimization in a computational framework for synthesis of parameters leading to desired patterns in reaction-diffusion systems. version:1
arxiv-1401-6169 | Parsimonious Topic Models with Salient Word Discovery | http://arxiv.org/abs/1401.6169 | id:1401.6169 author:Hossein Soleimani, David J. Miller category:cs.LG cs.CL cs.IR stat.ML  published:2014-01-22 summary:We propose a parsimonious topic model for text corpora. In related models such as Latent Dirichlet Allocation (LDA), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics. Our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model. Further, in LDA all topics are in principle present in every document. By contrast our model gives sparse topic representation, determining the (small) subset of relevant topics for each document. We derive a Bayesian Information Criterion (BIC), balancing model complexity and goodness of fit. Here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model. We minimize BIC to jointly determine our entire model -- the topic-specific words, document-specific topics, all model parameter values, {\it and} the total number of topics -- in a wholly unsupervised fashion. Results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to LDA and to a model designed to incorporate sparsity. version:2
arxiv-1409-2195 | Analyzing the Language of Food on Social Media | http://arxiv.org/abs/1409.2195 | id:1409.2195 author:Daniel Fried, Mihai Surdeanu, Stephen Kobourov, Melanie Hingle, Dane Bell category:cs.CL cs.CY cs.SI  published:2014-09-08 summary:We investigate the predictive power behind the language of food on social media. We collect a corpus of over three million food-related posts from Twitter and demonstrate that many latent population characteristics can be directly predicted from this data: overweight rate, diabetes rate, political leaning, and home geographical location of authors. For all tasks, our language-based models significantly outperform the majority-class baselines. Performance is further improved with more complex natural language processing, such as topic modeling. We analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. Lastly, we design and implement an online system for real-time query and visualization of the dataset. Visualization tools, such as geo-referenced heatmaps, semantics-preserving wordclouds and temporal histograms, allow us to discover more complex, global patterns mirrored in the language of food. version:2
arxiv-1409-3505 | DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection | http://arxiv.org/abs/1409.3505 | id:1409.3505 author:Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-09-11 summary:In this paper, we propose multi-stage and deformable deep convolutional neural networks for object detection. This new deep learning object detection diagram has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. With the proposed multi-stage training strategy, multiple classifiers are jointly optimized to process samples at different difficulty levels. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of modeling averaging. The proposed approach ranked \#2 in ILSVRC 2014. It improves the mean averaged precision obtained by RCNN, which is the state-of-the-art of object detection, from $31\%$ to $45\%$. Detailed component-wise analysis is also provided through extensive experimental evaluation. version:1
arxiv-1409-3446 | Consensus-Based Modelling using Distributed Feature Construction | http://arxiv.org/abs/1409.3446 | id:1409.3446 author:Haimonti Dutta, Ashwin Srinivasan category:cs.LG  published:2014-09-11 summary:A particularly successful role for Inductive Logic Programming (ILP) is as a tool for discovering useful relational features for subsequent use in a predictive model. Conceptually, the case for using ILP to construct relational features rests on treating these features as functions, the automated discovery of which necessarily requires some form of first-order learning. Practically, there are now several reports in the literature that suggest that augmenting any existing features with ILP-discovered relational features can substantially improve the predictive power of a model. While the approach is straightforward enough, much still needs to be done to scale it up to explore more fully the space of possible features that can be constructed by an ILP system. This is in principle, infinite and in practice, extremely large. Applications have been confined to heuristic or random selections from this space. In this paper, we address this computational difficulty by allowing features to be constructed in a distributed manner. That is, there is a network of computational units, each of which employs an ILP engine to construct some small number of features and then builds a (local) model. We then employ a consensus-based algorithm, in which neighboring nodes share information to update local models. For a category of models (those with convex loss functions), it can be shown that the algorithm will result in all nodes converging to a consensus model. In practice, it may be slow to achieve this convergence. Nevertheless, our results on synthetic and real datasets that suggests that in relatively short time the "best" node in the network reaches a model whose predictive accuracy is comparable to that obtained using more computational effort in a non-distributed setting (the best node is identified as the one whose weights converge first). version:1
arxiv-1409-4727 | Selection of Most Appropriate Backpropagation Training Algorithm in Data Pattern Recognition | http://arxiv.org/abs/1409.4727 | id:1409.4727 author:Hindayati Mustafidah, Sri Hartati, Retantyo Wardoyo, Agus Harjoko category:cs.NE  published:2014-09-11 summary:There are several training algorithms for backpropagation method in neural network. Not all of these algorithms have the same accuracy level demonstrated through the percentage level of suitability in recognizing patterns in the data. In this research tested 12 training algorithms specifically in recognize data patterns of test validity. The basic network parameters used are the maximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05. Of the twelve training algorithms each performed 20 times looping. The test results obtained that the percentage rate of the great match is trainlm algorithm with alpha 5% have adequate levels of suitability of 87.5% at the level of significance of 0.000. This means the most appropriate training algorithm in recognizing the the data pattern of test validity is the trainlm algorithm. version:1
arxiv-1409-3358 | Building Program Vector Representations for Deep Learning | http://arxiv.org/abs/1409.3358 | id:1409.3358 author:Lili Mou, Ge Li, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang category:cs.SE cs.LG cs.NE  published:2014-09-11 summary:Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation. In this pioneering paper, we propose the "coding criterion" to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations. To evaluate whether deep learning is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program classification task than "shallow" methods, such as logistic regression and the support vector machine. This result confirms the feasibility of deep learning to analyze programs. It also gives primary evidence of its success in this new field. We believe deep learning will become an outstanding technique for program analysis in the near future. version:1
arxiv-1404-1935 | Tyler's Covariance Matrix Estimator in Elliptical Models with Convex Structure | http://arxiv.org/abs/1404.1935 | id:1404.1935 author:Ilya Soloveychik, Ami Wiesel category:stat.ML  published:2014-04-07 summary:We address structured covariance estimation in elliptical distributions by assuming that the covariance is a priori known to belong to a given convex set, e.g., the set of Toeplitz or banded matrices. We consider the General Method of Moments (GMM) optimization applied to robust Tyler's scatter M-estimator subject to these convex constraints. Unfortunately, GMM turns out to be non-convex due to the objective. Instead, we propose a new COCA estimator - a convex relaxation which can be efficiently solved. We prove that the relaxation is tight in the unconstrained case for a finite number of samples, and in the constrained case asymptotically. We then illustrate the advantages of COCA in synthetic simulations with structured compound Gaussian distributions. In these examples, COCA outperforms competing methods such as Tyler's estimator and its projection onto the structure set. version:2
arxiv-1412-6153 | Intelligent Indoor Mobile Robot Navigation Using Stereo Vision | http://arxiv.org/abs/1412.6153 | id:1412.6153 author:Arjun B. Krishnan, Jayaram Kollipara category:cs.RO cs.AI cs.CV  published:2014-09-10 summary:Majority of the existing robot navigation systems, which facilitate the use of laser range finders, sonar sensors or artificial landmarks, has the ability to locate itself in an unknown environment and then build a map of the corresponding environment. Stereo vision, while still being a rapidly developing technique in the field of autonomous mobile robots, are currently less preferable due to its high implementation cost. This paper aims at describing an experimental approach for the building of a stereo vision system that helps the robots to avoid obstacles and navigate through indoor environments and at the same time remaining very much cost effective. This paper discusses the fusion techniques of stereo vision and ultrasound sensors which helps in the successful navigation through different types of complex environments. The data from the sensor enables the robot to create the two dimensional topological map of unknown environments and stereo vision systems models the three dimension model of the same environment. version:1
arxiv-1409-3512 | Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words | http://arxiv.org/abs/1409.3512 | id:1409.3512 author:Udaya Raj Dhungana, Subarna Shakya, Kabita Baral, Bharat Sharma category:cs.CL  published:2014-09-10 summary:This paper presents a new model of WordNet that is used to disambiguate the correct sense of polysemy word based on the clue words. The related words for each sense of a polysemy word as well as single sense word are referred to as the clue words. The conventional WordNet organizes nouns, verbs, adjectives and adverbs together into sets of synonyms called synsets each expressing a different concept. In contrast to the structure of WordNet, we developed a new model of WordNet that organizes the different senses of polysemy words as well as the single sense words based on the clue words. These clue words for each sense of a polysemy word as well as for single sense word are used to disambiguate the correct meaning of the polysemy word in the given context using knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word can be a noun, verb, adjective or adverb. version:1
arxiv-1409-3136 | Metric Learning for Temporal Sequence Alignment | http://arxiv.org/abs/1409.3136 | id:1409.3136 author:Damien Garreau, Rémi Lajugie, Sylvain Arlot, Francis Bach category:cs.LG  published:2014-09-10 summary:In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio to audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better performance for the alignment. version:1
arxiv-1409-3078 | An improved genetic algorithm with a local optimization strategy and an extra mutation level for solving traveling salesman problem | http://arxiv.org/abs/1409.3078 | id:1409.3078 author:Keivan Borna, Vahid Haji Hashemi category:cs.NE  published:2014-09-10 summary:The Traveling salesman problem (TSP) is proved to be NP-complete in most cases. The genetic algorithm (GA) is one of the most useful algorithms for solving this problem. In this paper a conventional GA is compared with an improved hybrid GA in solving TSP. The improved or hybrid GA consist of conventional GA and two local optimization strategies. The first strategy is extracting all sequential groups including four cities of samples and changing the two central cities with each other. The second local optimization strategy is similar to an extra mutation process. In this step with a low probability a sample is selected. In this sample two random cities are defined and the path between these cities is reversed. The computation results show that the proposed method also finds better paths than the conventional GA within an acceptable computation time. version:1
arxiv-1410-2175 | Image Denoising using New Adaptive Based Median Filters | http://arxiv.org/abs/1410.2175 | id:1410.2175 author:Suman Shrestha category:cs.CV  published:2014-09-10 summary:Noise is a major issue while transferring images through all kinds of electronic communication. One of the most common noise in electronic communication is an impulse noise which is caused by unstable voltage. In this paper, the comparison of known image denoising techniques is discussed and a new technique using the decision based approach has been used for the removal of impulse noise. All these methods can primarily preserve image details while suppressing impulsive noise. The principle of these techniques is at first introduced and then analysed with various simulation results using MATLAB. Most of the previously known techniques are applicable for the denoising of images corrupted with less noise density. Here a new decision based technique has been presented which shows better performances than those already being used. The comparisons are made based on visual appreciation and further quantitatively by Mean Square error (MSE) and Peak Signal to Noise Ratio (PSNR) of different filtered images.. version:1
arxiv-1409-3024 | One-Dimensional Vector based Pattern Matching | http://arxiv.org/abs/1409.3024 | id:1409.3024 author:Y. M. Fouda category:cs.CV  published:2014-09-10 summary:Template matching is a basic method in image analysis to extract useful information from images. In this paper, we suggest a new method for pattern matching. Our method transform the template image from two dimensional image into one dimensional vector. Also all sub-windows (same size of template) in the reference image will transform into one dimensional vectors. The three similarity measures SAD, SSD, and Euclidean are used to compute the likeness between template and all sub-windows in the reference image to find the best match. The experimental results show the superior performance of the proposed method over the conventional methods on various template of different sizes. version:1
arxiv-1409-3005 | A Study of Association Measures and their Combination for Arabic MWT Extraction | http://arxiv.org/abs/1409.3005 | id:1409.3005 author:Abdelkader El Mahdaouy, Saïd EL Alaoui Ouatik, Eric Gaussier category:cs.CL  published:2014-09-10 summary:Automatic Multi-Word Term (MWT) extraction is a very important issue to many applications, such as information retrieval, question answering, and text categorization. Although many methods have been used for MWT extraction in English and other European languages, few studies have been applied to Arabic. In this paper, we propose a novel, hybrid method which combines linguistic and statistical approaches for Arabic Multi-Word Term extraction. The main contribution of our method is to consider contextual information and both termhood and unithood for association measures at the statistical filtering step. In addition, our technique takes into account the problem of MWT variation in the linguistic filtering step. The performance of the proposed statistical measure (NLC-value) is evaluated using an Arabic environment corpus by comparing it with some existing competitors. Experimental results show that our NLC-value measure outperforms the other ones in term of precision for both bi-grams and tri-grams. version:1
arxiv-1409-2824 | Scalable Bayesian Modelling of Paired Symbols | http://arxiv.org/abs/1409.2824 | id:1409.2824 author:Ulrich Paquet, Noam Koenigstein, Ole Winther category:stat.ML  published:2014-09-09 summary:We present a novel, scalable and Bayesian approach to modelling the occurrence of pairs of symbols (i,j) drawn from a large vocabulary. Observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function. By basing inference on the well-founded principle of variational bounding, and using new site-independent bounds, we show how a scalable inference procedure can be obtained for large data sets. State of the art results are presented on real-world movie viewing data. version:2
arxiv-1409-2993 | "Look Ma, No Hands!" A Parameter-Free Topic Model | http://arxiv.org/abs/1409.2993 | id:1409.2993 author:Jian Tang, Ming Zhang, Qiaozhu Mei category:cs.LG cs.CL cs.IR  published:2014-09-10 summary:It has always been a burden to the users of statistical topic models to predetermine the right number of topics, which is a key parameter of most topic models. Conventionally, automatic selection of this parameter is done through either statistical model selection (e.g., cross-validation, AIC, or BIC) or Bayesian nonparametric models (e.g., hierarchical Dirichlet process). These methods either rely on repeated runs of the inference algorithm to search through a large range of parameter values which does not suit the mining of big data, or replace this parameter with alternative parameters that are less intuitive and still hard to be determined. In this paper, we explore to "eliminate" this parameter from a new perspective. We first present a nonparametric treatment of the PLSA model named nonparametric probabilistic latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of PLSA. This is achieved by substituting the parameter of the number of topics with an alternative parameter that is the minimal goodness of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity among the discovered topics or by a weak supervision from users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar topic. Experiments on both synthetic and real data prove that the parameter-free topic model extracts topics with a comparable quality comparing to classical topic models with "manual transmission". The quality of the topics outperforms those extracted through classical Bayesian nonparametric models. version:1
arxiv-1409-2918 | Quantum Edge Detection for Image Segmentation in Optical Environments | http://arxiv.org/abs/1409.2918 | id:1409.2918 author:Mario Mastriani category:cs.CV  published:2014-09-09 summary:A quantum edge detector for image segmentation in optical environments is presented in this work. A Boolean version of the same detector is presented too. The quantum version of the new edge detector works with computational basis states, exclusively. This way, we can easily avoid the problem of quantum measurement retrieving the result of applying the new detector on the image. Besides, a new criterion and logic based on projections onto vertical axis of Bloch's Sphere exclusively are presented too. This approach will allow us: 1) a simpler development of logic quantum operations, where they will closer to those used in the classical logic operations, 2) building simple and robust classical-to-quantum and quantum-to-classical interfaces. Said so far is extended to quantum algorithms outside image processing too. In a special section on metric and simulations, a new metric based on the comparison between the classical and quantum versions algorithms for edge detection of images is presented. Notable differences between the results of classical and quantum versions of such algorithms (outside and inside of quantum computer, respectively) show the existence of implementation problems involved in the experiment, and that they have not been properly modeled for optical environments. However, although they are different, the quantum results are equally valid. The latter is clearly seen in the computer simulations version:1
arxiv-1409-2905 | Non-Convex Boosting Overcomes Random Label Noise | http://arxiv.org/abs/1409.2905 | id:1409.2905 author:Sunsern Cheamanunkul, Evan Ettinger, Yoav Freund category:cs.LG  published:2014-09-09 summary:The sensitivity of Adaboost to random label noise is a well-studied problem. LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be less sensitive to noise than AdaBoost. We present the results of experiments evaluating these algorithms on both synthetic and real datasets. We compare the performance on each of datasets when the labels are corrupted by different levels of independent label noise. In presence of random label noise, we found that BrownBoost and RobustBoost perform significantly better than AdaBoost and LogitBoost, while the difference between each pair of algorithms is insignificant. We provide an explanation for the difference based on the margin distributions of the algorithms. version:1
arxiv-1409-2821 | Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain Clustered Records | http://arxiv.org/abs/1409.2821 | id:1409.2821 author:Meysam Ghaffari, Nasser Ghadiri category:cs.AI cs.CV  published:2014-09-09 summary:As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each input sample to belong to more than one cluster, providing more flexibility than non-fuzzy clustering methods. However, the accuracy of FCM is subject to false detections caused by noisy records, weak feature selection and low certainty of the algorithm in some cases. The false detections are very important in some decision-making application domains like network security and medical diagnosis, where weak decisions based on such false detections may lead to catastrophic outcomes. They are mainly emerged from making decisions about a subset of records that do not provide enough evidence to make a good decision. In this paper, we propose a method for detecting such ambiguous records in FCM by introducing a certainty factor to decrease invalid detections. This approach enables us to send the detected ambiguous records to another discrimination method for a deeper investigation, thus increasing the accuracy by lowering the error rate. Most of the records are still processed quickly and with low error rate which prevents performance loss compared to similar hybrid methods. Experimental results of applying the proposed method on several datasets from different domains show a significant decrease in error rate as well as improved sensitivity of the algorithm. version:1
arxiv-1409-2800 | Enforcing Label and Intensity Consistency for IR Target Detection | http://arxiv.org/abs/1409.2800 | id:1409.2800 author:Toufiq Parag category:cs.CV  published:2014-09-09 summary:This study formulates the IR target detection as a binary classification problem of each pixel. Each pixel is associated with a label which indicates whether it is a target or background pixel. The optimal label set for all the pixels of an image maximizes aposteriori distribution of label configuration given the pixel intensities. The posterior probability is factored into (or proportional to) a conditional likelihood of the intensity values and a prior probability of label configuration. Each of these two probabilities are computed assuming a Markov Random Field (MRF) on both pixel intensities and their labels. In particular, this study enforces neighborhood dependency on both intensity values, by a Simultaneous Auto Regressive (SAR) model, and on labels, by an Auto-Logistic model. The parameters of these MRF models are learned from labeled examples. During testing, an MRF inference technique, namely Iterated Conditional Mode (ICM), produces the optimal label for each pixel. The detection performance is further improved by incorporating temporal information through background subtraction. High performances on benchmark datasets demonstrate effectiveness of this method for IR target detection. version:1
arxiv-1402-0710 | Short-term plasticity as cause-effect hypothesis testing in distal reward learning | http://arxiv.org/abs/1402.0710 | id:1402.0710 author:Andrea Soltoggio category:cs.NE q-bio.NC  published:2014-02-04 summary:Asynchrony, overlaps and delays in sensory-motor signals introduce ambiguity as to which stimuli, actions, and rewards are causally related. Only the repetition of reward episodes helps distinguish true cause-effect relationships from coincidental occurrences. In the model proposed here, a novel plasticity rule employs short and long-term changes to evaluate hypotheses on cause-effect relationships. Transient weights represent hypotheses that are consolidated in long-term memory only when they consistently predict or cause future rewards. The main objective of the model is to preserve existing network topologies when learning with ambiguous information flows. Learning is also improved by biasing the exploration of the stimulus-response space towards actions that in the past occurred before rewards. The model indicates under which conditions beliefs can be consolidated in long-term memory, it suggests a solution to the plasticity-stability dilemma, and proposes an interpretation of the role of short-term plasticity. version:5
arxiv-1406-6315 | Automatic Dimension Selection for a Non-negative Factorization Approach to Clustering Multiple Random Graphs | http://arxiv.org/abs/1406.6315 | id:1406.6315 author:Nam H. Lee, I-Jeng Wang, Youngser Park, Care E. Priebe, Michael Rosen category:stat.ML  published:2014-06-24 summary:We consider a problem of grouping multiple graphs into several clusters using singular value thesholding and non-negative factorization. We derive a model selection information criterion to estimate the number of clusters. We demonstrate our approach using "Swimmer data set" as well as simulated data set, and compare its performance with two standard clustering algorithms. version:2
arxiv-1409-2713 | Context-specific independence in graphical log-linear models | http://arxiv.org/abs/1409.2713 | id:1409.2713 author:Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander category:stat.ML  published:2014-09-09 summary:Log-linear models are the popular workhorses of analyzing contingency tables. A log-linear parameterization of an interaction model can be more expressive than a direct parameterization based on probabilities, leading to a powerful way of defining restrictions derived from marginal, conditional and context-specific independence. However, parameter estimation is often simpler under a direct parameterization, provided that the model enjoys certain decomposability properties. Here we introduce a cyclical projection algorithm for obtaining maximum likelihood estimates of log-linear parameters under an arbitrary context-specific graphical log-linear model, which needs not satisfy criteria of decomposability. We illustrate that lifting the restriction of decomposability makes the models more expressive, such that additional context-specific independencies embedded in real data can be identified. It is also shown how a context-specific graphical model can correspond to a non-hierarchical log-linear parameterization with a concise interpretation. This observation can pave way to further development of non-hierarchical log-linear models, which have been largely neglected due to their believed lack of interpretability. version:1
arxiv-1409-2710 | eAnt-Miner : An Ensemble Ant-Miner to Improve the ACO Classification | http://arxiv.org/abs/1409.2710 | id:1409.2710 author:Gopinath Chennupati category:cs.NE  published:2014-09-09 summary:Ant Colony Optimization (ACO) has been applied in supervised learning in order to induce classification rules as well as decision trees, named Ant-Miners. Although these are competitive classifiers, the stability of these classifiers is an important concern that owes to their stochastic nature. In this paper, to address this issue, an acclaimed machine learning technique named, ensemble of classifiers is applied, where an ACO classifier is used as a base classifier to prepare the ensemble. The main trade-off is, the predictions in the new approach are determined by discovering a group of models as opposed to the single model classification. In essence, we prepare multiple models from the randomly replaced samples of training data from which, a unique model is prepared by aggregating the models to test the unseen data points. The main objective of this new approach is to increase the stability of the Ant-Miner results there by improving the performance of ACO classification. We found that the ensemble Ant-Miners significantly improved the stability by reducing the classification error on unseen data. version:1
arxiv-1409-2702 | F-formation Detection: Individuating Free-standing Conversational Groups in Images | http://arxiv.org/abs/1409.2702 | id:1409.2702 author:Francesco Setti, Chris Russell, Chiara Bassetti, Marco Cristani category:cs.CV  published:2014-09-09 summary:Detection of groups of interacting people is a very interesting and useful task in many modern technologies, with application fields spanning from video-surveillance to social robotics. In this paper we first furnish a rigorous definition of group considering the background of the social sciences: this allows us to specify many kinds of group, so far neglected in the Computer Vision literature. On top of this taxonomy, we present a detailed state of the art on the group detection algorithms. Then, as a main contribution, we present a brand new method for the automatic detection of groups in still images, which is based on a graph-cuts framework for clustering individuals; in particular we are able to codify in a computational sense the sociological definition of F-formation, that is very useful to encode a group having only proxemic information: position and orientation of people. We call the proposed method Graph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms all the state of the art methods in terms of different accuracy measures (some of them are brand new), demonstrating also a strong robustness to noise and versatility in recognizing groups of various cardinality. version:1
arxiv-1409-2579 | A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices | http://arxiv.org/abs/1409.2579 | id:1409.2579 author:Ting-ting Feng, Gang Wu category:cs.NA cs.CV cs.LG  published:2014-09-09 summary:The null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed. However, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient, and some useful discriminant information will be lost. In this paper, we investigate how to choose the random matrix properly, such that the two criteria of the null LDA method are satisfied theoretically. We give a necessary and sufficient condition to guarantee full column rank of the orientation matrix. Moreover, the geometric characterization of the condition is also described. version:1
arxiv-1409-2650 | Combining the analytical hierarchy process and the genetic algorithm to solve the timetable problem | http://arxiv.org/abs/1409.2650 | id:1409.2650 author:Ihab Sbeity, Mohamed Dbouk, Habib Kobeissi category:cs.AI cs.NE  published:2014-09-09 summary:The main problems of school course timetabling are time, curriculum, and classrooms. In addition there are other problems that vary from one institution to another. This paper is intended to solve the problem of satisfying the teachers preferred schedule in a way that regards the importance of the teacher to the supervising institute, i.e. his score according to some criteria. Genetic algorithm (GA) has been presented as an elegant method in solving timetable problem (TTP) in order to produce solutions with no conflict. In this paper, we consider the analytic hierarchy process (AHP) to efficiently obtain a score for each teacher, and consequently produce a GA-based TTP solution that satisfies most of the teachers preferences. version:1
arxiv-1409-2465 | Comparing Feature Detectors: A bias in the repeatability criteria, and how to correct it | http://arxiv.org/abs/1409.2465 | id:1409.2465 author:Ives Rey-Otero, Mauricio Delbracio, Jean-Michel Morel category:cs.CV  published:2014-09-08 summary:Most computer vision application rely on algorithms finding local correspondences between different images. These algorithms detect and compare stable local invariant descriptors centered at scale-invariant keypoints. Because of the importance of the problem, new keypoint detectors and descriptors are constantly being proposed, each one claiming to perform better (or to be complementary) to the preceding ones. This raises the question of a fair comparison between very diverse methods. This evaluation has been mainly based on a repeatability criterion of the keypoints under a series of image perturbations (blur, illumination, noise, rotations, homotheties, homographies, etc). In this paper, we argue that the classic repeatability criterion is biased towards algorithms producing redundant overlapped detections. To compensate this bias, we propose a variant of the repeatability rate taking into account the descriptors overlap. We apply this variant to revisit the popular benchmark by Mikolajczyk et al., on classic and new feature detectors. Experimental evidence shows that the hierarchy of these feature detectors is severely disrupted by the amended comparator. version:2
arxiv-1409-2620 | Learning Machines Implemented on Non-Deterministic Hardware | http://arxiv.org/abs/1409.2620 | id:1409.2620 author:Suyog Gupta, Vikas Sindhwani, Kailash Gopalakrishnan category:cs.LG stat.ML  published:2014-09-09 summary:This paper highlights new opportunities for designing large-scale machine learning systems as a consequence of blurring traditional boundaries that have allowed algorithm designers and application-level practitioners to stay -- for the most part -- oblivious to the details of the underlying hardware-level implementations. The hardware/software co-design methodology advocated here hinges on the deployment of compute-intensive machine learning kernels onto compute platforms that trade-off determinism in the computation for improvement in speed and/or energy efficiency. To achieve this, we revisit digital stochastic circuits for approximating matrix computations that are ubiquitous in machine learning algorithms. Theoretical and empirical evaluation is undertaken to assess the impact of the hardware-induced computational noise on algorithm performance. As a proof-of-concept, a stochastic hardware simulator is employed for training deep neural networks for image recognition problems. version:1
arxiv-1403-1840 | Multi-scale Orderless Pooling of Deep Convolutional Activation Features | http://arxiv.org/abs/1403.1840 | id:1403.1840 author:Yunchao Gong, Liwei Wang, Ruiqi Guo, Svetlana Lazebnik category:cs.CV  published:2014-03-07 summary:Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets. version:3
arxiv-1409-2450 | Exploiting Social Network Structure for Person-to-Person Sentiment Analysis | http://arxiv.org/abs/1409.2450 | id:1409.2450 author:Robert West, Hristo S. Paskov, Jure Leskovec, Christopher Potts category:cs.SI cs.CL physics.soc-ph  published:2014-09-08 summary:Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion. Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context. To capture such interactions, we develop a model that predicts individual A's opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B. We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus. version:1
arxiv-1409-2433 | Approximating solution structure of the Weighted Sentence Alignment problem | http://arxiv.org/abs/1409.2433 | id:1409.2433 author:Antonina Kolokolova, Renesa Nizamee category:cs.CL cs.CC cs.DS  published:2014-09-08 summary:We study the complexity of approximating solution structure of the bijective weighted sentence alignment problem of DeNero and Klein (2008). In particular, we consider the complexity of finding an alignment that has a significant overlap with an optimal alignment. We discuss ways of representing the solution for the general weighted sentence alignment as well as phrases-to-words alignment problem, and show that computing a string which agrees with the optimal sentence partition on more than half (plus an arbitrarily small polynomial fraction) positions for the phrases-to-words alignment is NP-hard. For the general weighted sentence alignment we obtain such bound from the agreement on a little over 2/3 of the bits. Additionally, we generalize the Hamming distance approximation of a solution structure to approximating it with respect to the edit distance metric, obtaining similar lower bounds. version:1
arxiv-1311-5552 | Bayesian Discovery of Threat Networks | http://arxiv.org/abs/1311.5552 | id:1311.5552 author:Steven T. Smith, Edward K. Kao, Kenneth D. Senne, Garrett Bernstein, Scott Philips category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH  published:2013-11-21 summary:A novel unified Bayesian framework for network detection is developed, under which a detection algorithm is derived based on random walks on graphs. The algorithm detects threat networks using partial observations of their activity, and is proved to be optimum in the Neyman-Pearson sense. The algorithm is defined by a graph, at least one observation, and a diffusion model for threat. A link to well-known spectral detection methods is provided, and the equivalence of the random walk and harmonic solutions to the Bayesian formulation is proven. A general diffusion model is introduced that utilizes spatio-temporal relationships between vertices, and is used for a specific space-time formulation that leads to significant performance improvements on coordinated covert networks. This performance is demonstrated using a new hybrid mixed-membership blockmodel introduced to simulate random covert networks with realistic properties. version:3
arxiv-1409-2390 | Symbolic regression of generative network models | http://arxiv.org/abs/1409.2390 | id:1409.2390 author:Telmo Menezes, Camille Roth category:cs.NE cs.SI physics.soc-ph  published:2014-09-08 summary:Networks are a powerful abstraction with applicability to a variety of scientific fields. Models explaining their morphology and growth processes permit a wide range of phenomena to be more systematically analysed and understood. At the same time, creating such models is often challenging and requires insights that may be counter-intuitive. Yet there currently exists no general method to arrive at better models. We have developed an approach to automatically detect realistic decentralised network growth models from empirical data, employing a machine learning technique inspired by natural selection and defining a unified formalism to describe such models as computer programs. As the proposed method is completely general and does not assume any pre-existing models, it can be applied "out of the box" to any given network. To validate our approach empirically, we systematically rediscover pre-defined growth laws underlying several canonical network generation models and credible laws for diverse real-world networks. We were able to find programs that are simple enough to lead to an actual understanding of the mechanisms proposed, namely for a simple brain and a social network. version:1
arxiv-1409-2287 | Variational Inference for Uncertainty on the Inputs of Gaussian Process Models | http://arxiv.org/abs/1409.2287 | id:1409.2287 author:Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence category:stat.ML cs.AI cs.CV cs.LG  published:2014-09-08 summary:The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data. version:1
arxiv-1406-1880 | Spectral Clustering of Graphs with the Bethe Hessian | http://arxiv.org/abs/1406.1880 | id:1406.1880 author:Alaa Saade, Florent Krzakala, Lenka Zdeborová category:cond-mat.dis-nn cs.SI physics.soc-ph stat.ML  published:2014-06-07 summary:Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices. version:2
arxiv-1409-2232 | When coding meets ranking: A joint framework based on local learning | http://arxiv.org/abs/1409.2232 | id:1409.2232 author:Jim Jing-Yan Wang category:cs.CV cs.LG stat.ML  published:2014-09-08 summary:Sparse coding, which represents a data point as a s- parse reconstruction code with regard to a dictionary, has been a popular data representation method. Meanwhile, in database retrieval problems, learn the ranking scores from data points plays an important role. Up to new, these two methods have always been used individually, assuming that data coding and ranking are two independent and irrele- vant problems. However, is there any internal relationship between sparse coding and ranking score learning? If yes, how to explore this internal relationship? In this paper, we try to answer these questions by developing the first join- t sparse coding and ranking score learning algorithm. To explore the local distribution in the sparse code space, and also to bridgecoding and rankingproblems, we assume that in the neighborhood of each data points, the ranking scores can be approximated from the corresponding sparse codes by a local linear function. By considering the local approx- imation error of ranking scores, reconstruction error and sparsity of sparse coding, and the query information pro- vided by the user, we construct an unified objective func- tion for learning of sparse codes, dictionary and rankings scores. An iterative algorithm is developed to optimize the objective function to jointly learn the sparse codes, dictio- nary and rankings scores. version:1
arxiv-1410-0371 | Real Time Fabric Defect Detection System on an Embedded DSP Platform | http://arxiv.org/abs/1410.0371 | id:1410.0371 author:J. L. Raheja, B. Ajay, Ankit Chaudhary category:cs.CV  published:2014-09-08 summary:In industrial fabric productions, automated real time systems are needed to find out the minor defects. It will save the cost by not transporting defected products and also would help in making compmay image of quality fabrics by sending out only undefected products. A real time fabric defect detection system (FDDS), implementd on an embedded DSP platform is presented here. Textural features of fabric image are extracted based on gray level co-occurrence matrix (GLCM). A sliding window technique is used for defect detection where window moves over the whole image computing a textural energy from the GLCM of the fabric image. The energy values are compared to a reference and the deviations beyond a threshold are reported as defects and also visually represented by a window. The implementation is carried out on a TI TMS320DM642 platform and programmed using code composer studio software. The real time output of this implementation was shown on a monitor. version:1
arxiv-1409-2177 | The Large Margin Mechanism for Differentially Private Maximization | http://arxiv.org/abs/1409.2177 | id:1409.2177 author:Kamalika Chaudhuri, Daniel Hsu, Shuang Song category:cs.LG cs.DS cs.IT math.IT math.ST stat.TH  published:2014-09-07 summary:A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine-learning. Previous algorithms for this problem are either range-dependent---i.e., their utility diminishes with the size of the universe---or only apply to very restricted function classes. This work provides the first general-purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning. version:1
arxiv-1409-2104 | A Computational Model of the Short-Cut Rule for 2D Shape Decomposition | http://arxiv.org/abs/1409.2104 | id:1409.2104 author:Lei Luo, Chunhua Shen, Xinwang Liu, Chunyuan Zhang category:cs.CV  published:2014-09-07 summary:We propose a new 2D shape decomposition method based on the short-cut rule. The short-cut rule originates from cognition research, and states that the human visual system prefers to partition an object into parts using the shortest possible cuts. We propose and implement a computational model for the short-cut rule and apply it to the problem of shape decomposition. The model we proposed generates a set of cut hypotheses passing through the points on the silhouette which represent the negative minima of curvature. We then show that most part-cut hypotheses can be eliminated by analysis of local properties of each. Finally, the remaining hypotheses are evaluated in ascending length order, which guarantees that of any pair of conflicting cuts only the shortest will be accepted. We demonstrate that, compared with state-of-the-art shape decomposition methods, the proposed approach achieves decomposition results which better correspond to human intuition as revealed in psychological experiments. version:1
arxiv-1409-2073 | An NLP Assistant for Clide | http://arxiv.org/abs/1409.2073 | id:1409.2073 author:Tobias Kortkamp category:cs.CL  published:2014-09-07 summary:This report describes an NLP assistant for the collaborative development environment Clide, that supports the development of NLP applications by providing easy access to some common NLP data structures. The assistant visualizes text fragments and their dependencies by displaying the semantic graph of a sentence, the coreference chain of a paragraph and mined triples that are extracted from a paragraph's semantic graphs and linked using its coreference chain. Using this information and a logic programming library, we create an NLP database which is used by a series of queries to mine the triples. The algorithm is tested by translating a natural language text describing a graph to an actual graph that is shown as an annotation in the text editor. version:1
arxiv-1409-2050 | Depth image hand tracking from an overhead perspective using partially labeled, unbalanced data: Development and real-world testing | http://arxiv.org/abs/1409.2050 | id:1409.2050 author:Stephen Czarnuch, Alex Mihailidis category:cs.CV  published:2014-09-06 summary:We present the development and evaluation of a hand tracking algorithm based on single depth images captured from an overhead perspective for use in the COACH prompting system. We train a random decision forest body part classifier using approximately 5,000 manually labeled, unbalanced, partially labeled training images. The classifier represents a random subset of pixels in each depth image with a learned probability density function across all trained body parts. A local mode-find approach is used to search for clusters present in the underlying feature space sampled by the classified pixels. In each frame, body part positions are chosen as the mode with the highest confidence. User hand positions are translated into hand washing task actions based on proximity to environmental objects. We validate the performance of the classifier and task action proposals on a large set of approximately 24,000 manually labeled images. version:1
arxiv-1409-2045 | Global Convergence of Online Limited Memory BFGS | http://arxiv.org/abs/1409.2045 | id:1409.2045 author:Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.LG stat.ML  published:2014-09-06 summary:Global convergence of an online (stochastic) limited memory version of the Broyden-Fletcher- Goldfarb-Shanno (BFGS) quasi-Newton method for solving optimization problems with stochastic objectives that arise in large scale machine learning is established. Lower and upper bounds on the Hessian eigenvalues of the sample functions are shown to suffice to guarantee that the curvature approximation matrices have bounded determinants and traces, which, in turn, permits establishing convergence to optimal arguments with probability 1. Numerical experiments on support vector machines with synthetic data showcase reductions in convergence time relative to stochastic gradient descent algorithms as well as reductions in storage and computation relative to other online quasi-Newton methods. Experimental evaluation on a search engine advertising problem corroborates that these advantages also manifest in practical applications. version:1
arxiv-1312-1349 | Improving self-calibration | http://arxiv.org/abs/1312.1349 | id:1312.1349 author:Torsten A. Enßlin, Henrik Junklewitz, Lars Winderling, Maksim Greiner, Marco Selig category:astro-ph.IM cs.IT math.IT physics.data-an stat.ML  published:2013-12-04 summary:Response calibration is the process of inferring how much the measured data depend on the signal one is interested in. It is essential for any quantitative signal estimation on the basis of the data. Here, we investigate self-calibration methods for linear signal measurements and linear dependence of the response on the calibration parameters. The common practice is to augment an external calibration solution using a known reference signal with an internal calibration on the unknown measurement signal itself. Contemporary self-calibration schemes try to find a self-consistent solution for signal and calibration by exploiting redundancies in the measurements. This can be understood in terms of maximizing the joint probability of signal and calibration. However, the full uncertainty structure of this joint probability around its maximum is thereby not taken into account by these schemes. Therefore better schemes -- in sense of minimal square error -- can be designed by accounting for asymmetries in the uncertainty of signal and calibration. We argue that at least a systematic correction of the common self-calibration scheme should be applied in many measurement situations in order to properly treat uncertainties of the signal on which one calibrates. Otherwise the calibration solutions suffer from a systematic bias, which consequently distorts the signal reconstruction. Furthermore, we argue that non-parametric, signal-to-noise filtered calibration should provide more accurate reconstructions than the common bin averages and provide a new, improved self-calibration scheme. We illustrate our findings with a simplistic numerical example. version:3
arxiv-1409-1976 | A Reduction of the Elastic Net to Support Vector Machines with an Application to GPU Computing | http://arxiv.org/abs/1409.1976 | id:1409.1976 author:Quan Zhou, Wenlin Chen, Shiji Song, Jacob R. Gardner, Kilian Q. Weinberger, Yixin Chen category:stat.ML cs.LG  published:2014-09-06 summary:The past years have witnessed many dedicated open-source projects that built and maintain implementations of Support Vector Machines (SVM), parallelized for GPU, multi-core CPUs and distributed systems. Up to this point, no comparable effort has been made to parallelize the Elastic Net, despite its popularity in many high impact applications, including genetics, neuroscience and systems biology. The first contribution in this paper is of theoretical nature. We establish a tight link between two seemingly different algorithms and prove that Elastic Net regression can be reduced to SVM with squared hinge loss classification. Our second contribution is to derive a practical algorithm based on this reduction. The reduction enables us to utilize prior efforts in speeding up and parallelizing SVMs to obtain a highly optimized and parallel solver for the Elastic Net and Lasso. With a simple wrapper, consisting of only 11 lines of MATLAB code, we obtain an Elastic Net implementation that naturally utilizes GPU and multi-core CPUs. We demonstrate on twelve real world data sets, that our algorithm yields identical results as the popular (and highly optimized) glmnet implementation but is one or several orders of magnitude faster. version:1
arxiv-1409-1320 | Marginal Structured SVM with Hidden Variables | http://arxiv.org/abs/1409.1320 | id:1409.1320 author:Wei Ping, Qiang Liu, Alexander Ihler category:stat.ML cs.LG  published:2014-09-04 summary:In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice. version:2
arxiv-1409-2697 | Particle Swarm Optimized Fuzzy Controller for Indirect Vector Control of Multilevel Inverter Fed Induction Motor | http://arxiv.org/abs/1409.2697 | id:1409.2697 author:Sanjaya Kumar Sahu, T. V. Dixit, D. D. Neema category:cs.NE  published:2014-09-05 summary:The Particle Swarm Optimized (PSO) fuzzy controller has been proposed for indirect vector control of induction motor. In this proposed scheme a Neutral Point Clamped (NPC) multilevel inverter is used and hysteresis current control technique has been adopted for switching the IGBTs. A Mamdani type fuzzy controller is used in place of conventional PI controller. To ensure better performance of fuzzy controller all parameters such as membership functions, normalizing and de-normalizing parameters are optimized using PSO. The performance of proposed controller is investigated under various load and speed conditions. The simulation results show its stability and robustness for high performance derives applications. version:1
arxiv-1409-1892 | Automatic Neuron Type Identification by Neurite Localization in the Drosophila Medulla | http://arxiv.org/abs/1409.1892 | id:1409.1892 author:Ting Zhao, Stephen M Plaza category:q-bio.NC cs.CV  published:2014-09-05 summary:Mapping the connectivity of neurons in the brain (i.e., connectomics) is a challenging problem due to both the number of connections in even the smallest organisms and the nanometer resolution required to resolve them. Because of this, previous connectomes contain only hundreds of neurons, such as in the C.elegans connectome. Recent technological advances will unlock the mysteries of increasingly large connectomes (or partial connectomes). However, the value of these maps is limited by our ability to reason with this data and understand any underlying motifs. To aid connectome analysis, we introduce algorithms to cluster similarly-shaped neurons, where 3D neuronal shapes are represented as skeletons. In particular, we propose a novel location-sensitive clustering algorithm. We show clustering results on neurons reconstructed from the Drosophila medulla that show high-accuracy. version:1
arxiv-1409-1917 | Novel Methods for Activity Classification and Occupany Prediction Enabling Fine-grained HVAC Control | http://arxiv.org/abs/1409.1917 | id:1409.1917 author:Rajib Rana, Brano Kusy, Josh Wall, Wen Hu category:cs.LG  published:2014-09-05 summary:Much of the energy consumption in buildings is due to HVAC systems, which has motivated several recent studies on making these systems more energy- efficient. Occupancy and activity are two important aspects, which need to be correctly estimated for optimal HVAC control. However, state-of-the-art methods to estimate occupancy and classify activity require infrastructure and/or wearable sensors which suffers from lower acceptability due to higher cost. Encouragingly, with the advancement of the smartphones, these are becoming more achievable. Most of the existing occupancy estimation tech- niques have the underlying assumption that the phone is always carried by its user. However, phones are often left at desk while attending meeting or other events, which generates estimation error for the existing phone based occupancy algorithms. Similarly, in the recent days the emerging theory of Sparse Random Classifier (SRC) has been applied for activity classification on smartphone, however, there are rooms to improve the on-phone process- ing. We propose a novel sensor fusion method which offers almost 100% accuracy for occupancy estimation. We also propose an activity classifica- tion algorithm, which offers similar accuracy as of the state-of-the-art SRC algorithms while offering 50% reduction in processing. version:1
arxiv-1407-1687 | KNET: A General Framework for Learning Word Embedding using Morphological Knowledge | http://arxiv.org/abs/1407.1687 | id:1407.1687 author:Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Tie-Yan Liu category:cs.CL cs.LG  published:2014-07-07 summary:Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to address text mining, information retrieval, and natural language processing tasks. Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words. However, it is challenging to handle unseen words or rare words with insufficient context. In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings. Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings. version:3
arxiv-1409-1789 | Identifying Synapses Using Deep and Wide Multiscale Recursive Networks | http://arxiv.org/abs/1409.1789 | id:1409.1789 author:Gary B. Huang, Stephen Plaza category:cs.CV  published:2014-09-05 summary:In this work, we propose a learning framework for identifying synapses using a deep and wide multi-scale recursive (DAWMR) network, previously considered in image segmentation applications. We apply this approach on electron microscopy data from invertebrate fly brain tissue. By learning features directly from the data, we are able to achieve considerable improvements over existing techniques that rely on a small set of hand-designed features. We show that this system can reduce the amount of manual annotation required, in both acquisition of training data as well as verification of inferred detections. version:1
arxiv-1409-1744 | Structure of an elite co-occurrence network | http://arxiv.org/abs/1409.1744 | id:1409.1744 author:V. A. Traag, R. Reinanda, G. van Klinken category:physics.soc-ph cs.CL cs.SI  published:2014-09-05 summary:The rise of social media allowed for rich analyses of their content and their network structure. As traditional media (i.e. newspapers and magazines) are being digitized, similar analyses can be undertaken. This provides a glimpse of the elite, as the news mostly revolves around the more influential members of society. We here focus on a network structure derived from co-occurrences of people in the media. This network has a strong core with peripheral clusters being connected to the core. Nonetheless, these characteristics seem to be mainly a result from the bipartite structure of the data. We employ a simple growing bipartite model that can qualitatively reproduce such a core-periphery structure. Two self-reinforcing processes are vital: (1) more frequently occurring persons are more likely to occur again; and (2) if two people co-occur frequently, they are more likely to co-occur again. This suggests that the core-periphery structure is not necessarily reflective of the elite network in society, but might be an artefact of how they are portrayed in the media. version:1
arxiv-1409-1715 | An Experimental Study of Adaptive Control for Evolutionary Algorithms | http://arxiv.org/abs/1409.1715 | id:1409.1715 author:Giacomo di Tollo, Frédéric Lardeux, Jorge Maturana, Frédéric Saubion category:cs.NE  published:2014-09-05 summary:The balance of exploration versus exploitation (EvE) is a key issue on evolutionary computation. In this paper we will investigate how an adaptive controller aimed to perform Operator Selection can be used to dynamically manage the EvE balance required by the search, showing that the search strategies determined by this control paradigm lead to an improvement of solution quality found by the evolutionary algorithm. version:1
