arxiv-1411-6998 | Solving the Periodic Timetabling Problem using a Genetic Algorithm | http://arxiv.org/abs/1411.6998 | id:1411.6998 author:Diego Arenas, Remy Chevirer, Said Hanafi, Joaquin Rodriguez category:cs.AI cs.NE  published:2014-11-24 summary:In railway operations, a timetable is established to determine the departure and arrival times for the trains or other rolling stock at the different stations or relevant points inside the rail network or a subset of this network. The elaboration of this timetable is done to respond to the commercial requirements for both passenger and freight traffic, but also it must respect a set of security and capacity constraints associated with the railway network, rolling stock and legislation. Combining these requirements and constraints, as well as the important number of trains and schedules to plan, makes the preparation of a feasible timetable a complex and time-consuming process, that normally takes several months to be completed. This article addresses the problem of generating periodic timetables, which means that the involved trains operate in a recurrent pattern. For instance, the trains belonging to the same train line, depart from some station every 15 minutes or one hour. To tackle the problem, we present a constraint-based model suitable for this kind of problem. Then, we propose a genetic algorithm, allowing a rapid generation of feasible periodic timetables. Finally, two case studies are presented, the first, describing a sub-set of the Netherlands rail network, and the second a large portion of the Nord-pas-de-Calais regional rail network, both of them are then solved using our algorithm and the results are presented and discussed. version:1
arxiv-1411-6447 | The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification | http://arxiv.org/abs/1411.6447 | id:1411.6447 author:Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang category:cs.CV  published:2014-11-24 summary:Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations. version:1
arxiv-1411-6406 | Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors | http://arxiv.org/abs/1411.6406 | id:1411.6406 author:Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang category:cs.CV  published:2014-11-24 summary:Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, % FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems. version:1
arxiv-1308-2893 | Multiclass learnability and the ERM principle | http://arxiv.org/abs/1308.2893 | id:1308.2893 author:Amit Daniely, Sivan Sabato, Shai Ben-David, Shai Shalev-Shwartz category:cs.LG  published:2013-08-13 summary:We study the sample complexity of multiclass prediction in several learning settings. For the PAC setting our analysis reveals a surprising phenomenon: In sharp contrast to binary classification, we show that there exist multiclass hypothesis classes for which some Empirical Risk Minimizers (ERM learners) have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learners will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning {\em symmetric} multiclass hypothesis classes---classes that are invariant under permutations of label names. We further provide a characterization of mistake and regret bounds for multiclass learning in the online setting and the bandit setting, using new generalizations of Littlestone's dimension. version:2
arxiv-1411-7935 | Multiple object tracking with context awareness | http://arxiv.org/abs/1411.7935 | id:1411.7935 author:Laura Leal-Taix√© category:cs.CV  published:2014-11-24 summary:Multiple people tracking is a key problem for many applications such as surveillance, animation or car navigation, and a key input for tasks such as activity recognition. In crowded environments occlusions and false detections are common, and although there have been substantial advances in recent years, tracking is still a challenging task. Tracking is typically divided into two steps: detection, i.e., locating the pedestrians in the image, and data association, i.e., linking detections across frames to form complete trajectories. For the data association task, approaches typically aim at developing new, more complex formulations, which in turn put the focus on the optimization techniques required to solve them. However, they still utilize very basic information such as distance between detections. In this thesis, I focus on the data association task and argue that there is contextual information that has not been fully exploited yet in the tracking community, mainly social context and spatial context coming from different views. version:1
arxiv-1411-6370 | Big Learning with Bayesian Methods | http://arxiv.org/abs/1411.6370 | id:1411.6370 author:Jun Zhu, Jianfei Chen, Wenbo Hu category:cs.LG stat.AP stat.CO stat.ME stat.ML F.1.2; G.3  published:2014-11-24 summary:Explosive growth in data and availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems, and applications with Big Data. Bayesian methods represent one important class of statistic methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including nonparametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications. version:1
arxiv-1411-6369 | Scale-Invariant Convolutional Neural Networks | http://arxiv.org/abs/1411.6369 | id:1411.6369 author:Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang category:cs.CV cs.LG cs.NE  published:2014-11-24 summary:Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations. version:1
arxiv-1411-6365 | On the mathematic modeling of non-parametric curves based on cubic B√©zier curves | http://arxiv.org/abs/1411.6365 | id:1411.6365 author:Ha Jong Won, Choe Chun Hwa, Li Kum Song category:cs.CV  published:2014-11-24 summary:B\'ezier splines are widely available in various systems with the curves and surface designs. In general, the B\'ezier spline can be specified with the B\'ezier curve segments and a B\'ezier curve segment can be fitted to any number of control points. The number of control points determines the degree of the B\'ezier polynomial. This paper presents a method which determines control points for B\'ezier curves approximating segments of obtained image outline(non-parametric curve) by using the properties of cubic B\'ezier curves. Proposed method is a technique to determine the control points that has generality and reduces the error of the B\'ezier curve approximation. Main advantage of proposed method is that it has higher accuracy and compression rate than previous methods. The cubic B\'ezier spline is obtained from cubic B\'ezier curve segments. To demonstrate the various performances of the proposed algorithm, experimental results are compared. version:1
arxiv-1411-6358 | A Hybrid Solution to improve Iteration Efficiency in the Distributed Learning | http://arxiv.org/abs/1411.6358 | id:1411.6358 author:Junxiong Wang, Hongzhi Wang, Chenxu Zhao category:cs.DC cs.LG 68  published:2014-11-24 summary:Currently, many machine learning algorithms contain lots of iterations. When it comes to existing large-scale distributed systems, some slave nodes may break down or have lower efficiency. Therefore traditional machine learning algorithm may fail because of the instability of distributed system.We presents a hybrid approach which not only own a high fault-tolerant but also achieve a balance of performance and efficiency.For each iteration, the result of slow machines will be abandoned. Then, we discuss the relationship between accuracy and abandon rate. Next we debate the convergence speed of this process. Finally, our experiments demonstrate our idea can dramatically reduce calculation time and be used in many platforms. version:1
arxiv-1411-6340 | Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex Priors | http://arxiv.org/abs/1411.6340 | id:1411.6340 author:Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann, Hongdong Li category:cs.CV  published:2014-11-24 summary:While widely acknowledged as highly effective in computer vision, multi-label MRFs with non-convex priors are difficult to optimize. To tackle this, we introduce an algorithm that iteratively approximates the original energy with an appropriately weighted surrogate energy that is easier to minimize. Our algorithm guarantees that the original energy decreases at each iteration. In particular, we consider the scenario where the global minimizer of the weighted surrogate energy can be obtained by a multi-label graph cut algorithm, and show that our algorithm then lets us handle of large variety of non-convex priors. We demonstrate the benefits of our method over state-of-the-art MRF energy minimization techniques on stereo and inpainting problems. version:1
arxiv-1210-5268 | Diffusion of Lexical Change in Social Media | http://arxiv.org/abs/1210.5268 | id:1210.5268 author:Jacob Eisenstein, Brendan O'Connor, Noah A. Smith, Eric P. Xing category:cs.CL cs.SI physics.soc-ph  published:2012-10-18 summary:Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages (authored by 2.7 million unique user accounts). Using a latent vector autoregressive model to aggregate across thousands of words, we identify high-level patterns in diffusion of linguistic change over the United States. Our model is robust to unpredictable changes in Twitter's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size. However, demographic similarity -- especially with regard to race -- plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence. Rather than moving towards a single unified "netspeak" dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English. version:4
arxiv-1411-6326 | Vision and Learning for Deliberative Monocular Cluttered Flight | http://arxiv.org/abs/1411.6326 | id:1411.6326 author:Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M. Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, J. Andrew Bagnell category:cs.RO cs.CV cs.LG  published:2014-11-24 summary:Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available. version:1
arxiv-1406-2083 | On the Decreasing Power of Kernel and Distance based Nonparametric Hypothesis Tests in High Dimensions | http://arxiv.org/abs/1406.2083 | id:1406.2083 author:Sashank J. Reddi, Aaditya Ramdas, Barnab√°s P√≥czos, Aarti Singh, Larry Wasserman category:stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH  published:2014-06-09 summary:This paper is about two related decision theoretic problems, nonparametric two-sample testing and independence testing. There is a belief that two recently proposed solutions, based on kernels and distances between pairs of points, behave well in high-dimensional settings. We identify different sources of misconception that give rise to the above belief. Specifically, we differentiate the hardness of estimation of test statistics from the hardness of testing whether these statistics are zero or not, and explicitly discuss a notion of "fair" alternative hypotheses for these problems as dimension increases. We then demonstrate that the power of these tests actually drops polynomially with increasing dimension against fair alternatives. We end with some theoretical insights and shed light on the \textit{median heuristic} for kernel bandwidth selection. Our work advances the current understanding of the power of modern nonparametric hypothesis tests in high dimensions. version:2
arxiv-1411-6314 | On the High-dimensional Power of Linear-time Kernel Two-Sample Testing under Mean-difference Alternatives | http://arxiv.org/abs/1411.6314 | id:1411.6314 author:Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry Wasserman category:math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH  published:2014-11-23 summary:Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ (\textit{general} alternatives), and those which are designed to specifically test easier alternatives, like a difference in means (\textit{mean-shift} alternatives). The main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test, designed for general alternatives, under a mean-shift alternative in the high-dimensional setting. Specifically, we explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means. As a corollary, we find that if the signal-to-noise ratio is held constant, then the test's power goes to one if the number of samples increases faster than the dimension increases. This is the first explicit power derivation for a general nonparametric test in the high-dimensional setting, and also the first analysis of how tests designed for general alternatives perform when faced with easier ones. version:1
arxiv-1411-6311 | Optimal variable selection in multi-group sparse discriminant analysis | http://arxiv.org/abs/1411.6311 | id:1411.6311 author:Irina Gaynanova, Mladen Kolar category:stat.ML  published:2014-11-23 summary:This article considers the problem of multi-group classification in the setting where the number of variables $p$ is larger than the number of observations $n$. Several methods have been proposed in the literature that address this problem, however their variable selection performance is either unknown or suboptimal to the results known in the two-group case. In this work we provide sharp conditions for the consistent recovery of relevant variables in the multi-group case using the discriminant analysis proposal of Gaynanova et al., 2014. We achieve the rates of convergence that attain the optimal scaling of the sample size $n$, number of variables $p$ and the sparsity level $s$. These rates are significantly faster than the best known results in the multi-group case. Moreover, they coincide with the optimal minimax rates for the two-group case. We validate our theoretical results with numerical analysis. version:1
arxiv-1411-6308 | A Convex Formulation for Spectral Shrunk Clustering | http://arxiv.org/abs/1411.6308 | id:1411.6308 author:Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang, Xiaofang Zhou category:cs.LG  published:2014-11-23 summary:Spectral clustering is a fundamental technique in the field of data mining and information processing. Most existing spectral clustering algorithms integrate dimensionality reduction into the clustering process assisted by manifold learning in the original space. However, the manifold in reduced-dimensional subspace is likely to exhibit altered properties in contrast with the original space. Thus, applying manifold information obtained from the original space to the clustering process in a low-dimensional subspace is prone to inferior performance. Aiming to address this issue, we propose a novel convex algorithm that mines the manifold structure in the low-dimensional subspace. In addition, our unified learning process makes the manifold learning particularly tailored for the clustering. Compared with other related methods, the proposed algorithm results in more structured clustering result. To validate the efficacy of the proposed algorithm, we perform extensive experiments on several benchmark datasets in comparison with some state-of-the-art clustering approaches. The experimental results demonstrate that the proposed algorithm has quite promising clustering performance. version:1
arxiv-1411-6307 | Diversifying Sparsity Using Variational Determinantal Point Processes | http://arxiv.org/abs/1411.6307 | id:1411.6307 author:Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis Kellis, Polina Golland, Luke Bornn category:cs.LG cs.AI stat.ML  published:2014-11-23 summary:We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy. version:1
arxiv-1411-6305 | Revenue Optimization in Posted-Price Auctions with Strategic Buyers | http://arxiv.org/abs/1411.6305 | id:1411.6305 author:Mehryar Mohri, Andres Mu√±oz Medina category:cs.LG  published:2014-11-23 summary:We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previously best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than $\Omega(\sqrt{T})$. We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in $O(\log T)$, an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several different scenarios. version:1
arxiv-1411-6285 | Target Fishing: A Single-Label or Multi-Label Problem? | http://arxiv.org/abs/1411.6285 | id:1411.6285 author:Avid M. Afzal, Hamse Y. Mussa, Richard E. Turner, Andreas Bender, Robert C. Glen category:q-bio.BM cs.LG stat.ML  published:2014-11-23 summary:According to Cobanoglu et al and Murphy, it is now widely acknowledged that the single target paradigm (one protein or target, one disease, one drug) that has been the dominant premise in drug development in the recent past is untenable. More often than not, a drug-like compound (ligand) can be promiscuous - that is, it can interact with more than one target protein. In recent years, in in silico target prediction methods the promiscuity issue has been approached computationally in different ways. In this study we confine attention to the so-called ligand-based target prediction machine learning approaches, commonly referred to as target-fishing. With a few exceptions, the target-fishing approaches that are currently ubiquitous in cheminformatics literature can be essentially viewed as single-label multi-classification schemes; these approaches inherently bank on the single target paradigm assumption that a ligand can home in on one specific target. In order to address the ligand promiscuity issue, one might be able to cast target-fishing as a multi-label multi-class classification problem. For illustrative and comparison purposes, single-label and multi-label Naive Bayes classification models (denoted here by SMM and MMM, respectively) for target-fishing were implemented. The models were constructed and tested on 65,587 compounds and 308 targets retrieved from the ChEMBL17 database. SMM and MMM performed differently: for 16,344 test compounds, the MMM model returned recall and precision values of 0.8058 and 0.6622, respectively; the corresponding recall and precision values yielded by the SMM model were 0.7805 and 0.7596, respectively. However, at a significance level of 0.05 and one degree of freedom McNemar test performed on the target prediction results returned by SMM and MMM for the 16,344 test ligands gave a chi-squared value of 15.656, in favour of the MMM approach. version:1
arxiv-1411-6275 | Detection of Non-Stationary Photometric Perturbations on Projection Screens | http://arxiv.org/abs/1411.6275 | id:1411.6275 author:Miguel Casta√±eda-Garay, Oscar Belmonte-Fern√°ndez, Hebert P√©rez-Ros√©s, Antonio Diaz-Tula category:cs.CV H.5.2; I.4.1  published:2014-11-23 summary:Interfaces based on projection screens have become increasingly more popular in recent years, mainly due to the large screen size and resolution that they provide, as well as their stereo-vision capabilities. This work shows a local method for real-time detection of non-stationary photometric perturbations in projected images by means of computer vision techniques. The method is based on the computation of differences between the images in the projector's frame buffer and the corresponding images on the projection screen observed by the camera. It is robust under spatial variations in the intensity of light emitted by the projector on the projection surface and also robust under stationary photometric perturbations caused by external factors. Moreover, we describe the experiments carried out to show the reliability of the method. version:1
arxiv-1409-7202 | A Boosting Framework on Grounds of Online Learning | http://arxiv.org/abs/1409.7202 | id:1409.7202 author:Tofigh Naghibi, Beat Pfister category:cs.LG  published:2014-09-25 summary:By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and some generalization to double-projection online learning algorithms, as a by-product. version:3
arxiv-1411-6235 | Balanced k-Means and Min-Cut Clustering | http://arxiv.org/abs/1411.6235 | id:1411.6235 author:Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang category:cs.LG  published:2014-11-23 summary:Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms. version:1
arxiv-1411-6233 | A Convex Sparse PCA for Feature Analysis | http://arxiv.org/abs/1411.6233 | id:1411.6233 author:Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang category:cs.LG  published:2014-11-23 summary:Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l 2 , 1 -norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms. version:1
arxiv-1411-6206 | Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for Dynamic 3D MRI reconstruction | http://arxiv.org/abs/1411.6206 | id:1411.6206 author:Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim category:cs.CV  published:2014-11-23 summary:It has been recently shown that incorporating priori knowledge significantly improves the performance of basic compressive sensing based approaches. We have managed to successfully exploit this idea for recovering a matrix as a summation of a Low-rank and a Sparse component from compressive measurements. When applied to the problem of construction of 4D Cardiac MR image sequences in real-time from highly under-sampled $k-$space data, our proposed method achieves superior reconstruction quality compared to the other state-of-the-art methods. version:1
arxiv-1411-6203 | Efficient Minimax Signal Detection on Graphs | http://arxiv.org/abs/1411.6203 | id:1411.6203 author:Jing Qian, Venkatesh Saligrama category:stat.ML  published:2014-11-23 summary:Several problems such as network intrusion, community detection, and disease outbreak can be described by observations attributed to nodes or edges of a graph. In these applications presence of intrusion, community or disease outbreak is characterized by novel observations on some unknown connected subgraph. These problems can be formulated in terms of optimization of suitable objectives on connected subgraphs, a problem which is generally computationally difficult. We overcome the combinatorics of connectivity by embedding connected subgraphs into linear matrix inequalities (LMI). Computationally efficient tests are then realized by optimizing convex objective functions subject to these LMI constraints. We prove, by means of a novel Euclidean embedding argument, that our tests are minimax optimal for exponential family of distributions on 1-D and 2-D lattices. We show that internal conductance of the connected subgraph family plays a fundamental role in characterizing detectability. version:1
arxiv-1411-6191 | Kickback cuts Backprop's red-tape: Biologically plausible credit assignment in neural networks | http://arxiv.org/abs/1411.6191 | id:1411.6191 author:David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann category:cs.LG cs.NE q-bio.NC  published:2014-11-23 summary:Error backpropagation is an extremely effective algorithm for assigning credit in artificial neural networks. However, weight updates under Backprop depend on lengthy recursive computations and require separate output and error messages -- features not shared by biological neurons, that are perhaps unnecessary. In this paper, we revisit Backprop and the credit assignment problem. We first decompose Backprop into a collection of interacting learning algorithms; provide regret bounds on the performance of these sub-algorithms; and factorize Backprop's error signals. Using these results, we derive a new credit assignment algorithm for nonparametric regression, Kickback, that is significantly simpler than Backprop. Finally, we provide a sufficient condition for Kickback to follow error gradients, and show that Kickback matches Backprop's performance on real-world regression benchmarks. version:1
arxiv-1411-6160 | Characterization of the equivalence of robustification and regularization in linear, median, and matrix regression | http://arxiv.org/abs/1411.6160 | id:1411.6160 author:Dimitris Bertsimas, Martin S. Copenhaver category:math.ST cs.LG math.OC stat.ML stat.TH  published:2014-11-22 summary:Sparsity is a key driver in modern statistical problems, from linear regression via the Lasso to matrix regression with nuclear norm penalties in matrix completion and beyond. In stark contrast to sparsity motivations for such problems, it is known in the field of robust optimization that a variety of vector regression problems, such as Lasso which appears as a loss function plus a regularization penalty, can arise by simply immunizing a nominal problem (with only a loss function) to uncertainty in the data. Such a robustification offers an explanation for why some linear regression methods perform well in the face of noise, even when these methods do not produce reliably sparse solutions. In this paper we deepen and extend the understanding of the connection between robustification and regularization in regression problems. Specifically, (a) in the context of linear regression, we characterize under which conditions on the model of uncertainty used and on the loss function penalties robustification and regularization are equivalent; (b) we show how to tractably robustify median regression problems; and (c) we extend the characterization of robustification and regularization to matrix regression problems (matrix completion and Principal Component Analysis). version:1
arxiv-1404-3862 | Optimizing the CVaR via Sampling | http://arxiv.org/abs/1404.3862 | id:1404.3862 author:Aviv Tamar, Yonatan Glassner, Shie Mannor category:stat.ML cs.AI cs.LG  published:2014-04-15 summary:Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the CVaR gradient, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risk-sensitive controller for the game of Tetris. version:4
arxiv-1402-5988 | Incremental Learning of Event Definitions with Inductive Logic Programming | http://arxiv.org/abs/1402.5988 | id:1402.5988 author:Nikos Katzouris, Alexander Artikis, George Paliouras category:cs.LG cs.AI  published:2014-02-24 summary:Event recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time. The manual development of such knowledge is a tedious and error-prone task, thus event-based applications may benefit from automated knowledge construction techniques, such as Inductive Logic Programming (ILP), which combines machine learning with the declarative and formal semantics of First-Order Logic. However, learning temporal logical formalisms, which are typically utilized by logic-based Event Recognition systems is a challenging task, which most ILP systems cannot fully undertake. In addition, event-based data is usually massive and collected at different times and under various circumstances. Ideally, systems that learn from temporal data should be able to operate in an incremental mode, that is, revise prior constructed knowledge in the face of new evidence. Most ILP systems are batch learners, in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch. Given the increased inherent complexity of ILP and the volumes of real-life temporal data, this results to algorithms that scale poorly. In this work we present an incremental method for learning and revising event-based knowledge, in the form of Event Calculus programs. The proposed algorithm relies on abductive-inductive learning and comprises a scalable clause refinement methodology, based on a compressive summarization of clause coverage in a stream of examples. We present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications. version:2
arxiv-1406-3781 | From Stochastic Mixability to Fast Rates | http://arxiv.org/abs/1406.3781 | id:1406.3781 author:Nishant A. Mehta, Robert C. Williamson category:cs.LG stat.ML  published:2014-06-14 summary:Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution $\mathsf{P}$ and returns a hypothesis $f$ chosen from a fixed class $\mathcal{F}$ with small loss $\ell$. In the parametric setting, depending upon $(\ell, \mathcal{F},\mathsf{P})$ ERM can have slow $(1/\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a function of the sample size $n$. There exist several results that give sufficient conditions for fast rates in terms of joint properties of $\ell$, $\mathcal{F}$, and $\mathsf{P}$, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss $\ell$ (there being no role there for $\mathcal{F}$ or $\mathsf{P}$). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of $(\ell,\mathcal{F}, \mathsf{P})$, and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible. version:2
arxiv-1411-6091 | Virtual View Networks for Object Reconstruction | http://arxiv.org/abs/1411.6091 | id:1411.6091 author:Jo√£o Carreira, Abhishek Kar, Shubham Tulsiani, Jitendra Malik category:cs.CV  published:2014-11-22 summary:All that structure from motion algorithms "see" are sets of 2D points. We show that these impoverished views of the world can be faked for the purpose of reconstructing objects in challenging settings, such as from a single image, or from a few ones far apart, by recognizing the object and getting help from a collection of images of other objects from the same class. We synthesize virtual views by computing geodesics on novel networks connecting objects with similar viewpoints, and introduce techniques to increase the specificity and robustness of factorization-based object reconstruction in this setting. We report accurate object shape reconstruction from a single image on challenging PASCAL VOC data, which suggests that the current domain of applications of rigid structure-from-motion techniques may be significantly extended. version:1
arxiv-1411-6081 | PU Learning for Matrix Completion | http://arxiv.org/abs/1411.6081 | id:1411.6081 author:Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit S. Dhillon category:cs.LG cs.NA stat.ML  published:2014-11-22 summary:In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M, and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only "likes" or "friendships" are observed. The problem of learning from only positive and unlabeled examples, called PU (positive-unlabeled) learning, has been studied in the context of binary classification. We consider the PU matrix completion problem, where an underlying real-valued matrix M is first quantized to generate one-bit observations and then a subset of positive entries is revealed. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a "shifted matrix completion" method that recovers M using only a subset of indices corresponding to ones, while for the second case, we propose a "biased matrix completion" method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds --- if M is n by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes the fraction of ones observed. This implies a sample complexity of O(n\log n) ones to achieve a small error, when M is dense and n is large. We extend our methods and guarantees to the inductive matrix completion problem, where rows and columns of M have associated features. We provide efficient and scalable optimization procedures for both the methods and demonstrate the effectiveness of the proposed methods for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks. version:1
arxiv-1410-8275 | Stable Autoencoding: A Flexible Framework for Regularized Low-Rank Matrix Estimation | http://arxiv.org/abs/1410.8275 | id:1410.8275 author:Julie Josse, Stefan Wager category:stat.ME cs.LG stat.ML  published:2014-10-30 summary:We develop a framework for low-rank matrix estimation that allows us to transform noise models into regularization schemes via a simple parametric bootstrap. Effectively, our procedure seeks an autoencoding basis for the observed matrix that is robust with respect to the specified noise model. In the simplest case, with an isotropic noise model, our procedure is equivalent to a classical singular value shrinkage estimator. For non-isotropic noise models, however, our method does not reduce to singular value shrinkage, and instead yields new estimators that perform well in experiments. Moreover, by iterating our stable autoencoding scheme, we can automatically generate low-rank estimates without specifying the target rank as a tuning parameter. version:2
arxiv-1411-6031 | Finding Action Tubes | http://arxiv.org/abs/1411.6031 | id:1411.6031 author:Georgia Gkioxari, Jitendra Malik category:cs.CV  published:2014-11-21 summary:We address the problem of action detection in videos. Driven by the latest progress in object detection from 2D images, we build action models using rich feature hierarchies derived from shape and kinematic cues. We incorporate appearance and motion in two ways. First, starting from image region proposals we select those that are motion salient and thus are more likely to contain the action. This leads to a significant reduction in the number of regions being processed and allows for faster computations. Second, we extract spatio-temporal feature representations to build strong classifiers using Convolutional Neural Networks. We link our predictions to produce detections consistent in time, which we call action tubes. We show that our approach outperforms other techniques in the task of action detection. version:1
arxiv-1411-5428 | Differentially Private Algorithms for Empirical Machine Learning | http://arxiv.org/abs/1411.5428 | id:1411.5428 author:Ben Stoddard, Yan Chen, Ashwin Machanavajjhala category:cs.LG  published:2014-11-20 summary:An important use of private data is to build machine learning classifiers. While there is a burgeoning literature on differentially private classification algorithms, we find that they are not practical in real applications due to two reasons. First, existing differentially private classifiers provide poor accuracy on real world datasets. Second, there is no known differentially private algorithm for empirically evaluating the private classifier on a private test dataset. In this paper, we develop differentially private algorithms that mirror real world empirical machine learning workflows. We consider the private classifier training algorithm as a blackbox. We present private algorithms for selecting features that are input to the classifier. Though adding a preprocessing step takes away some of the privacy budget from the actual classification process (thus potentially making it noisier and less accurate), we show that our novel preprocessing techniques significantly increase classifier accuracy on three real-world datasets. We also present the first private algorithms for empirically constructing receiver operating characteristic (ROC) curves on a private test set. version:2
arxiv-1411-5977 | On the Impossibility of Convex Inference in Human Computation | http://arxiv.org/abs/1411.5977 | id:1411.5977 author:Nihar B. Shah, Dengyong Zhou category:stat.ML cs.HC cs.LG  published:2014-11-21 summary:Human computation or crowdsourcing involves joint inference of the ground-truth-answers and the worker-abilities by optimizing an objective function, for instance, by maximizing the data likelihood based on an assumed underlying model. A variety of methods have been proposed in the literature to address this inference problem. As far as we know, none of the objective functions in existing methods is convex. In machine learning and applied statistics, a convex function such as the objective function of support vector machines (SVMs) is generally preferred, since it can leverage the high-performance algorithms and rigorous guarantees established in the extensive literature on convex optimization. One may thus wonder if there exists a meaningful convex objective function for the inference problem in human computation. In this paper, we investigate this convexity issue for human computation. We take an axiomatic approach by formulating a set of axioms that impose two mild and natural assumptions on the objective function for the inference. Under these axioms, we show that it is unfortunately impossible to ensure convexity of the inference problem. On the other hand, we show that interestingly, in the absence of a requirement to model "spammers", one can construct reasonable objective functions for crowdsourcing that guarantee convex inference. version:1
arxiv-1410-4510 | Graph-Sparse LDA: A Topic Model with Structured Sparsity | http://arxiv.org/abs/1410.4510 | id:1410.4510 author:Finale Doshi-Velez, Byron Wallace, Ryan Adams category:stat.ML cs.CL cs.LG  published:2014-10-16 summary:Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest. Unfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. To improve topic interpretability, we introduce Graph-Sparse LDA, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). In our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. Graph-Sparse LDA recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance. version:2
arxiv-1407-4000 | Uncertainty And Evolutionary Optimization: A Novel Approach | http://arxiv.org/abs/1407.4000 | id:1407.4000 author:Maumita Bhattacharya, R. Islam, A. Mahmood category:cs.NE 68T99 F.1.1  published:2014-07-15 summary:Evolutionary algorithms (EA) have been widely accepted as efficient solvers for complex real world optimization problems, including engineering optimization. However, real world optimization problems often involve uncertain environment including noisy and/or dynamic environments, which pose major challenges to EA-based optimization. The presence of noise interferes with the evaluation and the selection process of EA, and thus adversely affects its performance. In addition, as presence of noise poses challenges to the evaluation of the fitness function, it may need to be estimated instead of being evaluated. Several existing approaches attempt to address this problem, such as introduction of diversity (hyper mutation, random immigrants, special operators) or incorporation of memory of the past (diploidy, case based memory). However, these approaches fail to adequately address the problem. In this paper we propose a Distributed Population Switching Evolutionary Algorithm (DPSEA) method that addresses optimization of functions with noisy fitness using a distributed population switching architecture, to simulate a distributed self-adaptive memory of the solution space. Local regression is used in the pseudo-populations to estimate the fitness. Successful applications to benchmark test problems ascertain the proposed method's superior performance in terms of both robustness and accuracy. version:2
arxiv-1411-5873 | Randomized Dual Coordinate Ascent with Arbitrary Sampling | http://arxiv.org/abs/1411.5873 | id:1411.5873 author:Zheng Qu, Peter Richt√°rik, Tong Zhang category:math.OC cs.LG cs.NA math.NA  published:2014-11-21 summary:We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution. In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error. Depending on the choice of the sampling, we obtain efficient serial, parallel and distributed variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data. We calculate theoretical speedup factors and find that they are excellent predictors of actual speedup in practice. Moreover, we illustrate that it is possible to design an efficient mini-batch importance sampling. The distributed variant of Quartz is the first distributed SDCA-like method with an analysis for non-separable data. version:1
arxiv-1411-5825 | Assessment of algorithms for mitosis detection in breast cancer histopathology images | http://arxiv.org/abs/1411.5825 | id:1411.5825 author:Mitko Veta, Paul J. van Diest, Stefan M. Willems, Haibo Wang, Anant Madabhushi, Angel Cruz-Roa, Fabio Gonzalez, Anders B. L. Larsen, Jacob S. Vestergaard, Anders B. Dahl, Dan C. Cire≈üan, J√ºrgen Schmidhuber, Alessandro Giusti, Luca M. Gambardella, F. Boray Tek, Thomas Walter, Ching-Wei Wang, Satoshi Kondo, Bogdan J. Matuszewski, Frederic Precioso, Violet Snell, Josef Kittler, Teofilo E. de Campos, Adnan M. Khan, Nasir M. Rajpoot, Evdokia Arkoumani, Miangela M. Lacle, Max A. Viergever, Josien P. W. Pluim category:cs.CV  published:2014-11-21 summary:The proliferative activity of breast tumors, which is routinely estimated by counting of mitotic figures in hematoxylin and eosin stained histology sections, is considered to be one of the most important prognostic markers. However, mitosis counting is laborious, subjective and may suffer from low inter-observer agreement. With the wider acceptance of whole slide images in pathology labs, automatic image analysis has been proposed as a potential solution for these issues. In this paper, the results from the Assessment of Mitosis Detection Algorithms 2013 (AMIDA13) challenge are described. The challenge was based on a data set consisting of 12 training and 11 testing subjects, with more than one thousand annotated mitotic figures by multiple observers. Short descriptions and results from the evaluation of eleven methods are presented. The top performing method has an error rate that is comparable to the inter-observer agreement among pathologists. version:1
arxiv-1405-7752 | Learning to Act Greedily: Polymatroid Semi-Bandits | http://arxiv.org/abs/1405.7752 | id:1405.7752 author:Branislav Kveton, Zheng Wen, Azin Ashkan, Michal Valko category:cs.LG cs.AI stat.ML  published:2014-05-30 summary:Many important optimization problems, such as the minimum spanning tree and minimum-cost flow, can be solved optimally by a greedy method. In this work, we study a learning variant of these problems, where the model of the problem is unknown and has to be learned by interacting repeatedly with the environment in the bandit setting. We formalize our learning problem quite generally, as learning how to maximize an unknown modular function on a known polymatroid. We propose a computationally efficient algorithm for solving our problem and bound its expected cumulative regret. Our gap-dependent upper bound is tight up to a constant and our gap-free upper bound is tight up to polylogarithmic factors. Finally, we evaluate our method on three problems and demonstrate that it is practical. version:3
arxiv-1411-5796 | Pre-processing of Domain Ontology Graph Generation System in Punjabi | http://arxiv.org/abs/1411.5796 | id:1411.5796 author:Rajveer Kaur, Saurabh Sharma category:cs.CL  published:2014-11-21 summary:This paper describes pre-processing phase of ontology graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured representation of the input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, removal of duplicate terms, removal of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms. version:1
arxiv-1403-0057 | Real-time Topic-aware Influence Maximization Using Preprocessing | http://arxiv.org/abs/1403.0057 | id:1403.0057 author:Wei Chen, Tian Lin, Cheng Yang category:cs.SI cs.LG F.2.2  published:2014-03-01 summary:Influence maximization is the task of finding a set of seed nodes in a social network such that the influence spread of these seed nodes based on certain influence diffusion model is maximized. Topic-aware influence diffusion models have been recently proposed to address the issue that influence between a pair of users are often topic-dependent and information, ideas, innovations etc. being propagated in networks (referred collectively as items in this paper) are typically mixtures of topics. In this paper, we focus on the topic-aware influence maximization task. In particular, we study preprocessing methods for these topics to avoid redoing influence maximization for each item from scratch. We explore two preprocessing algorithms with theoretical justifications. Our empirical results on data obtained in a couple of existing studies demonstrate that one of our algorithms stands out as a strong candidate providing microsecond online response time and competitive influence spread, with reasonable preprocessing effort. version:2
arxiv-1404-2086 | Cascades of Regression Tree Fields for Image Restoration | http://arxiv.org/abs/1404.2086 | id:1404.2086 author:Uwe Schmidt, Jeremy Jancsary, Sebastian Nowozin, Stefan Roth, Carsten Rother category:cs.CV  published:2014-04-08 summary:Conditional random fields (CRFs) are popular discriminative models for computer vision and have been successfully applied in the domain of image restoration, especially to image denoising. For image deblurring, however, discriminative approaches have been mostly lacking. We posit two reasons for this: First, the blur kernel is often only known at test time, requiring any discriminative approach to cope with considerable variability. Second, given this variability it is quite difficult to construct suitable features for discriminative prediction. To address these challenges we first show a connection between common half-quadratic inference for generative image priors and Gaussian CRFs. Based on this analysis, we then propose a cascade model for image restoration that consists of a Gaussian CRF at each stage. Each stage of our cascade is semi-parametric, i.e. it depends on the instance-specific parameters of the restoration problem, such as the blur kernel. We train our model by loss minimization with synthetically generated training data. Our experiments show that when applied to non-blind image deblurring, the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur. Moreover, we demonstrate its suitability for image denoising, where we achieve competitive results for grayscale and color images. version:2
arxiv-1305-5753 | A probabilistic framework for analysing the compositionality of conceptual combinations | http://arxiv.org/abs/1305.5753 | id:1305.5753 author:Peter D. Bruza, Kirsty Kitto, Brentyn J. Ramm, Laurianne Sitbon category:cs.CL  published:2013-05-23 summary:Conceptual combination performs a fundamental role in creating the broad range of compound phrases utilized in everyday language. This article provides a novel probabilistic framework for assessing whether the semantics of conceptual combinations are compositional, and so can be considered as a function of the semantics of the constituent concepts, or not. While the systematicity and productivity of language provide a strong argument in favor of assuming compositionality, this very assumption is still regularly questioned in both cognitive science and philosophy. Additionally, the principle of semantic compositionality is underspecified, which means that notions of both "strong" and "weak" compositionality appear in the literature. Rather than adjudicating between different grades of compositionality, the framework presented here contributes formal methods for determining a clear dividing line between compositional and non-compositional semantics. In addition, we suggest that the distinction between these is contextually sensitive. Utilizing formal frameworks developed for analyzing composite systems in quantum theory, we present two methods that allow the semantics of conceptual combinations to be classified as "compositional" or "non-compositional". Compositionality is first formalised by factorising the joint probability distribution modeling the combination, where the terms in the factorisation correspond to individual concepts. This leads to the necessary and sufficient condition for the joint probability distribution to exist. A failure to meet this condition implies that the underlying concepts cannot be modeled in a single probability space when considering their combination, and the combination is thus deemed "non-compositional". The formal analysis methods are demonstrated by applying them to an empirical study of twenty-four non-lexicalised conceptual combinations. version:3
arxiv-1411-5732 | A Joint Probabilistic Classification Model of Relevant and Irrelevant Sentences in Mathematical Word Problems | http://arxiv.org/abs/1411.5732 | id:1411.5732 author:Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park, Ron Tzur category:cs.CL cs.IR cs.LG stat.ML  published:2014-11-21 summary:Estimating the difficulty level of math word problems is an important task for many educational applications. Identification of relevant and irrelevant sentences in math word problems is an important step for calculating the difficulty levels of such problems. This paper addresses a novel application of text categorization to identify two types of sentences in mathematical word problems, namely relevant and irrelevant sentences. A novel joint probabilistic classification model is proposed to estimate the joint probability of classification decisions for all sentences of a math word problem by utilizing the correlation among all sentences along with the correlation between the question sentence and other sentences, and sentence text. The proposed model is compared with i) a SVM classifier which makes independent classification decisions for individual sentences by only using the sentence text and ii) a novel SVM classifier that considers the correlation between the question sentence and other sentences along with the sentence text. An extensive set of experiments demonstrates the effectiveness of the joint probabilistic classification model for identifying relevant and irrelevant sentences as well as the novel SVM classifier that utilizes the correlation between the question sentence and other sentences. Furthermore, empirical results and analysis show that i) it is highly beneficial not to remove stopwords and ii) utilizing part of speech tagging does not make a significant improvement although it has been shown to be effective for the related task of math word problem type classification. version:1
arxiv-1411-5731 | Visual Sentiment Prediction with Deep Convolutional Neural Networks | http://arxiv.org/abs/1411.5731 | id:1411.5731 author:Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li category:cs.CV cs.NE stat.ML  published:2014-11-21 summary:Images have become one of the most popular types of media through which users convey their emotions within online social networks. Although vast amount of research is devoted to sentiment analysis of textual data, there has been very limited work that focuses on analyzing sentiment of image data. In this work, we propose a novel visual sentiment prediction framework that performs image understanding with Deep Convolutional Neural Networks (CNN). Specifically, the proposed sentiment prediction framework performs transfer learning from a CNN with millions of parameters, which is pre-trained on large-scale data for object recognition. Experiments conducted on two real-world datasets from Twitter and Tumblr demonstrate the effectiveness of the proposed visual sentiment analysis framework. version:1
arxiv-1411-5720 | Metric recovery from directed unweighted graphs | http://arxiv.org/abs/1411.5720 | id:1411.5720 author:Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola category:stat.ML cs.SI math.ST stat.ME stat.TH  published:2014-11-20 summary:We analyze directed, unweighted graphs obtained from $x_i\in \mathbb{R}^d$ by connecting vertex $i$ to $j$ iff $ x_i - x_j < \epsilon(x_i)$. Examples of such graphs include $k$-nearest neighbor graphs, where $\epsilon(x_i)$ varies from point to point, and, arguably, many real world graphs such as co-purchasing graphs. We ask whether we can recover the underlying Euclidean metric $\epsilon(x_i)$ and the associated density $p(x_i)$ given only the directed graph and $d$. We show that consistent recovery is possible up to isometric scaling when the vertex degree is at least $\omega(n^{2/(2+d)}\log(n)^{d/(d+2)})$. Our estimator is based on a careful characterization of a random walk over the directed graph and the associated continuum limit. As an algorithm, it resembles the PageRank centrality metric. We demonstrate empirically that the estimator performs well on simulated examples as well as on real-world co-purchasing graphs even with a small number of points and degree scaling as low as $\log(n)$. version:1
arxiv-1412-2067 | An algorithm for improving Non-Local Means operators via low-rank approximation | http://arxiv.org/abs/1412.2067 | id:1412.2067 author:Victor May, Yosi Keller, Nir Sharon, Yoel Shkolnisky category:cs.CV math.GM  published:2014-11-20 summary:We present a method for improving a Non Local Means operator by computing its low-rank approximation. The low-rank operator is constructed by applying a filter to the spectrum of the original Non Local Means operator. This results in an operator which is less sensitive to noise while preserving important properties of the original operator. The method is efficiently implemented based on Chebyshev polynomials and is demonstrated on the application of natural images denoising. For this application, we provide a comprehensive comparison of our method with leading denoising methods. version:1
arxiv-1411-5988 | Clustering evolving data using kernel-based methods | http://arxiv.org/abs/1411.5988 | id:1411.5988 author:Rocco Langone category:cs.SI cs.LG stat.ML  published:2014-11-20 summary:In this thesis, we propose several modelling strategies to tackle evolving data in different contexts. In the framework of static clustering, we start by introducing a soft kernel spectral clustering (SKSC) algorithm, which can better deal with overlapping clusters with respect to kernel spectral clustering (KSC) and provides more interpretable outcomes. Afterwards, a whole strategy based upon KSC for community detection of static networks is proposed, where the extraction of a high quality training sub-graph, the choice of the kernel function, the model selection and the applicability to large-scale data are key aspects. This paves the way for the development of a novel clustering algorithm for the analysis of evolving networks called kernel spectral clustering with memory effect (MKSC), where the temporal smoothness between clustering results in successive time steps is incorporated at the level of the primal optimization problem, by properly modifying the KSC formulation. Later on, an application of KSC to fault detection of an industrial machine is presented. Here, a smart pre-processing of the data by means of a proper windowing operation is necessary to catch the ongoing degradation process affecting the machine. In this way, in a genuinely unsupervised manner, it is possible to raise an early warning when necessary, in an online fashion. Finally, we propose a new algorithm called incremental kernel spectral clustering (IKSC) for online learning of non-stationary data. This ambitious challenge is faced by taking advantage of the out-of-sample property of kernel spectral clustering (KSC) to adapt the initial model, in order to tackle merging, splitting or drifting of clusters across time. Real-world applications considered in this thesis include image segmentation, time-series clustering, community detection of static and evolving networks. version:1
arxiv-1411-5654 | Learning a Recurrent Visual Representation for Image Caption Generation | http://arxiv.org/abs/1411.5654 | id:1411.5654 author:Xinlei Chen, C. Lawrence Zitnick category:cs.CV cs.AI cs.CL  published:2014-11-20 summary:In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features. version:1
arxiv-1411-5639 | N-sphere chord length distribution | http://arxiv.org/abs/1411.5639 | id:1411.5639 author:Panagiotis Sidiropoulos category:math.PR stat.ML  published:2014-11-20 summary:This work studies the chord length distribution, in the case where both ends lie on a $N$-dimensional hypersphere ($N \geq 2$). Actually, after connecting this distribution to the recently estimated surface of a hyperspherical cap \cite{SLi11}, closed-form expressions of both the probability density function and the cumulative distribution function are straightforwardly extracted, which are followed by a discussion on its basic properties, among which its dependence from the hypersphere dimension. Additionally, the distribution of the dot product of unitary vectors is estimated, a problem that is related to the chord length. version:1
arxiv-1406-5362 | Predicting the Future Behavior of a Time-Varying Probability Distribution | http://arxiv.org/abs/1406.5362 | id:1406.5362 author:Christoph H. Lampert category:stat.ML cs.LG  published:2014-06-20 summary:We study the problem of predicting the future, though only in the probabilistic sense of estimating a future state of a time-varying probability distribution. This is not only an interesting academic problem, but solving this extrapolation problem also has many practical application, e.g. for training classifiers that have to operate under time-varying conditions. Our main contribution is a method for predicting the next step of the time-varying distribution from a given sequence of sample sets from earlier time steps. For this we rely on two recent machine learning techniques: embedding probability distributions into a reproducing kernel Hilbert space, and learning operators by vector-valued regression. We illustrate the working principles and the practical usefulness of our method by experiments on synthetic and real data. We also highlight an exemplary application: training a classifier in a domain adaptation setting without having access to examples from the test time distribution at training time. version:2
arxiv-1411-3827 | Autonomization of Monoidal Categories | http://arxiv.org/abs/1411.3827 | id:1411.3827 author:Antonin Delpeuch category:math.CT cs.CL 18D10  published:2014-11-14 summary:We define the free autonomous category generated by a monoidal category and study some of its properties. From a linguistic perspective, this expands the range of possible models of meaning within the distributional compositional framework, by allowing nonlinearities in maps. From a categorical point of view, this provides a factorization of the construction in [Preller and Lambek, 2007] of the free autonomous category generated by a category. version:2
arxiv-1411-0877 | Vector Autoregressions with Parsimoniously Time Varying Parameters and an Application to Monetary Policy | http://arxiv.org/abs/1411.0877 | id:1411.0877 author:Laurent Callot, Johannes Tang Kristensen category:math.ST stat.ML stat.TH 91G70  published:2014-11-04 summary:This paper proposes a parsimoniously time varying parameter vector autoregressive model (with exogenous variables, VARX) and studies the properties of the Lasso and adaptive Lasso as estimators of this model. The parameters of the model are assumed to follow parsimonious random walks, where parsimony stems from the assumption that increments to the parameters have a non-zero probability of being exactly equal to zero. By varying the degree of parsimony our model can accommodate constant parameters, an unknown number of structural breaks, or parameters with a high degree of variation. We characterize the finite sample properties of the Lasso by deriving upper bounds on the estimation and prediction errors that are valid with high probability; and asymptotically we show that these bounds tend to zero with probability tending to one if the number of non zero increments grows slower than $\sqrt{T}$. By simulation experiments we investigate the properties of the Lasso and the adaptive Lasso in settings where the parameters are stable, experience structural breaks, or follow a parsimonious random walk. We use our model to investigate the monetary policy response to inflation and business cycle fluctuations in the US by estimating a parsimoniously time varying parameter Taylor rule. We document substantial changes in the policy response of the Fed in the 1980s and since 2008. version:2
arxiv-1411-5555 | Maximum Likelihood Directed Enumeration Method in Piecewise-Regular Object Recognition | http://arxiv.org/abs/1411.5555 | id:1411.5555 author:Andrey Savchenko category:cs.CV 68T10 I.5.1; I.5.4  published:2014-11-20 summary:We explore the problems of classification of composite object (images, speech signals) with low number of models per class. We study the question of improving recognition performance for medium-sized database (thousands of classes). The key issue of fast approximate nearest-neighbor methods widely applied in this task is their heuristic nature. It is possible to strongly prove their efficiency by using the theory of algorithms only for simple similarity measures and artificially generated tasks. On the contrary, in this paper we propose an alternative, statistically optimal greedy algorithm. At each step of this algorithm joint density (likelihood) of distances to previously checked models is estimated for each class. The next model to check is selected from the class with the maximal likelihood. The latter is estimated based on the asymptotic properties of the Kullback-Leibler information discrimination and mathematical model of piecewise-regular object with distribution of each regular segment of exponential type. Experimental results in face recognition for FERET dataset prove that the proposed method is much more effective than not only brute force and the baseline (directed enumeration method) but also approximate nearest neighbor methods from FLANN and NonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort). version:1
arxiv-1307-3176 | Fast gradient descent for drifting least squares regression, with application to bandits | http://arxiv.org/abs/1307.3176 | id:1307.3176 author:Nathaniel Korda, Prashanth L. A., R√©mi Munos category:cs.LG stat.ML  published:2013-07-11 summary:Online learning algorithms require to often recompute least squares regression estimates of parameters. We study improving the computational complexity of such algorithms by using stochastic gradient descent (SGD) type schemes in place of classic regression solvers. We show that SGD schemes efficiently track the true solutions of the regression problems, even in the presence of a drift. This finding coupled with an $O(d)$ improvement in complexity, where $d$ is the dimension of the data, make them attractive for implementation in the big data settings. In the case when strong convexity in the regression problem is guaranteed, we provide bounds on the error both in expectation and high probability (the latter is often needed to provide theoretical guarantees for higher level algorithms), despite the drifting least squares solution. As an example of this case we prove that the regret performance of an SGD version of the PEGE linear bandit algorithm [Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself only by a factor of $O(\log^4 n)$. When strong convexity of the regression problem cannot be guaranteed, we investigate using an adaptive regularisation. We make an empirical study of an adaptively regularised, SGD version of LinUCB [Li et al. 2010] in a news article recommendation application, which uses the large scale news recommendation dataset from Yahoo! front page. These experiments show a large gain in computational complexity, with a consistently low tracking error and click-through-rate (CTR) performance that is $75\%$ close. version:4
arxiv-1406-0156 | $l_1$-regularized Outlier Isolation and Regression | http://arxiv.org/abs/1406.0156 | id:1406.0156 author:Sheng Han, Suzhen Wang, Xinyu Wu category:cs.CV cs.LG stat.ML  published:2014-06-01 summary:This paper proposed a new regression model called $l_1$-regularized outlier isolation and regression (LOIRE) and a fast algorithm based on block coordinate descent to solve this model. Besides, assuming outliers are gross errors following a Bernoulli process, this paper also presented a Bernoulli estimate model which, in theory, should be very accurate and robust due to its complete elimination of affections caused by outliers. Though this Bernoulli estimate is hard to solve, it could be approximately achieved through a process which takes LOIRE as an important intermediate step. As a result, the approximate Bernoulli estimate is a good combination of Bernoulli estimate's accuracy and LOIRE regression's efficiency with several simulations conducted to strongly verify this point. Moreover, LOIRE can be further extended to realize robust rank factorization which is powerful in recovering low-rank component from massive corruptions. Extensive experimental results showed that the proposed method outperforms state-of-the-art methods like RPCA and GoDec in the aspect of computation speed with a competitive performance. version:2
arxiv-1303-7117 | Confidence sets for persistence diagrams | http://arxiv.org/abs/1303.7117 | id:1303.7117 author:Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, Larry Wasserman, Sivaraman Balakrishnan, Aarti Singh category:math.ST cs.CG cs.LG stat.TH  published:2013-03-28 summary:Persistent homology is a method for probing topological properties of point clouds and functions. The method involves tracking the birth and death of topological features (2000) as one varies a tuning parameter. Features with short lifetimes are informally considered to be "topological noise," and those with a long lifetime are considered to be "topological signal." In this paper, we bring some statistical ideas to persistent homology. In particular, we derive confidence sets that allow us to separate topological signal from topological noise. version:3
arxiv-1411-5458 | Liquid State Machine with Dendritically Enhanced Readout for Low-power, Neuromorphic VLSI Implementations | http://arxiv.org/abs/1411.5458 | id:1411.5458 author:Subhrajit Roy, Amitava Banerjee, Arindam Basu category:cs.ET cs.NE  published:2014-11-20 summary:In this paper, we describe a new neuro-inspired, hardware-friendly readout stage for the liquid state machine (LSM), a popular model for reservoir computing. Compared to the parallel perceptron architecture trained by the p-delta algorithm, which is the state of the art in terms of performance of readout stages, our readout architecture and learning algorithm can attain better performance with significantly less synaptic resources making it attractive for VLSI implementation. Inspired by the nonlinear properties of dendrites in biological neurons, our readout stage incorporates neurons having multiple dendrites with a lumped nonlinearity. The number of synaptic connections on each branch is significantly lower than the total number of connections from the liquid neurons and the learning algorithm tries to find the best 'combination' of input connections on each branch to reduce the error. Hence, the learning involves network rewiring (NRW) of the readout network similar to structural plasticity observed in its biological counterparts. We show that compared to a single perceptron using analog weights, this architecture for the readout can attain, even by using the same number of binary valued synapses, up to 3.3 times less error for a two-class spike train classification problem and 2.4 times less error for an input rate approximation task. Even with 60 times larger synapses, a group of 60 parallel perceptrons cannot attain the performance of the proposed dendritically enhanced readout. An additional advantage of this method for hardware implementations is that the 'choice' of connectivity can be easily implemented exploiting address event representation (AER) protocols commonly used in current neuromorphic systems where the connection matrix is stored in memory. Also, due to the use of binary synapses, our proposed method is more robust against statistical variations. version:1
arxiv-1409-2574 | Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures | http://arxiv.org/abs/1409.2574 | id:1409.2574 author:John R. Hershey, Jonathan Le Roux, Felix Weninger category:cs.LG cs.NE stat.ML  published:2014-09-09 summary:Model-based methods and deep neural networks have both been tremendously successful paradigms in machine learning. In model-based methods, problem domain knowledge can be built into the constraints of the model, typically at the expense of difficulties during inference. In contrast, deterministic deep neural networks are constructed in such a way that inference is straightforward, but their architectures are generic and it is unclear how to incorporate knowledge. This work aims to obtain the advantages of both approaches. To do so, we start with a model-based approach and an associated inference algorithm, and \emph{unfold} the inference iterations as layers in a deep network. Rather than optimizing the original model, we \emph{untie} the model parameters across layers, in order to create a more powerful network. The resulting architecture can be trained discriminatively to perform accurate inference within a fixed network size. We show how this framework allows us to interpret conventional networks as mean-field inference in Markov random fields, and to obtain new architectures by instead using belief propagation as the inference algorithm. We then show its application to a non-negative matrix factorization model that incorporates the problem-domain knowledge that sound sources are additive. Deep unfolding of this model yields a new kind of non-negative deep neural network, that can be trained using a multiplicative backpropagation-style update algorithm. We present speech enhancement experiments showing that our approach is competitive with conventional neural networks despite using far fewer parameters. version:4
arxiv-1209-6299 | Approximate evaluation of marginal association probabilities with belief propagation | http://arxiv.org/abs/1209.6299 | id:1209.6299 author:Jason L. Williams, Roslyn A. Lau category:cs.AI cs.CV  published:2012-09-12 summary:Data association, the problem of reasoning over correspondence between targets and measurements, is a fundamental problem in tracking. This paper presents a graphical model formulation of data association and applies an approximate inference method, belief propagation (BP), to obtain estimates of marginal association probabilities. We prove that BP is guaranteed to converge, and bound the number of iterations necessary. Experiments reveal a favourable comparison to prior methods in terms of accuracy and computational complexity. version:2
arxiv-1406-2431 | Budget-Constrained Item Cold-Start Handling in Collaborative Filtering Recommenders via Optimal Design | http://arxiv.org/abs/1406.2431 | id:1406.2431 author:Oren Anava, Shahar Golan, Nadav Golbandi, Zohar Karnin, Ronny Lempel, Oleg Rokhlenko, Oren Somekh category:cs.IR cs.LG 62K05  published:2014-06-10 summary:It is well known that collaborative filtering (CF) based recommender systems provide better modeling of users and items associated with considerable rating history. The lack of historical ratings results in the user and the item cold-start problems. The latter is the main focus of this work. Most of the current literature addresses this problem by integrating content-based recommendation techniques to model the new item. However, in many cases such content is not available, and the question arises is whether this problem can be mitigated using CF techniques only. We formalize this problem as an optimization problem: given a new item, a pool of available users, and a budget constraint, select which users to assign with the task of rating the new item in order to minimize the prediction error of our model. We show that the objective function is monotone-supermodular, and propose efficient optimal design based algorithms that attain an approximation to its optimum. Our findings are verified by an empirical study using the Netflix dataset, where the proposed algorithms outperform several baselines for the problem at hand. version:2
arxiv-1410-0745 | Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer Depth Camera and Synthetic Data | http://arxiv.org/abs/1410.0745 | id:1410.0745 author:Qiaosong Wang, Vignesh Jagadeesh, Bryan Ressler, Robinson Piramuthu category:cs.CV  published:2014-10-03 summary:Recent advances in consumer depth sensors have created many opportunities for human body measurement and modeling. Estimation of 3D body shape is particularly useful for fashion e-commerce applications such as virtual try-on or fit personalization. In this paper, we propose a method for capturing accurate human body shape and anthropometrics from a single consumer grade depth sensor. We first generate a large dataset of synthetic 3D human body models using real-world body size distributions. Next, we estimate key body measurements from a single monocular depth image. We combine body measurement estimates with local geometry features around key joint positions to form a robust multi-dimensional feature vector. This allows us to conduct a fast nearest-neighbor search to every sample in the dataset and return the closest one. Compared to existing methods, our approach is able to predict accurate full body parameters from a partial view using measurement parameters learned from the synthetic dataset. Furthermore, our system is capable of generating 3D human mesh models in real-time, which is significantly faster than methods which attempt to model shape and pose deformations. To validate the efficiency and applicability of our system, we collected a dataset that contains frontal and back scans of 83 clothed people with ground truth height and weight. Experiments on real-world dataset show that the proposed method can achieve real-time performance with competing results achieving an average error of 1.9 cm in estimated measurements. version:2
arxiv-1411-5340 | Affordances Provide a Fundamental Categorization Principle for Visual Scenes | http://arxiv.org/abs/1411.5340 | id:1411.5340 author:Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M. Beck, Li Fei-Fei category:q-bio.NC cs.CV cs.HC  published:2014-11-19 summary:How do we know that a kitchen is a kitchen by looking? Relatively little is known about how we conceptualize and categorize different visual environments. Traditional models of visual perception posit that scene categorization is achieved through the recognition of a scene's objects, yet these models cannot account for the mounting evidence that human observers are relatively insensitive to the local details in an image. Psychologists have long theorized that the affordances, or actionable possibilities of a stimulus are pivotal to its perception. To what extent are scene categories created from similar affordances? Using a large-scale experiment using hundreds of scene categories, we show that the activities afforded by a visual scene provide a fundamental categorization principle. Affordance-based similarity explained the majority of the structure in the human scene categorization patterns, outperforming alternative similarities based on objects or visual features. We all models were combined, affordances provided the majority of the predictive power in the combined model, and nearly half of the total explained variance is captured only by affordances. These results challenge many existing models of high-level visual perception, and provide immediately testable hypotheses for the functional organization of the human perceptual system. version:1
arxiv-1411-5331 | Visual Noise from Natural Scene Statistics Reveals Human Scene Category Representations | http://arxiv.org/abs/1411.5331 | id:1411.5331 author:Michelle R. Greene, Abraham P. Botros, Diane M. Beck, Li Fei-Fei category:cs.CV cs.HC  published:2014-11-19 summary:Our perceptions are guided both by the bottom-up information entering our eyes, as well as our top-down expectations of what we will see. Although bottom-up visual processing has been extensively studied, comparatively little is known about top-down signals. Here, we describe REVEAL (Representations Envisioned Via Evolutionary ALgorithm), a method for visualizing an observer's internal representation of a complex, real-world scene, allowing us to, for the first time, visualize the top-down information in an observer's mind. REVEAL rests on two innovations for solving this high dimensional problem: visual noise that samples from natural image statistics, and a computer algorithm that collaborates with human observers to efficiently obtain a solution. In this work, we visualize observers' internal representations of a visual scene category (street) using an experiment in which the observer views the naturalistic visual noise and collaborates with the algorithm to externalize his internal representation. As no scene information was presented, observers had to use their internal knowledge of the target, matching it with the visual features in the noise. We matched reconstructed images with images of real-world street scenes to enhance visualization. Critically, we show that the visualized mental images can be used to predict rapid scene detection performance, as each observer had faster and more accurate responses to detecting real-world images that were the most similar to his reconstructed street templates. These results show that it is possible to visualize previously unobservable mental representations of real world stimuli. More broadly, REVEAL provides a general method for objectively examining the content of previously private, subjective mental experiences. version:1
arxiv-1411-5328 | ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections | http://arxiv.org/abs/1411.5328 | id:1411.5328 author:Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV cs.AI cs.LG  published:2014-11-19 summary:Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition system, since it is expensive to obtain fully labeled data for a large number of concept categories. In this paper, we propose ConceptLearner, which is a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically, without human in the loop for additional annotation. We show that these learned detectors could be applied to recognize concepts at image-level and to detect concepts at image region-level accurately. Under domain-specific supervision, we further evaluate the learned concepts for scene recognition on SUN database and for object detection on Pascal VOC 2007. ConceptLearner shows promising performance compared to fully supervised and weakly supervised methods. version:1
arxiv-1411-5309 | End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression | http://arxiv.org/abs/1411.5309 | id:1411.5309 author:Li Wan, David Eigen, Rob Fergus category:cs.CV  published:2014-11-19 summary:Deformable Parts Models and Convolutional Networks each have achieved notable performance in object detection. Yet these two approaches find their strengths in complementary areas: DPMs are well-versed in object composition, modeling fine-grained spatial relationships between parts; likewise, ConvNets are adept at producing powerful image features, having been discriminatively trained directly on the pixels. In this paper, we propose a new model that combines these two approaches, obtaining the advantages of each. We train this model using a new structured loss function that considers all bounding boxes within an image, rather than isolated object instances. This enables the non-maximal suppression (NMS) operation, previously treated as a separate post-processing stage, to be integrated into the model. This allows for discriminative training of our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate our system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results on both benchmarks. version:1
arxiv-1411-5307 | Efficient Media Retrieval from Non-Cooperative Queries | http://arxiv.org/abs/1411.5307 | id:1411.5307 author:Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu category:cs.IR cs.CV  published:2014-11-19 summary:Text is ubiquitous in the artificial world and easily attainable when it comes to book title and author names. Using the images from the book cover set from the Stanford Mobile Visual Search dataset and additional book covers and metadata from openlibrary.org, we construct a large scale book cover retrieval dataset, complete with 100K distractor covers and title and author strings for each. Because our query images are poorly conditioned for clean text extraction, we propose a method for extracting a matching noisy and erroneous OCR readings and matching it against clean author and book title strings in a standard document look-up problem setup. Finally, we demonstrate how to use this text-matching as a feature in conjunction with popular retrieval features such as VLAD using a simple learning setup to achieve significant improvements in retrieval accuracy over that of either VLAD or the text alone. version:1
arxiv-1411-5268 | Sparse distributed localized gradient fused features of objects | http://arxiv.org/abs/1411.5268 | id:1411.5268 author:Swathikiran Sudhakarana, Alex Pappachen James category:cs.CV cs.AI  published:2014-11-19 summary:The sparse, hierarchical, and modular processing of natural signals is related to the ability of humans to recognize objects with high accuracy. In this study, we report a sparse feature processing and encoding method, which improved the recognition performance of an automated object recognition system. Randomly distributed localized gradient enhanced features were selected before employing aggregate functions for representation, where we used a modular and hierarchical approach to detect the object features. These object features were combined with a minimum distance classifier, thereby obtaining object recognition system accuracies of 93% using the Amsterdam library of object images (ALOI) database, 92% using the Columbia object image library (COIL)-100 database, and 69% using the PASCAL visual object challenge 2007 database. The object recognition performance was shown to be robust to variations in noise, object scaling, and object shifts. Finally, a comparison with eight existing object recognition methods indicated that our new method improved the recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10% with the PASCAL visual object challenge 2007 database. version:1
arxiv-1411-5260 | Large-Margin Classification with Multiple Decision Rules | http://arxiv.org/abs/1411.5260 | id:1411.5260 author:Patrick K. Kimes, D. Neil Hayes, J. S. Marron, Yufeng Liu category:stat.ML cs.LG  published:2014-11-19 summary:Binary classification is a common statistical learning problem in which a model is estimated on a set of covariates for some outcome indicating the membership of one of two classes. In the literature, there exists a distinction between hard and soft classification. In soft classification, the conditional class probability is modeled as a function of the covariates. In contrast, hard classification methods only target the optimal prediction boundary. While hard and soft classification methods have been studied extensively, not much work has been done to compare the actual tasks of hard and soft classification. In this paper we propose a spectrum of statistical learning problems which span the hard and soft classification tasks based on fitting multiple decision rules to the data. By doing so, we reveal a novel collection of learning tasks of increasing complexity. We study the problems using the framework of large-margin classifiers and a class of piecewise linear convex surrogates, for which we derive statistical properties and a corresponding sub-gradient descent algorithm. We conclude by applying our approach to simulation settings and a magnetic resonance imaging (MRI) dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. version:1
arxiv-1411-2316 | Zero-Aliasing Correlation Filters for Object Recognition | http://arxiv.org/abs/1411.2316 | id:1411.2316 author:Joseph A. Fernandez, Vishnu Naresh Boddeti, Andres Rodriguez, B. V. K. Vijaya Kumar category:cs.CV stat.ML  published:2014-11-10 summary:Correlation filters (CFs) are a class of classifiers that are attractive for object localization and tracking applications. Traditionally, CFs have been designed in the frequency domain using the discrete Fourier transform (DFT), where correlation is efficiently implemented. However, existing CF designs do not account for the fact that the multiplication of two DFTs in the frequency domain corresponds to a circular correlation in the time/spatial domain. Because this was previously unaccounted for, prior CF designs are not truly optimal, as their optimization criteria do not accurately quantify their optimization intention. In this paper, we introduce new zero-aliasing constraints that completely eliminate this aliasing problem by ensuring that the optimization criterion for a given CF corresponds to a linear correlation rather than a circular correlation. This means that previous CF designs can be significantly improved by this reformulation. We demonstrate the benefits of this new CF design approach with several important CFs. We present experimental results on diverse data sets and present solutions to the computational challenges associated with computing these CFs. Code for the CFs described in this paper and their respective zero-aliasing versions is available at http://vishnu.boddeti.net/projects/correlation-filters.html version:2
arxiv-1411-5172 | Learning nonparametric differential equations with operator-valued kernels and gradient matching | http://arxiv.org/abs/1411.5172 | id:1411.5172 author:Markus Heinonen, Florence d'Alch√©-Buc category:cs.LG stat.ML  published:2014-11-19 summary:Modeling dynamical systems with ordinary differential equations implies a mechanistic view of the process underlying the dynamics. However in many cases, this knowledge is not available. To overcome this issue, we introduce a general framework for nonparametric ODE models using penalized regression in Reproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels. Moreover, we extend the scope of gradient matching approaches to nonparametric ODE. A smooth estimate of the solution ODE is built to provide an approximation of the derivative of the ODE solution which is in turn used to learn the nonparametric ODE model. This approach benefits from the flexibility of penalized regression in RKHS allowing for ridge or (structured) sparse regression as well. Very good results are shown on 3 different ODE systems. version:1
arxiv-1411-5140 | Attentional Neural Network: Feature Selection Using Cognitive Feedback | http://arxiv.org/abs/1411.5140 | id:1411.5140 author:Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang category:cs.CV cs.NE  published:2014-11-19 summary:Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain. version:1
arxiv-1407-0312 | Identifying Outliers in Large Matrices via Randomized Adaptive Compressive Sampling | http://arxiv.org/abs/1407.0312 | id:1407.0312 author:Xingguo Li, Jarvis Haupt category:cs.IT cs.LG math.IT stat.ML  published:2014-07-01 summary:This paper examines the problem of locating outlier columns in a large, otherwise low-rank, matrix. We propose a simple two-step adaptive sensing and inference approach and establish theoretical guarantees for its performance; our results show that accurate outlier identification is achievable using very few linear summaries of the original data matrix -- as few as the squared rank of the low-rank component plus the number of outliers, times constant and logarithmic factors. We demonstrate the performance of our approach experimentally in two stylized applications, one motivated by robust collaborative filtering tasks, and the other by saliency map estimation tasks arising in computer vision and automated surveillance, and also investigate extensions to settings where the data are noisy, or possibly incomplete. version:3
arxiv-1406-6312 | Scalable Topical Phrase Mining from Text Corpora | http://arxiv.org/abs/1406.6312 | id:1406.6312 author:Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han category:cs.CL cs.IR cs.LG  published:2014-06-24 summary:While most topic modeling algorithms model text corpora with unigrams, human interpretation often relies on inherent grouping of terms into phrases. As such, we consider the problem of discovering topical phrases of mixed lengths. Existing work either performs post processing to the inference results of unigram-based topic models, or utilizes complex n-gram-discovery topic models. These methods generally produce low-quality topical phrases or suffer from poor scalability on even moderately-sized datasets. We propose a different approach that is both computationally efficient and effective. Our solution combines a novel phrase mining framework to segment a document into single and multi-word phrases, and a new topic model that operates on the induced document partition. Our approach discovers high quality topical phrases with negligible extra cost to the bag-of-words topic model in a variety of datasets including research publication titles, abstracts, reviews, and news articles. version:2
arxiv-1411-5878 | Salient Object Detection: A Survey | http://arxiv.org/abs/1411.5878 | id:1411.5878 author:Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li category:cs.CV cs.AI q-bio.NC  published:2014-11-18 summary:Detecting and segmenting salient objects in natural scenes, also known as salient object detection, has attracted a lot of focused research in computer vision and has resulted in many applications. However, while many such models exist, a deep understanding of achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in this field. We situate salient object detection among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 256 publications we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics in salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance and suggest future research directions. version:1
arxiv-1411-5053 | Model of Interaction between Learning and Evolution | http://arxiv.org/abs/1411.5053 | id:1411.5053 author:Vladimir G. Red'ko category:cs.NE  published:2014-11-18 summary:The model of interaction between learning and evolutionary optimization is designed and investigated. The evolving population of modeled organisms is considered. The mechanism of the genetic assimilation of the acquired features during a number of generations of Darwinian evolution is studied. It is shown that the genetic assimilation takes place as follows: phenotypes of modeled organisms move towards the optimum at learning; then the selection takes place; genotypes of selected organisms also move towards the optimum. The hiding effect is also studied; this effect means that strong learning can inhibit the evolutionary search for the optimal genotype. The mechanism of influence of the learning load on the interaction between learning and evolution is analyzed. It is shown that the learning load can lead to a significant acceleration of evolution. version:1
arxiv-1406-3497 | Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation Supplementary Material | http://arxiv.org/abs/1406.3497 | id:1406.3497 author:Matteo Pirotta, Simone Parisi, Marcello Restelli category:cs.AI cs.LG  published:2014-06-13 summary:This document contains supplementary material for the paper "Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation", published at the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15). The paper is about learning a continuous approximation of the Pareto frontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a policy-based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy-gradient multi-objective algorithms, where n optimization routines are use to have n solutions, our approach performs a single gradient-ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient-based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs. version:2
arxiv-1411-5010 | Nonnegative Tensor Factorization for Directional Blind Audio Source Separation | http://arxiv.org/abs/1411.5010 | id:1411.5010 author:Noah D. Stein category:stat.ML cs.LG  published:2014-11-18 summary:We augment the nonnegative matrix factorization method for audio source separation with cues about directionality of sound propagation. This improves separation quality greatly and removes the need for training data, but doubles the computation. version:1
arxiv-1411-4960 | Network Motifs Analysis of Croatian Literature | http://arxiv.org/abs/1411.4960 | id:1411.4960 author:Hana Rizviƒá, Sanda Martinƒçiƒá-Ip≈°iƒá, Ana Me≈°troviƒá category:cs.CL  published:2014-11-18 summary:In this paper we analyse network motifs in the co-occurrence directed networks constructed from five different texts (four books and one portal) in the Croatian language. After preparing the data and network construction, we perform the network motif analysis. We analyse the motif frequencies and Z-scores in the five networks. We present the triad significance profile for five datasets. Furthermore, we compare our results with the existing results for the linguistic networks. Firstly, we show that the triad significance profile for the Croatian language is very similar with the other languages and all the networks belong to the same family of networks. However, there are certain differences between the Croatian language and other analysed languages. We conclude that this is due to the free word-order of the Croatian language. version:1
arxiv-1411-4958 | Designing Deep Networks for Surface Normal Estimation | http://arxiv.org/abs/1411.4958 | id:1411.4958 author:Xiaolong Wang, David F. Fouhey, Abhinav Gupta category:cs.CV  published:2014-11-18 summary:In the past few years, convolutional neural nets (CNN) have shown incredible promise for learning visual representations. In this paper, we use CNNs for the task of predicting surface normals from a single image. But what is the right architecture we should use? We propose to build upon the decades of hard work in 3D scene understanding, to design new CNN architecture for the task of surface normal estimation. We show by incorporating several constraints (man-made, manhattan world) and meaningful intermediate representations (room layout, edge labels) in the architecture leads to state of the art performance on surface normal estimation. We also show that our network is quite robust and show state of the art results on other datasets as well without any fine-tuning. version:1
arxiv-1411-4925 | Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data | http://arxiv.org/abs/1411.4925 | id:1411.4925 author:A. Ramos-Soto, A. Bugar√≠n, S. Barro, J. Taboada category:cs.AI cs.CL  published:2014-11-18 summary:We present in this paper an application which automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data together with a natural language generation (NLG) system. The application, named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the natural language generation system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation GALiWeather will be released as a real service offering custom forecasts for a wide public. version:1
arxiv-1409-6911 | Do More Dropouts in Pool5 Feature Maps for Better Object Detection | http://arxiv.org/abs/1409.6911 | id:1409.6911 author:Zhiqiang Shen, Xiangyang Xue category:cs.CV  published:2014-09-24 summary:Deep Convolutional Neural Networks (CNNs) have gained great success in image classification and object detection. In these fields, the outputs of all layers of CNNs are usually considered as a high dimensional feature vector extracted from an input image and the correspondence between finer level feature vectors and concepts that the input image contains is all-important. However, fewer studies focus on this deserving issue. On considering the correspondence, we propose a novel approach which generates an edited version for each original CNN feature vector by applying the maximum entropy principle to abandon particular vectors. These selected vectors correspond to the unfriendly concepts in each image category. The classifier trained from merged feature sets can significantly improve model generalization of individual categories when training data is limited. The experimental results for classification-based object detection on canonical datasets including VOC 2007 (60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean average precision (mAP) with simple linear support vector machines. version:3
arxiv-1411-5935 | Towards Scene Understanding with Detailed 3D Object Representations | http://arxiv.org/abs/1411.5935 | id:1411.5935 author:M. Zeeshan Zia, Michael Stark, Konrad Schindler category:cs.CV  published:2014-11-18 summary:Current approaches to semantic image and scene understanding typically employ rather simple object representations such as 2D or 3D bounding boxes. While such coarse models are robust and allow for reliable object detection, they discard much of the information about objects' 3D shape and pose, and thus do not lend themselves well to higher-level reasoning. Here, we propose to base scene understanding on a high-resolution object representation. An object class - in our case cars - is modeled as a deformable 3D wireframe, which enables fine-grained modeling at the level of individual vertices and faces. We augment that model to explicitly include vertex-level occlusion, and embed all instances in a common coordinate frame, in order to infer and exploit object-object interactions. Specifically, from a single view we jointly estimate the shapes and poses of multiple objects in a common 3D frame. A ground plane in that frame is estimated by consensus among different objects, which significantly stabilizes monocular 3D pose estimation. The fine-grained model, in conjunction with the explicit 3D scene model, further allows one to infer part-level occlusions between the modeled objects, as well as occlusions by other, unmodeled scene elements. To demonstrate the benefits of such detailed object class models in the context of scene understanding we systematically evaluate our approach on the challenging KITTI street scene dataset. The experiments show that the model's ability to utilize image evidence at the level of individual parts improves monocular 3D pose estimation w.r.t. both location and (continuous) viewpoint. version:1
arxiv-1408-6804 | A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training Structural SVMs with a Costly max-Oracle | http://arxiv.org/abs/1408.6804 | id:1408.6804 author:Neel Shah, Vladimir Kolmogorov, Christoph H. Lampert category:cs.LG  published:2014-08-28 summary:Structural support vector machines (SSVMs) are amongst the best performing models for structured computer vision tasks, such as semantic image segmentation or human pose estimation. Training SSVMs, however, is computationally costly, because it requires repeated calls to a structured prediction subroutine (called \emph{max-oracle}), which has to solve an optimization problem itself, e.g. a graph cut. In this work, we introduce a new algorithm for SSVM training that is more efficient than earlier techniques when the max-oracle is computationally expensive, as it is frequently the case in computer vision tasks. The main idea is to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithm with efficient hyperplane caching, and (ii) use an automatic selection rule for deciding whether to call the exact max-oracle or to rely on an approximate one based on the cached hyperplanes. We show experimentally that this strategy leads to faster convergence to the optimum with respect to the number of requires oracle calls, and that this translates into faster convergence with respect to the total runtime when the max-oracle is slow compared to the other steps of the algorithm. A publicly available C++ implementation is provided at http://pub.ist.ac.at/~vnk/papers/SVM.html . version:2
arxiv-1411-5014 | Music Data Analysis: A State-of-the-art Survey | http://arxiv.org/abs/1411.5014 | id:1411.5014 author:Shubhanshu Gupta category:cs.DB cs.LG cs.SD 97M80 H.5.5; J.5  published:2014-11-18 summary:Music accounts for a significant chunk of interest among various online activities. This is reflected by wide array of alternatives offered in music related web/mobile apps, information portals, featuring millions of artists, songs and events attracting user activity at similar scale. Availability of large scale structured and unstructured data has attracted similar level of attention by data science community. This paper attempts to offer current state-of-the-art in music related analysis. Various approaches involving machine learning, information theory, social network analysis, semantic web and linked open data are represented in the form of taxonomy along with data sources and use cases addressed by the research community. version:1
arxiv-1406-7865 | Simple connectome inference from partial correlation statistics in calcium imaging | http://arxiv.org/abs/1406.7865 | id:1406.7865 author:Antonio Sutera, Arnaud Joly, Vincent Fran√ßois-Lavet, Zixiao Aaron Qiu, Gilles Louppe, Damien Ernst, Pierre Geurts category:stat.ML cs.CE cs.LG  published:2014-06-30 summary:In this work, we propose a simple yet effective solution to the problem of connectome inference in calcium imaging data. The proposed algorithm consists of two steps. First, processing the raw signals to detect neural peak activities. Second, inferring the degree of association between neurons from partial correlation statistics. This paper summarises the methodology that led us to win the Connectomics Challenge, proposes a simplified version of our method, and finally compares our results with respect to other inference methods. version:4
arxiv-1411-4834 | The NLMS algorithm with time-variant optimum stepsize derived from a Bayesian network perspective | http://arxiv.org/abs/1411.4834 | id:1411.4834 author:Christian Huemmer, Roland Maas, Walter Kellermann category:stat.ML  published:2014-11-18 summary:In this article, we derive a new stepsize adaptation for the normalized least mean square algorithm (NLMS) by describing the task of linear acoustic echo cancellation from a Bayesian network perspective. Similar to the well-known Kalman filter equations, we model the acoustic wave propagation from the loudspeaker to the microphone by a latent state vector and define a linear observation equation (to model the relation between the state vector and the observation) as well as a linear process equation (to model the temporal progress of the state vector). Based on additional assumptions on the statistics of the random variables in observation and process equation, we apply the expectation-maximization (EM) algorithm to derive an NLMS-like filter adaptation. By exploiting the conditional independence rules for Bayesian networks, we reveal that the resulting EM-NLMS algorithm has a stepsize update equivalent to the optimal-stepsize calculation proposed by Yamamoto and Kitayama in 1982, which has been adopted in many textbooks. As main difference, the instantaneous stepsize value is estimated in the M step of the EM algorithm (instead of being approximated by artificially extending the acoustic echo path). The EM-NLMS algorithm is experimentally verified for synthesized scenarios with both, white noise and male speech as input signal. version:1
arxiv-1411-4825 | Cognitive Systems and Question Answering | http://arxiv.org/abs/1411.4825 | id:1411.4825 author:Ulrich Furbach, Claudia Schon, Frieder Stolzenburg category:cs.AI cs.CL  published:2014-11-18 summary:This paper briefly characterizes the field of cognitive computing. As an exemplification, the field of natural language question answering is introduced together with its specific challenges. A possibility to master these challenges is illustrated by a detailed presentation of the LogAnswer system, which is a successful representative of the field of natural language question answering. version:1
arxiv-1411-4503 | Outlier-Robust Convex Segmentation | http://arxiv.org/abs/1411.4503 | id:1411.4503 author:Itamar Katz, Koby Crammer category:cs.LG stat.ML  published:2014-11-17 summary:We derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach, and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline segmentation algorithms. version:2
arxiv-1408-6686 | Sparse Generalized Eigenvalue Problem via Smooth Optimization | http://arxiv.org/abs/1408.6686 | id:1408.6686 author:Junxiao Song, Prabhu Babu, Daniel P. Palomar category:stat.ML cs.LG  published:2014-08-28 summary:In this paper, we consider an $\ell_{0}$-norm penalized formulation of the generalized eigenvalue problem (GEP), aimed at extracting the leading sparse generalized eigenvector of a matrix pair. The formulation involves maximization of a discontinuous nonconcave objective function over a nonconvex constraint set, and is therefore computationally intractable. To tackle the problem, we first approximate the $\ell_{0}$-norm by a continuous surrogate function. Then an algorithm is developed via iteratively majorizing the surrogate function by a quadratic separable function, which at each iteration reduces to a regular generalized eigenvalue problem. A preconditioned steepest ascent algorithm for finding the leading generalized eigenvector is provided. A systematic way based on smoothing is proposed to deal with the "singularity issue" that arises when a quadratic function is used to majorize the nondifferentiable surrogate function. For sparse GEPs with special structure, algorithms that admit a closed-form solution at every iteration are derived. Numerical experiments show that the proposed algorithms match or outperform existing algorithms in terms of computational complexity and support recovery. version:2
arxiv-1406-3650 | Smoothed Gradients for Stochastic Variational Inference | http://arxiv.org/abs/1406.3650 | id:1406.3650 author:Stephan Mandt, David Blei category:stat.ML cs.LG  published:2014-06-13 summary:Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora. version:2
arxiv-1411-1805 | Faithful Variable Screening for High-Dimensional Convex Regression | http://arxiv.org/abs/1411.1805 | id:1411.1805 author:Min Xu, Minhua Chen, John Lafferty category:math.ST stat.ML stat.TH  published:2014-11-07 summary:We study the problem of variable selection in convex nonparametric regression. Under the assumption that the true regression function is convex and sparse, we develop a screening procedure to select a subset of variables that contains the relevant variables. Our approach is a two-stage quadratic programming method that estimates a sum of one-dimensional convex functions, followed by one-dimensional concave regression fits on the residuals. In contrast to previous methods for sparse additive models, the optimization is finite dimensional and requires no tuning parameters for smoothness. Under appropriate assumptions, we prove that the procedure is faithful in the population setting, yielding no false negatives. We give a finite sample statistical analysis, and introduce algorithms for efficiently carrying out the required quadratic programs. The approach leads to computational and statistical advantages over fitting a full model, and provides an effective, practical approach to variable screening in convex regression. version:2
arxiv-1411-4702 | Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal Memory in Light of Frontal Cortical Function | http://arxiv.org/abs/1411.4702 | id:1411.4702 author:Michael R. Ferrier category:q-bio.NC cs.AI cs.NE I.2.6  published:2014-11-18 summary:A wide range of evidence points toward the existence of a common algorithm underlying the processing of information throughout the cerebral cortex. Several hypothesized features of this cortical algorithm are reviewed, including sparse distributed representation, Bayesian inference, hierarchical organization composed of alternating template matching and pooling layers, temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is a family of learning algorithms and corresponding theories of cortical function that embodies these principles. HTM has previously been applied mainly to perceptual tasks typical of posterior cortex. In order to evaluate HTM as a candidate model of cortical function, it is necessary also to investigate its compatibility with the requirements of frontal cortical function. To this end, a variety of models of frontal cortical function are reviewed and integrated, to arrive at the hypothesis that frontal functions including attention, working memory and action selection depend largely upon the same basic algorithms as do posterior functions, with the notable additions of a mechanism for the active maintenance of representations and of multiple cortico-striato-thalamo-cortical loops that allow communication between regions of frontal cortex to be gated in an adaptive manner. Computational models of this system are reviewed. Finally, there is a discussion of how HTM can contribute to the understanding of frontal cortical function, and of what the requirements of frontal cortical function mean for the future development of HTM. version:1
arxiv-1411-4701 | Structured Hough Voting for Vision-based Highway Border Detection | http://arxiv.org/abs/1411.4701 | id:1411.4701 author:Zhiding Yu, Wende Zhang, B. V. K. Vijaya Kumar, Dan Levi category:cs.CV cs.RO  published:2014-11-18 summary:We propose a vision-based highway border detection algorithm using structured Hough voting. Our approach takes advantage of the geometric relationship between highway road borders and highway lane markings. It uses a strategy where a number of trained road border and lane marking detectors are triggered, followed by Hough voting to generate corresponding detection of the border and lane marking. Since the initially triggered detectors usually result in large number of positives, conventional frame-wise Hough voting is not able to always generate robust border and lane marking results. Therefore, we formulate this problem as a joint detection-and-tracking problem under the structured Hough voting model, where tracking refers to exploiting inter-frame structural information to stabilize the detection results. Both qualitative and quantitative evaluations show the superiority of the proposed structured Hough voting model over a number of baseline methods. version:1
arxiv-1411-4695 | Feedback Solution to Optimal Switching Problems with Switching Cost | http://arxiv.org/abs/1411.4695 | id:1411.4695 author:Ali Heydari category:cs.SY math.OC stat.ML  published:2014-11-17 summary:The problem of optimal switching between nonlinear autonomous subsystems is investigated in this study where the objective is not only bringing the states to close to the desired point, but also adjusting the switching pattern, in the sense of penalizing switching occurrences and assigning different preferences to utilization of different modes. The mode sequence is unspecified and a switching cost term is used in the cost function for penalizing each switching. It is shown that once a switching cost is incorporated, the optimal cost-to-go function depends on the already active subsystem, i.e., the subsystem which was engaged in the previous time step. Afterwards, an approximate dynamic programming based method is developed which provides an approximation of the optimal solution to the problem in a feedback form and for different initial conditions. Finally, the performance of the method is analyzed through numerical examples. version:1
arxiv-1411-4691 | Group Regularized Estimation under Structural Hierarchy | http://arxiv.org/abs/1411.4691 | id:1411.4691 author:Yiyuan She, He Jiang category:math.ST stat.CO stat.ML stat.TH  published:2014-11-17 summary:In high-dimensional models that involve interactions, statisticians usually favor variable selection obeying certain logical hierarchical constraints. This paper focuses on structural hierarchy which means that the existence of an interaction term implies that at least one or both associated main effects must be present. Lately this problem has attracted a lot of attentions from statisticians, but existing computational algorithms converge slow and cannot meet the challenge of big data computation. More importantly, theoretical studies of hierarchical variable selection are extremely scarce, largely due to the difficulty that multiple sparsity-promoting penalties are enforced on the same subject. This work investigates a new type of estimator based on group multi-regularization to capture various types of structural parsimony simultaneously. We present non-asymptotic results based on combined statistical and computational analysis, and reveal the minimax optimal rate. A general-purpose algorithm is developed with a theoretical guarantee of strict iterate convergence and global optimality. Simulations and real data experiments demonstrate the efficiency and efficacy of the proposed approach. version:1
arxiv-1411-4679 | Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand Using Artificial Neural Network | http://arxiv.org/abs/1411.4679 | id:1411.4679 author:S. Paudel, M. Elmtiri, W. L. Kling, O. Le Corre, B. Lacarriere category:cs.CE cs.NE  published:2014-11-17 summary:This paper presents the building heating demand prediction model with occupancy profile and operational heating power level characteristics in short time horizon (a couple of days) using artificial neural network. In addition, novel pseudo dynamic transitional model is introduced, which consider time dependent attributes of operational power level characteristics and its effect in the overall model performance is outlined. Pseudo dynamic model is applied to a case study of French Institution building and compared its results with static and other pseudo dynamic neural network models. The results show the coefficients of correlation in static and pseudo dynamic neural network model of 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning phase, and 0.61 and 0.85 during the prediction phase respectively. Further, orthogonal array design is applied to the pseudo dynamic model to check the schedule of occupancy profile and operational heating power level characteristics. The results show the new schedule and provide the robust design for pseudo dynamic model. Due to prediction in short time horizon, it finds application for Energy Services Company (ESCOs) to manage the heating load for dynamic control of heat production system. version:1
arxiv-1411-4670 | AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline Arabic Handwriting Recognition | http://arxiv.org/abs/1411.4670 | id:1411.4670 author:Mohamed E. Hussein, Marwan Torki, Ahmed Elsallamy, Mahmoud Fayyaz category:cs.CV I.5.2; I.7.5  published:2014-11-17 summary:In this paper, we introduce the first phase of a new dataset for offline Arabic handwriting recognition. The aim is to collect a very large dataset of isolated Arabic words that covers all letters of the alphabet in all possible shapes using a small number of simple words. The end goal is to collect a very large dataset of segmented letter images, which can be used to build and evaluate Arabic handwriting recognition systems that are based on segmented letter recognition. The current version of the dataset contains $25114$ samples of $109$ unique Arabic words that cover all possible shapes of all alphabet letters. The samples were collected from $907$ writers. In its current form, the dataset can be used for the problem of closed-vocabulary word recognition. We evaluated a number of window-based descriptors and classifiers on this task and obtained an accuracy of $92.16\%$ using a SIFT-based descriptor and ANN. version:1
arxiv-1411-4648 | A unifying framework for relaxations of the causal assumptions in Bell's theorem | http://arxiv.org/abs/1411.4648 | id:1411.4648 author:Rafael Chaves, Richard Kueng, Jonatan Bohr Brask, David Gross category:quant-ph stat.ML  published:2014-11-17 summary:Bell's Theorem shows that quantum mechanical correlations can violate the constraints that the causal structure of certain experiments impose on any classical explanation. It is thus natural to ask to which degree the causal assumptions -- e.g. locality or measurement independence -- have to be relaxed in order to allow for a classical description of such experiments. Here, we develop a conceptual and computational framework for treating this problem. We employ the language of Bayesian networks to systematically construct alternative causal structures and bound the degree of relaxation using quantitative measures that originate from the mathematical theory of causality. The main technical insight is that the resulting problems can often be expressed as computationally tractable linear programs. We demonstrate the versatility of the framework by applying it to a variety of scenarios, ranging from relaxations of the measurement independence, locality and bilocality assumptions, to a novel causal interpretation of CHSH inequality violations. version:1
arxiv-1411-0740 | State-of-the-Art in Retinal Optical Coherence Tomography Image Analysis | http://arxiv.org/abs/1411.0740 | id:1411.0740 author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV  published:2014-11-04 summary:Optical Coherence Tomography (OCT) is one of the most emerging imaging modalities that has been used widely in the field of biomedical imaging. From its emergence in 1990's, plenty of hardware and software improvements have been made. Its applications range from ophthalmology to dermatology to coronary imaging etc. Here, the focus is on applications of OCT in ophthalmology and retinal imaging. OCT is able to non-invasively produce cross-sectional volume images of the tissues which are further used for analysis of the tissue structure and its properties. Due to the underlying physics, OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation, hence requiring specialized noise reduction techniques to remove the noise while preserving image details. Also, given the fact that OCT images are in the $\mu m$ -level, further analysis in needed to distinguish between the different structures in the imaged volume. Therefore the use of different segmentation techniques are of high importance. The movement of the tissue under imaging or the progression of disease in the tissue also imposes further implications both on the quality and the proper interpretation of the acquired images. Thus, use of image registration techniques can be very helpful. In this work, an overview of such image analysis techniques will be given. version:2
arxiv-1411-4618 | Relations World: A Possibilistic Graphical Model | http://arxiv.org/abs/1411.4618 | id:1411.4618 author:Christopher J. C. Burges, Erin Renshaw, Andrzej Pastusiak category:cs.CL cs.AI  published:2014-11-17 summary:We explore the idea of using a "possibilistic graphical model" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user's family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model. version:1
arxiv-1411-4598 | Joint Association Graph Screening and Decomposition for Large-scale Linear Dynamical Systems | http://arxiv.org/abs/1411.4598 | id:1411.4598 author:Yiyuan She, Yuejia He, Shijie Li, Dapeng Wu category:stat.CO stat.ML  published:2014-11-17 summary:This paper studies large-scale dynamical networks where the current state of the system is a linear transformation of the previous state, contaminated by a multivariate Gaussian noise. Examples include stock markets, human brains and gene regulatory networks. We introduce a transition matrix to describe the evolution, which can be translated to a directed Granger transition graph, and use the concentration matrix of the Gaussian noise to capture the second-order relations between nodes, which can be translated to an undirected conditional dependence graph. We propose regularizing the two graphs jointly in topology identification and dynamics estimation. Based on the notion of joint association graph (JAG), we develop a joint graphical screening and estimation (JGSE) framework for efficient network learning in big data. In particular, our method can pre-determine and remove unnecessary edges based on the joint graphical structure, referred to as JAG screening, and can decompose a large network into smaller subnetworks in a robust manner, referred to as JAG decomposition. JAG screening and decomposition can reduce the problem size and search space for fine estimation at a later stage. Experiments on both synthetic data and real-world applications show the effectiveness of the proposed framework in large-scale network topology identification and dynamics estimation. version:1
arxiv-1411-3519 | Window-Based Descriptors for Arabic Handwritten Alphabet Recognition: A Comparative Study on a Novel Dataset | http://arxiv.org/abs/1411.3519 | id:1411.3519 author:Marwan Torki, Mohamed E. Hussein, Ahmed Elsallamy, Mahmoud Fayyaz, Shehab Yaser category:cs.CV I.5.2; I.7.5  published:2014-11-13 summary:This paper presents a comparative study for window-based descriptors on the application of Arabic handwritten alphabet recognition. We show a detailed experimental evaluation of different descriptors with several classifiers. The objective of the paper is to evaluate different window-based descriptors on the problem of Arabic letter recognition. Our experiments clearly show that they perform very well. Moreover, we introduce a novel spatial pyramid partitioning scheme that enhances the recognition accuracy for most descriptors. In addition, we introduce a novel dataset for Arabic handwritten isolated alphabet letters, which can serve as a benchmark for future research. version:2
arxiv-1411-4565 | A Parallel Genetic Algorithm for Three Dimensional Bin Packing with Heterogeneous Bins | http://arxiv.org/abs/1411.4565 | id:1411.4565 author:Drona Pratap Chandu category:cs.DC cs.NE  published:2014-11-17 summary:This paper presents a parallel genetic algorithm for three dimensional bin packing with heterogeneous bins using Hadoop Map-Reduce framework. The most common three dimensional bin packing problem which packs given set of boxes into minimum number of equal sized bins is proven to be NP Hard. The variation of three dimensional bin packing problem that allows heterogeneous bin sizes and rotation of boxes is computationally more harder than common three dimensional bin packing problem. The proposed Map-Reduce implementation helps to run the genetic algorithm for three dimensional bin packing with heterogeneous bins on multiple machines parallely and computes the solution in relatively short time. version:1
arxiv-1410-6714 | Stochastic Blockmodeling for Online Advertising | http://arxiv.org/abs/1410.6714 | id:1410.6714 author:Li Chen, Matthew Patton category:stat.ML stat.AP  published:2014-10-24 summary:Online advertising is an important and huge industry. Having knowledge of the website attributes can contribute greatly to business strategies for ad-targeting, content display, inventory purchase or revenue prediction. Classical inferences on users and sites impose challenge, because the data is voluminous, sparse, high-dimensional and noisy. In this paper, we introduce a stochastic blockmodeling for the website relations induced by the event of online user visitation. We propose two clustering algorithms to discover the instrinsic structures of websites, and compare the performance with a goodness-of-fit method and a deterministic graph partitioning method. We demonstrate the effectiveness of our algorithms on both simulation and AOL website dataset. version:2
arxiv-1411-4521 | Implicitly Constrained Semi-Supervised Linear Discriminant Analysis | http://arxiv.org/abs/1411.4521 | id:1411.4521 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2014-11-17 summary:Semi-supervised learning is an important and active topic of research in pattern recognition. For classification using linear discriminant analysis specifically, several semi-supervised variants have been proposed. Using any one of these methods is not guaranteed to outperform the supervised classifier which does not take the additional unlabeled data into account. In this work we compare traditional Expectation Maximization type approaches for semi-supervised linear discriminant analysis with approaches based on intrinsic constraints and propose a new principled approach for semi-supervised linear discriminant analysis, using so-called implicit constraints. We explore the relationships between these methods and consider the question if and in what sense we can expect improvement in performance over the supervised procedure. The constraint based approaches are more robust to misspecification of the model, and may outperform alternatives that make more assumptions on the data, in terms of the log-likelihood of unseen objects. version:1
arxiv-1411-4510 | Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation | http://arxiv.org/abs/1411.4510 | id:1411.4510 author:Kian Hsiang Low, Jiangbo Yu, Jie Chen, Patrick Jaillet category:stat.ML cs.DC cs.LG  published:2014-11-17 summary:The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances. version:1
arxiv-1403-3155 | Spectral Unmixing via Data-guided Sparsity | http://arxiv.org/abs/1403.3155 | id:1403.3155 author:Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Shiming Xiang, Chunhong Pan category:cs.CV  published:2014-03-13 summary:Hyperspectral unmixing, the process of estimating a common set of spectral bases and their corresponding composite percentages at each pixel, is an important task for hyperspectral analysis, visualization and understanding. From an unsupervised learning perspective, this problem is very challenging---both the spectral bases and their composite percentages are unknown, making the solution space too large. To reduce the solution space, many approaches have been proposed by exploiting various priors. In practice, these priors would easily lead to some unsuitable solution. This is because they are achieved by applying an identical strength of constraints to all the factors, which does not hold in practice. To overcome this limitation, we propose a novel sparsity based method by learning a data-guided map to describe the individual mixed level of each pixel. Through this data-guided map, the $\ell_{p}(0<p<1)$ constraint is applied in an adaptive manner. Such implementation not only meets the practical situation, but also guides the spectral bases toward the pixels under highly sparse constraint. What's more, an elegant optimization scheme as well as its convergence proof have been provided in this paper. Extensive experiments on several datasets also demonstrate that the data-guided map is feasible, and high quality unmixing results could be obtained by our method. version:4
arxiv-1409-3660 | 10,000+ Times Accelerated Robust Subset Selection (ARSS) | http://arxiv.org/abs/1409.3660 | id:1409.3660 author:Feiyun Zhu, Bin Fan, Xinliang Zhu, Ying Wang, Shiming Xiang, Chunhong Pan category:cs.LG cs.CV stat.ML  published:2014-09-12 summary:Subset selection from massive data with noised information is increasingly popular for various applications. This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers. To address the above two issues, we propose an accelerated robust subset selection (ARSS) method. Specifically in the subset selection area, this is the first attempt to employ the $\ell_{p}(0<p\leq1)$-norm based measure for the representation loss, preventing large errors from dominating our objective. As a result, the robustness against outlier elements is greatly enhanced. Actually, data size is generally much larger than feature length, i.e. $N\gg L$. Based on this observation, we propose a speedup solver (via ALM and equivalent derivations) to highly reduce the computational cost, theoretically from $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods, but also runs 10,000+ times faster than the most related method. version:4
arxiv-1411-4472 | Opinion mining of text documents written in Macedonian language | http://arxiv.org/abs/1411.4472 | id:1411.4472 author:Andrej Gajduk, Ljupco Kocarev category:cs.CL  published:2014-11-17 summary:The ability to extract public opinion from web portals such as review sites, social networks and blogs will enable companies and individuals to form a view, an attitude and make decisions without having to do lengthy and costly researches and surveys. In this paper machine learning techniques are used for determining the polarity of forum posts on kajgana which are written in Macedonian language. The posts are classified as being positive, negative or neutral. We test different feature metrics and classifiers and provide detailed evaluation of their participation in improving the overall performance on a manually generated dataset. By achieving 92% accuracy, we show that the performance of systems for automated opinion mining is comparable to a human evaluator, thus making it a viable option for text data analysis. Finally, we present a few statistics derived from the forum posts using the developed system. version:1
arxiv-1411-4464 | Fully Convolutional Neural Networks for Crowd Segmentation | http://arxiv.org/abs/1411.4464 | id:1411.4464 author:Kai Kang, Xiaogang Wang category:cs.CV  published:2014-11-17 summary:In this paper, we propose a fast fully convolutional neural network (FCNN) for crowd segmentation. By replacing the fully connected layers in CNN with 1 by 1 convolution kernels, FCNN takes whole images as inputs and directly outputs segmentation maps by one pass of forward propagation. It has the property of translation invariance like patch-by-patch scanning but with much lower computation cost. Once FCNN is learned, it can process input images of any sizes without warping them to a standard size. These attractive properties make it extendable to other general image segmentation problems. Based on FCNN, a multi-stage deep learning is proposed to integrate appearance and motion cues for crowd segmentation. Both appearance filters and motion filers are pretrained stage-by-stage and then jointly optimized. Different combination methods are investigated. The effectiveness of our approach and component-wise analysis are evaluated on two crowd segmentation datasets created by us, which include image frames from 235 and 11 scenes, respectively. They are currently the largest crowd segmentation datasets and will be released to the public. version:1
arxiv-1411-4455 | Errata: Distant Supervision for Relation Extraction with Matrix Completion | http://arxiv.org/abs/1411.4455 | id:1411.4455 author:Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng, Edward Y. Chang category:cs.CL cs.LG  published:2014-11-17 summary:The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods. version:1
arxiv-1407-4764 | Efficient On-the-fly Category Retrieval using ConvNets and GPUs | http://arxiv.org/abs/1407.4764 | id:1407.4764 author:Ken Chatfield, Karen Simonyan, Andrew Zisserman category:cs.CV cs.LG cs.NE  published:2014-07-17 summary:We investigate the gains in precision and speed, that can be obtained by using Convolutional Networks (ConvNets) for on-the-fly retrieval - where classifiers are learnt at run time for a textual query from downloaded images, and used to rank large image or video datasets. We make three contributions: (i) we present an evaluation of state-of-the-art image representations for object category retrieval over standard benchmark datasets containing 1M+ images; (ii) we show that ConvNets can be used to obtain features which are incredibly performant, and yet much lower dimensional than previous state-of-the-art image representations, and that their dimensionality can be reduced further without loss in performance by compression using product quantization or binarization. Consequently, features with the state-of-the-art performance on large-scale datasets of millions of images can fit in the memory of even a commodity GPU card; (iii) we show that an SVM classifier can be learnt within a ConvNet framework on a GPU in parallel with downloading the new training images, allowing for a continuous refinement of the model as more images become available, and simultaneous training and ranking. The outcome is an on-the-fly system that significantly outperforms its predecessors in terms of: precision of retrieval, memory requirements, and speed, facilitating accurate on-the-fly learning and ranking in under a second on a single GPU. version:3
arxiv-1411-4379 | FGPGA: An Efficient Genetic Approach for Producing Feasible Graph Partitions | http://arxiv.org/abs/1411.4379 | id:1411.4379 author:Md. Lisul Islam, Novia Nurain, Swakkhar Shatabda, M Sohel Rahman category:cs.NE cs.AI cs.DC  published:2014-11-17 summary:Graph partitioning, a well studied problem of parallel computing has many applications in diversified fields such as distributed computing, social network analysis, data mining and many other domains. In this paper, we introduce FGPGA, an efficient genetic approach for producing feasible graph partitions. Our method takes into account the heterogeneity and capacity constraints of the partitions to ensure balanced partitioning. Such approach has various applications in mobile cloud computing that include feasible deployment of software applications on the more resourceful infrastructure in the cloud instead of mobile hand set. Our proposed approach is light weight and hence suitable for use in cloud architecture. We ensure feasibility of the partitions generated by not allowing over-sized partitions to be generated during the initialization and search. Our proposed method tested on standard benchmark datasets significantly outperforms the state-of-the-art methods in terms of quality of partitions and feasibility of the solutions. version:1
arxiv-1411-4378 | Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space | http://arxiv.org/abs/1411.4378 | id:1411.4378 author:Robert A. Vandermeulen, Clayton D. Scott category:stat.ML  published:2014-11-17 summary:While robust parameter estimation has been well studied in parametric density estimation, there has been little investigation into robust density estimation in the nonparametric setting. We present a robust version of the popular kernel density estimator (KDE). As with other estimators, a robust version of the KDE is useful since sample contamination is a common issue with datasets. What "robustness" means for a nonparametric density estimate is not straightforward and is a topic we explore in this paper. To construct a robust KDE we scale the traditional KDE and project it to its nearest weighted KDE in the $L^2$ norm. This yields a scaled and projected KDE (SPKDE). Because the squared $L^2$ norm penalizes point-wise errors superlinearly this causes the weighted KDE to allocate more weight to high density regions. We demonstrate the robustness of the SPKDE with numerical experiments and a consistency result which shows that asymptotically the SPKDE recovers the uncontaminated density under sufficient conditions on the contamination. version:1
arxiv-1411-0296 | Geodesic Exponential Kernels: When Curvature and Linearity Conflict | http://arxiv.org/abs/1411.0296 | id:1411.0296 author:Aasa Feragen, Francois Lauze, S√∏ren Hauberg category:cs.LG cs.CV  published:2014-11-02 summary:We consider kernel methods on general geodesic metric spaces and provide both negative and positive results. First we show that the common Gaussian kernel can only be generalized to a positive definite kernel on a geodesic metric space if the space is flat. As a result, for data on a Riemannian manifold, the geodesic Gaussian kernel is only positive definite if the Riemannian manifold is Euclidean. This implies that any attempt to design geodesic Gaussian kernels on curved Riemannian manifolds is futile. However, we show that for spaces with conditionally negative definite distances the geodesic Laplacian kernel can be generalized while retaining positive definiteness. This implies that geodesic Laplacian kernels can be generalized to some curved spaces, including spheres and hyperbolic spaces. Our theoretical results are verified empirically. version:2
arxiv-1407-1120 | From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices | http://arxiv.org/abs/1407.1120 | id:1407.1120 author:Mehrtash T. Harandi, Mathieu Salzmann, Richard Hartley category:cs.CV  published:2014-07-04 summary:Representing images and videos with Symmetric Positive Definite (SPD) matrices and considering the Riemannian geometry of the resulting space has proven beneficial for many recognition tasks. Unfortunately, computation on the Riemannian manifold of SPD matrices --especially of high-dimensional ones-- comes at a high cost that limits the applicability of existing techniques. In this paper we introduce an approach that lets us handle high-dimensional SPD matrices by constructing a lower-dimensional, more discriminative SPD manifold. To this end, we model the mapping from the high-dimensional SPD manifold to the low-dimensional one with an orthonormal projection. In particular, we search for a projection that yields a low-dimensional manifold with maximum discriminative power encoded via an affinity-weighted similarity measure based on metrics on the manifold. Learning can then be expressed as an optimization problem on a Grassmann manifold. Our evaluation on several classification tasks shows that our approach leads to a significant accuracy gain over state-of-the-art methods. version:2
arxiv-1411-4331 | A Latent Clothing Attribute Approach for Human Pose Estimation | http://arxiv.org/abs/1411.4331 | id:1411.4331 author:Weipeng Zhang, Jie Shen, Guangcan Liu, Yong Yu category:cs.CV  published:2014-11-16 summary:As a fundamental technique that concerns several vision tasks such as image parsing, action recognition and clothing retrieval, human pose estimation (HPE) has been extensively investigated in recent years. To achieve accurate and reliable estimation of the human pose, it is well-recognized that the clothing attributes are useful and should be utilized properly. Most previous approaches, however, require to manually annotate the clothing attributes and are therefore very costly. In this paper, we shall propose and explore a \emph{latent} clothing attribute approach for HPE. Unlike previous approaches, our approach models the clothing attributes as latent variables and thus requires no explicit labeling for the clothing attributes. The inference of the latent variables are accomplished by utilizing the framework of latent structured support vector machines (LSSVM). We employ the strategy of \emph{alternating direction} to train the LSSVM model: In each iteration, one kind of variables (e.g., human pose or clothing attribute) are fixed and the others are optimized. Our extensive experiments on two real-world benchmarks show the state-of-the-art performance of our proposed approach. version:1
arxiv-1408-1182 | Empirical non-parametric estimation of the Fisher Information | http://arxiv.org/abs/1408.1182 | id:1408.1182 author:Visar Berisha, Alfred O. Hero category:stat.CO cs.IT math.IT stat.ML  published:2014-08-06 summary:The Fisher information matrix (FIM) is a foundational concept in statistical signal processing. The FIM depends on the probability distribution, assumed to belong to a smooth parametric family. Traditional approaches to estimating the FIM require estimating the probability distribution function (PDF), or its parameters, along with its gradient or Hessian. However, in many practical situations the PDF of the data is not known but the statistician has access to an observation sample for any parameter value. Here we propose a method of estimating the FIM directly from sampled data that does not require knowledge of the underlying PDF. The method is based on non-parametric estimation of an $f$-divergence over a local neighborhood of the parameter space and a relation between curvature of the $f$-divergence and the FIM. Thus we obtain an empirical estimator of the FIM that does not require density estimation and is asymptotically consistent. We empirically evaluate the validity of our approach using two experiments. version:2
arxiv-1408-6746 | Non-Standard Words as Features for Text Categorization | http://arxiv.org/abs/1408.6746 | id:1408.6746 author:Slobodan Beliga, Sanda Martinƒçiƒá-Ip≈°iƒá category:cs.CL cs.LG  published:2014-08-28 summary:This paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. Non-Standard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments. version:2
arxiv-1411-4304 | Ten Years of Pedestrian Detection, What Have We Learned? | http://arxiv.org/abs/1411.4304 | id:1411.4304 author:Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele category:cs.CV  published:2014-11-16 summary:Paper-by-paper results make it easy to miss the forest for the trees.We analyse the remarkable progress of the last decade by discussing the main ideas explored in the 40+ detectors currently present in the Caltech pedestrian detection benchmark. We observe that there exist three families of approaches, all currently reaching similar detection quality. Based on our analysis, we study the complementarity of the most promising ideas by combining multiple published strategies. This new decision forest detector achieves the current best known performance on the challenging Caltech-USA dataset. version:1
arxiv-1411-4296 | Combining contextual and local edges for line segment extraction in cluttered images | http://arxiv.org/abs/1411.4296 | id:1411.4296 author:Rui F. C. Guerreiro category:cs.CV  published:2014-11-16 summary:Automatic extraction methods typically assume that line segments are pronounced, thin, few and far between, do not cross each other, and are noise and clutter-free. Since these assumptions often fail in realistic scenarios, many line segments are not detected or are fragmented. In more severe cases, i.e., many who use the Hough Transform, extraction can fail entirely. In this paper, we propose a method that tackles these issues. Its key aspect is the combination of thresholded image derivatives obtained with filters of large and small footprints, which we denote as contextual and local edges, respectively. Contextual edges are robust to noise and we use them to select valid local edges, i.e., local edges that are of the same type as contextual ones: dark-to-bright transition of vice-versa. If the distance between valid local edges does not exceed a maximum distance threshold, we enforce connectivity by marking them and the pixels in between as edge points. This originates connected edge maps that are robust and well localized. We use a powerful two-sample statistical test to compute contextual edges, which we introduce briefly, as they are unfamiliar to the image processing community. Finally, we present experiments that illustrate, with synthetic and real images, how our method is efficient in extracting complete segments of all lengths and widths in several situations where current methods fail. version:1
arxiv-1411-4286 | HIPAD - A Hybrid Interior-Point Alternating Direction algorithm for knowledge-based SVM and feature selection | http://arxiv.org/abs/1411.4286 | id:1411.4286 author:Zhiwei Qin, Xiaocheng Tang, Ioannis Akrotirianakis, Amit Chakraborty category:stat.ML cs.LG  published:2014-11-16 summary:We consider classification tasks in the regime of scarce labeled training data in high dimensional feature space, where specific expert knowledge is also available. We propose a new hybrid optimization algorithm that solves the elastic-net support vector machine (SVM) through an alternating direction method of multipliers in the first phase, followed by an interior-point method for the classical SVM in the second phase. Both SVM formulations are adapted to knowledge incorporation. Our proposed algorithm addresses the challenges of automatic feature selection, high optimization accuracy, and algorithmic flexibility for taking advantage of prior knowledge. We demonstrate the effectiveness and efficiency of our algorithm and compare it with existing methods on a collection of synthetic and real-world data. version:1
arxiv-1411-4246 | GreMuTRRR: A Novel Genetic Algorithm to Solve Distance Geometry Problem for Protein Structures | http://arxiv.org/abs/1411.4246 | id:1411.4246 author:Md. Lisul Islam, Swakkhar Shatabda, M. Sohel Rahman category:cs.NE cs.CE  published:2014-11-16 summary:Nuclear Magnetic Resonance (NMR) Spectroscopy is a widely used technique to predict the native structure of proteins. However, NMR machines are only able to report approximate and partial distances between pair of atoms. To build the protein structure one has to solve the Euclidean distance geometry problem given the incomplete interval distance data produced by NMR machines. In this paper, we propose a new genetic algorithm for solving the Euclidean distance geometry problem for protein structure prediction given sparse NMR data. Our genetic algorithm uses a greedy mutation operator to intensify the search, a twin removal technique for diversification in the population and a random restart method to recover stagnation. On a standard set of benchmark dataset, our algorithm significantly outperforms standard genetic algorithms. version:1
arxiv-1411-4229 | Efficient and Accurate Approximations of Nonlinear Convolutional Networks | http://arxiv.org/abs/1411.4229 | id:1411.4229 author:Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun category:cs.CV  published:2014-11-16 summary:This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4x is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the "AlexNet", but is 4.7% more accurate. version:1
arxiv-1310-3561 | ECA: High Dimensional Elliptical Component Analysis in non-Gaussian Distributions | http://arxiv.org/abs/1310.3561 | id:1310.3561 author:Fang Han, Han Liu category:stat.ML  published:2013-10-14 summary:We propose a robust alternative to principal component analysis (PCA) -- named elliptical component analysis (ECA) -- for analyzing high dimensional elliptically distributed data. ECA aims at estimating the eigenspace of the covariance matrix of the elliptical data. To cope with the heavy-tailed elliptical distributions, a multivariate rank statistic is exploited. At the model-level, we consider two settings that the leading eigenvectors of the covariance matrix are non-sparse or sparse. Methodologically, we propose ECA procedures corresponding to both non-sparse and sparse settings. Theoretically, we provide both non-asymptotic and asymptotic analysis in quantifying the theoretical performances of ECA. Under the non-sparse setting, we show that ECA's performance is highly related to the effective rank of the covariance matrix. Under the sparse setting, the results are in two folds: (i) We show that the sparse ECA estimator based on a combinatoric program attains the optimal rate of convergence; (ii) Built upon some recent developments in estimating sparse leading eigenvectors, we show that a computationally efficient sparse ECA estimator attains the optimal rate of convergence under a suboptimal scaling. version:3
arxiv-1310-0188 | Graph connection Laplacian and random matrices with random blocks | http://arxiv.org/abs/1310.0188 | id:1310.0188 author:Noureddine El Karoui, Hau-tieng Wu category:math.PR math.SP stat.ME stat.ML  published:2013-10-01 summary:Graph connection Laplacian (GCL) is a modern data analysis technique that is starting to be applied for the analysis of high dimensional and massive datasets. Motivated by this technique, we study matrices that are akin to the ones appearing in the null case of GCL, i.e the case where there is no structure in the dataset under investigation. Developing this understanding is important in making sense of the output of the algorithms based on GCL. We hence develop a theory explaining the behavior of the spectral distribution of a large class of random matrices, in particular random matrices with random block entries of fixed size. Part of the theory covers the case where there is significant dependence between the blocks. Numerical work shows that the agreement between our theoretical predictions and numerical simulations is generally very good. version:2
arxiv-1411-4199 | Revisiting Kernelized Locality-Sensitive Hashing for Improved Large-Scale Image Retrieval | http://arxiv.org/abs/1411.4199 | id:1411.4199 author:Ke Jiang, Qichao Que, Brian Kulis category:cs.CV cs.LG stat.ML  published:2014-11-16 summary:We present a simple but powerful reinterpretation of kernelized locality-sensitive hashing (KLSH), a general and popular method developed in the vision community for performing approximate nearest-neighbor searches in an arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based on viewing the steps of the KLSH algorithm in an appropriately projected space, and has several key theoretical and practical benefits. First, it eliminates the problematic conceptual difficulties that are present in the existing motivation of KLSH. Second, it yields the first formal retrieval performance bounds for KLSH. Third, our analysis reveals two techniques for boosting the empirical performance of KLSH. We evaluate these extensions on several large-scale benchmark image retrieval data sets, and show that our analysis leads to improved recall performance of at least 12%, and sometimes much higher, over the standard KLSH method. version:1
arxiv-1411-4194 | ROSS User's Guide and Reference Manual (Version 1.0) | http://arxiv.org/abs/1411.4194 | id:1411.4194 author:Glenn R. Hofford category:cs.AI cs.CL  published:2014-11-15 summary:The ROSS method is a new approach in the area of knowledge representation that is useful for many artificial intelligence and natural language understanding representation and reasoning tasks. (ROSS stands for "Representation", "Ontology", "Structure", "Star" language). ROSS is a physical symbol-based representational scheme. ROSS provides a complex model for the declarative representation of physical structure and for the representation of processes and causality. From the metaphysical perspective, the ROSS view of external reality involves a 4D model, wherein discrete single-time-point unit-sized locations with states are the basis for all objects, processes and aspects that can be modeled. ROSS includes a language called "Star" for the specification of ontology classes. The ROSS method also includes a formal scheme called the "instance model". Instance models are used in the area of natural language meaning representation to represent situations. This document is an in-depth specification of the ROSS method. version:1
arxiv-1408-1245 | Racing to Learn: Statistical Inference and Learning in a Single Spiking Neuron with Adaptive Kernels | http://arxiv.org/abs/1408.1245 | id:1408.1245 author:Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Tara Julia Hamilton category:cs.NE q-bio.NC  published:2014-08-06 summary:This paper describes the Synapto-dendritic Kernel Adapting Neuron (SKAN), a simple spiking neuron model that performs statistical inference and unsupervised learning of spatiotemporal spike patterns. SKAN is the first proposed neuron model to investigate the effects of dynamic synapto-dendritic kernels and demonstrate their computational power even at the single neuron scale. The rule-set defining the neuron is simple there are no complex mathematical operations such as normalization, exponentiation or even multiplication. The functionalities of SKAN emerge from the real-time interaction of simple additive and binary processes. Like a biological neuron, SKAN is robust to signal and parameter noise, and can utilize both in its operations. At the network scale neurons are locked in a race with each other with the fastest neuron to spike effectively hiding its learnt pattern from its neighbors. The robustness to noise, high speed and simple building blocks not only make SKAN an interesting neuron model in computational neuroscience, but also make it ideal for implementation in digital and analog neuromorphic systems which is demonstrated through an implementation in a Field Programmable Gate Array (FPGA). version:4
arxiv-1411-4148 | Diversity Handling In Evolutionary Landscape | http://arxiv.org/abs/1411.4148 | id:1411.4148 author:Maumita Bhattacharya category:cs.NE 68T05  published:2014-11-15 summary:The search ability of an Evolutionary Algorithm (EA) depends on the variation among the individuals in the population. Maintaining an optimal level of diversity in the EA population is imperative to ensure that progress of the EA search is unhindered by premature convergence to suboptimal solutions. Clearer understanding of the concept of population diversity, in the context of evolutionary search and premature convergence in particular, is the key to designing efficient EAs. To this end, this paper first presents a comprehensive analysis of the EA population diversity issues. Next we present an investigation on a counter-niching EA technique that introduces and maintains constructive diversity in the population. The proposed approach uses informed genetic operations to reach promising, but un-explored or under-explored areas of the search space, while discouraging premature local convergence. Simulation runs on a number of standard benchmark test functions with Genetic Algorithm (GA) implementation shows promising results. version:1
arxiv-1411-4116 | Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning | http://arxiv.org/abs/1411.4116 | id:1411.4116 author:Jianpeng Cheng, Dimitri Kartsaklis, Edward Grefenstette category:cs.CL cs.LG cs.NE  published:2014-11-15 summary:This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models. version:1
arxiv-1411-4114 | Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading | http://arxiv.org/abs/1411.4114 | id:1411.4114 author:Ha Jong Won, Li Gwang Chol, Kim Hyok Chol, Li Kum Song category:cs.CL cs.CV cs.LG  published:2014-11-15 summary:In this paper, we defined the viseme (visual speech element) and described about the method of extracting visual feature vector. We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features. Lastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency. version:1
arxiv-1411-4109 | Resolution of Difficult Pronouns Using the ROSS Method | http://arxiv.org/abs/1411.4109 | id:1411.4109 author:Glenn R. Hofford category:cs.CL cs.AI  published:2014-11-15 summary:A new natural language understanding method for disambiguation of difficult pronouns is described. Difficult pronouns are those pronouns for which a level of world or domain knowledge is needed in order to perform anaphoral or other types of resolution. Resolution of difficult pronouns may in some cases require a prior step involving the application of inference to a situation that is represented by the natural language text. A general method is described: it performs entity resolution and pronoun resolution. An extension to the general pronoun resolution method performs inference as an embedded commonsense reasoning method. The general method and the embedded method utilize features of the ROSS representational scheme; in particular the methods use ROSS ontology classes and the ROSS situation model. The overall method is a working solution that solves the following Winograd schemas: a) trophy and suitcase, b) person lifts person, c) person pays detective, and d) councilmen and demonstrators. version:1
arxiv-1411-4102 | Anisotropic Agglomerative Adaptive Mean-Shift | http://arxiv.org/abs/1411.4102 | id:1411.4102 author:Rahul Sawhney, Henrik I. Christensen, Gary R. Bradski category:cs.CV cs.LG  published:2014-11-15 summary:Mean Shift today, is widely used for mode detection and clustering. The technique though, is challenged in practice due to assumptions of isotropicity and homoscedasticity. We present an adaptive Mean Shift methodology that allows for full anisotropic clustering, through unsupervised local bandwidth selection. The bandwidth matrices evolve naturally, adapting locally through agglomeration, and in turn guiding further agglomeration. The online methodology is practical and effecive for low-dimensional feature spaces, preserving better detail and clustering salience. Additionally, conventional Mean Shift either critically depends on a per instance choice of bandwidth, or relies on offline methods which are inflexible and/or again data instance specific. The presented approach, due to its adaptive design, also alleviates this issue - with a default form performing generally well. The methodology though, allows for effective tuning of results. version:1
arxiv-1411-4101 | Deep Deconvolutional Networks for Scene Parsing | http://arxiv.org/abs/1411.4101 | id:1411.4101 author:Rahul Mohan category:stat.ML cs.CV cs.LG  published:2014-11-15 summary:Scene parsing is an important and challenging prob- lem in computer vision. It requires labeling each pixel in an image with the category it belongs to. Tradition- ally, it has been approached with hand-engineered features from color information in images. Recently convolutional neural networks (CNNs), which automatically learn hierar- chies of features, have achieved record performance on the task. These approaches typically include a post-processing technique, such as superpixels, to produce the final label- ing. In this paper, we propose a novel network architecture that combines deep deconvolutional neural networks with CNNs. Our experiments show that deconvolutional neu- ral networks are capable of learning higher order image structure beyond edge primitives in comparison to CNNs. The new network architecture is employed for multi-patch training, introduced as part of this work. Multi-patch train- ing makes it possible to effectively learn spatial priors from scenes. The proposed approach yields state-of-the-art per- formance on four scene parsing datasets, namely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our system has the added advantage of having a training system that can be completely automated end-to-end with- out requiring any post-processing. version:1
arxiv-1404-5122 | Spatiotemporal Sparse Bayesian Learning with Applications to Compressed Sensing of Multichannel Physiological Signals | http://arxiv.org/abs/1404.5122 | id:1404.5122 author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao category:cs.IT cs.LG math.IT stat.ML  published:2014-04-21 summary:Energy consumption is an important issue in continuous wireless telemonitoring of physiological signals. Compressed sensing (CS) is a promising framework to address it, due to its energy-efficient data compression procedure. However, most CS algorithms have difficulty in data recovery due to non-sparsity characteristic of many physiological signals. Block sparse Bayesian learning (BSBL) is an effective approach to recover such signals with satisfactory recovery quality. However, it is time-consuming in recovering multichannel signals, since its computational load almost linearly increases with the number of channels. This work proposes a spatiotemporal sparse Bayesian learning algorithm to recover multichannel signals simultaneously. It not only exploits temporal correlation within each channel signal, but also exploits inter-channel correlation among different channel signals. Furthermore, its computational load is not significantly affected by the number of channels. The proposed algorithm was applied to brain computer interface (BCI) and EEG-based driver's drowsiness estimation. Results showed that the algorithm had both better recovery performance and much higher speed than BSBL. Particularly, the proposed algorithm ensured that the BCI classification and the drowsiness estimation had little degradation even when data were compressed by 80%, making it very suitable for continuous wireless telemonitoring of multichannel signals. version:2
arxiv-1411-4098 | GASP : Geometric Association with Surface Patches | http://arxiv.org/abs/1411.4098 | id:1411.4098 author:Rahul Sawhney, Fuxin Li, Henrik I. Christensen category:cs.CV cs.GR cs.RO  published:2014-11-15 summary:A fundamental challenge to sensory processing tasks in perception and robotics is the problem of obtaining data associations across views. We present a robust solution for ascertaining potentially dense surface patch (superpixel) associations, requiring just range information. Our approach involves decomposition of a view into regularized surface patches. We represent them as sequences expressing geometry invariantly over their superpixel neighborhoods, as uniquely consistent partial orderings. We match these representations through an optimal sequence comparison metric based on the Damerau-Levenshtein distance - enabling robust association with quadratic complexity (in contrast to hitherto employed joint matching formulations which are NP-complete). The approach is able to perform under wide baselines, heavy rotations, partial overlaps, significant occlusions and sensor noise. The technique does not require any priors -- motion or otherwise, and does not make restrictive assumptions on scene structure and sensor movement. It does not require appearance -- is hence more widely applicable than appearance reliant methods, and invulnerable to related ambiguities such as textureless or aliased content. We present promising qualitative and quantitative results under diverse settings, along with comparatives with popular approaches based on range as well as RGB-D data. version:1
arxiv-1411-4086 | Error Rate Bounds and Iterative Weighted Majority Voting for Crowdsourcing | http://arxiv.org/abs/1411.4086 | id:1411.4086 author:Hongwei Li, Bin Yu category:stat.ML cs.HC cs.LG math.PR math.ST stat.TH  published:2014-11-15 summary:Crowdsourcing has become an effective and popular tool for human-powered computation to label large datasets. Since the workers can be unreliable, it is common in crowdsourcing to assign multiple workers to one task, and to aggregate the labels in order to obtain results of high quality. In this paper, we provide finite-sample exponential bounds on the error rate (in probability and in expectation) of general aggregation rules under the Dawid-Skene crowdsourcing model. The bounds are derived for multi-class labeling, and can be used to analyze many aggregation methods, including majority voting, weighted majority voting and the oracle Maximum A Posteriori (MAP) rule. We show that the oracle MAP rule approximately optimizes our upper bound on the mean error rate of weighted majority voting in certain setting. We propose an iterative weighted majority voting (IWMV) method that optimizes the error rate bound and approximates the oracle MAP rule. Its one step version has a provable theoretical guarantee on the error rate. The IWMV method is intuitive and computationally simple. Experimental results on simulated and real data show that IWMV performs at least on par with the state-of-the-art methods, and it has a much lower computational cost (around one hundred times faster) than the state-of-the-art methods. version:1
arxiv-1411-4080 | 6 Seconds of Sound and Vision: Creativity in Micro-Videos | http://arxiv.org/abs/1411.4080 | id:1411.4080 author:Miriam Redi, Neil O Hare, Rossano Schifanella, Michele Trevisiol, Alejandro Jaimes category:cs.MM cs.CV cs.HC  published:2014-11-14 summary:The notion of creativity, as opposed to related concepts such as beauty or interestingness, has not been studied from the perspective of automatic analysis of multimedia content. Meanwhile, short online videos shared on social media platforms, or micro-videos, have arisen as a new medium for creative expression. In this paper we study creative micro-videos in an effort to understand the features that make a video creative, and to address the problem of automatic detection of creative content. Defining creative videos as those that are novel and have aesthetic value, we conduct a crowdsourcing experiment to create a dataset of over 3,800 micro-videos labelled as creative and non-creative. We propose a set of computational features that we map to the components of our definition of creativity, and conduct an analysis to determine which of these features correlate most with creative video. Finally, we evaluate a supervised approach to automatically detect creative video, with promising results, showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy. version:1
arxiv-1411-4077 | A framework for studying synaptic plasticity with neural spike train data | http://arxiv.org/abs/1411.4077 | id:1411.4077 author:Scott W. Linderman, Christopher H. Stock, Ryan P. Adams category:stat.ML q-bio.NC  published:2014-11-14 summary:Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules. version:1
arxiv-1411-4076 | Association Rule Based Flexible Machine Learning Module for Embedded System Platforms like Android | http://arxiv.org/abs/1411.4076 | id:1411.4076 author:Amiraj Dhawan, Shruti Bhave, Amrita Aurora, Vishwanathan Iyer category:cs.CY cs.HC cs.LG  published:2014-11-14 summary:The past few years have seen a tremendous growth in the popularity of smartphones. As newer features continue to be added to smartphones to increase their utility, their significance will only increase in future. Combining machine learning with mobile computing can enable smartphones to become 'intelligent' devices, a feature which is hitherto unseen in them. Also, the combination of machine learning and context aware computing can enable smartphones to gauge user's requirements proactively, depending upon their environment and context. Accordingly, necessary services can be provided to users. In this paper, we have explored the methods and applications of integrating machine learning and context aware computing on the Android platform, to provide higher utility to the users. To achieve this, we define a Machine Learning (ML) module which is incorporated in the basic Android architecture. Firstly, we have outlined two major functionalities that the ML module should provide. Then, we have presented three architectures, each of which incorporates the ML module at a different level in the Android architecture. The advantages and shortcomings of each of these architectures have been evaluated. Lastly, we have explained a few applications in which our proposed system can be incorporated such that their functionality is improved. version:1
arxiv-1411-4072 | Learning Multi-Relational Semantics Using Neural-Embedding Models | http://arxiv.org/abs/1411.4072 | id:1411.4072 author:Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng category:cs.CL cs.LG stat.ML  published:2014-11-14 summary:In this paper we present a unified framework for modeling multi-relational representations, scoring, and learning, and conduct an empirical study of several recent multi-relational embedding models under the framework. We investigate the different choices of relation operators based on linear and bilinear transformations, and also the effects of entity representations by incorporating unsupervised vectors pre-trained on extra textual resources. Our results show several interesting findings, enabling the design of a simple embedding model that achieves the new state-of-the-art performance on a popular knowledge base completion task evaluated on Freebase. version:1
arxiv-1411-4070 | A unified view of generative models for networks: models, methods, opportunities, and challenges | http://arxiv.org/abs/1411.4070 | id:1411.4070 author:Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.LG cs.SI physics.soc-ph  published:2014-11-14 summary:Research on probabilistic models of networks now spans a wide variety of fields, including physics, sociology, biology, statistics, and machine learning. These efforts have produced a diverse ecology of models and methods. Despite this diversity, many of these models share a common underlying structure: pairwise interactions (edges) are generated with probability conditional on latent vertex attributes. Differences between models generally stem from different philosophical choices about how to learn from data or different empirically-motivated goals. The highly interdisciplinary nature of work on these generative models, however, has inhibited the development of a unified view of their similarities and differences. For instance, novel theoretical models and optimization techniques developed in machine learning are largely unknown within the social and biological sciences, which have instead emphasized model interpretability. Here, we describe a unified view of generative models for networks that draws together many of these disparate threads and highlights the fundamental similarities and differences that span these fields. We then describe a number of opportunities and challenges for future work that are revealed by this view. version:1
arxiv-1411-4068 | Dynamic Programming for Instance Annotation in Multi-instance Multi-label Learning | http://arxiv.org/abs/1411.4068 | id:1411.4068 author:Anh T. Pham, Raviv Raich, Xiaoli Z. Fern category:stat.ML cs.LG  published:2014-11-14 summary:Labeling data for classification requires significant human effort. To reduce labeling cost, instead of labeling every instance, a group of instances (bag) is labeled by a single bag label. Computer algorithms are then used to infer the label for each instance in a bag, a process referred to as instance annotation. This task is challenging due to the ambiguity regarding the instance labels. We propose a discriminative probabilistic model for the instance annotation problem and introduce an expectation maximization framework for inference, based on the maximum likelihood approach. For many probabilistic approaches, brute-force computation of the instance label posterior probability given its bag label is exponential in the number of instances in the bag. Our key contribution is a dynamic programming method for computing the posterior that is linear in the number of instances. We evaluate our methods using both benchmark and real world data sets, in the domain of bird song, image annotation, and activity recognition. In many cases, the proposed framework outperforms, sometimes significantly, the current state-of-the-art MIML learning methods, both in instance label prediction and bag label prediction. version:1
arxiv-1411-4064 | A Faster Method for Tracking and Scoring Videos Corresponding to Sentences | http://arxiv.org/abs/1411.4064 | id:1411.4064 author:Haonan Yu, Daniel P. Barrett, Jeffrey Mark Siskind category:cs.CV  published:2014-11-14 summary:Prior work presented the sentence tracker, a method for scoring how well a sentence describes a video clip or alternatively how well a video clip depicts a sentence. We present an improved method for optimizing the same cost function employed by this prior work, reducing the space complexity from exponential in the sentence length to polynomial, as well as producing a qualitatively identical result in time polynomial in the sentence length instead of exponential. Since this new method is plug-compatible with the prior method, it can be used for the same applications: video retrieval with sentential queries, generating sentential descriptions of video clips, and focusing the attention of a tracker with a sentence, while allowing these applications to scale with significantly larger numbers of object detections, word meanings modeled with HMMs with significantly larger numbers of states, and significantly longer sentences, with no appreciable degradation in quality of results. version:1
arxiv-1403-0989 | Detecting change points in the large-scale structure of evolving networks | http://arxiv.org/abs/1403.0989 | id:1403.0989 author:Leto Peel, Aaron Clauset category:cs.SI physics.soc-ph stat.ML  published:2014-03-05 summary:Interactions among people or objects are often dynamic in nature and can be represented as a sequence of networks, each providing a snapshot of the interactions over a brief period of time. An important task in analyzing such evolving networks is change-point detection, in which we both identify the times at which the large-scale pattern of interactions changes fundamentally and quantify how large and what kind of change occurred. Here, we formalize for the first time the network change-point detection problem within an online probabilistic learning framework and introduce a method that can reliably solve it. This method combines a generalized hierarchical random graph model with a Bayesian hypothesis test to quantitatively determine if, when, and precisely how a change point has occurred. We analyze the detectability of our method using synthetic data with known change points of different types and magnitudes, and show that this method is more accurate than several previously used alternatives. Applied to two high-resolution evolving social networks, this method identifies a sequence of change points that align with known external "shocks" to these networks. version:2
arxiv-1411-4006 | A Discriminative CNN Video Representation for Event Detection | http://arxiv.org/abs/1411.4006 | id:1411.4006 author:Zhongwen Xu, Yi Yang, Alexander G. Hauptmann category:cs.CV  published:2014-11-14 summary:In this paper, we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available. The focus of this paper is to effectively leverage deep Convolutional Neural Networks (CNNs) to advance event detection, where only frame level static descriptors can be extracted by the existing CNN toolkit. This paper makes two contributions to the inference of CNN video representation. First, while average pooling and max pooling have long been the standard approaches to aggregating frame level static features, we show that performance can be significantly improved by taking advantage of an appropriate encoding method. Second, we propose using a set of latent concept descriptors as the frame descriptor, which enriches visual information while keeping it computationally affordable. The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets. Compared to improved Dense Trajectories, which has been recognized as the best video representation for event detection, our new representation improves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVID MEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset. This work is the core part of the winning solution of our CMU-Informedia team in TRECVID MED 2014 competition. version:1
arxiv-1411-4005 | A convex formulation for hyperspectral image superresolution via subspace-based regularization | http://arxiv.org/abs/1411.4005 | id:1411.4005 author:Miguel Sim√µes, Jos√© Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot category:cs.CV physics.data-an stat.ML  published:2014-11-14 summary:Hyperspectral remote sensing images (HSIs) usually have high spectral resolution and low spatial resolution. Conversely, multispectral images (MSIs) usually have low spectral and high spatial resolutions. The problem of inferring images which combine the high spectral and high spatial resolutions of HSIs and MSIs, respectively, is a data fusion problem that has been the focus of recent active research due to the increasing availability of HSIs and MSIs retrieved from the same geographical area. We formulate this problem as the minimization of a convex objective function containing two quadratic data-fitting terms and an edge-preserving regularizer. The data-fitting terms account for blur, different resolutions, and additive noise. The regularizer, a form of vector Total Variation, promotes piecewise-smooth solutions with discontinuities aligned across the hyperspectral bands. The downsampling operator accounting for the different spatial resolutions, the non-quadratic and non-smooth nature of the regularizer, and the very large size of the HSI to be estimated lead to a hard optimization problem. We deal with these difficulties by exploiting the fact that HSIs generally "live" in a low-dimensional subspace and by tailoring the Split Augmented Lagrangian Shrinkage Algorithm (SALSA), which is an instance of the Alternating Direction Method of Multipliers (ADMM), to this optimization problem, by means of a convenient variable splitting. The spatial blur and the spectral linear operators linked, respectively, with the HSI and MSI acquisition processes are also estimated, and we obtain an effective algorithm that outperforms the state-of-the-art, as illustrated in a series of experiments with simulated and real-life data. version:1
arxiv-1406-3332 | Convolutional Kernel Networks | http://arxiv.org/abs/1406.3332 | id:1406.3332 author:Julien Mairal, Piotr Koniusz, Zaid Harchaoui, Cordelia Schmid category:cs.CV cs.LG stat.ML  published:2014-06-12 summary:An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art. version:2
arxiv-1411-4046 | Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy | http://arxiv.org/abs/1411.4046 | id:1411.4046 author:Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour category:cs.LG cs.CV  published:2014-11-14 summary:Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at "http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html". version:1
arxiv-1410-5224 | Supervised mid-level features for word image representation | http://arxiv.org/abs/1410.5224 | id:1410.5224 author:Albert Gordo category:cs.CV  published:2014-10-20 summary:This paper addresses the problem of learning word image representations: given the cropped image of a word, we are interested in finding a descriptive, robust, and compact fixed-length representation. Machine learning techniques can then be supplied with these representations to produce models useful for word retrieval or recognition tasks. Although many works have focused on the machine learning aspect once a global representation has been produced, little work has been devoted to the construction of those base image representations: most works use standard coding and aggregation techniques directly on top of standard computer vision features such as SIFT or HOG. We propose to learn local mid-level features suitable for building word image representations. These features are learnt by leveraging character bounding box annotations on a small set of training images. However, contrary to other approaches that use character bounding box information, our approach does not rely on detecting the individual characters explicitly at testing time. Our local mid-level features can then be aggregated to produce a global word image signature. When pairing these features with the recent word attributes framework of Almaz\'an et al., we obtain results comparable with or better than the state-of-the-art on matching and recognition tasks using global descriptors of only 96 dimensions. version:2
arxiv-1303-3240 | A Unified Framework for Probabilistic Component Analysis | http://arxiv.org/abs/1303.3240 | id:1303.3240 author:Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic category:cs.LG cs.CV stat.ML  published:2013-03-13 summary:We present a unifying framework which reduces the construction of probabilistic component analysis techniques to a mere selection of the latent neighbourhood, thus providing an elegant and principled framework for creating novel component analysis models as well as constructing probabilistic equivalents of deterministic component analysis methods. Under our framework, we unify many very popular and well-studied component analysis algorithms, such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Locality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some of which have no probabilistic equivalents in literature thus far. We firstly define the Markov Random Fields (MRFs) which encapsulate the latent connectivity of the aforementioned component analysis techniques; subsequently, we show that the projection directions produced by all PCA, LDA, LPP and SFA are also produced by the Maximum Likelihood (ML) solution of a single joint probability density function, composed by selecting one of the defined MRF priors while utilising a simple observation model. Furthermore, we propose novel Expectation Maximization (EM) algorithms, exploiting the proposed joint PDF, while we generalize the proposed methodologies to arbitrary connectivities via parameterizable MRF products. Theoretical analysis and experiments on both simulated and real world data show the usefulness of the proposed framework, by deriving methods which well outperform state-of-the-art equivalents. version:2
arxiv-1202-1121 | rFerns: An Implementation of the Random Ferns Method for General-Purpose Machine Learning | http://arxiv.org/abs/1202.1121 | id:1202.1121 author:Miron B. Kursa category:cs.LG stat.ML  published:2012-02-06 summary:In this paper I present an extended implementation of the Random ferns algorithm contained in the R package rFerns. It differs from the original by the ability of consuming categorical and numerical attributes instead of only binary ones. Also, instead of using simple attribute subspace ensemble it employs bagging and thus produce error approximation and variable importance measure modelled after Random forest algorithm. I also present benchmarks' results which show that although Random ferns' accuracy is mostly smaller than achieved by Random forest, its speed and good quality of importance measure it provides make rFerns a reasonable choice for a specific applications. version:2
arxiv-1411-3919 | Sample-targeted clinical trial adaptation | http://arxiv.org/abs/1411.3919 | id:1411.3919 author:Ognjen Arandjelovic category:cs.LG  published:2014-11-14 summary:Clinical trial adaptation refers to any adjustment of the trial protocol after the onset of the trial. The main goal is to make the process of introducing new medical interventions to patients more efficient by reducing the cost and the time associated with evaluating their safety and efficacy. The principal question is how should adaptation be performed so as to minimize the chance of distorting the outcome of the trial. We propose a novel method for achieving this. Unlike previous work our approach focuses on trial adaptation by sample size adjustment. We adopt a recently proposed stratification framework based on collected auxiliary data and show that this information together with the primary measured variables can be used to make a probabilistically informed choice of the particular sub-group a sample should be removed from. Experiments on simulated data are used to illustrate the effectiveness of our method and its application in practice. version:1
arxiv-1411-3895 | Learning Fuzzy Controllers in Mobile Robotics with Embedded Preprocessing | http://arxiv.org/abs/1411.3895 | id:1411.3895 author:I. Rodr√≠guez-Fdez, M. Mucientes, A. Bugar√≠n category:cs.RO cs.AI cs.LG  published:2014-11-14 summary:The automatic design of controllers for mobile robots usually requires two stages. In the first stage,sensorial data are preprocessed or transformed into high level and meaningful values of variables whichare usually defined from expert knowledge. In the second stage, a machine learning technique is applied toobtain a controller that maps these high level variables to the control commands that are actually sent tothe robot. This paper describes an algorithm that is able to embed the preprocessing stage into the learningstage in order to get controllers directly starting from sensorial raw data with no expert knowledgeinvolved. Due to the high dimensionality of the sensorial data, this approach uses Quantified Fuzzy Rules(QFRs), that are able to transform low-level input variables into high-level input variables, reducingthe dimensionality through summarization. The proposed learning algorithm, called Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic programming. IQFRL is able to learn rules with differentstructures, and can manage linguistic variables with multiple granularities. The algorithm has been testedwith the implementation of the wall-following behavior both in several realistic simulated environmentswith different complexity and on a Pioneer 3-AT robot in two real environments. Results have beencompared with several well-known learning algorithms combined with different data preprocessingtechniques, showing that IQFRL exhibits a better and statistically significant performance. Moreover,three real world applications for which IQFRL plays a central role are also presented: path and objecttracking with static and moving obstacles avoidance. version:1
arxiv-1411-3159 | Part Detector Discovery in Deep Convolutional Neural Networks | http://arxiv.org/abs/1411.3159 | id:1411.3159 author:Marcel Simon, Erik Rodner, Joachim Denzler category:cs.CV I.4.8  published:2014-11-12 summary:Current fine-grained classification approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and efficient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called "part detector discovery" (PDD) is based on analyzing the gradient maps of the network outputs and finding activation centers spatially related to annotated semantic parts or bounding boxes. This allows us not just to obtain excellent performance on the CUB200-2011 dataset, but in contrast to previous approaches also to perform detection and bird classification jointly without requiring a given bounding box annotation during testing and ground-truth parts during training. The code is available at http://www.inf-cv.uni-jena.de/part_discovery and https://github.com/cvjena/PartDetectorDisovery. version:2
arxiv-1411-3825 | Statistical Models for Degree Distributions of Networks | http://arxiv.org/abs/1411.3825 | id:1411.3825 author:Kayvan Sadeghi, Alessandro Rinaldo category:math.ST stat.ML stat.TH  published:2014-11-14 summary:We define and study the statistical models in exponential family form whose sufficient statistics are the degree distributions and the bi-degree distributions of undirected labelled simple graphs. Graphs that are constrained by the joint degree distributions are called $dK$-graphs in the computer science literature and this paper attempts to provide the first statistically grounded analysis of this type of models. In addition to formalizing these models, we provide some preliminary results for the parameter estimation and the asymptotic behaviour of the model for degree distribution, and discuss the parameter estimation for the model for bi-degree distribution. version:1
arxiv-1411-3806 | Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing Problem with Time Windows | http://arxiv.org/abs/1411.3806 | id:1411.3806 author:Sandhya Bansal, V. Katiyar category:cs.AI cs.CE cs.NE  published:2014-11-14 summary:In this paper fuzzy VRPTW with an uncertain travel time is considered. Credibility theory is used to model the problem and specifies a preference index at which it is desired that the travel times to reach the customers fall into their time windows. We propose the integration of fuzzy and ant colony system based evolutionary algorithm to solve the problem while preserving the constraints. Computational results for certain benchmark problems having short and long time horizons are presented to show the effectiveness of the algorithm. Comparison between different preferences indexes have been obtained to help the user in making suitable decisions. version:1
arxiv-1411-3803 | Stochastic Compositional Gradient Descent: Algorithms for Minimizing Compositions of Expected-Value Functions | http://arxiv.org/abs/1411.3803 | id:1411.3803 author:Mengdi Wang, Ethan X. Fang, Han Liu category:stat.ML  published:2014-11-14 summary:Classical stochastic gradient methods are well suited for minimizing expected-value objective functions. However, they do not apply to the minimization of a nonlinear function involving expected values or a composition of two expected-value functions, i.e., problems of the form $\min_x \mathbf{E}_v [f_v\big(\mathbf{E}_w [g_w(x)]\big)]$. In order to solve this stochastic composition problem, we propose a class of stochastic compositional gradient descent (SCGD) algorithms that can be viewed as stochastic versions of quasi-gradient method. SCGD update the solutions based on noisy sample gradients of $f_v,g_{w}$ and use an auxiliary variable to track the unknown quantity $\mathbf{E}_w[g_w(x)]$. We prove that the SCGD converge almost surely to an optimal solution for convex optimization problems, as long as such a solution exists. The convergence involves the interplay of two iterations with different time scales. For nonsmooth convex problems, the SCGD achieve a convergence rate of $O(k^{-1/4})$ in the general case and $O(k^{-2/3})$ in the strongly convex case, after taking $k$ samples. For smooth convex problems, the SCGD can be accelerated to converge at a rate of $O(k^{-2/7})$ in the general case and $O(k^{-4/5})$ in the strongly convex case. For nonconvex problems, we prove that any limit point generated by SCGD is a stationary point, for which we also provide the convergence rate analysis. Indeed, the stochastic setting where one wants to optimize compositions of expected-value functions is very common in practice. The proposed SCGD methods find wide applications in learning, estimation, dynamic programming, etc. version:1
arxiv-1411-3787 | Asymmetric Minwise Hashing | http://arxiv.org/abs/1411.3787 | id:1411.3787 author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DB cs.DS cs.IR cs.LG  published:2014-11-14 summary:Minwise hashing (Minhash) is a widely popular indexing scheme in practice. Minhash is designed for estimating set resemblance and is known to be suboptimal in many applications where the desired measure is set overlap (i.e., inner product between binary vectors) or set containment. Minhash has inherent bias towards smaller sets, which adversely affects its performance in applications where such a penalization is not desirable. In this paper, we propose asymmetric minwise hashing (MH-ALSH), to provide a solution to this problem. The new scheme utilizes asymmetric transformations to cancel the bias of traditional minhash towards smaller sets, making the final "collision probability" monotonic in the inner product. Our theoretical comparisons show that for the task of retrieving with binary inner products asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithms for general inner products. Thus, we obtain an algorithmic improvement over existing approaches in the literature. Experimental evaluations on four publicly available high-dimensional datasets validate our claims and the proposed scheme outperforms, often significantly, other hashing algorithms on the task of near neighbor retrieval with set containment. Our proposal is simple and easy to implement in practice. version:1
arxiv-1401-4489 | An Analysis of Random Projections in Cancelable Biometrics | http://arxiv.org/abs/1401.4489 | id:1401.4489 author:Devansh Arpit, Ifeoma Nwogu, Gaurav Srivastava, Venu Govindaraju category:cs.CV cs.LG stat.ML  published:2014-01-17 summary:With increasing concerns about security, the need for highly secure physical biometrics-based authentication systems utilizing \emph{cancelable biometric} technologies is on the rise. Because the problem of cancelable template generation deals with the trade-off between template security and matching performance, many state-of-the-art algorithms successful in generating high quality cancelable biometrics all have random projection as one of their early processing steps. This paper therefore presents a formal analysis of why random projections is an essential step in cancelable biometrics. By formally defining the notion of an \textit{Independent Subspace Structure} for datasets, it can be shown that random projection preserves the subspace structure of data vectors generated from a union of independent linear subspaces. The bound on the minimum number of random vectors required for this to hold is also derived and is shown to depend logarithmically on the number of data samples, not only in independent subspaces but in disjoint subspace settings as well. The theoretical analysis presented is supported in detail with empirical results on real-world face recognition datasets. version:3
arxiv-1210-7559 | Tensor decompositions for learning latent variable models | http://arxiv.org/abs/1210.7559 | id:1210.7559 author:Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, Matus Telgarsky category:cs.LG math.NA stat.ML  published:2012-10-29 summary:This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models. version:4
arxiv-1410-5410 | Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner Product Search (MIPS) | http://arxiv.org/abs/1410.5410 | id:1410.5410 author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DS cs.IR cs.LG  published:2014-10-20 summary:Recently it was shown that the problem of Maximum Inner Product Search (MIPS) is efficient and it admits provably sub-linear hashing algorithms. Asymmetric transformations before hashing were the key in solving MIPS which was otherwise hard. In the prior work, the authors use asymmetric transformations which convert the problem of approximate MIPS into the problem of approximate near neighbor search which can be efficiently solved using hashing. In this work, we provide a different transformation which converts the problem of approximate MIPS into the problem of approximate cosine similarity search which can be efficiently solved using signed random projections. Theoretical analysis show that the new scheme is significantly better than the original scheme for MIPS. Experimental evaluations strongly support the theoretical findings. version:2
arxiv-1411-3652 | Jamming Bandits | http://arxiv.org/abs/1411.3652 | id:1411.3652 author:SaiDhiraj Amuru, Cem Tekin, Mihaela van der Schaar, R. Michael Buehrer category:cs.IT cs.LG math.IT  published:2014-11-13 summary:Can an intelligent jammer learn and adapt to unknown environments in an electronic warfare-type scenario? In this paper, we answer this question in the positive, by developing a cognitive jammer that adaptively and optimally disrupts the communication between a victim transmitter-receiver pair. We formalize the problem using a novel multi-armed bandit framework where the jammer can choose various physical layer parameters such as the signaling scheme, power level and the on-off/pulsing duration in an attempt to obtain power efficient jamming strategies. We first present novel online learning algorithms to maximize the jamming efficacy against static transmitter-receiver pairs and prove that our learning algorithm converges to the optimal (in terms of the error rate inflicted at the victim and the energy used) jamming strategy. Even more importantly, we prove that the rate of convergence to the optimal jamming strategy is sub-linear, i.e. the learning is fast in comparison to existing reinforcement learning algorithms, which is particularly important in dynamically changing wireless environments. Also, we characterize the performance of the proposed bandit-based learning algorithm against multiple static and adaptive transmitter-receiver pairs. version:1
arxiv-1411-3650 | DUM: Diversity-Weighted Utility Maximization for Recommendations | http://arxiv.org/abs/1411.3650 | id:1411.3650 author:Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, Zheng Wen category:cs.IR stat.ML  published:2014-11-13 summary:The need for diversification of recommendation lists manifests in a number of recommender systems use cases. However, an increase in diversity may undermine the utility of the recommendations, as relevant items in the list may be replaced by more diverse ones. In this work we propose a novel method for maximizing the utility of the recommended items subject to the diversity of user's tastes, and show that an optimal solution to this problem can be found greedily. We evaluate the proposed method in two online user studies as well as in an offline analysis incorporating a number of evaluation metrics. The results of evaluations show the superiority of our method over a number of baselines. version:1
arxiv-1411-3715 | Acoustic Scene Classification | http://arxiv.org/abs/1411.3715 | id:1411.3715 author:Daniele Barchiesi, Dimitrios Giannoulis, Dan Stowell, Mark D. Plumbley category:cs.SD cs.LG  published:2014-11-13 summary:In this article we present an account of the state-of-the-art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different imple- mentations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The dataset recorded for this purpose is presented, along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods. We use a baseline method that employs MFCCS, GMMS and a maximum likelihood criterion as a benchmark, and only find sufficient evidence to conclude that three algorithms significantly outperform it. We also evaluate the human classification accuracy in performing a similar classification task. The best performing algorithm achieves a mean accuracy that matches the median accuracy obtained by humans, and common pairs of classes are misclassified by both computers and humans. However, all acoustic scenes are correctly classified by at least some individuals, while there are scenes that are misclassified by all algorithms. version:1
arxiv-1410-0718 | Not All Neural Embeddings are Born Equal | http://arxiv.org/abs/1410.0718 | id:1410.0718 author:Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio category:cs.CL  published:2014-10-02 summary:Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status. version:2
arxiv-1411-3561 | A Text to Speech (TTS) System with English to Punjabi Conversion | http://arxiv.org/abs/1411.3561 | id:1411.3561 author:Prabhsimran Singh, Amritpal Singh category:cs.CL  published:2014-11-13 summary:The paper aims to show how an application can be developed that converts the English language into the Punjabi Language, and the same application can convert the Text to Speech(TTS) i.e. pronounce the text. This application can be really beneficial for those with special needs. version:1
arxiv-1411-3553 | Greedy metrics in orthogonal greedy learning | http://arxiv.org/abs/1411.3553 | id:1411.3553 author:Lin Xu, Shaobo Lin, Jinshan Zeng, Zongben Xu category:cs.LG F.2.2  published:2014-11-13 summary:Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a new atom from a dictionary via the steepest gradient descent and build the estimator via orthogonal projecting the target function to the space spanned by the selected atoms in each greedy step. Here, "greed" means choosing a new atom according to the steepest gradient descent principle. OGL then avoids the overfitting/underfitting by selecting an appropriate iteration number. In this paper, we point out that the overfitting/underfitting can also be avoided via redefining "greed" in OGL. To this end, we introduce a new greedy metric, called $\delta$-greedy thresholds, to refine "greed" and theoretically verifies its feasibility. Furthermore, we reveals that such a greedy metric can bring an adaptive termination rule on the premise of maintaining the prominent learning performance of OGL. Our results show that the steepest gradient descent is not the unique greedy metric of OGL and some other more suitable metric may lessen the hassle of model-selection of OGL. version:1
arxiv-1411-3525 | Gaze Stabilization for Humanoid Robots: a Comprehensive Framework | http://arxiv.org/abs/1411.3525 | id:1411.3525 author:Alessandro Roncone, Ugo Pattacini, Giorgio Metta, Lorenzo Natale category:cs.RO cs.CV  published:2014-11-13 summary:Gaze stabilization is an important requisite for humanoid robots. Previous work on this topic has focused on the integration of inertial and visual information. Little attention has been given to a third component, which is the knowledge that the robot has about its own movement. In this work we propose a comprehensive framework for gaze stabilization in a humanoid robot. We focus on the problem of compensating for disturbances induced in the cameras due to self-generated movements of the robot. In this work we employ two separate signals for stabilization: (1) an anticipatory term obtained from the velocity commands sent to the joints while the robot moves autonomously; (2) a feedback term from the on board gyroscope, which compensates unpredicted external disturbances. We first provide the mathematical formulation to derive the forward and the differential kinematics of the fixation point of the stereo system. We finally test our method on the iCub robot. We show that the stabilization consistently reduces the residual optical flow during the movement of the robot and in presence of external disturbances. We also demonstrate that proper integration of the neck DoF is crucial to achieve correct stabilization. version:1
arxiv-1403-1626 | Can Image-Level Labels Replace Pixel-Level Labels for Image Parsing | http://arxiv.org/abs/1403.1626 | id:1403.1626 author:Zhiwu Lu, Zhenyong Fu, Tao Xiang, Liwei Wang, Ji-Rong Wen category:cs.CV  published:2014-03-07 summary:This paper presents a weakly supervised sparse learning approach to the problem of noisily tagged image parsing, or segmenting all the objects within a noisily tagged image and identifying their categories (i.e. tags). Different from the traditional image parsing that takes pixel-level labels as strong supervisory information, our noisily tagged image parsing is provided with noisy tags of all the images (i.e. image-level labels), which is a natural setting for social image collections (e.g. Flickr). By oversegmenting all the images into regions, we formulate noisily tagged image parsing as a weakly supervised sparse learning problem over all the regions, where the initial labels of each region are inferred from image-level labels. Furthermore, we develop an efficient algorithm to solve such weakly supervised sparse learning problem. The experimental results on two benchmark datasets show the effectiveness of our approach. More notably, the reported surprising results shed some light on answering the question: can image-level labels replace pixel-level labels (hard to access) as supervisory information for image parsing. version:3
arxiv-1308-4747 | Joint modeling of multiple time series via the beta process with application to motion capture segmentation | http://arxiv.org/abs/1308.4747 | id:1308.4747 author:Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, Michael I. Jordan category:stat.ME stat.ML  published:2013-08-22 summary:We propose a Bayesian nonparametric approach to the problem of jointly modeling multiple related time series. Our model discovers a latent set of dynamical behaviors shared among the sequences, and segments each time series into regions defined by a subset of these behaviors. Using a beta process prior, the size of the behavior set and the sharing pattern are both inferred from data. We develop Markov chain Monte Carlo (MCMC) methods based on the Indian buffet process representation of the predictive distribution of the beta process. Our MCMC inference algorithm efficiently adds and removes behaviors via novel split-merge moves as well as data-driven birth and death proposals, avoiding the need to consider a truncated model. We demonstrate promising results on unsupervised segmentation of human motion capture data. version:3
arxiv-1312-6117 | Comparison three methods of clustering: k-means, spectral clustering and hierarchical clustering | http://arxiv.org/abs/1312.6117 | id:1312.6117 author:Kamran Kowsari category:cs.LG 68T10 H.3.3; I.5.3  published:2013-12-19 summary:Comparison of three kind of the clustering and find cost function and loss function and calculate them. Error rate of the clustering methods and how to calculate the error percentage always be one on the important factor for evaluating the clustering methods, so this paper introduce one way to calculate the error rate of clustering methods. Clustering algorithms can be divided into several categories including partitioning clustering algorithms, hierarchical algorithms and density based algorithms. Generally speaking we should compare clustering algorithms by Scalability, Ability to work with different attribute, Clusters formed by conventional, Having minimal knowledge of the computer to recognize the input parameters, Classes for dealing with noise and extra deposition that same error rate for clustering a new data, Thus, there is no effect on the input data, different dimensions of high levels, K-means is one of the simplest approach to clustering that clustering is an unsupervised problem. version:2
arxiv-1411-3436 | SelfieBoost: A Boosting Algorithm for Deep Learning | http://arxiv.org/abs/1411.3436 | id:1411.3436 author:Shai Shalev-Shwartz category:stat.ML cs.LG  published:2014-11-13 summary:We describe and analyze a new boosting algorithm for deep learning called SelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct ensembles of classifiers, SelfieBoost boosts the accuracy of a single network. We prove a $\log(1/\epsilon)$ convergence rate for SelfieBoost under some "SGD success" assumption which seems to hold in practice. version:1
arxiv-1411-3423 | A Comparative Study of Techniques of Distant Reconstruction of Displacement Fields by using DISTRESS Simulator | http://arxiv.org/abs/1411.3423 | id:1411.3423 author:Ghulam Mubashar Hassan, Arcady V. Dyskin, Cara K. MacNish category:cs.CV  published:2014-11-13 summary:Reconstruction and monitoring of displacement and strain fields is an important problem in engineering. We analyze the remote and non-obtrusive methods of strain measurement based on photogrammetry and Digital Image Correlation (DIC). The method is based on covering the photographed surface with a pattern of speckles and comparing the images taken before and after the deformation. In this study, a comprehensive literature review and comparative analysis of photogrammetric solutions is presented. The analysis is based on a specially developed Digital Image Synthesizer To Reconstruct Strain in Solids (DISTRESS) Simulator to generate synthetic images of displacement and stress fields in order to investigate the intrinsic accuracy of the existing variants of DIC. We investigated the Basic DIC and a commercial software VIC 2D, both based on displacement field reconstruction with post processing strain determination based on numerical differentiation. We also investigated what we call the Extended DIC where the strain field is determined independently of the displacement field. While the Basic DIC and VIC 2D are faster, the Extended DIC delivers the best accuracy of strain reconstruction. The speckle pattern is found to be playing a critical role in achieving high accuracy for DIC. Increase in subset size for DIC does not significantly improves the accuracy, while the smallest subset size depends on the speckle pattern and speckle size. Increase in the overall image size provides more details but does not play significant role in improving the accuracy, while significantly increasing the computation cost. version:1
arxiv-1411-3413 | Multi-view Anomaly Detection via Probabilistic Latent Variable Models | http://arxiv.org/abs/1411.3413 | id:1411.3413 author:Tomoharu Iwata, Makoto Yamada category:stat.ML cs.LG  published:2014-11-13 summary:We propose a nonparametric Bayesian probabilistic latent variable model for multi-view anomaly detection, which is the task of finding instances that have inconsistent views. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies and imputing missing values in multi-view data with anomalies. version:1
arxiv-1411-3410 | Person Re-identification Based on Color Histogram and Spatial Configuration of Dominant Color Regions | http://arxiv.org/abs/1411.3410 | id:1411.3410 author:Kwangchol Jang, Sokmin Han, Insong Kim category:cs.CV  published:2014-11-13 summary:There is a requirement to determine whether a given person of interest has already been observed over a network of cameras in video surveillance systems. A human appearance obtained in one camera is usually different from the ones obtained in another camera due to difference in illumination, pose and viewpoint, camera parameters. Being related to appearance-based approaches for person re-identification, we propose a novel method based on the dominant color histogram and spatial configuration of dominant color regions on human body parts. Dominant color histogram and spatial configuration of the dominant color regions based on dominant color descriptor(DCD) can be considered to be robust to illumination and pose, viewpoint changes. The proposed method is evaluated using benchmark video datasets. Experimental results using the cumulative matching characteristic(CMC) curve demonstrate the effectiveness of our approach for person re-identification. version:1
arxiv-1411-3409 | A Randomized Algorithm for CCA | http://arxiv.org/abs/1411.3409 | id:1411.3409 author:Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG  published:2014-11-13 summary:We present RandomizedCCA, a randomized algorithm for computing canonical analysis, suitable for large datasets stored either out of core or on a distributed file system. Accurate results can be obtained in as few as two data passes, which is relevant for distributed processing frameworks in which iteration is expensive (e.g., Hadoop). The strategy also provides an excellent initializer for standard iterative solutions. version:1
arxiv-1405-0931 | Universal Memcomputing Machines | http://arxiv.org/abs/1405.0931 | id:1405.0931 author:Fabio L. Traversa, Massimiliano Di Ventra category:cs.NE cond-mat.mes-hall cs.ET cs.IT math.IT  published:2014-05-05 summary:We introduce the notion of universal memcomputing machines (UMMs): a class of brain-inspired general-purpose computing machines based on systems with memory, whereby processing and storing of information occur on the same physical location. We analytically prove that the memory properties of UMMs endow them with universal computing power - they are Turing-complete -, intrinsic parallelism, functional polymorphism, and information overhead, namely their collective states can support exponential data compression directly in memory. We also demonstrate that a UMM has the same computational power as a non-deterministic Turing machine, namely it can solve NP--complete problems in polynomial time. However, by virtue of its information overhead, a UMM needs only an amount of memory cells (memprocessors) that grows polynomially with the problem size. As an example we provide the polynomial-time solution of the subset-sum problem and a simple hardware implementation of the same. Even though these results do not prove the statement NP=P within the Turing paradigm, the practical realization of these UMMs would represent a paradigm shift from present von Neumann architectures bringing us closer to brain-like neural computation. version:2
arxiv-1406-2199 | Two-Stream Convolutional Networks for Action Recognition in Videos | http://arxiv.org/abs/1406.2199 | id:1406.2199 author:Karen Simonyan, Andrew Zisserman category:cs.CV  published:2014-06-09 summary:We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification. version:2
arxiv-1411-3315 | Statistically Significant Detection of Linguistic Change | http://arxiv.org/abs/1411.3315 | id:1411.3315 author:Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, Steven Skiena category:cs.CL cs.IR cs.LG H.3.3; I.2.6  published:2014-11-12 summary:We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts. We consider and analyze three approaches of increasing complexity to generate such linguistic property time series, the culmination of which uses distributional characteristics inferred from word co-occurrences. Using recently proposed deep neural language models, we first train vector representations of words for each time period. Second, we warp the vector spaces into one unified coordinate system. Finally, we construct a distance-based distributional time series for each word to track it's linguistic displacement over time. We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Book-ngrams. Our analysis reveals interesting patterns of language usage change commensurate with each medium. version:1
arxiv-1411-3302 | Using Gaussian Measures for Efficient Constraint Based Clustering | http://arxiv.org/abs/1411.3302 | id:1411.3302 author:Chandrima Sarkar, Atanu Roy category:cs.LG cs.IR  published:2014-11-12 summary:In this paper we present a novel iterative multiphase clustering technique for efficiently clustering high dimensional data points. For this purpose we implement clustering feature (CF) tree on a real data set and a Gaussian density distribution constraint on the resultant CF tree. The post processing by the application of Gaussian density distribution function on the micro-clusters leads to refinement of the previously formed clusters thus improving their quality. This algorithm also succeeds in overcoming the inherent drawbacks of conventional hierarchical methods of clustering like inability to undo the change made to the dendogram of the data points. Moreover, the constraint measure applied in the algorithm makes this clustering technique suitable for need driven data analysis. We provide veracity of our claim by evaluating our algorithm with other similar clustering algorithms. Introduction version:1
arxiv-1411-3277 | Using Ants as a Genetic Crossover Operator in GLS to Solve STSP | http://arxiv.org/abs/1411.3277 | id:1411.3277 author:Hassan Ismkhan category:cs.NE  published:2014-11-12 summary:Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two optimization algorithms that have been successfully applied to the Traveling Salesman Problem (TSP). In this paper we define new crossover operator then redefine ACAs ants as operate according to defined crossover operator then put forward our GLS that uses these ants to solve Symmetric TSP (STSP) instances. version:1
arxiv-1411-3251 | Identification of Helicopter Dynamics based on Flight Data using Nature Inspired Techniques | http://arxiv.org/abs/1411.3251 | id:1411.3251 author:S. N. Omkar, Dheevatsa Mudigere, J Senthilnath, M. Vijaya Kumar category:cs.CE cs.NE  published:2014-11-12 summary:The complexity of helicopter flight dynamics makes modeling and helicopter system identification a very difficult task. Most of the traditional techniques require a model structure to be defined apriori and in case of helicopter dynamics, this is difficult due to its complexity and the interplay between various subsystems.To overcome this difficulty, non-parametric approaches are commonly adopted for helicopter system identification. Artificial Neural Network are a widely used class of algorithms for non-parametric system identification, among them, the Nonlinear Auto Regressive eXogeneous input network (NARX) model is very popular, but it also necessitates some in depth knowledge regarding the system being modeled. There have been many approaches proposed to circumvent this and yet still retain the advantageous characteristics. In this paper we carry out an extensive study of one such newly proposed approach using a modified NARX model with a two tiered, externally driven recurrent neural network architecture. This is coupled with an outer optimization routine for evolving the order of the system. This generic architecture is comprehensively explored to ascertain its usability and critically asses its potential. Different instantiations of this architecture, based on nature inspired computational techniques (Artificial Bee Colony, Artificial Immune System and Particle Swarm Optimization) are evaluated and critically compared in this paper. Simulations have been carried out for identifying the longitudinally uncoupled dynamics. Results of identification indicate a quite close correlation between the actual and the predicted response of the helicopter for all the models. version:1
arxiv-1410-4461 | Map Matching based on Conditional Random Fields and Route Preference Mining for Uncertain Trajectories | http://arxiv.org/abs/1410.4461 | id:1410.4461 author:Xu Ming, Du Yi-man, Wu Jian-ping, Zhou Yang category:cs.NI cs.LG  published:2014-10-16 summary:In order to improve offline map matching accuracy of low-sampling-rate GPS, a map matching algorithm based on conditional random fields (CRF) and route preference mining is proposed. In this algorithm, road offset distance and the temporal-spatial relationship between the sampling points are used as features of GPS trajectory in CRF model, which can utilize the advantages of integrating the context information into features flexibly. When the sampling rate is too low, it is difficult to guarantee the effectiveness using temporal-spatial context modeled in CRF, and route preference of a driver is used as replenishment to be superposed on the temporal-spatial transition features. The experimental results show that this method can improve the accuracy of the matching, especially in the case of low sampling rate. version:2
arxiv-1409-7384 | A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure | http://arxiv.org/abs/1409.7384 | id:1409.7384 author:Tofigh Naghibi, Sarah Hoffmann, Beat Pfister category:cs.LG  published:2014-09-25 summary:Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximum-cut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account. version:2
arxiv-1411-3169 | On Coarse Graining of Information and Its Application to Pattern Recognition | http://arxiv.org/abs/1411.3169 | id:1411.3169 author:Ali Ghaderi category:cs.CV stat.ML  published:2014-11-12 summary:We propose a method based on finite mixture models for classifying a set of observations into number of different categories. In order to demonstrate the method, we show how the component densities for the mixture model can be derived by using the maximum entropy method in conjunction with conservation of Pythagorean means. Several examples of distributions belonging to the Pythagorean family are derived. A discussion on estimation of model parameters and the number of categories is also given. version:1
arxiv-1411-3146 | Distributed Representations for Compositional Semantics | http://arxiv.org/abs/1411.3146 | id:1411.3146 author:Karl Moritz Hermann category:cs.CL  published:2014-11-12 summary:The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP. This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP. version:1
arxiv-1404-1238 | Exact Estimation of Multiple Directed Acyclic Graphs | http://arxiv.org/abs/1404.1238 | id:1404.1238 author:Chris J. Oates, Jim Q. Smith, Sach Mukherjee, James Cussens category:stat.ML  published:2014-04-04 summary:This paper considers the problem of estimating the structure of multiple related directed acyclic graph (DAG) models. Building on recent developments in exact estimation of DAGs using integer linear programming (ILP), we present an ILP approach for joint estimation over multiple DAGs, that does not require that the vertices in each DAG share a common ordering. Furthermore, we allow also for (potentially unknown) dependency structure between the DAGs. Results are presented on both simulated data and fMRI data obtained from multiple subjects. version:3
arxiv-1411-3041 | Collecting Image Description Datasets using Crowdsourcing | http://arxiv.org/abs/1411.3041 | id:1411.3041 author:Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh category:cs.CV  published:2014-11-12 summary:We describe our two new datasets with images described by humans. Both the datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform. The two datasets contain significantly more descriptions per image than other existing datasets. One is based on a popular image description dataset called the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract Scenes dataset con- taining images made from clipart objects. In this paper we describe our interfaces, analyze some properties of and show example descriptions from our two datasets. version:1
arxiv-1411-1119 | Projecting Markov Random Field Parameters for Fast Mixing | http://arxiv.org/abs/1411.1119 | id:1411.1119 author:Xianghang Liu, Justin Domke category:cs.LG stat.ML  published:2014-11-05 summary:Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters. version:3
arxiv-1406-1845 | A Novel Test for Additivity in Supervised Ensemble Learners | http://arxiv.org/abs/1406.1845 | id:1406.1845 author:Lucas Mentch, Giles Hooker category:stat.ML stat.AP  published:2014-06-07 summary:Additive models remain popular statistical tools due to their ease of interpretation and as a result, hypothesis tests for additivity have been developed to assess the appropriateness of these models. However, as data grows in size and complexity, learning algorithms continue to gain popularity due to their exceptional predictive performance. Due to the black-box nature of these learning methods, the increase in predictive power is assumed to come at the cost of interpretability and inference. However, recent work suggests that many popular learning techniques, such as bagged trees and random forests, have desirable asymptotic properties which allow for formal statistical inference when base learners are built with proper subsamples. This work extends hypothesis tests previously developed and demonstrates that by enforcing a grid structure on an appropriate test set, we may perform formal hypothesis tests for additivity among features. We develop notions of total and partial additivity and demonstrate that both tests can be carried out at no additional computational cost. We also suggest a new testing procedure based on random projections that allows for testing on larger grids, even when the grid size is larger than that of the training set. Simulations and demonstrations on real data are provided. version:2
arxiv-1411-2919 | Bounded Regret for Finite-Armed Structured Bandits | http://arxiv.org/abs/1411.2919 | id:1411.2919 author:Tor Lattimore, Remi Munos category:cs.LG  published:2014-11-11 summary:We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal. version:1
arxiv-1401-4988 | Marginal Pseudo-Likelihood Learning of Markov Network structures | http://arxiv.org/abs/1401.4988 | id:1401.4988 author:Johan Pensar, Henrik Nyman, Juha Niiranen, Jukka Corander category:stat.ML  published:2014-01-20 summary:Undirected graphical models known as Markov networks are popular for a wide variety of applications ranging from statistical physics to computational biology. Traditionally, learning of the network structure has been done under the assumption of chordality which ensures that efficient scoring methods can be used. In general, non-chordal graphs have intractable normalizing constants which renders the calculation of Bayesian and other scores difficult beyond very small-scale systems. Recently, there has been a surge of interest towards the use of regularized pseudo-likelihood methods for structural learning of large-scale Markov network models, as such an approach avoids the assumption of chordality. The currently available methods typically necessitate the use of a tuning parameter to adapt the level of regularization for a particular dataset, which can be optimized for example by cross-validation. Here we introduce a Bayesian version of pseudo-likelihood scoring of Markov networks, which enables an automatic regularization through marginalization over the nuisance parameters in the model. We prove consistency of the resulting MPL estimator for the network structure via comparison with the pseudo information criterion. Identification of the MPL-optimal network on a prescanned graph space is considered with both greedy hill climbing and exact pseudo-Boolean optimization algorithms. We find that for reasonable sample sizes the hill climbing approach most often identifies networks that are at a negligible distance from the restricted global optimum. Using synthetic and existing benchmark networks, the marginal pseudo-likelihood method is shown to generally perform favorably against recent popular inference methods for Markov networks. version:2
arxiv-1411-2897 | Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic Operator | http://arxiv.org/abs/1411.2897 | id:1411.2897 author:Hassan Ismkhan category:cs.NE  published:2014-11-11 summary:This paper research review Ant colony optimization (ACO) and Genetic Algorithm (GA), both are two powerful meta-heuristics. This paper explains some major defects of these two algorithm at first then proposes a new model for ACO in which, artificial ants use a quick genetic operator and accelerate their actions in selecting next state. Experimental results show that proposed hybrid algorithm is effective and its performance including speed and accuracy beats other version. version:1
arxiv-1410-6751 | Detecting Figures and Part Labels in Patents: Competition-Based Development of Image Processing Algorithms | http://arxiv.org/abs/1410.6751 | id:1410.6751 author:Christoph Riedl, Richard Zanibbi, Marti A. Hearst, Siyu Zhu, Michael Menietti, Jason Crusan, Ivan Metelsky, Karim R. Lakhani category:cs.CV cs.IR  published:2014-10-24 summary:We report the findings of a month-long online competition in which participants developed algorithms for augmenting the digital version of patent documents published by the United States Patent and Trademark Office (USPTO). The goal was to detect figures and part labels in U.S. patent drawing pages. The challenge drew 232 teams of two, of which 70 teams (30%) submitted solutions. Collectively, teams submitted 1,797 solutions that were compiled on the competition servers. Participants reported spending an average of 63 hours developing their solutions, resulting in a total of 5,591 hours of development time. A manually labeled dataset of 306 patents was used for training, online system tests, and evaluation. The design and performance of the top-5 systems are presented, along with a system developed after the competition which illustrates that winning teams produced near state-of-the-art results under strict time and computation constraints. For the 1st place system, the harmonic mean of recall and precision (f-measure) was 88.57% for figure region detection, 78.81% for figure regions with correctly recognized figure titles, and 70.98% for part label detection and character recognition. Data and software from the competition are available through the online UCI Machine Learning repository to inspire follow-on work by the image processing community. version:3
arxiv-1411-2821 | Turn Down that Noise: Synaptic Encoding of Afferent SNR in a Single Spiking Neuron | http://arxiv.org/abs/1411.2821 | id:1411.2821 author:Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Philip de Chazal, Tara Julia Hamilton category:cs.NE q-bio.NC  published:2014-11-11 summary:We have added a simplified neuromorphic model of Spike Time Dependent Plasticity (STDP) to the Synapto-dendritic Kernel Adapting Neuron (SKAN). The resulting neuron model is the first to show synaptic encoding of afferent signal to noise ratio in addition to the unsupervised learning of spatio temporal spike patterns. The neuron model is particularly suitable for implementation in digital neuromorphic hardware as it does not use any complex mathematical operations and uses a novel approach to achieve synaptic homeostasis. The neurons noise compensation properties are characterized and tested on noise corrupted zeros digits of the MNIST handwritten dataset. Results show the simultaneously learning common patterns in its input data while dynamically weighing individual afferent channels based on their signal to noise ratio. Despite its simplicity the interesting behaviors of the neuron model and the resulting computational power may offer insights into biological systems. version:1
arxiv-1411-2820 | Supervised Classification of Flow Cytometric Samples via the Joint Clustering and Matching (JCM) Procedure | http://arxiv.org/abs/1411.2820 | id:1411.2820 author:Sharon X. Lee, Geoffrey J. McLachlan, Saumyadipta Pyne category:q-bio.QM stat.ME stat.ML  published:2014-11-11 summary:We consider the use of the Joint Clustering and Matching (JCM) procedure for the supervised classification of a flow cytometric sample with respect to a number of predefined classes of such samples. The JCM procedure has been proposed as a method for the unsupervised classification of cells within a sample into a number of clusters and in the case of multiple samples, the matching of these clusters across the samples. The two tasks of clustering and matching of the clusters are performed simultaneously within the JCM framework. In this paper, we consider the case where there is a number of distinct classes of samples whose class of origin is known, and the problem is to classify a new sample of unknown class of origin to one of these predefined classes. For example, the different classes might correspond to the types of a particular disease or to the various health outcomes of a patient subsequent to a course of treatment. We show and demonstrate on some real datasets how the JCM procedure can be used to carry out this supervised classification task. A mixture distribution is used to model the distribution of the expressions of a fixed set of markers for each cell in a sample with the components in the mixture model corresponding to the various populations of cells in the composition of the sample. For each class of samples, a class template is formed by the adoption of random-effects terms to model the inter-sample variation within a class. The classification of a new unclassified sample is undertaken by assigning the unclassified sample to the class that minimizes the Kullback-Leibler distance between its fitted mixture density and each class density provided by the class templates. version:1
arxiv-1411-2795 | Speaker Identification From Youtube Obtained Data | http://arxiv.org/abs/1411.2795 | id:1411.2795 author:Nitesh Kumar Chaudhary category:cs.SD cs.LG  published:2014-11-11 summary:An efficient, and intuitive algorithm is presented for the identification of speakers from a long dataset (like YouTube long discussion, Cocktail party recorded audio or video).The goal of automatic speaker identification is to identify the number of different speakers and prepare a model for that speaker by extraction, characterization and speaker-specific information contained in the speech signal. It has many diverse application specially in the field of Surveillance, Immigrations at Airport, cyber security, transcription in multi-source of similar sound source, where it is difficult to assign transcription arbitrary. The most commonly speech parametrization used in speaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixture modeling, which is the speaker modeling technique is then explained. Gaussian mixture models (GMM), perhaps the most robust machine learning algorithm has been introduced examine and judge carefully speaker identification in text independent. The application or employment of Gaussian mixture models for monitoring & Analysing speaker identity is encouraged by the familiarity, awareness, or understanding gained through experience that Gaussian spectrum depict the characteristics of speaker's spectral conformational pattern and remarkable ability of GMM to construct capricious densities after that we illustrate 'Expectation maximization' an iterative algorithm which takes some arbitrary value in initial estimation and carry on the iterative process until the convergence of value is observed,so by doing various number of experiments we are able to obtain 79 ~ 82% of identification rate using Vector quantization and 85 ~ 92.6% of identification rate using GMM modeling by Expectation maximization parameter estimation depending on variation of parameter. version:1
arxiv-1411-3197 | Warranty Cost Estimation Using Bayesian Network | http://arxiv.org/abs/1411.3197 | id:1411.3197 author:Karamjit Singh, Puneet Agarwal, Gautam Shroff category:cs.AI cs.LG  published:2014-11-11 summary:All multi-component product manufacturing companies face the problem of warranty cost estimation. Failure rate analysis of components plays a key role in this problem. Data source used for failure rate analysis has traditionally been past failure data of components. However, failure rate analysis can be improved by means of fusion of additional information, such as symptoms observed during after-sale service of the product, geographical information (hilly or plains areas), and information from tele-diagnostic analytics. In this paper, we propose an approach, which learns dependency between part-failures and symptoms gleaned from such diverse sources of information, to predict expected number of failures with better accuracy. We also indicate how the optimum warranty period can be computed. We demonstrate, through empirical results, that our method can improve the warranty cost estimates significantly. version:1
arxiv-1410-4393 | The HAWKwood Database | http://arxiv.org/abs/1410.4393 | id:1410.4393 author:Christopher Herbon category:cs.CV  published:2014-10-16 summary:We present a database consisting of wood pile images, which can be used as a benchmark to evaluate the performance of wood pile detection and surveying algorithms. We distinguish six database cate- gories which can be used for different types of algorithms. Images of real and synthetic scenes are provided, which consist of 7655 images divided into 354 data sets. Depending on the category the data sets either include ground truth data or forestry specific measurements with which algorithms may be compared. version:2
arxiv-1411-2679 | Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks | http://arxiv.org/abs/1411.2679 | id:1411.2679 author:Jiwei Li, Alan Ritter, Dan Jurafsky category:cs.SI cs.AI cs.CL cs.LG  published:2014-11-11 summary:We propose a framework for inferring the latent attitudes or preferences of users by performing probabilistic first-order logical reasoning over the social network graph. Our method answers questions about Twitter users like {\em Does this user like sushi?} or {\em Is this user a New York Knicks fan?} by building a probabilistic model that reasons over user attributes (the user's location or gender) and the social network (the user's friends and spouse), via inferences like homophily (I am more likely to like sushi if spouse or friends like sushi, I am more likely to like the Knicks if I live in New York). The algorithm uses distant supervision, semi-supervised data harvesting and vector space models to extract user attributes (e.g. spouse, education, location) and preferences (likes and dislikes) from text. The extracted propositions are then fed into a probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft Logic). Our experiments show that probabilistic logical reasoning significantly improves the performance on attribute and relation extraction, and also achieves an F-score of 0.791 at predicting a users likes or dislikes, significantly better than two strong baselines. version:1
arxiv-1408-6141 | Recursive Total Least-Squares Algorithm Based on Inverse Power Method and Dichotomous Coordinate-Descent Iterations | http://arxiv.org/abs/1408.6141 | id:1408.6141 author:Reza Arablouei, Kutluyƒ±l Doƒüan√ßay, Stefan Werner category:cs.SY cs.LG  published:2014-08-25 summary:We develop a recursive total least-squares (RTLS) algorithm for errors-in-variables system identification utilizing the inverse power method and the dichotomous coordinate-descent (DCD) iterations. The proposed algorithm, called DCD-RTLS, outperforms the previously-proposed RTLS algorithms, which are based on the line-search method, with reduced computational complexity. We perform a comprehensive analysis of the DCD-RTLS algorithm and show that it is asymptotically unbiased as well as being stable in the mean. We also find a lower bound for the forgetting factor that ensures mean-square stability of the algorithm and calculate the theoretical steady-state mean-square deviation (MSD). We verify the effectiveness of the proposed algorithm and the accuracy of the predicted steady-state MSD via simulations. version:2
arxiv-1411-2645 | Non-crossing dependencies: least effort, not grammar | http://arxiv.org/abs/1411.2645 | id:1411.2645 author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph  published:2014-11-10 summary:The use of null hypotheses (in a statistical sense) is common in hard sciences but not in theoretical linguistics. Here the null hypothesis that the low frequency of syntactic dependency crossings is expected by an arbitrary ordering of words is rejected. It is shown that this would require star dependency structures, which are both unrealistic and too restrictive. The hypothesis of the limited resources of the human brain is revisited. Stronger null hypotheses taking into account actual dependency lengths for the likelihood of crossings are presented. Those hypotheses suggests that crossings are likely to reduce when dependencies are shortened. A hypothesis based on pressure to reduce dependency lengths is more parsimonious than a principle of minimization of crossings or a grammatical ban that is totally dissociated from the general and non-linguistic principle of economy. version:1
arxiv-1411-2635 | A chain rule for the expected suprema of Gaussian processes | http://arxiv.org/abs/1411.2635 | id:1411.2635 author:Andreas Maurer category:cs.LG  published:2014-11-10 summary:The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models. version:1
arxiv-1411-2581 | Deep Exponential Families | http://arxiv.org/abs/1411.2581 | id:1411.2581 author:Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei category:stat.ML cs.LG  published:2014-11-10 summary:We describe \textit{deep exponential families} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent "black box" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models. version:1
arxiv-1409-3040 | Towards Optimal Algorithms for Prediction with Expert Advice | http://arxiv.org/abs/1409.3040 | id:1409.3040 author:Nick Gravin, Yuval Peres, Balasubramanian Sivan category:cs.LG cs.GT math.PR  published:2014-09-10 summary:We study the classical problem of prediction with expert advice in the adversarial setting with a geometric stopping time. In 1965, Cover gave the optimal algorithm for the case of $2$ experts. In this paper, we design the optimal algorithm, adversary and regret for the case of $3$ experts. Further, we show that the optimal algorithm for $2$ and $3$ experts is a probability matching algorithm (analogous to Thompson sampling) against a particular randomized adversary. Remarkably, it turns out that this algorithm is not only optimal against this adversary, but also minimax optimal against all possible adversaries. We establish a constant factor separation between the regrets achieved by the optimal algorithm and the widely used multiplicative weights algorithm. Along the way, we improve the regret lower bounds for the multiplicative weights algorithm for an arbitrary number of experts and show that this is tight for $2$ experts. A novel aspect of our analysis is that we develop upper and lower bounds simultaneously, analogous to the primal-dual method. The analysis of the optimal adversary relies on delicate random walk estimates. We further use this connection to develop an improved regret bound for the case of $4$ experts, and provide a general framework for designing the optimal algorithm for an arbitrary number of experts. version:4
arxiv-1411-2539 | Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models | http://arxiv.org/abs/1411.2539 | id:1411.2539 author:Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel category:cs.LG cs.CL cs.CV  published:2014-11-10 summary:Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison. version:1
arxiv-1405-5868 | Learning to Generate Networks | http://arxiv.org/abs/1405.5868 | id:1405.5868 author:James Atwood, Don Towsley, Krista Gile, David Jensen category:cs.LG cs.SI physics.soc-ph  published:2014-05-22 summary:We investigate the problem of learning to generate complex networks from data. Specifically, we consider whether deep belief networks, dependency networks, and members of the exponential random graph family can learn to generate networks whose complex behavior is consistent with a set of input examples. We find that the deep model is able to capture the complex behavior of small networks, but that no model is able capture this behavior for networks with more than a handful of nodes. version:2
arxiv-1312-4176 | Distributed k-means algorithm | http://arxiv.org/abs/1312.4176 | id:1312.4176 author:Gabriele Oliva, Roberto Setola, Christoforos N. Hadjicostis category:cs.LG cs.DC  published:2013-12-15 summary:In this paper we provide a fully distributed implementation of the k-means clustering algorithm, intended for wireless sensor networks where each agent is endowed with a possibly high-dimensional observation (e.g., position, humidity, temperature, etc.) The proposed algorithm, by means of one-hop communication, partitions the agents into measure-dependent groups that have small in-group and large out-group "distances". Since the partitions may not have a relation with the topology of the network--members of the same clusters may not be spatially close--the algorithm is provided with a mechanism to compute the clusters'centroids even when the clusters are disconnected in several sub-clusters.The results of the proposed distributed algorithm coincide, in terms of minimization of the objective function, with the centralized k-means algorithm. Some numerical examples illustrate the capabilities of the proposed solution. version:3
arxiv-1411-2405 | Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior | http://arxiv.org/abs/1411.2405 | id:1411.2405 author:Zahra Sabetsarvestani, Hamidreza Amindavar category:cs.IT math.IT stat.ML  published:2014-11-10 summary:In this paper, the use of the Generalized Beta Mixture (GBM) and Horseshoe distributions as priors in the Bayesian Compressive Sensing framework is proposed. The distributions are considered in a two-layer hierarchical model, making the corresponding inference problem amenable to Expectation Maximization (EM). We present an explicit, algebraic EM-update rule for the models, yielding two fast and experimentally validated algorithms for signal recovery. Experimental results show that our algorithms outperform state-of-the-art methods on a wide range of sparsity levels and amplitudes in terms of reconstruction accuracy, convergence rate and sparsity. The largest improvement can be observed for sparse signals with high amplitudes. version:1
arxiv-1411-2337 | Multi-Task Metric Learning on Network Data | http://arxiv.org/abs/1411.2337 | id:1411.2337 author:Chen Fang, Daniel N. Rockmore category:stat.ML cs.LG  published:2014-11-10 summary:Multi-task learning (MTL) improves prediction performance in different contexts by learning models jointly on multiple different, but related tasks. Network data, which are a priori data with a rich relational structure, provide an important context for applying MTL. In particular, the explicit relational structure implies that network data is not i.i.d. data. Network data also often comes with significant metadata (i.e., attributes) associated with each entity (node). Moreover, due to the diversity and variation in network data (e.g., multi-relational links or multi-category entities), various tasks can be performed and often a rich correlation exists between them. Learning algorithms should exploit all of these additional sources of information for better performance. In this work we take a metric-learning point of view for the MTL problem in the network context. Our approach builds on structure preserving metric learning (SPML). In particular SPML learns a Mahalanobis distance metric for node attributes using network structure as supervision, so that the learned distance function encodes the structure and can be used to predict link patterns from attributes. SPML is described for single-task learning on single network. Herein, we propose a multi-task version of SPML, abbreviated as MT-SPML, which is able to learn across multiple related tasks on multiple networks via shared intermediate parametrization. MT-SPML learns a specific metric for each task and a common metric for all tasks. The task correlation is carried through the common metric and the individual metrics encode task specific information. When combined together, they are structure-preserving with respect to individual tasks. MT-SPML works on general networks, thus is suitable for a wide variety of problems. In experiments, we challenge MT-SPML on two real-word problems, where MT-SPML achieves significant improvement. version:1
arxiv-1411-2335 | An Improved Tracking using IMU and Vision Fusion for Mobile Augmented Reality Applications | http://arxiv.org/abs/1411.2335 | id:1411.2335 author:Kriti Kumar, Ashley Varghese, Pavan K Reddy, N Narendra, Prashanth Swamy, M Girish Chandra, P Balamuralidhar category:cs.CV  published:2014-11-10 summary:Mobile Augmented Reality (MAR) is becoming an important cyber-physical system application given the ubiquitous availability of mobile phones. With the need to operate in unprepared environments, accurate and robust registration and tracking has become an important research problem to solve. In fact, when MAR is used for tele-interactive applications involving large distances, say from an accident site to insurance office, tracking at both the ends is desirable and further it is essential to appropriately fuse inertial and vision sensors data. In this paper, we present results and discuss some insights gained in marker-less tracking during the development of a prototype pertaining to an example use case related to breakdown or damage assessment of a vehicle. The novelty of this paper is in bringing together different components and modules with appropriate enhancements towards a complete working system. version:1
arxiv-1411-1147 | Conditional Random Field Autoencoders for Unsupervised Structured Prediction | http://arxiv.org/abs/1411.1147 | id:1411.1147 author:Waleed Ammar, Chris Dyer, Noah A. Smith category:cs.LG cs.CL  published:2014-11-05 summary:We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observable data using a feature-rich conditional random field. Then a reconstruction of the input is (re)generated, conditional on the latent structure, using models for which maximum likelihood estimation has a closed-form. Our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. We show competitive results with instantiations of the model for two canonical NLP tasks: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines. version:2
arxiv-1411-2331 | N$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for Large and High-dimensional Data | http://arxiv.org/abs/1411.2331 | id:1411.2331 author:Makoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, Yi Chang category:stat.ML cs.LG  published:2014-11-10 summary:We propose a feature selection method that finds non-redundant features from a large and high-dimensional data in nonlinear way. Specifically, we propose a nonlinear extension of the non-negative least-angle regression (LARS) called N${}^3$LARS, where the similarity between input and output is measured through the normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). An advantage of N${}^3$LARS is that it can easily incorporate with map-reduce frameworks such as Hadoop and Spark. Thus, with the help of distributed computing, a set of features can be efficiently selected from a large and high-dimensional data. Moreover, N${}^3$LARS is a convex method and can find a global optimum solution. The effectiveness of the proposed method is first demonstrated through feature selection experiments for classification and regression with small and high-dimensional datasets. Finally, we evaluate our proposed method over a large and high-dimensional biology dataset. version:1
arxiv-1411-2328 | Modeling Word Relatedness in Latent Dirichlet Allocation | http://arxiv.org/abs/1411.2328 | id:1411.2328 author:Xun Wang category:cs.CL cs.AI  published:2014-11-10 summary:Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA. version:1
arxiv-1411-2305 | Model-Parallel Inference for Big Topic Models | http://arxiv.org/abs/1411.2305 | id:1411.2305 author:Xun Zheng, Jin Kyu Kim, Qirong Ho, Eric P. Xing category:cs.DC cs.LG stat.ML  published:2014-11-10 summary:In real world industrial applications of topic modeling, the ability to capture gigantic conceptual space by learning an ultra-high dimensional topical representation, i.e., the so-called "big model", is becoming the next desideratum after enthusiasms on "big data", especially for fine-grained downstream tasks such as online advertising, where good performances are usually achieved by regression-based predictors built on millions if not billions of input features. The conventional data-parallel approach for training gigantic topic models turns out to be rather inefficient in utilizing the power of parallelism, due to the heavy dependency on a centralized image of "model". Big model size also poses another challenge on the storage, where available model size is bounded by the smallest RAM of nodes. To address these issues, we explore another type of parallelism, namely model-parallelism, which enables training of disjoint blocks of a big topic model in parallel. By integrating data-parallelism with model-parallelism, we show that dependencies between distributed elements can be handled seamlessly, achieving not only faster convergence but also an ability to tackle significantly bigger model size. We describe an architecture for model-parallel inference of LDA, and present a variant of collapsed Gibbs sampling algorithm tailored for it. Experimental results demonstrate the ability of this system to handle topic modeling with unprecedented amount of 200 billion model variables only on a low-end cluster with very limited computational resources and bandwidth. version:1
arxiv-1411-2276 | Trade-Offs in Exploiting Body Morphology for Control: from Simple Bodies and Model-Based Control to Complex Bodies with Model-Free Distributed Control Schemes | http://arxiv.org/abs/1411.2276 | id:1411.2276 author:Matej Hoffmann, Vincent C. M√ºller category:cs.RO cs.NE cs.SY  published:2014-11-09 summary:Tailoring the design of robot bodies for control purposes is implicitly performed by engineers, however, a methodology or set of tools is largely absent and optimization of morphology (shape, material properties of robot bodies, etc.) is lagging behind the development of controllers. This has become even more prominent with the advent of compliant, deformable or "soft" bodies. These carry substantial potential regarding their exploitation for control---sometimes referred to as "morphological computation" in the sense of offloading computation needed for control to the body. Here, we will argue in favor of a dynamical systems rather than computational perspective on the problem. Then, we will look at the pros and cons of simple vs. complex bodies, critically reviewing the attractive notion of "soft" bodies automatically taking over control tasks. We will address another key dimension of the design space---whether model-based control should be used and to what extent it is feasible to develop faithful models for different morphologies. version:1
arxiv-1411-2584 | Applications of sampling Kantorovich operators to thermographic images for seismic engineering | http://arxiv.org/abs/1411.2584 | id:1411.2584 author:Danilo Costarelli, Federico Cluni, Anna Maria Minotti, Gianluca Vinti category:cs.CV math.NA  published:2014-11-09 summary:In this paper, we present some applications of the multivariate sampling Kantorovich operators $S_w$ to seismic engineering. The mathematical theory of these operators, both in the space of continuous functions and in Orlicz spaces, show how it is possible to approximate/reconstruct multivariate signals, such as images. In particular, to obtain applications for thermographic images a mathematical algorithm is developed using MATLAB and matrix calculus. The setting of Orlicz spaces is important since allow us to reconstruct not necessarily continuous signals by means of $S_w$. The reconstruction of thermographic images of buildings by our sampling Kantorovich algorithm allow us to obtain models for the simulation of the behavior of structures under seismic action. We analyze a real world case study in term of structural analysis and we compare the behavior of the building under seismic action using various models. version:1
arxiv-1411-2214 | Abnormal Object Recognition: A Comprehensive Study | http://arxiv.org/abs/1411.2214 | id:1411.2214 author:Babak Saleh, Ali Farhadi, Ahmed Elgammal category:cs.CV  published:2014-11-09 summary:When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities. version:1
arxiv-1411-2173 | Stacked Quantizers for Compositional Vector Compression | http://arxiv.org/abs/1411.2173 | id:1411.2173 author:Julieta Martinez, Holger H. Hoos, James J. Little category:cs.CV  published:2014-11-08 summary:Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), a generalization of Product Quantization (PQ) where a non-independent set of codebooks is used to compress vectors into small binary codes. Unfortunately, under this scheme encoding cannot be done independently in each codebook, and optimal encoding is an NP-hard problem. In this paper, we observe that PQ and AQ are both compositional quantizers that lie on the extremes of the codebook dependence-independence assumption, and explore an intermediate approach that exploits a hierarchical structure in the codebooks. This results in a method that achieves quantization error on par with or lower than AQ, while being several orders of magnitude faster. We perform a complexity analysis of PQ, AQ and our method, and evaluate our approach on standard benchmarks of SIFT and GIST descriptors, as well as on new datasets of features obtained from state-of-the-art convolutional neural networks. version:1
arxiv-1411-2153 | Evolving intraday foreign exchange trading strategies utilizing multiple instruments price series | http://arxiv.org/abs/1411.2153 | id:1411.2153 author:Simone Cirillo, Stefan Lloyd, Peter Nordin category:cs.NE q-fin.TR I.2.2  published:2014-11-08 summary:We propose a Genetic Programming architecture for the generation of foreign exchange trading strategies. The system's principal features are the evolution of free-form strategies which do not rely on any prior models and the utilization of price series from multiple instruments as input data. This latter feature constitutes an innovation with respect to previous works documented in literature. In this article we utilize Open, High, Low, Close bar data at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPY currency pairs. We will test the implementation analyzing the in-sample and out-of-sample performance of strategies for trading the USD.JPY obtained across multiple algorithm runs. We will also evaluate the differences between strategies selected according to two different criteria: one relies on the fitness obtained on the training set only, the second one makes use of an additional validation dataset. Strategy activity and trade accuracy are remarkably stable between in and out of sample results. From a profitability aspect, the two criteria both result in strategies successful on out-of-sample data but exhibiting different characteristics. The overall best performing out-of-sample strategy achieves a yearly return of 19%. version:1
arxiv-1411-2141 | Fast Mesh-Based Medical Image Registration | http://arxiv.org/abs/1411.2141 | id:1411.2141 author:Ahmadreza Baghaie, Zeyun Yu, Roshan M. D'souza category:cs.CV  published:2014-11-08 summary:In this paper a fast triangular mesh based registration method is proposed. Having Template and Reference images as inputs, the template image is triangulated using a content adaptive mesh generation algorithm. Considering the pixel values at mesh nodes, interpolated using spline interpolation method for both of the images, the energy functional needed for image registration is minimized. The minimization process was achieved using a mesh based discretization of the distance measure and regularization term which resulted in a sparse system of linear equations, which due to the smaller size in comparison to the pixel-wise registration method, can be solved directly. Mean Squared Di?erence (MSD) is used as a metric for evaluating the results. Using the mesh based technique, higher speed was achieved compared to pixel-based curvature registration technique with fast DCT solver. The implementation was done in MATLAB without any speci?c optimization. Higher speeds can be achieved using C/C++ implementations. version:1
arxiv-1408-4551 | Dimensionality Reduction of Affine Variational Inequalities Using Random Projections | http://arxiv.org/abs/1408.4551 | id:1408.4551 author:Bharat Prabhakar, Ankur A. Kulkarni category:math.OC cs.LG cs.SY  published:2014-08-20 summary:We present a method for dimensionality reduction of an affine variational inequality (AVI) defined over a compact feasible region. Centered around the Johnson Lindenstrauss lemma, our method is a randomized algorithm that produces with high probability an approximate solution for the given AVI by solving a lower-dimensional AVI. The algorithm allows the lower dimension to be chosen based on the quality of approximation desired. The algorithm can also be used as a subroutine in an exact algorithm for generating an initial point close to the solution. The lower-dimensional AVI is obtained by appropriately projecting the original AVI on a randomly chosen subspace. The lower-dimensional AVI is solved using standard solvers and from this solution an approximate solution to the original AVI is recovered through an inexpensive process. Our numerical experiments corroborate the theoretical results and validate that the algorithm provides a good approximation at low dimensions and substantial savings in time for an exact solution. version:2
arxiv-1411-2090 | Parallax Effect Free Mosaicing of Underwater Video Sequence Based on Texture Features | http://arxiv.org/abs/1411.2090 | id:1411.2090 author:Nagaraja S., Prabhakar C. J., Praveen Kumar P. U category:cs.CV  published:2014-11-08 summary:In this paper, we present feature-based technique for construction of mosaic image from underwater video sequence, which suffers from parallax distortion due to propagation properties of light in the underwater environment. The most of the available mosaic tools and underwater image mosaicing techniques yields final result with some artifacts such as blurring, ghosting and seam due to presence of parallax in the input images. The removal of parallax from input images may not reduce its effects instead it must be corrected in successive steps of mosaicing. Thus, our approach minimizes the parallax effects by adopting an efficient local alignment technique after global registration. We extract texture features using Centre Symmetric Local Binary Pattern (CS-LBP) descriptor in order to find feature correspondences, which are used further for estimation of homography through RANSAC. In order to increase the accuracy of global registration, we perform preprocessing such as colour alignment between two selected frames based on colour distribution adjustment. Because of existence of 100% overlap in consecutive frames of underwater video, we select frames with minimum overlap based on mutual offset in order to reduce the computation cost during mosaicing. Our approach minimizes the parallax effects considerably in final mosaic constructed using our own underwater video sequences. version:1
arxiv-1405-1665 | On Communication Cost of Distributed Statistical Estimation and Dimensionality | http://arxiv.org/abs/1405.1665 | id:1405.1665 author:Ankit Garg, Tengyu Ma, Huy L. Nguyen category:cs.LG cs.IT math.IT  published:2014-05-07 summary:We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean $\vec{\theta}$ of an unknown $d$ dimensional gaussian distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among $m$ different machines. The goal is to estimate the mean $\vec{\theta}$ at the optimal minimax rate while communicating as few bits as possible. We show that in this setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting \cite{ZDJW13} and to our improved bounds for the simultaneous setting, we prove new lower bounds of $\Omega(md/\log(m))$ and $\Omega(md)$ for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. To complement, we also demonstrate an interactive protocol achieving the minimax squared loss with $O(md)$ bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is promised to be $s$-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a $d/s$ factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor. version:2
arxiv-1504-03315 | A Novel Approach to Develop a New Hybrid Technique for Trademark Image Retrieval | http://arxiv.org/abs/1504.03315 | id:1504.03315 author:Saurabh Agarwal, Punit Kumar Johari category:cs.CV  published:2014-11-08 summary:Trademark Image Retrieval is playing a vital role as a part of CBIR System. Trademark is of great significance because it carries the status value of any company. To retrieve such a fake or copied trademark we design a retrieval system which is based on hybrid techniques. It contains a mixture of two different feature vector which combined together to give a suitable retrieval system. In the proposed system we extract the corner feature which is applied on an edge pixel image. This feature is used to extract the relevant image and to more purify the result we apply other feature which is the invariant moment feature. From the experimental result we conclude that the system is 85 percent efficient. version:1
arxiv-1411-2057 | Online Collaborative-Filtering on Graphs | http://arxiv.org/abs/1411.2057 | id:1411.2057 author:Siddhartha Banerjee, Sujay Sanghavi, Sanjay Shakkottai category:cs.LG  published:2014-11-07 summary:A common phenomena in modern recommendation systems is the use of feedback from one user to infer the `value' of an item to other users. This results in an exploration vs. exploitation trade-off, in which items of possibly low value have to be presented to users in order to ascertain their value. Existing approaches to solving this problem focus on the case where the number of items are small, or admit some underlying structure -- it is unclear, however, if good recommendation is possible when dealing with content-rich settings with unstructured content. We consider this problem under a simple natural model, wherein the number of items and the number of item-views are of the same order, and an `access-graph' constrains which user is allowed to see which item. Our main insight is that the presence of the access-graph in fact makes good recommendation possible -- however this requires the exploration policy to be designed to take advantage of the access-graph. Our results demonstrate the importance of `serendipity' in exploration, and how higher graph-expansion translates to a higher quality of recommendations; it also suggests a reason why in some settings, simple policies like Twitter's `Latest-First' policy achieve a good performance. From a technical perspective, our model presents a way to study exploration-exploitation tradeoffs in settings where the number of `trials' and `strategies' are large (potentially infinite), and more importantly, of the same order. Our algorithms admit competitive-ratio guarantees which hold for the worst-case user, under both finite-population and infinite-horizon settings, and are parametrized in terms of properties of the underlying graph. Conversely, we also demonstrate that improperly-designed policies can be highly sub-optimal, and that in many settings, our results are order-wise optimal. version:1
arxiv-1411-2045 | Multivariate f-Divergence Estimation With Confidence | http://arxiv.org/abs/1411.2045 | id:1411.2045 author:Kevin R. Moon, Alfred O. Hero III category:cs.IT math.IT stat.ML  published:2014-11-07 summary:The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several nonparametric divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O(1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error. version:1
arxiv-1407-1543 | Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method | http://arxiv.org/abs/1407.1543 | id:1407.1543 author:Boaz Barak, Jonathan A. Kelner, David Steurer category:cs.DS cs.LG stat.ML F.2.1; F.2.2; I.2.6  published:2014-07-06 summary:We give a new approach to the dictionary learning (also known as "sparse coding") problem of recovering an unknown $n\times m$ matrix $A$ (for $m \geq n$) from examples of the form \[ y = Ax + e, \] where $x$ is a random vector in $\mathbb R^m$ with at most $\tau m$ nonzero coordinates, and $e$ is a random noise vector in $\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$, our algorithm recovers every column of $A$ within arbitrarily good constant accuracy in time $m^{O(\log m/\log(\tau^{-1}))}$, in particular achieving polynomial time if $\tau = m^{-\delta}$ for any $\delta>0$, and time $m^{O(\log m)}$ if $\tau$ is (a sufficiently small) constant. Prior algorithms with comparable assumptions on the distribution required the vector $x$ to be much sparser---at most $\sqrt{n}$ nonzero coordinates---and there were intrinsic barriers preventing these algorithms from applying for denser $x$. We achieve this by designing an algorithm for noisy tensor decomposition that can recover, under quite general conditions, an approximate rank-one decomposition of a tensor $T$, given access to a tensor $T'$ that is $\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our knowledge, this is the first algorithm for tensor decomposition that works in the constant spectral-norm noise regime, where there is no guarantee that the local optima of $T$ and $T'$ have similar structures. Our algorithm is based on a novel approach to using and analyzing the Sum of Squares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and it can be viewed as an indication of the utility of this very general and powerful tool for unsupervised learning problems. version:2
arxiv-1411-2005 | Scalable Variational Gaussian Process Classification | http://arxiv.org/abs/1411.2005 | id:1411.2005 author:James Hensman, Alex Matthews, Zoubin Ghahramani category:stat.ML  published:2014-11-07 summary:Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments. version:1
arxiv-1411-1999 | Azhary: An Arabic Lexical Ontology | http://arxiv.org/abs/1411.1999 | id:1411.1999 author:Hossam Ishkewy, Hany Harb, Hassan Farahat category:cs.AI cs.CL  published:2014-11-07 summary:Arabic language is the most spoken languages in the Semitic languages group, and one of the most common languages in the world spoken by more than 422 million. It is also of paramount importance to Muslims, it is a sacred language of the Islamic Holly Book (Quran) and prayer (and other acts of worship) in Islam is performed only by mastering some of Arabic words. Arabic is also a major ritual language of a number of Christian churches in the Arab world and it is also used in writing several intellectual and religious Jewish books in the Middle Ages. Despite this, there is no semantic Arabic lexicon which researchers can depend on. In this paper we introduce Azhary as a lexical ontology for the Arabic language. It groups Arabic words into sets of synonyms called synsets, and records a number of relationships between words such as synonym, antonym, hypernym, hyponym, meronym, holonym and association relations. The ontology contains 26,195 words organized in 13,328 synsets. It has been developed and contrasted against AWN which is the most common available Arabic lexical ontology. version:1
arxiv-1411-1997 | Differential gene co-expression networks via Bayesian biclustering models | http://arxiv.org/abs/1411.1997 | id:1411.1997 author:Chuan Gao, Shiwen Zhao, Ian C. McDowell, Christopher D. Brown, Barbara E. Engelhardt category:stat.ME q-bio.GN q-bio.MN stat.ML  published:2014-11-07 summary:Identifying latent structure in large data matrices is essential for exploring biological processes. Here, we consider recovering gene co-expression networks from gene expression data, where each network encodes relationships between genes that are locally co-regulated by shared biological mechanisms. To do this, we develop a Bayesian statistical model for biclustering to infer subsets of co-regulated genes whose covariation may be observed in only a subset of the samples. Our biclustering method, BicMix, has desirable properties, including allowing overcomplete representations of the data, computational tractability, and jointly modeling unknown confounders and biological signals. Compared with related biclustering methods, BicMix recovers latent structure with higher precision across diverse simulation scenarios. Further, we develop a method to recover gene co-expression networks from the estimated sparse biclustering matrices. We apply BicMix to breast cancer gene expression data and recover a gene co-expression network that is differential across ER+ and ER- samples. version:1
arxiv-1402-7015 | Data-driven HRF estimation for encoding and decoding models | http://arxiv.org/abs/1402.7015 | id:1402.7015 author:Fabian Pedregosa, Michael Eickenberg, Philippe Ciuciu, Bertrand Thirion, Alexandre Gramfort category:cs.CE cs.LG  published:2014-02-27 summary:Despite the common usage of a canonical, data-independent, hemodynamic response function (HRF), it is known that the shape of the HRF varies across brain regions and subjects. This suggests that a data-driven estimation of this function could lead to more statistical power when modeling BOLD fMRI data. However, unconstrained estimation of the HRF can yield highly unstable results when the number of free parameters is large. We develop a method for the joint estimation of activation and HRF using a rank constraint causing the estimated HRF to be equal across events/conditions, yet permitting it to be different across voxels. Model estimation leads to an optimization problem that we propose to solve with an efficient quasi-Newton method exploiting fast gradient computations. This model, called GLM with Rank-1 constraint (R1-GLM), can be extended to the setting of GLM with separate designs which has been shown to improve decoding accuracy in brain activity decoding experiments. We compare 10 different HRF modeling methods in terms of encoding and decoding score in two different datasets. Our results show that the R1-GLM model significantly outperforms competing methods in both encoding and decoding settings, positioning it as an attractive method both from the points of view of accuracy and computational efficiency. version:6
arxiv-1305-0751 | Marginal AMP Chain Graphs | http://arxiv.org/abs/1305.0751 | id:1305.0751 author:Jose M. Pe√±a category:stat.ML cs.AI  published:2013-05-03 summary:We present a new family of models that is based on graphs that may have undirected, directed and bidirected edges. We name these new models marginal AMP (MAMP) chain graphs because each of them is Markov equivalent to some AMP chain graph under marginalization of some of its nodes. However, MAMP chain graphs do not only subsume AMP chain graphs but also multivariate regression chain graphs. We describe global and pairwise Markov properties for MAMP chain graphs and prove their equivalence for compositional graphoids. We also characterize when two MAMP chain graphs are Markov equivalent. For Gaussian probability distributions, we also show that every MAMP chain graph is Markov equivalent to some directed and acyclic graph with deterministic nodes under marginalization and conditioning on some of its nodes. This is important because it implies that the independence model represented by a MAMP chain graph can be accounted for by some data generating process that is partially observed and has selection bias. Finally, we modify MAMP chain graphs so that they are closed under marginalization for Gaussian probability distributions. This is a desirable feature because it guarantees parsimonious models under marginalization. version:6
arxiv-1411-1537 | Large-Margin Determinantal Point Processes | http://arxiv.org/abs/1411.1537 | id:1411.1537 author:Boqing Gong, Wei-lun Chao, Kristen Grauman, Fei Sha category:stat.ML cs.CV cs.LG  published:2014-11-06 summary:Determinantal point processes (DPPs) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. We study the problem of learning the parameters (the kernel matrix) of a DPP from labeled training data. We make two contributions. First, we show how to reparameterize a DPP's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. Second, we propose a novel parameter estimation technique based on the principle of large margin separation. In contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). Extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable. version:2
arxiv-1402-3902 | Sparse Polynomial Learning and Graph Sketching | http://arxiv.org/abs/1402.3902 | id:1402.3902 author:Murat Kocaoglu, Karthikeyan Shanmugam, Alexandros G. Dimakis, Adam Klivans category:cs.LG  published:2014-02-17 summary:Let $f:\{-1,1\}^n$ be a polynomial with at most $s$ non-zero real coefficients. We give an algorithm for exactly reconstructing f given random examples from the uniform distribution on $\{-1,1\}^n$ that runs in time polynomial in $n$ and $2s$ and succeeds if the function satisfies the unique sign property: there is one output value which corresponds to a unique set of values of the participating parities. This sufficient condition is satisfied when every coefficient of f is perturbed by a small random noise, or satisfied with high probability when s parity functions are chosen randomly or when all the coefficients are positive. Learning sparse polynomials over the Boolean domain in time polynomial in $n$ and $2s$ is considered notoriously hard in the worst-case. Our result shows that the problem is tractable for almost all sparse polynomials. Then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts. We also provide experimental results on a real world dataset. version:4
arxiv-1411-1792 | How transferable are features in deep neural networks? | http://arxiv.org/abs/1411.1792 | id:1411.1792 author:Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson category:cs.LG cs.NE  published:2014-11-06 summary:Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset. version:1
arxiv-1404-7174 | Computer vision-based recognition of liquid surfaces and phase boundaries in transparent vessels, with emphasis on chemistry applications | http://arxiv.org/abs/1404.7174 | id:1404.7174 author:Sagi Eppel, Tal Kachman category:cs.CV  published:2014-04-28 summary:The ability to recognize the liquid surface and the liquid level in transparent containers is perhaps the most commonly used evaluation method when dealing with fluids. Such recognition is essential in determining the liquid volume, fill level, phase boundaries and phase separation in various fluid systems. The recognition of liquid surfaces is particularly important in solution chemistry, where it is essential to many laboratory techniques (e.g., extraction, distillation, titration). A general method for the recognition of interfaces between liquid and air or between phase-separating liquids could have a wide range of applications and contribute to the understanding of the visual properties of such interfaces. This work examines a computer vision method for the recognition of liquid surfaces and liquid levels in various transparent containers. The method can be applied to recognition of both liquid-air and liquid-liquid surfaces. No prior knowledge of the number of phases is required. The method receives the image of the liquid container and the boundaries of the container in the image and scans all possible curves that could correspond to the outlines of liquid surfaces in the image. The method then compares each curve to the image to rate its correspondence with the outline of the real liquid surface by examining various image properties in the area surrounding each point of the curve. The image properties that were found to give the best indication of the liquid surface are the relative intensity change, the edge density change and the gradient direction relative to the curve normal. version:7
arxiv-1411-1784 | Conditional Generative Adversarial Nets | http://arxiv.org/abs/1411.1784 | id:1411.1784 author:Mehdi Mirza, Simon Osindero category:cs.LG cs.AI cs.CV stat.ML  published:2014-11-06 summary:Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels. version:1
arxiv-1411-1752 | Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets | http://arxiv.org/abs/1411.1752 | id:1411.1752 author:Adarsh Prasad, Stefanie Jegelka, Dhruv Batra category:cs.LG cs.AI cs.CV cs.IR stat.ML  published:2014-11-06 summary:To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals. version:1
arxiv-1411-1670 | Stochastic Variational Inference for Hidden Markov Models | http://arxiv.org/abs/1411.1670 | id:1411.1670 author:Nicholas J. Foti, Jason Xu, Dillon Laird, Emily B. Fox category:stat.ML  published:2014-11-06 summary:Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible. version:1
arxiv-1406-2035 | Learning Word Representations with Hierarchical Sparse Coding | http://arxiv.org/abs/1406.2035 | id:1406.2035 author:Dani Yogatama, Manaal Faruqui, Chris Dyer, Noah A. Smith category:cs.CL cs.LG stat.ML  published:2014-06-08 summary:We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks---word similarity ranking, analogies, sentence completion, and sentiment analysis---demonstrate that the method outperforms or is competitive with state-of-the-art methods. Our word representations are available at \url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}. version:2
arxiv-1411-1623 | A Hybrid Recurrent Neural Network For Music Transcription | http://arxiv.org/abs/1411.1623 | id:1411.1623 author:Siddharth Sigtia, Emmanouil Benetos, Nicolas Boulanger-Lewandowski, Tillman Weyde, Artur S. d'Avila Garcez, Simon Dixon category:cs.LG  published:2014-11-06 summary:We investigate the problem of incorporating higher-level symbolic score-like information into Automatic Music Transcription (AMT) systems to improve their performance. We use recurrent neural networks (RNNs) and their variants as music language models (MLMs) and present a generative architecture for combining these models with predictions from a frame level acoustic classifier. We also compare different neural network architectures for acoustic modeling. The proposed model computes a distribution over possible output sequences given the acoustic input signal and we present an algorithm for performing a global search for good candidate transcriptions. The performance of the proposed model is evaluated on piano music from the MAPS dataset and we observe that the proposed model consistently outperforms existing transcription methods. version:1
arxiv-1411-1316 | Rapid Skill Capture in a First-Person Shooter | http://arxiv.org/abs/1411.1316 | id:1411.1316 author:David Buckley, Ke Chen, Joshua Knowles category:cs.HC cs.LG  published:2014-11-05 summary:Various aspects of computer game design, including adaptive elements of game levels, characteristics of 'bot' behavior, and player matching in multiplayer games, would ideally be sensitive to a player's skill level. Yet, while difficulty and player learning have been explored in the context of games, there has been little work analyzing skill per se, and how it pertains to a player's input. To this end, we present a data set of 476 game logs from over 40 players of a first-person shooter game (Red Eclipse) as a basis of a case study. We then analyze different metrics of skill and show that some of these can be predicted using only a few seconds of keyboard and mouse input. We argue that the techniques used here are useful for adapting games to match players' skill levels rapidly, perhaps more rapidly than solutions based on performance averaging such as TrueSkill. version:2
arxiv-1411-1557 | Proof Supplement - Learning Sparse Causal Models is not NP-hard (UAI2013) | http://arxiv.org/abs/1411.1557 | id:1411.1557 author:Tom Claassen, Joris M. Mooij, Tom Heskes category:stat.ML  published:2014-11-06 summary:This article contains detailed proofs and additional examples related to the UAI-2013 submission `Learning Sparse Causal Models is not NP-hard'. It describes the FCI+ algorithm: a method for sound and complete causal model discovery in the presence of latent confounders and/or selection bias, that has worst case polynomial complexity of order $N^{2(k+1)}$ in the number of independence tests, for sparse graphs over $N$ nodes, bounded by node degree $k$. The algorithm is an adaptation of the well-known FCI algorithm by (Spirtes et al., 2000) that is also sound and complete, but has worst case complexity exponential in $N$. version:1
arxiv-1411-1509 | Convolutional Neural Network-based Place Recognition | http://arxiv.org/abs/1411.1509 | id:1411.1509 author:Zetao Chen, Obadiah Lam, Adam Jacobson, Michael Milford category:cs.CV cs.LG cs.NE  published:2014-11-06 summary:Recently Convolutional Neural Networks (CNNs) have been shown to achieve state-of-the-art performance on various classification tasks. In this paper, we present for the first time a place recognition technique based on CNN models, by combining the powerful features learnt by CNNs with a spatial and sequential filter. Applying the system to a 70 km benchmark place recognition dataset we achieve a 75% increase in recall at 100% precision, significantly outperforming all previous state of the art techniques. We also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition, both for the benchmark dataset and for a second dataset with more significant viewpoint changes. version:1
arxiv-1406-1584 | Learning to Discover Efficient Mathematical Identities | http://arxiv.org/abs/1406.1584 | id:1406.1584 author:Wojciech Zaremba, Karol Kurach, Rob Fergus category:cs.LG  published:2014-06-06 summary:In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a set of grammar rules we build trees that combine different rules, looking for branches which yield compositions that are analytically equivalent to a target expression, but of lower computational complexity. However, as the size of the trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation. version:3
arxiv-1411-1469 | A Generic Sample Splitting Approach for Refined Community Recovery in Stochastic Block Models | http://arxiv.org/abs/1411.1469 | id:1411.1469 author:Jing Lei, Lingxue Zhu category:stat.ML math.ST stat.TH  published:2014-11-06 summary:We propose and analyze a generic method for community recovery in stochastic block models and degree corrected block models. This approach can exactly recover the hidden communities with high probability when the expected node degrees are of order $\log n$ or higher. Starting from a roughly correct community partition given by some conventional community recovery algorithm, this method refines the partition in a cross clustering step. Our results simplify and extend some of the previous work on exact community recovery, discovering the key role played by sample splitting. The proposed method is simple and can be implemented with many practical community recovery algorithms. version:1
arxiv-1411-1446 | Electrocardiography Separation of Mother and Baby | http://arxiv.org/abs/1411.1446 | id:1411.1446 author:Wei Wang category:cs.CV cs.LG  published:2014-11-05 summary:Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is a challenging task, because one single device is used and it receives a mixture of multiple heart beats. In this paper, we would like to design a filter to separate the signals from each other. version:1
arxiv-1411-1442 | Optical Character Recognition, Using K-Nearest Neighbors | http://arxiv.org/abs/1411.1442 | id:1411.1442 author:Wei Wang category:cs.CV  published:2014-11-05 summary:The problem of optical character recognition, OCR, has been widely discussed in the literature. Having a hand-written text, the program aims at recognizing the text. Even though there are several approaches to this issue, it is still an open problem. In this paper we would like to propose an approach that uses K-nearest neighbors algorithm, and has the accuracy of more than 90%. The training and run time is also very short. version:1
arxiv-1312-7606 | Distributed Policy Evaluation Under Multiple Behavior Strategies | http://arxiv.org/abs/1312.7606 | id:1312.7606 author:Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, Ali H. Sayed category:cs.MA cs.AI cs.DC cs.LG  published:2013-12-30 summary:We apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space). version:2
arxiv-1411-1372 | Online SLAM with Any-time Self-calibration and Automatic Change Detection | http://arxiv.org/abs/1411.1372 | id:1411.1372 author:Nima Keivan, Gabe Sibley category:cs.CV cs.RO  published:2014-11-05 summary:A framework for online simultaneous localization, mapping and self-calibration is presented which can detect and handle significant change in the calibration parameters. Estimates are computed in constant-time by factoring the problem and focusing on segments of the trajectory that are most informative for the purposes of calibration. A novel technique is presented to detect the probability that a significant change is present in the calibration parameters. The system is then able to re-calibrate. Maximum likelihood trajectory and map estimates are computed using an asynchronous and adaptive optimization. The system requires no prior information and is able to initialize without any special motions or routines, or in the case where observability over calibration parameters is delayed. The system is experimentally validated to calibrate camera intrinsic parameters for a nonlinear camera model on a monocular dataset featuring a significant zoom event partway through, and achieves high accuracy despite unknown initial calibration parameters. Self-calibration and re-calibration parameters are shown to closely match estimates computed using a calibration target. The accuracy of the system is demonstrated with SLAM results that achieve sub-1% distance-travel error even in the presence of significant re-calibration events. version:1
arxiv-1411-4297 | Application of Multi-core Parallel Programming to a Combination of Ant Colony Optimization and Genetic Algorithm | http://arxiv.org/abs/1411.4297 | id:1411.4297 author:Rishita Kalyani category:cs.NE  published:2014-11-05 summary:This Paper will deal with a combination of Ant Colony and Genetic Programming Algorithm to optimize Travelling Salesmen problem (NP-Hard). However, the complexity of the algorithm requires considerable computational time and resources. Parallel implementation can reduce the computational time. In this paper, emphasis in the parallelizing section is given to Multi-core architecture and Multi-Processor Systems which is developed and used almost everywhere today and hence, multi-core parallelization to the combination of algorithm is achieved by OpenMP library by Intel Corporation. version:1
arxiv-1411-1297 | Edge Detection based on Kernel Density Estimation | http://arxiv.org/abs/1411.1297 | id:1411.1297 author:Osvaldo Pereira, Esley Torre, Yasel Garc√©s, Roberto Rodr√≠guez category:cs.CV  published:2014-11-05 summary:Edges of an image are considered a crucial type of information. These can be extracted by applying edge detectors with different methodology. Edge detection is a vital step in computer vision tasks, because it is an essential issue for pattern recognition and visual interpretation. In this paper, we propose a new method for edge detection in images, based on the estimation by kernel of the probability density function. In our algorithm, pixels in the image with minimum value of density function are labeled as edges. The boundary between two homogeneous regions is defined in two domains: the spatial/lattice domain and the range/color domain. Extensive experimental evaluations proved that our edge detection method is significantly a competitive algorithm. version:1
arxiv-1411-1285 | Controlling false discoveries in high-dimensional situations: Boosting with stability selection | http://arxiv.org/abs/1411.1285 | id:1411.1285 author:Benjamin Hofner, Luigi Boccuto, Markus G√∂ker category:stat.ML stat.AP stat.CO  published:2014-11-05 summary:Modern biotechnologies often result in high-dimensional data sets with much more variables than observations (n $\ll$ p). These data sets pose new challenges to statistical analysis: Variable selection becomes one of the most important tasks in this setting. We assess the recently proposed flexible framework for variable selection called stability selection. By the use of resampling procedures, stability selection adds a finite sample error control to high-dimensional variable selection procedures such as Lasso or boosting. We consider the combination of boosting and stability selection and present results from a detailed simulation study that provides insights into the usefulness of this combination. Limitations are discussed and guidance on the specification and tuning of stability selection is given. The interpretation of the used error bounds is elaborated and insights for practical data analysis are given. The results will be used to detect differentially expressed phenotype measurements in patients with autism spectrum disorders. All methods are implemented in the freely available R package stabs. version:1
arxiv-1411-1243 | Using Twitter to predict football outcomes | http://arxiv.org/abs/1411.1243 | id:1411.1243 author:Stylianos Kampakis, Andreas Adamides category:stat.ML cs.CL cs.SI I.2.m  published:2014-11-05 summary:Twitter has been proven to be a notable source for predictive modelling on various domains such as the stock market, the dissemination of diseases or sports outcomes. However, such a study has not been conducted in football (soccer) so far. The purpose of this research was to study whether data mined from Twitter can be used for this purpose. We built a set of predictive models for the outcome of football games of the English Premier League for a 3 month period based on tweets and we studied whether these models can overcome predictive models which use only historical data and simple football statistics. Moreover, combined models are constructed using both Twitter and historical data. The final results indicate that data mined from Twitter can indeed be a useful source for predicting games in the Premier League. The final Twitter-based model performs significantly better than chance when measured by Cohen's kappa and is comparable to the model that uses simple statistics and historical data. Combining both models raises the performance higher than it was achieved by each individual model. Thereby, this study provides evidence that Twitter derived features can indeed provide useful information for the prediction of football (soccer) outcomes. version:1
arxiv-1408-0173 | Variational Depth from Focus Reconstruction | http://arxiv.org/abs/1408.0173 | id:1408.0173 author:Michael Moeller, Martin Benning, Carola Sch√∂nlieb, Daniel Cremers category:cs.CV math.OC  published:2014-08-01 summary:This paper deals with the problem of reconstructing a depth map from a sequence of differently focused images, also known as depth from focus or shape from focus. We propose to state the depth from focus problem as a variational problem including a smooth but nonconvex data fidelity term, and a convex nonsmooth regularization, which makes the method robust to noise and leads to more realistic depth maps. Additionally, we propose to solve the nonconvex minimization problem with a linearized alternating directions method of multipliers (ADMM), allowing to minimize the energy very efficiently. A numerical comparison to classical methods on simulated as well as on real data is presented. version:2
arxiv-1410-4485 | A Gesture Recognition System for Detecting Behavioral Patterns of ADHD | http://arxiv.org/abs/1410.4485 | id:1410.4485 author:Miguel √Ångel Bautista, Antonio Hern√°ndez-Vela, Sergio Escalera, Laura Igual, Oriol Pujol, Josep Moya, Ver√≥nica Violant, Mar√≠a Teresa Anguera category:cs.CV  published:2014-10-16 summary:We present an application of gesture recognition using an extension of Dynamic Time Warping (DTW) to recognize behavioural patterns of Attention Deficit Hyperactivity Disorder (ADHD). We propose an extension of DTW using one-class classifiers in order to be able to encode the variability of a gesture category, and thus, perform an alignment between a gesture sample and a gesture class. We model the set of gesture samples of a certain gesture category using either GMMs or an approximation of Convex Hulls. Thus, we add a theoretical contribution to classical warping path in DTW by including local modeling of intra-class gesture variability. This methodology is applied in a clinical context, detecting a group of ADHD behavioural patterns defined by experts in psychology/psychiatry, to provide support to clinicians in the diagnose procedure. The proposed methodology is tested on a novel multi-modal dataset (RGB plus Depth) of ADHD children recordings with behavioural patterns. We obtain satisfying results when compared to standard state-of-the-art approaches in the DTW context. version:2
arxiv-1405-3531 | Return of the Devil in the Details: Delving Deep into Convolutional Nets | http://arxiv.org/abs/1405.3531 | id:1405.3531 author:Ken Chatfield, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV  published:2014-05-14 summary:The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available. version:4
arxiv-1411-1172 | Tensor object classification via multilinear discriminant analysis network | http://arxiv.org/abs/1411.1172 | id:1411.1172 author:Rui Zeng, Jiasong Wu, Lotfi Senhadji, Huazhong Shu category:cs.CV  published:2014-11-05 summary:This paper proposes a multilinear discriminant analysis network (MLDANet) for the recognition of multidimensional objects, known as tensor objects. The MLDANet is a variation of linear discriminant analysis network (LDANet) and principal component analysis network (PCANet), both of which are the recently proposed deep learning algorithms. The MLDANet consists of three parts: 1) The encoder learned by MLDA from tensor data. 2) Features maps ob-tained from decoder. 3) The use of binary hashing and histogram for feature pooling. A learning algorithm for MLDANet is described. Evaluations on UCF11 database indicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA, and MLDA in terms of classification for tensor objects. version:1
arxiv-1411-1171 | Multilinear Principal Component Analysis Network for Tensor Object Classification | http://arxiv.org/abs/1411.1171 | id:1411.1171 author:Rui Zeng, Jiasong Wu, Zhuhong Shao, Lotfi Senhadji, Huazhong Shu category:cs.CV  published:2014-11-05 summary:The recently proposed principal component analysis network (PCANet) has been proved high performance for visual content classification. In this letter, we develop a tensorial extension of PCANet, namely, multilinear principal analysis component network (MPCANet), for tensor object classification. Compared to PCANet, the proposed MPCANet uses the spatial structure and the relationship between each dimension of tensor objects much more efficiently. Experiments were conducted on different visual content datasets including UCF sports action video sequences database and UCF11 database. The experimental results have revealed that the proposed MPCANet achieves higher classification accuracy than PCANet for tensor object classification. version:1
arxiv-1406-6474 | On the Convergence Rate of Decomposable Submodular Function Minimization | http://arxiv.org/abs/1406.6474 | id:1406.6474 author:Robert Nishihara, Stefanie Jegelka, Michael I. Jordan category:math.OC cs.DM cs.DS cs.LG cs.NA  published:2014-06-25 summary:Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of "simple" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory. version:3
arxiv-1411-1006 | A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages | http://arxiv.org/abs/1411.1006 | id:1411.1006 author:Javid Dadashkarimi, Azadeh Shakery, Heshaam Faili category:cs.IR cs.CL  published:2014-11-04 summary:Translation ambiguity, out of vocabulary words and missing some translations in bilingual dictionaries make dictionary-based Cross-language Information Retrieval (CLIR) a challenging task. Moreover, in agglutinative languages which do not have reliable stemmers, missing various lexical formations in bilingual dictionaries degrades CLIR performance. This paper aims to introduce a probabilistic translation model to solve the ambiguity problem, and also to provide most likely formations of a dictionary candidate. We propose Minimum Edit Support Candidates (MESC) method that exploits a monolingual corpus and a bilingual dictionary to translate users' native language queries to documents' language. Our experiments show that the proposed method outperforms state-of-the-art dictionary-based English-Persian CLIR. version:2
arxiv-1307-3102 | Statistical Active Learning Algorithms for Noise Tolerance and Differential Privacy | http://arxiv.org/abs/1307.3102 | id:1307.3102 author:Maria Florina Balcan, Vitaly Feldman category:cs.LG cs.DS stat.ML  published:2013-07-11 summary:We describe a framework for designing efficient active learning algorithms that are tolerant to random classification noise and are differentially-private. The framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of filtered random examples. It builds on the powerful statistical query framework of Kearns (1993). We show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of "uncorrelated" noise. The complexity of the resulting algorithms has information-theoretically optimal quadratic dependence on $1/(1-2\eta)$, where $\eta$ is the noise rate. We show that commonly studied concept classes including thresholds, rectangles, and linear separators can be efficiently actively learned in our framework. These results combined with our generic conversion lead to the first computationally-efficient algorithms for actively learning some of these concept classes in the presence of random classification noise that provide exponential improvement in the dependence on the error $\epsilon$ over their passive counterparts. In addition, we show that our algorithms can be automatically converted to efficient active differentially-private algorithms. This leads to the first differentially-private active learning algorithms with exponential label savings over the passive case. version:4
arxiv-1411-1158 | On the Complexity of Learning with Kernels | http://arxiv.org/abs/1411.1158 | id:1411.1158 author:Nicol√≤ Cesa-Bianchi, Yishay Mansour, Ohad Shamir category:cs.LG stat.ML  published:2014-11-05 summary:A well-recognized limitation of kernel learning is the requirement to handle a kernel matrix, whose size is quadratic in the number of training examples. Many methods have been proposed to reduce this computational cost, mostly by using a subset of the kernel matrix entries, or some form of low-rank matrix approximation, or a random projection method. In this paper, we study lower bounds on the error attainable by such methods as a function of the number of entries observed in the kernel matrix or the rank of an approximate kernel matrix. We show that there are kernel learning problems where no such method will lead to non-trivial computational savings. Our results also quantify how the problem difficulty depends on parameters such as the nature of the loss function, the regularization parameter, the norm of the desired predictor, and the kernel matrix rank. Our results also suggest cases where more efficient kernel learning might be possible. version:1
arxiv-1411-0894 | Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions | http://arxiv.org/abs/1411.0894 | id:1411.0894 author:S√©bastien Gadat, Thierry Klein, Cl√©ment Marteau category:math.ST stat.ML stat.TH  published:2014-11-04 summary:Given an $n$-sample of random vectors $(X_i,Y_i)_{1 \leq i \leq n}$ whose joint law is unknown, the long-standing problem of supervised classification aims to \textit{optimally} predict the label $Y$ of a given a new observation $X$. In this context, the nearest neighbor rule is a popular flexible and intuitive method in non-parametric situations. Even if this algorithm is commonly used in the machine learning and statistics communities, less is known about its prediction ability in general finite dimensional spaces, especially when the support of the density of the observations is $\mathbb{R}^d$. This paper is devoted to the study of the statistical properties of the nearest neighbor rule in various situations. In particular, attention is paid to the marginal law of $X$, as well as the smoothness and margin properties of the \textit{regression function} $\eta(X) = \mathbb{E}[Y X]$. We identify two necessary and sufficient conditions to obtain uniform consistency rates of classification and to derive sharp estimates in the case of the nearest neighbor rule. Some numerical experiments are proposed at the end of the paper to help illustrate the discussion. version:2
arxiv-1408-0853 | Adaptive Learning in Cartesian Product of Reproducing Kernel Hilbert Spaces | http://arxiv.org/abs/1408.0853 | id:1408.0853 author:Masahiro Yukawa category:cs.LG stat.ML  published:2014-08-05 summary:We propose a novel adaptive learning algorithm based on iterative orthogonal projections in the Cartesian product of multiple reproducing kernel Hilbert spaces (RKHSs). The task is estimating/tracking nonlinear functions which are supposed to contain multiple components such as (i) linear and nonlinear components, (ii) high- and low- frequency components etc. In this case, the use of multiple RKHSs permits a compact representation of multicomponent functions. The proposed algorithm is where two different methods of the author meet: multikernel adaptive filtering and the algorithm of hyperplane projection along affine subspace (HYPASS). In a certain particular case, the sum space of the RKHSs is isomorphic to the product space and hence the proposed algorithm can also be regarded as an iterative projection method in the sum space. The efficacy of the proposed algorithm is shown by numerical examples. version:2
arxiv-1404-7584 | High-Speed Tracking with Kernelized Correlation Filters | http://arxiv.org/abs/1404.7584 | id:1404.7584 author:Jo√£o F. Henriques, Rui Caseiro, Pedro Martins, Jorge Batista category:cs.CV  published:2014-04-30 summary:The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies -- any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the Discrete Fourier Transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new Kernelized Correlation Filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call Dual Correlation Filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source. version:3
arxiv-1411-1125 | Distributed Low-Rank Estimation Based on Joint Iterative Optimization in Wireless Sensor Networks | http://arxiv.org/abs/1411.1125 | id:1411.1125 author:S. Xu, R. C. de Lamare, H. V. Poor category:cs.IT cs.LG math.IT  published:2014-11-05 summary:This paper proposes a novel distributed reduced--rank scheme and an adaptive algorithm for distributed estimation in wireless sensor networks. The proposed distributed scheme is based on a transformation that performs dimensionality reduction at each agent of the network followed by a reduced-dimension parameter vector. A distributed reduced-rank joint iterative estimation algorithm is developed, which has the ability to achieve significantly reduced communication overhead and improved performance when compared with existing techniques. Simulation results illustrate the advantages of the proposed strategy in terms of convergence rate and mean square error performance. version:1
arxiv-1411-1091 | Do Convnets Learn Correspondence? | http://arxiv.org/abs/1411.1091 | id:1411.1091 author:Jonathan Long, Ning Zhang, Trevor Darrell category:cs.CV cs.LG cs.NE  published:2014-11-04 summary:Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass alignment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011. version:1
arxiv-1411-1088 | Expectation-Maximization for Learning Determinantal Point Processes | http://arxiv.org/abs/1411.1088 | id:1411.1088 author:Jennifer Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar category:stat.ML cs.LG  published:2014-11-04 summary:A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix. version:1
arxiv-1411-1087 | Fast Exact Matrix Completion with Finite Samples | http://arxiv.org/abs/1411.1087 | id:1411.1087 author:Prateek Jain, Praneeth Netrapalli category:cs.NA cs.DS cs.IT cs.LG math.IT stat.ML  published:2014-11-04 summary:Matrix completion is the problem of recovering a low rank matrix by observing a small fraction of its entries. A series of recent works [KOM12,JNS13,HW14] have proposed fast non-convex optimization based iterative algorithms to solve this problem. However, the sample complexity in all these results is sub-optimal in its dependence on the rank, condition number and the desired accuracy. In this paper, we present a fast iterative algorithm that solves the matrix completion problem by observing $O(nr^5 \log^3 n)$ entries, which is independent of the condition number and the desired accuracy. The run time of our algorithm is $O(nr^7\log^3 n\log 1/\epsilon)$ which is near linear in the dimension of the matrix. To the best of our knowledge, this is the first near linear time algorithm for exact matrix completion with finite sample complexity (i.e. independent of $\epsilon$). Our algorithm is based on a well known projected gradient descent method, where the projection is onto the (non-convex) set of low rank matrices. There are two key ideas in our result: 1) our argument is based on a $\ell_{\infty}$ norm potential function (as opposed to the spectral norm) and provides a novel way to obtain perturbation bounds for it. 2) we prove and use a natural extension of the Davis-Kahan theorem to obtain perturbation bounds on the best low rank approximation of matrices with good eigen-gap. Both of these ideas may be of independent interest. version:1
arxiv-1411-1076 | A statistical model for tensor PCA | http://arxiv.org/abs/1411.1076 | id:1411.1076 author:Andrea Montanari, Emile Richard category:cs.LG cs.IT math.IT stat.ML  published:2014-11-04 summary:We consider the Principal Component Analysis problem for large tensors of arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio $\beta$ becomes larger than $C\sqrt{k\log k}$ (and in particular $\beta$ can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate. version:1
arxiv-1110-2897 | Randomized Dimensionality Reduction for k-means Clustering | http://arxiv.org/abs/1110.2897 | id:1110.2897 author:Christos Boutsidis, Anastasios Zouzias, Michael W. Mahoney, Petros Drineas category:cs.DS cs.LG  published:2011-10-13 summary:We study the topic of dimensionality reduction for $k$-means clustering. Dimensionality reduction encompasses the union of two approaches: \emph{feature selection} and \emph{feature extraction}. A feature selection based algorithm for $k$-means clustering selects a small subset of the input features and then applies $k$-means clustering on the selected features. A feature extraction based algorithm for $k$-means clustering constructs a small set of new artificial features and then applies $k$-means clustering on the constructed features. Despite the significance of $k$-means clustering as well as the wealth of heuristic methods addressing it, provably accurate feature selection methods for $k$-means clustering are not known. On the other hand, two provably accurate feature extraction methods for $k$-means clustering are known in the literature; one is based on random projections and the other is based on the singular value decomposition (SVD). This paper makes further progress towards a better understanding of dimensionality reduction for $k$-means clustering. Namely, we present the first provably accurate feature selection method for $k$-means clustering and, in addition, we present two feature extraction methods. The first feature extraction method is based on random projections and it improves upon the existing results in terms of time complexity and number of features needed to be extracted. The second feature extraction method is based on fast approximate SVD factorizations and it also improves upon the existing results in terms of time complexity. The proposed algorithms are randomized and provide constant-factor approximation guarantees with respect to the optimal $k$-means objective value. version:3
arxiv-1411-0997 | Iterated geometric harmonics for data imputation and reconstruction of missing data | http://arxiv.org/abs/1411.0997 | id:1411.0997 author:Chad Eckman, Jonathan A. Lindgren, Erin P. J. Pearse, David J. Sacco, Zachariah Zhang category:cs.LG stat.ML  published:2014-11-04 summary:The method of geometric harmonics is adapted to the situation of incomplete data by means of the iterated geometric harmonics (IGH) scheme. The method is tested on natural and synthetic data sets with 50--500 data points and dimensionality of 400--10,000. Experiments suggest that the algorithm converges to a near optimal solution within 4--6 iterations, at runtimes of less than 30 minutes on a medium-grade desktop computer. The imputation of missing data values is applied to collections of damaged images (suffering from data annihilation rates of up to 70\%) which are reconstructed with a surprising degree of accuracy. version:1
arxiv-1404-3378 | Complexity theoretic limitations on learning DNF's | http://arxiv.org/abs/1404.3378 | id:1404.3378 author:Amit Daniely, Shai Shalev-Shwatz category:cs.LG cs.CC  published:2014-04-13 summary:Using the recently developed framework of [Daniely et al, 2014], we show that under a natural assumption on the complexity of refuting random K-SAT formulas, learning DNF formulas is hard. Furthermore, the same assumption implies the hardness of learning intersections of $\omega(\log(n))$ halfspaces, agnostically learning conjunctions, as well as virtually all (distribution free) learning problems that were previously shown hard (under complexity assumptions). version:2
arxiv-1407-3399 | Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations | http://arxiv.org/abs/1407.3399 | id:1407.3399 author:Xianjie Chen, Alan Yuille category:cs.CV  published:2014-07-12 summary:We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training. version:2
arxiv-1411-0972 | Convex Optimization for Big Data | http://arxiv.org/abs/1411.0972 | id:1411.0972 author:Volkan Cevher, Stephen Becker, Mark Schmidt category:math.OC cs.LG stat.ML  published:2014-11-04 summary:This article reviews recent advances in convex optimization algorithms for Big Data, which aim to reduce the computational, storage, and communications bottlenecks. We provide an overview of this emerging field, describe contemporary approximation techniques like first-order methods and randomization for scalability, and survey the important role of parallel and distributed computation. The new Big Data algorithms are based on surprisingly simple principles and attain staggering accelerations even on classical problems. version:1
arxiv-1411-0939 | Simple approximate MAP Inference for Dirichlet processes | http://arxiv.org/abs/1411.0939 | id:1411.0939 author:Yordan P. Raykov, Alexis Boukouvalas, Max A. Little category:stat.ML  published:2014-11-04 summary:The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesian nonparametric statistical model. However, full probabilistic inference in this model is analytically intractable, so that computationally intensive techniques such as Gibb's sampling are required. As a result, DPM-based methods, which have considerable potential, are restricted to applications in which computational resources and time for inference is plentiful. For example, they would not be practical for digital signal processing on embedded hardware, where computational resources are at a serious premium. Here, we develop simplified yet statistically rigorous approximate maximum a-posteriori (MAP) inference algorithms for DPMs. This algorithm is as simple as K-means clustering, performs in experiments as well as Gibb's sampling, while requiring only a fraction of the computational effort. Unlike related small variance asymptotics, our algorithm is non-degenerate and so inherits the "rich get richer" property of the Dirichlet process. It also retains a non-degenerate closed-form likelihood which enables standard tools such as cross-validation to be used. This is a well-posed approximation to the MAP solution of the probabilistic DPM model. version:1
arxiv-1409-6110 | Best-Arm Identification in Linear Bandits | http://arxiv.org/abs/1409.6110 | id:1409.6110 author:Marta Soare, Alessandro Lazaric, R√©mi Munos category:cs.LG  published:2014-09-22 summary:We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the $G$-optimality criterion used in optimal experimental design. version:2
arxiv-1406-4441 | Scaling laws and fluctuations in the statistics of word frequencies | http://arxiv.org/abs/1406.4441 | id:1406.4441 author:Martin Gerlach, Eduardo G. Altmann category:physics.soc-ph cs.CL physics.data-an  published:2014-06-17 summary:In this paper we combine statistical analysis of large text databases and simple stochastic models to explain the appearance of scaling laws in the statistics of word frequencies. Besides the sublinear scaling of the vocabulary size with database size (Heaps' law), here we report a new scaling of the fluctuations around this average (fluctuation scaling analysis). We explain both scaling laws by modeling the usage of words by simple stochastic processes in which the overall distribution of word-frequencies is fat tailed (Zipf's law) and the frequency of a single word is subject to fluctuations across documents (as in topic models). In this framework, the mean and the variance of the vocabulary size can be expressed as quenched averages, implying that: i) the inhomogeneous dissemination of words cause a reduction of the average vocabulary size in comparison to the homogeneous case, and ii) correlations in the co-occurrence of words lead to an increase in the variance and the vocabulary size becomes a non-self-averaging quantity. We address the implications of these observations to the measurement of lexical richness. We test our results in three large text databases (Google-ngram, Enlgish Wikipedia, and a collection of scientific articles). version:2
arxiv-1411-0900 | Kernel Mean Estimation via Spectral Filtering | http://arxiv.org/abs/1411.0900 | id:1411.0900 author:Krikamol Muandet, Bharath Sriperumbudur, Bernhard Sch√∂lkopf category:stat.ML math.ST stat.TH  published:2014-11-04 summary:The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Muandet et al. (2014) has shown that shrinkage can help in constructing "better" estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in Muandet et al. (2014), and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice. version:1
arxiv-1411-0895 | Tied Probabilistic Linear Discriminant Analysis for Speech Recognition | http://arxiv.org/abs/1411.0895 | id:1411.0895 author:Liang Lu, Steve Renals category:cs.CL cs.AI  published:2014-11-04 summary:Acoustic models using probabilistic linear discriminant analysis (PLDA) capture the correlations within feature vectors using subspaces which do not vastly expand the model. This allows high dimensional and correlated feature spaces to be used, without requiring the estimation of multiple high dimension covariance matrices. In this letter we extend the recently presented PLDA mixture model for speech recognition through a tied PLDA approach, which is better able to control the model size to avoid overfitting. We carried out experiments using the Switchboard corpus, with both mel frequency cepstral coefficient features and bottleneck feature derived from a deep neural network. Reductions in word error rate were obtained by using tied PLDA, compared with the PLDA mixture model, subspace Gaussian mixture models, and deep neural networks. version:1
arxiv-1411-0861 | Using Linguistic Features to Estimate Suicide Probability of Chinese Microblog Users | http://arxiv.org/abs/1411.0861 | id:1411.0861 author:Lei Zhang, Xiaolei Huang, Tianli Liu, Zhenxiang Chen, Tingshao Zhu category:cs.SI cs.CL  published:2014-11-04 summary:If people with high risk of suicide can be identified through social media like microblog, it is possible to implement an active intervention system to save their lives. Based on this motivation, the current study administered the Suicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a leading microblog service provider in China. Two NLP (Natural Language Processing) methods, the Chinese edition of Linguistic Inquiry and Word Count (LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract linguistic features from the Sina Weibo data. We trained predicting models by machine learning algorithm based on these two types of features, to estimate suicide probability based on linguistic features. The experiment results indicate that LDA can find topics that relate to suicide probability, and improve the performance of prediction. Our study adds value in prediction of suicidal probability of social network users with their behaviors. version:1
arxiv-1411-0860 | CUR Algorithm for Partially Observed Matrices | http://arxiv.org/abs/1411.0860 | id:1411.0860 author:Miao Xu, Rong Jin, Zhi-Hua Zhou category:cs.LG  published:2014-11-04 summary:CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they need an access to the {\it full} matrix, a requirement that can be difficult to fulfill in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only $O(n r\ln r)$ observed entries are needed by the proposed algorithm to perfectly recover a rank $r$ matrix of size $n\times n$, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm. version:1
arxiv-1411-0814 | A random algorithm for low-rank decomposition of large-scale matrices with missing entries | http://arxiv.org/abs/1411.0814 | id:1411.0814 author:Yiguang Liu category:cs.NA cs.CV  published:2014-11-04 summary:A Random SubMatrix method (RSM) is proposed to calculate the low-rank decomposition of large-scale matrices with known entry percentage \rho. RSM is very fast as the floating-point operations (flops) required are compared favorably with the state-of-the-art algorithms. Meanwhile RSM is very memory-saving. With known entries homogeneously distributed in the given matrix, sub-matrices formed by known entries are randomly selected. According to the just proved theorem that subspace related to smaller singular values is less perturbed by noise, the null vectors or the right singular vectors associated with the minor singular values are calculated for each submatrix. The vectors are the null vectors of the corresponding submatrix in the ground truth of the given large-scale matrix. If enough sub-matrices are randomly chosen, the low-rank decomposition is estimated. The experimental results on random synthetical matrices with sizes such as 131072X1024 and on real data sets indicate that RSM is much faster and memory-saving, and, meanwhile, has considerable high precision achieving or approximating to the best. version:1
arxiv-1411-0802 | Simultaneous Localization, Mapping, and Manipulation for Unsupervised Object Discovery | http://arxiv.org/abs/1411.0802 | id:1411.0802 author:Lu Ma, Mahsa Ghafarianzadeh, Dave Coleman, Nikolaus Correll, Gabe Sibley category:cs.RO cs.CV  published:2014-11-04 summary:We present an unsupervised framework for simultaneous appearance-based object discovery, detection, tracking and reconstruction using RGBD cameras and a robot manipulator. The system performs dense 3D simultaneous localization and mapping concurrently with unsupervised object discovery. Putative objects that are spatially and visually coherent are manipulated by the robot to gain additional motion-cues. The robot uses appearance alone, followed by structure and motion cues, to jointly discover, verify, learn and improve models of objects. Induced motion segmentation reinforces learned models which are represented implicitly as 2D and 3D level sets to capture both shape and appearance. We compare three different approaches for appearance-based object discovery and find that a novel form of spatio-temporal super-pixels gives the highest quality candidate object models in terms of precision and recall. Live experiments with a Baxter robot demonstrate a holistic pipeline capable of automatic discovery, verification, detection, tracking and reconstruction of unknown objects. version:1
arxiv-1411-0791 | A Robust Point Sets Matching Method | http://arxiv.org/abs/1411.0791 | id:1411.0791 author:Xiao Liu, Congying Han, Tiande Guo category:cs.CV  published:2014-11-04 summary:Point sets matching method is very important in computer vision, feature extraction, fingerprint matching, motion estimation and so on. This paper proposes a robust point sets matching method. We present an iterative algorithm that is robust to noise case. Firstly, we calculate all transformations between two points. Then similarity matrix are computed to measure the possibility that two transformation are both true. We iteratively update the matching score matrix by using the similarity matrix. By using matching algorithm on graph, we obtain the matching result. Experimental results obtained by our approach show robustness to outlier and jitter. version:1
arxiv-1409-6075 | The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets | http://arxiv.org/abs/1409.6075 | id:1409.6075 author:Tyler Ward category:cs.LG  published:2014-09-22 summary:This document discusses the Information Theoretically Efficient Model (ITEM), a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset. More specifically, this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables. This research shows that for large datasets, the resulting models can be produced on modern computers in a tractable amount of time. These models are also resistant to overfitting, and as such they tend to produce interpretable models with only a limited number of features, all of which are designed to be well behaved. version:3
arxiv-1410-6991 | A provable SVD-based algorithm for learning topics in dominant admixture corpus | http://arxiv.org/abs/1410.6991 | id:1410.6991 author:Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan category:stat.ML cs.LG  published:2014-10-26 summary:Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents are drawn from admixtures of distributions over words, known as topics. The inference problem of recovering topics from admixtures, is NP-hard. Assuming separability, a strong assumption, [4] gave the first provable algorithm for inference. For LDA model, [6] gave a provable algorithm using tensor-methods. But [4,6] do not learn topic vectors with bounded $l_1$ error (a natural measure for probability vectors). Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as SVD, which provably solves the inference problem for the model with bounded $l_1$ error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this, we introduce topic specific Catchwords, group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than others. Apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on $w_0$, the lowest probability that a topic is dominant, and is better than [4]. Empirical evidence shows that on several real world corpora, both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5]. version:3
arxiv-1411-0778 | Detecting Suicidal Ideation in Chinese Microblogs with Psychological Lexicons | http://arxiv.org/abs/1411.0778 | id:1411.0778 author:Xiaolei Huang, Lei Zhang, Tianli Liu, David Chiu, Tingshao Zhu, Xin Li category:cs.CL  published:2014-11-04 summary:Suicide is among the leading causes of death in China. However, technical approaches toward preventing suicide are challenging and remaining under development. Recently, several actual suicidal cases were preceded by users who posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media network akin to Twitter. It would therefore be desirable to detect suicidal ideations from microblogs in real-time, and immediately alert appropriate support groups, which may lead to successful prevention. In this paper, we propose a real-time suicidal ideation detection system deployed over Weibo, using machine learning and known psychological techniques. Currently, we have identified 53 known suicidal cases who posted suicide notes on Weibo prior to their deaths.We explore linguistic features of these known cases using a psychological lexicon dictionary, and train an effective suicidal Weibo post detection model. 6714 tagged posts and several classifiers are used to verify the model. By combining both machine learning and psychological knowledge, SVM classifier has the best performance of different classifiers, yielding an F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%. version:1
arxiv-1406-1134 | Local Decorrelation For Improved Detection | http://arxiv.org/abs/1406.1134 | id:1406.1134 author:Woonhyun Nam, Piotr Doll√°r, Joon Hee Han category:cs.CV  published:2014-06-04 summary:Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art. version:2
arxiv-1411-0763 | A Weighted Common Subgraph Matching Algorithm | http://arxiv.org/abs/1411.0763 | id:1411.0763 author:Xu Yang, Hong Qiao, Zhi-Yong Liu category:cs.DS cs.CV  published:2014-11-04 summary:We propose a weighted common subgraph (WCS) matching algorithm to find the most similar subgraphs in two labeled weighted graphs. WCS matching, as a natural generalization of the equal-sized graph matching or subgraph matching, finds wide applications in many computer vision and machine learning tasks. In this paper, the WCS matching is first formulated as a combinatorial optimization problem over the set of partial permutation matrices. Then it is approximately solved by a recently proposed combinatorial optimization framework - Graduated NonConvexity and Concavity Procedure (GNCCP). Experimental comparisons on both synthetic graphs and real world images validate its robustness against noise level, problem size, outlier number, and edge density. version:1
arxiv-1205-3767 | Universal Algorithm for Online Trading Based on the Method of Calibration | http://arxiv.org/abs/1205.3767 | id:1205.3767 author:Vladimir V'yugin, Vladimir Trunov category:cs.LG q-fin.PM  published:2012-05-16 summary:We present a universal algorithm for online trading in Stock Market which performs asymptotically at least as good as any stationary trading strategy that computes the investment at each step using a fixed function of the side information that belongs to a given RKHS (Reproducing Kernel Hilbert Space). Using a universal kernel, we extend this result for any continuous stationary strategy. In this learning process, a trader rationally chooses his gambles using predictions made by a randomized well-calibrated algorithm. Our strategy is based on Dawid's notion of calibration with more general checking rules and on some modification of Kakade and Foster's randomized rounding algorithm for computing the well-calibrated forecasts. We combine the method of randomized calibration with Vovk's method of defensive forecasting in RKHS. Unlike the statistical theory, no stochastic assumptions are made about the stock prices. Our empirical results on historical markets provide strong evidence that this type of technical trading can "beat the market" if transaction costs are ignored. version:3
arxiv-1411-0728 | A Learning Scheme for Approachability in MDPs and Stackelberg Stochastic Games | http://arxiv.org/abs/1411.0728 | id:1411.0728 author:Dileep Kalathil, Vivek Borkar, Rahul Jain category:cs.LG cs.GT cs.SY math.OC  published:2014-11-03 summary:The notion of approachability was introduced by Blackwell in the context of vector-valued repeated games. The famous approachability theorem prescribes a strategy for approachability, i.e., for `steering' the average vector-cost of a given player towards a given target set, irrespective of the strategies of the other players. In this paper, motivated from the multi-objective optimization/decision making problems in dynamically changing environments, we address the approachability problem in Markov Decision Processes (MDPs) and Stackelberg stochastic games with vector-valued cost functions. We make two main contributions. Firstly, we give simple and computationally tractable strategy for approachability for MDPs and Stackelberg stochastic games. Secondly, we give reinforcement learning algorithms to learn the approachable strategy when the transition kernel is unknown. We also show that the conditions that we give for approachability are both necessary and sufficient for convex sets and thus a complete characterization. We also give sufficient conditions for non-convex sets. version:1
arxiv-1411-0707 | A Nonparametric Adaptive Nonlinear Statistical Filter | http://arxiv.org/abs/1411.0707 | id:1411.0707 author:Michael Busch, Jeff Moehlis category:stat.ML  published:2014-11-03 summary:We use statistical learning methods to construct an adaptive state estimator for nonlinear stochastic systems. Optimal state estimation, in the form of a Kalman filter, requires knowledge of the system's process and measurement uncertainty. We propose that these uncertainties can be estimated from (conditioned on) past observed data, and without making any assumptions of the system's prior distribution. The system's prior distribution at each time step is constructed from an ensemble of least-squares estimates on sub-sampled sets of the data via jackknife sampling. As new data is acquired, the state estimates, process uncertainty, and measurement uncertainty are updated accordingly, as described in this manuscript. version:1
