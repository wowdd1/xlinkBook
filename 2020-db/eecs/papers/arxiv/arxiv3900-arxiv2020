arxiv-1310-6808 | Gender Classification Using Gradient Direction Pattern | http://arxiv.org/abs/1310.6808 | id:1310.6808 author:Mohammad shahidul Islam category:cs.CV I.5.4  published:2013-10-25 summary:A novel methodology for gender classification is presented in this paper. It extracts feature from local region of a face using gray color intensity difference. The facial area is divided into sub-regions and GDP histogram extracted from those regions are concatenated into a single vector to represent the face. The classification accuracy obtained by using support vector machine has outperformed all traditional feature descriptors for gender classification. It is evaluated on the images collected from FERET database and obtained very high accuracy. version:1
arxiv-1302-2684 | A Tensor Approach to Learning Mixed Membership Community Models | http://arxiv.org/abs/1302.2684 | id:1302.2684 author:Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade category:cs.LG cs.SI stat.ML  published:2013-02-12 summary:Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of 3-star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g. singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model. version:4
arxiv-1310-6775 | Durkheim Project Data Analysis Report | http://arxiv.org/abs/1310.6775 | id:1310.6775 author:Linas Vepstas category:cs.AI cs.CL cs.LG  published:2013-10-24 summary:This report describes the suicidality prediction models created under the DARPA DCAPS program in association with the Durkheim Project [http://durkheimproject.org/]. The models were built primarily from unstructured text (free-format clinician notes) for several hundred patient records obtained from the Veterans Health Administration (VHA). The models were constructed using a genetic programming algorithm applied to bag-of-words and bag-of-phrases datasets. The influence of additional structured data was explored but was found to be minor. Given the small dataset size, classification between cohorts was high fidelity (98%). Cross-validation suggests these models are reasonably predictive, with an accuracy of 50% to 69% on five rotating folds, with ensemble averages of 58% to 67%. One particularly noteworthy result is that word-pairs can dramatically improve classification accuracy; but this is the case only when one of the words in the pair is already known to have a high predictive value. By contrast, the set of all possible word-pairs does not improve on a simple bag-of-words model. version:1
arxiv-1310-6772 | Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive Writing for Linking Identities | http://arxiv.org/abs/1310.6772 | id:1310.6772 author:Thamar Solorio, Ragib Hasan, Mainul Mizan category:cs.CL cs.CR cs.CY  published:2013-10-24 summary:This paper describes the corpus of sockpuppet cases we gathered from Wikipedia. A sockpuppet is an online user account created with a fake identity for the purpose of covering abusive behavior and/or subverting the editing regulation process. We used a semi-automated method for crawling and curating a dataset of real sockpuppet investigation cases. To the best of our knowledge, this is the first corpus available on real-world deceptive writing. We describe the process for crawling the data and some preliminary results that can be used as baseline for benchmarking research. The dataset will be released under a Creative Commons license from our project website: http://docsig.cis.uab.edu. version:1
arxiv-1310-6304 | Combining Structured and Unstructured Randomness in Large Scale PCA | http://arxiv.org/abs/1310.6304 | id:1310.6304 author:Nikos Karampatziakis, Paul Mineiro category:cs.LG  published:2013-10-23 summary:Principal Component Analysis (PCA) is a ubiquitous tool with many applications in machine learning including feature construction, subspace embedding, and outlier detection. In this paper, we present an algorithm for computing the top principal components of a dataset with a large number of rows (examples) and columns (features). Our algorithm leverages both structured and unstructured random projections to retain good accuracy while being computationally efficient. We demonstrate the technique on the winning submission the KDD 2010 Cup. version:2
arxiv-1310-6654 | Pseudo vs. True Defect Classification in Printed Circuits Boards using Wavelet Features | http://arxiv.org/abs/1310.6654 | id:1310.6654 author:Sahil Sikka, Karan Sikka, M. K. Bhuyan, Yuji Iwahori category:cs.CV  published:2013-10-24 summary:In recent years, Printed Circuit Boards (PCB) have become the backbone of a large number of consumer electronic devices leading to a surge in their production. This has made it imperative to employ automatic inspection systems to identify manufacturing defects in PCB before they are installed in the respective systems. An important task in this regard is the classification of defects as either true or pseudo defects, which decides if the PCB is to be re-manufactured or not. This work proposes a novel approach to detect most common defects in the PCBs. The problem has been approached by employing highly discriminative features based on multi-scale wavelet transform, which are further boosted by using a kernalized version of the support vector machines (SVM). A real world printed circuit board dataset has been used for quantitative analysis. Experimental results demonstrated the efficacy of the proposed method. version:1
arxiv-1310-6740 | Active Learning of Linear Embeddings for Gaussian Processes | http://arxiv.org/abs/1310.6740 | id:1310.6740 author:Roman Garnett, Michael A. Osborne, Philipp Hennig category:stat.ML cs.LG 68T05 I.2.6; I.5.2; G.3  published:2013-10-24 summary:We propose an active learning method for discovering low-dimensional structure in high-dimensional Gaussian process (GP) tasks. Such problems are increasingly frequent and important, but have hitherto presented severe practical difficulties. We further introduce a novel technique for approximately marginalizing GP hyperparameters, yielding marginal predictions robust to hyperparameter mis-specification. Our method offers an efficient means of performing GP regression, quadrature, or Bayesian optimization in high-dimensional spaces. version:1
arxiv-1310-1867 | Mean Field Bayes Backpropagation: scalable training of multilayer neural networks with binary weights | http://arxiv.org/abs/1310.1867 | id:1310.1867 author:Daniel Soudry, Ron Meir category:stat.ML  published:2013-10-07 summary:Significant success has been reported recently using deep neural networks for classification. Such large networks can be computationally intensive, even after training is over. Implementing these trained networks in hardware chips with a limited precision of synaptic weights may improve their speed and energy efficiency by several orders of magnitude, thus enabling their integration into small and low-power electronic devices. With this motivation, we develop a computationally efficient learning algorithm for multilayer neural networks with binary weights, assuming all the hidden neurons have a fan-out of one. This algorithm, derived within a Bayesian probabilistic online setting, is shown to work well for both synthetic and real-world problems, performing comparably to algorithms with real-valued weights, while retaining computational tractability. version:4
arxiv-1310-6547 | Sparse Predictive Structure of Deconvolved Functional Brain Networks | http://arxiv.org/abs/1310.6547 | id:1310.6547 author:Tommaso Furlanello, Marco Cristoforetti, Cesare Furlanello, Giuseppe Jurman category:q-bio.NC q-bio.QM stat.ML  published:2013-10-24 summary:The functional and structural representation of the brain as a complex network is marked by the fact that the comparison of noisy and intrinsically correlated high-dimensional structures between experimental conditions or groups shuns typical mass univariate methods. Furthermore most network estimation methods cannot distinguish between real and spurious correlation arising from the convolution due to nodes' interaction, which thus introduces additional noise in the data. We propose a machine learning pipeline aimed at identifying multivariate differences between brain networks associated to different experimental conditions. The pipeline (1) leverages the deconvolved individual contribution of each edge and (2) maps the task into a sparse classification problem in order to construct the associated "sparse deconvolved predictive network", i.e., a graph with the same nodes of those compared but whose edge weights are defined by their relevance for out of sample predictions in classification. We present an application of the proposed method by decoding the covert attention direction (left or right) based on the single-trial functional connectivity matrix extracted from high-frequency magnetoencephalography (MEG) data. Our results demonstrate how network deconvolution matched with sparse classification methods outperforms typical approaches for MEG decoding. version:1
arxiv-1310-6536 | Randomized co-training: from cortical neurons to machine learning and back again | http://arxiv.org/abs/1310.6536 | id:1310.6536 author:David Balduzzi category:cs.LG q-bio.NC stat.ML  published:2013-10-24 summary:Despite its size and complexity, the human cortex exhibits striking anatomical regularities, suggesting there may simple meta-algorithms underlying cortical learning and computation. We expect such meta-algorithms to be of interest since they need to operate quickly, scalably and effectively with little-to-no specialized assumptions. This note focuses on a specific question: How can neurons use vast quantities of unlabeled data to speed up learning from the comparatively rare labels provided by reward systems? As a partial answer, we propose randomized co-training as a biologically plausible meta-algorithm satisfying the above requirements. As evidence, we describe a biologically-inspired algorithm, Correlated Nystrom Views (XNV) that achieves state-of-the-art performance in semi-supervised learning, and sketch work in progress on a neuronal implementation. version:1
arxiv-1310-5543 | Universalities of Reproducing Kernels Revisited | http://arxiv.org/abs/1310.5543 | id:1310.5543 author:Benxun Wang, Haizhang Zhang category:stat.ML  published:2013-10-21 summary:Kernel methods have been widely applied to machine learning and other questions of approximating an unknown function from its finite sample data. To ensure arbitrary accuracy of such approximation, various denseness conditions are imposed on the selected kernel. This note contributes to the study of universal, characteristic, and $C_0$-universal kernels. We first give simple and direct description of the difference and relation among these three kinds of universalities of kernels. We then focus on translation-invariant and weighted polynomial kernels. A simple and shorter proof of the known characterization of characteristic translation-invariant kernels will be presented. The main purpose of the note is to give a delicate discussion on the universalities of weighted polynomial kernels. version:2
arxiv-1310-6736 | Fast 3D Salient Region Detection in Medical Images using GPUs | http://arxiv.org/abs/1310.6736 | id:1310.6736 author:Rahul Thota, Sharan Vaswani, Amit Kale, Nagavijayalakshmi Vydyanathan category:cs.CV  published:2013-10-24 summary:Automated detection of visually salient regions is an active area of research in computer vision. Salient regions can serve as inputs for object detectors as well as inputs for region based registration algorithms. In this paper we consider the problem of speeding up computationally intensive bottom-up salient region detection in 3D medical volumes.The method uses the Kadir Brady formulation of saliency. We show that in the vicinity of a salient region, entropy is a monotonically increasing function of the degree of overlap of a candidate window with the salient region. This allows us to initialize a sparse seed-point grid as the set of tentative salient region centers and iteratively converge to the local entropy maxima, thereby reducing the computation complexity compared to the Kadir Brady approach of performing this computation at every point in the image. We propose two different approaches for achieving this. The first approach involves evaluating entropy in the four quadrants around the seed point and iteratively moving in the direction that increases entropy. The second approach we propose makes use of mean shift tracking framework to affect entropy maximizing moves. Specifically, we propose the use of uniform pmf as the target distribution to seek high entropy regions. We demonstrate the use of our algorithm on medical volumes for left ventricle detection in PET images and tumor localization in brain MR sequences. version:1
arxiv-1310-6376 | Can Facial Uniqueness be Inferred from Impostor Scores? | http://arxiv.org/abs/1310.6376 | id:1310.6376 author:Abhishek Dutta, Raymond Veldhuis, Luuk Spreeuwers category:cs.CV  published:2013-10-23 summary:In Biometrics, facial uniqueness is commonly inferred from impostor similarity scores. In this paper, we show that such uniqueness measures are highly unstable in the presence of image quality variations like pose, noise and blur. We also experimentally demonstrate the instability of a recently introduced impostor-based uniqueness measure of [Klare and Jain 2013] when subject to poor quality facial images. version:1
arxiv-1310-6343 | Provable Bounds for Learning Some Deep Representations | http://arxiv.org/abs/1310.6343 | id:1310.6343 author:Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma category:cs.LG cs.AI stat.ML  published:2013-10-23 summary:We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an $n$ node multilayer neural net that has degree at most $n^{\gamma}$ for some $\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our algorithm learns {\em almost all} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights. version:1
arxiv-1310-6338 | Risk aversion as an evolutionary adaptation | http://arxiv.org/abs/1310.6338 | id:1310.6338 author:Arend Hintze, Randal S. Olson, Christoph Adami, Ralph Hertwig category:q-bio.PE cs.GT cs.NE  published:2013-10-23 summary:Risk aversion is a common behavior universal to humans and animals alike. Economists have traditionally defined risk preferences by the curvature of the utility function. Psychologists and behavioral economists also make use of concepts such as loss aversion and probability weighting to model risk aversion. Neurophysiological evidence suggests that loss aversion has its origins in relatively ancient neural circuitries (e.g., ventral striatum). Could there thus be an evolutionary origin to risk avoidance? We study this question by evolving strategies that adapt to play the equivalent mean payoff gamble. We hypothesize that risk aversion in the equivalent mean payoff gamble is beneficial as an adaptation to living in small groups, and find that a preference for risk averse strategies only evolves in small populations of less than 1,000 individuals, while agents exhibit no such strategy preference in larger populations. Further, we discover that risk aversion can also evolve in larger populations, but only when the population is segmented into small groups of around 150 individuals. Finally, we observe that risk aversion only evolves when the gamble is a rare event that has a large impact on the individual's fitness. These findings align with earlier reports that humans lived in small groups for a large portion of their evolutionary history. As such, we suggest that rare, high-risk, high-payoff events such as mating and mate competition could have driven the evolution of risk averse behavior in humans living in small groups. version:1
arxiv-1310-6288 | Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery EEG in Rehabilitation Training | http://arxiv.org/abs/1310.6288 | id:1310.6288 author:Hao Zhang, Liqing Zhang category:stat.ML cs.AI cs.LG  published:2013-10-23 summary:Current studies about motor imagery based rehabilitation training systems for stroke subjects lack an appropriate analytic method, which can achieve a considerable classification accuracy, at the same time detects gradual changes of imagery patterns during rehabilitation process and disinters potential mechanisms about motor function recovery. In this study, we propose an adaptive boosting algorithm based on the cortex plasticity and spectral band shifts. This approach models the usually predetermined spatial-spectral configurations in EEG study into variable preconditions, and introduces a new heuristic of stochastic gradient boost for training base learners under these preconditions. We compare our proposed algorithm with commonly used methods on datasets collected from 2 months' clinical experiments. The simulation results demonstrate the effectiveness of the method in detecting the variations of stroke patients' EEG patterns. By chronologically reorganizing the weight parameters of the learned additive model, we verify the spatial compensatory mechanism on impaired cortex and detect the changes of accentuation bands in spectral domain, which may contribute important prior knowledge for rehabilitation practice. version:1
arxiv-1310-6092 | A Ray-based Approach for Boundary Estimation of Fiber Bundles Derived from Diffusion Tensor Imaging | http://arxiv.org/abs/1310.6092 | id:1310.6092 author:Miriam H. A. Bauer, Sebastiano Barbieri, Jan Klein, Jan Egger, Daniela Kuhnt, Bernd Freisleben, Horst K. Hahn, Christopher Nimsky category:cs.CV  published:2013-10-23 summary:Diffusion Tensor Imaging (DTI) is a non-invasive imaging technique that allows estimation of the location of white matter tracts in-vivo, based on the measurement of water diffusion properties. For each voxel, a second-order tensor can be calculated by using diffusion-weighted sequences (DWI) that are sensitive to the random motion of water molecules. Given at least 6 diffusion-weighted images with different gradients and one unweighted image, the coefficients of the symmetric diffusion tensor matrix can be calculated. Deriving the eigensystem of the tensor, the eigenvectors and eigenvalues can be calculated to describe the three main directions of diffusion and its magnitude. Using DTI data, fiber bundles can be determined, to gain information about eloquent brain structures. Especially in neurosurgery, information about location and dimension of eloquent structures like the corticospinal tract or the visual pathways is of major interest. Therefore, the fiber bundle boundary has to be determined. In this paper, a novel ray-based approach for boundary estimation of tubular structures is presented. version:1
arxiv-1303-4566 | Inferring Fitness in Finite Populations with Moran-like dynamics | http://arxiv.org/abs/1303.4566 | id:1303.4566 author:Marc Harper category:math.DS cs.NE q-bio.PE 91A22  published:2013-03-19 summary:Biological fitness is not an observable quantity and must be inferred from population dynamics. Bayesian inference applied to the Moran process and variants yields a robust inference method that can infer fitness in populations evolving via a Moran dynamic and generalizations. Information about fitness is derived solely from birth-events in birth-death and death-birth processes in which selection acts proportionally to fitness, which allows the method to be applied to populations on a network where the network itself may be changing in time. Populations may also be allowed to change size while still allowing estimates for fitness to be inferred. version:3
arxiv-1211-5189 | Optimally fuzzy temporal memory | http://arxiv.org/abs/1211.5189 | id:1211.5189 author:Karthik H. Shankar, Marc W. Howard category:cs.AI cs.LG  published:2012-11-22 summary:Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system. version:2
arxiv-1310-6067 | Multiple Kernel Learning for Brain-Computer Interfacing | http://arxiv.org/abs/1310.6067 | id:1310.6067 author:Wojciech Samek, Alexander Binder, Klaus-Robert Müller category:stat.ML  published:2013-10-22 summary:Combining information from different sources is a common way to improve classification accuracy in Brain-Computer Interfacing (BCI). For instance, in small sample settings it is useful to integrate data from other subjects or sessions in order to improve the estimation quality of the spatial filters or the classifier. Since data from different subjects may show large variability, it is crucial to weight the contributions according to importance. Many multi-subject learning algorithms determine the optimal weighting in a separate step by using heuristics, however, without ensuring that the selected weights are optimal with respect to classification. In this work we apply Multiple Kernel Learning (MKL) to this problem. MKL has been widely used for feature fusion in computer vision and allows to simultaneously learn the classifier and the optimal weighting. We compare the MKL method to two baseline approaches and investigate the reasons for performance improvement. version:1
arxiv-1310-6066 | Skin Segmentation based Elastic Bunch Graph Matching for efficient multiple Face Recognition | http://arxiv.org/abs/1310.6066 | id:1310.6066 author:Sayantan Sarkar category:cs.CV  published:2013-10-22 summary:This paper is aimed at developing and combining different algorithms for face detection and face recognition to generate an efficient mechanism that can detect and recognize the facial regions of input image. For the detection of face from complex region, skin segmentation isolates the face-like regions in a complex image and following operations of morphology and template matching rejects false matches to extract facial region. For the recognition of the face, the image database is now converted into a database of facial segments. Hence, implementing the technique of Elastic Bunch Graph matching (EBGM) after skin segmentation generates Face Bunch Graphs that acutely represents the features of an individual face enhances the quality of the training set. This increases the matching probability significantly. version:1
arxiv-1310-6063 | Word Spotting in Cursive Handwritten Documents using Modified Character Shape Codes | http://arxiv.org/abs/1310.6063 | id:1310.6063 author:Sayantan Sarkar category:cs.CV  published:2013-10-22 summary:There is a large collection of Handwritten English paper documents of Historical and Scientific importance. But paper documents are not recognized directly by computer. Hence the closest way of indexing these documents is by storing their document digital image. Hence a large database of document images can replace the paper documents. But the document and data corresponding to each image cannot be directly recognized by the computer. This paper applies the technique of word spotting using Modified Character Shape Code to Handwritten English document images for quick and efficient query search of words on a database of document images. It is different from other Word Spotting techniques as it implements two level of selection for word segments to match search query. First based on word size and then based on character shape code of query. It makes the process faster and more efficient and reduces the need of multiple pre-processing. version:1
arxiv-1310-6062 | Combined l_1 and greedy l_0 penalized least squares for linear model selection | http://arxiv.org/abs/1310.6062 | id:1310.6062 author:Piotr Pokarowski, Jan Mielniczuk category:stat.ML  published:2013-10-22 summary:We introduce a computationally effective algorithm for a linear model selection consisting of three steps: screening--ordering--selection (SOS). Screening of predictors is based on the thresholded Lasso that is l_1 penalized least squares. The screened predictors are then fitted using least squares (LS) and ordered with respect to their t statistics. Finally, a model is selected using greedy generalized information criterion (GIC) that is l_0 penalized LS in a nested family induced by the ordering. We give non-asymptotic upper bounds on error probability of each step of the SOS algorithm in terms of both penalties. Then we obtain selection consistency for different (n, p) scenarios under conditions which are needed for screening consistency of the Lasso. For the traditional setting (n >p) we give Sanov-type bounds on the error probabilities of the ordering--selection algorithm. Its surprising consequence is that the selection error of greedy GIC is asymptotically not larger than of exhaustive GIC. We also obtain new bounds on prediction and estimation errors for the Lasso which are proved in parallel for the algorithm used in practice and its formal version. version:1
arxiv-1310-5999 | Improvement of Automatic Hemorrhages Detection Methods Using Shapes Recognition | http://arxiv.org/abs/1310.5999 | id:1310.5999 author:Nidhal Khdhair El Abbadi, Enas Hamood Al Saadi category:cs.CV  published:2013-10-22 summary:Diabetic Retinopathy is a medical condition where the retina is damaged because fluid leaks from blood vessels into the retina. The presence of hemorrhages in the retina is the earliest symptom of diabetic retinopathy. The number and shape of hemorrhages is used to indicate the severity of the disease. Early automated hemorrhage detection can help reduce the incidence of blindness. This paper introduced new method depending on the hemorrhage shape to detect the dot hemorrhage (DH), its number, and size at early stage, this can be achieved by reducing the retinal image details. Detection and recognize the DH by following three sequential steps, removing the fovea, removing the vasculature and recognize DH by determining the circularity for all the objects in the image, finally determine the shape factor which is related to DH recognition, this stage strengthens the recognition process. The proposed method recognizes and separates all the DH. version:1
arxiv-1310-5965 | Fusion of Hyperspectral and Panchromatic Images using Spectral Uumixing Results | http://arxiv.org/abs/1310.5965 | id:1310.5965 author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV  published:2013-10-22 summary:Hyperspectral imaging, due to providing high spectral resolution images, is one of the most important tools in the remote sensing field. Because of technological restrictions hyperspectral sensors has a limited spatial resolution. On the other hand panchromatic image has a better spatial resolution. Combining this information together can provide a better understanding of the target scene. Spectral unmixing of mixed pixels in hyperspectral images results in spectral signature and abundance fractions of endmembers but gives no information about their location in a mixed pixel. In this paper we have used spectral unmixing results of hyperspectral images and segmentation results of panchromatic image for data fusion. The proposed method has been applied on simulated data using AVRIS Indian Pines datasets. Results show that this method can effectively combine information in hyperspectral and panchromatic images. version:1
arxiv-1310-5963 | Improving the methods of email classification based on words ontology | http://arxiv.org/abs/1310.5963 | id:1310.5963 author:Foruzan Kiamarzpour, Rouhollah Dianat, Mohammad bahrani, Mehdi Sadeghzadeh category:cs.IR cs.CL  published:2013-10-22 summary:The Internet has dramatically changed the relationship among people and their relationships with others people and made the valuable information available for the users. Email is the service, which the Internet provides today for its own users; this service has attracted most of the users' attention due to the low cost. Along with the numerous benefits of Email, one of the weaknesses of this service is that the number of received emails is continually being enhanced, thus the ways are needed to automatically filter these disturbing letters. Most of these filters utilize a combination of several techniques such as the Black or white List, using the keywords and so on in order to identify the spam more accurately In this paper, we introduce a new method to classify the spam. We are seeking to increase the accuracy of Email classification by combining the output of several decision trees and the concept of ontology. version:1
arxiv-1101-3712 | Generic identification of binary-valued hidden Markov processes | http://arxiv.org/abs/1101.3712 | id:1101.3712 author:Alexander Schönhuth category:math.ST math.AG stat.ML stat.TH  published:2011-01-19 summary:The generic identification problem is to decide whether a stochastic process $(X_t)$ is a hidden Markov process and if yes to infer its parameters for all but a subset of parametrizations that form a lower-dimensional subvariety in parameter space. Partial answers so far available depend on extra assumptions on the processes, which are usually centered around stationarity. Here we present a general solution for binary-valued hidden Markov processes. Our approach is rooted in algebraic statistics hence it is geometric in nature. We find that the algebraic varieties associated with the probability distributions of binary-valued hidden Markov processes are zero sets of determinantal equations which draws a connection to well-studied objects from algebra. As a consequence, our solution allows for algorithmic implementation based on elementary (linear) algebraic routines. version:6
arxiv-1202-6228 | PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification | http://arxiv.org/abs/1202.6228 | id:1202.6228 author:Emilie Morvant, Sokol Koço, Liva Ralaivola category:stat.ML cs.LG  published:2012-02-28 summary:In this work, we propose a PAC-Bayes bound for the generalization risk of the Gibbs classifier in the multi-class classification framework. The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate. Thanks to very recent and beautiful results on matrix concentration inequalities, we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper-bounded by its empirical risk plus a term depending on the number of training examples in each class. To the best of our knowledge, this is the first PAC-Bayes bounds based on confusion matrices. version:6
arxiv-1306-1154 | Sparse Representation of a Polytope and Recovery of Sparse Signals and Low-rank Matrices | http://arxiv.org/abs/1306.1154 | id:1306.1154 author:T. Tony Cai, Anru Zhang category:cs.IT math.IT math.ST stat.ML stat.TH  published:2013-06-05 summary:This paper considers compressed sensing and affine rank minimization in both noiseless and noisy cases and establishes sharp restricted isometry conditions for sparse signal and low-rank matrix recovery. The analysis relies on a key technical tool which represents points in a polytope by convex combinations of sparse vectors. The technique is elementary while leads to sharp results. It is shown that for any given constant $t\ge {4/3}$, in compressed sensing $\delta_{tk}^A < \sqrt{(t-1)/t}$ guarantees the exact recovery of all $k$ sparse signals in the noiseless case through the constrained $\ell_1$ minimization, and similarly in affine rank minimization $\delta_{tr}^\mathcal{M}< \sqrt{(t-1)/t}$ ensures the exact reconstruction of all matrices with rank at most $r$ in the noiseless case via the constrained nuclear norm minimization. Moreover, for any $\epsilon>0$, $\delta_{tk}^A<\sqrt{\frac{t-1}{t}}+\epsilon$ is not sufficient to guarantee the exact recovery of all $k$-sparse signals for large $k$. Similar result also holds for matrix recovery. In addition, the conditions $\delta_{tk}^A < \sqrt{(t-1)/t}$ and $\delta_{tr}^\mathcal{M}< \sqrt{(t-1)/t}$ are also shown to be sufficient respectively for stable recovery of approximately sparse signals and low-rank matrices in the noisy case. version:2
arxiv-1310-5781 | RANSAC: Identification of Higher-Order Geometric Features and Applications in Humanoid Robot Soccer | http://arxiv.org/abs/1310.5781 | id:1310.5781 author:Madison Flannery, Shannon Fenn, David Budden category:cs.RO cs.AI cs.CV  published:2013-10-22 summary:The ability for an autonomous agent to self-localise is directly proportional to the accuracy and precision with which it can perceive salient features within its local environment. The identification of such features by recognising geometric profile allows robustness against lighting variations, which is necessary in most industrial robotics applications. This paper details a framework by which the random sample consensus (RANSAC) algorithm, often applied to parameter fitting in linear models, can be extended to identify higher-order geometric features. Goalpost identification within humanoid robot soccer is investigated as an application, with the developed system yielding an order-of-magnitude improvement in classification performance relative to a traditional histogramming methodology. version:1
arxiv-1310-5767 | Contextual Hypergraph Modelling for Salient Object Detection | http://arxiv.org/abs/1310.5767 | id:1310.5767 author:Xi Li, Yao Li, Chunhua Shen, Anthony Dick, Anton van den Hengel category:cs.CV  published:2013-10-22 summary:Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hypergraph that utilizes a set of hyperedges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyperedges in the hypergraph. The main advantage of hypergraph modeling is that it takes into account each pixel's (or region's) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on center-versus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the state-of-the-art approaches to salient object detection. version:1
arxiv-1310-5755 | Determination, Calculation and Representation of the Upper and Lower Sealing Zones During Virtual Stenting of Aneurysms | http://arxiv.org/abs/1310.5755 | id:1310.5755 author:Jan Egger, Miriam H. A. Bauer, Stefan Großkopf, Christina Biermann, Bernd Freisleben, Christopher Nimsky category:cs.CV physics.med-ph q-bio.TO  published:2013-10-21 summary:In this contribution, a novel method for stent simulation in preoperative computed tomography angiography (CTA) acquisitions of patients is presented where the sealing zones are automatically calculated and visualized. The method is eligible for non-bifurcated and bifurcated stents (Y-stents). Results of the proposed stent simulation with an automatic calculation of the sealing zones for specific diseases (abdominal aortic aneurysms (AAA), thoracic aortic aneurysms (TAA), iliac aneurysms) are presented. The contribution is organized as follows. Section 2 presents the proposed approach. In Section 3, experimental results are discussed. Section 4 concludes the contribution and outlines areas for future work. version:1
arxiv-1310-5738 | A Kernel for Hierarchical Parameter Spaces | http://arxiv.org/abs/1310.5738 | id:1310.5738 author:Frank Hutter, Michael A. Osborne category:stat.ML cs.LG  published:2013-10-21 summary:We define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite. version:1
arxiv-1310-5666 | Distributed parameter estimation of discrete hierarchical models via marginal likelihoods | http://arxiv.org/abs/1310.5666 | id:1310.5666 author:Helene Massam, Nanwei Wang category:stat.ML  published:2013-10-21 summary:We consider discrete graphical models Markov with respect to a graph $G$ and propose two distributed marginal methods to estimate the maximum likelihood estimate of the canonical parameter of the model. Both methods are based on a relaxation of the marginal likelihood obtained by considering the density of the variables represented by a vertex $v$ of $G$ and a neighborhood. The two methods differ by the size of the neighborhood of $v$. We show that the estimates are consistent and that those obtained with the larger neighborhood have smaller asymptotic variance than the ones obtained through the smaller neighborhood. version:1
arxiv-1302-4385 | Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization | http://arxiv.org/abs/1302.4385 | id:1302.4385 author:Nicolas Gillis, Robert Luce category:stat.ML math.OC  published:2013-02-18 summary:Nonnegative matrix factorization (NMF) has been shown recently to be tractable under the separability assumption, under which all the columns of the input data matrix belong to the convex cone generated by only a few of these columns. Bittorf, Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming (LP) model, referred to as Hottopixx, which is robust under any small perturbation of the input matrix. However, Hottopixx has two important drawbacks: (i) the input matrix has to be normalized, and (ii) the factorization rank has to be known in advance. In this paper, we generalize Hottopixx in order to resolve these two drawbacks, that is, we propose a new LP model which does not require normalization and detects the factorization rank automatically. Moreover, the new LP model is more flexible, significantly more tolerant to noise, and can easily be adapted to handle outliers and other noise models. Finally, we show on several synthetic datasets that it outperforms Hottopixx while competing favorably with two state-of-the-art methods. version:2
arxiv-1310-5619 | Devnagari Handwritten Numeral Recognition using Geometric Features and Statistical Combination Classifier | http://arxiv.org/abs/1310.5619 | id:1310.5619 author:Vikas J. Dongre, Vijay H. Mankar category:cs.CV  published:2013-10-21 summary:This paper presents a Devnagari Numerical recognition method based on statistical discriminant functions. 17 geometric features based on pixel connectivity, lines, line directions, holes, image area, perimeter, eccentricity, solidity, orientation etc. are used for representing the numerals. Five discriminant functions viz. Linear, Quadratic, Diaglinear, Diagquadratic and Mahalanobis distance are used for classification. 1500 handwritten numerals are used for training. Another 1500 handwritten numerals are used for testing. Experimental results show that Linear, Quadratic and Mahalanobis discriminant functions provide better results. Results of these three Discriminants are fed to a majority voting type Combination classifier. It is found that Combination classifier offers better results over individual classifiers. version:1
arxiv-1310-1949 | Least Squares Revisited: Scalable Approaches for Multi-class Prediction | http://arxiv.org/abs/1310.1949 | id:1310.1949 author:Alekh Agarwal, Sham M. Kakade, Nikos Karampatziakis, Le Song, Gregory Valiant category:cs.LG stat.ML  published:2013-10-07 summary:This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively large. These robust and parameter free algorithms are essentially iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we present a scalable stagewise variant of our approach, which achieves dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art accuracies. version:2
arxiv-1303-2130 | Convex Discriminative Multitask Clustering | http://arxiv.org/abs/1303.2130 | id:1303.2130 author:Xiao-Lei Zhang category:cs.LG  published:2013-03-08 summary:Multitask clustering tries to improve the clustering performance of multiple tasks simultaneously by taking their relationship into account. Most existing multitask clustering algorithms fall into the type of generative clustering, and none are formulated as convex optimization problems. In this paper, we propose two convex Discriminative Multitask Clustering (DMTC) algorithms to address the problems. Specifically, we first propose a Bayesian DMTC framework. Then, we propose two convex DMTC objectives within the framework. The first one, which can be seen as a technical combination of the convex multitask feature learning and the convex Multiclass Maximum Margin Clustering (M3C), aims to learn a shared feature representation. The second one, which can be seen as a combination of the convex multitask relationship learning and M3C, aims to learn the task relationship. The two objectives are solved in a uniform procedure by the efficient cutting-plane algorithm. Experimental results on a toy problem and two benchmark datasets demonstrate the effectiveness of the proposed algorithms. version:2
arxiv-1310-5568 | Towards Application of the RBNK Model | http://arxiv.org/abs/1310.5568 | id:1310.5568 author:Larry Bull category:cs.CE cs.NE  published:2013-10-21 summary:The computational modeling of genetic regulatory networks is now common place, either by fitting a system to experimental data or by exploring the behaviour of abstract systems with the aim of identifying underlying principles. This paper presents an approach to the latter, considering the response to environmental changes of a well-known model placed upon tunable fitness landscapes. The effects on genome size and gene connectivity are explored. version:1
arxiv-1310-5542 | Ship Detection and Segmentation using Image Correlation | http://arxiv.org/abs/1310.5542 | id:1310.5542 author:Alexander Kadyrov, Hui Yu, Honghai Liu category:cs.CV  published:2013-10-21 summary:There have been intensive research interests in ship detection and segmentation due to high demands on a wide range of civil applications in the last two decades. However, existing approaches, which are mainly based on statistical properties of images, fail to detect smaller ships and boats. Specifically, known techniques are not robust enough in view of inevitable small geometric and photometric changes in images consisting of ships. In this paper a novel approach for ship detection is proposed based on correlation of maritime images. The idea comes from the observation that a fine pattern of the sea surface changes considerably from time to time whereas the ship appearance basically keeps unchanged. We want to examine whether the images have a common unaltered part, a ship in this case. To this end, we developed a method - Focused Correlation (FC) to achieve robustness to geometric distortions of the image content. Various experiments have been conducted to evaluate the effectiveness of the proposed approach. version:1
arxiv-1210-2380 | Stable and robust sampling strategies for compressive imaging | http://arxiv.org/abs/1210.2380 | id:1210.2380 author:Felix Krahmer, Rachel Ward category:cs.CV cs.IT math.IT math.NA  published:2012-10-08 summary:In many signal processing applications, one wishes to acquire images that are sparse in transform domains such as spatial finite differences or wavelets using frequency domain samples. For such applications, overwhelming empirical evidence suggests that superior image reconstruction can be obtained through variable density sampling strategies that concentrate on lower frequencies. The wavelet and Fourier transform domains are not incoherent because low-order wavelets and low-order frequencies are correlated, so compressive sensing theory does not immediately imply sampling strategies and reconstruction guarantees. In this paper we turn to a more refined notion of coherence -- the so-called local coherence -- measuring for each sensing vector separately how correlated it is to the sparsity basis. For Fourier measurements and Haar wavelet sparsity, the local coherence can be controlled and bounded explicitly, so for matrices comprised of frequencies sampled from a suitable inverse square power-law density, we can prove the restricted isometry property with near-optimal embedding dimensions. Consequently, the variable-density sampling strategy we provide allows for image reconstructions that are stable to sparsity defects and robust to measurement noise. Our results cover both reconstruction by $\ell_1$-minimization and by total variation minimization. The local coherence framework developed in this paper should be of independent interest in sparse recovery problems more generally, as it implies that for optimal sparse recovery results, it suffices to have bounded \emph{average} coherence from sensing basis to sparsity basis -- as opposed to bounded maximal coherence -- as long as the sampling strategy is adapted accordingly. version:3
arxiv-1310-5393 | Multi-Task Regularization with Covariance Dictionary for Linear Classifiers | http://arxiv.org/abs/1310.5393 | id:1310.5393 author:Fanyi Xiao, Ruikun Luo, Zhiding Yu category:cs.LG  published:2013-10-21 summary:In this paper we propose a multi-task linear classifier learning problem called D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covariance shared by all tasks to do multi-task knowledge transfer among different tasks. We formally define the learning problem of D-SVM and show two interpretations of this problem, from both the probabilistic and kernel perspectives. From the probabilistic perspective, we show that our learning formulation is actually a MAP estimation on all optimization variables. We also show its equivalence to a multiple kernel learning problem in which one is trying to find a re-weighting kernel for features from a dictionary of basis (despite the fact that only linear classifiers are learned). Finally, we describe an alternative optimization scheme to minimize the objective function and present empirical studies to valid our algorithm. version:1
arxiv-1310-5347 | Bayesian Extensions of Kernel Least Mean Squares | http://arxiv.org/abs/1310.5347 | id:1310.5347 author:Il Memming Park, Sohan Seth, Steven Van Vaerenbergh category:stat.ML cs.LG  published:2013-10-20 summary:The kernel least mean squares (KLMS) algorithm is a computationally efficient nonlinear adaptive filtering method that "kernelizes" the celebrated (linear) least mean squares algorithm. We demonstrate that the least mean squares algorithm is closely related to the Kalman filtering, and thus, the KLMS can be interpreted as an approximate Bayesian filtering method. This allows us to systematically develop extensions of the KLMS by modifying the underlying state-space and observation models. The resulting extensions introduce many desirable properties such as "forgetting", and the ability to learn from discrete data, while retaining the computational simplicity and time complexity of the original algorithm. version:1
arxiv-1310-5249 | Graph-Based Approaches to Clustering Network-Constrained Trajectory Data | http://arxiv.org/abs/1310.5249 | id:1310.5249 author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:cs.LG  published:2013-10-19 summary:Clustering trajectory data attracted considerable attention in the last few years. Most of prior work assumed that moving objects can move freely in an euclidean space and did not consider the eventual presence of an underlying road network and its influence on evaluating the similarity between trajectories. In this paper, we present an approach to clustering such network-constrained trajectory data. More precisely we aim at discovering groups of road segments that are often travelled by the same trajectories. To achieve this end, we model the interactions between segments w.r.t. their similarity as a weighted graph to which we apply a community detection algorithm to discover meaningful clusters. We showcase our proposition through experimental results obtained on synthetic datasets. version:1
arxiv-1310-3447 | Image Restoration using Total Variation with Overlapping Group Sparsity | http://arxiv.org/abs/1310.3447 | id:1310.3447 author:Jun Liu, Ting-Zhu Huang, Ivan W. Selesnick, Xiao-Guang Lv, Po-Yu Chen category:cs.CV math.NA  published:2013-10-13 summary:Image restoration is one of the most fundamental issues in imaging science. Total variation (TV) regularization is widely used in image restoration problems for its capability to preserve edges. In the literature, however, it is also well known for producing staircase-like artifacts. Usually, the high-order total variation (HTV) regularizer is an good option except its over-smoothing property. In this work, we study a minimization problem where the objective includes an usual $l_2$ data-fidelity term and an overlapping group sparsity total variation regularizer which can avoid staircase effect and allow edges preserving in the restored image. We also proposed a fast algorithm for solving the corresponding minimization problem and compare our method with the state-of-the-art TV based methods and HTV based method. The numerical experiments illustrate the efficiency and effectiveness of the proposed method in terms of PSNR, relative error and computing time. version:2
arxiv-1310-5107 | Advances in Hyperspectral Image Classification: Earth monitoring with statistical learning methods | http://arxiv.org/abs/1310.5107 | id:1310.5107 author:Gustavo Camps-Valls, Devis Tuia, Lorenzo Bruzzone, Jón Atli Benediktsson category:cs.CV  published:2013-10-18 summary:Hyperspectral images show similar statistical properties to natural grayscale or color photographic images. However, the classification of hyperspectral images is more challenging because of the very high dimensionality of the pixels and the small number of labeled examples typically available for learning. These peculiarities lead to particular signal processing problems, mainly characterized by indetermination and complex manifolds. The framework of statistical learning has gained popularity in the last decade. New methods have been presented to account for the spatial homogeneity of images, to include user's interaction via active learning, to take advantage of the manifold structure with semisupervised learning, to extract and encode invariances, or to adapt classifiers and image representations to unseen yet similar scenes. This tutuorial reviews the main advances for hyperspectral remote sensing image classification through illustrative examples. version:1
arxiv-1211-6302 | Duality between subgradient and conditional gradient methods | http://arxiv.org/abs/1211.6302 | id:1211.6302 author:Francis Bach category:cs.LG math.OC stat.ML  published:2012-11-27 summary:Given a convex optimization problem and its dual, there are many possible first-order algorithms. In this paper, we show the equivalence between mirror descent algorithms and algorithms generalizing the conditional gradient method. This is done through convex duality, and implies notably that for certain problems, such as for supervised machine learning problems with non-smooth losses or problems regularized by non-smooth regularizers, the primal subgradient method and the dual conditional gradient method are formally equivalent. The dual interpretation leads to a form of line search for mirror descent, as well as guarantees of convergence for primal-dual certificates. version:3
arxiv-1310-5095 | Regularization in Relevance Learning Vector Quantization Using l one Norms | http://arxiv.org/abs/1310.5095 | id:1310.5095 author:Martin Riedel, Marika Kästner, Fabrice Rossi, Thomas Villmann category:stat.ML cs.LG  published:2013-10-18 summary:We propose in this contribution a method for l one regularization in prototype based relevance learning vector quantization (LVQ) for sparse relevance profiles. Sparse relevance profiles in hyperspectral data analysis fade down those spectral bands which are not necessary for classification. In particular, we consider the sparsity in the relevance profile enforced by LASSO optimization. The latter one is obtained by a gradient learning scheme using a differentiable parametrized approximation of the $l_{1}$-norm, which has an upper error bound. We extend this regularization idea also to the matrix learning variant of LVQ as the natural generalization of relevance learning. version:1
arxiv-1310-5089 | Kernel Multivariate Analysis Framework for Supervised Subspace Learning: A Tutorial on Linear and Kernel Multivariate Methods | http://arxiv.org/abs/1310.5089 | id:1310.5089 author:Jerónimo Arenas-García, Kaare Brandt Petersen, Gustavo Camps-Valls, Lars Kai Hansen category:stat.ML cs.LG  published:2013-10-18 summary:Feature extraction and dimensionality reduction are important tasks in many fields of science dealing with signal processing and analysis. The relevance of these techniques is increasing as current sensory devices are developed with ever higher resolution, and problems involving multimodal data sources become more common. A plethora of feature extraction methods are available in the literature collectively grouped under the field of Multivariate Analysis (MVA). This paper provides a uniform treatment of several methods: Principal Component Analysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis (CCA) and Orthonormalized PLS (OPLS), as well as their non-linear extensions derived by means of the theory of reproducing kernel Hilbert spaces. We also review their connections to other methods for classification and statistical dependence estimation, and introduce some recent developments to deal with the extreme cases of large-scale and low-sized problems. To illustrate the wide applicability of these methods in both classification and regression problems, we analyze their performance in a benchmark of publicly available data sets, and pay special attention to specific real applications involving audio processing for music genre prediction and hyperspectral satellite images for Earth and climate monitoring. version:1
arxiv-1310-5082 | On the Suitable Domain for SVM Training in Image Coding | http://arxiv.org/abs/1310.5082 | id:1310.5082 author:Gustavo Camps-Valls, Juan Gutiérrez, Gabriel Gómez-Pérez, Jesús Malo category:cs.CV cs.LG stat.ML  published:2013-10-18 summary:Conventional SVM-based image coding methods are founded on independently restricting the distortion in every image coefficient at some particular image representation. Geometrically, this implies allowing arbitrary signal distortions in an $n$-dimensional rectangle defined by the $\varepsilon$-insensitivity zone in each dimension of the selected image representation domain. Unfortunately, not every image representation domain is well-suited for such a simple, scalar-wise, approach because statistical and/or perceptual interactions between the coefficients may exist. These interactions imply that scalar approaches may induce distortions that do not follow the image statistics and/or are perceptually annoying. Taking into account these relations would imply using non-rectangular $\varepsilon$-insensitivity regions (allowing coupled distortions in different coefficients), which is beyond the conventional SVM formulation. In this paper, we report a condition on the suitable domain for developing efficient SVM image coding schemes. We analytically demonstrate that no linear domain fulfills this condition because of the statistical and perceptual inter-coefficient relations that exist in these domains. This theoretical result is experimentally confirmed by comparing SVM learning in previously reported linear domains and in a recently proposed non-linear perceptual domain that simultaneously reduces the statistical and perceptual relations (so it is closer to fulfilling the proposed condition). These results highlight the relevance of an appropriate choice of the image representation before SVM learning. version:1
arxiv-1305-4525 | Robustness of Random Forest-based gene selection methods | http://arxiv.org/abs/1305.4525 | id:1305.4525 author:Miron B. Kursa category:cs.LG q-bio.QM  published:2013-05-20 summary:Gene selection is an important part of microarray data analysis because it provides information that can lead to a better mechanistic understanding of an investigated phenomenon. At the same time, gene selection is very difficult because of the noisy nature of microarray data. As a consequence, gene selection is often performed with machine learning methods. The Random Forest method is particularly well suited for this purpose. In this work, four state-of-the-art Random Forest-based feature selection methods were compared in a gene selection context. The analysis focused on the stability of selection because, although it is necessary for determining the significance of results, it is often ignored in similar studies. The comparison of post-selection accuracy in the validation of Random Forest classifiers revealed that all investigated methods were equivalent in this context. However, the methods substantially differed with respect to the number of selected genes and the stability of selection. Of the analysed methods, the Boruta algorithm predicted the most genes as potentially important. The post-selection classifier error rate, which is a frequently used measure, was found to be a potentially deceptive measure of gene selection quality. When the number of consistently selected genes was considered, the Boruta algorithm was clearly the best. Although it was also the most computationally intensive method, the Boruta algorithm's computational demands could be reduced to levels comparable to those of other algorithms by replacing the Random Forest importance with a comparable measure from Random Ferns (a similar but simplified classifier). Despite their design assumptions, the minimal optimal selection methods, were found to select a high fraction of false positives. version:3
arxiv-1310-5042 | Distributional semantics beyond words: Supervised learning of analogy and paraphrase | http://arxiv.org/abs/1310.5042 | id:1310.5042 author:Peter D. Turney category:cs.LG cs.AI cs.CL cs.IR H.3.1; I.2.6; I.2.7  published:2013-10-18 summary:There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions). version:1
arxiv-1310-4977 | Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear Spectral Penalties | http://arxiv.org/abs/1310.4977 | id:1310.4977 author:Marco Signoretto, Lieven De Lathauwer, Johan A. K. Suykens category:cs.LG  published:2013-10-18 summary:We present a general framework to learn functions in tensor product reproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on a novel representer theorem suitable for existing as well as new spectral penalties for tensors. When the functions in the TP-RKHS are defined on the Cartesian product of finite discrete sets, in particular, our main problem formulation admits as a special case existing tensor completion problems. Other special cases include transfer learning with multimodal side information and multilinear multitask learning. For the latter case, our kernel-based view is instrumental to derive nonlinear extensions of existing model classes. We give a novel algorithm and show in experiments the usefulness of the proposed extensions. version:1
arxiv-1310-4938 | A Logic-based Approach for Recognizing Textual Entailment Supported by Ontological Background Knowledge | http://arxiv.org/abs/1310.4938 | id:1310.4938 author:Andreas Wotzlaw, Ravi Coote category:cs.CL cs.AI cs.LO  published:2013-10-18 summary:We present the architecture and the evaluation of a new system for recognizing textual entailment (RTE). In RTE we want to identify automatically the type of a logical relation between two input texts. In particular, we are interested in proving the existence of an entailment between them. We conceive our system as a modular environment allowing for a high-coverage syntactic and semantic text analysis combined with logical inference. For the syntactic and semantic analysis we combine a deep semantic analysis with a shallow one supported by statistical models in order to increase the quality and the accuracy of results. For RTE we use logical inference of first-order employing model-theoretic techniques and automated reasoning tools. The inference is supported with problem-relevant background knowledge extracted automatically and on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or other, more experimental sources with, e.g., manually defined presupposition resolutions, or with axiomatized general and common sense knowledge. The results show that fine-grained and consistent knowledge coming from diverse sources is a necessary condition determining the correctness and traceability of results. version:1
arxiv-1310-4909 | Text Classification For Authorship Attribution Analysis | http://arxiv.org/abs/1310.4909 | id:1310.4909 author:M. Sudheep Elayidom, Chinchu Jose, Anitta Puthussery, Neenu K Sasi category:cs.DL cs.CL cs.LG  published:2013-10-18 summary:Authorship attribution mainly deals with undecided authorship of literary texts. Authorship attribution is useful in resolving issues like uncertain authorship, recognize authorship of unknown texts, spot plagiarism so on. Statistical methods can be used to set apart the approach of an author numerically. The basic methodologies that are made use in computational stylometry are word length, sentence length, vocabulary affluence, frequencies etc. Each author has an inborn style of writing, which is particular to himself. Statistical quantitative techniques can be used to differentiate the approach of an author in a numerical way. The problem can be broken down into three sub problems as author identification, author characterization and similarity detection. The steps involved are pre-processing, extracting features, classification and author identification. For this different classifiers can be used. Here fuzzy learning classifier and SVM are used. After author identification the SVM was found to have more accuracy than Fuzzy classifier. Later combined the classifiers to obtain a better accuracy when compared to individual SVM and fuzzy classifier. version:1
arxiv-1310-4891 | Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution | http://arxiv.org/abs/1310.4891 | id:1310.4891 author:Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, Brian C. Lovell category:cs.CV  published:2013-10-18 summary:Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graph-embedding Grassmann discriminant analysis. version:1
arxiv-1310-2700 | Analyzing Big Data with Dynamic Quantum Clustering | http://arxiv.org/abs/1310.2700 | id:1310.2700 author:M. Weinstein, F. Meirer, A. Hume, Ph. Sciau, G. Shaked, R. Hofstetter, E. Persi, A. Mehta, D. Horn category:physics.data-an cs.LG physics.comp-ph  published:2013-10-10 summary:How does one search for a needle in a multi-dimensional haystack without knowing what a needle is and without knowing if there is one in the haystack? This kind of problem requires a paradigm shift - away from hypothesis driven searches of the data - towards a methodology that lets the data speak for itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a powerful visual method that works with big, high-dimensional data. It exploits variations of the density of the data (in feature space) and unearths subsets of the data that exhibit correlations among all the measured variables. The outcome of a DQC analysis is a movie that shows how and why sets of data-points are eventually classified as members of simple clusters or as members of - what we call - extended structures. This allows DQC to be successfully used in a non-conventional exploratory mode where one searches data for unexpected information without the need to model the data. We show how this works for big, complex, real-world datasets that come from five distinct fields: i.e., x-ray nano-chemistry, condensed matter, biology, seismology and finance. These studies show how DQC excels at uncovering unexpected, small - but meaningful - subsets of the data that contain important information. We also establish an important new result: namely, that big, complex datasets often contain interesting structures that will be missed by many conventional clustering techniques. Experience shows that these structures appear frequently enough that it is crucial to know they can exist, and that when they do, they encode important hidden information. In short, we not only demonstrate that DQC can be flexibly applied to datasets that present significantly different challenges, we also show how a simple analysis can be used to look for the needle in the haystack, determine what it is, and find what this means. version:2
arxiv-1310-4759 | Fine-grained Categorization -- Short Summary of our Entry for the ImageNet Challenge 2012 | http://arxiv.org/abs/1310.4759 | id:1310.4759 author:Christoph Göring, Alexander Freytag, Erik Rodner, Joachim Denzler category:cs.CV  published:2013-10-17 summary:In this paper, we tackle the problem of visual categorization of dog breeds, which is a surprisingly challenging task due to simultaneously present low interclass distances and high intra-class variances. Our approach combines several techniques well known in our community but often not utilized for fine-grained recognition: (1) automatic segmentation, (2) efficient part detection, and (3) combination of multiple features. In particular, we demonstrate that a simple head detector embedded in an off-the-shelf recognition pipeline can improve recognition accuracy quite significantly, highlighting the importance of part features for fine-grained recognition tasks. Using our approach, we achieved a 24.59% mean average precision performance on the Stanford dog dataset. version:1
arxiv-1310-4713 | Calibration of an Articulated Camera System with Scale Factor Estimation | http://arxiv.org/abs/1310.4713 | id:1310.4713 author:Junzhou Chen, Kin Hong Wong category:cs.CV cs.CG  published:2013-10-17 summary:Multiple Camera Systems (MCS) have been widely used in many vision applications and attracted much attention recently. There are two principle types of MCS, one is the Rigid Multiple Camera System (RMCS); the other is the Articulated Camera System (ACS). In a RMCS, the relative poses (relative 3-D position and orientation) between the cameras are invariant. While, in an ACS, the cameras are articulated through movable joints, the relative pose between them may change. Therefore, through calibration of an ACS we want to find not only the relative poses between the cameras but also the positions of the joints in the ACS. In this paper, we developed calibration algorithms for the ACS using a simple constraint: the joint is fixed relative to the cameras connected with it during the transformations of the ACS. When the transformations of the cameras in an ACS can be estimated relative to the same coordinate system, the positions of the joints in the ACS can be calculated by solving linear equations. However, in a non-overlapping view ACS, only the ego-transformations of the cameras and can be estimated. We proposed a two-steps method to deal with this problem. In both methods, the ACS is assumed to have performed general transformations in a static environment. The efficiency and robustness of the proposed methods are tested by simulation and real experiments. In the real experiment, the intrinsic and extrinsic parameters of the ACS are obtained simultaneously by our calibration procedure using the same image sequences, no extra data capturing step is required. The corresponding trajectory is recovered and illustrated using the calibration results of the ACS. Since the estimated translations of different cameras in an ACS may scaled by different scale factors, a scale factor estimation algorithm is also proposed. To our knowledge, we are the first to study the calibration of ACS. version:1
arxiv-1310-4579 | Discriminative Link Prediction using Local Links, Node Features and Community Structure | http://arxiv.org/abs/1310.4579 | id:1310.4579 author:Abir De, Niloy Ganguly, Soumen Chakrabarti category:cs.LG cs.SI physics.soc-ph  published:2013-10-17 summary:A link prediction (LP) algorithm is given a graph, and has to rank, for each node, other nodes that are candidates for new linkage. LP is strongly motivated by social search and recommendation applications. LP techniques often focus on global properties (graph conductance, hitting or commute times, Katz score) or local properties (Adamic-Adar and many variations, or node feature vectors), but rarely combine these signals. Furthermore, neither of these extremes exploit link densities at the intermediate level of communities. In this paper we describe a discriminative LP algorithm that exploits two new signals. First, a co-clustering algorithm provides community level link density estimates, which are used to qualify observed links with a surprise value. Second, links in the immediate neighborhood of the link to be predicted are not interpreted at face value, but through a local model of node feature similarities. These signals are combined into a discriminative link predictor. We evaluate the new predictor using five diverse data sets that are standard in the literature. We report on significant accuracy boosts compared to standard LP methods (including Adamic-Adar and random walk). Apart from the new predictor, another contribution is a rigorous protocol for benchmarking and reporting LP algorithms, which reveals the regions of strengths and weaknesses of all the predictors studied here, and establishes the new proposal as the most robust. version:1
arxiv-1310-5008 | Thompson Sampling in Dynamic Systems for Contextual Bandit Problems | http://arxiv.org/abs/1310.5008 | id:1310.5008 author:Tianbing Xu, Yaming Yu, John Turner, Amelia Regan category:cs.LG  published:2013-10-17 summary:We consider the multiarm bandit problems in the timevarying dynamic system for rich structural features. For the nonlinear dynamic model, we propose the approximate inference for the posterior distributions based on Laplace Approximation. For the context bandit problems, Thompson Sampling is adopted based on the underlying posterior distributions of the parameters. More specifically, we introduce the discount decays on the previous samples impact and analyze the different decay rates with the underlying sample dynamics. Consequently, the exploration and exploitation is adaptively tradeoff according to the dynamics in the system. version:1
arxiv-1310-5007 | Online Classification Using a Voted RDA Method | http://arxiv.org/abs/1310.5007 | id:1310.5007 author:Tianbing Xu, Jianfeng Gao, Lin Xiao, Amelia Regan category:cs.LG stat.ML  published:2013-10-17 summary:We propose a voted dual averaging method for online classification problems with explicit regularization. This method employs the update rule of the regularized dual averaging (RDA) method, but only on the subsequence of training examples where a classification error is made. We derive a bound on the number of mistakes made by this method on the training set, as well as its generalization error rate. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and generalization performance. We experimented with the method using $\ell_1$ regularization on a large-scale natural language processing task, and obtained state-of-the-art classification performance with fairly sparse models. version:1
arxiv-1311-0202 | A systematic comparison of supervised classifiers | http://arxiv.org/abs/1311.0202 | id:1311.0202 author:D. R. Amancio, C. H. Comin, D. Casanova, G. Travieso, O. M. Bruno, F. A. Rodrigues, L. da F. Costa category:cs.LG  published:2013-10-17 summary:Pattern recognition techniques have been employed in a myriad of industrial, medical, commercial and academic applications. To tackle such a diversity of data, many techniques have been devised. However, despite the long tradition of pattern recognition research, there is no technique that yields the best classification in all scenarios. Therefore, the consideration of as many as possible techniques presents itself as an fundamental practice in applications aiming at high accuracy. Typical works comparing methods either emphasize the performance of a given algorithm in validation tests or systematically compare various algorithms, assuming that the practical use of these methods is done by experts. In many occasions, however, researchers have to deal with their practical classification tasks without an in-depth knowledge about the underlying mechanisms behind parameters. Actually, the adequate choice of classifiers and parameters alike in such practical circumstances constitutes a long-standing problem and is the subject of the current paper. We carried out a study on the performance of nine well-known classifiers implemented by the Weka framework and compared the dependence of the accuracy with their configuration parameter configurations. The analysis of performance with default parameters revealed that the k-nearest neighbors method exceeds by a large margin the other methods when high dimensional datasets are considered. When other configuration of parameters were allowed, we found that it is possible to improve the quality of SVM in more than 20% even if parameters are set randomly. Taken together, the investigation conducted in this paper suggests that, apart from the SVM implementation, Weka's default configuration of parameters provides an performance close the one achieved with the optimal configuration. version:1
arxiv-1302-7220 | A New Monte Carlo Based Algorithm for the Gaussian Process Classification Problem | http://arxiv.org/abs/1302.7220 | id:1302.7220 author:Amir F. Atiya, Hatem A. Fayed, Ahmed H. Abdel-Gawad category:stat.ML  published:2013-02-28 summary:Gaussian process is a very promising novel technology that has been applied to both the regression problem and the classification problem. While for the regression problem it yields simple exact solutions, this is not the case for the classification problem, because we encounter intractable integrals. In this paper we develop a new derivation that transforms the problem into that of evaluating the ratio of multivariate Gaussian orthant integrals. Moreover, we develop a new Monte Carlo procedure that evaluates these integrals. It is based on some aspects of bootstrap sampling and acceptancerejection. The proposed approach has beneficial properties compared to the existing Markov Chain Monte Carlo approach, such as simplicity, reliability, and speed. version:2
arxiv-1310-4546 | Distributed Representations of Words and Phrases and their Compositionality | http://arxiv.org/abs/1310.4546 | id:1310.4546 author:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean category:cs.CL cs.LG stat.ML  published:2013-10-16 summary:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible. version:1
arxiv-1310-4456 | Inference, Sampling, and Learning in Copula Cumulative Distribution Networks | http://arxiv.org/abs/1310.4456 | id:1310.4456 author:Stefan Douglas Webb category:stat.ML cs.LG  published:2013-10-16 summary:The cumulative distribution network (CDN) is a recently developed class of probabilistic graphical models (PGMs) permitting a copula factorization, in which the CDF, rather than the density, is factored. Despite there being much recent interest within the machine learning community about copula representations, there has been scarce research into the CDN, its amalgamation with copula theory, and no evaluation of its performance. Algorithms for inference, sampling, and learning in these models are underdeveloped compared those of other PGMs, hindering widerspread use. One advantage of the CDN is that it allows the factors to be parameterized as copulae, combining the benefits of graphical models with those of copula theory. In brief, the use of a copula parameterization enables greater modelling flexibility by separating representation of the marginals from the dependence structure, permitting more efficient and robust learning. Another advantage is that the CDN permits the representation of implicit latent variables, whose parameterization and connectivity are not required to be specified. Unfortunately, that the model can encode only latent relationships between variables severely limits its utility. In this thesis, we present inference, learning, and sampling for CDNs, and further the state-of-the-art. First, we explain the basics of copula theory and the representation of copula CDNs. Then, we discuss inference in the models, and develop the first sampling algorithm. We explain standard learning methods, propose an algorithm for learning from data missing completely at random (MCAR), and develop a novel algorithm for learning models of arbitrary treewidth and size. Properties of the models and algorithms are investigated through Monte Carlo simulations. We conclude with further discussion of the advantages and limitations of CDNs, and suggest future work. version:1
arxiv-1310-4495 | Multiple Attractor Cellular Automata (MACA) for Addressing Major Problems in Bioinformatics | http://arxiv.org/abs/1310.4495 | id:1310.4495 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi Nedunuri category:cs.CE cs.LG  published:2013-10-16 summary:CA has grown as potential classifier for addressing major problems in bioinformatics. Lot of bioinformatics problems like predicting the protein coding region, finding the promoter region, predicting the structure of protein and many other problems in bioinformatics can be addressed through Cellular Automata. Even though there are some prediction techniques addressing these problems, the approximate accuracy level is very less. An automated procedure was proposed with MACA (Multiple Attractor Cellular Automata) which can address all these problems. The genetic algorithm is also used to find rules with good fitness values. Extensive experiments are conducted for reporting the accuracy of the proposed tool. The average accuracy of MACA when tested with ENCODE, BG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%. version:1
arxiv-1310-4366 | An FCA-based Boolean Matrix Factorisation for Collaborative Filtering | http://arxiv.org/abs/1310.4366 | id:1310.4366 author:Elena Nenova, Dmitry I. Ignatov, Andrey V. Konstantinov category:cs.IR cs.DS stat.ML 06B99  03G10  15B34 H.2.8; H.2.3  published:2013-10-16 summary:We propose a new approach for Collaborative Filtering which is based on Boolean Matrix Factorisation (BMF) and Formal Concept Analysis. In a series of experiments on real data (Movielens dataset) we compare the approach with the SVD- and NMF-based algorithms in terms of Mean Average Error (MAE). One of the experimental consequences is that it is enough to have a binary-scaled rating data to obtain almost the same quality in terms of MAE by BMF than for the SVD-based algorithm in case of non-scaled data. version:1
arxiv-1310-4362 | Bayesian Information Sharing Between Noise And Regression Models Improves Prediction of Weak Effects | http://arxiv.org/abs/1310.4362 | id:1310.4362 author:Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J Kangas, Pasi Soininen, Marjo-Riitta Järvelin, Mika Ala-Korpela, Samuel Kaski category:stat.ML cs.LG  published:2013-10-16 summary:We consider the prediction of weak effects in a multiple-output regression setup, when covariates are expected to explain a small amount, less than $\approx 1%$, of the variance of the target variables. To facilitate the prediction of the weak effects, we constrain our model structure by introducing a novel Bayesian approach of sharing information between the regression model and the noise model. Further reduction of the effective number of parameters is achieved by introducing an infinite shrinkage prior and group sparsity in the context of the Bayesian reduced rank regression, and using the Bayesian infinite factor model as a flexible low-rank noise model. In our experiments the model incorporating the novelties outperformed alternatives in genomic prediction of rich phenotype data. In particular, the information sharing between the noise and regression models led to significant improvement in prediction accuracy. version:1
arxiv-1304-7284 | Supervised Heterogeneous Multiview Learning for Joint Association Study and Disease Diagnosis | http://arxiv.org/abs/1304.7284 | id:1304.7284 author:Shandian Zhe, Zenglin Xu, Yuan Qi category:cs.LG cs.CE stat.ML  published:2013-04-26 summary:Given genetic variations and various phenotypical traits, such as Magnetic Resonance Imaging (MRI) features, we consider two important and related tasks in biomedical research: i)to select genetic and phenotypical markers for disease diagnosis and ii) to identify associations between genetic and phenotypical data. These two tasks are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. While a variety of sparse models have been applied for disease diagnosis and canonical correlation analysis and its extensions have bee widely used in association studies (e.g., eQTL analysis), these two tasks have been treated separately. To unify these two tasks, we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels based on Gaussian process ordinal regression; in return, the disease status is used to guide the discovery of relationships between the data sources. The sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. To learn the model from data, we develop an efficient variational expectation maximization algorithm. Simulation results demonstrate that our approach achieves higher accuracy in both predicting ordinal labels and discovering associations between data sources than alternative methods. We apply our approach to an imaging genetics dataset for the study of Alzheimer's Disease (AD). Our method identifies biologically meaningful relationships between genetic variations, MRI features, and AD status, and achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods. version:2
arxiv-1310-4252 | Multilabel Consensus Classification | http://arxiv.org/abs/1310.4252 | id:1310.4252 author:Sihong Xie, Xiangnan Kong, Jing Gao, Wei Fan, Philip S. Yu category:stat.ML cs.LG  published:2013-10-16 summary:In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources to obtain the final predictions without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods. version:1
arxiv-1306-4391 | On the Fundamental Limits of Recovering Tree Sparse Vectors from Noisy Linear Measurements | http://arxiv.org/abs/1306.4391 | id:1306.4391 author:Akshay Soni, Jarvis Haupt category:cs.IT math.IT math.ST stat.ML stat.TH  published:2013-06-18 summary:Recent breakthrough results in compressive sensing (CS) have established that many high dimensional signals can be accurately recovered from a relatively small number of non-adaptive linear observations, provided that the signals possess a sparse representation in some basis. Subsequent efforts have shown that the performance of CS can be improved by exploiting additional structure in the locations of the nonzero signal coefficients during inference, or by utilizing some form of data-dependent adaptive measurement focusing during the sensing process. To our knowledge, our own previous work was the first to establish the potential benefits that can be achieved when fusing the notions of adaptive sensing and structured sparsity -- that work examined the task of support recovery from noisy linear measurements, and established that an adaptive sensing strategy specifically tailored to signals that are tree-sparse can significantly outperform adaptive and non-adaptive sensing strategies that are agnostic to the underlying structure. In this work we establish fundamental performance limits for the task of support recovery of tree-sparse signals from noisy measurements, in settings where measurements may be obtained either non-adaptively (using a randomized Gaussian measurement strategy motivated by initial CS investigations) or by any adaptive sensing strategy. Our main results here imply that the adaptive tree sensing procedure analyzed in our previous work is nearly optimal, in the sense that no other sensing and estimation strategy can perform fundamentally better for identifying the support of tree-sparse signals. version:2
arxiv-1304-5583 | Distributed Low-rank Subspace Segmentation | http://arxiv.org/abs/1304.5583 | id:1304.5583 author:Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I. Jordan category:cs.CV cs.DC cs.LG stat.ML  published:2013-04-20 summary:Vision problems ranging from image clustering to motion segmentation to semi-supervised learning can naturally be framed as subspace segmentation problems, in which one aims to recover multiple low-dimensional subspaces from noisy and corrupted input data. Low-Rank Representation (LRR), a convex formulation of the subspace segmentation problem, is provably and empirically accurate on small problems but does not scale to the massive sizes of modern vision datasets. Moreover, past work aimed at scaling up low-rank matrix factorization is not applicable to LRR given its non-decomposable constraints. In this work, we propose a novel divide-and-conquer algorithm for large-scale subspace segmentation that can cope with LRR's non-decomposable constraints and maintains LRR's strong recovery guarantees. This has immediate implications for the scalability of subspace segmentation, which we demonstrate on a benchmark face recognition dataset and in simulations. We then introduce novel applications of LRR-based subspace segmentation to large-scale semi-supervised learning for multimedia event detection, concept detection, and image tagging. In each case, we obtain state-of-the-art results and order-of-magnitude speed ups. version:2
arxiv-1310-4227 | On Measure Concentration of Random Maximum A-Posteriori Perturbations | http://arxiv.org/abs/1310.4227 | id:1310.4227 author:Francesco Orabona, Tamir Hazan, Anand D. Sarwate, Tommi Jaakkola category:cs.LG math.PR  published:2013-10-15 summary:The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving expected estimations. version:1
arxiv-1302-3446 | Adaptive Temporal Compressive Sensing for Video | http://arxiv.org/abs/1302.3446 | id:1302.3446 author:Xin Yuan, Jianbo Yang, Patrick Llull, Xuejun Liao, Guillermo Sapiro, David J. Brady, Lawrence Carin category:stat.AP cs.CV cs.MM  published:2013-02-14 summary:This paper introduces the concept of adaptive temporal compressive sensing (CS) for video. We propose a CS algorithm to adapt the compression ratio based on the scene's temporal complexity, computed from the compressed data, without compromising the quality of the reconstructed video. The temporal adaptivity is manifested by manipulating the integration time of the camera, opening the possibility to real-time implementation. The proposed algorithm is a generalized temporal CS approach that can be incorporated with a diverse set of existing hardware systems. version:3
arxiv-1310-4223 | Exact Learning of RNA Energy Parameters From Structure | http://arxiv.org/abs/1310.4223 | id:1310.4223 author:Hamidreza Chitsaz, Mohammad Aminisharifabad category:q-bio.BM cs.LG  published:2013-10-15 summary:We consider the problem of exact learning of parameters of a linear RNA energy model from secondary structure data. A necessary and sufficient condition for learnability of parameters is derived, which is based on computing the convex hull of union of translated Newton polytopes of input sequences. The set of learned energy parameters is characterized as the convex cone generated by the normal vectors to those facets of the resulting polytope that are incident to the origin. In practice, the sufficient condition may not be satisfied by the entire training data set; hence, computing a maximal subset of training data for which the sufficient condition is satisfied is often desired. We show that problem is NP-hard in general for an arbitrary dimensional feature space. Using a randomized greedy algorithm, we select a subset of RNA STRAND v2.0 database that satisfies the sufficient condition for separate A-U, C-G, G-U base pair counting model. The set of learned energy parameters includes experimentally measured energies of A-U, C-G, and G-U pairs; hence, our parameter set is in agreement with the Turner parameters. version:1
arxiv-1310-4217 | Optimal Sensor Placement and Enhanced Sparsity for Classification | http://arxiv.org/abs/1310.4217 | id:1310.4217 author:B. W. Brunton, S. L. Brunton, J. L. Proctor, J. N. Kutz category:cs.CV  published:2013-10-15 summary:The goal of compressive sensing is efficient reconstruction of data from few measurements, sometimes leading to a categorical decision. If only classification is required, reconstruction can be circumvented and the measurements needed are orders-of-magnitude sparser still. We define enhanced sparsity as the reduction in number of measurements required for classification over reconstruction. In this work, we exploit enhanced sparsity and learn spatial sensor locations that optimally inform a categorical decision. The algorithm solves an l1-minimization to find the fewest entries of the full measurement vector that exactly reconstruct the discriminant vector in feature space. Once the sensor locations have been identified from the training data, subsequent test samples are classified with remarkable efficiency, achieving performance comparable to that obtained by discrimination using the full image. Sensor locations may be learned from full images, or from a random subsample of pixels. For classification between more than two categories, we introduce a coupling parameter whose value tunes the number of sensors selected, trading accuracy for economy. We demonstrate the algorithm on example datasets from image recognition using PCA for feature extraction and LDA for discrimination; however, the method can be broadly applied to non-image data and adapted to work with other methods for feature extraction and discrimination. version:1
arxiv-1302-0336 | Sharp Inequalities for $f$-divergences | http://arxiv.org/abs/1302.0336 | id:1302.0336 author:Adityanand Guntuboyina, Sujayam Saha, Geoffrey Schiebinger category:math.ST cs.IT math.IT math.OC math.PR stat.ML stat.TH  published:2013-02-02 summary:$f$-divergences are a general class of divergences between probability measures which include as special cases many commonly used divergences in probability, mathematical statistics and information theory such as Kullback-Leibler divergence, chi-squared divergence, squared Hellinger distance, total variation distance etc. In this paper, we study the problem of maximizing or minimizing an $f$-divergence between two probability measures subject to a finite number of constraints on other $f$-divergences. We show that these infinite-dimensional optimization problems can all be reduced to optimization problems over small finite dimensional spaces which are tractable. Our results lead to a comprehensive and unified treatment of the problem of obtaining sharp inequalities between $f$-divergences. We demonstrate that many of the existing results on inequalities between $f$-divergences can be obtained as special cases of our results and we also improve on some existing non-sharp inequalities. version:2
arxiv-1310-4485 | The BeiHang Keystroke Dynamics Authentication System | http://arxiv.org/abs/1310.4485 | id:1310.4485 author:Juan Liu, Baochang Zhang, Linlin Shen, Jianzhuang Liu, Jason Zhao category:cs.CR cs.LG  published:2013-10-15 summary:Keystroke Dynamics is an important biometric solution for person authentication. Based upon keystroke dynamics, this paper designs an embedded password protection device, develops an online system, collects two public databases for promoting the research on keystroke authentication, exploits the Gabor filter bank to characterize keystroke dynamics, and provides benchmark results of three popular classification algorithms, one-class support vector machine, Gaussian classifier, and nearest neighbour classifier. version:1
arxiv-1202-2723 | Sparse Matrix Inversion with Scaled Lasso | http://arxiv.org/abs/1202.2723 | id:1202.2723 author:Tingni Sun, Cun-Hui Zhang category:math.ST stat.ML stat.TH  published:2012-02-13 summary:We propose a new method of learning a sparse nonnegative-definite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm first estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other $\ell_1$ regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the $\ell_1$ and spectrum norms of the target inverse matrix diverges to infinity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. version:2
arxiv-1310-3805 | Green Heron Swarm Optimization Algorithm - State-of-the-Art of a New Nature Inspired Discrete Meta-Heuristics | http://arxiv.org/abs/1310.3805 | id:1310.3805 author:Chiranjib Sur, Anupam Shukla category:cs.NE  published:2013-10-14 summary:Many real world problems are NP-Hard problems are a very large part of them can be represented as graph based problems. This makes graph theory a very important and prevalent field of study. In this work a new bio-inspired meta-heuristics called Green Heron Swarm Optimization (GHOSA) Algorithm is being introduced which is inspired by the fishing skills of the bird. The algorithm basically suited for graph based problems like combinatorial optimization etc. However introduction of an adaptive mathematical variation operator called Location Based Neighbour Influenced Variation (LBNIV) makes it suitable for high dimensional continuous domain problems. The new algorithm is being operated on the traditional benchmark equations and the results are compared with Genetic Algorithm and Particle Swarm Optimization. The algorithm is also operated on Travelling Salesman Problem, Quadratic Assignment Problem, Knapsack Problem dataset. The procedure to operate the algorithm on the Resource Constraint Shortest Path and road network optimization is also discussed. The results clearly demarcates the GHOSA algorithm as an efficient algorithm specially considering that the number of algorithms for the discrete optimization is very low and robust and more explorative algorithm is required in this age of social networking and mostly graph based problem scenarios. version:1
arxiv-1310-3717 | Misfire Detection in IC Engine using Kstar Algorithm | http://arxiv.org/abs/1310.3717 | id:1310.3717 author:Anish Bahri, V Sugumaran, S Babu Devasenapati category:cs.CV  published:2013-10-14 summary:Misfire in an IC Engine continues to be a problem leading to reduced fuel efficiency, increased power loss and emissions containing heavy concentration of hydrocarbons. Misfiring creates a unique vibration pattern attributed to a particular cylinder. Useful features can be extracted from these patterns and can be analyzed to detect misfire. Statistical features from these vibration signals were extracted. Out of these, useful features were identified using the J48 decision tree algorithm and selected features were used for classification using the Kstar algorithm. In this paper performance analysis of Kstar algorithm is presented. version:1
arxiv-1308-6797 | Online Ranking: Discrete Choice, Spearman Correlation and Other Feedback | http://arxiv.org/abs/1308.6797 | id:1308.6797 author:Nir Ailon category:cs.LG cs.GT stat.ML  published:2013-08-30 summary:Given a set $V$ of $n$ objects, an online ranking system outputs at each time step a full ranking of the set, observes a feedback of some form and suffers a loss. We study the setting in which the (adversarial) feedback is an element in $V$, and the loss is the position (0th, 1st, 2nd...) of the item in the outputted ranking. More generally, we study a setting in which the feedback is a subset $U$ of at most $k$ elements in $V$, and the loss is the sum of the positions of those elements. We present an algorithm of expected regret $O(n^{3/2}\sqrt{Tk})$ over a time horizon of $T$ steps with respect to the best single ranking in hindsight. This improves previous algorithms and analyses either by a factor of either $\Omega(\sqrt{k})$, a factor of $\Omega(\sqrt{\log n})$ or by improving running time from quadratic to $O(n\log n)$ per round. We also prove a matching lower bound. Our techniques also imply an improved regret bound for online rank aggregation over the Spearman correlation measure, and to other more complex ranking loss functions. version:5
arxiv-1310-3697 | Variance Adjusted Actor Critic Algorithms | http://arxiv.org/abs/1310.3697 | id:1310.3697 author:Aviv Tamar, Shie Mannor category:stat.ML cs.LG cs.SY  published:2013-10-14 summary:We present an actor-critic framework for MDPs where the objective is the variance-adjusted expected return. Our critic uses linear function approximation, and we extend the concept of compatible features to the variance-adjusted setting. We present an episodic actor-critic algorithm and show that it converges almost surely to a locally optimal point of the objective function. version:1
arxiv-1310-3607 | Predicting college basketball match outcomes using machine learning techniques: some results and lessons learned | http://arxiv.org/abs/1310.3607 | id:1310.3607 author:Albrecht Zimmermann, Sruthi Moorthy, Zifan Shi category:cs.LG stat.AP  published:2013-10-14 summary:Most existing work on predicting NCAAB matches has been developed in a statistical context. Trusting the capabilities of ML techniques, particularly classification learners, to uncover the importance of features and learn their relationships, we evaluated a number of different paradigms on this task. In this paper, we summarize our work, pointing out that attributes seem to be more important than models, and that there seems to be an upper limit to predictive quality. version:1
arxiv-1307-2855 | Flow-Based Algorithms for Local Graph Clustering | http://arxiv.org/abs/1307.2855 | id:1307.2855 author:Lorenzo Orecchia, Zeyuan Allen Zhu category:cs.DS cs.LG stat.ML  published:2013-07-10 summary:Given a subset S of vertices of an undirected graph G, the cut-improvement problem asks us to find a subset S that is similar to A but has smaller conductance. A very elegant algorithm for this problem has been given by Andersen and Lang [AL08] and requires solving a small number of single-commodity maximum flow computations over the whole graph G. In this paper, we introduce LocalImprove, the first cut-improvement algorithm that is local, i.e. that runs in time dependent on the size of the input set A rather than on the size of the entire graph. Moreover, LocalImprove achieves this local behaviour while essentially matching the same theoretical guarantee as the global algorithm of Andersen and Lang. The main application of LocalImprove is to the design of better local-graph-partitioning algorithms. All previously known local algorithms for graph partitioning are random-walk based and can only guarantee an output conductance of O(\sqrt{OPT}) when the target set has conductance OPT \in [0,1]. Very recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT / \sqrt{CONN}) where the internal connectivity parameter CONN \in [0,1] is defined as the reciprocal of the mixing time of the random walk over the graph induced by the target set. In this work, we show how to use LocalImprove to obtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This yields the first flow-based algorithm. Moreover, its performance strictly outperforms the ones based on random walks and surprisingly matches that of the best known global algorithm, which is SDP-based, in this parameter regime [MMV12]. Finally, our results show that spectral methods are not the only viable approach to the construction of local graph partitioning algorithm and open door to the study of algorithms with even better approximation and locality guarantees. version:2
arxiv-1310-3500 | Can Twitter Predict Royal Baby's Name ? | http://arxiv.org/abs/1310.3500 | id:1310.3500 author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.CY  published:2013-10-13 summary:In this paper, we analyze the existence of possible correlation between public opinion of twitter users and the decision-making of persons who are influential in the society. We carry out this analysis on the example of the discussion of probable name of the British crown baby, born in July, 2013. In our study, we use the methods of quantitative processing of natural language, the theory of frequent sets, the algorithms of visual displaying of users' communities. We also analyzed the time dynamics of keyword frequencies. The analysis showed that the main predictable name was dominating in the spectrum of names before the official announcement. Using the theories of frequent sets, we showed that the full name consisting of three component names was the part of top 5 by the value of support. It was revealed that the structure of dynamically formed users' communities participating in the discussion is determined by only a few leaders who influence significantly the viewpoints of other users. version:1
arxiv-1310-3499 | Forecasting of Events by Tweet Data Mining | http://arxiv.org/abs/1310.3499 | id:1310.3499 author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.CY  published:2013-10-13 summary:This paper describes the analysis of quantitative characteristics of frequent sets and association rules in the posts of Twitter microblogs related to different event discussions. For the analysis, we used a theory of frequent sets, association rules and a theory of formal concept analysis. We revealed the frequent sets and association rules which characterize the semantic relations between the concepts of analyzed subjects. The support of some frequent sets reaches its global maximum before the expected event but with some time delay. Such frequent sets may be considered as predictive markers that characterize the significance of expected events for blogosphere users. We showed that the time dynamics of confidence in some revealed association rules can also have predictive characteristics. Exceeding a certain threshold may be a signal for corresponding reaction in the society within the time interval between the maximum and the probable coming of an event. In this paper, we considered two types of events: the Olympic tennis tournament final in London, 2012 and the prediction of Eurovision 2013 winner. version:1
arxiv-1310-3492 | Predicting Social Links for New Users across Aligned Heterogeneous Social Networks | http://arxiv.org/abs/1310.3492 | id:1310.3492 author:Jiawei Zhang, Xiangnan Kong, Philip S. Yu category:cs.SI cs.LG physics.soc-ph  published:2013-10-13 summary:Online social networks have gained great success in recent years and many of them involve multiple kinds of nodes and complex relationships. Among these relationships, social links among users are of great importance. Many existing link prediction methods focus on predicting social links that will appear in the future among all users based upon a snapshot of the social network. In real-world social networks, many new users are joining in the service every day. Predicting links for new users are more important. Different from conventional link prediction problems, link prediction for new users are more challenging due to the following reasons: (1) differences in information distributions between new users and the existing active users (i.e., old users); (2) lack of information from the new users in the network. We propose a link prediction method called SCAN-PS (Supervised Cross Aligned Networks link prediction with Personalized Sampling), to solve the link prediction problem for new users with information transferred from both the existing active users in the target network and other source networks through aligned accounts. We proposed a within-target-network personalized sampling method to process the existing active users' information in order to accommodate the differences in information distributions before the intra-network knowledge transfer. SCAN-PS can also exploit information in other source networks, where the user accounts are aligned with the target network. In this way, SCAN-PS could solve the cold start problem when information of these new users is total absent in the target network. version:1
arxiv-1304-1014 | A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale SVM Training | http://arxiv.org/abs/1304.1014 | id:1304.1014 author:Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori category:cs.CV cs.AI cs.LG math.OC stat.ML  published:2013-04-03 summary:Recently, there has been a renewed interest in the machine learning community for variants of a sparse greedy approximation procedure for concave optimization known as {the Frank-Wolfe (FW) method}. In particular, this procedure has been successfully applied to train large-scale instances of non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has allowed to obtain efficient algorithms but also important theoretical results, including convergence analysis of training algorithms and new characterizations of model sparsity. In this paper, we present and analyze a novel variant of the FW method based on a new way to perform away steps, a classic strategy used to accelerate the convergence of the basic FW procedure. Our formulation and analysis is focused on a general concave maximization problem on the simplex. However, the specialization of our algorithm to quadratic forms is strongly related to some classic methods in computational geometry, namely the Gilbert and MDM algorithms. On the theoretical side, we demonstrate that the method matches the guarantees in terms of convergence rate and number of iterations obtained by using classic away steps. In particular, the method enjoys a linear rate of convergence, a result that has been recently proved for MDM on quadratic forms. On the practical side, we provide experiments on several classification datasets, and evaluate the results using statistical tests. Experiments show that our method is faster than the FW method with classic away steps, and works well even in the cases in which classic away steps slow down the algorithm. Furthermore, these improvements are obtained without sacrificing the predictive accuracy of the obtained SVM model. version:2
arxiv-1310-3452 | Dense Scattering Layer Removal | http://arxiv.org/abs/1310.3452 | id:1310.3452 author:Qiong Yan, Li Xu, Jiaya Jia category:cs.CV I.4.1  published:2013-10-13 summary:We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved. version:1
arxiv-1108-0353 | Cross-moments computation for stochastic context-free grammars | http://arxiv.org/abs/1108.0353 | id:1108.0353 author:Velimir M. Ilic, Miroslav D. Ciric, Miomir S. Stankovic category:cs.CL  published:2011-08-01 summary:In this paper we consider the problem of efficient computation of cross-moments of a vector random variable represented by a stochastic context-free grammar. Two types of cross-moments are discussed. The sample space for the first one is the set of all derivations of the context-free grammar, and the sample space for the second one is the set of all derivations which generate a string belonging to the language of the grammar. In the past, this problem was widely studied, but mainly for the cross-moments of scalar variables and up to the second order. This paper presents new algorithms for computing the cross-moments of an arbitrary order, and the previously developed ones are derived as special cases. version:2
arxiv-1305-0665 | Spectral Classification Using Restricted Boltzmann Machine | http://arxiv.org/abs/1305.0665 | id:1305.0665 author:Fuqiang Chen, Yan Wu, Yude Bu, Guodong Zhao category:cs.LG  published:2013-05-03 summary:In this study, a novel machine learning algorithm, restricted Boltzmann machine (RBM), is introduced. The algorithm is applied for the spectral classification in astronomy. RBM is a bipartite generative graphical model with two separate layers (one visible layer and one hidden layer), which can extract higher level features to represent the original data. Despite generative, RBM can be used for classification when modified with a free energy and a soft-max function. Before spectral classification, the original data is binarized according to some rule. Then we resort to the binary RBM to classify cataclysmic variables (CVs) and non-CVs (one half of all the given data for training and the other half for testing). The experiment result shows state-of-the-art accuracy of 100%, which indicates the efficiency of the binary RBM algorithm. version:2
arxiv-1209-3442 | Negative Binomial Process Count and Mixture Modeling | http://arxiv.org/abs/1209.3442 | id:1209.3442 author:Mingyuan Zhou, Lawrence Carin category:stat.ME stat.ML  published:2012-09-15 summary:The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural and computational advantages of the NB process. A variety of NB processes, including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters. version:3
arxiv-1310-3438 | On Optimal Probabilities in Stochastic Coordinate Descent Methods | http://arxiv.org/abs/1310.3438 | id:1310.3438 author:Peter Richtárik, Martin Takáč category:stat.ML cs.DC math.OC  published:2013-10-13 summary:We propose and analyze a new parallel coordinate descent method---`NSync---in which at each iteration a random subset of coordinates is updated, in parallel, allowing for the subsets to be chosen non-uniformly. We derive convergence rates under a strong convexity assumption, and comment on how to assign probabilities to the sets to optimize the bound. The complexity and practical performance of the method can outperform its uniform variant by an order of magnitude. Surprisingly, the strategy of updating a single randomly selected coordinate per iteration---with optimal probabilities---may require less iterations, both in theory and practice, than the strategy of updating all coordinates at every iteration. version:1
arxiv-1310-3407 | Joint Indoor Localization and Radio Map Construction with Limited Deployment Load | http://arxiv.org/abs/1310.3407 | id:1310.3407 author:Sameh Sorour, Yves Lostanlen, Shahrokh Valaee category:cs.NI cs.LG  published:2013-10-12 summary:One major bottleneck in the practical implementation of received signal strength (RSS) based indoor localization systems is the extensive deployment efforts required to construct the radio maps through fingerprinting. In this paper, we aim to design an indoor localization scheme that can be directly employed without building a full fingerprinted radio map of the indoor environment. By accumulating the information of localized RSSs, this scheme can also simultaneously construct the radio map with limited calibration. To design this scheme, we employ a source data set that possesses the same spatial correlation of the RSSs in the indoor environment under study. The knowledge of this data set is then transferred to a limited number of calibration fingerprints and one or several RSS observations with unknown locations, in order to perform direct localization of these observations using manifold alignment. We test two different source data sets, namely a simulated radio propagation map and the environments plan coordinates. For moving users, we exploit the correlation of their observations to improve the localization accuracy. The online testing in two indoor environments shows that the plan coordinates achieve better results than the simulated radio maps, and a negligible degradation with 70-85% reduction in calibration load. version:1
arxiv-1304-3760 | Identification of biologically relevant subtypes via preweighted sparse clustering | http://arxiv.org/abs/1304.3760 | id:1304.3760 author:Sheila Gaynor, Eric Bair category:stat.ME cs.LG q-bio.QM stat.AP stat.ML  published:2013-04-13 summary:Cluster analysis methods are used to identify homogeneous subgroups in a data set. Frequently one applies cluster analysis in order to identify biologically interesting subgroups. In particular, one may wish to identify subgroups that are associated with a particular outcome of interest. Conventional clustering methods often fail to identify such subgroups, particularly when there are a large number of high-variance features in the data set. Conventional methods may identify clusters associated with these high-variance features when one wishes to obtain secondary clusters that are more interesting biologically or more strongly associated with a particular outcome of interest. We describe a modification of the sparse clustering method of Witten and Tibshirani (2010) that can be used to identify such secondary clusters or clusters associated with an outcome of interest. We show that this method can correctly identify such clusters of interest in several simulation scenarios. The method is also applied to a large case-control study of TMD and a leukemia microarray data set. version:2
arxiv-1310-3366 | PCG-Cut: Graph Driven Segmentation of the Prostate Central Gland | http://arxiv.org/abs/1310.3366 | id:1310.3366 author:Jan Egger category:cs.CV  published:2013-10-12 summary:Prostate cancer is the most abundant cancer in men, with over 200,000 expected new cases and around 28,000 deaths in 2012 in the US alone. In this study, the segmentation results for the prostate central gland (PCG) in MR scans are presented. The aim of this research study is to apply a graph-based algorithm to automated segmentation (i.e. delineation) of organ limits for the prostate central gland. The ultimate goal is to apply automated segmentation approach to facilitate efficient MR-guided biopsy and radiation treatment planning. The automated segmentation algorithm used is graph-driven based on a spherical template. Therefore, rays are sent through the surface points of a polyhedron to sample the graph's nodes. After graph construction - which only requires the center of the polyhedron defined by the user and located inside the prostate center gland - the minimal cost closed set on the graph is computed via a polynomial time s-t-cut, which results in the segmentation of the prostate center gland's boundaries and volume. The algorithm has been realized as a C++ modul within the medical research platform MeVisLab and the ground truth of the central gland boundaries were manually extracted by clinical experts (interventional radiologists) with several years of experience in prostate treatment. For evaluation the automated segmentations of the proposed scheme have been compared with the manual segmentations, yielding an average Dice Similarity Coefficient (DSC) of 78.94 +/- 10.85%. version:1
arxiv-1310-3333 | Visualizing Bags of Vectors | http://arxiv.org/abs/1310.3333 | id:1310.3333 author:Sriramkumar Balasubramanian, Raghuram Reddy Nagireddy category:cs.IR cs.CL cs.LG  published:2013-10-12 summary:The motivation of this work is two-fold - a) to compare between two different modes of visualizing data that exists in a bag of vectors format b) to propose a theoretical model that supports a new mode of visualizing data. Visualizing high dimensional data can be achieved using Minimum Volume Embedding, but the data has to exist in a format suitable for computing similarities while preserving local distances. This paper compares the visualization between two methods of representing data and also proposes a new method providing sample visualizations for that method. version:1
arxiv-1209-4129 | Comunication-Efficient Algorithms for Statistical Optimization | http://arxiv.org/abs/1209.4129 | id:1209.4129 author:Yuchen Zhang, John C. Duchi, Martin Wainwright category:stat.ML cs.LG stat.CO  published:2012-09-19 summary:We analyze two communication-efficient algorithms for distributed statistical optimization on large-scale data sets. The first algorithm is a standard averaging method that distributes the $N$ data samples evenly to $\nummac$ machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\order(N^{-1}+(N/m)^{-2})$. Whenever $m \le \sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $\totalnumobs$ samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as $\order(N^{-1} + (N/m)^{-3})$, and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as $O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in the rate of convergence. We also provide experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efficiently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with $N \approx 2.4 \times 10^8$ samples and $d \approx 740,000$ covariates. version:3
arxiv-1305-0556 | A quantum teleportation inspired algorithm produces sentence meaning from word meaning and grammatical structure | http://arxiv.org/abs/1305.0556 | id:1305.0556 author:Stephen Clark, Bob Coecke, Edward Grefenstette, Stephen Pulman, Mehrnoosh Sadrzadeh category:cs.CL quant-ph 68T50 I.2.7  published:2013-05-02 summary:We discuss an algorithm which produces the meaning of a sentence given meanings of its words, and its resemblance to quantum teleportation. In fact, this protocol was the main source of inspiration for this algorithm which has many applications in the area of Natural Language Processing. version:2
arxiv-1301-6791 | Guarantees of Total Variation Minimization for Signal Recovery | http://arxiv.org/abs/1301.6791 | id:1301.6791 author:Jian-Feng Cai, Weiyu Xu category:cs.IT cs.CV cs.LG math.IT  published:2013-01-28 summary:In this paper, we consider using total variation minimization to recover signals whose gradients have a sparse support, from a small number of measurements. We establish the proof for the performance guarantee of total variation (TV) minimization in recovering \emph{one-dimensional} signal with sparse gradient support. This partially answers the open problem of proving the fidelity of total variation minimization in such a setting \cite{TVMulti}. In particular, we have shown that the recoverable gradient sparsity can grow linearly with the signal dimension when TV minimization is used. Recoverable sparsity thresholds of TV minimization are explicitly computed for 1-dimensional signal by using the Grassmann angle framework. We also extend our results to TV minimization for multidimensional signals. Stability of recovering signal itself using 1-D TV minimization has also been established through a property called "almost Euclidean property for 1-dimensional TV norm". We further give a lower bound on the number of random Gaussian measurements for recovering 1-dimensional signal vectors with $N$ elements and $K$-sparse gradients. Interestingly, the number of needed measurements is lower bounded by $\Omega((NK)^{\frac{1}{2}})$, rather than the $O(K\log(N/K))$ bound frequently appearing in recovering $K$-sparse signal vectors. version:6
arxiv-1308-4648 | PACE: Pattern Accurate Computationally Efficient Bootstrapping for Timely Discovery of Cyber-Security Concepts | http://arxiv.org/abs/1308.4648 | id:1308.4648 author:Nikki McNeil, Robert A. Bridges, Michael D. Iannacone, Bogdan Czejdo, Nicolas Perez, John R. Goodall category:cs.IR cs.CL IEEE  published:2013-08-21 summary:Public disclosure of important security information, such as knowledge of vulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, and other online sources months before proper classification into structured databases. In order to facilitate timely discovery of such knowledge, we propose a novel semi-supervised learning algorithm, PACE, for identifying and classifying relevant entities in text sources. The main contribution of this paper is an enhancement of the traditional bootstrapping method for entity extraction by employing a time-memory trade-off that simultaneously circumvents a costly corpus search while strengthening pattern nomination, which should increase accuracy. An implementation in the cyber-security domain is discussed as well as challenges to Natural Language Processing imposed by the security domain. version:3
arxiv-1310-3101 | Deep Multiple Kernel Learning | http://arxiv.org/abs/1310.3101 | id:1310.3101 author:Eric Strobl, Shyam Visweswaran category:stat.ML cs.LG  published:2013-10-11 summary:Deep learning methods have predominantly been applied to large artificial neural networks. Despite their state-of-the-art performance, these large networks typically do not generalize well to datasets with limited sample sizes. In this paper, we take a different approach by learning multiple layers of kernels. We combine kernels at each layer and then optimize over an estimate of the support vector machine leave-one-out error rather than the dual objective function. Our experiments on a variety of datasets show that each layer successively increases performance with only a few base kernels. version:1
arxiv-1310-2905 | Two discussions of the paper "Bayesian measures of model complexity and fit" by D. Spiegelhalter et al., Read before The Royal Statistical Society at a meeting organized by the Research Section on Wednesday, March 13th, 2002 | http://arxiv.org/abs/1310.2905 | id:1310.2905 author:E. Moreno, F. -J. Vazquez-Polo, C. P. Robert category:stat.ME stat.ML  published:2013-10-10 summary:These are the written discussions of the paper "Bayesian measures of model complexity and fit" by D. Spiegelhalter et al. (2002), following the discussions given at the Annual Meeting of the Royal Statistical Society in Newcastle-upon-Tyne on September 3rd, 2013. version:2
arxiv-1310-3004 | Flexible High-dimensional Classification Machines and Their Asymptotic Properties | http://arxiv.org/abs/1310.3004 | id:1310.3004 author:Xingye Qiao, Lingsong Zhang category:stat.ML  published:2013-10-11 summary:Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large margin classification methods, Support Vector Machine (SVM) and Distance Weighted Discrimination (DWD), under two contexts: the high-dimensional, low-sample size data and the imbalanced data. A unified family of classification machines, the FLexible Assortment MachinE (FLAME) is proposed, within which DWD and SVM are special cases. The FLAME family helps to identify the similarities and differences between SVM and DWD. It is well known that many classifiers overfit the data in the high-dimensional setting; and others are sensitive to the imbalanced data, that is, the class with a larger sample size overly influences the classifier and pushes the decision boundary towards the minority class. SVM is resistant to the imbalanced data issue, but it overfits high-dimensional data sets by showing the undesired data-piling phenomena. The DWD method was proposed to improve SVM in the high-dimensional setting, but its decision boundary is sensitive to the imbalanced ratio of sample sizes. Our FLAME family helps to understand an intrinsic connection between SVM and DWD, and improves both methods by providing a better trade-off between sensitivity to the imbalanced data and overfitting the high-dimensional data. Several asymptotic properties of the FLAME classifiers are studied. Simulations and real data applications are investigated to illustrate the usefulness of the FLAME classifiers. version:1
arxiv-1310-2955 | Spontaneous Analogy by Piggybacking on a Perceptual System | http://arxiv.org/abs/1310.2955 | id:1310.2955 author:Marc Pickett, David W. Aha category:cs.AI cs.LG  published:2013-10-10 summary:Most computational models of analogy assume they are given a delineated source domain and often a specified target domain. These systems do not address how analogs can be isolated from large domains and spontaneously retrieved from long-term memory, a process we call spontaneous analogy. We present a system that represents relational structures as feature bags. Using this representation, our system leverages perceptual algorithms to automatically create an ontology of relational structures and to efficiently retrieve analogs for new relational structures from long-term memory. We provide a demonstration of our approach that takes a set of unsegmented stories, constructs an ontology of analogical schemas (corresponding to plot devices), and uses this ontology to efficiently find analogs within new stories, yielding significant time-savings over linear analog retrieval at a small accuracy cost. version:1
arxiv-1210-2085 | Privacy Aware Learning | http://arxiv.org/abs/1210.2085 | id:1210.2085 author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright category:stat.ML cs.IT cs.LG math.IT  published:2012-10-07 summary:We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure. version:2
arxiv-1207-0577 | Robust Dequantized Compressive Sensing | http://arxiv.org/abs/1207.0577 | id:1207.0577 author:Ji Liu, Stephen J. Wright category:stat.ML cs.LG  published:2012-07-03 summary:We consider the reconstruction problem in compressed sensing in which the observations are recorded in a finite number of bits. They may thus contain quantization errors (from being rounded to the nearest representable value) and saturation errors (from being outside the range of representable values). Our formulation has an objective of weighted $\ell_2$-$\ell_1$ type, along with constraints that account explicitly for quantization and saturation errors, and is solved with an augmented Lagrangian method. We prove a consistency result for the recovered solution, stronger than those that have appeared to date in the literature, showing in particular that asymptotic consistency can be obtained without oversampling. We present extensive computational comparisons with formulations proposed previously, and variants thereof. version:2
arxiv-1310-2842 | Wavelet methods for shape perception in electro-sensing | http://arxiv.org/abs/1310.2842 | id:1310.2842 author:Habib Ammari, Stéphane Mallat, Irène Waldspurger, Han Wang category:math.NA cs.CV  published:2013-10-10 summary:This paper aims at presenting a new approach to the electro-sensing problem using wavelets. It provides an efficient algorithm for recognizing the shape of a target from micro-electrical impedance measurements. Stability and resolution capabilities of the proposed algorithm are quantified in numerical simulations. version:1
arxiv-1307-7306 | Kronecker Sum Decompositions of Space-Time Data | http://arxiv.org/abs/1307.7306 | id:1307.7306 author:Kristjan Greenewald, Theodoros Tsiligkaridis, Alfred O Hero III category:stat.ME stat.ML  published:2013-07-27 summary:In this paper we consider the use of the space vs. time Kronecker product decomposition in the estimation of covariance matrices for spatio-temporal data. This decomposition imposes lower dimensional structure on the estimated covariance matrix, thus reducing the number of samples required for estimation. To allow a smooth tradeoff between the reduction in the number of parameters (to reduce estimation variance) and the accuracy of the covariance approximation (affecting estimation bias), we introduce a diagonally loaded modification of the sum of kronecker products representation [1]. We derive a Cramer-Rao bound (CRB) on the minimum attainable mean squared predictor coefficient estimation error for unbiased estimators of Kronecker structured covariance matrices. We illustrate the accuracy of the diagonally loaded Kronecker sum decomposition by applying it to video data of human activity. version:2
arxiv-1310-2816 | Gibbs Max-margin Topic Models with Data Augmentation | http://arxiv.org/abs/1310.2816 | id:1310.2816 author:Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang category:stat.ML cs.LG stat.CO stat.ME  published:2013-10-10 summary:Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems. Furthermore, each step of the "augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors on binary, multi-class and multi-label classification tasks. version:1
arxiv-1310-2805 | MizAR 40 for Mizar 40 | http://arxiv.org/abs/1310.2805 | id:1310.2805 author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO cs.MS  published:2013-10-10 summary:As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40% of the theorems in the latest official version of the Mizar Mathematical Library (MML). This is a considerable improvement over previous performance of large- theory AI/ATP methods measured on the whole MML. To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML. This reduces the training times over the corpus to 1-3 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users (MizAR). version:1
arxiv-1310-2797 | Lemma Mining over HOL Light | http://arxiv.org/abs/1310.2797 | id:1310.2797 author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO  published:2013-10-10 summary:Large formal mathematical libraries consist of millions of atomic inference steps that give rise to a corresponding number of proved statements (lemmas). Analogously to the informal mathematical practice, only a tiny fraction of such statements is named and re-used in later proofs by formal mathematicians. In this work, we suggest and implement criteria defining the estimated usefulness of the HOL Light lemmas for proving further theorems. We use these criteria to mine the large inference graph of all lemmas in the core HOL Light library, adding thousands of the best lemmas to the pool of named statements that can be re-used in later proofs. The usefulness of the new lemmas is then evaluated by comparing the performance of automated proving of the core HOL Light theorems with and without such added lemmas. version:1
arxiv-1310-3233 | Bayesian Estimation of White Matter Atlas from High Angular Resolution Diffusion Imaging | http://arxiv.org/abs/1310.3233 | id:1310.3233 author:Jia Du, Alvina Goh, Anqi Qiu category:cs.CV  published:2013-10-10 summary:We present a Bayesian probabilistic model to estimate the brain white matter atlas from high angular resolution diffusion imaging (HARDI) data. This model incorporates a shape prior of the white matter anatomy and the likelihood of individual observed HARDI datasets. We first assume that the atlas is generated from a known hyperatlas through a flow of diffeomorphisms and its shape prior can be constructed based on the framework of large deformation diffeomorphic metric mapping (LDDMM). LDDMM characterizes a nonlinear diffeomorphic shape space in a linear space of initial momentum uniquely determining diffeomorphic geodesic flows from the hyperatlas. Therefore, the shape prior of the HARDI atlas can be modeled using a centered Gaussian random field (GRF) model of the initial momentum. In order to construct the likelihood of observed HARDI datasets, it is necessary to study the diffeomorphic transformation of individual observations relative to the atlas and the probabilistic distribution of orientation distribution functions (ODFs). To this end, we construct the likelihood related to the transformation using the same construction as discussed for the shape prior of the atlas. The probabilistic distribution of ODFs is then constructed based on the ODF Riemannian manifold. We assume that the observed ODFs are generated by an exponential map of random tangent vectors at the deformed atlas ODF. Hence, the likelihood of the ODFs can be modeled using a GRF of their tangent vectors in the ODF Riemannian manifold. We solve for the maximum a posteriori using the Expectation-Maximization algorithm and derive the corresponding update equations. Finally, we illustrate the HARDI atlas constructed based on a Chinese aging cohort of 94 adults and compare it with that generated by averaging the coefficients of spherical harmonics of the ODF across subjects. version:1
arxiv-1310-2646 | Localized Iterative Methods for Interpolation in Graph Structured Data | http://arxiv.org/abs/1310.2646 | id:1310.2646 author:Sunil K. Narang, Akshay Gadde, Eduard Sanou, Antonio Ortega category:cs.LG  published:2013-10-09 summary:In this paper, we present two localized graph filtering based methods for interpolating graph signals defined on the vertices of arbitrary graphs from only a partial set of samples. The first method is an extension of previous work on reconstructing bandlimited graph signals from partially observed samples. The iterative graph filtering approach very closely approximates the solution proposed in the that work, while being computationally more efficient. As an alternative, we propose a regularization based framework in which we define the cost of reconstruction to be a combination of smoothness of the graph signal and the reconstruction error with respect to the known samples, and find solutions that minimize this cost. We provide both a closed form solution and a computationally efficient iterative solution of the optimization problem. The experimental results on the recommendation system datasets demonstrate effectiveness of the proposed methods. version:1
arxiv-1310-2641 | Duality in Graphical Models | http://arxiv.org/abs/1310.2641 | id:1310.2641 author:Dhafer Malouche, Bala Rajaratnam, Benjamin T. Rolfs category:math.PR stat.ML  published:2013-10-09 summary:Graphical models have proven to be powerful tools for representing high-dimensional systems of random variables. One example of such a model is the undirected graph, in which lack of an edge represents conditional independence between two random variables given the rest. Another example is the bidirected graph, in which absence of edges encodes pairwise marginal independence. Both of these classes of graphical models have been extensively studied, and while they are considered to be dual to one another, except in a few instances this duality has not been thoroughly investigated. In this paper, we demonstrate how duality between undirected and bidirected models can be used to transport results for one class of graphical models to the dual model in a transparent manner. We proceed to apply this technique to extend previously existing results as well as to prove new ones, in three important domains. First, we discuss the pairwise and global Markov properties for undirected and bidirected models, using the pseudographoid and reverse-pseudographoid rules which are weaker conditions than the typically used intersection and composition rules. Second, we investigate these pseudographoid and reverse pseudographoid rules in the context of probability distributions, using the concept of duality in the process. Duality allows us to quickly relate them to the more familiar intersection and composition properties. Third and finally, we apply the dualization method to understand the implications of faithfulness, which in turn leads to a more general form of an existing result. version:1
arxiv-1309-1193 | Confidence-constrained joint sparsity recovery under the Poisson noise model | http://arxiv.org/abs/1309.1193 | id:1309.1193 author:E. Chunikhina, R. Raich, T. Nguyen category:stat.ML cs.LG  published:2013-09-04 summary:Our work is focused on the joint sparsity recovery problem where the common sparsity pattern is corrupted by Poisson noise. We formulate the confidence-constrained optimization problem in both least squares (LS) and maximum likelihood (ML) frameworks and study the conditions for perfect reconstruction of the original row sparsity and row sparsity pattern. However, the confidence-constrained optimization problem is non-convex. Using convex relaxation, an alternative convex reformulation of the problem is proposed. We evaluate the performance of the proposed approach using simulation results on synthetic data and show the effectiveness of proposed row sparsity and row sparsity pattern recovery framework. version:2
arxiv-1302-3931 | Understanding Boltzmann Machine and Deep Learning via A Confident Information First Principle | http://arxiv.org/abs/1302.3931 | id:1302.3931 author:Xiaozhao Zhao, Yuexian Hou, Qian Yu, Dawei Song, Wenjie Li category:cs.NE cs.LG stat.ML  published:2013-02-16 summary:Typical dimensionality reduction methods focus on directly reducing the number of random variables while retaining maximal variations in the data. In this paper, we consider the dimensionality reduction in parameter spaces of binary multivariate distributions. We propose a general Confident-Information-First (CIF) principle to maximally preserve parameters with confident estimates and rule out unreliable or noisy parameters. Formally, the confidence of a parameter can be assessed by its Fisher information, which establishes a connection with the inverse variance of any unbiased estimate for the parameter via the Cram\'{e}r-Rao bound. We then revisit Boltzmann machines (BM) and theoretically show that both single-layer BM without hidden units (SBM) and restricted BM (RBM) can be solidly derived using the CIF principle. This can not only help us uncover and formalize the essential parts of the target density that SBM and RBM capture, but also suggest that the deep neural network consisting of several layers of RBM can be seen as the layer-wise application of CIF. Guided by the theoretical analysis, we develop a sample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and a CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are studied in a series of density estimation experiments. version:7
arxiv-1212-4799 | Towards common-sense reasoning via conditional simulation: legacies of Turing in Artificial Intelligence | http://arxiv.org/abs/1212.4799 | id:1212.4799 author:Cameron E. Freer, Daniel M. Roy, Joshua B. Tenenbaum category:cs.AI math.LO stat.ML  published:2012-12-19 summary:The problem of replicating the flexibility of human common-sense reasoning has captured the imagination of computer scientists since the early days of Alan Turing's foundational work on computation and the philosophy of artificial intelligence. In the intervening years, the idea of cognition as computation has emerged as a fundamental tenet of Artificial Intelligence (AI) and cognitive science. But what kind of computation is cognition? We describe a computational formalism centered around a probabilistic Turing machine called QUERY, which captures the operation of probabilistic conditioning via conditional simulation. Through several examples and analyses, we demonstrate how the QUERY abstraction can be used to cast common-sense reasoning as probabilistic inference in a statistical model of our observations and the uncertain structure of the world that generated that experience. This formulation is a recent synthesis of several research programs in AI and cognitive science, but it also represents a surprising convergence of several of Turing's pioneering insights in AI, the foundations of computation, and statistics. version:2
arxiv-1310-2451 | M-Power Regularized Least Squares Regression | http://arxiv.org/abs/1310.2451 | id:1310.2451 author:Julien Audiffren, Hachem Kadri category:stat.ML cs.LG  published:2013-10-09 summary:Regularization is used to find a solution that both fits the data and is sufficiently smooth, and thereby is very effective for designing and refining learning algorithms. But the influence of its exponent remains poorly understood. In particular, it is unclear how the exponent of the reproducing kernel Hilbert space (RKHS) regularization term affects the accuracy and the efficiency of kernel-based learning algorithms. Here we consider regularized least squares regression (RLSR) with an RKHS regularization raised to the power of m, where m is a variable real exponent. We design an efficient algorithm for solving the associated minimization problem, we provide a theoretical analysis of its stability, and we {compare it %/ demonstrate its advantage with respect to computational complexity, speed of convergence and prediction accuracy to %/over} the classical kernel ridge regression algorithm where the regularization exponent m is fixed at 2. Our results show that the m-power RLSR problem can be solved efficiently, and support the suggestion that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm.} version:1
arxiv-1310-0575 | Development of Marathi Part of Speech Tagger Using Statistical Approach | http://arxiv.org/abs/1310.0575 | id:1310.0575 author:Jyoti Singh, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-10-02 summary:Part-of-speech (POS) tagging is a process of assigning the words in a text corresponding to a particular part of speech. A fundamental version of POS tagging is the identification of words as nouns, verbs, adjectives etc. For processing natural languages, Part of Speech tagging is a prominent tool. It is one of the simplest as well as most constant and statistical model for many NLP applications. POS Tagging is an initial stage of linguistics, text analysis like information retrieval, machine translator, text to speech synthesis, information extraction etc. In POS Tagging we assign a Part of Speech tag to each word in a sentence and literature. Various approaches have been proposed to implement POS taggers. In this paper we present a Marathi part of speech tagger. It is morphologically rich language. Marathi is spoken by the native people of Maharashtra. The general approach used for development of tagger is statistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear idea about all the algorithms with suitable examples. It also introduces a tag set for Marathi which can be used for tagging Marathi text. In this paper we have shown the development of the tagger as well as compared to check the accuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram, Trigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82% respectively. version:2
arxiv-1308-6415 | Learning-Based Procedural Content Generation | http://arxiv.org/abs/1308.6415 | id:1308.6415 author:Jonathan Roberts, Ke Chen category:cs.AI cs.HC cs.LG cs.NE  published:2013-08-29 summary:Procedural content generation (PCG) has recently become one of the hottest topics in computational intelligence and AI game researches. Among a variety of PCG techniques, search-based approaches overwhelmingly dominate PCG development at present. While SBPCG leads to promising results and successful applications, it poses a number of challenges ranging from representation to evaluation of the content being generated. In this paper, we present an alternative yet generic PCG framework, named learning-based procedure content generation (LBPCG), to provide potential solutions to several challenging problems in existing PCG techniques. By exploring and exploiting information gained in game development and public beta test via data-driven learning, our framework can generate robust content adaptable to end-user or target players on-line with minimal interruption to their experience. Furthermore, we develop enabling techniques to implement the various models required in our framework. For a proof of concept, we have developed a prototype based on the classic open source first-person shooter game, Quake. Simulation results suggest that our framework is promising in generating quality content. version:2
arxiv-1310-2409 | Discriminative Relational Topic Models | http://arxiv.org/abs/1310.2409 | id:1310.2409 author:Ning Chen, Jun Zhu, Fei Xia, Bo Zhang category:cs.LG cs.IR stat.ML  published:2013-10-09 summary:Many scientific and engineering fields involve analyzing network data. For document networks, relational topic models (RTMs) provide a probabilistic generative process to describe both the link structure and document contents, and they have shown promise on predicting network structures and discovering latent topic representations. However, existing RTMs have limitations in both the restricted model expressiveness and incapability of dealing with imbalanced network data. To expand the scope and improve the inference accuracy of RTMs, this paper presents three extensions: 1) unlike the common link likelihood with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference (RegBayes) with a regularization parameter to deal with the imbalanced link structure issue in common real networks and improve the discriminative ability of learned latent representations; and 3) instead of doing variational approximation with strict mean-field assumptions, we present collapsed Gibbs sampling algorithms for the generalized relational topic models by exploring data augmentation without making restricting assumptions. Under the generic RegBayes framework, we carefully investigate two popular discriminative loss functions, namely, the logistic log-loss and the max-margin hinge loss. Experimental results on several real network datasets demonstrate the significance of these extensions on improving the prediction performance, and the time efficiency can be dramatically improved with a simple fast approximation method. version:1
arxiv-1310-2408 | Improved Bayesian Logistic Supervised Topic Models with Data Augmentation | http://arxiv.org/abs/1310.2408 | id:1310.2408 author:Jun Zhu, Xun Zheng, Bo Zhang category:cs.LG cs.CL stat.AP stat.ML  published:2013-10-09 summary:Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency. version:1
arxiv-1310-7440 | Neural perceptual model to global-local vision for recognition of the logical structure of administrative documents | http://arxiv.org/abs/1310.7440 | id:1310.7440 author:Boulbaba Ben Ammar category:cs.CV  published:2013-10-09 summary:This paper gives the definition of Transparent Neural Network "TNN" for the simulation of the globallocal vision and its application to the segmentation of administrative document image. We have developed and have adapted a recognition method which models the contextual effects reported from studies in experimental psychology. Then, we evaluated and tested the TNN and the multi-layer perceptron "MLP", which showed its effectiveness in the field of the recognition, in order to show that the TNN is clearer for the user and more powerful on the level of the recognition. Indeed, the TNN is the only system which makes it possible to recognize the document and its structure. version:1
arxiv-1310-2350 | The Generalized Traveling Salesman Problem solved with Ant Algorithms | http://arxiv.org/abs/1310.2350 | id:1310.2350 author:Camelia-M. Pintea, Petrica C. Pop, Camelia Chira category:cs.AI cs.NE  published:2013-10-09 summary:A well known N P-hard problem called the Generalized Traveling Salesman Problem (GTSP) is considered. In GTSP the nodes of a complete undirected graph are partitioned into clusters. The objective is to find a minimum cost tour passing through exactly one node from each cluster. An exact exponential time algorithm and an effective meta-heuristic algorithm for the problem are presented. The meta-heuristic proposed is a modified Ant Colony System (ACS) algorithm called Reinforcing Ant Colony System (RACS) which introduces new correction rules in the ACS algorithm. Computational results are reported for many standard test problems. The proposed algorithm is competitive with the other already proposed heuristics for the GTSP in both solution quality and computational time. version:1
arxiv-1303-1749 | Simplifying Energy Optimization using Partial Enumeration | http://arxiv.org/abs/1303.1749 | id:1303.1749 author:Carl Olsson, Johannes Ulen, Yuri Boykov, Vladimir Kolmogorov category:cs.CV  published:2013-03-07 summary:Energies with high-order non-submodular interactions have been shown to be very useful in vision due to their high modeling power. Optimization of such energies, however, is generally NP-hard. A naive approach that works for small problem instances is exhaustive search, that is, enumeration of all possible labelings of the underlying graph. We propose a general minimization approach for large graphs based on enumeration of labelings of certain small patches. This partial enumeration technique reduces complex high-order energy formulations to pairwise Constraint Satisfaction Problems with unary costs (uCSP), which can be efficiently solved using standard methods like TRW-S. Our approach outperforms a number of existing state-of-the-art algorithms on well known difficult problems (e.g. curvature regularization, stereo, deconvolution); it gives near global minimum and better speed. Our main application of interest is curvature regularization. In the context of segmentation, our partial enumeration technique allows to evaluate curvature directly on small patches using a novel integral geometry approach. version:2
arxiv-1310-2527 | Treating clitics with minimalist grammars | http://arxiv.org/abs/1310.2527 | id:1310.2527 author:Maxime Amblard category:cs.CL cs.LO  published:2013-10-08 summary:We propose an extension of Stabler's version of clitics treatment for a wider coverage of the French language. For this, we present the lexical entries needed in the lexicon. Then, we show the recognition of complex syntactic phenomena as (left and right) dislo- cation, clitic climbing over modal and extraction from determiner phrase. The aim of this presentation is the syntax-semantic interface for clitics analyses in which we will stress on clitic climbing over verb and raising verb. version:1
arxiv-1401-3230 | Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet | http://arxiv.org/abs/1401.3230 | id:1401.3230 author:K Paramesha, K C Ravishankar category:cs.CL cs.IR  published:2013-10-08 summary:The task of sentiment analysis of reviews is carried out using manually built / automatically generated lexicon resources of their own with which terms are matched with lexicon to compute the term count for positive and negative polarity. On the other hand the Sentiwordnet, which is quite different from other lexicon resources that gives scores (weights) of the positive and negative polarity for each word. The polarity of a word namely positive, negative and neutral have the score ranging between 0 to 1 indicates the strength/weight of the word with that sentiment orientation. In this paper, we show that using the Sentiwordnet, how we could enhance the performance of the classification at both sentence and document level. version:1
arxiv-1307-6235 | Graphical law beneath each written natural language | http://arxiv.org/abs/1307.6235 | id:1307.6235 author:Anindya Kumar Biswas category:physics.gen-ph cs.CL  published:2013-07-18 summary:We study twenty four written natural languages. We draw in the log scale, number of words starting with a letter vs rank of the letter, both normalised. We find that all the graphs are of the similar type. The graphs are tantalisingly closer to the curves of reduced magnetisation vs reduced temperature for magnetic materials. We make a weak conjecture that a curve of magnetisation underlies a written natural language. version:4
arxiv-1310-2085 | A Robust Variational Model for Positive Image Deconvolution | http://arxiv.org/abs/1310.2085 | id:1310.2085 author:Martin Welk category:cs.CV I.4.3; I.4.4; G.1.9  published:2013-10-08 summary:In this paper, an iterative method for robust deconvolution with positivity constraints is discussed. It is based on the known variational interpretation of the Richardson-Lucy iterative deconvolution as fixed-point iteration for the minimisation of an information divergence functional under a multiplicative perturbation model. The asymmetric penaliser function involved in this functional is then modified into a robust penaliser, and complemented with a regulariser. The resulting functional gives rise to a fixed point iteration that we call robust and regularised Richardson-Lucy deconvolution. It achieves an image restoration quality comparable to state-of-the-art robust variational deconvolution with a computational efficiency similar to that of the original Richardson-Lucy method. Experiments on synthetic and real-world image data demonstrate the performance of the proposed method. version:1
arxiv-1310-2071 | Predicting Students' Performance Using ID3 And C4.5 Classification Algorithms | http://arxiv.org/abs/1310.2071 | id:1310.2071 author:Kalpesh Adhatrao, Aditya Gaykar, Amiraj Dhawan, Rohit Jha, Vipul Honrao category:cs.CY cs.LG  published:2013-10-08 summary:An educational institution needs to have an approximate prior knowledge of enrolled students to predict their performance in future academics. This helps them to identify promising students and also provides them an opportunity to pay attention to and improve those who would probably get lower grades. As a solution, we have developed a system which can predict the performance of students from their previous performances using concepts of data mining techniques under Classification. We have analyzed the data set containing information about students, such as gender, marks scored in the board examinations of classes X and XII, marks and rank in entrance examinations and results in first year of the previous batch of students. By applying the ID3 (Iterative Dichotomiser 3) and C4.5 classification algorithms on this data, we have predicted the general and individual performance of freshly admitted students in future examinations. version:1
arxiv-1310-2059 | Distributed Coordinate Descent Method for Learning with Big Data | http://arxiv.org/abs/1310.2059 | id:1310.2059 author:Peter Richtárik, Martin Takáč category:stat.ML cs.DC cs.LG math.OC  published:2013-10-08 summary:In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix. version:1
arxiv-1310-2053 | The role of RGB-D benchmark datasets: an overview | http://arxiv.org/abs/1310.2053 | id:1310.2053 author:Kai Berger category:cs.CV  published:2013-10-08 summary:The advent of the Microsoft Kinect three years ago stimulated not only the computer vision community for new algorithms and setups to tackle well-known problems in the community but also sparked the launch of several new benchmark datasets to which future algorithms can be compared 019 to. This review of the literature and industry developments concludes that the current RGB-D benchmark datasets can be useful to determine the accuracy of a variety of applications of a single or multiple RGB-D sensors. version:1
arxiv-1310-2050 | A State Of the Art Report on Research in Multiple RGB-D sensor Setups | http://arxiv.org/abs/1310.2050 | id:1310.2050 author:Kai Berger category:cs.CV  published:2013-10-08 summary:That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end consumer sector has been anticipated by the developers. That it also impacted in rigorous computer vision research has probably been a surprise to the whole community. Shortly before the commercial deployment of its successor, Kinect One, the research literature fills with resumees and state-of-the art papers to summarize the development over the past 3 years. This particular report describes significant research projects which have built on sensoring setups that include two or more RGB-D sensors in one scene. version:1
arxiv-1310-2049 | Fast Multi-Instance Multi-Label Learning | http://arxiv.org/abs/1310.2049 | id:1310.2049 author:Sheng-Jun Huang, Zhi-Hua Zhou category:cs.LG  published:2013-10-08 summary:In many real-world tasks, particularly those involving data objects with complicated semantics such as images and texts, one object can be represented by multiple instances and simultaneously be associated with multiple labels. Such tasks can be formulated as multi-instance multi-label learning (MIML) problems, and have been extensively studied during the past few years. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, in this paper we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less; particularly, on a data set with 20K bags and 180K instances, MIMLfast is more than 100 times faster than existing MIML approaches. On a larger data set where none of existing approaches can return results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output label semantics. version:1
arxiv-1310-7813 | Smoothness-Constrained Image Recovery from Block-Based Random Projections | http://arxiv.org/abs/1310.7813 | id:1310.7813 author:Giulio Coluccia, Diego Valsesia, Enrico Magli category:cs.CV cs.IT math.IT  published:2013-10-08 summary:In this paper we address the problem of visual quality of images reconstructed from block-wise random projections. Independent reconstruction of the blocks can severely affect visual quality, by displaying artifacts along block borders. We propose a method to enforce smoothness across block borders by modifying the sensing and reconstruction process so as to employ partially overlapping blocks. The proposed algorithm accomplishes this by computing a fast preview from the blocks, whose purpose is twofold. On one hand, it allows to enforce a set of constraints to drive the reconstruction algorithm towards a smooth solution, imposing the similarity of block borders. On the other hand, the preview is used as a predictor of the entire block, allowing to recover the prediction error, only. The quality improvement over the result of independent reconstruction can be easily assessed both visually and in terms of PSNR and SSIM index. version:1
arxiv-1111-6453 | Learning with Submodular Functions: A Convex Optimization Perspective | http://arxiv.org/abs/1111.6453 | id:1111.6453 author:Francis Bach category:cs.LG math.OC  published:2011-11-28 summary:Submodular functions are relevant to machine learning for at least two reasons: (1) some problems may be expressed directly as the optimization of submodular functions and (2) the lovasz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning. In this monograph, we present the theory of submodular functions from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems. In particular, we show how submodular function minimization is equivalent to solving a wide variety of convex optimization problems. This allows the derivation of new efficient algorithms for approximate and exact submodular function minimization with theoretical guarantees and good practical performance. By listing many examples of submodular functions, we review various applications to machine learning, such as clustering, experimental design, sensor placement, graphical model structure learning or subset selection, as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions. version:2
arxiv-1309-2375 | Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization | http://arxiv.org/abs/1309.2375 | id:1309.2375 author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG cs.NA stat.CO  published:2013-09-10 summary:We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings. version:2
arxiv-1310-1976 | Feature Selection Strategies for Classifying High Dimensional Astronomical Data Sets | http://arxiv.org/abs/1310.1976 | id:1310.1976 author:Ciro Donalek, Arun Kumar A., S. G. Djorgovski, Ashish A. Mahabal, Matthew J. Graham, Thomas J. Fuchs, Michael J. Turmon, N. Sajeeth Philip, Michael Ting-Chang Yang, Giuseppe Longo category:astro-ph.IM cs.CV  published:2013-10-08 summary:The amount of collected data in many scientific fields is increasing, all of them requiring a common task: extract knowledge from massive, multi parametric data sets, as rapidly and efficiently possible. This is especially true in astronomy where synoptic sky surveys are enabling new research frontiers in the time domain astronomy and posing several new object classification challenges in multi dimensional spaces; given the high number of parameters available for each object, feature selection is quickly becoming a crucial task in analyzing astronomical data sets. Using data sets extracted from the ongoing Catalina Real-Time Transient Surveys (CRTS) and the Kepler Mission we illustrate a variety of feature selection strategies used to identify the subsets that give the most information and the results achieved applying these techniques to three major astronomical problems. version:1
arxiv-1310-1975 | ARKref: a rule-based coreference resolution system | http://arxiv.org/abs/1310.1975 | id:1310.1975 author:Brendan O'Connor, Michael Heilman category:cs.CL  published:2013-10-08 summary:ARKref is a tool for noun phrase coreference. It is a deterministic, rule-based system that uses syntactic information from a constituent parser, and semantic information from an entity recognition component. Its architecture is based on the work of Haghighi and Klein (2009). ARKref was originally written in 2009. At the time of writing, the last released version was in March 2011. This document describes that version, which is open-source and publicly available at: http://www.ark.cs.cmu.edu/ARKref version:1
arxiv-1310-1964 | Named entity recognition using conditional random fields with non-local relational constraints | http://arxiv.org/abs/1310.1964 | id:1310.1964 author:Flavio Massimiliano Cecchini, Elisabetta Fersini category:cs.CL  published:2013-10-07 summary:We begin by introducing the Computer Science branch of Natural Language Processing, then narrowing the attention on its subbranch of Information Extraction and particularly on Named Entity Recognition, discussing briefly its main methodological approaches. It follows an introduction to state-of-the-art Conditional Random Fields under the form of linear chains. Subsequently, the idea of constrained inference as a way to model long-distance relationships in a text is presented, based on an Integer Linear Programming representation of the problem. Adding such relationships to the problem as automatically inferred logical formulas, translatable into linear conditions, we propose to solve the resulting more complex problem with the aid of Lagrangian relaxation, of which some technical details are explained. Lastly, we give some experimental results. version:1
arxiv-1310-1947 | Bayesian Optimization With Censored Response Data | http://arxiv.org/abs/1310.1947 | id:1310.1947 author:Frank Hutter, Holger Hoos, Kevin Leyton-Brown category:cs.AI cs.LG stat.ML G.3; G.1.6  published:2013-10-07 summary:Bayesian optimization (BO) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available. Here, we address the problem of BO under partially right-censored response data, where in some evaluations we only obtain a lower bound on the function value. The ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value. One important application giving rise to such censored data is the runtime-minimizing variant of the algorithm configuration problem: finding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution. We demonstrate that terminating slow algorithm runs prematurely and handling the resulting right-censored observations can substantially improve the state of the art in model-based algorithm configuration. version:1
arxiv-1310-1934 | Discriminative Features via Generalized Eigenvectors | http://arxiv.org/abs/1310.1934 | id:1310.1934 author:Nikos Karampatziakis, Paul Mineiro category:cs.LG stat.ML  published:2013-10-07 summary:Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results. version:1
arxiv-1310-1869 | Singular Value Decomposition of Images from Scanned Photographic Plates | http://arxiv.org/abs/1310.1869 | id:1310.1869 author:Vasil Kolev, Katya Tsvetkova, Milcho Tsvetkov category:cs.CV astro-ph.IM cs.CE  published:2013-10-07 summary:We want to approximate the mxn image A from scanned astronomical photographic plates (from the Sofia Sky Archive Data Center) by using far fewer entries than in the original matrix. By using rank of a matrix, k we remove the redundant information or noise and use as Wiener filter, when rank k<m or k<n. With this approximation more than 98% compression ration of image of astronomical plate without that image details, is obtained. The SVD of images from scanned photographic plates (SPP) is considered and its possible image compression. version:1
arxiv-1310-1855 | Early Fire Detection Using HEP and Space-time Analysis | http://arxiv.org/abs/1310.1855 | id:1310.1855 author:Junzhou Chen, Yong You category:cs.CV cs.MM  published:2013-10-07 summary:In this article, a video base early fire alarm system is developed by monitoring the smoke in the scene. There are two major contributions in this work. First, to find the best texture feature for smoke detection, a general framework, named Histograms of Equivalent Patterns (HEP), is adopted to achieve an extensive evaluation of various kinds of texture features. Second, the \emph{Block based Inter-Frame Difference} (BIFD) and a improved version of LBP-TOP are proposed and ensembled to describe the space-time characteristics of the smoke. In order to reduce the false alarms, the Smoke History Image (SHI) is utilized to register the recent classification results of candidate smoke blocks. Experimental results using SVM show that the proposed method can achieve better accuracy and less false alarm compared with the state-of-the-art technologies. version:1
arxiv-1310-1840 | Parallel coordinate descent for the Adaboost problem | http://arxiv.org/abs/1310.1840 | id:1310.1840 author:Olivier Fercoq category:cs.LG math.OC stat.ML  published:2013-10-07 summary:We design a randomised parallel version of Adaboost based on previous studies on parallel coordinate descent. The algorithm uses the fact that the logarithm of the exponential loss is a function with coordinate-wise Lipschitz continuous gradient, in order to define the step lengths. We provide the proof of convergence for this randomised Adaboost algorithm and a theoretical parallelisation speedup factor. We finally provide numerical examples on learning problems of various sizes that show that the algorithm is competitive with concurrent approaches, especially for large scale problems. version:1
arxiv-1310-1811 | End-to-End Text Recognition with Hybrid HMM Maxout Models | http://arxiv.org/abs/1310.1811 | id:1310.1811 author:Ouais Alsharif, Joelle Pineau category:cs.CV  published:2013-10-07 summary:The problem of detecting and recognizing text in natural scenes has proved to be more challenging than its counterpart in documents, with most of the previous work focusing on a single part of the problem. In this work, we propose new solutions to the character and word recognition problems and then show how to combine these solutions in an end-to-end text-recognition system. We do so by leveraging the recently introduced Maxout networks along with hybrid HMM models that have proven useful for voice recognition. Using these elements, we build a tunable and highly accurate recognition system that beats state-of-the-art results on all the sub-problems for both the ICDAR 2003 and SVT benchmark datasets. version:1
arxiv-1310-1800 | Generalized Negative Binomial Processes and the Representation of Cluster Structures | http://arxiv.org/abs/1310.1800 | id:1310.1800 author:Mingyuan Zhou category:stat.ME math.ST stat.ML stat.TH  published:2013-10-07 summary:The paper introduces the concept of a cluster structure to define a joint distribution of the sample size and its exchangeable random partitions. The cluster structure allows the probability distribution of the random partitions of a subset of the sample to be dependent on the sample size, a feature not presented in a partition structure. A generalized negative binomial process count-mixture model is proposed to generate a cluster structure, where in the prior the number of clusters is finite and Poisson distributed and the cluster sizes follow a truncated negative binomial distribution. The number and sizes of clusters can be controlled to exhibit distinct asymptotic behaviors. Unique model properties are illustrated with example clustering results using a generalized Polya urn sampling scheme. The paper provides new methods to generate exchangeable random partitions and to control both the cluster-number and cluster-size distributions. version:1
arxiv-1208-1237 | Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix Factorization | http://arxiv.org/abs/1208.1237 | id:1208.1237 author:Nicolas Gillis, Stephen A. Vavasis category:stat.ML cs.LG math.OC  published:2012-08-06 summary:In this paper, we study the nonnegative matrix factorization problem under the separability assumption (that is, there exists a cone spanned by a small subset of the columns of the input nonnegative data matrix containing all columns), which is equivalent to the hyperspectral unmixing problem under the linear mixing model and the pure-pixel assumption. We present a family of fast recursive algorithms, and prove they are robust under any small perturbations of the input data matrix. This family generalizes several existing hyperspectral unmixing algorithms and hence provides for the first time a theoretical justification of their better practical performance. version:3
arxiv-1304-1978 | Constructing Low Star Discrepancy Point Sets with Genetic Algorithms | http://arxiv.org/abs/1304.1978 | id:1304.1978 author:Carola Doerr, Francois-Michel De Rainville category:cs.NE cs.NA F.2.1; I.2.8  published:2013-04-07 summary:Geometric discrepancies are standard measures to quantify the irregularity of distributions. They are an important notion in numerical integration. One of the most important discrepancy notions is the so-called \emph{star discrepancy}. Roughly speaking, a point set of low star discrepancy value allows for a small approximation error in quasi-Monte Carlo integration. It is thus the most studied discrepancy notion. In this work we present a new algorithm to compute point sets of low star discrepancy. The two components of the algorithm (for the optimization and the evaluation, respectively) are based on evolutionary principles. Our algorithm clearly outperforms existing approaches. To the best of our knowledge, it is also the first algorithm which can be adapted easily to optimize inverse star discrepancies. version:2
arxiv-1310-1771 | Potts model, parametric maxflow and k-submodular functions | http://arxiv.org/abs/1310.1771 | id:1310.1771 author:Igor Gridchyn, Vladimir Kolmogorov category:cs.CV  published:2013-10-07 summary:The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [19,20]. It identifies a part of an optimal solution by running $k$ maxflow computations, where $k$ is the number of labels. The number of "labeled" pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to $O(\log k)$ maxflow computations (or one {\em parametric maxflow} computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for {\em Tree Metrics}. We also show a connection to {\em $k$-submodular functions} from combinatorial optimization, and discuss {\em $k$-submodular relaxations} for general energy functions. version:1
arxiv-1211-0174 | Laplace approximation for logistic Gaussian process density estimation and regression | http://arxiv.org/abs/1211.0174 | id:1211.0174 author:Jaakko Riihimäki, Aki Vehtari category:stat.CO stat.ME stat.ML  published:2012-11-01 summary:Logistic Gaussian process (LGP) priors provide a flexible alternative for modelling unknown densities. The smoothness properties of the density estimates can be controlled through the prior covariance structure of the LGP, but the challenge is the analytically intractable inference. In this paper, we present approximate Bayesian inference for LGP density estimation in a grid using Laplace's method to integrate over the non-Gaussian posterior distribution of latent function values and to determine the covariance function parameters with type-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace's method with MAP is sufficiently fast for practical interactive visualisation of 1D and 2D densities. Our experiments with simulated and real 1D data sets show that the estimation accuracy is close to a Markov chain Monte Carlo approximation and state-of-the-art hierarchical infinite Gaussian mixture models. We also construct a reduced-rank approximation to speed up the computations for dense 2D grids, and demonstrate density regression with the proposed Laplace approach. version:3
arxiv-1310-1690 | Online Unsupervised Feature Learning for Visual Tracking | http://arxiv.org/abs/1310.1690 | id:1310.1690 author:Fayao Liu, Chunhua Shen, Ian Reid, Anton van den Hengel category:cs.CV  published:2013-10-07 summary:Feature encoding with respect to an over-complete dictionary learned by unsupervised methods, followed by spatial pyramid pooling, and linear classification, has exhibited powerful strength in various vision applications. Here we propose to use the feature learning pipeline for visual tracking. Tracking is implemented using tracking-by-detection and the resulted framework is very simple yet effective. First, online dictionary learning is used to build a dictionary, which captures the appearance changes of the tracking target as well as the background changes. Given a test image window, we extract local image patches from it and each local patch is encoded with respect to the dictionary. The encoded features are then pooled over a spatial pyramid to form an aggregated feature vector. Finally, a simple linear classifier is trained on these features. Our experiments show that the proposed powerful---albeit simple---tracker, outperforms all the state-of-the-art tracking methods that we have tested. Moreover, we evaluate the performance of different dictionary learning and feature encoding methods in the proposed tracking framework, and analyse the impact of each component in the tracking scenario. We also demonstrate the flexibility of feature learning by plugging it into Hare et al.'s tracking method. The outcome is, to our knowledge, the best tracker ever reported, which facilitates the advantages of both feature learning and structured output prediction. version:1
arxiv-1310-1659 | MINT: Mutual Information based Transductive Feature Selection for Genetic Trait Prediction | http://arxiv.org/abs/1310.1659 | id:1310.1659 author:Dan He, Irina Rish, David Haws, Simon Teyssedre, Zivan Karaman, Laxmi Parida category:cs.LG cs.CE  published:2013-10-07 summary:Whole genome prediction of complex phenotypic traits using high-density genotyping arrays has attracted a great deal of attention, as it is relevant to the fields of plant and animal breeding and genetic epidemiology. As the number of genotypes is generally much bigger than the number of samples, predictive models suffer from the curse-of-dimensionality. The curse-of-dimensionality problem not only affects the computational efficiency of a particular genomic selection method, but can also lead to poor performance, mainly due to correlation among markers. In this work we proposed the first transductive feature selection method based on the MRMR (Max-Relevance and Min-Redundancy) criterion which we call MINT. We applied MINT on genetic trait prediction problems and showed that in general MINT is a better feature selection method than the state-of-the-art inductive method mRMR. version:1
arxiv-1303-3901 | Efficient Evolutionary Algorithm for Single-Objective Bilevel Optimization | http://arxiv.org/abs/1303.3901 | id:1303.3901 author:Ankur Sinha, Pekka Malo, Kalyanmoy Deb category:cs.NE  published:2013-03-15 summary:Bilevel optimization problems are a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, the optimal solutions to the lower level problem become possible feasible candidates to the upper level problem. Such a requirement makes the optimization problem difficult to solve, and has kept the researchers busy towards devising methodologies, which can efficiently handle the problem. Despite the efforts, there hardly exists any effective methodology, which is capable of handling a complex bilevel problem. In this paper, we introduce bilevel evolutionary algorithm based on quadratic approximations (BLEAQ) of optimal lower level variables with respect to the upper level variables. The approach is capable of handling bilevel problems with different kinds of complexities in relatively smaller number of function evaluations. Ideas from classical optimization have been hybridized with evolutionary methods to generate an efficient optimization algorithm for generic bilevel problems. The efficacy of the algorithm has been shown on two sets of test problems. The first set is a recently proposed SMD test set, which contains problems with controllable complexities, and the second set contains standard test problems collected from the literature. The proposed method has been evaluated against two benchmarks, and the performance gain is observed to be significant. version:2
arxiv-1307-8187 | Towards Minimax Online Learning with Unknown Time Horizon | http://arxiv.org/abs/1307.8187 | id:1307.8187 author:Haipeng Luo, Robert E. Schapire category:cs.LG  published:2013-07-31 summary:We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some known distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which "pretends" that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen, the worst-case regret is of the optimal rate. Furthermore, our algorithm can be combined and applied in many ways, for instance, to online convex optimization, follow the perturbed leader, exponential weights algorithm and first order bounds. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting. version:2
arxiv-1310-1597 | Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning | http://arxiv.org/abs/1310.1597 | id:1310.1597 author:Mengqiu Wang, Christopher D. Manning category:cs.CL cs.AI  published:2013-10-06 summary:We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets. version:1
arxiv-1310-1590 | Evolution of the Modern Phase of Written Bangla: A Statistical Study | http://arxiv.org/abs/1310.1590 | id:1310.1590 author:Paheli Bhattacharya, Arnab Bhattacharya category:cs.CL I.2.7  published:2013-10-06 summary:Active languages such as Bangla (or Bengali) evolve over time due to a variety of social, cultural, economic, and political issues. In this paper, we analyze the change in the written form of the modern phase of Bangla quantitatively in terms of character-level, syllable-level, morpheme-level and word-level features. We collect three different types of corpora---classical, newspapers and blogs---and test whether the differences in their features are statistically significant. Results suggest that there are significant changes in the length of a word when measured in terms of characters, but there is not much difference in usage of different characters, syllables and morphemes in a word or of different words in a sentence. To the best of our knowledge, this is the first work on Bangla of this kind. version:1
arxiv-1306-2733 | Copula Mixed-Membership Stochastic Blockmodel for Intra-Subgroup Correlations | http://arxiv.org/abs/1306.2733 | id:1306.2733 author:Xuhui Fan, Longbing Cao, Richard Yi Da Xu category:cs.LG stat.ML  published:2013-06-12 summary:The \emph{Mixed-Membership Stochastic Blockmodel (MMSB)} is a popular framework for modeling social network relationships. It can fully exploit each individual node's participation (or membership) in a social structure. Despite its powerful representations, this model makes an assumption that the distributions of relational membership indicators between two nodes are independent. Under many social network settings, however, it is possible that certain known subgroups of people may have high or low correlations in terms of their membership categories towards each other, and such prior information should be incorporated into the model. To this end, we introduce a \emph{Copula Mixed-Membership Stochastic Blockmodel (cMMSB)} where an individual Copula function is employed to jointly model the membership pairs of those nodes within the subgroup of interest. The model enables the use of various Copula functions to suit the scenario, while maintaining the membership's marginal distribution, as needed, for modeling membership indicators with other nodes outside of the subgroup of interest. We describe the proposed model and its inference algorithm in detail for both the finite and infinite cases. In the experiment section, we compare our algorithms with other popular models in terms of link prediction, using both synthetic and real world data. version:2
arxiv-1310-1545 | Learning Hidden Structures with Relational Models by Adequately Involving Rich Information in A Network | http://arxiv.org/abs/1310.1545 | id:1310.1545 author:Xuhui Fan, Richard Yi Da Xu, Longbing Cao, Yin Song category:cs.LG cs.SI stat.ML  published:2013-10-06 summary:Effectively modelling hidden structures in a network is very practical but theoretically challenging. Existing relational models only involve very limited information, namely the binary directional link data, embedded in a network to learn hidden networking structures. There is other rich and meaningful information (e.g., various attributes of entities and more granular information than binary elements such as "like" or "dislike") missed, which play a critical role in forming and understanding relations in a network. In this work, we propose an informative relational model (InfRM) framework to adequately involve rich information and its granularity in a network, including metadata information about each entity and various forms of link data. Firstly, an effective metadata information incorporation method is employed on the prior information from relational models MMSB and LFRM. This is to encourage the entities with similar metadata information to have similar hidden structures. Secondly, we propose various solutions to cater for alternative forms of link data. Substantial efforts have been made towards modelling appropriateness and efficiency, for example, using conjugate priors. We evaluate our framework and its inference algorithms in different datasets, which shows the generality and effectiveness of our models in capturing implicit structures in networks. version:1
arxiv-1310-1531 | DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition | http://arxiv.org/abs/1310.1531 | id:1310.1531 author:Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell category:cs.CV  published:2013-10-06 summary:We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms. version:1
arxiv-1310-1518 | Contraction Principle based Robust Iterative Algorithms for Machine Learning | http://arxiv.org/abs/1310.1518 | id:1310.1518 author:Rangeet Mitra, Amit Kumar Mishra category:cs.LG stat.ML  published:2013-10-05 summary:Iterative algorithms are ubiquitous in the field of data mining. Widely known examples of such algorithms are the least mean square algorithm, backpropagation algorithm of neural networks. Our contribution in this paper is an improvement upon this iterative algorithms in terms of their respective performance metrics and robustness. This improvement is achieved by a new scaling factor which is multiplied to the error term. Our analysis shows that in essence, we are minimizing the corresponding LASSO cost function, which is the reason of its increased robustness. We also give closed form expressions for the number of iterations for convergence and the MSE floor of the original cost function for a minimum targeted value of the L1 norm. As a concluding theme based on the stochastic subgradient algorithm, we give a comparison between the well known Dantzig selector and our algorithm based on contraction principle. By these simulations we attempt to show the optimality of our approach for any widely used parent iterative optimization problem. version:1
arxiv-1308-3381 | High dimensional Sparse Gaussian Graphical Mixture Model | http://arxiv.org/abs/1308.3381 | id:1308.3381 author:Anani Lotsi, Ernst Wit category:stat.ML cs.LG  published:2013-08-15 summary:This paper considers the problem of networks reconstruction from heterogeneous data using a Gaussian Graphical Mixture Model (GGMM). It is well known that parameter estimation in this context is challenging due to large numbers of variables coupled with the degeneracy of the likelihood. We propose as a solution a penalized maximum likelihood technique by imposing an $l_{1}$ penalty on the precision matrix. Our approach shrinks the parameters thereby resulting in better identifiability and variable selection. We use the Expectation Maximization (EM) algorithm which involves the graphical LASSO to estimate the mixing coefficients and the precision matrices. We show that under certain regularity conditions the Penalized Maximum Likelihood (PML) estimates are consistent. We demonstrate the performance of the PML estimator through simulations and we show the utility of our method for high dimensional data analysis in a genomic application. version:3
arxiv-1310-1426 | Local Feature or Mel Frequency Cepstral Coefficients - Which One is Better for MLN-Based Bangla Speech Recognition? | http://arxiv.org/abs/1310.1426 | id:1310.1426 author:Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman, Mohammad Nasiruddin, Md. Abdul Latif, Mohammad Nurul Huda category:cs.CL 68T50 I.2.7  published:2013-10-05 summary:This paper discusses the dominancy of local features (LFs), as input to the multilayer neural network (MLN), extracted from a Bangla input speech over mel frequency cepstral coefficients (MFCCs). Here, LF-based method comprises three stages: (i) LF extraction from input speech, (ii) phoneme probabilities extraction using MLN from LF and (iii) the hidden Markov model (HMM) based classifier to obtain more accurate phoneme strings. In the experiments on Bangla speech corpus prepared by us, it is observed that the LFbased automatic speech recognition (ASR) system provides higher phoneme correct rate than the MFCC-based system. Moreover, the proposed system requires fewer mixture components in the HMMs. version:1
arxiv-1310-1425 | A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages | http://arxiv.org/abs/1310.1425 | id:1310.1425 author:Mohammad Nasiruddin category:cs.CL 68T50 I.2.7  published:2013-10-05 summary:Word Sense Disambiguation (WSD), the process of automatically identifying the meaning of a polysemous word in a sentence, is a fundamental task in Natural Language Processing (NLP). Progress in this approach to WSD opens up many promising developments in the field of NLP and its applications. Indeed, improvement over current performance levels could allow us to take a first step towards natural language understanding. Due to the lack of lexical resources it is sometimes difficult to perform WSD for under-resourced languages. This paper is an investigation on how to initiate research in WSD for under-resourced languages by applying Word Sense Induction (WSI) and suggests some interesting topics to focus on. version:1
arxiv-1310-1415 | Narrowing the Gap: Random Forests In Theory and In Practice | http://arxiv.org/abs/1310.1415 | id:1310.1415 author:Misha Denil, David Matheson, Nando de Freitas category:stat.ML cs.LG  published:2013-10-04 summary:Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoretically tractable variant of random regression forests and prove that our algorithm is consistent. We also provide an empirical evaluation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in practice. Our experiments provide insight into the relative importance of different simplifications that theoreticians have made to obtain tractable models for analysis. version:1
arxiv-1303-0582 | Multiple Kernel Sparse Representations for Supervised and Unsupervised Learning | http://arxiv.org/abs/1303.0582 | id:1303.0582 author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:cs.CV  published:2013-03-03 summary:In complex visual recognition tasks it is typical to adopt multiple descriptors, that describe different aspects of the images, for obtaining an improved recognition performance. Descriptors that have diverse forms can be fused into a unified feature space in a principled manner using kernel methods. Sparse models that generalize well to the test data can be learned in the unified kernel space, and appropriate constraints can be incorporated for application in supervised and unsupervised learning. In this paper, we propose to perform sparse coding and dictionary learning in the multiple kernel space, where the weights of the ensemble kernel are tuned based on graph-embedding principles such that class discrimination is maximized. In our proposed algorithm, dictionaries are inferred using multiple levels of 1-D subspace clustering in the kernel space, and the sparse codes are obtained using a simple levelwise pursuit scheme. Empirical results for object recognition and image clustering show that our algorithm outperforms existing sparse coding based approaches, and compares favorably to other state-of-the-art methods. version:2
arxiv-1310-1404 | Sequential Monte Carlo Bandits | http://arxiv.org/abs/1310.1404 | id:1310.1404 author:Michael Cherkassky, Luke Bornn category:stat.ML cs.LG stat.ME  published:2013-10-04 summary:In this paper we propose a flexible and efficient framework for handling multi-armed bandits, combining sequential Monte Carlo algorithms with hierarchical Bayesian modeling techniques. The framework naturally encompasses restless bandits, contextual bandits, and other bandit variants under a single inferential model. Despite the model's generality, we propose efficient Monte Carlo algorithms to make inference scalable, based on recent developments in sequential Monte Carlo methods. Through two simulation studies, the framework is shown to outperform other empirical methods, while also naturally scaling to more complex problems for which existing approaches can not cope. Additionally, we successfully apply our framework to online video-based advertising recommendation, and show its increased efficacy as compared to current state of the art bandit algorithms. version:1
arxiv-1310-1259 | A Novel Progressive Image Scanning and Reconstruction Scheme based on Compressed Sensing and Linear Prediction | http://arxiv.org/abs/1310.1259 | id:1310.1259 author:Giulio Coluccia, Enrico Magli category:cs.IT cs.CV math.IT  published:2013-10-04 summary:Compressed sensing (CS) is an innovative technique allowing to represent signals through a small number of their linear projections. In this paper we address the application of CS to the scenario of progressive acquisition of 2D visual signals in a line-by-line fashion. This is an important setting which encompasses diverse systems such as flatbed scanners and remote sensing imagers. The use of CS in such setting raises the problem of reconstructing a very high number of samples, as are contained in an image, from their linear projections. Conventional reconstruction algorithms, whose complexity is cubic in the number of samples, are computationally intractable. In this paper we develop an iterative reconstruction algorithm that reconstructs an image by iteratively estimating a row, and correlating adjacent rows by means of linear prediction. We develop suitable predictors and test the proposed algorithm in the context of flatbed scanners and remote sensing imaging systems. We show that this approach can significantly improve the results of separate reconstruction of each row, providing very good reconstruction quality with reasonable complexity. version:1
arxiv-1310-1249 | Reading Stockholm Riots 2013 in social media by text-mining | http://arxiv.org/abs/1310.1249 | id:1310.1249 author:Andrzej Jarynowski, Amir Rostami category:cs.SI cs.CL physics.soc-ph stat.AP  published:2013-10-04 summary:The riots in Stockholm in May 2013 were an event that reverberated in the world media for its dimension of violence that had spread through the Swedish capital. In this study we have investigated the role of social media in creating media phenomena via text mining and natural language processing. We have focused on two channels of communication for our analysis: Twitter and Poloniainfo.se (Forum of Polish community in Sweden). Our preliminary results show some hot topics driving discussion related mostly to Swedish Police and Swedish Politics by counting word usage. Typical features for media intervention are presented. We have built networks of most popular phrases, clustered by categories (geography, media institution, etc.). Sentiment analysis shows negative connotation with Police. The aim of this preliminary exploratory quantitative study was to generate questions and hypotheses, which we could carefully follow by deeper more qualitative methods. version:1
arxiv-1310-1227 | The Novel Approach of Adaptive Twin Probability for Genetic Algorithm | http://arxiv.org/abs/1310.1227 | id:1310.1227 author:Anagha P. Khedkar, Shaila Subbaraman category:cs.NE  published:2013-10-04 summary:The performance of GA is measured and analyzed in terms of its performance parameters against variations in its genetic operators and associated parameters. Since last four decades huge numbers of researchers have been working on the performance of GA and its enhancement. This earlier research work on analyzing the performance of GA enforces the need to further investigate the exploration and exploitation characteristics and observe its impact on the behavior and overall performance of GA. This paper introduces the novel approach of adaptive twin probability associated with the advanced twin operator that enhances the performance of GA. The design of the advanced twin operator is extrapolated from the twin offspring birth due to single ovulation in natural genetic systems as mentioned in the earlier works. The twin probability of this operator is adaptively varied based on the fitness of best individual thereby relieving the GA user from statically defining its value. This novel approach of adaptive twin probability is experimented and tested on the standard benchmark optimization test functions. The experimental results show the increased accuracy in terms of the best individual and reduced convergence time. version:1
arxiv-1310-1221 | Spatially Scalable Compressed Image Sensing with Hybrid Transform and Inter-layer Prediction Model | http://arxiv.org/abs/1310.1221 | id:1310.1221 author:Diego Valsesia, Enrico Magli category:cs.IT cs.CV cs.MM math.IT  published:2013-10-04 summary:Compressive imaging is an emerging application of compressed sensing, devoted to acquisition, encoding and reconstruction of images using random projections as measurements. In this paper we propose a novel method to provide a scalable encoding of an image acquired by means of compressed sensing techniques. Two bit-streams are generated to provide two distinct quality levels: a low-resolution base layer and full-resolution enhancement layer. In the proposed method we exploit a fast preview of the image at the encoder in order to perform inter-layer prediction and encode the prediction residuals only. The proposed method successfully provides resolution and quality scalability with modest complexity and it provides gains in the quality of the reconstructed images with respect to separate encoding of the quality layers. Remarkably, we also show that the scheme can also provide significant gains with respect to a direct, non-scalable system, thus accomplishing two features at once: scalability and improved reconstruction performance. version:1
arxiv-1310-1187 | Labeled Directed Acyclic Graphs: a generalization of context-specific independence in directed graphical models | http://arxiv.org/abs/1310.1187 | id:1310.1187 author:Johan Pensar, Henrik Nyman, Timo Koski, Jukka Corander category:stat.ML cs.AI cs.LG  published:2013-10-04 summary:We introduce a novel class of labeled directed acyclic graph (LDAG) models for finite sets of discrete variables. LDAGs generalize earlier proposals for allowing local structures in the conditional probability distribution of a node, such that unrestricted label sets determine which edges can be deleted from the underlying directed acyclic graph (DAG) for a given context. Several properties of these models are derived, including a generalization of the concept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is enabled by introducing an LDAG-based factorization of the Dirichlet prior for the model parameters, such that the marginal likelihood can be calculated analytically. In addition, we develop a novel prior distribution for the model structures that can appropriately penalize a model for its labeling complexity. A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill climbing approach is used for illustrating the useful properties of LDAG models for both real and synthetic data sets. version:1
arxiv-1303-2663 | Spectral Clustering with Epidemic Diffusion | http://arxiv.org/abs/1303.2663 | id:1303.2663 author:Laura M. Smith, Kristina Lerman, Cristina Garcia-Cardona, Allon G. Percus, Rumi Ghosh category:cs.SI cs.LG physics.soc-ph stat.ML I.5.3  published:2013-03-11 summary:Spectral clustering is widely used to partition graphs into distinct modules or communities. Existing methods for spectral clustering use the eigenvalues and eigenvectors of the graph Laplacian, an operator that is closely associated with random walks on graphs. We propose a new spectral partitioning method that exploits the properties of epidemic diffusion. An epidemic is a dynamic process that, unlike the random walk, simultaneously transitions to all the neighbors of a given node. We show that the replicator, an operator describing epidemic diffusion, is equivalent to the symmetric normalized Laplacian of a reweighted graph with edges reweighted by the eigenvector centralities of their incident nodes. Thus, more weight is given to edges connecting more central nodes. We describe a method that partitions the nodes based on the componentwise ratio of the replicator's second eigenvector to the first, and compare its performance to traditional spectral clustering techniques on synthetic graphs with known community structure. We demonstrate that the replicator gives preference to dense, clique-like structures, enabling it to more effectively discover communities that may be obscured by dense intercommunity linking. version:2
arxiv-1201-0022 | Spatio-temporal wavelet regularization for parallel MRI reconstruction: application to functional MRI | http://arxiv.org/abs/1201.0022 | id:1201.0022 author:Lotfi Chaari, Sébastien Mériaux, Jean-Christophe Pesquet, Philippe Ciuciu category:stat.AP cs.CV physics.med-ph  published:2011-12-23 summary:Parallel MRI is a fast imaging technique that enables the acquisition of highly resolved images in space or/and in time. The performance of parallel imaging strongly depends on the reconstruction algorithm, which can proceed either in the original k-space (GRAPPA, SMASH) or in the image domain (SENSE-like methods). To improve the performance of the widely used SENSE algorithm, 2D- or slice-specific regularization in the wavelet domain has been deeply investigated. In this paper, we extend this approach using 3D-wavelet representations in order to handle all slices together and address reconstruction artifacts which propagate across adjacent slices. The gain induced by such extension (3D-Unconstrained Wavelet Regularized -SENSE: 3D-UWR-SENSE) is validated on anatomical image reconstruction where no temporal acquisition is considered. Another important extension accounts for temporal correlations that exist between successive scans in functional MRI (fMRI). In addition to the case of 2D+t acquisition schemes addressed by some other methods like kt-FOCUSS, our approach allows us to deal with 3D+t acquisition schemes which are widely used in neuroimaging. The resulting 3D-UWR-SENSE and 4D-UWR-SENSE reconstruction schemes are fully unsupervised in the sense that all regularization parameters are estimated in the maximum likelihood sense on a reference scan. The gain induced by such extensions is illustrated on both anatomical and functional image reconstruction, and also measured in terms of statistical sensitivity for the 4D-UWR-SENSE approach during a fast event-related fMRI protocol. Our 4D-UWR-SENSE algorithm outperforms the SENSE reconstruction at the subject and group levels (15 subjects) for different contrasts of interest (eg, motor or computation tasks) and using different parallel acceleration factors (R=2 and R=4) on 2x2x3mm3 EPI images. version:3
arxiv-1310-1076 | Compressed Counting Meets Compressed Sensing | http://arxiv.org/abs/1310.1076 | id:1310.1076 author:Ping Li, Cun-Hui Zhang, Tong Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT  published:2013-10-03 summary:Compressed sensing (sparse signal recovery) has been a popular and important research topic in recent years. By observing that natural signals are often nonnegative, we propose a new framework for nonnegative signal recovery using Compressed Counting (CC). CC is a technique built on maximally-skewed p-stable random projections originally developed for data stream computations. Our recovery procedure is computationally very efficient in that it requires only one linear scan of the coordinates. Our analysis demonstrates that, when 0<p<=0.5, it suffices to use M= O(C/eps^p log N) measurements so that all coordinates will be recovered within eps additive precision, in one scan of the coordinates. The constant C=1 when p->0 and C=pi/2 when p=0.5. In particular, when p->0 the required number of measurements is essentially M=K\log N, where K is the number of nonzero coordinates of the signal. version:1
arxiv-1306-4947 | Machine Teaching for Bayesian Learners in the Exponential Family | http://arxiv.org/abs/1306.4947 | id:1306.4947 author:Xiaojin Zhu category:cs.LG  published:2013-06-20 summary:What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for finding the optimal teaching set. Our algorithm optimizes the aggregate sufficient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework. version:2
arxiv-1310-1022 | Multivariate regression and fit function uncertainty | http://arxiv.org/abs/1310.1022 | id:1310.1022 author:Peter Kovesarki, Ian C. Brock category:stat.ML stat.CO 62J02  published:2013-10-03 summary:This article describes a multivariate polynomial regression method where the uncertainty of the input parameters are approximated with Gaussian distributions, derived from the central limit theorem for large weighted sums, directly from the training sample. The estimated uncertainties can be propagated into the optimal fit function, as an alternative to the statistical bootstrap method. This uncertainty can be propagated further into a loss function like quantity, with which it is possible to calculate the expected loss function, and allows to select the optimal polynomial degree with statistical significance. Combined with simple phase space splitting methods, it is possible to model most features of the training data even with low degree polynomials or constants. version:1
arxiv-1310-0201 | Cross-Recurrence Quantification Analysis of Categorical and Continuous Time Series: an R package | http://arxiv.org/abs/1310.0201 | id:1310.0201 author:Moreno I. Coco, Rick Dale category:cs.CL stat.AP  published:2013-10-01 summary:This paper describes the R package crqa to perform cross-recurrence quantification analysis of two time series of either a categorical or continuous nature. Streams of behavioral information, from eye movements to linguistic elements, unfold over time. When two people interact, such as in conversation, they often adapt to each other, leading these behavioral levels to exhibit recurrent states. In dialogue, for example, interlocutors adapt to each other by exchanging interactive cues: smiles, nods, gestures, choice of words, and so on. In order for us to capture closely the goings-on of dynamic interaction, and uncover the extent of coupling between two individuals, we need to quantify how much recurrence is taking place at these levels. Methods available in crqa would allow researchers in cognitive science to pose such questions as how much are two people recurrent at some level of analysis, what is the characteristic lag time for one person to maximally match another, or whether one person is leading another. First, we set the theoretical ground to understand the difference between 'correlation' and 'co-visitation' when comparing two time series, using an aggregative or cross-recurrence approach. Then, we describe more formally the principles of cross-recurrence, and show with the current package how to carry out analyses applying them. We end the paper by comparing computational efficiency, and results' consistency, of crqa R package, with the benchmark MATLAB toolbox crptoolbox. We show perfect comparability between the two libraries on both levels. version:2
arxiv-1212-1666 | Developments in the theory of randomized shortest paths with a comparison of graph node distances | http://arxiv.org/abs/1212.1666 | id:1212.1666 author:Ilkka Kivimäki, Masashi Shimbo, Marco Saerens category:stat.ML  published:2012-12-07 summary:There have lately been several suggestions for parametrized distances on a graph that generalize the shortest path distance and the commute time or resistance distance. The need for developing such distances has risen from the observation that the above-mentioned common distances in many situations fail to take into account the global structure of the graph. In this article, we develop the theory of one family of graph node distances, known as the randomized shortest path dissimilarity, which has its foundation in statistical physics. We show that the randomized shortest path dissimilarity can be easily computed in closed form for all pairs of nodes of a graph. Moreover, we come up with a new definition of a distance measure that we call the free energy distance. The free energy distance can be seen as an upgrade of the randomized shortest path dissimilarity as it defines a metric, in addition to which it satisfies the graph-geodetic property. The derivation and computation of the free energy distance are also straightforward. We then make a comparison between a set of generalized distances that interpolate between the shortest path distance and the commute time, or resistance distance. This comparison focuses on the applicability of the distances in graph node clustering and classification. The comparison, in general, shows that the parametrized distances perform well in the tasks. In particular, we see that the results obtained with the free energy distance are among the best in all the experiments. version:2
arxiv-1310-0900 | Efficient pedestrian detection by directly optimize the partial area under the ROC curve | http://arxiv.org/abs/1310.0900 | id:1310.0900 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG  published:2013-10-03 summary:Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). Effective cascade-based classification, for example, depends on training node classifiers that achieve the maximal detection rate at a moderate false positive rate, e.g., around 40% to 50%. We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. By optimizing for different ranges of false positive rates, the proposed method can be used to train either a single strong classifier or a node classifier forming part of a cascade classifier. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the proposed structured ensemble learning method. version:1
arxiv-1310-0890 | Multiple Kernel Learning in the Primal for Multi-modal Alzheimer's Disease Classification | http://arxiv.org/abs/1310.0890 | id:1310.0890 author:Fayao Liu, Luping Zhou, Chunhua Shen, Jianping Yin category:cs.LG cs.CE  published:2013-10-03 summary:To achieve effective and efficient detection of Alzheimer's disease (AD), many machine learning methods have been introduced into this realm. However, the general case of limited training samples, as well as different feature representations typically makes this problem challenging. In this work, we propose a novel multiple kernel learning framework to combine multi-modal features for AD classification, which is scalable and easy to implement. Contrary to the usual way of solving the problem in the dual space, we look at the optimization from a new perspective. By conducting Fourier transform on the Gaussian kernel, we explicitly compute the mapping function, which leads to a more straightforward solution of the problem in the primal space. Furthermore, we impose the mixed $L_{21}$ norm constraint on the kernel weights, known as the group lasso regularization, to enforce group sparsity among different feature modalities. This actually acts as a role of feature modality selection, while at the same time exploiting complementary information among different kernels. Therefore it is able to extract the most discriminative features for classification. Experiments on the ADNI data set demonstrate the effectiveness of the proposed method. version:1
arxiv-1304-5758 | Prior-free and prior-dependent regret bounds for Thompson Sampling | http://arxiv.org/abs/1304.5758 | id:1304.5758 author:Sébastien Bubeck, Che-Yu Liu category:stat.ML cs.LG  published:2013-04-21 summary:We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by $\frac{1}{20} \sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors. version:2
arxiv-1310-0754 | Stemmers for Tamil Language: Performance Analysis | http://arxiv.org/abs/1310.0754 | id:1310.0754 author:M. Thangarasu, R. Manavalan category:cs.CL  published:2013-10-02 summary:Stemming is the process of extracting root word from the given inflection word and also plays significant role in numerous application of Natural Language Processing (NLP). Tamil Language raises several challenges to NLP, since it has rich morphological patterns than other languages. The rule based approach light-stemmer is proposed in this paper, to find stem word for given inflection Tamil word. The performance of proposed approach is compared to a rule based suffix removal stemmer based on correctly and incorrectly predicted. The experimental result clearly show that the proposed approach light stemmer for Tamil language perform better than suffix removal stemmer and also more effective in Information Retrieval System (IRS). version:1
arxiv-1310-0307 | Using the Random Sprays Retinex Algorithm for Global Illumination Estimation | http://arxiv.org/abs/1310.0307 | id:1310.0307 author:Nikola Banić, Sven Lončarić category:cs.CV  published:2013-10-01 summary:In this paper the use of Random Sprays Retinex (RSR) algorithm for global illumination estimation is proposed and its feasibility tested. Like other algorithms based on the Retinex model, RSR also provides local illumination estimation and brightness adjustment for each pixel and it is faster than other path-wise Retinex algorithms. As the assumption of the uniform illumination holds in many cases, it should be possible to use the mean of local illumination estimations of RSR as a global illumination estimation for images with (assumed) uniform illumination allowing also the accuracy to be easily measured. Therefore we propose a method for estimating global illumination estimation based on local RSR results. To our best knowledge this is the first time that RSR algorithm is used to obtain global illumination estimation. For our tests we use a publicly available color constancy image database for testing. The results are presented and discussed and it turns out that the proposed method outperforms many existing unsupervised color constancy algorithms. The source code is available at http://www.fer.unizg.hr/ipg/resources/color_constancy/. version:2
arxiv-1308-1004 | Boundary identification of events in clinical named entity recognition | http://arxiv.org/abs/1308.1004 | id:1308.1004 author:Azad Dehghan category:cs.CL  published:2013-08-05 summary:The problem of named entity recognition in the medical/clinical domain has gained increasing attention do to its vital role in a wide range of clinical decision support applications. The identification of complete and correct term span is vital for further knowledge synthesis (e.g., coding/mapping concepts thesauruses and classification standards). This paper investigates boundary adjustment by sequence labeling representations models and post-processing techniques in the problem of clinical named entity recognition (recognition of clinical events). Using current state-of-the-art sequence labeling algorithm (conditional random fields), we show experimentally that sequence labeling representation and post-processing can be significantly helpful in strict boundary identification of clinical events. version:3
arxiv-1310-0581 | Rule Based Stemmer in Urdu | http://arxiv.org/abs/1310.0581 | id:1310.0581 author:Vaishali Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-10-02 summary:Urdu is a combination of several languages like Arabic, Hindi, English, Turkish, Sanskrit etc. It has a complex and rich morphology. This is the reason why not much work has been done in Urdu language processing. Stemming is used to convert a word into its respective root form. In stemming, we separate the suffix and prefix from the word. It is useful in search engines, natural language processing and word processing, spell checkers, word parsing, word frequency and count studies. This paper presents a rule based stemmer for Urdu. The stemmer that we have discussed here is used in information retrieval. We have also evaluated our results by verifying it with a human expert. version:1
arxiv-1310-0578 | Subjective and Objective Evaluation of English to Urdu Machine Translation | http://arxiv.org/abs/1310.0578 | id:1310.0578 author:Vaishali Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-10-02 summary:Machine translation is research based area where evaluation is very important phenomenon for checking the quality of MT output. The work is based on the evaluation of English to Urdu Machine translation. In this research work we have evaluated the translation quality of Urdu language which has been translated by using different Machine Translation systems like Google, Babylon and Ijunoon. The evaluation process is done by using two approaches - Human evaluation and Automatic evaluation. We have worked for both the approaches where in human evaluation emphasis is given to scales and parameters while in automatic evaluation emphasis is given to some automatic metric such as BLEU, GTM, METEOR and ATEC. version:1
arxiv-1310-0576 | Learning Lambek grammars from proof frames | http://arxiv.org/abs/1310.0576 | id:1310.0576 author:Roberto Bonato, Christian Retoré category:cs.LG cs.AI cs.LO math.LO F.4.1; I.2.6; I.2.7  published:2013-10-02 summary:In addition to their limpid interface with semantics, categorial grammars enjoy another important property: learnability. This was first noticed by Buskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorial grammars. What about Lambek categorial grammars? In a previous paper we showed that product free Lambek grammars where learnable from structured sentences, the structures being incomplete natural deductions. These grammars were shown to be unlearnable from strings by Foret and Le Nir. In the present paper we show that Lambek grammars, possibly with product, are learnable from proof frames that are incomplete proof nets. After a short reminder on grammatical inference \`a la Gold, we provide an algorithm that learns Lambek grammars with product from proof frames and we prove its convergence. We do so for 1-valued also known as rigid Lambek grammars with product, since standard techniques can extend our result to $k$-valued grammars. Because of the correspondence between cut-free proof nets and normal natural deductions, our initial result on product free Lambek grammars can be recovered. We are sad to dedicate the present paper to Philippe Darondeau, with whom we started to study such questions in Rennes at the beginning of the millennium, and who passed away prematurely. We are glad to dedicate the present paper to Jim Lambek for his 90 birthday: he is the living proof that research is an eternal learning process. version:1
arxiv-1310-0573 | Improving the Quality of MT Output using Novel Name Entity Translation Scheme | http://arxiv.org/abs/1310.0573 | id:1310.0573 author:Deepti Bhalla, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-10-02 summary:This paper presents a novel approach to machine translation by combining the state of art name entity translation scheme. Improper translation of name entities lapse the quality of machine translated output. In this work, name entities are transliterated by using statistical rule based approach. This paper describes the translation and transliteration of name entities from English to Punjabi. We have experimented on four types of name entities which are: Proper names, Location names, Organization names and miscellaneous. Various rules for the purpose of syllabification have been constructed. Transliteration of name entities is accomplished with the help of Probability calculation. N-Gram probabilities for the extracted syllables have been calculated using statistical machine translation toolkit MOSES. version:1
arxiv-1309-0123 | A Robust Alternating Direction Method for Constrained Hybrid Variational Deblurring Model | http://arxiv.org/abs/1309.0123 | id:1309.0123 author:Ryan Wen Liu, Tian Xu category:cs.CV 65K10  68U10 I.4.4; G.1.6  published:2013-08-31 summary:In this work, a new constrained hybrid variational deblurring model is developed by combining the non-convex first- and second-order total variation regularizers. Moreover, a box constraint is imposed on the proposed model to guarantee high deblurring performance. The developed constrained hybrid variational model could achieve a good balance between preserving image details and alleviating ringing artifacts. In what follows, we present the corresponding numerical solution by employing an iteratively reweighted algorithm based on alternating direction method of multipliers. The experimental results demonstrate the superior performance of the proposed method in terms of quantitative and qualitative image quality assessments. version:2
arxiv-1310-0432 | Online Learning of Dynamic Parameters in Social Networks | http://arxiv.org/abs/1310.0432 | id:1310.0432 author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI stat.ML  published:2013-10-01 summary:This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance. Then, we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth, per individual. We observe that only one of the estimators recovers the optimal MSD, which underscores the impact of the objective function decomposition on the learning quality. Finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a finite time. version:1
arxiv-1310-0376 | Joint Bayesian estimation of close subspaces from noisy measurements | http://arxiv.org/abs/1310.0376 | id:1310.0376 author:Olivier Besson, Nicolas Dobigeon, Jean-Yves Tourneret category:stat.ME stat.ML  published:2013-10-01 summary:In this letter, we consider two sets of observations defined as subspace signals embedded in noise and we wish to analyze the distance between these two subspaces. The latter entails evaluating the angles between the subspaces, an issue reminiscent of the well-known Procrustes problem. A Bayesian approach is investigated where the subspaces of interest are considered as random with a joint prior distribution (namely a Bingham distribution), which allows the closeness of the two subspaces to be adjusted. Within this framework, the minimum mean-square distance estimator of both subspaces is formulated and implemented via a Gibbs sampler. A simpler scheme based on alternative maximum a posteriori estimation is also presented. The new schemes are shown to provide more accurate estimates of the angles between the subspaces, compared to singular value decomposition based independent estimation of the two subspaces. version:1
arxiv-1310-0365 | The complex-valued encoding for dicision-making based on aliasing data | http://arxiv.org/abs/1310.0365 | id:1310.0365 author:P. A. Golovinski, V. A. Astapenko category:cs.CV  published:2013-10-01 summary:It is proposed a complex valued channel encoding for multidimensional data. The basic approach contains overlapping of complex nonlinear mappings. Its development leads to sparse representation of multi-channel data, increasing their dimensions and the distance between the images. version:1
arxiv-1310-0317 | An Overview and Evaluation of Various Face and Eyes Detection Algorithms for Driver Fatigue Monitoring Systems | http://arxiv.org/abs/1310.0317 | id:1310.0317 author:Markan Lopar, Slobodan Ribarić category:cs.CV  published:2013-10-01 summary:In this work various methods and algorithms for face and eyes detection are examined in order to decide which of them are applicable for use in a driver fatigue monitoring system. In the case of face detection the standard Viola-Jones face detector has shown best results, while the method of finding the eye centers by means of gradients has proven to be most appropriate in the case of eyes detection. The later method has also a potential for retrieving behavioral parameters needed for estimation of the level of driver fatigue. This possibility will be examined in future work. version:1
arxiv-1310-0316 | Classifying Traffic Scenes Using The GIST Image Descriptor | http://arxiv.org/abs/1310.0316 | id:1310.0316 author:Ivan Sikirić, Karla Brkić, Siniša Šegvić category:cs.CV  published:2013-10-01 summary:This paper investigates classification of traffic scenes in a very low bandwidth scenario, where an image should be coded by a small number of features. We introduce a novel dataset, called the FM1 dataset, consisting of 5615 images of eight different traffic scenes: open highway, open road, settlement, tunnel, tunnel exit, toll booth, heavy traffic and the overpass. We evaluate the suitability of the GIST descriptor as a representation of these images, first by exploring the descriptor space using PCA and k-means clustering, and then by using an SVM classifier and recording its 10-fold cross-validation performance on the introduced FM1 dataset. The obtained recognition rates are very encouraging, indicating that the use of the GIST descriptor alone could be sufficiently descriptive even when very high performance is required. version:1
arxiv-1310-0315 | Computer Vision Systems in Road Vehicles: A Review | http://arxiv.org/abs/1310.0315 | id:1310.0315 author:Kristian Kovačić, Edouard Ivanjko, Hrvoje Gold category:cs.CV  published:2013-10-01 summary:The number of road vehicles significantly increased in recent decades. This trend accompanied a build-up of road infrastructure and development of various control systems to increase road traffic safety, road capacity and travel comfort. In traffic safety significant development has been made and today's systems more and more include cameras and computer vision methods. Cameras are used as part of the road infrastructure or in vehicles. In this paper a review on computer vision systems in vehicles from the stand point of traffic engineering is given. Safety problems of road vehicles are presented, current state of the art in-vehicle vision systems is described and open problems with future research directions are discussed. version:1
arxiv-1310-0314 | Global Localization Based on 3D Planar Surface Segments | http://arxiv.org/abs/1310.0314 | id:1310.0314 author:Robert Cupec, Emmanuel Karlo Nyarko, Damir Filko, Andrej Kitanov, Ivan Petrović category:cs.CV  published:2013-10-01 summary:Global localization of a mobile robot using planar surface segments extracted from depth images is considered. The robot's environment is represented by a topological map consisting of local models, each representing a particular location modeled by a set of planar surface segments. The discussed localization approach segments a depth image acquired by a 3D camera into planar surface segments which are then matched to model surface segments. The robot pose is estimated by the Extended Kalman Filter using surface segment pairs as measurements. The reliability and accuracy of the considered approach are experimentally evaluated using a mobile robot equipped by a Microsoft Kinect sensor. version:1
arxiv-1310-0311 | Multiclass Road Sign Detection using Multiplicative Kernel | http://arxiv.org/abs/1310.0311 | id:1310.0311 author:Valentina Zadrija, Siniša Šegvić category:cs.CV  published:2013-10-01 summary:We consider the problem of multiclass road sign detection using a classification function with multiplicative kernel comprised from two kernels. We show that problems of detection and within-foreground classification can be jointly solved by using one kernel to measure object-background differences and another one to account for within-class variations. The main idea behind this approach is that road signs from different foreground variations can share features that discriminate them from backgrounds. The classification function training is accomplished using SVM, thus feature sharing is obtained through support vector sharing. Training yields a family of linear detectors, where each detector corresponds to a specific foreground training sample. The redundancy among detectors is alleviated using k-medoids clustering. Finally, we report detection and classification results on a set of road sign images obtained from a camera on a moving vehicle. version:1
arxiv-1310-0310 | A Novel Georeferenced Dataset for Stereo Visual Odometry | http://arxiv.org/abs/1310.0310 | id:1310.0310 author:Ivan Krešo, Marko Ševrović, Siniša Šegvić category:cs.CV  published:2013-10-01 summary:In this work, we present a novel dataset for assessing the accuracy of stereo visual odometry. The dataset has been acquired by a small-baseline stereo rig mounted on the top of a moving car. The groundtruth is supplied by a consumer grade GPS device without IMU. Synchronization and alignment between GPS readings and stereo frames are recovered after the acquisition. We show that the attained groundtruth accuracy allows to draw useful conclusions in practice. The presented experiments address influence of camera calibration, baseline distance and zero-disparity features to the achieved reconstruction performance. version:1
arxiv-1310-0308 | Combining Spatio-Temporal Appearance Descriptors and Optical Flow for Human Action Recognition in Video Data | http://arxiv.org/abs/1310.0308 | id:1310.0308 author:Karla Brkić, Srđan Rašić, Axel Pinz, Siniša Šegvić, Zoran Kalafatić category:cs.CV  published:2013-10-01 summary:This paper proposes combining spatio-temporal appearance (STA) descriptors with optical flow for human action recognition. The STA descriptors are local histogram-based descriptors of space-time, suitable for building a partial representation of arbitrary spatio-temporal phenomena. Because of the possibility of iterative refinement, they are interesting in the context of online human action recognition. We investigate the use of dense optical flow as the image function of the STA descriptor for human action recognition, using two different algorithms for computing the flow: the Farneb\"ack algorithm and the TVL1 algorithm. We provide a detailed analysis of the influencing optical flow algorithm parameters on the produced optical flow fields. An extensive experimental validation of optical flow-based STA descriptors in human action recognition is performed on the KTH human action dataset. The encouraging experimental results suggest the potential of our approach in online human action recognition. version:1
arxiv-1310-0306 | Flexible Visual Quality Inspection in Discrete Manufacturing | http://arxiv.org/abs/1310.0306 | id:1310.0306 author:Tomislav Petković, Darko Jurić, Sven Lončarić category:cs.CV  published:2013-10-01 summary:Most visual quality inspections in discrete manufacturing are composed of length, surface, angle or intensity measurements. Those are implemented as end-user configurable inspection tools that should not require an image processing expert to set up. Currently available software solutions providing such capability use a flowchart based programming environment, but do not fully address an inspection flowchart robustness and can require a redefinition of the flowchart if a small variation is introduced. In this paper we propose an acquire-register-analyze image processing pattern designed for discrete manufacturing that aims to increase the robustness of the inspection flowchart by consistently addressing variations in product position, orientation and size. A proposed pattern is transparent to the end-user and simplifies the flowchart. We describe a developed software solution that is a practical implementation of the proposed pattern. We give an example of its real-life use in industrial production of electric components. version:1
arxiv-1310-0305 | Filtering for More Accurate Dense Tissue Segmentation in Digitized Mammograms | http://arxiv.org/abs/1310.0305 | id:1310.0305 author:Mario Muštra, Mislav Grgić category:cs.CV  published:2013-10-01 summary:Breast tissue segmentation into dense and fat tissue is important for determining the breast density in mammograms. Knowing the breast density is important both in diagnostic and computer-aided detection applications. There are many different ways to express the density of a breast and good quality segmentation should provide the possibility to perform accurate classification no matter which classification rule is being used. Knowing the right breast density and having the knowledge of changes in the breast density could give a hint of a process which started to happen within a patient. Mammograms generally suffer from a problem of different tissue overlapping which results in the possibility of inaccurate detection of tissue types. Fibroglandular tissue presents rather high attenuation of X-rays and is visible as brighter in the resulting image but overlapping fibrous tissue and blood vessels could easily be replaced with fibroglandular tissue in automatic segmentation algorithms. Small blood vessels and microcalcifications are also shown as bright objects with similar intensities as dense tissue but do have some properties which makes possible to suppress them from the final results. In this paper we try to divide dense and fat tissue by suppressing the scattered structures which do not represent glandular or dense tissue in order to divide mammograms more accurately in the two major tissue types. For suppressing blood vessels and microcalcifications we have used Gabor filters of different size and orientation and a combination of morphological operations on filtered image with enhanced contrast. version:1
arxiv-1310-0302 | Surface Registration Using Genetic Algorithm in Reduced Search Space | http://arxiv.org/abs/1310.0302 | id:1310.0302 author:Vedran Hrgetić, Tomislav Pribanić category:cs.CV  published:2013-10-01 summary:Surface registration is a technique that is used in various areas such as object recognition and 3D model reconstruction. Problem of surface registration can be analyzed as an optimization problem of seeking a rigid motion between two different views. Genetic algorithms can be used for solving this optimization problem, both for obtaining the robust parameter estimation and for its fine-tuning. The main drawback of genetic algorithms is that they are time consuming which makes them unsuitable for online applications. Modern acquisition systems enable the implementation of the solutions that would immediately give the information on the rotational angles between the different views, thus reducing the dimension of the optimization problem. The paper gives an analysis of the genetic algorithm implemented in the conditions when the rotation matrix is known and a comparison of these results with results when this information is not available. version:1
arxiv-1107-0193 | On the origin of ambiguity in efficient communication | http://arxiv.org/abs/1107.0193 | id:1107.0193 author:Jordi Fortuny, Bernat Corominas-Murtra category:cs.CL  published:2011-07-01 summary:This article studies the emergence of ambiguity in communication through the concept of logical irreversibility and within the framework of Shannon's information theory. This leads us to a precise and general expression of the intuition behind Zipf's vocabulary balance in terms of a symmetry equation between the complexities of the coding and the decoding processes that imposes an unavoidable amount of logical uncertainty in natural communication. Accordingly, the emergence of irreversible computations is required if the complexities of the coding and the decoding processes are balanced in a symmetric scenario, which means that the emergence of ambiguous codes is a necessary condition for natural communication to succeed. version:3
arxiv-1310-0171 | Object Detection Using Keygraphs | http://arxiv.org/abs/1310.0171 | id:1310.0171 author:Marcelo Hashimoto, Roberto Marcondes Cesar Junior category:cs.CV  published:2013-10-01 summary:We propose a new framework for object detection based on a generalization of the keypoint correspondence framework. This framework is based on replacing keypoints by keygraphs, i.e. isomorph directed graphs whose vertices are keypoints, in order to explore relative and structural information. Unlike similar works in the literature, we deal directly with graphs in the entire pipeline: we search for graph correspondences instead of searching for individual point correspondences and then building graph correspondences from them afterwards. We also estimate the pose from graph correspondences instead of falling back to point correspondences through a voting table. The contributions of this paper are the proposed framework and an implementation that properly handles its inherent issues of loss of locality and combinatorial explosion, showing its viability for real-time applications. In particular, we introduce the novel concept of keytuples to solve a running time issue. The accuracy of the implementation is shown by results of over 800 experiments with a well-known database of images. The speed is illustrated by real-time tracking with two different cameras in ordinary hardware. version:1
arxiv-1303-4207 | Improving CUR Matrix Decomposition and the Nyström Approximation via Adaptive Sampling | http://arxiv.org/abs/1303.4207 | id:1303.4207 author:Shusen Wang, Zhihua Zhang category:cs.LG cs.NA  published:2013-03-18 summary:The CUR matrix decomposition and the Nystr\"{o}m approximation are two important low-rank matrix approximation techniques. The Nystr\"{o}m method approximates a symmetric positive semidefinite matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystr\"{o}m approximation. In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr\"{o}m algorithms with expected relative-error bounds. The proposed CUR and Nystr\"{o}m algorithms also have low time complexity and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystr\"{o}m method and the ensemble Nystr\"{o}m method. The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices. version:7
arxiv-1309-7512 | Structured learning of sum-of-submodular higher order energy functions | http://arxiv.org/abs/1309.7512 | id:1309.7512 author:Alexander Fix, Thorsten Joachims, Sam Park, Ramin Zabih category:cs.CV cs.LG stat.ML  published:2013-09-28 summary:Submodular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow \cite{KZ:PAMI04} has had significant impact in computer vision \cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we address the important class of sum-of-submodular (SoS) functions \cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via a variant of max flow called submodular flow \cite{Edmonds:ADM77}. SoS functions can naturally express higher order priors involving, e.g., local image patches; however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach \cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the training problem in terms of quadratic programming; as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems \cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flow problem. Experimental comparisons are made against the OpenCV implementation of the GrabCut interactive segmentation technique \cite{Rother:GrabCut04}, which uses hand-tuned parameters instead of machine learning. On a standard dataset \cite{Gulshan:CVPR10} our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels. version:2
arxiv-1310-0110 | An information measure for comparing top $k$ lists | http://arxiv.org/abs/1310.0110 | id:1310.0110 author:Arun Konagurthu, James Collier category:cs.IT cs.LG math.IT  published:2013-10-01 summary:Comparing the top $k$ elements between two or more ranked results is a common task in many contexts and settings. A few measures have been proposed to compare top $k$ lists with attractive mathematical properties, but they face a number of pitfalls and shortcomings in practice. This work introduces a new measure to compare any two top k lists based on measuring the information these lists convey. Our method investigates the compressibility of the lists, and the length of the message to losslessly encode them gives a natural and robust measure of their variability. This information-theoretic measure objectively reconciles all the main considerations that arise when measuring (dis-)similarity between lists: the extent of their non-overlapping elements in each of the lists; the amount of disarray among overlapping elements between the lists; the measurement of displacement of actual ranks of their overlapping elements. version:1
arxiv-1310-0036 | Personal Identification from Lip-Print Features using a Statistical Model | http://arxiv.org/abs/1310.0036 | id:1310.0036 author:Saptarshi Bhattacharjee, S Arunkumar, Samir Kumar Bandyopadhyay category:cs.CV  published:2013-09-30 summary:This paper presents a novel approach towards identification of human beings from the statistical analysis of their lip prints. Lip features are extracted by studying the spatial orientations of the grooves present in lip prints of individuals using standard edge detection techniques. Horizontal, vertical and diagonal groove features are analysed using connected-component analysis to generate the region-specific edge datasets. Comparison between test and reference sample datasets against a threshold value to define a match yield satisfactory results. FAR, FRR and ROC metrics have been used to gauge the performance of the algorithm for real-world deployment in unimodal and multimodal biometric verification systems. version:1
arxiv-1309-7912 | An Image-Based Fluid Surface Pattern Model | http://arxiv.org/abs/1309.7912 | id:1309.7912 author:Mauro de Amorim, Ricardo Fabbri, Lucia Maria dos Santos Pinto, Francisco Duarte Moura Neto category:cs.CV  published:2013-09-30 summary:This work aims at generating a model of the ocean surface and its dynamics from one or more video cameras. The idea is to model wave patterns from video as a first step towards a larger system of photogrammetric monitoring of marine conditions for use in offshore oil drilling platforms. The first part of the proposed approach consists in reducing the dimensionality of sensor data made up of the many pixels of each frame of the input video streams. This enables finding a concise number of most relevant parameters to model the temporal dataset, yielding an efficient data-driven model of the evolution of the observed surface. The second part proposes stochastic modeling to better capture the patterns embedded in the data. One can then draw samples from the final model, which are expected to simulate the behavior of previously observed flow, in order to determine conditions that match new observations. In this paper we focus on proposing and discussing the overall approach and on comparing two different techniques for dimensionality reduction in the first stage: principal component analysis and diffusion maps. Work is underway on the second stage of constructing better stochastic models of fluid surface dynamics as proposed here. version:1
arxiv-1309-7824 | Linear Regression as a Non-Cooperative Game | http://arxiv.org/abs/1309.7824 | id:1309.7824 author:Stratis Ioannidis, Patrick Loiseau category:cs.GT cs.LG math.ST stat.TH  published:2013-09-30 summary:Linear regression amounts to estimating a linear model that maps features (e.g., age or gender) to corresponding data (e.g., the answer to a survey or the outcome of a medical exam). It is a ubiquitous tool in experimental sciences. We study a setting in which features are public but the data is private information. While the estimation of the linear model may be useful to participating individuals, (if, e.g., it leads to the discovery of a treatment to a disease), individuals may be reluctant to disclose their data due to privacy concerns. In this paper, we propose a generic game-theoretic model to express this trade-off. Users add noise to their data before releasing it. In particular, they choose the variance of this noise to minimize a cost comprising two components: (a) a privacy cost, representing the loss of privacy incurred by the release; and (b) an estimation cost, representing the inaccuracy in the linear model estimate. We study the Nash equilibria of this game, establishing the existence of a unique non-trivial equilibrium. We determine its efficiency for several classes of privacy and estimation costs, using the concept of the price of stability. Finally, we prove that, for a specific estimation cost, the generalized least-square estimator is optimal among all linear unbiased estimators in our non-cooperative setting: this result extends the famous Aitken/Gauss-Markov theorem in statistics, establishing that its conclusion persists even in the presence of strategic individuals. version:1
arxiv-1309-7804 | On statistics, computation and scalability | http://arxiv.org/abs/1309.7804 | id:1309.7804 author:Michael I. Jordan category:stat.ML cs.LG math.ST stat.TH  published:2013-09-30 summary:How should statistical procedures be designed so as to be scalable computationally to the massive datasets that are increasingly the norm? When coupled with the requirement that an answer to an inferential question be delivered within a certain time budget, this question has significant repercussions for the field of statistics. With the goal of identifying "time-data tradeoffs," we investigate some of the statistical consequences of computational perspectives on scability, in particular divide-and-conquer methodology and hierarchies of convex relaxations. version:1
arxiv-1211-1328 | Random walk kernels and learning curves for Gaussian process regression on random graphs | http://arxiv.org/abs/1211.1328 | id:1211.1328 author:Matthew Urry, Peter Sollich category:stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG  published:2012-11-06 summary:We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike, graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance matrices of e.g.\ Gaussian processes (GPs). In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from the probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation make it clear that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs. version:2
arxiv-1309-5594 | Generic Image Classification Approaches Excel on Face Recognition | http://arxiv.org/abs/1309.5594 | id:1309.5594 author:Fumin Shen, Chunhua Shen category:cs.CV  published:2013-09-22 summary:The main finding of this work is that the standard image classification pipeline, which consists of dictionary learning, feature encoding, spatial pyramid pooling and linear classification, outperforms all state-of-the-art face recognition methods on the tested benchmark datasets (we have tested on AR, Extended Yale B, the challenging FERET, and LFW-a datasets). This surprising and prominent result suggests that those advances in generic image classification can be directly applied to improve face recognition systems. In other words, face recognition may not need to be viewed as a separate object classification problem. While recently a large body of residual based face recognition methods focus on developing complex dictionary learning algorithms, in this work we show that a dictionary of randomly extracted patches (even from non-face images) can achieve very promising results using the image classification pipeline. That means, the choice of dictionary learning methods may not be important. Instead, we find that learning multiple dictionaries using different low-level image features often improve the final classification accuracy. Our proposed face recognition approach offers the best reported results on the widely-used face recognition benchmark datasets. In particular, on the challenging FERET and LFW-a datasets, we improve the best reported accuracies in the literature by about 20% and 30% respectively. version:2
arxiv-1309-7698 | Signed Networks, Triadic Interactions and the Evolution of Cooperation | http://arxiv.org/abs/1309.7698 | id:1309.7698 author:Simone Righi, Károly Takács category:cs.SI cs.GT cs.NE physics.soc-ph  published:2013-09-30 summary:We outline a model to study the evolution of cooperation in a population of agents playing the prisoner's dilemma in signed networks. We highlight that if only dyadic interactions are taken into account, cooperation never evolves. However, when triadic considerations are introduced, a window of opportunity for emergence of cooperation as a stable behaviour emerges. version:1
arxiv-1309-7697 | Semi-structured data extraction and modelling: the WIA Project | http://arxiv.org/abs/1309.7697 | id:1309.7697 author:Gianluca Colombo, Ettore Colombo, Andrea Bonomi, Alessandro Mosca, Simone Bassis category:cs.SE cs.CY cs.NE H3; I.2; H.1.2  published:2013-09-30 summary:Over the last decades, the amount of data of all kinds available electronically has increased dramatically. Data are accessible through a range of interfaces including Web browsers, database query languages, application-specific interfaces, built on top of a number of different data exchange formats. All these data span from un-structured to highly structured data. Very often, some of them have structure even if the structure is implicit, and not as rigid or regular as that found in standard database systems. Spreadsheet documents are prototypical in this respect. Spreadsheets are the lightweight technology able to supply companies with easy to build business management and business intelligence applications, and business people largely adopt spreadsheets as smart vehicles for data files generation and sharing. Actually, the more spreadsheets grow in complexity (e.g., their use in product development plans and quoting), the more their arrangement, maintenance, and analysis appear as a knowledge-driven activity. The algorithmic approach to the problem of automatic data structure extraction from spreadsheet documents (i.e., grid-structured and free topological-related data) emerges from the WIA project: Worksheets Intelligent Analyser. The WIA-algorithm shows how to provide a description of spreadsheet contents in terms of higher level of abstractions or conceptualisations. In particular, the WIA-algorithm target is about the extraction of i) the calculus work-flow implemented in the spreadsheets formulas and ii) the logical role played by the data which take part into the calculus. The aim of the resulting conceptualisations is to provide spreadsheets with abstract representations useful for further model refinements and optimizations through evolutionary algorithms computations. version:1
arxiv-1309-7690 | A Hybrid Monte Carlo Ant Colony Optimization Approach for Protein Structure Prediction in the HP Model | http://arxiv.org/abs/1309.7690 | id:1309.7690 author:Andrea G. Citrolo, Giancarlo Mauri category:cs.NE cs.CE  published:2013-09-30 summary:The hydrophobic-polar (HP) model has been widely studied in the field of protein structure prediction (PSP) both for theoretical purposes and as a benchmark for new optimization strategies. In this work we introduce a new heuristics based on Ant Colony Optimization (ACO) and Markov Chain Monte Carlo (MCMC) that we called Hybrid Monte Carlo Ant Colony Optimization (HMCACO). We describe this method and compare results obtained on well known HP instances in the 3 dimensional cubic lattice to those obtained with standard ACO and Simulated Annealing (SA). All methods were implemented using an unconstrained neighborhood and a modified objective function to prevent the creation of overlapping walks. Results show that our methods perform better than the other heuristics in all benchmark instances. version:1
arxiv-1309-7676 | An upper bound on prototype set size for condensed nearest neighbor | http://arxiv.org/abs/1309.7676 | id:1309.7676 author:Eric Christiansen category:cs.LG stat.ML  published:2013-09-29 summary:The condensed nearest neighbor (CNN) algorithm is a heuristic for reducing the number of prototypical points stored by a nearest neighbor classifier, while keeping the classification rule given by the reduced prototypical set consistent with the full set. I present an upper bound on the number of prototypical points accumulated by CNN. The bound originates in a bound on the number of times the decision rule is updated during training in the multiclass perceptron algorithm, and thus is independent of training set size. version:1
arxiv-1306-6843 | Error AMP Chain Graphs | http://arxiv.org/abs/1306.6843 | id:1306.6843 author:Jose M. Peña category:stat.ML cs.AI  published:2013-06-28 summary:Any regular Gaussian probability distribution that can be represented by an AMP chain graph (CG) can be expressed as a system of linear equations with correlated errors whose structure depends on the CG. However, the CG represents the errors implicitly, as no nodes in the CG correspond to the errors. We propose in this paper to add some deterministic nodes to the CG in order to represent the errors explicitly. We call the result an EAMP CG. We will show that, as desired, every AMP CG is Markov equivalent to its corresponding EAMP CG under marginalization of the error nodes. We will also show that every EAMP CG under marginalization of the error nodes is Markov equivalent to some LWF CG under marginalization of the error nodes, and that the latter is Markov equivalent to some directed and acyclic graph (DAG) under marginalization of the error nodes and conditioning on some selection nodes. This is important because it implies that the independence model represented by an AMP CG can be accounted for by some data generating process that is partially observed and has selection bias. Finally, we will show that EAMP CGs are closed under marginalization. This is a desirable feature because it guarantees parsimonious models under marginalization. version:2
arxiv-1309-7615 | Correcting Multi-focus Images via Simple Standard Deviation for Image Fusion | http://arxiv.org/abs/1309.7615 | id:1309.7615 author:Firas A. Jassim category:cs.CV  published:2013-09-29 summary:Image fusion is one of the recent trends in image registration which is an essential field of image processing. The basic principle of this paper is to fuse multi-focus images using simple statistical standard deviation. Firstly, the simple standard deviation for the k-by-k window inside each of the multi-focus images was computed. The contribution in this paper came from the idea that the focused part inside an image had high details rather than the unfocused part. Hence, the dispersion between pixels inside the focused part is higher than the dispersion inside the unfocused part. Secondly, a simple comparison between the standard deviation for each k-by-k window in the multi-focus images could be computed. The highest standard deviation between all the computed standard deviations for the multi-focus images could be treated as the optimal that is to be placed in the fused image. The experimental visual results show that the proposed method produces very satisfactory results in spite of its simplicity. version:1
arxiv-1211-4909 | Fast Marginalized Block Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1211.4909 | id:1211.4909 author:Benyuan Liu, Zhilin Zhang, Hongqi Fan, Qiang Fu category:cs.IT cs.LG math.IT stat.ML  published:2012-11-21 summary:The performance of sparse signal recovery from noise corrupted, underdetermined measurements can be improved if both sparsity and correlation structure of signals are exploited. One typical correlation structure is the intra-block correlation in block sparse signals. To exploit this structure, a framework, called block sparse Bayesian learning (BSBL), has been proposed recently. Algorithms derived from this framework showed superior performance but they are not very fast, which limits their applications. This work derives an efficient algorithm from this framework, using a marginalized likelihood maximization method. Compared to existing BSBL algorithms, it has close recovery performance but is much faster. Therefore, it is more suitable for large scale datasets and applications requiring real-time implementation. version:7
arxiv-1309-7611 | Context-aware recommendations from implicit data via scalable tensor factorization | http://arxiv.org/abs/1309.7611 | id:1309.7611 author:Balázs Hidasi, Domonkos Tikk category:cs.LG cs.IR  published:2013-09-29 summary:Albeit the implicit feedback based recommendation problem - when only the user history is available but there are no ratings - is the most typical setting in real-world applications, it is much less researched than the explicit feedback case. State-of-the-art algorithms that are efficient on the explicit case cannot be automatically transformed to the implicit case if scalability should be maintained. There are few implicit feedback benchmark data sets, therefore new ideas are usually experimented on explicit benchmarks. In this paper, we propose a generic context-aware implicit feedback recommender algorithm, coined iTALS. iTALS applies a fast, ALS-based tensor factorization learning method that scales linearly with the number of non-zero elements in the tensor. We also present two approximate and faster variants of iTALS using coordinate descent and conjugate gradient methods at learning. The method also allows us to incorporate various contextual information into the model while maintaining its computational efficiency. We present two context-aware variants of iTALS incorporating seasonality and item purchase sequentiality into the model to distinguish user behavior at different time intervals, and product types with different repetitiveness. Experiments run on six data sets shows that iTALS clearly outperforms context-unaware models and context aware baselines, while it is on par with factorization machines (beats 7 times out of 12 cases) both in terms of recall and MAP. version:1
arxiv-1309-7609 | Identificación y Registro Catastral de Cuerpos de Agua mediante Técnicas de Procesamiento Digital de Imagenes | http://arxiv.org/abs/1309.7609 | id:1309.7609 author:Kevin Rojas Laura, Christhian Cardenas Alvarez category:cs.CV  published:2013-09-29 summary:The effects of global climate change on Peruvian glaciers have brought about several processes of deglaciation during the last few years. The immediate effect is the change of size of lakes and rivers. Public institutions that monitor water resources currently have only recent studies which make up less than 10% of the total. The effects of climate change and the lack of updated information intensify social-economic problems related to water resources in Peru. The objective of this research is to develop a software application to automate the Cadastral Registry of Water Bodies in Peru, using techniques of digital image processing, which would provide tools for detection, record, temporal analysis and visualization of water bodies. The images used are from the satellite Landsat5, which undergo a pre-processing of calibration and correction of the satellite. Detection results are archived into a file that contains location vectors and images of the segmentated bodies of water. version:1
arxiv-1309-7598 | On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations | http://arxiv.org/abs/1309.7598 | id:1309.7598 author:Tamir Hazan, Subhransu Maji, Tommi Jaakkola category:cs.LG  published:2013-09-29 summary:In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical "high signal - high coupling" regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. version:1
arxiv-1309-7524 | Meme and Variations: A Computer Model of Cultural Evolution | http://arxiv.org/abs/1309.7524 | id:1309.7524 author:Liane Gabora category:cs.MA cs.NE  published:2013-09-29 summary:Holland's (1975) genetic algorithm is a minimal computer model of natural selection that made it possible to investigate the effect of manipulating specific parameters on the evolutionary process. If culture is, like biology, a form of evolution, it should be possible to similarly abstract the underlying skeleton of the process and develop a minimal model of it. Meme and Variations, or MAV, is a computational model, inspired by the genetic algorithm, of how ideas evolve in a society of interacting individuals (Gabora 1995). The name is a pun on the classical music form 'theme and variations', because it is based on the premise that novel ideas are variations of old ones; they result from tweaking or combining existing ideas in new ways (Holland et al. 1981). MAV explores the impact of biological phenomena such as over-dominance and epistasis as well as cognitive and social phenomena such as the ability to learn generalizations or imitate others on the fitness and diversity of cultural transmissible actions. version:1
arxiv-1309-7522 | An Application of Backpropagation Artificial Neural Network Method for Measuring The Severity of Osteoarthritis | http://arxiv.org/abs/1309.7522 | id:1309.7522 author:Dian Pratiwi, Diaz D. Santika, Bens Pardamean category:cs.NE cs.CE cs.CV  published:2013-09-29 summary:The examination of Osteoarthritis disease through X-ray by rheumatology can be classified into four grade of severity. This paper discusses about the application of artificial neural network backpropagation method for measuring the severity of the disease, where the observed X-ray range from wrist to fingers. The main procedures of system in this paper is divided into three, which are image processing, feature extraction, and artificial neural network process. First, an X-ray image digital (200x150 pixels and greyscale) will be thresholded, then extracted features based on probabilistic values of the color intensity of seven bit quantization result, and statistical textures. That feature values then will be normalizing to interval [0.1, 0.9], and then the result would be processing on backpropagation artificial neural network system as input to determine the severity of disease from an X-ray had input before it. From testing with learning rate 0.3, momentum 0.4, hidden units five pieces and about 132 feature vectors, this system had had a level of accuracy of 100% for learning data, 80% for learning and non-learning data, and 66.6% for non-learning data version:1
arxiv-1309-7484 | CSIFT Based Locality-constrained Linear Coding for Image Classification | http://arxiv.org/abs/1309.7484 | id:1309.7484 author:Chen Junzhou, Li Qing, Peng Qiang, Kin Hong Wong category:cs.CV  published:2013-09-28 summary:In the past decade, SIFT descriptor has been witnessed as one of the most robust local invariant feature descriptors and widely used in various vision tasks. Most traditional image classification systems depend on the luminance-based SIFT descriptors, which only analyze the gray level variations of the images. Misclassification may happen since their color contents are ignored. In this article, we concentrate on improving the performance of existing image classification algorithms by adding color information. To achieve this purpose, different kinds of colored SIFT descriptors are introduced and implemented. Locality-constrained Linear Coding (LLC), a state-of-the-art sparse coding technology, is employed to construct the image classification system for the evaluation. The real experiments are carried out on several benchmarks. With the enhancements of color SIFT, the proposed image classification system obtains approximate 3% improvement of classification accuracy on the Caltech-101 dataset and approximate 4% improvement of classification accuracy on the Caltech-256 dataset. version:1
arxiv-1309-7439 | Optimal Hybrid Channel Allocation:Based On Machine Learning Algorithms | http://arxiv.org/abs/1309.7439 | id:1309.7439 author:K Viswanadh, Dr. G Rama Murthy category:cs.NI cs.LG  published:2013-09-28 summary:Recent advances in cellular communication systems resulted in a huge increase in spectrum demand. To meet the requirements of the ever-growing need for spectrum, efficient utilization of the existing resources is of utmost importance. Channel Allocation, has thus become an inevitable research topic in wireless communications. In this paper, we propose an optimal channel allocation scheme, Optimal Hybrid Channel Allocation (OHCA) for an effective allocation of channels. We improvise upon the existing Fixed Channel Allocation (FCA) technique by imparting intelligence to the existing system by employing the multilayer perceptron technique. version:1
arxiv-1309-7434 | Face Verification Using Boosted Cross-Image Features | http://arxiv.org/abs/1309.7434 | id:1309.7434 author:Dong Zhang, Omar Oreifej, Mubarak Shah category:cs.CV  published:2013-09-28 summary:This paper proposes a new approach for face verification, where a pair of images needs to be classified as belonging to the same person or not. This problem is relatively new and not well-explored in the literature. Current methods mostly adopt techniques borrowed from face recognition, and process each of the images in the pair independently, which is counter intuitive. In contrast, we propose to extract cross-image features, i.e. features across the pair of images, which, as we demonstrate, is more discriminative to the similarity and the dissimilarity of faces. Our features are derived from the popular Haar-like features, however, extended to handle the face verification problem instead of face detection. We collect a large bank of cross-image features using filters of different sizes, locations, and orientations. Consequently, we use AdaBoost to select and weight the most discriminative features. We carried out extensive experiments on the proposed ideas using three standard face verification datasets, and obtained promising results outperforming state-of-the-art. version:1
arxiv-1309-6301 | Solving OSCAR regularization problems by proximal splitting algorithms | http://arxiv.org/abs/1309.6301 | id:1309.6301 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.LG stat.ML  published:2013-09-24 summary:The OSCAR (octagonal selection and clustering algorithm for regression) regularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for its grouping behavior) and was proposed to encourage group sparsity in scenarios where the groups are a priori unknown. The OSCAR regularizer has a non-trivial proximity operator, which limits its applicability. We reformulate this regularizer as a weighted sorted L_1 norm, and propose its grouping proximity operator (GPO) and approximate proximity operator (APO), thus making state-of-the-art proximal splitting algorithms (PSAs) available to solve inverse problems with OSCAR regularization. The GPO is in fact the APO followed by additional grouping and averaging operations, which are costly in time and storage, explaining the reason why algorithms with APO are much faster than that with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an exact proximity operator. Although convergence of PSAs with APO is may not be guaranteed, we have experimentally found that APO behaves similarly to GPO when the regularization parameter of the pair-wise L_inf norm is set to an appropriately small value. Experiments on recovery of group-sparse signals (with unknown groups) show that PSAs with APO are very fast and accurate. version:2
arxiv-1309-7312 | Development and Transcription of Assamese Speech Corpus | http://arxiv.org/abs/1309.7312 | id:1309.7312 author:Himangshu Sarma, Navanath Saharia, Utpal Sharma, Smriti Kumar Sinha, Mancha Jyoti Malakar category:cs.CL  published:2013-09-27 summary:A balanced speech corpus is the basic need for any speech processing task. In this report we describe our effort on development of Assamese speech corpus. We mainly focused on some issues and challenges faced during development of the corpus. Being a less computationally aware language, this is the first effort to develop speech corpus for Assamese. As corpus development is an ongoing process, in this paper we report only the initial task. version:1
arxiv-1309-7311 | Bayesian Inference in Sparse Gaussian Graphical Models | http://arxiv.org/abs/1309.7311 | id:1309.7311 author:Peter Orchard, Felix Agakov, Amos Storkey category:stat.ML cs.LG  published:2013-09-27 summary:One of the fundamental tasks of science is to find explainable relationships between observed phenomena. One approach to this task that has received attention in recent years is based on probabilistic graphical modelling with sparsity constraints on model structures. In this paper, we describe two new approaches to Bayesian inference of sparse structures of Gaussian graphical models (GGMs). One is based on a simple modification of the cutting-edge block Gibbs sampler for sparse GGMs, which results in significant computational gains in high dimensions. The other method is based on a specific construction of the Hamiltonian Monte Carlo sampler, which results in further significant improvements. We compare our fully Bayesian approaches with the popular regularisation-based graphical LASSO, and demonstrate significant advantages of the Bayesian treatment under the same computing costs. We apply the methods to a broad range of simulated data sets, and a real-life financial data set. version:1
arxiv-1211-3295 | Order-independent constraint-based causal structure learning | http://arxiv.org/abs/1211.3295 | id:1211.3295 author:Diego Colombo, Marloes H. Maathuis category:stat.ML cs.LG  published:2012-11-14 summary:We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993), Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first step of all these algorithms consists of the PC-algorithm. This algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg. version:2
arxiv-1309-7270 | Evaluating the Usefulness of Sentiment Information for Focused Crawlers | http://arxiv.org/abs/1309.7270 | id:1309.7270 author:Tianjun Fu, Ahmed Abbasi, Daniel Zeng, Hsinchun Chen category:cs.IR cs.CL  published:2013-09-27 summary:Despite the prevalence of sentiment-related content on the Web, there has been limited work on focused crawlers capable of effectively collecting such content. In this study, we evaluated the efficacy of using sentiment-related information for enhanced focused crawling of opinion-rich web content regarding a particular topic. We also assessed the impact of using sentiment-labeled web graphs to further improve collection accuracy. Experimental results on a large test bed encompassing over half a million web pages revealed that focused crawlers utilizing sentiment information as well as sentiment-labeled web graphs are capable of gathering more holistic collections of opinion-related content regarding a particular topic. The results have important implications for business and marketing intelligence gathering efforts in the Web 2.0 era. version:1
arxiv-1309-7266 | Evaluating Link-Based Techniques for Detecting Fake Pharmacy Websites | http://arxiv.org/abs/1309.7266 | id:1309.7266 author:Ahmed Abbasi, Siddharth Kaza, F. Mariam Zahedi category:cs.CY cs.LG  published:2013-09-27 summary:Fake online pharmacies have become increasingly pervasive, constituting over 90% of online pharmacy websites. There is a need for fake website detection techniques capable of identifying fake online pharmacy websites with a high degree of accuracy. In this study, we compared several well-known link-based detection techniques on a large-scale test bed with the hyperlink graph encompassing over 80 million links between 15.5 million web pages, including 1.2 million known legitimate and fake pharmacy pages. We found that the QoC and QoL class propagation algorithms achieved an accuracy of over 90% on our dataset. The results revealed that algorithms that incorporate dual class propagation as well as inlink and outlink information, on page-level or site-level graphs, are better suited for detecting fake pharmacy websites. In addition, site-level analysis yielded significantly better results than page-level analysis for most algorithms evaluated. version:1
arxiv-1309-7958 | A Statistical Learning Based System for Fake Website Detection | http://arxiv.org/abs/1309.7958 | id:1309.7958 author:Ahmed Abbasi, Zhu Zhang, Hsinchun Chen category:cs.CY cs.LG  published:2013-09-27 summary:Existing fake website detection systems are unable to effectively detect fake websites. In this study, we advocate the development of fake website detection systems that employ classification methods grounded in statistical learning theory (SLT). Experimental results reveal that a prototype system developed using SLT-based methods outperforms seven existing fake website detection systems on a test bed encompassing 900 real and fake websites. version:1
arxiv-1309-7261 | Detecting Fake Escrow Websites using Rich Fraud Cues and Kernel Based Methods | http://arxiv.org/abs/1309.7261 | id:1309.7261 author:Ahmed Abbasi, Hsinchun Chen category:cs.CY cs.LG  published:2013-09-27 summary:The ability to automatically detect fraudulent escrow websites is important in order to alleviate online auction fraud. Despite research on related topics, fake escrow website categorization has received little attention. In this study we evaluated the effectiveness of various features and techniques for detecting fake escrow websites. Our analysis included a rich set of features extracted from web page text, image, and link information. We also proposed a composite kernel tailored to represent the properties of fake websites, including content duplication and structural attributes. Experiments were conducted to assess the proposed features, techniques, and kernels on a test bed encompassing nearly 90,000 web pages derived from 410 legitimate and fake escrow sites. The combination of an extended feature set and the composite kernel attained over 98% accuracy when differentiating fake sites from real ones, using the support vector machines algorithm. The results suggest that automated web-based information systems for detecting fake escrow sites could be feasible and may be utilized as authentication mechanisms. version:1
arxiv-1303-2823 | Gaussian Processes for Nonlinear Signal Processing | http://arxiv.org/abs/1303.2823 | id:1303.2823 author:Fernando Pérez-Cruz, Steven Van Vaerenbergh, Juan José Murillo-Fuentes, Miguel Lázaro-Gredilla, Ignacio Santamaria category:cs.LG cs.IT math.IT stat.ML  published:2013-03-12 summary:Gaussian processes (GPs) are versatile tools that have been successfully employed to solve nonlinear estimation problems in machine learning, but that are rarely used in signal processing. In this tutorial, we present GPs for regression as a natural nonlinear extension to optimal Wiener filtering. After establishing their basic formulation, we discuss several important aspects and extensions, including recursive and adaptive algorithms for dealing with non-stationarity, low-complexity solutions, non-Gaussian noise models and classification scenarios. Furthermore, we provide a selection of relevant applications to wireless digital communications. version:2
arxiv-1309-7170 | An Efficient Index for Visual Search in Appearance-based SLAM | http://arxiv.org/abs/1309.7170 | id:1309.7170 author:Kiana Hajebi, Hong Zhang category:cs.CV cs.RO  published:2013-09-27 summary:Vector-quantization can be a computationally expensive step in visual bag-of-words (BoW) search when the vocabulary is large. A BoW-based appearance SLAM needs to tackle this problem for an efficient real-time operation. We propose an effective method to speed up the vector-quantization process in BoW-based visual SLAM. We employ a graph-based nearest neighbor search (GNNS) algorithm to this aim, and experimentally show that it can outperform the state-of-the-art. The graph-based search structure used in GNNS can efficiently be integrated into the BoW model and the SLAM framework. The graph-based index, which is a k-NN graph, is built over the vocabulary words and can be extracted from the BoW's vocabulary construction procedure, by adding one iteration to the k-means clustering, which adds small extra cost. Moreover, exploiting the fact that images acquired for appearance-based SLAM are sequential, GNNS search can be initiated judiciously which helps increase the speedup of the quantization process considerably. version:1
arxiv-1306-1350 | Diffusion map for clustering fMRI spatial maps extracted by independent component analysis | http://arxiv.org/abs/1306.1350 | id:1306.1350 author:Tuomo Sipola, Fengyu Cong, Tapani Ristaniemi, Vinoo Alluri, Petri Toiviainen, Elvira Brattico, Asoke K. Nandi category:cs.CE cs.LG stat.ML  published:2013-06-06 summary:Functional magnetic resonance imaging (fMRI) produces data about activity inside the brain, from which spatial maps can be extracted by independent component analysis (ICA). In datasets, there are n spatial maps that contain p voxels. The number of voxels is very high compared to the number of analyzed spatial maps. Clustering of the spatial maps is usually based on correlation matrices. This usually works well, although such a similarity matrix inherently can explain only a certain amount of the total variance contained in the high-dimensional data where n is relatively small but p is large. For high-dimensional space, it is reasonable to perform dimensionality reduction before clustering. In this research, we used the recently developed diffusion map for dimensionality reduction in conjunction with spectral clustering. This research revealed that the diffusion map based clustering worked as well as the more traditional methods, and produced more compact clusters when needed. version:4
arxiv-1309-7122 | Proceedings Wivace 2013 - Italian Workshop on Artificial Life and Evolutionary Computation | http://arxiv.org/abs/1309.7122 | id:1309.7122 author:Alex Graudenzi, Giulio Caravagna, Giancarlo Mauri, Marco Antoniotti category:cs.CE cs.NE  published:2013-09-27 summary:The Wivace 2013 Electronic Proceedings in Theoretical Computer Science (EPTCS) contain some selected long and short articles accepted for the presentation at Wivace 2013 - Italian Workshop on Artificial Life and Evolutionary Computation, which was held at the University of Milan-Bicocca, Milan, on the 1st and 2nd of July, 2013. version:1
arxiv-1309-7119 | Market Index and Stock Price Direction Prediction using Machine Learning Techniques: An empirical study on the KOSPI and HSI | http://arxiv.org/abs/1309.7119 | id:1309.7119 author:Yanshan Wang, In-Chan Choi category:cs.CE cs.LG q-fin.ST  published:2013-09-27 summary:The prediction of a stock market direction may serve as an early recommendation system for short-term investors and as an early financial distress warning system for long-term shareholders. In this paper, we propose an empirical study on the Korean and Hong Kong stock market with an integrated machine learning framework that employs Principal Component Analysis (PCA) and Support Vector Machine (SVM). We try to predict the upward or downward direction of stock market index and stock price. In the proposed framework, PCA, as a feature selection method, identifies principal components in the stock market movement and SVM, as a classifier for future stock market movement, processes them along with other economic factors in training and forecasting. We present the results of an extensive empirical study of the proposed method on the Korean composite stock price index (KOSPI) and Hangseng index (HSI), as well as the individual constituents included in the indices. In our experiment, ten years data (from January 1st, 2002 to January 1st, 2012) are collected and schemed by rolling windows to predict one-day-ahead directions. The experimental results show notably high hit ratios in predicting the movements of the individual constituents in the KOSPI and HSI. The results also varify the \textit{co-movement} effect between the Korean (Hong Kong) stock market and the American stock market. version:1
arxiv-1309-6933 | Estimating Undirected Graphs Under Weak Assumptions | http://arxiv.org/abs/1309.6933 | id:1309.6933 author:Larry Wasserman, Mladen Kolar, Alessandro Rinaldo category:math.ST cs.LG stat.ML stat.TH 62H12  published:2013-09-26 summary:We consider the problem of providing nonparametric confidence guarantees for undirected graphs under weak assumptions. In particular, we do not assume sparsity, incoherence or Normality. We allow the dimension $D$ to increase with the sample size $n$. First, we prove lower bounds that show that if we want accurate inferences with low assumptions then there are limitations on the dimension as a function of sample size. When the dimension increases slowly with sample size, we show that methods based on Normal approximations and on the bootstrap lead to valid inferences and we provide Berry-Esseen bounds on the accuracy of the Normal approximation. When the dimension is large relative to sample size, accurate inferences for graphs under low assumptions are not possible. Instead we propose to estimate something less demanding than the entire partial correlation graph. In particular, we consider: cluster graphs, restricted partial correlation graphs and correlation graphs. version:1
arxiv-1309-7982 | On the Feature Discovery for App Usage Prediction in Smartphones | http://arxiv.org/abs/1309.7982 | id:1309.7982 author:Zhung-Xun Liao, Shou-Chung Li, Wen-Chih Peng, Philip S Yu category:cs.LG  published:2013-09-26 summary:With the increasing number of mobile Apps developed, they are now closely integrated into daily life. In this paper, we develop a framework to predict mobile Apps that are most likely to be used regarding the current device status of a smartphone. Such an Apps usage prediction framework is a crucial prerequisite for fast App launching, intelligent user experience, and power management of smartphones. By analyzing real App usage log data, we discover two kinds of features: The Explicit Feature (EF) from sensing readings of built-in sensors, and the Implicit Feature (IF) from App usage relations. The IF feature is derived by constructing the proposed App Usage Graph (abbreviated as AUG) that models App usage transitions. In light of AUG, we are able to discover usage relations among Apps. Since users may have different usage behaviors on their smartphones, we further propose one personalized feature selection algorithm. We explore minimum description length (MDL) from the training data and select those features which need less length to describe the training data. The personalized feature selection can successfully reduce the log size and the prediction time. Finally, we adopt the kNN classification model to predict Apps usage. Note that through the features selected by the proposed personalized feature selection algorithm, we only need to keep these features, which in turn reduces the prediction time and avoids the curse of dimensionality when using the kNN classifier. We conduct a comprehensive experimental study based on a real mobile App usage dataset. The results demonstrate the effectiveness of the proposed framework and show the predictive capability for App usage prediction. version:1
arxiv-1309-6876 | Bennett-type Generalization Bounds: Large-deviation Case and Faster Rate of Convergence | http://arxiv.org/abs/1309.6876 | id:1309.6876 author:Chao Zhang category:stat.ML cs.LG  published:2013-09-26 summary:In this paper, we present the Bennett-type generalization bounds of the learning process for i.i.d. samples, and then show that the generalization bounds have a faster rate of convergence than the traditional results. In particular, we first develop two types of Bennett-type deviation inequality for the i.i.d. learning process: one provides the generalization bounds based on the uniform entropy number; the other leads to the bounds based on the Rademacher complexity. We then adopt a new method to obtain the alternative expressions of the Bennett-type generalization bounds, which imply that the bounds have a faster rate o(N^{-1/2}) of convergence than the traditional results O(N^{-1/2}). Additionally, we find that the rate of the bounds will become faster in the large-deviation case, which refers to a situation where the empirical risk is far away from (at least not close to) the expected risk. Finally, we analyze the asymptotical convergence of the learning process and compare our analysis with the existing results. version:1
arxiv-1309-6875 | Active Learning with Expert Advice | http://arxiv.org/abs/1309.6875 | id:1309.6875 author:Peilin Zhao, Steven Hoi, Jinfeng Zhuang category:cs.LG stat.ML  published:2013-09-26 summary:Conventional learning with expert advice methods assumes a learner is always receiving the outcome (e.g., class labels) of every incoming training instance at the end of each trial. In real applications, acquiring the outcome from oracle can be costly or time consuming. In this paper, we address a new problem of active learning with expert advice, where the outcome of an instance is disclosed only when it is requested by the online learner. Our goal is to learn an accurate prediction model by asking the oracle the number of questions as small as possible. To address this challenge, we propose a framework of active forecasters for online active learning with expert advice, which attempts to extend two regular forecasters, i.e., Exponentially Weighted Average Forecaster and Greedy Forecaster, to tackle the task of active learning with expert advice. We prove that the proposed algorithms satisfy the Hannan consistency under some proper assumptions, and validate the efficacy of our technique by an extensive set of experiments. version:1
arxiv-1309-6874 | Integrating Document Clustering and Topic Modeling | http://arxiv.org/abs/1309.6874 | id:1309.6874 author:Pengtao Xie, Eric P. Xing category:cs.LG cs.CL cs.IR stat.ML  published:2013-09-26 summary:Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model. version:1
arxiv-1309-6869 | Finite-Time Analysis of Kernelised Contextual Bandits | http://arxiv.org/abs/1309.6869 | id:1309.6869 author:Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, Nelo Cristianini category:cs.LG stat.ML  published:2013-09-26 summary:We tackle the problem of online reward maximisation over a large finite set of actions described by their contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume that we have access to the similarities between actions' contexts and that the expected reward is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCB algorithm, and give a cumulative regret bound through a frequentist analysis. For contextual bandits, the related algorithm GP-UCB turns out to be a special case of our algorithm, and our finite-time analysis improves the regret bound of GP-UCB for the agnostic case, both in the terms of the kernel-dependent quantity and the RKHS norm of the reward function. Moreover, for the linear kernel, our regret bound matches the lower bound for contextual linear bandits. version:1
arxiv-1309-6868 | Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs | http://arxiv.org/abs/1309.6868 | id:1309.6868 author:Charles Tripp, Ross D. Shachter category:cs.LG stat.ML  published:2013-09-26 summary:We seek to learn an effective policy for a Markov Decision Process (MDP) with continuous states via Q-Learning. Given a set of basis functions over state action pairs we search for a corresponding set of linear weights that minimizes the mean Bellman residual. Our algorithm uses a Kalman filter model to estimate those weights and we have developed a simpler approximate Kalman filter model that outperforms the current state of the art projected TD-Learning methods on several standard benchmark problems. version:1
arxiv-1309-6867 | Speedy Model Selection (SMS) for Copula Models | http://arxiv.org/abs/1309.6867 | id:1309.6867 author:Yaniv Tenzer, Gal Elidan category:cs.LG stat.ME  published:2013-09-26 summary:We tackle the challenge of efficiently learning the structure of expressive multivariate real-valued densities of copula graphical models. We start by theoretically substantiating the conjecture that for many copula families the magnitude of Spearman's rank correlation coefficient is monotone in the expected contribution of an edge in network, namely the negative copula entropy. We then build on this theory and suggest a novel Bayesian approach that makes use of a prior over values of Spearman's rho for learning copula-based models that involve a mix of copula families. We demonstrate the generalization effectiveness of our highly efficient approach on sizable and varied real-life datasets. version:1
arxiv-1309-6865 | Modeling Documents with Deep Boltzmann Machines | http://arxiv.org/abs/1309.6865 | id:1309.6865 author:Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E. Hinton category:cs.LG cs.IR stat.ML  published:2013-09-26 summary:We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks. version:1
arxiv-1309-6863 | Sparse Nested Markov models with Log-linear Parameters | http://arxiv.org/abs/1309.6863 | id:1309.6863 author:Ilya Shpitser, Robin J. Evans, Thomas S. Richardson, James M. Robins category:cs.LG cs.AI stat.ML  published:2013-09-26 summary:Hidden variables are ubiquitous in practical data analysis, and therefore modeling marginal densities and doing inference with the resulting models is an important problem in statistics, machine learning, and causal inference. Recently, a new type of graphical model, called the nested Markov model, was developed which captures equality constraints found in marginals of directed acyclic graph (DAG) models. Some of these constraints, such as the so called `Verma constraint', strictly generalize conditional independence. To make modeling and inference with nested Markov models practical, it is necessary to limit the number of parameters in the model, while still correctly capturing the constraints in the marginal of a DAG model. Placing such limits is similar in spirit to sparsity methods for undirected graphical models, and regression models. In this paper, we give a log-linear parameterization which allows sparse modeling with nested Markov models. We illustrate the advantages of this parameterization with a simulation study. version:1
arxiv-1309-6862 | Determinantal Clustering Processes - A Nonparametric Bayesian Approach to Kernel Based Semi-Supervised Clustering | http://arxiv.org/abs/1309.6862 | id:1309.6862 author:Amar Shah, Zoubin Ghahramani category:cs.LG stat.ML  published:2013-09-26 summary:Semi-supervised clustering is the task of clustering data points into clusters where only a fraction of the points are labelled. The true number of clusters in the data is often unknown and most models require this parameter as an input. Dirichlet process mixture models are appealing as they can infer the number of clusters from the data. However, these models do not deal with high dimensional data well and can encounter difficulties in inference. We present a novel nonparameteric Bayesian kernel based method to cluster data points without the need to prespecify the number of clusters or to model complicated densities from which data points are assumed to be generated from. The key insight is to use determinants of submatrices of a kernel matrix as a measure of how close together a set of points are. We explore some theoretical properties of the model and derive a natural Gibbs based algorithm with MCMC hyperparameter learning. The model is implemented on a variety of synthetic and real world data sets. version:1
arxiv-1309-6860 | Identifying Finite Mixtures of Nonparametric Product Distributions and Causal Inference of Confounders | http://arxiv.org/abs/1309.6860 | id:1309.6860 author:Eleni Sgouritsa, Dominik Janzing, Jonas Peters, Bernhard Schoelkopf category:cs.LG cs.AI stat.ML  published:2013-09-26 summary:We propose a kernel method to identify finite mixtures of nonparametric product distributions. It is based on a Hilbert space embedding of the joint distribution. The rank of the constructed tensor is equal to the number of mixture components. We present an algorithm to recover the components by partitioning the data points into clusters such that the variables are jointly conditionally independent given the cluster. This method can be used to identify finite confounders. version:1
arxiv-1309-6858 | The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature Models | http://arxiv.org/abs/1309.6858 | id:1309.6858 author:Novi Quadrianto, Viktoriia Sharmanska, David A. Knowles, Zoubin Ghahramani category:cs.LG stat.ML  published:2013-09-26 summary:We propose a probabilistic model to infer supervised latent variables in the Hamming space from observed data. Our model allows simultaneous inference of the number of binary latent variables, and their values. The latent variables preserve neighbourhood structure of the data in a sense that objects in the same semantic concept have similar latent values, and objects in different concepts have dissimilar latent values. We formulate the supervised infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the inferred supervised latent variables can be directly used to perform a nearest neighbour search for the purpose of retrieval. We introduce a new application of dynamically extending hash codes, and show how to effectively couple the structure of the hash codes with continuously growing structure of the neighbourhood preserving infinite latent feature space. version:1
arxiv-1309-6852 | Stochastic Rank Aggregation | http://arxiv.org/abs/1309.6852 | id:1309.6852 author:Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng category:cs.LG cs.IR stat.ML  published:2013-09-26 summary:This paper addresses the problem of rank aggregation, which aims to find a consensus ranking among multiple ranking inputs. Traditional rank aggregation methods are deterministic, and can be categorized into explicit and implicit methods depending on whether rank information is explicitly or implicitly utilized. Surprisingly, experimental results on real data sets show that explicit rank aggregation methods would not work as well as implicit methods, although rank information is critical for the task. Our analysis indicates that the major reason might be the unreliable rank information from incomplete ranking inputs. To solve this problem, we propose to incorporate uncertainty into rank aggregation and tackle the problem in both unsupervised and supervised scenario. We call this novel framework {stochastic rank aggregation} (St.Agg for short). Specifically, we introduce a prior distribution on ranks, and transform the ranking functions or objectives in traditional explicit methods to their expectations over this distribution. Our experiments on benchmark data sets show that the proposed St.Agg outperforms the baselines in both unsupervised and supervised scenarios. version:1
arxiv-1309-6851 | Treedy: A Heuristic for Counting and Sampling Subsets | http://arxiv.org/abs/1309.6851 | id:1309.6851 author:Teppo Niinimaki, Mikko Koivisto category:cs.DS cs.AI cs.LG  published:2013-09-26 summary:Consider a collection of weighted subsets of a ground set N. Given a query subset Q of N, how fast can one (1) find the weighted sum over all subsets of Q, and (2) sample a subset of Q proportionally to the weights? We present a tree-based greedy heuristic, Treedy, that for a given positive tolerance d answers such counting and sampling queries to within a guaranteed relative error d and total variation distance d, respectively. Experimental results on artificial instances and in application to Bayesian structure discovery in Bayesian networks show that approximations yield dramatic savings in running time compared to exact computation, and that Treedy typically outperforms a previously proposed sorting-based heuristic. version:1
arxiv-1309-6850 | Structured Convex Optimization under Submodular Constraints | http://arxiv.org/abs/1309.6850 | id:1309.6850 author:Kiyohito Nagano, Yoshinobu Kawahara category:cs.LG cs.DS stat.ML  published:2013-09-26 summary:A number of discrete and continuous optimization problems in machine learning are related to convex minimization problems under submodular constraints. In this paper, we deal with a submodular function with a directed graph structure, and we show that a wide range of convex optimization problems under submodular constraints can be solved much more efficiently than general submodular optimization methods by a reduction to a maximum flow problem. Furthermore, we give some applications, including sparse optimization methods, in which the proposed methods are effective. Additionally, we evaluate the performance of the proposed method through computational experiments. version:1
arxiv-1309-6849 | Cyclic Causal Discovery from Continuous Equilibrium Data | http://arxiv.org/abs/1309.6849 | id:1309.6849 author:Joris Mooij, Tom Heskes category:cs.LG cs.AI stat.ML  published:2013-09-26 summary:We propose a method for learning cyclic causal models from a combination of observational and interventional equilibrium data. Novel aspects of the proposed method are its ability to work with continuous data (without assuming linearity) and to deal with feedback loops. Within the context of biochemical reactions, we also propose a novel way of modeling interventions that modify the activity of compounds instead of their abundance. For computational reasons, we approximate the nonlinear causal mechanisms by (coupled) local linearizations, one for each experimental condition. We apply the method to reconstruct a cellular signaling network from the flow cytometry data measured by Sachs et al. (2005). We show that our method finds evidence in the data for feedback loops and that it gives a more accurate quantitative description of the data at comparable model complexity. version:1
arxiv-1309-6847 | Learning Max-Margin Tree Predictors | http://arxiv.org/abs/1309.6847 | id:1309.6847 author:Ofer Meshi, Elad Eban, Gal Elidan, Amir Globerson category:cs.LG stat.ML  published:2013-09-26 summary:Structured prediction is a powerful framework for coping with joint prediction of interacting outputs. A central difficulty in using this framework is that often the correct label dependence structure is unknown. At the same time, we would like to avoid an overly complex structure that will lead to intractable prediction. In this work we address the challenge of learning tree structured predictive models that achieve high accuracy while at the same time facilitate efficient (linear time) inference. We start by proving that this task is in general NP-hard, and then suggest an approximate alternative. Briefly, our CRANK approach relies on a novel Circuit-RANK regularizer that penalizes non-tree structures and that can be optimized using a CCCP procedure. We demonstrate the effectiveness of our approach on several domains and show that, despite the relative simplicity of the structure, prediction accuracy is competitive with a fully connected model that is computationally costly at prediction time. version:1
arxiv-1309-6840 | Constrained Bayesian Inference for Low Rank Multitask Learning | http://arxiv.org/abs/1309.6840 | id:1309.6840 author:Oluwasanmi Koyejo, Joydeep Ghosh category:cs.LG stat.ML  published:2013-09-26 summary:We present a novel approach for constrained Bayesian inference. Unlike current methods, our approach does not require convexity of the constraint set. We reduce the constrained variational inference to a parametric optimization over the feasible set of densities and propose a general recipe for such problems. We apply the proposed constrained Bayesian inference approach to multitask learning subject to rank constraints on the weight matrix. Further, constrained parameter estimation is applied to recover the sparse conditional independence structure encoded by prior precision matrices. Our approach is motivated by reverse inference for high dimensional functional neuroimaging, a domain where the high dimensionality and small number of examples requires the use of constraints to ensure meaningful and effective models. For this application, we propose a model that jointly learns a weight matrix and the prior inverse covariance structure between different tasks. We present experimental validation showing that the proposed approach outperforms strong baseline models in terms of predictive performance and structure recovery. version:1
arxiv-1309-6838 | Inverse Covariance Estimation for High-Dimensional Data in Linear Time and Space: Spectral Methods for Riccati and Sparse Models | http://arxiv.org/abs/1309.6838 | id:1309.6838 author:Jean Honorio, Tommi S. Jaakkola category:cs.LG stat.ML  published:2013-09-26 summary:We propose maximum likelihood estimation for learning Gaussian graphical models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show that our optimization problem leads to a Riccati matrix equation, which has a closed form solution. We propose an efficient algorithm that performs a singular value decomposition of the training data. Our algorithm is O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is tailored to high-dimensional problems (N gg T), in which sparseness promoting methods become intractable. Furthermore, instead of obtaining a single solution for a specific regularization parameter, our algorithm finds the whole solution path. We show that the method has logarithmic sample complexity under the spiked covariance model. We also propose sparsification of the dense solution with provable performance guarantees. We provide techniques for using our learnt models, such as removing unimportant variables, computing likelihoods and conditional distributions. Finally, we show promising results in several gene expressions datasets. version:1
arxiv-1309-6835 | Gaussian Processes for Big Data | http://arxiv.org/abs/1309.6835 | id:1309.6835 author:James Hensman, Nicolo Fusi, Neil D. Lawrence category:cs.LG stat.ML  published:2013-09-26 summary:We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets. version:1
arxiv-1309-6834 | Unsupervised Learning of Noisy-Or Bayesian Networks | http://arxiv.org/abs/1309.6834 | id:1309.6834 author:Yonatan Halpern, David Sontag category:cs.LG stat.ML  published:2013-09-26 summary:This paper considers the problem of learning the parameters in Bayesian networks of discrete variables with known structure and hidden variables. Previous approaches in these settings typically use expectation maximization; when the network has high treewidth, the required expectations might be approximated using Monte Carlo or variational methods. We show how to avoid inference altogether during learning by giving a polynomial-time algorithm based on the method-of-moments, building upon recent work on learning discrete-valued mixture models. In particular, we show how to learn the parameters for a family of bipartite noisy-or Bayesian networks. In our experimental results, we demonstrate an application of our algorithm to learning QMR-DT, a large Bayesian network used for medical diagnosis. We show that it is possible to fully learn the parameters of QMR-DT even when only the findings are observed in the training data (ground truth diseases unknown). version:1
arxiv-1309-6833 | Multiple Instance Learning by Discriminative Training of Markov Networks | http://arxiv.org/abs/1309.6833 | id:1309.6833 author:Hossein Hajimirsadeghi, Jinling Li, Greg Mori, Mohammad Zaki, Tarek Sayed category:cs.LG stat.ML  published:2013-09-26 summary:We introduce a graphical framework for multiple instance learning (MIL) based on Markov networks. This framework can be used to model the traditional MIL definition as well as more general MIL definitions. Different levels of ambiguity -- the portion of positive instances in a bag -- can be explored in weakly supervised data. To train these models, we propose a discriminative max-margin learning algorithm leveraging efficient inference for cardinality-based cliques. The efficacy of the proposed framework is evaluated on a variety of data sets. Experimental results verify that encoding or learning the degree of ambiguity can improve classification performance. version:1
arxiv-1309-6831 | Batch-iFDD for Representation Expansion in Large MDPs | http://arxiv.org/abs/1309.6831 | id:1309.6831 author:Alborz Geramifard, Thomas J. Walsh, Nicholas Roy, Jonathan How category:cs.LG stat.ML  published:2013-09-26 summary:Matching pursuit (MP) methods are a promising class of feature construction algorithms for value function approximation. Yet existing MP methods require creating a pool of potential features, mandating expert knowledge or enumeration of a large feature pool, both of which hinder scalability. This paper introduces batch incremental feature dependency discovery (Batch-iFDD) as an MP method that inherits a provable convergence property. Additionally, Batch-iFDD does not require a large pool of features, leading to lower computational complexity. Empirical policy evaluation results across three domains with up to one million states highlight the scalability of Batch-iFDD over the previous state of the art MP algorithm. version:1
arxiv-1309-6830 | Building Bridges: Viewing Active Learning from the Multi-Armed Bandit Lens | http://arxiv.org/abs/1309.6830 | id:1309.6830 author:Ravi Ganti, Alexander G. Gray category:cs.LG stat.ML  published:2013-09-26 summary:In this paper we propose a multi-armed bandit inspired, pool based active learning algorithm for the problem of binary classification. By carefully constructing an analogy between active learning and multi-armed bandits, we utilize ideas such as lower confidence bounds, and self-concordant regularization from the multi-armed bandit literature to design our proposed algorithm. Our algorithm is a sequential algorithm, which in each round assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for the label of this sampled point. The design of this sampling distribution is also inspired by the analogy between active learning and multi-armed bandits. We show how to derive lower confidence bounds required by our algorithm. Experimental comparisons to previously proposed active learning algorithms show superior performance on some standard UCI datasets. version:1
arxiv-1309-6829 | Bethe-ADMM for Tree Decomposition based Parallel MAP Inference | http://arxiv.org/abs/1309.6829 | id:1309.6829 author:Qiang Fu, Huahua Wang, Arindam Banerjee category:cs.AI cs.LG stat.ML  published:2013-09-26 summary:We consider the problem of maximum a posteriori (MAP) inference in discrete graphical models. We present a parallel MAP inference algorithm called Bethe-ADMM based on two ideas: tree-decomposition of the graph and the alternating direction method of multipliers (ADMM). However, unlike the standard ADMM, we use an inexact ADMM augmented with a Bethe-divergence based proximal function, which makes each subproblem in ADMM easy to solve in parallel using the sum-product algorithm. We rigorously prove global convergence of Bethe-ADMM. The proposed algorithm is extensively evaluated on both synthetic and real datasets to illustrate its effectiveness. Further, the parallel Bethe-ADMM is shown to scale almost linearly with increasing number of cores. version:1
arxiv-1309-6823 | Convex Relaxations of Bregman Divergence Clustering | http://arxiv.org/abs/1309.6823 | id:1309.6823 author:Hao Cheng, Xinhua Zhang, Dale Schuurmans category:cs.LG stat.ML  published:2013-09-26 summary:Although many convex relaxations of clustering have been proposed in the past decade, current formulations remain restricted to spherical Gaussian or discriminative models and are susceptible to imbalanced clusters. To address these shortcomings, we propose a new class of convex relaxations that can be flexibly applied to more general forms of Bregman divergence clustering. By basing these new formulations on normalized equivalence relations we retain additional control on relaxation quality, which allows improvement in clustering quality. We furthermore develop optimization methods that improve scalability by exploiting recent implicit matrix norm methods. In practice, we find that the new formulations are able to efficiently produce tighter clusterings that improve the accuracy of state of the art methods. version:1
arxiv-1309-6821 | Sample Complexity of Multi-task Reinforcement Learning | http://arxiv.org/abs/1309.6821 | id:1309.6821 author:Emma Brunskill, Lihong Li category:cs.LG stat.ML  published:2013-09-26 summary:Transferring knowledge across a sequence of reinforcement-learning tasks is challenging, and has a number of important applications. Though there is encouraging empirical evidence that transfer can improve performance in subsequent reinforcement-learning tasks, there has been very little theoretical analysis. In this paper, we introduce a new multi-task algorithm for a sequence of reinforcement-learning tasks when each task is sampled independently from (an unknown) distribution over a finite set of Markov decision processes whose parameters are initially unknown. For this setting, we prove under certain assumptions that the per-task sample complexity of exploration is reduced significantly due to transfer compared to standard single-task algorithms. Our multi-task algorithm also has the desired characteristic that it is guaranteed not to exhibit negative transfer: in the worst case its per-task sample complexity is comparable to the corresponding single-task algorithm. version:1
arxiv-1309-6820 | SparsityBoost: A New Scoring Function for Learning Bayesian Network Structure | http://arxiv.org/abs/1309.6820 | id:1309.6820 author:Eliot Brenner, David Sontag category:cs.LG cs.AI stat.ML  published:2013-09-26 summary:We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to scorebased structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, we give empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning. version:1
arxiv-1309-6819 | Hilbert Space Embeddings of Predictive State Representations | http://arxiv.org/abs/1309.6819 | id:1309.6819 author:Byron Boots, Geoffrey Gordon, Arthur Gretton category:cs.LG stat.ML  published:2013-09-26 summary:Predictive State Representations (PSRs) are an expressive class of models for controlled stochastic processes. PSRs represent state as a set of predictions of future observable events. Because PSRs are defined entirely in terms of observable data, statistically consistent estimates of PSR parameters can be learned efficiently by manipulating moments of observed training data. Most learning algorithms for PSRs have assumed that actions and observations are finite with low cardinality. In this paper, we generalize PSRs to infinite sets of observations and actions, using the recent concept of Hilbert space embeddings of distributions. The essence is to represent the state as a nonparametric conditional embedding operator in a Reproducing Kernel Hilbert Space (RKHS) and leverage recent work in kernel methods to estimate, predict, and update the representation. We show that these Hilbert space embeddings of PSRs are able to gracefully handle continuous actions and observations, and that our learned models outperform competing system identification algorithms on several prediction benchmarks. version:1
arxiv-1309-6818 | Boosting in the presence of label noise | http://arxiv.org/abs/1309.6818 | id:1309.6818 author:Jakramate Bootkrajang, Ata Kaban category:cs.LG stat.ML  published:2013-09-26 summary:Boosting is known to be sensitive to label noise. We studied two approaches to improve AdaBoost's robustness against labelling errors. One is to employ a label-noise robust classifier as a base learner, while the other is to modify the AdaBoost algorithm to be more robust. Empirical evaluation shows that a committee of robust classifiers, although converges faster than non label-noise aware AdaBoost, is still susceptible to label noise. However, pairing it with the new robust Boosting algorithm we propose here results in a more resilient algorithm under mislabelling. version:1
arxiv-1309-6814 | High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning | http://arxiv.org/abs/1309.6814 | id:1309.6814 author:Krishnakumar Balasubramanian, Kai Yu, Tong Zhang category:cs.LG stat.ML  published:2013-09-26 summary:Joint sparsity regularization in multi-task learning has attracted much attention in recent years. The traditional convex formulation employs the group Lasso relaxation to achieve joint sparsity across tasks. Although this approach leads to a simple convex formulation, it suffers from several issues due to the looseness of the relaxation. To remedy this problem, we view jointly sparse multi-task learning as a specialized random effects model, and derive a convex relaxation approach that involves two steps. The first step learns the covariance matrix of the coefficients using a convex formulation which we refer to as sparse covariance coding; the second step solves a ridge regression problem with a sparse quadratic regularizer based on the covariance matrix obtained in the first step. It is shown that this approach produces an asymptotically optimal quadratic regularizer in the multitask learning setting when the number of tasks approaches infinity. Experimental results demonstrate that the convex formulation obtained via the proposed model significantly outperforms group Lasso (and related multi-stage formulations version:1
arxiv-1309-6813 | Hinge-loss Markov Random Fields: Convex Inference for Structured Prediction | http://arxiv.org/abs/1309.6813 | id:1309.6813 author:Stephen Bach, Bert Huang, Ben London, Lise Getoor category:cs.LG stat.ML  published:2013-09-26 summary:Graphical models for structured domains are powerful tools, but the computational complexities of combinatorial prediction spaces can force restrictions on models, or require approximate inference in order to be tractable. Instead of working in a combinatorial space, we use hinge-loss Markov random fields (HL-MRFs), an expressive class of graphical models with log-concave density functions over continuous variables, which can represent confidences in discrete predictions. This paper demonstrates that HL-MRFs are general tools for fast and accurate structured prediction. We introduce the first inference algorithm that is both scalable and applicable to the full class of HL-MRFs, and show how to train HL-MRFs with several learning algorithms. Our experiments show that HL-MRFs match or surpass the predictive performance of state-of-the-art methods, including discrete models, in four application domains. version:1
arxiv-1309-6812 | The Bregman Variational Dual-Tree Framework | http://arxiv.org/abs/1309.6812 | id:1309.6812 author:Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht category:cs.LG stat.ML  published:2013-09-26 summary:Graph-based methods provide a powerful tool set for many non-parametric frameworks in Machine Learning. In general, the memory and computational complexity of these methods is quadratic in the number of examples in the data which makes them quickly infeasible for moderate to large scale datasets. A significant effort to find more efficient solutions to the problem has been made in the literature. One of the state-of-the-art methods that has been recently introduced is the Variational Dual-Tree (VDT) framework. Despite some of its unique features, VDT is currently restricted only to Euclidean spaces where the Euclidean distance quantifies the similarity. In this paper, we extend the VDT framework beyond the Euclidean distance to more general Bregman divergences that include the Euclidean distance as a special case. By exploiting the properties of the general Bregman divergence, we show how the new framework can maintain all the pivotal features of the VDT framework and yet significantly improve its performance in non-Euclidean domains. We apply the proposed framework to different text categorization problems and demonstrate its benefits over the original VDT. version:1
arxiv-1309-6811 | Generative Multiple-Instance Learning Models For Quantitative Electromyography | http://arxiv.org/abs/1309.6811 | id:1309.6811 author:Tameem Adel, Benn Smith, Ruth Urner, Daniel Stashuk, Daniel J. Lizotte category:cs.LG stat.ML  published:2013-09-26 summary:We present a comprehensive study of the use of generative modeling approaches for Multiple-Instance Learning (MIL) problems. In MIL a learner receives training instances grouped together into bags with labels for the bags only (which might not be correct for the comprised instances). Our work was motivated by the task of facilitating the diagnosis of neuromuscular disorders using sets of motor unit potential trains (MUPTs) detected within a muscle which can be cast as a MIL problem. Our approach leads to a state-of-the-art solution to the problem of muscle classification. By introducing and analyzing generative models for MIL in a general framework and examining a variety of model structures and components, our work also serves as a methodological guide to modelling MIL tasks. We evaluate our proposed methods both on MUPT datasets and on the MUSK1 dataset, one of the most widely used benchmarks for MIL. version:1
arxiv-1309-7276 | Adopting level set theory based algorithms to segment human ear | http://arxiv.org/abs/1309.7276 | id:1309.7276 author:Bijeesh T. V, Nimmi I. P category:cs.CV  published:2013-09-26 summary:Human identification has always been a topic that interested researchers around the world. Biometric methods are found to be more effective and much easier for the users than the traditional identification methods like keys, smart cards and passwords. Unlike with the traditional methods, with biometric methods the data acquisition is most of the times passive, which means the users do not take active part in data acquisition. Data acquisition can be performed using cameras, scanners or sensors. Human physiological biometrics such as face, eye and ear are good candidates for uniquely identifying an individual. However, human ear scores over face and eye because of certain advantages it has over face. The most challenging phase in human identification based on ear biometric is the segmentation of the ear image from the captured image which may contain many unwanted details. In this work, PDE based image processing techniques are used to segment out the ear image. Level Set Theory based image processing is employed to obtain the contour of the ear image. A few Level set algorithms are compared for their efficiency in segmenting test ear images. version:1
arxiv-1309-6722 | Domain-Specific Sentiment Word Extraction by Seed Expansion and Pattern Generation | http://arxiv.org/abs/1309.6722 | id:1309.6722 author:Tang Duyu, Qin Bing, Zhou LanJun, Wong KamFai, Zhao Yanyan, Liu Ting category:cs.CL  published:2013-09-26 summary:This paper focuses on the automatic extraction of domain-specific sentiment word (DSSW), which is a fundamental subtask of sentiment analysis. Most previous work utilizes manual patterns for this task. However, the performance of those methods highly relies on the labelled patterns or selected seeds. In order to overcome the above problem, this paper presents an automatic framework to detect large-scale domain-specific patterns for DSSW extraction. To this end, sentiment seeds are extracted from massive dataset of user comments. Subsequently, these sentiment seeds are expanded by synonyms using a bootstrapping mechanism. Simultaneously, a synonymy graph is built and the graph propagation algorithm is applied on the built synonymy graph. Afterwards, syntactic and sequential relations between target words and high-ranked sentiment words are extracted automatically to construct large-scale patterns, which are further used to extracte DSSWs. The experimental results in three domains reveal the effectiveness of our method. version:1
arxiv-1309-6691 | Characterness: An Indicator of Text in the Wild | http://arxiv.org/abs/1309.6691 | id:1309.6691 author:Yao Li, Wenjing Jia, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2013-09-25 summary:Text in an image provides vital information for interpreting its contents, and text in a scene can aide with a variety of tasks from navigation, to obstacle avoidance, and odometry. Despite its value, however, identifying general text in images remains a challenging research problem. Motivated by the need to consider the widely varying forms of natural text, we propose a bottom-up approach to the problem which reflects the `characterness' of an image region. In this sense our approach mirrors the move from saliency detection methods to measures of `objectness'. In order to measure the characterness we develop three novel cues that are tailored for character detection, and a Bayesian method for their integration. Because text is made up of sets of characters, we then design a Markov random field (MRF) model so as to exploit the inherent dependencies between characters. We experimentally demonstrate the effectiveness of our characterness cues as well as the advantage of Bayesian multi-cue integration. The proposed text detector outperforms state-of-the-art methods on a few benchmark scene text detection datasets. We also show that our measurement of `characterness' is superior than state-of-the-art saliency detection models when applied to the same task. version:1
arxiv-1309-3006 | The Classification Accuracy of Multiple-Metric Learning Algorithm on Multi-Sensor Fusion | http://arxiv.org/abs/1309.3006 | id:1309.3006 author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV  published:2013-09-11 summary:This paper focuses on two main issues; first one is the impact of Similarity Search to learning the training sample in metric space, and searching based on supervised learning classi-fication. In particular, four metrics space searching are based on spatial information that are introduced as the following; Cheby-shev Distance (CD); Bray Curtis Distance (BCD); Manhattan Distance (MD) and Euclidean Distance(ED) classifiers. The second issue investigates the performance of combination of mul-ti-sensor images on the supervised learning classification accura-cy. QuickBird multispectral data (MS) and panchromatic data (PAN) have been used in this study to demonstrate the enhance-ment and accuracy assessment of fused image over the original images. The supervised classification results of fusion image generated better than the MS did. QuickBird and the best results with ED classifier than the other did. version:2
arxiv-1309-6650 | An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching | http://arxiv.org/abs/1309.6650 | id:1309.6650 author:Haytham Al-Feel, Ralph Schafermeier, Adrian Paschke category:cs.CL cs.DL I.2.4; I.2.7; I.2  published:2013-09-25 summary:Ontologies are considered as the backbone of the Semantic Web. With the rising success of the Semantic Web, the number of participating communities from different countries is constantly increasing. The growing number of ontologies available in different natural languages leads to an interoperability problem. In this paper, we discuss several approaches for ontology matching; examine similarities and differences, identify weaknesses, and compare the existing automated approaches with the manual approaches for integrating multilingual ontologies. In addition to that, we propose a new architecture for a multilingual ontology matching service. As a case study we used an example of two multilingual enterprise ontologies - the university ontology of Freie Universitaet Berlin and the ontology for Fayoum University in Egypt. version:1
arxiv-1303-0448 | Learning Stable Multilevel Dictionaries for Sparse Representations | http://arxiv.org/abs/1303.0448 | id:1303.0448 author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:cs.CV stat.ML  published:2013-03-03 summary:Sparse representations using learned dictionaries are being increasingly used with success in several data processing and machine learning applications. The availability of abundant training data necessitates the development of efficient, robust and provably good dictionary learning algorithms. Algorithmic stability and generalization are desirable characteristics for dictionary learning algorithms that aim to build global dictionaries which can efficiently model any test data similar to the training samples. In this paper, we propose an algorithm to learn dictionaries for sparse representations from large scale data, and prove that the proposed learning algorithm is stable and generalizable asymptotically. The algorithm employs a 1-D subspace clustering procedure, the K-hyperline clustering, in order to learn a hierarchical dictionary with multiple levels. We also propose an information-theoretic scheme to estimate the number of atoms needed in each level of learning and develop an ensemble approach to learn robust dictionaries. Using the proposed dictionaries, the sparse code for novel test data can be computed using a low-complexity pursuit procedure. We demonstrate the stability and generalization characteristics of the proposed algorithm using simulations. We also evaluate the utility of the multilevel dictionaries in compressed recovery and subspace learning applications. version:2
arxiv-1309-6584 | Should I Stay or Should I Go: Coordinating Biological Needs with Continuously-updated Assessments of the Environment | http://arxiv.org/abs/1309.6584 | id:1309.6584 author:Liane Gabora category:cs.NE cs.LG q-bio.NC  published:2013-09-25 summary:This paper presents Wanderer, a model of how autonomous adaptive systems coordinate internal biological needs with moment-by-moment assessments of the probabilities of events in the external world. The extent to which Wanderer moves about or explores its environment reflects the relative activations of two competing motivational sub-systems: one represents the need to acquire energy and it excites exploration, and the other represents the need to avoid predators and it inhibits exploration. The environment contains food, predators, and neutral stimuli. Wanderer responds to these events in a way that is adaptive in the short turn, and reassesses the probabilities of these events so that it can modify its long term behaviour appropriately. When food appears, Wanderer be-comes satiated and exploration temporarily decreases. When a predator appears, Wanderer both decreases exploration in the short term, and becomes more "cautious" about exploring in the future. Wanderer also forms associations between neutral features and salient ones (food and predators) when they are present at the same time, and uses these associations to guide its behaviour. version:1
arxiv-1308-4757 | Online Douglas-Rachford splitting method | http://arxiv.org/abs/1308.4757 | id:1308.4757 author:Ziqiang Shi category:cs.NA cs.LG stat.ML  published:2013-08-22 summary:Online learning has emerged as powerful tool in large scale optimization. In this work, we generalize the Douglas-Rachford splitting method for minimizing composite functions to online settings. version:5
arxiv-1309-6391 | Multiple-object tracking in cluttered and crowded public spaces | http://arxiv.org/abs/1309.6391 | id:1309.6391 author:Rhys Martin, Ognjen Arandjelović category:cs.CV  published:2013-09-25 summary:This paper addresses the problem of tracking moving objects of variable appearance in challenging scenes rich with features and texture. Reliable tracking is of pivotal importance in surveillance applications. It is made particularly difficult by the nature of objects encountered in such scenes: these too change in appearance and scale, and are often articulated (e.g. humans). We propose a method which uses fast motion detection and segmentation as a constraint for both building appearance models and their robust propagation (matching) in time. The appearance model is based on sets of local appearances automatically clustered using spatio-kinetic similarity, and is updated with each new appearance seen. This integration of all seen appearances of a tracked object makes it extremely resilient to errors caused by occlusion and the lack of permanence of due to low data quality, appearance change or background clutter. These theoretical strengths of our algorithm are empirically demonstrated on two hour long video footage of a busy city marketplace. version:1
arxiv-1309-6390 | Contextually learnt detection of unusual motion-based behaviour in crowded public spaces | http://arxiv.org/abs/1309.6390 | id:1309.6390 author:Ognjen Arandjelović category:cs.CV  published:2013-09-25 summary:In this paper we are interested in analyzing behaviour in crowded public places at the level of holistic motion. Our aim is to learn, without user input, strong scene priors or labelled data, the scope of "normal behaviour" for a particular scene and thus alert to novelty in unseen footage. The first contribution is a low-level motion model based on what we term tracklet primitives, which are scene-specific elementary motions. We propose a clustering-based algorithm for tracklet estimation from local approximations to tracks of appearance features. This is followed by two methods for motion novelty inference from tracklet primitives: (a) we describe an approach based on a non-hierarchial ensemble of Markov chains as a means of capturing behavioural characteristics at different scales, and (b) a more flexible alternative which exhibits a higher generalizing power by accounting for constraints introduced by intentionality and goal-oriented planning of human motion in a particular scene. Evaluated on a 2h long video of a busy city marketplace, both algorithms are shown to be successful at inferring unusual behaviour, the latter model achieving better performance for novelties at a larger spatial scale. version:1
arxiv-1309-6379 | Diffeomorphic Metric Mapping and Probabilistic Atlas Generation of Hybrid Diffusion Imaging based on BFOR Signal Basis | http://arxiv.org/abs/1309.6379 | id:1309.6379 author:Jia Du, A. Pasha Hosseinbor, Moo K. Chung, Barbara B. Bendlin, Gaurav Suryawanshi, Andrew L. Alexander, Anqi Qiu category:cs.CV  published:2013-09-25 summary:We propose a large deformation diffeomorphic metric mapping algorithm to align multiple b-value diffusion weighted imaging (mDWI) data, specifically acquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then propose a Bayesian model for estimating the white matter atlas from HYDIs. We adopt the work given in Hosseinbor et al. (2012) and represent the q-space diffusion signal with the Bessel Fourier orientation reconstruction (BFOR) signal basis. The BFOR framework provides the representation of mDWI in the q-space and thus reduces memory requirement. In addition, since the BFOR signal basis is orthonormal, the L2 norm that quantifies the differences in the q-space signals of any two mDWI datasets can be easily computed as the sum of the squared differences in the BFOR expansion coefficients. In this work, we show that the reorientation of the $q$-space signal due to spatial transformation can be easily defined on the BFOR signal basis. We incorporate the BFOR signal basis into the LDDMM framework and derive the gradient descent algorithm for LDDMM-HYDI with explicit orientation optimization. Additionally, we extend the previous Bayesian atlas estimation framework for scalar-valued images to HYDIs and derive the expectation-maximization algorithm for solving the HYDI atlas estimation problem. Using real HYDI datasets, we show the Bayesian model generates the white matter atlas with anatomical details. Moreover, we show that it is important to consider the variation of mDWI reorientation due to a small change in diffeomorphic transformation in the LDDMM-HYDI optimization and to incorporate the full information of HYDI for aligning mDWI. version:1
arxiv-1309-6352 | Using Nuances of Emotion to Identify Personality | http://arxiv.org/abs/1309.6352 | id:1309.6352 author:Saif M. Mohammad, Svetlana Kiritchenko category:cs.CL  published:2013-09-24 summary:Past work on personality detection has shown that frequency of lexical categories such as first person pronouns, past tense verbs, and sentiment words have significant correlations with personality traits. In this paper, for the first time, we show that fine affect (emotion) categories such as that of excitement, guilt, yearning, and admiration are significant indicators of personality. Additionally, we perform experiments to show that the gains provided by the fine affect categories are not obtained by using coarse affect categories alone or with specificity features alone. We employ these features in five SVM classifiers for detecting five personality traits through essays. We find that the use of fine emotion features leads to statistically significant improvement over a competitive baseline, whereas the use of coarse affect and specificity features does not. version:1
arxiv-1309-6347 | Tracking Sentiment in Mail: How Genders Differ on Emotional Axes | http://arxiv.org/abs/1309.6347 | id:1309.6347 author:Saif M. Mohammad, Tony, Yang category:cs.CL  published:2013-09-24 summary:With the widespread use of email, we now have access to unprecedented amounts of text that we ourselves have written. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in many types of mail. We create a large word--emotion association lexicon by crowdsourcing, and use it to compare emotions in love letters, hate mail, and suicide notes. We show that there are marked differences across genders in how they use emotion words in work-place email. For example, women use many words from the joy--sadness axis, whereas men prefer terms from the fear--trust axis. Finally, we show visualizations that can help people track emotions in their emails. version:1
arxiv-1309-6202 | Sentiment Analysis in the News | http://arxiv.org/abs/1309.6202 | id:1309.6202 author:Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella, Erik van der Goot, Matina Halkia, Bruno Pouliquen, Jenya Belyaeva category:cs.CL  published:2013-09-24 summary:Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles - author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which (a) we test the relative suitability of various sentiment dictionaries and (b) we attempt to separate positive or negative opinion from good or bad news. In the experiments described here, we tested whether or not subject domain-defining vocabulary should be ignored. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance. version:1
arxiv-1309-6185 | Acronym recognition and processing in 22 languages | http://arxiv.org/abs/1309.6185 | id:1309.6185 author:Maud Ehrmann, Leonida della Rocca, Ralf Steinberger, Hristo Tanev category:cs.CL  published:2013-09-24 summary:We are presenting work on recognising acronyms of the form Long-Form (Short-Form) such as "International Monetary Fund (IMF)" in millions of news articles in twenty-two languages, as part of our more general effort to recognise entities and their variants in news text and to use them for the automatic analysis of the news, including the linking of related news across languages. We show how the acronym recognition patterns, initially developed for medical terms, needed to be adapted to the more general news domain and we present evaluation results. We describe our effort to automatically merge the numerous long-form variants referring to the same short-form, while keeping non-related long-forms separate. Finally, we provide extensive statistics on the frequency and the distribution of short-form/long-form pairs across languages. version:1
arxiv-1309-6162 | JRC-Names: A freely available, highly multilingual named entity resource | http://arxiv.org/abs/1309.6162 | id:1309.6162 author:Ralf Steinberger, Bruno Pouliquen, Mijail Kabadjov, Erik van der Goot category:cs.CL  published:2013-09-24 summary:This paper describes a new, freely available, highly multilingual named entity resource for person and organisation names that has been compiled over seven years of large-scale multilingual news analysis combined with Wikipedia mining, resulting in 205,000 per-son and organisation names plus about the same number of spelling variants written in over 20 different scripts and in many more languages. This resource, produced as part of the Europe Media Monitor activity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number of purposes. These include improving name search in databases or on the internet, seeding machine learning systems to learn named entity recognition rules, improve machine translation results, and more. We describe here how this resource was created; we give statistics on its current size; we address the issue of morphological inflection; and we give details regarding its functionality. Updates to this resource will be made available daily. version:1
arxiv-1309-6158 | Random Forests on Distance Matrices for Imaging Genetics Studies | http://arxiv.org/abs/1309.6158 | id:1309.6158 author:Aaron Sim, Dimosthenis Tsagkrasoulis, Giovanni Montana category:stat.ML stat.AP  published:2013-09-24 summary:We propose a non-parametric regression methodology, Random Forests on Distance Matrices (RFDM), for detecting genetic variants associated to quantitative phenotypes representing the human brain's structure or function, and obtained using neuroimaging techniques. RFDM, which is an extension of decision forests, requires a distance matrix as response that encodes all pair-wise phenotypic distances in the random sample. We discuss ways to learn such distances directly from the data using manifold learning techniques, and how to define such distances when the phenotypes are non-vectorial objects such as brain connectivity networks. We also describe an extension of RFDM to detect espistatic effects while keeping the computational complexity low. Extensive simulation results and an application to an imaging genetics study of Alzheimer's Disease are presented and discussed. version:1
arxiv-1309-6047 | Non-negative Matrix Factorization with Linear Constraints for Single-Channel Speech Enhancement | http://arxiv.org/abs/1309.6047 | id:1309.6047 author:Nikolay Lyubimov, Mikhail Kotov category:cs.SD cs.CL  published:2013-09-24 summary:This paper investigates a non-negative matrix factorization (NMF)-based approach to the semi-supervised single-channel speech enhancement problem where only non-stationary additive noise signals are given. The proposed method relies on sinusoidal model of speech production which is integrated inside NMF framework using linear constraints on dictionary atoms. This method is further developed to regularize harmonic amplitudes. Simple multiplicative algorithms are presented. The experimental evaluation was made on TIMIT corpus mixed with various types of noise. It has been shown that the proposed method outperforms some of the state-of-the-art noise suppression techniques in terms of signal-to-noise ratio. version:1
arxiv-1309-6013 | A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion | http://arxiv.org/abs/1309.6013 | id:1309.6013 author:T. Tony Cai, Wen-Xin Zhou category:stat.ML math.ST stat.TH  published:2013-09-24 summary:We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. version:1
arxiv-1309-5979 | Asymptotic Analysis of LASSOs Solution Path with Implications for Approximate Message Passing | http://arxiv.org/abs/1309.5979 | id:1309.5979 author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH  published:2013-09-23 summary:This paper concerns the performance of the LASSO (also knows as basis pursuit denoising) for recovering sparse signals from undersampled, randomized, noisy measurements. We consider the recovery of the signal $x_o \in \mathbb{R}^N$ from $n$ random and noisy linear observations $y= Ax_o + w$, where $A$ is the measurement matrix and $w$ is the noise. The LASSO estimate is given by the solution to the optimization problem $x_o$ with $\hat{x}_{\lambda} = \arg \min_x \frac{1}{2} \ y-Ax\ _2^2 + \lambda \ x\ _1$. Despite major progress in the theoretical analysis of the LASSO solution, little is known about its behavior as a function of the regularization parameter $\lambda$. In this paper we study two questions in the asymptotic setting (i.e., where $N \rightarrow \infty$, $n \rightarrow \infty$ while the ratio $n/N$ converges to a fixed number in $(0,1)$): (i) How does the size of the active set $\ \hat{x}_\lambda\ _0/N$ behave as a function of $\lambda$, and (ii) How does the mean square error $\ \hat{x}_{\lambda} - x_o\ _2^2/N$ behave as a function of $\lambda$? We then employ these results in a new, reliable algorithm for solving LASSO based on approximate message passing (AMP). version:1
