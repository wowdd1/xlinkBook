arxiv-1506-06981 | R-CNN minus R | http://arxiv.org/abs/1506.06981 | id:1506.06981 author:Karel Lenc, Andrea Vedaldi category:cs.CV  published:2015-06-23 summary:Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection. version:1
arxiv-1506-06975 | Efficient approximate Bayesian inference for models with intractable likelihoods | http://arxiv.org/abs/1506.06975 | id:1506.06975 author:Johan Dahlin, Mattias Villani, Thomas B. Schön category:stat.CO q-fin.RM stat.ML  published:2015-06-23 summary:We consider the problem of approximate Bayesian parameter inference in nonlinear state space models with intractable likelihoods. Sequential Monte Carlo with approximate Bayesian computations (SMC-ABC) is an approach to approximate the likelihood in this type of models. However, such approximations can be noisy and computationally costly which hinders efficient implementations using standard methods based on optimisation and statistical simulation. We propose a novel method based on the combination of Gaussian process optimisation (GPO) and SMC-ABC to create a Laplace approximation of the intractable posterior. The properties of the resulting GPO-ABC method are studied using stochastic volatility (SV) models with both synthetic and real-world data. We conclude that the algorithm enjoys: good accuracy comparable to particle Markov chain Monte Carlo with a significant reduction in computational cost and better robustness to noise in the estimates compared with a gradient-based optimisation algorithm. Finally, we make use of GPO-ABC to estimate the Value-at-Risk for a portfolio using a copula model with SV models for the margins. version:1
arxiv-1506-06972 | GEFCOM 2014 - Probabilistic Electricity Price Forecasting | http://arxiv.org/abs/1506.06972 | id:1506.06972 author:Gergo Barta, Gyula Borbely, Gabor Nagy, Sandor Kazi, Tamas Henk category:stat.ML cs.CE cs.LG stat.AP  published:2015-06-23 summary:Energy price forecasting is a relevant yet hard task in the field of multi-step time series forecasting. In this paper we compare a well-known and established method, ARMA with exogenous variables with a relatively new technique Gradient Boosting Regression. The method was tested on data from Global Energy Forecasting Competition 2014 with a year long rolling window forecast. The results from the experiment reveal that a multi-model approach is significantly better performing in terms of error metrics. Gradient Boosting can deal with seasonality and auto-correlation out-of-the box and achieve lower rate of normalized mean absolute error on real-world data. version:1
arxiv-1506-06962 | Graphs in machine learning: an introduction | http://arxiv.org/abs/1506.06962 | id:1506.06962 author:Pierre Latouche, Fabrice Rossi category:stat.ML cs.LG cs.SI physics.soc-ph  published:2015-06-23 summary:Graphs are commonly used to characterise interactions between objects of interest. Because they are based on a straightforward formalism, they are used in many scientific fields from computer science to historical sciences. In this paper, we give an introduction to some methods relying on graphs for learning. This includes both unsupervised and supervised methods. Unsupervised learning algorithms usually aim at visualising graphs in latent spaces and/or clustering the nodes. Both focus on extracting knowledge from graph topologies. While most existing techniques are only applicable to static graphs, where edges do not evolve through time, recent developments have shown that they could be extended to deal with evolving networks. In a supervised context, one generally aims at inferring labels or numerical values attached to nodes using both the graph and, when they are available, node characteristics. Balancing the two sources of information can be challenging, especially as they can disagree locally or globally. In both contexts, supervised and un-supervised, data can be relational (augmented with one or several global graphs) as described above, or graph valued. In this latter case, each object of interest is given as a full graph (possibly completed by other characteristics). In this context, natural tasks include graph clustering (as in producing clusters of graphs rather than clusters of nodes in a single graph), graph classification, etc. 1 Real networks One of the first practical studies on graphs can be dated back to the original work of Moreno [51] in the 30s. Since then, there has been a growing interest in graph analysis associated with strong developments in the modelling and the processing of these data. Graphs are now used in many scientific fields. In Biology [54, 2, 7], for instance, metabolic networks can describe pathways of biochemical reactions [41], while in social sciences networks are used to represent relation ties between actors [66, 56, 36, 34]. Other examples include powergrids [71] and the web [75]. Recently, networks have also been considered in other areas such as geography [22] and history [59, 39]. In machine learning, networks are seen as powerful tools to model problems in order to extract information from data and for prediction purposes. This is the object of this paper. For more complete surveys, we refer to [28, 62, 49, 45]. In this section, we introduce notations and highlight properties shared by most real networks. In Section 2, we then consider methods aiming at extracting information from a unique network. We will particularly focus on clustering methods where the goal is to find clusters of vertices. Finally, in Section 3, techniques that take a series of networks into account, where each network is version:1
arxiv-1310-7782 | Individual Biases, Cultural Evolution, and the Statistical Nature of Language Universals: The Case of Colour Naming Systems | http://arxiv.org/abs/1310.7782 | id:1310.7782 author:Andrea Baronchelli, Vittorio Loreto, Andrea Puglisi category:physics.soc-ph cs.CL cs.MA q-bio.PE  published:2013-10-29 summary:Language universals have long been attributed to an innate Universal Grammar. An alternative explanation states that linguistic universals emerged independently in every language in response to shared cognitive or perceptual biases. A computational model has recently shown how this could be the case, focusing on the paradigmatic example of the universal properties of colour naming patterns, and producing results in quantitative agreement with the experimental data. Here we investigate the role of an individual perceptual bias in the framework of the model. We study how, and to what extent, the structure of the bias influences the corresponding linguistic universal patterns. We show that the cultural history of a group of speakers introduces population-specific constraints that act against the pressure for uniformity arising from the individual bias, and we clarify the interplay between these two forces. version:2
arxiv-1504-06952 | Random Forest for the Contextual Bandit Problem - extended version | http://arxiv.org/abs/1504.06952 | id:1504.06952 author:Raphaël Féraud, Robin Allesiardo, Tanguy Urvoy, Fabrice Clérot category:cs.LG  published:2015-04-27 summary:To address the contextual bandit problem, we propose an online random forest algorithm. The analysis of the proposed algorithm is based on the sample complexity needed to find the optimal decision stump. Then, the decision stumps are assembled in a random collection of decision trees, Bandit Forest. We show that the proposed algorithm is optimal up to logarithmic factors. The dependence of the sample complexity upon the number of contextual variables is logarithmic. The computational cost of the proposed algorithm with respect to the time horizon is linear. These analytical results allow the proposed algorithm to be efficient in real applications, where the number of events to process is huge, and where we expect that some contextual variables, chosen from a large set, have potentially non- linear dependencies with the rewards. In the experiments done to illustrate the theoretical analysis, Bandit Forest obtain promising results in comparison with state-of-the-art algorithms. version:19
arxiv-1505-06556 | Differentially Private Distributed Online Learning | http://arxiv.org/abs/1505.06556 | id:1505.06556 author:Chencheng Li, Pan Zhou category:cs.LG  published:2015-05-25 summary:Online learning has been in the spotlight from the machine learning society for a long time. To handle massive data in Big Data era, one single learner could never efficiently finish this heavy task. Hence, in this paper, we propose a novel distributed online learning algorithm to solve the problem. Comparing to typical centralized online learner, the distributed learners optimize their own learning parameters based on local data sources and timely communicate with neighbors. However, communication may lead to a privacy breach. Thus, we use differential privacy to preserve the privacy of learners, and study the influence of guaranteeing differential privacy on the utility of the distributed online learning algorithm. Furthermore, by using the results from Kakade and Tewari (2009), we use the regret bounds of online learning to achieve fast convergence rates for offline learning algorithms in distributed scenarios, which provides tighter utility performance than the existing state-of-the-art results. In simulation, we demonstrate that the differentially private offline learning algorithm has high variance, but we can use mini-batch to improve the performance. Finally, the simulations show that the analytical results of our proposed theorems are right and our private distributed online learning algorithm is a general framework. version:2
arxiv-1506-06905 | Person re-identification via efficient inference in fully connected CRF | http://arxiv.org/abs/1506.06905 | id:1506.06905 author:Jiuqing Wan, Menglin Xing category:cs.CV  published:2015-06-23 summary:In this paper, we address the problem of person re-identification problem, i.e., retrieving instances from gallery which are generated by the same person as the given probe image. This is very challenging because the person's appearance usually undergoes significant variations due to changes in illumination, camera angle and view, background clutter, and occlusion over the camera network. In this paper, we assume that the matched gallery images should not only be similar to the probe, but also be similar to each other, under suitable metric. We express this assumption with a fully connected CRF model in which each node corresponds to a gallery and every pair of nodes are connected by an edge. A label variable is associated with each node to indicate whether the corresponding image is from target person. We define unary potential for each node using existing feature calculation and matching techniques, which reflect the similarity between probe and gallery image, and define pairwise potential for each edge in terms of a weighed combination of Gaussian kernels, which encode appearance similarity between pair of gallery images. The specific form of pairwise potential allows us to exploit an efficient inference algorithm to calculate the marginal distribution of each label variable for this dense connected CRF. We show the superiority of our method by applying it to public datasets and comparing with the state of the art. version:1
arxiv-1506-06904 | New Approach to translation of Isolated Units in English-Korean Machine Translation | http://arxiv.org/abs/1506.06904 | id:1506.06904 author:Kim Song Jon, An Hae Gum category:cs.CL  published:2015-06-23 summary:It is the most effective way for quick translation of tremendous amount of explosively increasing science and technique information material to develop a practicable machine translation system and introduce it into translation practice. This essay treats problems arising from translation of isolated units on the basis of the practical materials and experiments obtained in the development and introduction of English-Korean machine translation system. In other words, this essay considers establishment of information for isolated units and their Korean equivalents and word order. version:1
arxiv-1506-06882 | SALSA: A Novel Dataset for Multimodal Group Behavior Analysis | http://arxiv.org/abs/1506.06882 | id:1506.06882 author:Xavier Alameda-Pineda, Jacopo Staiano, Ramanathan Subramanian, Ligia Batrinca, Elisa Ricci, Bruno Lepri, Oswald Lanz, Nicu Sebe category:cs.CV  published:2015-06-23 summary:Studying free-standing conversational groups (FCGs) in unstructured social settings (e.g., cocktail party ) is gratifying due to the wealth of information available at the group (mining social networks) and individual (recognizing native behavioral and personality traits) levels. However, analyzing social scenes involving FCGs is also highly challenging due to the difficulty in extracting behavioral cues such as target locations, their speaking activity and head/body pose due to crowdedness and presence of extreme occlusions. To this end, we propose SALSA, a novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis, and make two main contributions to research on automated social interaction analysis: (1) SALSA records social interactions among 18 participants in a natural, indoor environment for over 60 minutes, under the poster presentation and cocktail party contexts presenting difficulties in the form of low-resolution images, lighting variations, numerous occlusions, reverberations and interfering sound sources; (2) To alleviate these problems we facilitate multimodal analysis by recording the social interplay using four static surveillance cameras and sociometric badges worn by each participant, comprising the microphone, accelerometer, bluetooth and infrared sensors. In addition to raw data, we also provide annotations concerning individuals' personality as well as their position, head, body orientation and F-formation information over the entire event duration. Through extensive experiments with state-of-the-art approaches, we show (a) the limitations of current methods and (b) how the recorded multiple cues synergetically aid automatic analysis of social interactions. SALSA is available at http://tev.fbk.eu/salsa. version:1
arxiv-1506-06881 | Automatic vehicle tracking and recognition from aerial image sequences | http://arxiv.org/abs/1506.06881 | id:1506.06881 author:Ognjen Arandjelovic category:cs.CV  published:2015-06-23 summary:This paper addresses the problem of automated vehicle tracking and recognition from aerial image sequences. Motivated by its successes in the existing literature focus on the use of linear appearance subspaces to describe multi-view object appearance and highlight the challenges involved in their application as a part of a practical system. A working solution which includes steps for data extraction and normalization is described. In experiments on real-world data the proposed methodology achieved promising results with a high correct recognition rate and few, meaningful errors (type II errors whereby genuinely similar targets are sometimes being confused with one another). Directions for future research and possible improvements of the proposed method are discussed. version:1
arxiv-1506-06876 | Autonomous 3D Reconstruction Using a MAV | http://arxiv.org/abs/1506.06876 | id:1506.06876 author:Alexander Popov, Dimitrios Zermas, Nikolaos Papanikolopoulos category:cs.CV  published:2015-06-23 summary:An approach is proposed for high resolution 3D reconstruction of an object using a Micro Air Vehicle (MAV). A system is described which autonomously captures images and performs a dense 3D reconstruction via structure from motion with no prior knowledge of the environment. Only the MAVs own sensors, the front facing camera and the Inertial Measurement Unit (IMU) are utilized. Precision agriculture is considered as an example application for the system. version:1
arxiv-1506-06868 | Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data | http://arxiv.org/abs/1506.06868 | id:1506.06868 author:Luping Zhou, Lei Wang, Lingqiao Liu, Philip Ogunbona, Dinggang Shen category:cs.CV cs.LG  published:2015-06-23 summary:Due to its causal semantics, Bayesian networks (BN) have been widely employed to discover the underlying data relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables, BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly optimize the classification performance of the GBNs. The advantages and disadvantages of the two frameworks are discussed and experimentally compared. Both of them demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN. version:1
arxiv-1506-06848 | A Feature-Based Analysis on the Impact of Set of Constraints for e-Constrained Differential Evolution | http://arxiv.org/abs/1506.06848 | id:1506.06848 author:Shayan Poursoltan, FranK Neumann category:cs.NE  published:2015-06-23 summary:Different types of evolutionary algorithms have been developed for constrained continuous optimization. We carry out a feature-based analysis of evolved constrained continuous optimization instances to understand the characteristics of constraints that make problems hard for evolutionary algorithm. In our study, we examine how various sets of constraints can influence the behaviour of e-Constrained Differential Evolution. Investigating the evolved instances, we obtain knowledge of what type of constraints and their features make a problem difficult for the examined algorithm. version:1
arxiv-1506-06628 | Modality-dependent Cross-media Retrieval | http://arxiv.org/abs/1506.06628 | id:1506.06628 author:Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi Feng, Shuicheng Yan category:cs.CV cs.IR cs.LG  published:2015-06-22 summary:In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5\%, which is a new state-of-the-art performance on the Wikipedia dataset. version:2
arxiv-1506-06832 | Detection and Analysis of Emotion From Speech Signals | http://arxiv.org/abs/1506.06832 | id:1506.06832 author:Assel Davletcharova, Sherin Sugathan, Bibia Abraham, Alex Pappachen James category:cs.SD cs.CL cs.HC  published:2015-06-23 summary:Recognizing emotion from speech has become one the active research themes in speech processing and in applications based on human-computer interaction. This paper conducts an experimental study on recognizing emotions from human speech. The emotions considered for the experiments include neutral, anger, joy and sadness. The distinuishability of emotional features in speech were studied first followed by emotion classification performed on a custom dataset. The classification was performed for different classifiers. One of the main feature attribute considered in the prepared dataset was the peak-to-peak distance obtained from the graphical representation of the speech signals. After performing the classification tests on a dataset formed from 30 different subjects, it was found that for getting better accuracy, one should consider the data collected from one person rather than considering the data from a group of people. version:1
arxiv-1506-06825 | DeepStereo: Learning to Predict New Views from the World's Imagery | http://arxiv.org/abs/1506.06825 | id:1506.06825 author:John Flynn, Ivan Neulander, James Philbin, Noah Snavely category:cs.CV  published:2015-06-22 summary:Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision, but their use in graphics problems has been limited. In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. To verify our method we show that it can convincingly reproduce known test views from nearby imagery. Additionally we show images rendered from novel viewpoints. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery. version:1
arxiv-1410-7455 | Parallel training of DNNs with Natural Gradient and Parameter Averaging | http://arxiv.org/abs/1410.7455 | id:1410.7455 author:Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur category:cs.NE cs.LG stat.ML  published:2014-10-27 summary:We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine. version:8
arxiv-1506-06726 | Skip-Thought Vectors | http://arxiv.org/abs/1506.06726 | id:1506.06726 author:Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler category:cs.CL cs.LG  published:2015-06-22 summary:We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available. version:1
arxiv-1506-06724 | Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books | http://arxiv.org/abs/1506.06724 | id:1506.06724 author:Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler category:cs.CV cs.CL  published:2015-06-22 summary:Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for. version:1
arxiv-1506-06714 | A Neural Network Approach to Context-Sensitive Generation of Conversational Responses | http://arxiv.org/abs/1506.06714 | id:1506.06714 author:Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, Bill Dolan category:cs.CL cs.AI cs.LG cs.NE  published:2015-06-22 summary:We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines. version:1
arxiv-1210-3709 | A Rank-Corrected Procedure for Matrix Completion with Fixed Basis Coefficients | http://arxiv.org/abs/1210.3709 | id:1210.3709 author:Weimin Miao, Shaohua Pan, Defeng Sun category:math.OC cs.IT cs.NA math.IT stat.ML  published:2012-10-13 summary:For the problems of low-rank matrix completion, the efficiency of the widely-used nuclear norm technique may be challenged under many circumstances, especially when certain basis coefficients are fixed, for example, the low-rank correlation matrix completion in various fields such as the financial market and the low-rank density matrix completion from the quantum state tomography. To seek a solution of high recovery quality beyond the reach of the nuclear norm, in this paper, we propose a rank-corrected procedure using a nuclear semi-norm to generate a new estimator. For this new estimator, we establish a non-asymptotic recovery error bound. More importantly, we quantify the reduction of the recovery error bound for this rank-corrected procedure. Compared with the one obtained for the nuclear norm penalized least squares estimator, this reduction can be substantial (around 50%). We also provide necessary and sufficient conditions for rank consistency in the sense of Bach (2008). Very interestingly, these conditions are highly related to the concept of constraint nondegeneracy in matrix optimization. As a byproduct, our results provide a theoretical foundation for the majorized penalty method of Gao and Sun (2010) and Gao (2010) for structured low-rank matrix optimization problems. Extensive numerical experiments demonstrate that our proposed rank-corrected procedure can simultaneously achieve a high recovery accuracy and capture the low-rank structure. version:3
arxiv-1506-06681 | Adaptive Digital Scan Variable Pixels | http://arxiv.org/abs/1506.06681 | id:1506.06681 author:Sherin Sugathan, Reshma Scaria, Alex Pappachen James category:cs.CV  published:2015-06-22 summary:The square and rectangular shape of the pixels in the digital images for sensing and display purposes introduces several inaccuracies in the representation of digital images. The major disadvantage of square pixel shapes is the inability to accurately capture and display the details in the objects having variable orientations to edges, shapes and regions. This effect can be observed by the inaccurate representation of diagonal edges in low resolution square pixel images. This paper explores a less investigated idea of using variable shaped pixels for improving visual quality of image scans without increasing the square pixel resolution. The proposed adaptive filtering technique reports an improvement in image PSNR. version:1
arxiv-1506-03229 | A cognitive neural architecture able to learn and communicate through natural language | http://arxiv.org/abs/1506.03229 | id:1506.03229 author:Bruno Golosio, Angelo Cangelosi, Olesya Gamotina, Giovanni Luca Masala category:cs.CL  published:2015-06-10 summary:Communicative interactions involve a kind of procedural knowledge that is used by the human brain for processing verbal and nonverbal inputs and for language production. Although considerable work has been done on modeling human language abilities, it has been difficult to bring them together to a comprehensive tabula rasa system compatible with current knowledge of how verbal information is processed in the brain. This work presents a cognitive system, entirely based on a large-scale neural architecture, which was developed to shed light on the procedural knowledge involved in language elaboration. The main component of this system is the central executive, which is a supervising system that coordinates the other components of the working memory. In our model, the central executive is a neural network that takes as input the neural activation states of the short-term memory and yields as output mental actions, which control the flow of information among the working memory components through neural gating mechanisms. The proposed system is capable of learning to communicate through natural language starting from tabula rasa, without any a priori knowledge of the structure of phrases, meaning of words, role of the different classes of words, only by interacting with a human through a text-based interface, using an open-ended incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, and to use them in expressive language. The model was validated on a corpus of 1587 input sentences, based on literature on early language assessment, at the level of about 4-years old child, and produced 521 output sentences, expressing a broad range of language processing functionalities. version:3
arxiv-1506-04701 | Multi-path Convolutional Neural Networks for Complex Image Classification | http://arxiv.org/abs/1506.04701 | id:1506.04701 author:Mingming Wang category:cs.CV  published:2015-06-15 summary:Convolutional Neural Networks demonstrate high performance on ImageNet Large-Scale Visual Recognition Challenges contest. Nevertheless, the published results only show the overall performance for all image classes. There is no further analysis why certain images get worse results and how they could be improved. In this paper, we provide deep performance analysis based on different types of images and point out the weaknesses of convolutional neural networks through experiment. We design a novel multiple paths convolutional neural network, which feeds different versions of images into separated paths to learn more comprehensive features. This model has better presentation for image than the traditional single path model. We acquire better classification results on complex validation set on both top 1 and top 5 scores than the best ILSVRC 2013 classification model. version:3
arxiv-1506-06659 | Target Tracking In Real Time Surveillance Cameras and Videos | http://arxiv.org/abs/1506.06659 | id:1506.06659 author:Nayyab Naseem, Mehreen Sirshar category:cs.CV  published:2015-06-22 summary:Security concerns has been kept on increasing, so it is important for everyone to keep their property safe from thefts and destruction. So the need for surveillance techniques are also increasing. The system has been developed to detect the motion in a video. A system has been developed for real time applications by using the techniques of background subtraction and frame differencing. In this system, motion is detected from the webcam or from the real time video. Background subtraction and frames differencing method has been used to detect the moving target. In background subtraction method, current frame is subtracted from the referenced frame and then the threshold is applied. If the difference is greater than the threshold then it is considered as the pixel from the moving object, otherwise it is considered as background pixel. Similarly, two frames difference method takes difference between two continuous frames. Then that resultant difference frame is thresholded and the amount of difference pixels is calculated. version:1
arxiv-1506-06650 | Multi-Modulus Algorithms Using Hyperbolic and Givens Rotations for MIMO Deconvolution | http://arxiv.org/abs/1506.06650 | id:1506.06650 author:Syed A. W. Shah, Karim Abed-Meraim, Tareq Y. Al-Naffouri category:cs.IT math.IT stat.ML  published:2015-06-22 summary:This paper addresses the problem of blind multiple-input multiple-output deconvolution of a communication system. Two new iterative blind source separation (BSS) algorithms are presented, based on the minimization of multi-modulus criterion. Further, we show that the design of algorithm in the complex domain is quite complicated, so a special structure of real filtering matrix is suggested and maintained throughout the design. Then, a first multi-modulus algorithm based on data whitening and Givens rotations is proposed. An improved version of the latter is introduced for small sample sizes by combining Hyperbolic (Shear) with Givens rotations to compensate for the ill whitening that occurs in this case. Proposed methods are finally compared with several BSS algorithms in terms of signal-to-interference and noise ratio, symbol error rate and convergence rate. Simulation results show that the proposed methods outperform the contemporary BSS algorithms. version:1
arxiv-1412-7156 | Representation Learning for cold-start recommendation | http://arxiv.org/abs/1412.7156 | id:1412.7156 author:Gabriella Contardo, Ludovic Denoyer, Thierry Artieres category:cs.IR cs.LG  published:2014-12-22 summary:A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunatly, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. This paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases. version:5
arxiv-1503-00095 | Task-Oriented Learning of Word Embeddings for Semantic Relation Classification | http://arxiv.org/abs/1503.00095 | id:1503.00095 author:Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, Yoshimasa Tsuruoka category:cs.CL  published:2015-02-28 summary:We present a novel learning method for word embeddings designed for relation classification. Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus. This allows us to explicitly incorporate relation-specific information into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classification model. On a well-established semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources. version:3
arxiv-1506-06579 | Understanding Neural Networks Through Deep Visualization | http://arxiv.org/abs/1506.06579 | id:1506.06579 author:Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson category:cs.CV cs.LG cs.NE  published:2015-06-22 summary:Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup. version:1
arxiv-1506-06573 | PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures | http://arxiv.org/abs/1506.06573 | id:1506.06573 author:Akshay Balsubramani category:cs.LG math.PR stat.ML  published:2015-06-22 summary:We give tight concentration bounds for mixtures of martingales that are simultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense; and (b) all finite times. These bounds are proved in terms of the martingale variance, extending classical Bernstein inequalities, and sharpening and simplifying prior work. version:1
arxiv-1502-02344 | Regularization Path of Cross-Validation Error Lower Bounds | http://arxiv.org/abs/1502.02344 | id:1502.02344 author:Atsushi Shibagaki, Yoshiki Suzuki, Masayuki Karasuyama, Ichiro Takeuchi category:stat.ML  published:2015-02-09 summary:Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. We demonstrate through numerical experiments that a theoretically guaranteed a choice of regularization parameter in the above sense is possible with reasonable computational costs. version:2
arxiv-1506-06490 | Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering | http://arxiv.org/abs/1506.06490 | id:1506.06490 author:Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, Xiaolong Wang category:cs.CL cs.IR cs.LG  published:2015-06-22 summary:In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of question-answer pair firstly, and then uses the joint representation as input of the long short-term memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach. version:1
arxiv-1506-06472 | The Ebb and Flow of Deep Learning: a Theory of Local Learning | http://arxiv.org/abs/1506.06472 | id:1506.06472 author:Pierre Baldi, Peter Sadowski category:cs.LG cs.NE stat.ML  published:2015-06-22 summary:In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules must first define the nature of the local variables, and then the functional form that ties them together into each learning rule. We consider polynomial local learning rules and analyze their behavior and capabilities in both linear and non-linear networks. As a byproduct, this framework enables also the discovery of new learning rules as well as important relationships between learning rules and group symmetries. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is propagated to the deep layers through a backward channel. The nature of the propagated information about the targets, and the backward channel through which this information is propagated, partition the space of learning algorithms. For any learning algorithm, the capacity of the backward channel can be defined as the number of bits provided about the gradient per weight, divided by the number of required operations per weight. We estimate the capacity associated with several learning algorithms and show that backpropagation outperforms them and achieves the maximum possible capacity. The theory clarifies the concept of Hebbian learning, what is learnable by Hebbian learning, and explains the sparsity of the space of learning rules discovered so far. version:1
arxiv-1506-04217 | On the Equivalence of CoCoA+ and DisDCA | http://arxiv.org/abs/1506.04217 | id:1506.04217 author:Ching-pei Lee category:cs.LG  published:2015-06-13 summary:In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015) under the setting used in their experiments, which is also the best setting suggested by the authors that proposed this algorithm, is equivalent to the practical variant of DisDCA (Yang, NIPS, 2013). version:2
arxiv-1506-06448 | DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation | http://arxiv.org/abs/1506.06448 | id:1506.06448 author:Holger R. Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV  published:2015-06-22 summary:Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-to-fine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via $P{-}\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional ConvNet ($R_1{-}\mathrm{ConvNet}$) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a "zoom-out" fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked $R_2{-}\mathrm{ConvNet}$ leveraging the joint space of CT intensities and the $P{-}\mathrm{ConvNet}$ dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6$\pm$6.3% in training and 71.8$\pm$10.7% in testing. version:1
arxiv-1503-03167 | Deep Convolutional Inverse Graphics Network | http://arxiv.org/abs/1503.03167 | id:1503.03167 author:Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, Joshua B. Tenenbaum category:cs.CV cs.GR cs.LG cs.NE  published:2015-03-11 summary:This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine. version:4
arxiv-1506-06418 | Extreme Extraction: Only One Hour per Relation | http://arxiv.org/abs/1506.06418 | id:1506.06418 author:Raphael Hoffmann, Luke Zettlemoyer, Daniel S. Weld category:cs.CL cs.AI cs.IR  published:2015-06-21 summary:Information Extraction (IE) aims to automatically generate a large knowledge base from natural language text, but progress remains slow. Supervised learning requires copious human annotation, while unsupervised and weakly supervised approaches do not deliver competitive accuracy. As a result, most fielded applications of IE, as well as the leading TAC-KBP systems, rely on significant amounts of manual engineering. Even "Extreme" methods, such as those reported in Freedman et al. 2011, require about 10 hours of expert labor per relation. This paper shows how to reduce that effort by an order of magnitude. We present a novel system, InstaRead, that streamlines authoring with an ensemble of methods: 1) encoding extraction rules in an expressive and compositional representation, 2) guiding the user to promising rules based on corpus statistics and mined resources, and 3) introducing a new interactive development cycle that provides immediate feedback --- even on large datasets. Experiments show that experts can create quality extractors in under an hour and even NLP novices can author good extractors. These extractors equal or outperform ones obtained by comparably supervised and state-of-the-art distantly supervised approaches. version:1
arxiv-1506-03163 | Permutation Search Methods are Efficient, Yet Faster Search is Possible | http://arxiv.org/abs/1506.03163 | id:1506.03163 author:Bilegsaikhan Naidan, Leonid Boytsov, Eric Nyberg category:cs.LG cs.DB cs.DS  published:2015-06-10 summary:We survey permutation-based methods for approximate k-nearest neighbor search. In these methods, every data point is represented by a ranked list of pivots sorted by the distance to this point. Such ranked lists are called permutations. The underpinning assumption is that, for both metric and non-metric spaces, the distance between permutations is a good proxy for the distance between original points. Thus, it should be possible to efficiently retrieve most true nearest neighbors by examining only a tiny subset of data points whose permutations are similar to the permutation of a query. We further test this assumption by carrying out an extensive experimental evaluation where permutation methods are pitted against state-of-the art benchmarks (the multi-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety of realistically large data set from the image and textual domain. The focus is on the high-accuracy retrieval methods for generic spaces. Additionally, we assume that both data and indices are stored in main memory. We find permutation methods to be reasonably efficient and describe a setup where these methods are most useful. To ease reproducibility, we make our software and data sets publicly available. version:3
arxiv-1502-07816 | Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms for Localization | http://arxiv.org/abs/1502.07816 | id:1502.07816 author:Joshua I. Glaser, Bradley M. Zamft, George M. Church, Konrad P. Kording category:q-bio.NC cs.CE cs.CV q-bio.QM  published:2015-02-27 summary:Current high-resolution imaging techniques require an intact sample that preserves spatial relationships. We here present a novel approach, "puzzle imaging," that allows imaging a spatially scrambled sample. This technique takes many spatially disordered samples, and then pieces them back together using local properties embedded within the sample. We show that puzzle imaging can efficiently produce high-resolution images using dimensionality reduction algorithms. We demonstrate the theoretical capabilities of puzzle imaging in three biological scenarios, showing that (1) relatively precise 3-dimensional brain imaging is possible; (2) the physical structure of a neural network can often be recovered based only on the neural connectivity matrix; and (3) a chemical map could be reproduced using bacteria with chemosensitive DNA and conjugative transfer. The ability to reconstruct scrambled images promises to enable imaging based on DNA sequencing of homogenized tissue samples. version:3
arxiv-1506-06366 | A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined with the Longest Common/Repeated Sub-sequence | http://arxiv.org/abs/1506.06366 | id:1506.06366 author:He-Wen Chen, Zih-Ci Wang, Shu-Yu Kuo, Yao-Hsin Chou category:cs.CE cs.AI cs.NE  published:2015-06-21 summary:Stock price forecasting is an important issue for investors since extreme accuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS) and Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues for forecasting prices. However, to the best of our knowledge, there are no significant studies using LCS/LRS to predict stock prices. It is impossible that prices stay exactly the same as historic prices. Therefore, this paper proposes a state-of-the-art method which combines FTS and LCS/LRS to predict stock prices. This method is based on the principle that history will repeat itself. It uses different interval lengths in FTS to fuzzify the prices, and LCS/LRS to look for the same pattern in the historical prices to predict future stock prices. In the experiment, we examine various intervals of fuzzy time sets in order to achieve high prediction accuracy. The proposed method outperforms traditional methods in terms of prediction accuracy and, furthermore, it is easy to implement. version:1
arxiv-1506-06318 | Communication Efficient Distributed Agnostic Boosting | http://arxiv.org/abs/1506.06318 | id:1506.06318 author:Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau category:cs.LG stat.ML  published:2015-06-21 summary:We consider the problem of learning from distributed data in the agnostic setting, i.e., in the presence of arbitrary forms of noise. Our main contribution is a general distributed boosting-based procedure for learning an arbitrary concept space, that is simultaneously noise tolerant, communication efficient, and computationally efficient. This improves significantly over prior works that were either communication efficient only in noise-free scenarios or computationally prohibitive. Empirical results on large synthetic and real-world datasets demonstrate the effectiveness and scalability of the proposed approach. version:1
arxiv-1505-02462 | Soft-Deep Boltzmann Machines | http://arxiv.org/abs/1505.02462 | id:1505.02462 author:Taichi Kiwaki category:cs.NE cs.LG stat.ML  published:2015-05-11 summary:We present a layered Boltzmann machine (BM) that can better exploit the advantages of a distributed representation. It is widely believed that deep BMs (DBMs) have far greater representational power than its shallow counterpart, restricted Boltzmann machines (RBMs). However, this expectation on the supremacy of DBMs over RBMs has not ever been validated in a theoretical fashion. In this paper, we provide both theoretical and empirical evidences that the representational power of DBMs can be actually rather limited in taking advantages of distributed representations. We propose an approximate measure for the representational power of a BM regarding to the efficiency of a distributed representation. With this measure, we show a surprising fact that DBMs can make inefficient use of distributed representations. Based on these observations, we propose an alternative BM architecture, which we dub soft-deep BMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed representations in terms of the measure. Experiments demonstrate that sDBMs outperform several state-of-the-art models, including DBMs, in generative tasks on binarized MNIST and Caltech-101 silhouettes. version:3
arxiv-1412-8197 | Metacarpal Bones Localization in X-ray Imagery Using Particle Filter Segmentation | http://arxiv.org/abs/1412.8197 | id:1412.8197 author:Z. Bardosi, D. Granata, G. Lugos, A. P. Tafti, S. Saxena category:cs.CV  published:2014-12-28 summary:Statistical methods such as sequential Monte Carlo Methods were proposed for detection, segmentation and tracking of objects in digital images. A similar approach, called Shape Particle Filters was introduced for the segmentation of vertebra, lungs and hearts [1]. In this contribution, a global shape and a local appearance model are derived from specific object annotated X-ray images of the metacarpal bones. In the test data a unique labeling of the bone boundary and the background points and a manual annotation is given. Using a set of local features (Haar-like) in the neighborhood of each pixel a probabilistic pixel classifier is built using the random forest algorithm. To fit the shape model to a new image, a label probability map is extracted and then the optimal shape is obtained by maximizing the probability of each landmark with the Differential Evolution algorithm. version:2
arxiv-1411-5908 | Understanding image representations by measuring their equivariance and equivalence | http://arxiv.org/abs/1411.5908 | id:1411.5908 author:Karel Lenc, Andrea Vedaldi category:cs.CV cs.LG cs.NE  published:2014-11-21 summary:Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too. version:2
arxiv-1506-06274 | Pose Estimation Based on 3D Models | http://arxiv.org/abs/1506.06274 | id:1506.06274 author:Chuiwen Ma, Hao Su, Liang Shi category:cs.CV cs.LG cs.RO  published:2015-06-20 summary:In this paper, we proposed a pose estimation system based on rendered image training set, which predicts the pose of objects in real image, with knowledge of object category and tight bounding box. We developed a patch-based multi-class classification algorithm, and an iterative approach to improve the accuracy. We achieved state-of-the-art performance on pose estimation task. version:1
arxiv-1506-06273 | 3D Reconstruction from Full-view Fisheye Camera | http://arxiv.org/abs/1506.06273 | id:1506.06273 author:Chuiwen Ma, Liang Shi, Hanlu Huang, Mengyuan Yan category:cs.CV  published:2015-06-20 summary:In this report, we proposed a 3D reconstruction method for the full-view fisheye camera. The camera we used is Ricoh Theta, which captures spherical images and has a wide field of view (FOV). The conventional stereo apporach based on perspective camera model cannot be directly applied and instead we used a spherical camera model to depict the relation between 3D point and its corresponding observation in the image. We implemented a system that can reconstruct the 3D scene using captures from two or more cameras. A GUI is also created to allow users to control the view perspective and obtain a better intuition of how the scene is rebuilt. Experiments showed that our reconstruction results well preserved the structure of the scene in the real world. version:1
arxiv-1506-06272 | Aligning where to see and what to tell: image caption with region-based attention and scene factorization | http://arxiv.org/abs/1506.06272 | id:1506.06272 author:Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, Changshui Zhang category:cs.CV cs.LG stat.ML  published:2015-06-20 summary:Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of "abstract meaning", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance. version:1
arxiv-1505-00359 | Can deep learning help you find the perfect match? | http://arxiv.org/abs/1505.00359 | id:1505.00359 author:Harm de Vries, Jason Yosinski category:cs.LG  published:2015-05-02 summary:Is he/she my type or not? The answer to this question depends on the personal preferences of the one asking it. The individual process of obtaining a full answer may generally be difficult and time consuming, but often an approximate answer can be obtained simply by looking at a photo of the potential match. Such approximate answers based on visual cues can be produced in a fraction of a second, a phenomenon that has led to a series of recently successful dating apps in which users rate others positively or negatively using primarily a single photo. In this paper we explore using convolutional networks to create a model of an individual's personal preferences based on rated photos. This introduced task is difficult due to the large number of variations in profile pictures and the noise in attractiveness labels. Toward this task we collect a dataset comprised of $9364$ pictures and binary labels for each. We compare performance of convolutional models trained in three ways: first directly on the collected dataset, second with features transferred from a network trained to predict gender, and third with features transferred from a network trained on ImageNet. Our findings show that ImageNet features transfer best, producing a model that attains $68.1\%$ accuracy on the test set and is moderately successful at predicting matches. version:2
arxiv-1506-06256 | Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science | http://arxiv.org/abs/1506.06256 | id:1506.06256 author:Grigori Fursin, Abdul Memon, Christophe Guillon, Anton Lokhmotov category:cs.SE cs.LG cs.PF  published:2015-06-20 summary:Nowadays, engineers have to develop software often without even knowing which hardware it will eventually run on in numerous mobile phones, tablets, desktops, laptops, data centers, supercomputers and cloud services. Unfortunately, optimizing compilers are not keeping pace with ever increasing complexity of computer systems anymore and may produce severely underperforming executable codes while wasting expensive resources and energy. We present our practical and collaborative solution to this problem via light-weight wrappers around any software piece when more than one implementation or optimization choice available. These wrappers are connected with a public Collective Mind autotuning infrastructure and repository of knowledge (c-mind.org/repo) to continuously monitor various important characteristics of these pieces (computational species) across numerous existing hardware configurations together with randomly selected optimizations. Similar to natural sciences, we can now continuously track winning solutions (optimizations for a given hardware) that minimize all costs of a computation (execution time, energy spent, code size, failures, memory and storage footprint, optimization time, faults, contentions, inaccuracy and so on) of a given species on a Pareto frontier along with any unexpected behavior. The community can then collaboratively classify solutions, prune redundant ones, and correlate them with various features of software, its inputs (data sets) and used hardware either manually or using powerful predictive analytics techniques. Our approach can then help create a large, realistic, diverse, representative, and continuously evolving benchmark with related optimization knowledge while gradually covering all possible software and hardware to be able to predict best optimizations and improve compilers and hardware depending on usage scenarios and requirements. version:1
arxiv-1504-07107 | Fast Sampling for Bayesian Max-Margin Models | http://arxiv.org/abs/1504.07107 | id:1504.07107 author:Wenbo Hu, Jun Zhu, Minjie Xu, Bo Zhang category:stat.ML cs.AI cs.LG  published:2015-04-27 summary:Bayesian max-margin models have shown great superiority in various machine learning tasks with a likelihood regularization, while the probabilistic Monte Carlo sampling for these models still remains challenging, especially for large-scale settings. In analogy to the data augmentation technique to tackle with non-smoothness of the hinge loss, we present a stochastic subgradient MCMC method which is easy to implement and computationally efficient. We investigate the variants that use adaptive stepsizes and thermostats to improve mixing speeds for Bayesian linear SVM. Furthermore, we design a stochastic subgradient HMC within Gibbs method and a doubly stochastic HMC algorithm for mixture of SVMs, a popular extension of linear classifiers. Experimental results on a wide range of problems demonstrate the effectiveness of our approach. version:4
arxiv-1411-4342 | Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations | http://arxiv.org/abs/1411.4342 | id:1411.4342 author:Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, James M. Robins category:stat.ML cs.AI cs.LG  published:2014-11-17 summary:We propose and analyze estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators. version:3
arxiv-1506-06179 | Detectability thresholds and optimal algorithms for community structure in dynamic networks | http://arxiv.org/abs/1506.06179 | id:1506.06179 author:Amir Ghasemian, Pan Zhang, Aaron Clauset, Cristopher Moore, Leto Peel category:stat.ML cond-mat.dis-nn cs.LG cs.SI physics.data-an  published:2015-06-19 summary:We study the fundamental limits on learning latent community structure in dynamic networks. Specifically, we study dynamic stochastic block models where nodes change their community membership over time, but where edges are generated independently at each time step. In this setting (which is a special case of several existing models), we are able to derive the detectability threshold exactly, as a function of the rate of change and the strength of the communities. Below this threshold, we claim that no algorithm can identify the communities better than chance. We then give two algorithms that are optimal in the sense that they succeed all the way down to this limit. The first uses belief propagation (BP), which gives asymptotically optimal accuracy, and the second is a fast spectral clustering algorithm, based on linearizing the BP equations. We verify our analytic and algorithmic results via numerical simulation, and close with a brief discussion of extensions and open questions. version:1
arxiv-1506-06158 | Structured Training for Neural Network Transition-Based Parsing | http://arxiv.org/abs/1506.06158 | id:1506.06158 author:David Weiss, Chris Alberti, Michael Collins, Slav Petrov category:cs.CL  published:2015-06-19 summary:We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in-depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy. version:1
arxiv-1412-2954 | Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity | http://arxiv.org/abs/1412.2954 | id:1412.2954 author:Santosh S. Vempala, Ying Xiao category:cs.DS cs.LG stat.ML  published:2014-12-09 summary:We present a simple, general technique for reducing the sample complexity of matrix and tensor decomposition algorithms applied to distributions. We use the technique to give a polynomial-time algorithm for standard ICA with sample complexity nearly linear in the dimension, thereby improving substantially on previous bounds. The analysis is based on properties of random polynomials, namely the spacings of an ensemble of polynomials. Our technique also applies to other applications of tensor decompositions, including spherical Gaussian mixture models. version:3
arxiv-1506-03694 | Learning language through pictures | http://arxiv.org/abs/1506.03694 | id:1506.03694 author:Grzegorz Chrupała, Ákos Kádár, Afra Alishahi category:cs.CL  published:2015-06-11 summary:We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases. version:2
arxiv-1506-06100 | Approximate Inference with the Variational Holder Bound | http://arxiv.org/abs/1506.06100 | id:1506.06100 author:Guillaume Bouchard, Balaji Lakshminarayanan category:stat.ML cs.LG math.FA  published:2015-06-19 summary:We introduce the Variational Holder (VH) bound as an alternative to Variational Bayes (VB) for approximate Bayesian inference. Unlike VB which typically involves maximization of a non-convex lower bound with respect to the variational parameters, the VH bound involves minimization of a convex upper bound to the intractable integral with respect to the variational parameters. Minimization of the VH bound is a convex optimization problem; hence the VH method can be applied using off-the-shelf convex optimization algorithms and the approximation error of the VH bound can also be analyzed using tools from convex optimization literature. We present experiments on the task of integrating a truncated multivariate Gaussian distribution and compare our method to VB, EP and a state-of-the-art numerical integration method for this problem. version:1
arxiv-1506-06096 | Graph-based compression of dynamic 3D point cloud sequences | http://arxiv.org/abs/1506.06096 | id:1506.06096 author:Dorina Thanou, Philip A. Chou, Pascal Frossard category:cs.CV cs.GR  published:2015-06-19 summary:This paper addresses the problem of compression of 3D point cloud sequences that are characterized by moving 3D positions and color attributes. As temporally successive point cloud frames are similar, motion estimation is key to effective compression of these sequences. It however remains a challenging problem as the point cloud frames have varying numbers of points without explicit correspondence information. We represent the time-varying geometry of these sequences with a set of graphs, and consider 3D positions and color attributes of the points clouds as signals on the vertices of the graphs. We then cast motion estimation as a feature matching problem between successive graphs. The motion is estimated on a sparse set of representative vertices using new spectral graph wavelet descriptors. A dense motion field is eventually interpolated by solving a graph-based regularization problem. The estimated motion is finally used for removing the temporal redundancy in the predictive coding of the 3D positions and the color characteristics of the point cloud sequences. Experimental results demonstrate that our method is able to accurately estimate the motion between consecutive frames. Moreover, motion estimation is shown to bring significant improvement in terms of the overall compression performance of the sequence. To the best of our knowledge, this is the first paper that exploits both the spatial correlation inside each frame (through the graph) and the temporal correlation between the frames (through the motion estimation) to compress the color and the geometry of 3D point cloud sequences in an efficient way. version:1
arxiv-1506-06072 | Quantifying the Effect of Sentiment on Information Diffusion in Social Media | http://arxiv.org/abs/1506.06072 | id:1506.06072 author:Emilio Ferrara, Zeyao Yang category:cs.SI cs.LG physics.soc-ph  published:2015-06-19 summary:Social media have become the main vehicle of information production and consumption online. Millions of users every day log on their Facebook or Twitter accounts to get updates and news, read about their topics of interest, and become exposed to new opportunities and interactions. Although recent studies suggest that the contents users produce will affect the emotions of their readers, we still lack a rigorous understanding of the role and effects of contents sentiment on the dynamics of information diffusion. This work aims at quantifying the effect of sentiment on information diffusion, to understand: (i) whether positive conversations spread faster and/or broader than negative ones (or vice-versa); (ii) what kind of emotions are more typical of popular conversations on social media; and, (iii) what type of sentiment is expressed in conversations characterized by different temporal dynamics. Our findings show that, at the level of contents, negative messages spread faster than positive ones, but positive ones reach larger audiences, suggesting that people are more inclined to share and favorite positive contents, the so-called positive bias. As for the entire conversations, we highlight how different temporal dynamics exhibit different sentiment patterns: for example, positive sentiment builds up for highly-anticipated events, while unexpected events are mainly characterized by negative sentiment. Our contribution is a milestone to understand how the emotions expressed in short texts affect their spreading in online social ecosystems, and may help to craft effective policies and strategies for content generation and diffusion. version:1
arxiv-1506-06068 | A general framework for the IT-based clustering methods | http://arxiv.org/abs/1506.06068 | id:1506.06068 author:Teng Qiu, Yongjie Li category:cs.CV cs.LG stat.ML  published:2015-06-19 summary:Previously, we proposed a physically inspired rule to organize the data points in a sparse yet effective structure, called the in-tree (IT) graph, which is able to capture a wide class of underlying cluster structures in the datasets, especially for the density-based datasets. Although there are some redundant edges or lines between clusters requiring to be removed by computer, this IT graph has a big advantage compared with the k-nearest-neighborhood (k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in the IT graph are much more distinguishable and thus can be easily determined by several methods previously proposed by us. In this paper, we propose a general framework to re-construct the IT graph, based on an initial neighborhood graph, such as the k-NN or MST, etc, and the corresponding graph distances. For this general framework, our previous way of constructing the IT graph turns out to be a special case of it. This general framework 1) can make the IT graph capture a wider class of underlying cluster structures in the datasets, especially for the manifolds, and 2) should be more effective to cluster the sparse or graph-based datasets. version:1
arxiv-1506-06040 | Tensor Analysis and Fusion of Multimodal Brain Images | http://arxiv.org/abs/1506.06040 | id:1506.06040 author:Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A. Valdes-Hernandez, Pedro A. Valdes-Sosa category:stat.ME cs.NA stat.AP stat.ML  published:2015-06-19 summary:Current high-throughput data acquisition technologies probe dynamical systems with different imaging modalities, generating massive data sets at different spatial and temporal resolutions posing challenging problems in multimodal data fusion. A case in point is the attempt to parse out the brain structures and networks that underpin human cognitive processes by analysis of different neuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the multimodal, multi-scale nature of neuroimaging data is well reflected by a multi-way (tensor) structure where the underlying processes can be summarized by a relatively small number of components or "atoms". We introduce Markov-Penrose diagrams - an integration of Bayesian DAG and tensor network notation in order to analyze these models. These diagrams not only clarify matrix and tensor EEG and fMRI time/frequency analysis and inverse problems, but also help understand multimodal fusion via Multiway Partial Least Squares and Coupled Matrix-Tensor Factorization. We show here, for the first time, that Granger causal analysis of brain networks is a tensor regression problem, thus allowing the atomic decomposition of brain networks. Analysis of EEG and fMRI recordings shows the potential of the methods and suggests their use in other scientific domains. version:1
arxiv-1506-06039 | moco: Fast Motion Correction for Calcium Imaging | http://arxiv.org/abs/1506.06039 | id:1506.06039 author:Alexander Dubbs, James Guevara, Darcy S. Peterka, Rafael Yuste category:cs.CV  published:2015-06-19 summary:Motion correction is the first in a pipeline of algorithms to analyze calcium imaging videos and extract biologically relevant information, for example the network structure of the neurons therein. Fast motion correction would be especially critical for closed-loop activity triggered stimulation experiments, where accurate detection and targeting of specific cells in necessary. Our algorithm uses a Fourier-transform approach, and its efficiency derives from a combination of judicious downsampling and the accelerated computation of many $L_2$ norms using dynamic programming and two-dimensional, fft-accelerated convolutions. Its accuracy is comparable to that of established community-used algorithms, and it is more stable to large translational motions. It is programmed in Java and is compatible with ImageJ. version:1
arxiv-1506-06021 | Measuring Emotional Contagion in Social Media | http://arxiv.org/abs/1506.06021 | id:1506.06021 author:Emilio Ferrara, Zeyao Yang category:cs.SI cs.LG physics.soc-ph  published:2015-06-19 summary:Social media are used as main discussion channels by millions of individuals every day. The content individuals produce in daily social-media-based micro-communications, and the emotions therein expressed, may impact the emotional states of others. A recent experiment performed on Facebook hypothesized that emotions spread online, even in absence of non-verbal cues typical of in-person interactions, and that individuals are more likely to adopt positive or negative emotions if these are over-expressed in their social network. Experiments of this type, however, raise ethical concerns, as they require massive-scale content manipulation with unknown consequences for the individuals therein involved. Here, we study the dynamics of emotional contagion using Twitter. Rather than manipulating content, we devise a null model that discounts some confounding factors (including the effect of emotional contagion). We measure the emotional valence of content the users are exposed to before posting their own tweets. We determine that on average a negative post follows an over-exposure to 4.34% more negative content than baseline, while positive posts occur after an average over-exposure to 4.50% more positive contents. We highlight the presence of a linear relationship between the average emotional valence of the stimuli users are exposed to, and that of the responses they produce. We also identify two different classes of individuals: highly and scarcely susceptible to emotional contagion. Highly susceptible users are significantly less inclined to adopt negative emotions than the scarcely susceptible ones, but equally likely to adopt positive emotions. In general, the likelihood of adopting positive emotions is much greater than that of negative emotions. version:1
arxiv-1506-06006 | Crowd Flow Segmentation in Compressed Domain using CRF | http://arxiv.org/abs/1506.06006 | id:1506.06006 author:Srinivas S. S. Kruthiventi, R. Venkatesh Babu category:cs.CV  published:2015-06-19 summary:Crowd flow segmentation is an important step in many video surveillance tasks. In this work, we propose an algorithm for segmenting flows in H.264 compressed videos in a completely unsupervised manner. Our algorithm works on motion vectors which can be obtained by partially decoding the compressed video without extracting any additional features. Our approach is based on modelling the motion vector field as a Conditional Random Field (CRF) and obtaining oriented motion segments by finding the optimal labelling which minimises the global energy of CRF. These oriented motion segments are recursively merged based on gradient across their boundaries to obtain the final flow segments. This work in compressed domain can be easily extended to pixel domain by substituting motion vectors with motion based features like optical flow. The proposed algorithm is experimentally evaluated on a standard crowd flow dataset and its superior performance in both accuracy and computational time are demonstrated through quantitative results. version:1
arxiv-1507-01889 | Design of OFDM radar pulses using genetic algorithm based techniques | http://arxiv.org/abs/1507.01889 | id:1507.01889 author:Gabriel Lellouch, Amit Kumar Mishra, Michael Inggs category:cs.NE  published:2015-06-19 summary:The merit of evolutionary algorithms (EA) to solve convex optimization problems is widely acknowledged. In this paper, a genetic algorithm (GA) optimization based waveform design framework is used to improve the features of radar pulses relying on the orthogonal frequency division multiplexing (OFDM) structure. Our optimization techniques focus on finding optimal phase code sequences for the OFDM signal. Several optimality criteria are used since we consider two different radar processing solutions which call either for single or multiple-objective optimizations. When minimization of the so-called peak-to-mean envelope power ratio (PMEPR) single-objective is tackled, we compare our findings with existing methods and emphasize on the merit of our approach. In the scope of the two-objective optimization, we first address PMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach based on the non-dominated sorting genetic algorithm-II (NSGA-II) provides design solutions with noticeable improvements as opposed to random sets of phase codes. We then look at another case of interest where the objective functions are two measures of the sidelobe level, namely PSLR and the integrated-sidelobe level ratio (ISLR) and propose to modify the NSGA-II to include a constrain on the PMEPR instead. In the last part, we illustrate via a case study how our encoding solution makes it possible to minimize the single objective PMEPR while enabling a target detection enhancement strategy, when the SNR metric would be chosen for the detection framework. version:1
arxiv-1506-06001 | Stereoscopic Cinema | http://arxiv.org/abs/1506.06001 | id:1506.06001 author:Frédéric Devernay, Paul Beardsley category:cs.CV  published:2015-06-19 summary:Stereoscopic cinema has seen a surge of activity in recent years, and for the first time all of the major Hollywood studios released 3-D movies in 2009. This is happening alongside the adoption of 3-D technology for sports broadcasting, and the arrival of 3-D TVs for the home. Two previous attempts to introduce 3-D cinema in the 1950s and the 1980s failed because the contemporary technology was immature and resulted in viewer discomfort. But current technologies -- such as accurately-adjustable 3-D camera rigs with onboard computers to automatically inform a camera operator of inappropriate stereoscopic shots, digital processing for post-shooting rectification of the 3-D imagery, digital projectors for accurate positioning of the two stereo projections on the cinema screen, and polarized silver screens to reduce cross-talk between the viewers left- and right-eyes -- mean that the viewer experience is at a much higher level of quality than in the past. Even so, creation of stereoscopic cinema is an open, active research area, and there are many challenges from acquisition to post-production to automatic adaptation for different-sized display. This chapter describes the current state-of-the-art in stereoscopic cinema, and directions of future work. version:1
arxiv-1506-05985 | Enhanced Lasso Recovery on Graph | http://arxiv.org/abs/1506.05985 | id:1506.05985 author:Xavier Bresson, Thomas Laurent, James von Brecht category:cs.LG stat.ML  published:2015-06-19 summary:This work aims at recovering signals that are sparse on graphs. Compressed sensing offers techniques for signal recovery from a few linear measurements and graph Fourier analysis provides a signal representation on graph. In this paper, we leverage these two frameworks to introduce a new Lasso recovery algorithm on graphs. More precisely, we present a non-convex, non-smooth algorithm that outperforms the standard convex Lasso technique. We carry out numerical experiments on three benchmark graph datasets. version:1
arxiv-0801-1179 | Corpus sp{é}cialis{é} et ressource de sp{é}cialit{é} | http://arxiv.org/abs/0801.1179 | id:0801.1179 author:Bernard Jacquemin, Sabine Ploux category:cs.IR cs.CL  published:2008-01-08 summary:"Semantic Atlas" is a mathematic and statistic model to visualise word senses according to relations between words. The model, that has been applied to proximity relations from a corpus, has shown its ability to distinguish word senses as the corpus' contributors comprehend them. We propose to use the model and a specialised corpus in order to create automatically a specialised dictionary relative to the corpus' domain. A morpho-syntactic analysis performed on the corpus makes it possible to create the dictionary from syntactic relations between lexical units. The semantic resource can be used to navigate semantically - and not only lexically - through the corpus, to create classical dictionaries or for diachronic studies of the language. version:2
arxiv-1501-07719 | Montblanc: GPU accelerated Radio Interferometer Measurement Equations in support of Bayesian Inference for Radio Observations | http://arxiv.org/abs/1501.07719 | id:1501.07719 author:Simon Perkins, Patrick Marais, Jonathan Zwart, Iniyan Natarajan, Cyril Tasse, Oleg Smirnov category:cs.DC astro-ph.IM cs.CV  published:2015-01-30 summary:We present Montblanc, a GPU implementation of the Radio interferometer measurement equation (RIME) in support of the Bayesian inference for radio observations (BIRO) technique. BIRO uses Bayesian inference to select sky models that best match the visibilities observed by a radio interferometer. To accomplish this, BIRO evaluates the RIME multiple times, varying sky model parameters to produce multiple model visibilities. Chi-squared values computed from the model and observed visibilities are used as likelihood values to drive the Bayesian sampling process and select the best sky model. As most of the elements of the RIME and chi-squared calculation are independent of one another, they are highly amenable to parallel computation. Additionally, Montblanc caters for iterative RIME evaluation to produce multiple chi-squared values. Modified model parameters are transferred to the GPU between each iteration. We implemented Montblanc as a Python package based upon NVIDIA's CUDA architecture. As such, it is easy to extend and implement different pipelines. At present, Montblanc supports point and Gaussian morphologies, but is designed for easy addition of new source profiles. Montblanc's RIME implementation is performant: On an NVIDIA K40, it is approximately 250 times faster than MeqTrees on a dual hexacore Intel E5-2620v2 CPU. Compared to the OSKAR simulator's GPU-implemented RIME components it is 7.7 and 12 times faster on the same K40 for single and double-precision floating point respectively. However, OSKAR's RIME implementation is more general than Montblanc's BIRO-tailored RIME. Theoretical analysis of Montblanc's dominant CUDA kernel suggests that it is memory bound. In practice, profiling shows that is balanced between compute and memory, as much of the data required by the problem is retained in L1 and L2 cache. version:3
arxiv-1406-6909 | Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks | http://arxiv.org/abs/1406.6909 | id:1406.6909 author:Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox category:cs.LG cs.CV cs.NE  published:2014-06-26 summary:Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While such generic features cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor. version:2
arxiv-1509-03200 | A new Initial Centroid finding Method based on Dissimilarity Tree for K-means Algorithm | http://arxiv.org/abs/1509.03200 | id:1509.03200 author:Abhishek Kumar, Suresh Chandra Gupta category:cs.LG  published:2015-06-19 summary:Cluster analysis is one of the primary data analysis technique in data mining and K-means is one of the commonly used partitioning clustering algorithm. In K-means algorithm, resulting set of clusters depend on the choice of initial centroids. If we can find initial centroids which are coherent with the arrangement of data, the better set of clusters can be obtained. This paper proposes a method based on the Dissimilarity Tree to find, the better initial centroid as well as every bit more accurate cluster with less computational time. Theory analysis and experimental results indicate that the proposed method can effectively improve the accuracy of clusters and reduce the computational complexity of the K-means algorithm. version:1
arxiv-1506-05950 | Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels | http://arxiv.org/abs/1506.05950 | id:1506.05950 author:Tapio Pahikkala, Markus Viljanen, Antti Airola, Willem Waegeman category:cs.LG stat.ML  published:2015-06-19 summary:We consider the problem of learning regression functions from pairwise data when there exists prior knowledge that the relation to be learned is symmetric or anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing or anti-symmetrizing pairwise kernel functions. Through spectral analysis, we show that these transformations reduce the kernel's effective dimension. Further, we provide an analysis of the approximation properties of the resulting kernels, and bound the regularization bias of the kernels in terms of the corresponding bias of the original kernel. version:1
arxiv-1506-05937 | A Tight Runtime Analysis of the $(1+(λ, λ))$ Genetic Algorithm on OneMax | http://arxiv.org/abs/1506.05937 | id:1506.05937 author:Benjamin Doerr, Carola Doerr category:cs.NE  published:2015-06-19 summary:Understanding how crossover works is still one of the big challenges in evolutionary computation research, and making our understanding precise and proven by mathematical means might be an even bigger one. As one of few examples where crossover provably is useful, the $(1+(\lambda, \lambda))$ Genetic Algorithm (GA) was proposed recently in [Doerr, Doerr, Ebel: TCS 2015]. Using the fitness level method, the expected optimization time on general OneMax functions was analyzed and a $O(\max\{n\log(n)/\lambda, \lambda n\})$ bound was proven for any offspring population size $\lambda \in [1..n]$. We improve this work in several ways, leading to sharper bounds and a better understanding of how the use of crossover speeds up the runtime in this algorithm. We first improve the upper bound on the runtime to $O(\max\{n\log(n)/\lambda, n\lambda \log\log(\lambda)/\log(\lambda)\})$. This improvement is made possible from observing that in the parallel generation of $\lambda$ offspring via crossover (but not mutation), the best of these often is better than the expected value, and hence several fitness levels can be gained in one iteration. We then present the first lower bound for this problem. It matches our upper bound for all values of $\lambda$. This allows to determine the asymptotically optimal value for the population size. It is $\lambda = \Theta(\sqrt{\log(n)\log\log(n)/\log\log\log(n)})$, which gives an optimization time of $\Theta(n \sqrt{\log(n)\log\log\log(n)/\log\log(n)})$. Hence the improved runtime analysis gives a better runtime guarantee along with a better suggestion for the parameter $\lambda$. We finally give a tail bound for the upper tail of the runtime distribution, which shows that the actual runtime exceeds our runtime guarantee by a factor of $(1+\delta)$ with probability $O((n/\lambda^2)^{-\delta})$ only. version:1
arxiv-1506-05936 | Sampling constrained probability distributions using Spherical Augmentation | http://arxiv.org/abs/1506.05936 | id:1506.05936 author:Shiwei Lan, Babak Shahbaba category:stat.CO stat.ML  published:2015-06-19 summary:Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit, many copula models, and latent Dirichlet allocation (LDA). Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. In this paper, we propose a novel augmentation technique that handles a wide range of constraints by mapping the constrained domain to a sphere in the augmented space. By moving freely on the surface of this sphere, sampling algorithms handle constraints implicitly and generate proposals that remain within boundaries when mapped back to the original space. Our proposed method, called {Spherical Augmentation}, provides a mathematically natural and computationally efficient framework for sampling from constrained probability distributions. We show the advantages of our method over state-of-the-art sampling algorithms, such as exact Hamiltonian Monte Carlo, using several examples including truncated Gaussian distributions, Bayesian Lasso, Bayesian bridge regression, reconstruction of quantized stationary Gaussian process, and LDA for topic modeling. version:1
arxiv-1506-05934 | Expectation Particle Belief Propagation | http://arxiv.org/abs/1506.05934 | id:1506.05934 author:Thibaut Lienart, Yee Whye Teh, Arnaud Doucet category:stat.CO cs.AI stat.ML  published:2015-06-19 summary:We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of Ihler and McAllester (2009) at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure. version:1
arxiv-1506-05929 | Exploring the influence of scale on artist attribution | http://arxiv.org/abs/1506.05929 | id:1506.05929 author:Nanne van Noord, Eric Postma category:cs.CV  published:2015-06-19 summary:Previous work has shown that the artist of an artwork can be identified by use of computational methods that analyse digital images. However, the digitised artworks are often investigated at a coarse scale discarding many of the important details that may define an artist's style. In recent years high resolution images of artworks have become available, which, combined with increased processing power and new computational techniques, allow us to analyse digital images of artworks at a very fine scale. In this work we train and evaluate a Convolutional Neural Network (CNN) on the task of artist attribution using artwork images of varying resolutions. To this end, we combine two existing methods to enable the application of high resolution images to CNNs. By comparing the attribution performances obtained at different scales, we find that in most cases finer scales are beneficial to the attribution performance, whereas for a minority of the artists, coarser scales appear to be preferable. We conclude that artist attribution would benefit from a multi-scale CNN approach which vastly expands the possibilities for computational art forensics. version:1
arxiv-1506-05920 | New Descriptor for Glomerulus Detection in Kidney Microscopy Image | http://arxiv.org/abs/1506.05920 | id:1506.05920 author:Tsuyoshi Kato, Raissa Relator, Hayliang Ngouv, Yoshihiro Hirohashi, Tetsuhiro Kakimoto, Kinya Okada category:cs.CV  published:2015-06-19 summary:Glomerulus detection is a key step in histopathological evaluation of microscopy images of kidneys. However, the task of automatic detection of glomeruli poses challenges due to the disparity in sizes and shapes of glomeruli in renal sections. Moreover, extensive variations of their intensities due to heterogeneity in immunohistochemistry staining are also encountered. Despite being widely recognized as a powerful descriptor for general object detection, the rectangular histogram of oriented gradients (Rectangular HOG) suffers from many false positives due to the aforementioned difficulties in the context of glomerulus detection. A new descriptor referred to as Segmental HOG is developed to perform a comprehensive detection of hundreds of glomeruli in images of whole kidney sections. The new descriptor possesses flexible blocks that can be adaptively fitted to input images to acquire robustness to deformations of glomeruli. Moreover, the novel segmentation technique employed herewith generates high quality segmentation outputs and the algorithm is assured to converge to an optimal solution. Consequently, experiments using real world image data reveal that Segmental HOG achieves significant improvements in detection performance compared to Rectangular HOG. The proposed descriptor and method for glomeruli detection present promising results and is expected to be useful in pathological evaluation. version:1
arxiv-1506-05913 | Solving Problems with Unknown Solution Length at (Almost) No Extra Cost | http://arxiv.org/abs/1506.05913 | id:1506.05913 author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE  published:2015-06-19 summary:Most research in the theory of evolutionary computation assumes that the problem at hand has a fixed problem size. This assumption does not always apply to real-world optimization challenges, where the length of an optimal solution may be unknown a priori. Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] we analyze variants of the (1+1) evolutionary algorithm for problems with unknown solution length. For their setting, in which the solution length is sampled from a geometric distribution, we provide mutation rates that yield an expected optimization time that is of the same order as that of the (1+1) EA knowing the solution length. We then show that almost the same run times can be achieved even if \emph{no} a priori information on the solution length is available. Finally, we provide mutation rates suitable for settings in which neither the solution length nor the positions of the relevant bits are known. Again we obtain almost optimal run times for the \textsc{OneMax} and \textsc{LeadingOnes} test functions, thus solving an open problem from Cathabard et al. version:1
arxiv-1506-05908 | Deep Knowledge Tracing | http://arxiv.org/abs/1506.05908 | id:1506.05908 author:Chris Piech, Jonathan Spencer, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas Guibas, Jascha Sohl-Dickstein category:cs.AI cs.CY cs.LG K.3.1  published:2015-06-19 summary:Knowledge tracing---where a machine models the knowledge of a student as they interact with coursework---is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs. version:1
arxiv-1506-05900 | Representation Learning for Clustering: A Statistical Framework | http://arxiv.org/abs/1506.05900 | id:1506.05900 author:Hassan Ashtiani, Shai Ben-David category:stat.ML cs.LG  published:2015-06-19 summary:We address the problem of communicating domain knowledge from a user to the designer of a clustering algorithm. We propose a protocol in which the user provides a clustering of a relatively small random sample of a data set. The algorithm designer then uses that sample to come up with a data representation under which $k$-means clustering results in a clustering (of the full data set) that is aligned with the user's clustering. We provide a formal statistical model for analyzing the sample complexity of learning a clustering representation with this paradigm. We then introduce a notion of capacity of a class of possible representations, in the spirit of the VC-dimension, showing that classes of representations that have finite such dimension can be successfully learned with sample size error bounds, and end our discussion with an analysis of that dimension for classes of representations induced by linear embeddings. version:1
arxiv-1503-02768 | Novel Bernstein-like Concentration Inequalities for the Missing Mass | http://arxiv.org/abs/1503.02768 | id:1503.02768 author:Bahman Yari Saeed Khanloo, Gholamreza Haffari category:stat.ML  published:2015-03-10 summary:We are concerned with obtaining novel concentration inequalities for the missing mass, i.e. the total probability mass of the outcomes not observed in the sample. We not only derive - for the first time - distribution-free Bernstein-like deviation bounds with sublinear exponents in deviation size for missing mass, but also improve the results of McAllester and Ortiz (2003) andBerend and Kontorovich (2013, 2012) for small deviations which is the most interesting case in learning theory. It is known that the majority of standard inequalities cannot be directly used to analyze heterogeneous sums i.e. sums whose terms have large difference in magnitude. Our generic and intuitive approach shows that the heterogeneity issue introduced in McAllester and Ortiz (2003) is resolvable at least in the case of missing mass via regulating the terms using our novel thresholding technique. version:2
arxiv-1502-07073 | Strongly Adaptive Online Learning | http://arxiv.org/abs/1502.07073 | id:1502.07073 author:Amit Daniely, Alon Gonen, Shai Shalev-Shwartz category:cs.LG  published:2015-02-25 summary:Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems. version:3
arxiv-1506-05870 | To Know Where We Are: Vision-Based Positioning in Outdoor Environments | http://arxiv.org/abs/1506.05870 | id:1506.05870 author:Kuan-Wen Chen, Chun-Hsin Wang, Xiao Wei, Qiao Liang, Ming-Hsuan Yang, Chu-Song Chen, Yi-Ping Hung category:cs.CV  published:2015-06-19 summary:Augmented reality (AR) displays become more and more popular recently, because of its high intuitiveness for humans and high-quality head-mounted display have rapidly developed. To achieve such displays with augmented information, highly accurate image registration or ego-positioning are required, but little attention have been paid for out-door environments. This paper presents a method for ego-positioning in outdoor environments with low cost monocular cameras. To reduce the computational and memory requirements as well as the communication overheads, we formulate the model compression algorithm as a weighted k-cover problem for better preserving model structures. Specifically for real-world vision-based positioning applications, we consider the issues with large scene change and propose a model update algorithm to tackle these problems. A long- term positioning dataset with more than one month, 106 sessions, and 14,275 images is constructed. Based on both local and up-to-date models constructed in our approach, extensive experimental results show that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be achieved, which outperforms existing vision-based algorithms. version:1
arxiv-1506-02344 | Stay on path: PCA along graph paths | http://arxiv.org/abs/1506.02344 | id:1506.02344 author:Megasthenis Asteris, Anastasios Kyrillidis, Alexandros G. Dimakis, Han-Gyol Yi and, Bharath Chandrasekaran category:stat.ML cs.IT cs.LG math.IT math.OC  published:2015-06-08 summary:We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph $G$ on $p$ vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in $G$. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets. version:2
arxiv-1506-06129 | A simple application of FIC to model selection | http://arxiv.org/abs/1506.06129 | id:1506.06129 author:Paul A. Wiggins category:physics.data-an cs.LG stat.ML  published:2015-06-19 summary:We have recently proposed a new information-based approach to model selection, the Frequentist Information Criterion (FIC), that reconciles information-based and frequentist inference. The purpose of this current paper is to provide a simple example of the application of this criterion and a demonstration of the natural emergence of model complexities with both AIC-like ($N^0$) and BIC-like ($\log N$) scaling with observation number $N$. The application developed is deliberately simplified to make the analysis analytically tractable. version:1
arxiv-1506-05849 | An Iterative Convolutional Neural Network Algorithm Improves Electron Microscopy Image Segmentation | http://arxiv.org/abs/1506.05849 | id:1506.05849 author:Xundong Wu category:cs.NE cs.LG  published:2015-06-18 summary:To build the connectomics map of the brain, we developed a new algorithm that can automatically refine the Membrane Detection Probability Maps (MDPM) generated to perform automatic segmentation of electron microscopy (EM) images. To achieve this, we executed supervised training of a convolutional neural network to recover the removed center pixel label of patches sampled from a MDPM. MDPM can be generated from other machine learning based algorithms recognizing whether a pixel in an image corresponds to the cell membrane. By iteratively applying this network over MDPM for multiple rounds, we were able to significantly improve membrane segmentation results. version:1
arxiv-1506-05843 | Dependent Multinomial Models Made Easy: Stick Breaking with the Pólya-Gamma Augmentation | http://arxiv.org/abs/1506.05843 | id:1506.05843 author:Scott W. Linderman, Matthew J. Johnson, Ryan P. Adams category:stat.ML  published:2015-06-18 summary:Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in P\'olya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead. version:1
arxiv-1506-04782 | Cheap Bandits | http://arxiv.org/abs/1506.04782 | id:1506.04782 author:Manjesh Kumar Hanawal, Venkatesh Saligrama, Michal Valko, R\' emi Munos category:cs.LG  published:2015-06-15 summary:We consider stochastic sequential learning problems where the learner can observe the \textit{average reward of several actions}. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually \textit{cheaper} to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is \textit{smooth} over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while \textit{minimizing the sensing cost}. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a $\Omega(\sqrt{dT})$ lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension $d$. version:2
arxiv-1506-05822 | Optimal model-free prediction from multivariate time series | http://arxiv.org/abs/1506.05822 | id:1506.05822 author:Jakob Runge, Reik V. Donner, Jürgen Kurths category:stat.ML stat.ME  published:2015-06-18 summary:Forecasting a time series from multivariate predictors constitutes a challenging problem, especially using model-free approaches. Most techniques, such as nearest-neighbor prediction, quickly suffer from the curse of dimensionality and overfitting for more than a few predictors which has limited their application mostly to the univariate case. Therefore, selection strategies are needed that harness the available information as efficiently as possible. Since often the right combination of predictors matters, ideally all subsets of possible predictors should be tested for their predictive power, but the exponentially growing number of combinations makes such an approach computationally prohibitive. Here a prediction scheme that overcomes this strong limitation is introduced utilizing a causal pre-selection step which drastically reduces the number of possible predictors to the most predictive set of causal drivers making a globally optimal search scheme tractable. The information-theoretic optimality is derived and practical selection criteria are discussed. As demonstrated for multivariate nonlinear stochastic delay processes, the optimal scheme can even be less computationally expensive than commonly used sub-optimal schemes like forward selection. The method suggests a general framework to apply the optimal model-free approach to select variables and subsequently fit a model to further improve a prediction or learn statistical dependencies. The performance of this framework is illustrated on a climatological index of El Ni\~no Southern Oscillation. version:1
arxiv-1506-03509 | Convolutional Dictionary Learning through Tensor Factorization | http://arxiv.org/abs/1506.03509 | id:1506.03509 author:Furong Huang, Animashree Anandkumar category:cs.LG stat.ML  published:2015-06-10 summary:Tensor methods have emerged as a powerful paradigm for consistent learning of many latent variable models such as topic models, independent component analysis and dictionary learning. Model parameters are estimated via CP decomposition of the observed higher order input moments. However, in many domains, additional invariances such as shift invariances exist, enforced via models such as convolutional dictionary learning. In this paper, we develop novel tensor decomposition algorithms for parameter estimation of convolutional models. Our algorithm is based on the popular alternating least squares method, but with efficient projections onto the space of stacked circulant matrices. Our method is embarrassingly parallel and consists of simple operations such as fast Fourier transforms and matrix multiplications. Our algorithm converges to the dictionary much faster and more accurately compared to the alternating minimization over filters and activation maps. version:3
arxiv-1503-01811 | Optimally Combining Classifiers Using Unlabeled Data | http://arxiv.org/abs/1503.01811 | id:1503.01811 author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML  published:2015-03-05 summary:We develop a worst-case analysis of aggregation of classifier ensembles for binary classification. The task of predicting to minimize error is formulated as a game played over a given set of unlabeled data (a transductive setting), where prior label information is encoded as constraints on the game. The minimax solution of this game identifies cases where a weighted combination of the classifiers can perform significantly better than any single classifier. version:3
arxiv-1506-05776 | A tree augmented naive Bayesian network experiment for breast cancer prediction | http://arxiv.org/abs/1506.05776 | id:1506.05776 author:Ping Ren category:stat.ML q-bio.QM  published:2015-06-18 summary:In order to investigate the breast cancer prediction problem on the aging population with the grades of DCIS, we conduct a tree augmented naive Bayesian network experiment trained and tested on a large clinical dataset including consecutive diagnostic mammography examinations, consequent biopsy outcomes and related cancer registry records in the population of women across all ages. The aggregated results of our ten-fold cross validation method recommend a biopsy threshold higher than 2% for the aging population. version:1
arxiv-1412-4271 | Multi-Context Models for Reasoning under Partial Knowledge: Generative Process and Inference Grammar | http://arxiv.org/abs/1412.4271 | id:1412.4271 author:Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos category:cs.AI math.LO math.PR stat.ML  published:2014-12-13 summary:Arriving at the complete probabilistic knowledge of a domain, i.e., learning how all variables interact, is indeed a demanding task. In reality, settings often arise for which an individual merely possesses partial knowledge of the domain, and yet, is expected to give adequate answers to a variety of posed queries. That is, although precise answers to some queries, in principle, cannot be achieved, a range of plausible answers is attainable for each query given the available partial knowledge. In this paper, we propose the Multi-Context Model (MCM), a new graphical model to represent the state of partial knowledge as to a domain. MCM is a middle ground between Probabilistic Logic, Bayesian Logic, and Probabilistic Graphical Models. For this model we discuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to form partial beliefs regarding a domain in a gradual and probabilistically consistent way, and (ii) how to perform inference, i.e., to evaluate a probability of interest involving some variables of the domain. version:2
arxiv-1412-1866 | Integer Programming Ensemble of Classifiers for Temporal Relations | http://arxiv.org/abs/1412.1866 | id:1412.1866 author:Catherine Kerr, Terri Hoare, Jakub Marecek, Paula Carroll category:cs.CL cs.LG math.OC  published:2014-12-05 summary:Extraction of events and understanding related temporal expression among them is a major challenge in natural language processing. In longer texts, processing on sentence-by-sentence or expression-by-expression basis often fails, in part due to the disregard for the consistency of the processed data. We present an ensemble method, which reconciles the output of multiple classifiers for temporal expressions, subject to consistency constraints across the whole text. The use of integer programming to enforce the consistency constraints globally improves upon the best published results from the TempEval-3 Challenge considerably. version:2
arxiv-1506-04422 | A Fast Incremental Gaussian Mixture Model | http://arxiv.org/abs/1506.04422 | id:1506.04422 author:Rafael Pinto, Paulo Engel category:cs.LG I.2.6  published:2015-06-14 summary:This work builds upon previous efforts in online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of learning from data streams in a single-pass by improving its model after analyzing each data point and discarding it thereafter. Nevertheless, it suffers from the scalability point-of-view, due to its asymptotic time complexity of $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data points, $K$ Gaussian components and $D$ dimensions, rendering it inadequate for high-dimensional data. In this paper, we manage to reduce this complexity to $\operatorname{O}\bigl(NKD^2\bigr)$ by deriving formulas for working directly with precision matrices instead of covariance matrices. The final result is a much faster and scalable algorithm which can be applied to high dimensional tasks. This is confirmed by applying the modified algorithm to high-dimensional classification datasets. version:2
arxiv-1506-05751 | Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks | http://arxiv.org/abs/1506.05751 | id:1506.05751 author:Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus category:cs.CV  published:2015-06-18 summary:In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset. version:1
arxiv-1502-02251 | From Pixels to Torques: Policy Learning with Deep Dynamical Models | http://arxiv.org/abs/1502.02251 | id:1502.02251 author:Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:stat.ML cs.LG cs.RO cs.SY  published:2015-02-08 summary:Data-efficient learning in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. In this paper, we consider one instance of this challenge, the pixels to torques problem, where an agent must learn a closed-loop control policy from pixel information only. We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information. The key ingredient is a deep dynamical model that uses deep auto-encoders to learn a low-dimensional embedding of images jointly with a predictive model in this low-dimensional feature space. Joint learning ensures that not only static but also dynamic properties of the data are accounted for. This is crucial for long-term predictions, which lie at the core of the adaptive model predictive control strategy that we use for closed-loop control. Compared to state-of-the-art reinforcement learning methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces and is an important step toward fully autonomous learning from pixels to torques. version:3
arxiv-1506-04147 | On the accuracy of self-normalized log-linear models | http://arxiv.org/abs/1506.04147 | id:1506.04147 author:Jacob Andreas, Maxim Rabinovich, Dan Klein, Michael I. Jordan category:stat.ML cs.CL cs.LG stat.ME  published:2015-06-12 summary:Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as "self-normalization", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions. version:2
arxiv-1506-05703 | "The Sum of Its Parts": Joint Learning of Word and Phrase Representations with Autoencoders | http://arxiv.org/abs/1506.05703 | id:1506.05703 author:Rémi Lebret, Ronan Collobert category:cs.CL  published:2015-06-18 summary:Recently, there has been a lot of effort to represent words in continuous vector spaces. Those representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We introduce a novel model that jointly learns word vector representations and their summation. Word representations are learnt using the word co-occurrence statistical information. To embed sequences of words (i.e. phrases) with different sizes into a common semantic space, we propose to average word vector representations. In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors. We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations. While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation. Such representations of phrases could be interesting for many tasks in natural language processing. version:1
arxiv-1506-05692 | A hybrid algorithm for Bayesian network structure learning with application to multi-label learning | http://arxiv.org/abs/1506.05692 | id:1506.05692 author:Maxime Gasse, Alex Aussem, Haytham Elghazel category:stat.ML cs.AI cs.LG  published:2015-06-18 summary:We present a novel hybrid algorithm for Bayesian network structure learning, called H2PC. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. The algorithm is based on divide-and-conquer constraint-based subroutines to learn the local structure around a target variable. We conduct two series of experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning. First, we use eight well-known Bayesian network benchmarks with various data sizes to assess the quality of the learned structure returned by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in terms of goodness of fit to new data and quality of the network structure with respect to the true dependence structure of the data. Second, we investigate H2PC's ability to solve the multi-label learning problem. We provide theoretical results to characterize and identify graphically the so-called minimal label powersets that appear as irreducible factors in the joint distribution under the faithfulness condition. The multi-label learning problem is then decomposed into a series of multi-class classification problems, where each multi-class variable encodes a label powerset. H2PC is shown to compare favorably to MMHC in terms of global classification accuracy over ten multi-label data sets covering different application domains. Overall, our experiments support the conclusions that local structural learning with H2PC in the form of local neighborhood induction is a theoretically well-motivated and empirically effective learning framework that is well suited to multi-label learning. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available. version:1
arxiv-1504-02162 | Concentric network symmetry grasps authors' styles in word adjacency networks | http://arxiv.org/abs/1504.02162 | id:1504.02162 author:Diego R. Amancio, Filipi N. Silva, Luciano da F. Costa category:cs.CL  published:2015-04-09 summary:Several characteristics of written texts have been inferred from statistical analysis derived from networked models. Even though many network measurements have been adapted to study textual properties at several levels of complexity, some textual aspects have been disregarded. In this paper, we study the symmetry of word adjacency networks, a well-known representation of text as a graph. A statistical analysis of the symmetry distribution performed in several novels showed that most of the words do not display symmetric patterns of connectivity. More specifically, the merged symmetry displayed a distribution similar to the ubiquitous power-law distribution. Our experiments also revealed that the studied metrics do not correlate with other traditional network measurements, such as the degree or betweenness centrality. The effectiveness of the symmetry measurements was verified in the authorship attribution task. Interestingly, we found that specific authors prefer particular types of symmetric motifs. As a consequence, the authorship of books could be accurately identified in 82.5% of the cases, in a dataset comprising books written by 8 authors. Because the proposed measurements for text analysis are complementary to the traditional approach, they can be used to improve the characterization of text networks, which might be useful for related applications, such as those relying on the identification of topical words and information retrieval. version:2
arxiv-1506-05666 | Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure | http://arxiv.org/abs/1506.05666 | id:1506.05666 author:Hiroaki Sasaki, Michael U. Gutmann, Hayaru Shouno, Aapo Hyvärinen category:stat.ML  published:2015-06-18 summary:The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the linear independent components. It would thus be very useful to estimate the dependency structure from data. While such models have been proposed, they usually concentrated on higher-order correlations such as energy (square) correlations. Yet, linear correlations are a fundamental and informative form of dependency in many real data sets. Linear correlations are usually completely removed by ICA and related methods, so they can only be analyzed by developing new methods which explicitly allow for linearly correlated components. In this paper, we propose a probabilistic model of linear non-Gaussian components which are allowed to have both linear and energy correlations. The precision matrix of the linear components is assumed to be randomly generated by a higher-order process and explicitly parametrized by a parameter matrix. The estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using simulations with artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their correlation structure simultaneously. Applications on simulated complex cells with natural image input, as well as spectrograms of natural audio data show that the method finds new kinds of dependencies between the components. version:1
arxiv-1405-2664 | FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test | http://arxiv.org/abs/1405.2664 | id:1405.2664 author:Ji Zhao, Deyu Meng category:cs.AI cs.LG stat.ML  published:2014-05-12 summary:The maximum mean discrepancy (MMD) is a recently proposed test statistic for two-sample test. Its quadratic time complexity, however, greatly hampers its availability to large-scale applications. To accelerate the MMD calculation, in this study we propose an efficient method called FastMMD. The core idea of FastMMD is to equivalently transform the MMD with shift-invariant kernels into the amplitude expectation of a linear combination of sinusoid components based on Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Taking advantage of sampling of Fourier transform, FastMMD decreases the time complexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$ are the size and dimension of the sample set, respectively. Here $L$ is the number of basis functions for approximating kernels which determines the approximation accuracy. For kernels that are spherically invariant, the computation can be further accelerated to $O(L N \log d)$ by using the Fastfood technique (Le et al., 2013). The uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates. We have further provided a geometric explanation for our method, namely ensemble of circular discrepancy, which facilitates us to understand the insight of MMD, and is hopeful to help arouse more extensive metrics for assessing two-sample test. Experimental results substantiate that FastMMD is with similar accuracy as exact MMD, while with faster computation speed and lower variance than the existing MMD approximation methods. version:2
arxiv-1506-05603 | Point-wise Map Recovery and Refinement from Functional Correspondence | http://arxiv.org/abs/1506.05603 | id:1506.05603 author:Emanuele Rodolà, Michael Moeller, Daniel Cremers category:cs.CV cs.CG  published:2015-06-18 summary:Since their introduction in the shape analysis community, functional maps have met with considerable success due to their ability to compactly represent dense correspondences between deformable shapes, with applications ranging from shape matching and image segmentation, to exploration of large shape collections. Despite the numerous advantages of such representation, however, the problem of converting a given functional map back to a point-to-point map has received a surprisingly limited interest. In this paper we analyze the general problem of point-wise map recovery from arbitrary functional maps. In doing so, we rule out many of the assumptions required by the currently established approach -- most notably, the limiting requirement of the input shapes being nearly-isometric. We devise an efficient recovery process based on a simple probabilistic model. Experiments confirm that this approach achieves remarkable accuracy improvements in very challenging cases. version:1
arxiv-1409-2944 | Collaborative Deep Learning for Recommender Systems | http://arxiv.org/abs/1409.2944 | id:1409.2944 author:Hao Wang, Naiyan Wang, Dit-Yan Yeung category:cs.LG cs.CL cs.IR cs.NE stat.ML  published:2014-09-10 summary:Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art. version:2
arxiv-1503-01397 | Bethe Projections for Non-Local Inference | http://arxiv.org/abs/1503.01397 | id:1503.01397 author:Luke Vilnis, David Belanger, Daniel Sheldon, Andrew McCallum category:stat.ML cs.CL cs.LG  published:2015-03-04 summary:Many inference problems in structured prediction are naturally solved by augmenting a tractable dependency structure with complex, non-local auxiliary objectives. This includes the mean field family of variational inference algorithms, soft- or hard-constrained inference using Lagrangian relaxation or linear programming, collective graphical models, and forms of semi-supervised learning such as posterior regularization. We present a method to discriminatively learn broad families of inference objectives, capturing powerful non-local statistics of the latent variables, while maintaining tractable and provably fast inference using non-Euclidean projected gradient descent with a distance-generating function given by the Bethe entropy. We demonstrate the performance and flexibility of our method by (1) extracting structured citations from research papers by learning soft global constraints, (2) achieving state-of-the-art results on a widely-used handwriting recognition task using a novel learned non-convex inference procedure, and (3) providing a fast and highly scalable algorithm for the challenging problem of inference in a collective graphical model applied to bird migration. version:2
arxiv-1506-05561 | Comparing and evaluating extended Lambek calculi | http://arxiv.org/abs/1506.05561 | id:1506.05561 author:Richard Moot category:cs.CL cs.LO  published:2015-06-18 summary:Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, was innovative in many ways, notably as a precursor of linear logic. But it also showed that we could treat our grammatical framework as a logic (as opposed to a logical theory). However, though it was successful in giving at least a basic treatment of many linguistic phenomena, it was also clear that a slightly more expressive logical calculus was needed for many other cases. Therefore, many extensions and variants of the Lambek calculus have been proposed, since the eighties and up until the present day. As a result, there is now a large class of calculi, each with its own empirical successes and theoretical results, but also each with its own logical primitives. This raises the question: how do we compare and evaluate these different logical formalisms? To answer this question, I present two unifying frameworks for these extended Lambek calculi. Both are proof net calculi with graph contraction criteria. The first calculus is a very general system: you specify the structure of your sequents and it gives you the connectives and contractions which correspond to it. The calculus can be extended with structural rules, which translate directly into graph rewrite rules. The second calculus is first-order (multiplicative intuitionistic) linear logic, which turns out to have several other, independently proposed extensions of the Lambek calculus as fragments. I will illustrate the use of each calculus in building bridges between analyses proposed in different frameworks, in highlighting differences and in helping to identify problems. version:1
arxiv-1503-02129 | Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior | http://arxiv.org/abs/1503.02129 | id:1503.02129 author:Qingming Tang, Siqi Sun, Jinbo Xu category:cs.LG cs.AI stat.ML  published:2015-03-07 summary:Learning the network structure underlying data is an important problem in machine learning. This paper introduces a novel prior to study the inference of scale-free networks, which are widely used to model social and biological networks. The prior not only favors a desirable global node degree distribution, but also takes into consideration the relative strength of all the possible edges adjacent to the same node and the estimated degree of each individual node. To fulfill this, ranking is incorporated into the prior, which makes the problem challenging to solve. We employ an ADMM (alternating direction method of multipliers) framework to solve the Gaussian Graphical model regularized by this prior. Our experiments on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than the others such as the scale-free inducing prior, the hub-inducing prior and the $l_1$ norm. version:3
arxiv-1503-02128 | Exact Hybrid Covariance Thresholding for Joint Graphical Lasso | http://arxiv.org/abs/1503.02128 | id:1503.02128 author:Qingming Tang, Chao Yang, Jian Peng, Jinbo Xu category:cs.LG cs.AI stat.ML  published:2015-03-07 summary:This paper considers the problem of estimating multiple related Gaussian graphical models from a $p$-dimensional dataset consisting of different classes. Our work is based upon the formulation of this problem as group graphical lasso. This paper proposes a novel hybrid covariance thresholding algorithm that can effectively identify zero entries in the precision matrices and split a large joint graphical lasso problem into small subproblems. Our hybrid covariance thresholding method is superior to existing uniform thresholding methods in that our method can split the precision matrix of each individual class using different partition schemes and thus split group graphical lasso into much smaller subproblems, each of which can be solved very fast. In addition, this paper establishes necessary and sufficient conditions for our hybrid covariance thresholding algorithm. The superior performance of our thresholding method is thoroughly analyzed and illustrated by a few experiments on simulated data and real gene expression data. version:2
arxiv-1503-03832 | FaceNet: A Unified Embedding for Face Recognition and Clustering | http://arxiv.org/abs/1503.03832 | id:1503.03832 author:Florian Schroff, Dmitry Kalenichenko, James Philbin category:cs.CV  published:2015-03-12 summary:Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result by 30% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other. version:3
arxiv-1506-05514 | Learning Contextualized Semantics from Co-occurring Terms via a Siamese Architecture | http://arxiv.org/abs/1506.05514 | id:1506.05514 author:Ubai Sandouk, Ke Chen category:cs.IR cs.CL cs.LG I.2.6  published:2015-06-17 summary:One of the biggest challenges in Multimedia information retrieval and understanding is to bridge the semantic gap by properly modeling concept semantics in context. The presence of out of vocabulary (OOV) concepts exacerbates this difficulty. To address the semantic gap issues, we formulate a problem on learning contextualized semantics from descriptive terms and propose a novel Siamese architecture to model the contextualized semantics from descriptive terms. By means of pattern aggregation and probabilistic topic models, our Siamese architecture captures contextualized semantics from the co-occurring descriptive terms via unsupervised learning, which leads to a concept embedding space of the terms in context. Furthermore, the co-occurring OOV concepts can be easily represented in the learnt concept embedding space. The main properties of the concept embedding space are demonstrated via visualization. Using various settings in semantic priming, we have carried out a thorough evaluation by comparing our approach to a number of state-of-the-art methods on six annotation corpora in different domains, i.e., MagTag5K, CAL500 and Million Song Dataset in the music domain as well as Corel5K, LabelMe and SUNDatabase in the image domain. Experimental results on semantic priming suggest that our approach outperforms those state-of-the-art methods considerably in various aspects. version:1
arxiv-1506-05676 | Pragmatic Side Effects | http://arxiv.org/abs/1506.05676 | id:1506.05676 author:Jiri Marsik, Maxime Amblard category:cs.CL  published:2015-06-17 summary:In the quest to give a formal compositional semantics to natural languages, semanticists have started turning their attention to phenomena that have been also considered as parts of pragmatics (e.g., discourse anaphora and presupposition projection). To account for these phenomena, the very kinds of meanings assigned to words and phrases are often revisited. To be more specific, in the prevalent paradigm of modeling natural language denotations using the simply-typed lambda calculus (higher-order logic) this means revisiting the types of denotations assigned to individual parts of speech. However, the lambda calculus also serves as a fundamental theory of computation, and in the study of computation, similar type shifts have been employed to give a meaning to side effects. Side effects in programming languages correspond to actions that go beyond the lexical scope of an expression (a thrown exception might propagate throughout a program, a variable modified at one point might later be read at an another) or even beyond the scope of the program itself (a program might interact with the outside world by e.g., printing documents, making sounds, operating robotic limbs...). version:1
arxiv-1506-05427 | Real time unsupervised learning of visual stimuli in neuromorphic VLSI systems | http://arxiv.org/abs/1506.05427 | id:1506.05427 author:Massimiliano Giulioni, Federico Corradi, Vittorio Dante, Paolo del Giudice category:cs.NE q-bio.NC  published:2015-06-17 summary:Neuromorphic chips embody computational principles operating in the nervous system, into microelectronic devices. In this domain it is important to identify computational primitives that theory and experiments suggest as generic and reusable cognitive elements. One such element is provided by attractor dynamics in recurrent networks. Point attractors are equilibrium states of the dynamics (up to fluctuations), determined by the synaptic structure of the network; a `basin' of attraction comprises all initial states leading to a given attractor upon relaxation, hence making attractor dynamics suitable to implement robust associative memory. The initial network state is dictated by the stimulus, and relaxation to the attractor state implements the retrieval of the corresponding memorized prototypical pattern. In a previous work we demonstrated that a neuromorphic recurrent network of spiking neurons and suitably chosen, fixed synapses supports attractor dynamics. Here we focus on learning: activating on-chip synaptic plasticity and using a theory-driven strategy for choosing network parameters, we show that autonomous learning, following repeated presentation of simple visual stimuli, shapes a synaptic connectivity supporting stimulus-selective attractors. Associative memory develops on chip as the result of the coupled stimulus-driven neural activity and ensuing synaptic dynamics, with no artificial separation between learning and retrieval phases. version:1
arxiv-1506-05424 | Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization | http://arxiv.org/abs/1506.05424 | id:1506.05424 author:Conrado Silva Miranda, Fernando José Von Zuben category:cs.NE cs.AI  published:2015-06-17 summary:This paper introduces a high-performance hybrid algorithm, called Hybrid Hypervolume Maximization Algorithm (H2MA), for multi-objective optimization that alternates between exploring the decision space and exploiting the already obtained non-dominated solutions. The proposal is centered on maximizing the hypervolume indicator, thus converting the multi-objective problem into a single-objective one. The exploitation employs gradient-based methods, but considering a single candidate efficient solution at a time, to overcome limitations associated with population-based approaches and also to allow an easy control of the number of solutions provided. There is an interchange between two steps. The first step is a deterministic local exploration, endowed with an automatic procedure to detect stagnation. When stagnation is detected, the search is switched to a second step characterized by a stochastic global exploration using an evolutionary algorithm. Using five ZDT benchmarks with 30 variables, the performance of the new algorithm is compared to state-of-the-art algorithms for multi-objective optimization, more specifically NSGA-II, SPEA2, and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and smaller distance to the true Pareto frontier with significantly less function evaluations, even when the gradient is estimated numerically. Furthermore, although only continuous decision spaces have been considered here, discrete decision spaces could also have been treated, replacing gradient-based search by hill-climbing. Finally, a thorough explanation is provided to support the expressive gain in performance that was achieved. version:1
arxiv-1506-05402 | Editorial for the First Workshop on Mining Scientific Papers: Computational Linguistics and Bibliometrics | http://arxiv.org/abs/1506.05402 | id:1506.05402 author:Iana Atanassova, Marc Bertin, Philipp Mayr category:cs.CL cs.DL cs.IR  published:2015-06-17 summary:The workshop "Mining Scientific Papers: Computational Linguistics and Bibliometrics" (CLBib 2015), co-located with the 15th International Society of Scientometrics and Informetrics Conference (ISSI 2015), brought together researchers in Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scale text analytics and sense mining of scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing (NLP). The goals of the workshop were to answer questions like: How can we enhance author network analysis and Bibliometrics using data obtained by text analytics? What insights can NLP provide on the structure of scientific writing, on citation networks, and on in-text citation analysis? This workshop is the first step to foster the reflection on the interdisciplinarity and the benefits that the two disciplines Bibliometrics and Natural Language Processing can drive from it. version:1
arxiv-1506-05393 | MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance Fingerprinting | http://arxiv.org/abs/1506.05393 | id:1506.05393 author:Ze Wang category:cs.DS cs.CV  published:2015-06-17 summary:Magnetic resonance fingerprinting (MRF) is a new technique for simultaneously quantifying multiple MR parameters using one temporally resolved MR scan. But its brute-force dictionary generating and searching (DGS) process causes a huge disk space demand and computational burden, prohibiting it from a practical multiple slice high-definition imaging. The purpose of this paper was to provide a fast and space efficient DGS algorithm for MRF. Based on an empirical analysis of properties of the distance function of the acquired MRF signal and the pre-defined MRF dictionary entries, we proposed a parameter separable MRF DGS method, which breaks the multiplicative computation complexity into an additive one and enabling a resolution scalable multi-resolution DGS process, which was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM was hundreds or thousands of times faster than the original brute-force DGS method. The acceleration was even higher when considering the time difference for generating the dictionary. Using a high precision quantification, MRF can find the right parameter values for a 64x64 imaging slice in 117 secs. Our data also showed that spatial constraints can be used to further speed up MRF ZOOM. version:1
arxiv-1406-7157 | An Incentive Compatible Multi-Armed-Bandit Crowdsourcing Mechanism with Quality Assurance | http://arxiv.org/abs/1406.7157 | id:1406.7157 author:Shweta Jain, Sujit Gujar, Satyanath Bhat, Onno Zoeter, Y. Narahari category:cs.GT cs.LG  published:2014-06-27 summary:Consider a requester who wishes to crowdsource a series of identical binary labeling tasks to a pool of workers so as to achieve an assured accuracy for each task, in a cost optimal way. The workers are heterogeneous with unknown but fixed qualities and their costs are private. The problem is to select for each task an optimal subset of workers so that the outcome obtained from the selected workers guarantees a target accuracy level. The problem is a challenging one even in a non strategic setting since the accuracy of aggregated label depends on unknown qualities. We develop a novel multi-armed bandit (MAB) mechanism for solving this problem. First, we propose a framework, Assured Accuracy Bandit (AAB), which leads to an MAB algorithm, Constrained Confidence Bound for a Non Strategic setting (CCB-NS). We derive an upper bound on the number of time steps the algorithm chooses a sub-optimal set that depends on the target accuracy level and true qualities. A more challenging situation arises when the requester not only has to learn the qualities of the workers but also elicit their true costs. We modify the CCB-NS algorithm to obtain an adaptive exploration separated algorithm which we call { \em Constrained Confidence Bound for a Strategic setting (CCB-S)}. CCB-S algorithm produces an ex-post monotone allocation rule and thus can be transformed into an ex-post incentive compatible and ex-post individually rational mechanism that learns the qualities of the workers and guarantees a given target accuracy level in a cost optimal way. We provide a lower bound on the number of times any algorithm should select a sub-optimal set and we see that the lower bound matches our upper bound upto a constant factor. We provide insights on the practical implementation of this framework through an illustrative example and we show the efficacy of our algorithms through simulations. version:3
arxiv-1412-2432 | MLitB: Machine Learning in the Browser | http://arxiv.org/abs/1412.2432 | id:1412.2432 author:Edward Meeds, Remco Hendriks, Said Al Faraby, Magiel Bruntink, Max Welling category:cs.DC cs.LG stat.ML  published:2014-12-08 summary:With few exceptions, the field of Machine Learning (ML) research has largely ignored the browser as a computational engine. Beyond an educational resource for ML, the browser has vast potential to not only improve the state-of-the-art in ML research, but also, inexpensively and on a massive scale, to bring sophisticated ML learning and prediction to the public at large. This paper introduces MLitB, a prototype ML framework written entirely in JavaScript, capable of performing large-scale distributed computing with heterogeneous classes of devices. The development of MLitB has been driven by several underlying objectives whose aim is to make ML learning and usage ubiquitous (by using ubiquitous compute devices), cheap and effortlessly distributed, and collaborative. This is achieved by allowing every internet capable device to run training algorithms and predictive models with no software installation and by saving models in universally readable formats. Our prototype library is capable of training deep neural networks with synchronized, distributed stochastic gradient descent. MLitB offers several important opportunities for novel ML research, including: development of distributed learning algorithms, advancement of web GPU algorithms, novel field and mobile applications, privacy preserving computing, and green grid-computing. MLitB is available as open source software. version:2
arxiv-1411-4000 | How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets | http://arxiv.org/abs/1411.4000 | id:1411.4000 author:Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri Garakani, Dong Guo, Aurélien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, Fei Sha category:cs.LG stat.ML  published:2014-11-14 summary:The computational complexity of kernel methods has often been a major barrier for applying them to large-scale learning problems. We argue that this barrier can be effectively overcome. In particular, we develop methods to scale up kernel models to successfully tackle large-scale learning problems that are so far only approachable by deep learning architectures. Based on the seminal work by Rahimi and Recht on approximating kernel functions with features derived from random projections, we advance the state-of-the-art by proposing methods that can efficiently train models with hundreds of millions of parameters, and learn optimal representations from multiple kernels. We conduct extensive empirical studies on problems from image recognition and automatic speech recognition, and show that the performance of our kernel models matches that of well-engineered deep neural nets (DNNs). To the best of our knowledge, this is the first time that a direct comparison between these two methods on large-scale problems is reported. Our kernel methods have several appealing properties: training with convex optimization, cost for training a single model comparable to DNNs, and significantly reduced total cost due to fewer hyperparameters to tune for model selection. Our contrastive study between these two very different but equally competitive models sheds light on fundamental questions such as how to learn good representations. version:2
arxiv-1506-05268 | Deep Denoising Auto-encoder for Statistical Speech Synthesis | http://arxiv.org/abs/1506.05268 | id:1506.05268 author:Zhenzhou Wu, Shinji Takaki, Junichi Yamagishi category:cs.SD cs.LG  published:2015-06-17 summary:This paper proposes a deep denoising auto-encoder technique to extract better acoustic features for speech synthesis. The technique allows us to automatically extract low-dimensional features from high dimensional spectral features in a non-linear, data-driven, unsupervised way. We compared the new stochastic feature extractor with conventional mel-cepstral analysis in analysis-by-synthesis and text-to-speech experiments. Our results confirm that the proposed method increases the quality of synthetic speech in both experiments. version:1
arxiv-1506-02816 | Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering | http://arxiv.org/abs/1506.02816 | id:1506.02816 author:George Gkotsis, Maria Liakata, Carlos Pedrinaci, John Domingue category:cs.CL cs.IR H.3.1  published:2015-06-09 summary:This paper addresses the problem of determining the best answer in Community-based Question Answering (CQA) websites by focussing on the content. In particular, we present a system, ACQUA [http://acqua.kmi.open.ac.uk], that can be installed onto the majority of browsers as a plugin. The service offers a seamless and accurate prediction of the answer to be accepted. Previous research on this topic relies on the exploitation of community feedback on the answers, which involves rating of either users (e.g., reputation) or answers (e.g. scores manually assigned to answers). We propose a new technique that leverages the content/textual features of answers in a novel way. Our approach delivers better results than related linguistics-based solutions and manages to match rating-based approaches. More specifically, the gain in performance is achieved by rendering the values of these features into a discretised form. We also show how our technique manages to deliver equally good results in real-time settings, as opposed to having to rely on information not always readily available, such as user ratings and answer scores. We ran an evaluation on 21 StackExchange websites covering around 4 million questions and more than 8 million answers. We obtain 84% average precision and 70% recall, which shows that our technique is robust, effective, and widely applicable. version:2
arxiv-1506-05257 | CFORB: Circular FREAK-ORB Visual Odometry | http://arxiv.org/abs/1506.05257 | id:1506.05257 author:Daniel J. Mankowitz, Ehud Rivlin category:cs.CV  published:2015-06-17 summary:We present a novel Visual Odometry algorithm entitled Circular FREAK-ORB (CFORB). This algorithm detects features using the well-known ORB algorithm [12] and computes feature descriptors using the FREAK algorithm [14]. CFORB is invariant to both rotation and scale changes, and is suitable for use in environments with uneven terrain. Two visual geometric constraints have been utilized in order to remove invalid feature descriptor matches. These constraints have not previously been utilized in a Visual Odometry algorithm. A variation to circular matching [16] has also been implemented. This allows features to be matched between images without having to be dependent upon the epipolar constraint. This algorithm has been run on the KITTI benchmark dataset and achieves a competitive average translational error of $3.73 \%$ and average rotational error of $0.0107 deg/m$. CFORB has also been run in an indoor environment and achieved an average translational error of $3.70 \%$. After running CFORB in a highly textured environment with an approximately uniform feature spread across the images, the algorithm achieves an average translational error of $2.4 \%$ and an average rotational error of $0.009 deg/m$. version:1
arxiv-1506-04924 | Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation | http://arxiv.org/abs/1506.04924 | id:1506.04924 author:Seunghoon Hong, Hyeonwoo Noh, Bohyung Han category:cs.CV  published:2015-06-16 summary:We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches even with much less training images with strong annotations in PASCAL VOC dataset. version:2
arxiv-1503-04253 | Novel Super-Resolution Method Based on High Order Nonlocal-Means | http://arxiv.org/abs/1503.04253 | id:1503.04253 author:Kang Yong-Rim, Kim Yong-Jin category:cs.IT cs.CV math.IT  published:2015-03-14 summary:Super-resolution without explicit sub-pixel motion estimation is a very active subject of image reconstruction containing general motion. The Non-Local Means (NLM) method is a simple image reconstruction method without explicit motion estimation. In this paper we generalize NLM method to higher orders using kernel regression can apply to super-resolution reconstruction. The performance of the generalized method is compared with other methods. version:3
arxiv-1506-05230 | Non-distributional Word Vector Representations | http://arxiv.org/abs/1506.05230 | id:1506.05230 author:Manaal Faruqui, Chris Dyer category:cs.CL  published:2015-06-17 summary:Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches. version:1
arxiv-1412-6583 | Discovering Hidden Factors of Variation in Deep Networks | http://arxiv.org/abs/1412.6583 | id:1412.6583 author:Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, Bruno A. Olshausen category:cs.LG cs.CV cs.NE  published:2014-12-20 summary:Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate `hidden' variation in the supervised signal. version:4
arxiv-1502-02367 | Gated Feedback Recurrent Neural Networks | http://arxiv.org/abs/1502.02367 | id:1502.02367 author:Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML  published:2015-02-09 summary:In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions. version:4
arxiv-1506-05215 | Robust Estimation of Structured Covariance Matrix for Heavy-Tailed Elliptical Distributions | http://arxiv.org/abs/1506.05215 | id:1506.05215 author:Ying Sun, Prabhu Babu, Daniel P. Palomar category:stat.AP stat.ML  published:2015-06-17 summary:This paper considers the problem of robustly estimating a structured covariance matrix with an elliptical underlying distribution with known mean. In applications where the covariance matrix naturally possesses a certain structure, taking the prior structure information into account in the estimation procedure is beneficial to improve the estimation accuracy. We propose incorporating the prior structure information into Tyler's M-estimator and formulate the problem as minimizing the cost function of Tyler's estimator under the prior structural constraint. First, the estimation under a general convex structural constraint is introduced with an efficient algorithm for finding the estimator derived based on the majorization minimization (MM) algorithm framework. Then, the algorithm is tailored to several special structures that enjoy a wide range of applications in signal processing related fields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz structure. In addition, two types of non-convex structures, i.e., the Kronecker structure and the spiked covariance structure, are also discussed, where it is shown that simple algorithms can be derived under the guidelines of MM. Numerical results show that the proposed estimator achieves a smaller estimation error than the benchmark estimators at a lower computational cost. version:1
arxiv-1506-05212 | Learning Spike time codes through Morphological Learning with Binary Synapses | http://arxiv.org/abs/1506.05212 | id:1506.05212 author:Subhrajit Roy, Phyo Phyo San, Shaista Hussain, Lee Wang Wei, Arindam Basu category:cs.NE  published:2015-06-17 summary:In this paper, a neuron with nonlinear dendrites (NNLD) and binary synapses that is able to learn temporal features of spike input patterns is considered. Since binary synapses are considered, learning happens through formation and elimination of connections between the inputs and the dendritic branches to modify the structure or "morphology" of the NNLD. A morphological learning algorithm inspired by the 'Tempotron', i.e., a recently proposed temporal learning algorithm-is presented in this work. Unlike 'Tempotron', the proposed learning rule uses a technique to automatically adapt the NNLD threshold during training. Experimental results indicate that our NNLD with 1-bit synapses can obtain similar accuracy as a traditional Tempotron with 4-bit synapses in classifying single spike random latency and pair-wise synchrony patterns. Hence, the proposed method is better suited for robust hardware implementation in the presence of statistical variations. We also present results of applying this rule to real life spike classification problems from the field of tactile sensing. version:1
arxiv-1506-05196 | A Discriminative Representation of Convolutional Features for Indoor Scene Recognition | http://arxiv.org/abs/1506.05196 | id:1506.05196 author:Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel category:cs.CV  published:2015-06-17 summary:Indoor scene recognition is a multi-faceted and challenging problem due to the diverse intra-class variations and the confusing inter-class similarities. This paper presents a novel approach which exploits rich mid-level convolutional features to categorize indoor scenes. Traditionally used convolutional features preserve the global spatial structure, which is a desirable property for general object recognition. However, we argue that this structuredness is not much helpful when we have large variations in scene layouts, e.g., in indoor scenes. We propose to transform the structured convolutional activations to another highly discriminative feature space. The representation in the transformed space not only incorporates the discriminative aspects of the target dataset, but it also encodes the features in terms of the general object categories that are present in indoor scenes. To this end, we introduce a new large-scale dataset of 1300 object categories which are commonly present in indoor scenes. Our proposed approach achieves a significant performance boost over previous state of the art approaches on five major scene classification datasets. version:1
arxiv-1506-05187 | Robust High Quality Image Guided Depth Upsampling | http://arxiv.org/abs/1506.05187 | id:1506.05187 author:Wei Liu, Yijun Li, Xiaogang Chen, Jie Yang, Qiang Wu, Jingyi Yu category:cs.CV  published:2015-06-17 summary:Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at a high frame rate. However, its low resolution and sensitivity to the noise are always a concern. A popular solution is upsampling the obtained noisy low resolution depth map with the guidance of the companion high resolution color image. However, due to the constrains in the existing upsampling models, the high resolution depth map obtained in such way may suffer from either texture copy artifacts or blur of depth discontinuity. In this paper, a novel optimization framework is proposed with the brand new data term and smoothness term. The comprehensive experiments using both synthetic data and real data show that the proposed method well tackles the problem of texture copy artifacts and blur of depth discontinuity. It also demonstrates sufficient robustness to the noise. Moreover, a data driven scheme is proposed to adaptively estimate the parameter in the upsampling optimization framework. The encouraging performance is maintained even in the case of large upsampling e.g. $8\times$ and $16\times$. version:1
arxiv-1506-03850 | Generalized Additive Model Selection | http://arxiv.org/abs/1506.03850 | id:1506.03850 author:Alexandra Chouldechova, Trevor Hastie category:stat.ML  published:2015-06-11 summary:We introduce GAMSEL (Generalized Additive Model Selection), a penalized likelihood approach for fitting sparse generalized additive models in high dimension. Our method interpolates between null, linear and additive models by allowing the effect of each variable to be estimated as being either zero, linear, or a low-complexity curve, as determined by the data. We present a blockwise coordinate descent procedure for efficiently optimizing the penalized likelihood objective over a dense grid of the tuning parameter, producing a regularization path of additive models. We demonstrate the performance of our method on both real and simulated data examples, and compare it with existing techniques for additive model selection. version:2
arxiv-1506-02739 | Connotation Frames: Typed Relations of Implied Sentiment in Predicate-Argument Structure | http://arxiv.org/abs/1506.02739 | id:1506.02739 author:Hannah Rashkin, Sameer Singh, Yejin Choi category:cs.CL  published:2015-06-09 summary:Through a choice of a predicate (e.g., "violate"), a writer can convey subtle sentiments and value judgements toward the arguments of a verb (e.g., projecting the agent as an "antagonist" and the theme as a "victim"). We introduce connotation frames to encode the rich dimensions of implied sentiment, value judgements, and effect evaluation as typed relations that these choices influence, and propose a factor graph formulation that captures the inter-play among different types of connotative relations at the lexicon-level. Experimental results confirm that our model is effective in predicting connotative sentiments compared to strong baselines and existing sentiment lexicons. version:2
arxiv-1404-4408 | Geometric Inference for General High-Dimensional Linear Inverse Problems | http://arxiv.org/abs/1404.4408 | id:1404.4408 author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH  published:2014-04-17 summary:This paper presents a unified geometric framework for the statistical analysis of a general ill-posed linear inverse model which includes as special cases noisy compressed sensing, sign vector recovery, trace regression, orthogonal matrix estimation, and noisy matrix completion. We propose computationally feasible convex programs for statistical inference including estimation, confidence intervals and hypothesis testing. A theoretical framework is developed to characterize the local estimation rate of convergence and to provide statistical inference guarantees. Our results are built based on the local conic geometry and duality. The difficulty of statistical inference is captured by the geometric characterization of the local tangent cone through the Gaussian width and Sudakov minoration estimate. version:3
arxiv-1506-05163 | Deep Convolutional Networks on Graph-Structured Data | http://arxiv.org/abs/1506.05163 | id:1506.05163 author:Mikael Henaff, Joan Bruna, Yann LeCun category:cs.LG cs.CV cs.NE  published:2015-06-16 summary:Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate. version:1
arxiv-1407-7390 | A discussion on the validation tests employed to compare human action recognition methods using the MSR Action3D dataset | http://arxiv.org/abs/1407.7390 | id:1407.7390 author:José Ramón Padilla-López, Alexandros André Chaaraoui, Francisco Flórez-Revuelta category:cs.CV  published:2014-07-28 summary:This paper aims to determine which is the best human action recognition method based on features extracted from RGB-D devices, such as the Microsoft Kinect. A review of all the papers that make reference to MSR Action3D, the most used dataset that includes depth information acquired from a RGB-D device, has been performed. We found that the validation method used by each work differs from the others. So, a direct comparison among works cannot be made. However, almost all the works present their results comparing them without taking into account this issue. Therefore, we present different rankings according to the methodology used for the validation in orden to clarify the existing confusion. version:3
arxiv-1506-04304 | Combinatorial Energy Learning for Image Segmentation | http://arxiv.org/abs/1506.04304 | id:1506.04304 author:Jeremy Maitin-Shepard, Viren Jain, Michal Januszewski, Peter Li, Jörgen Kornfeld, Julia Buhmann, Pieter Abbeel category:cs.CV  published:2015-06-13 summary:We introduce a new machine learning approach for image segmentation, based on a joint energy model over image features and novel local binary shape descriptors. These descriptors compactly represent rich shape information at multiple scales, including interactions between multiple objects. Our approach, which does not rely on any hand-designed features, reflects the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the joint energy, and for local optimization of this energy in the space of supervoxel agglomerations. We demonstrate the effectiveness of our approach on 3-D biological data, where rich shape information appears to be critical for resolving ambiguities. On two challenging 3-D electron microscopy datasets highly relevant to ongoing efforts towards large-scale dense mapping of neural circuits, we achieve state-of-the-art segmentation accuracy. version:2
arxiv-1505-01504 | A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models | http://arxiv.org/abs/1505.01504 | id:1505.01504 author:Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai category:cs.NE cs.CL cs.LG  published:2015-05-06 summary:In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs. version:2
arxiv-1506-05055 | Numeric Input Relations for Relational Learning with Applications to Community Structure Analysis | http://arxiv.org/abs/1506.05055 | id:1506.05055 author:Jiuchuan Jiang, Manfred Jaeger category:cs.LG  published:2015-06-16 summary:Most work in the area of statistical relational learning (SRL) is focussed on discrete data, even though a few approaches for hybrid SRL models have been proposed that combine numerical and discrete variables. In this paper we distinguish numerical random variables for which a probability distribution is defined by the model from numerical input variables that are only used for conditioning the distribution of discrete response variables. We show how numerical input relations can very easily be used in the Relational Bayesian Network framework, and that existing inference and learning methods need only minor adjustments to be applied in this generalized setting. The resulting framework provides natural relational extensions of classical probabilistic models for categorical data. We demonstrate the usefulness of RBN models with numeric input relations by several examples. In particular, we use the augmented RBN framework to define probabilistic models for multi-relational (social) networks in which the probability of a link between two nodes depends on numeric latent feature vectors associated with the nodes. A generic learning procedure can be used to obtain a maximum-likelihood fit of model parameters and latent feature values for a variety of models that can be expressed in the high-level RBN representation. Specifically, we propose a model that allows us to interpret learned latent feature values as community centrality degrees by which we can identify nodes that are central for one community, that are hubs between communities, or that are isolated nodes. In a multi-relational setting, the model also provides a characterization of how different relations are associated with each community. version:1
arxiv-1506-05036 | Depth Perception in Autostereograms: 1/f-Noise is Best | http://arxiv.org/abs/1506.05036 | id:1506.05036 author:Yael Yankelevsky, Ishai Shvartz, Tamar Avraham, Alfred M. Bruckstein category:cs.CV  published:2015-06-16 summary:An autostereogram is a single image that encodes depth information that pops out when looking at it. The trick is achieved by replicating a vertical strip that sets a basic two-dimensional pattern with disparity shifts that encode a three-dimensional scene. It is of interest to explore the dependency between the ease of perceiving depth in autostereograms and the choice of the basic pattern used for generating them. In this work we confirm a theory proposed by Bruckstein et al. to explain the process of autostereographic depth perception, providing a measure for the ease of "locking into" the depth profile, based on the spectral properties of the basic pattern used. We report the results of three sets of psychophysical experiments using autostereograms generated from two-dimensional random noise patterns having power spectra of the form $1/f^\beta$. The experiments were designed to test the ability of human subjects to identify smooth, low resolution surfaces, as well as detail, in the form of higher resolution objects in the depth profile, and to determine limits in identifying small objects as a function of their size. In accordance with the theory, we discover a significant advantage of the $1/f$ noise pattern (pink noise) for fast depth lock-in and fine detail detection, showing that such patterns are optimal choices for autostereogram design. Validating the theoretical model predictions strengthens its underlying assumptions, and contributes to a better understanding of the visual system's binocular disparity mechanisms. version:1
arxiv-1506-05012 | Emotion Analysis of Songs Based on Lyrical and Audio Features | http://arxiv.org/abs/1506.05012 | id:1506.05012 author:Adit Jamdar, Jessica Abraham, Karishma Khanna, Rahul Dubey category:cs.CL cs.AI cs.SD  published:2015-06-16 summary:In this paper, a method is proposed to detect the emotion of a song based on its lyrical and audio features. Lyrical features are generated by segmentation of lyrics during the process of data extraction. ANEW and WordNet knowledge is then incorporated to compute Valence and Arousal values. In addition to this, linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed. Audio features are used to supplement the lyrical ones and include attributes like energy, tempo, and danceability. These features are extracted from The Echo Nest, a widely used music intelligence platform. Construction of training and test sets is done on the basis of social tags extracted from the last.fm website. The classification is done by applying feature weighting and stepwise threshold reduction on the k-Nearest Neighbors algorithm to provide fuzziness in the classification. version:1
arxiv-1506-05001 | Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection | http://arxiv.org/abs/1506.05001 | id:1506.05001 author:Liliana Lo Presti, Marco La Cascia category:cs.CV cs.AI cs.RO  published:2015-06-16 summary:This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted. version:1
arxiv-1506-04940 | Recognize Foreign Low-Frequency Words with Similar Pairs | http://arxiv.org/abs/1506.04940 | id:1506.04940 author:Xi Ma, Xiaoxi Wang, Dong Wang, Zhiyong Zhang category:cs.CL  published:2015-06-16 summary:Low-frequency words place a major challenge for automatic speech recognition (ASR). The probabilities of these words, which are often important name entities, are generally under-estimated by the language model (LM) due to their limited occurrences in the training data. Recently, we proposed a word-pair approach to deal with the problem, which borrows information of frequent words to enhance the probabilities of low-frequency words. This paper presents an extension to the word-pair method by involving multiple `predicting words' to produce better estimation for low-frequency words. We also employ this approach to deal with out-of-language words in the task of multi-lingual speech recognition. version:1
arxiv-1506-04935 | Post-Reconstruction Deconvolution of PET Images by Total Generalized Variation Regularization | http://arxiv.org/abs/1506.04935 | id:1506.04935 author:Stéphanie Guérit, Laurent Jacques, Benoît Macq, John A. Lee category:cs.CV math.OC  published:2015-06-16 summary:Improving the quality of positron emission tomography (PET) images, affected by low resolution and high level of noise, is a challenging task in nuclear medicine and radiotherapy. This work proposes a restoration method, achieved after tomographic reconstruction of the images and targeting clinical situations where raw data are often not accessible. Based on inverse problem methods, our contribution introduces the recently developed total generalized variation (TGV) norm to regularize PET image deconvolution. Moreover, we stabilize this procedure with additional image constraints such as positivity and photometry invariance. A criterion for updating and adjusting automatically the regularization parameter in case of Poisson noise is also presented. Experiments are conducted on both synthetic data and real patient images. version:1
arxiv-1506-04912 | Subsampled terahertz data reconstruction based on spatio-temporal dictionary learning | http://arxiv.org/abs/1506.04912 | id:1506.04912 author:Vahid Abolghasemi, Hao Shen, Yaochun Shen, Lu Gan category:cs.CV  published:2015-06-16 summary:In this paper, the problem of terahertz pulsed imaging and reconstruction is addressed. It is assumed that an incomplete (subsampled) three dimensional THz data set has been acquired and the aim is to recover all missing samples. A sparsity-inducing approach is proposed for this purpose. First, a simple interpolation is applied to incomplete noisy data. Then, we propose a spatio-temporal dictionary learning method to obtain an appropriate sparse representation of data based on a joint sparse recovery algorithm. Then, using the sparse coefficients and the learned dictionary, the 3D data is effectively denoised by minimizing a simple cost function. We consider two types of terahertz data to evaluate the performance of the proposed approach; THz data acquired for a model sample with clear layered structures (e.g., a T-shape plastic sheet buried in a polythene pellet), and pharmaceutical tablet data (with low spatial resolution). The achieved signal-to-noise-ratio for reconstruction of T-shape data, from only 5% observation was 19 dB. Moreover, the accuracies of obtained thickness and depth measurements for pharmaceutical tablet data after reconstruction from 10% observation were 98.8%, and 99.9%, respectively. These results, along with chemical mapping analysis, presented at the end of this paper, confirm the accuracy of the proposed method. version:1
arxiv-1506-04897 | Parsing Natural Language Sentences by Semi-supervised Methods | http://arxiv.org/abs/1506.04897 | id:1506.04897 author:Rudolf Rosa category:cs.CL I.2.7  published:2015-06-16 summary:We present our work on semi-supervised parsing of natural language sentences, focusing on multi-source crosslingual transfer of delexicalized dependency parsers. We first evaluate the influence of treebank annotation styles on parsing performance, focusing on adposition attachment style. Then, we present KLcpos3, an empirical language similarity measure, designed and tuned for source parser weighting in multi-source delexicalized parser transfer. And finally, we introduce a novel resource combination method, based on interpolation of trained parser models. version:1
arxiv-1506-04891 | Author Identification using Multi-headed Recurrent Neural Networks | http://arxiv.org/abs/1506.04891 | id:1506.04891 author:Douglas Bagnall category:cs.CL cs.LG cs.NE 68T50  published:2015-06-16 summary:Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN 2015 Author Identification task. This paper describes a novel approach where the output layer of a character-level RNN language model is split into several independent predictive sub-models, each representing an author, while the recurrent layer is shared by all. This allows the recurrent layer to model the language as a whole without over-fitting, while the outputs select aspects of the underlying model that reflect their author's style. The method proves competitive, ranking first in two of the four languages. version:1
arxiv-1503-01138 | Learning Super-Resolution Jointly from External and Internal Examples | http://arxiv.org/abs/1503.01138 | id:1503.01138 author:Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Jianchao Yang, Thomas S. Huang category:cs.CV  published:2015-03-03 summary:Single image super-resolution (SR) aims to estimate a high-resolution (HR) image from a lowresolution (LR) input. Image priors are commonly learned to regularize the otherwise seriously ill-posed SR problem, either using external LR-HR pairs or internal similar patterns. We propose joint SR to adaptively combine the advantages of both external and internal SR methods. We define two loss functions using sparse coding based external examples, and epitomic matching based on internal examples, as well as a corresponding adaptive weight to automatically balance their contributions according to their reconstruction errors. Extensive SR results demonstrate the effectiveness of the proposed method over the existing state-of-the-art methods, and is also verified by our subjective evaluation study. version:3
arxiv-1506-04843 | Detection and Estimation of Iris Centre | http://arxiv.org/abs/1506.04843 | id:1506.04843 author:Anirban Dasgupta, Aurobinda Routray, Sai Nataraj Mallavollu category:cs.CV  published:2015-06-16 summary:Detection of iris center is an active area of research in the field of computer vision and human-machine interaction systems. The major issues involved in the detection of iris involves glint on the corneal region, occlusion of the iris by eye-lids, occlusions due to eye gaze, high speed of processing etc. This paper presents an algorithm for detecting and estimating the iris center thereby addressing some of these issues. version:1
arxiv-1506-04838 | Spectral Sparsification and Regret Minimization Beyond Matrix Multiplicative Updates | http://arxiv.org/abs/1506.04838 | id:1506.04838 author:Zeyuan Allen-Zhu, Zhenyu Liao, Lorenzo Orecchia category:cs.LG cs.DS math.OC stat.ML  published:2015-06-16 summary:In this paper, we provide a novel construction of the linear-sized spectral sparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous constructions required $\Omega(n^4)$ running time [BSS14, Zou12], our sparsification routine can be implemented in almost-quadratic running time $O(n^{2+\varepsilon})$. The fundamental conceptual novelty of our work is the leveraging of a strong connection between sparsification and a regret minimization problem over density matrices. This connection was known to provide an interpretation of the randomized sparsifiers of Spielman and Srivastava [SS11] via the application of matrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we explain how matrix MWU naturally arises as an instance of the Follow-the-Regularized-Leader framework and generalize this approach to yield a larger class of updates. This new class allows us to accelerate the construction of linear-sized spectral sparsifiers, and give novel insights on the motivation behind Batson, Spielman and Srivastava [BSS14]. version:1
arxiv-1312-1666 | Semi-Stochastic Gradient Descent Methods | http://arxiv.org/abs/1312.1666 | id:1312.1666 author:Jakub Konečný, Peter Richtárik category:stat.ML cs.LG cs.NA math.NA math.OC  published:2013-12-05 summary:In this paper we study the problem of minimizing the average of a large number ($n$) of smooth convex loss functions. We propose a new method, S2GD (Semi-Stochastic Gradient Descent), which runs for one or several epochs in each of which a single full gradient and a random number of stochastic gradients is computed, following a geometric law. The total work needed for the method to output an $\varepsilon$-accurate solution in expectation, measured in the number of passes over data, or equivalently, in units equivalent to the computation of a single gradient of the loss, is $O((\kappa/n)\log(1/\varepsilon))$, where $\kappa$ is the condition number. This is achieved by running the method for $O(\log(1/\varepsilon))$ epochs, with a single gradient evaluation and $O(\kappa)$ stochastic gradient evaluations in each. The SVRG method of Johnson and Zhang arises as a special case. If our method is limited to a single epoch only, it needs to evaluate at most $O((\kappa/\varepsilon)\log(1/\varepsilon))$ stochastic gradients. In contrast, SVRG requires $O(\kappa/\varepsilon^2)$ stochastic gradients. To illustrate our theoretical results, S2GD only needs the workload equivalent to about 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for a problem with $n=10^9$ and $\kappa=10^3$. version:2
arxiv-1506-04803 | Exploiting Text and Network Context for Geolocation of Social Media Users | http://arxiv.org/abs/1506.04803 | id:1506.04803 author:Afshin Rahimi, Duy Vu, Trevor Cohn, Timothy Baldwin category:cs.CL cs.SI  published:2015-06-16 summary:Research on automatically geolocating social media users has conventionally been based on the text content of posts from a given user or the social network of the user, with very little crossover between the two, and no bench-marking of the two approaches over compara- ble datasets. We bring the two threads of research together in first proposing a text-based method based on adaptive grids, followed by a hybrid network- and text-based method. Evaluating over three Twitter datasets, we show that the empirical difference between text- and network-based methods is not great, and that hybridisation of the two is superior to the component methods, especially in contexts where the user graph is not well connected. We achieve state-of-the-art results on all three datasets. version:1
arxiv-1405-1380 | Is Joint Training Better for Deep Auto-Encoders? | http://arxiv.org/abs/1405.1380 | id:1405.1380 author:Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju category:stat.ML cs.LG cs.NE  published:2014-05-06 summary:Traditionally, when generative models of data are developed via deep architectures, greedy layer-wise pre-training is employed. In a well-trained model, the lower layer of the architecture models the data distribution conditional upon the hidden variables, while the higher layers model the hidden distribution prior. But due to the greedy scheme of the layerwise training technique, the parameters of lower layers are fixed when training higher layers. This makes it extremely challenging for the model to learn the hidden distribution prior, which in turn leads to a suboptimal model for the data distribution. We therefore investigate joint training of deep autoencoders, where the architecture is viewed as one stack of two or more single-layer autoencoders. A single global reconstruction objective is jointly optimized, such that the objective for the single autoencoders at each layer acts as a local, layer-level regularizer. We empirically evaluate the performance of this joint training scheme and observe that it not only learns a better data model, but also learns better higher layer representations, which highlights its potential for unsupervised feature learning. In addition, we find that the usage of regularizations in the joint training scheme is crucial in achieving good performance. In the supervised setting, joint training also shows superior performance when training deeper models. The joint training framework can thus provide a platform for investigating more efficient usage of different types of regularizers, especially in light of the growing volumes of available unlabeled data. version:4
arxiv-1502-00731 | Incremental Knowledge Base Construction Using DeepDive | http://arxiv.org/abs/1502.00731 | id:1502.00731 author:Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, Christopher Ré category:cs.DB cs.CL cs.LG  published:2015-02-03 summary:Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality. version:4
arxiv-1506-04776 | Encog: Library of Interchangeable Machine Learning Models for Java and C# | http://arxiv.org/abs/1506.04776 | id:1506.04776 author:Jeff Heaton category:cs.MS cs.LG 68T01 I.2  published:2015-06-15 summary:This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was 1st released in 2008. Encog allows a variety of machine learning models to be applied to datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from http://www.encog.org. version:1
arxiv-1506-04757 | Image-based Recommendations on Styles and Substitutes | http://arxiv.org/abs/1506.04757 | id:1506.04757 author:Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel category:cs.CV cs.IR  published:2015-06-15 summary:Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications. version:1
arxiv-1506-04744 | Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game | http://arxiv.org/abs/1506.04744 | id:1506.04744 author:Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML  published:2015-06-15 summary:Interpersonal relations are fickle, with close friendships often dissolving into enmity. In this work, we explore linguistic cues that presage such transitions by studying dyadic interactions in an online strategy game where players form alliances and break those alliances through betrayal. We characterize friendships that are unlikely to last and examine temporal patterns that foretell betrayal. We reveal that subtle signs of imminent betrayal are encoded in the conversational patterns of the dyad, even if the victim is not aware of the relationship's fate. In particular, we find that lasting friendships exhibit a form of balance that manifests itself through language. In contrast, sudden changes in the balance of certain conversational attributes---such as positive sentiment, politeness, or focus on future planning---signal impending betrayal. version:1
arxiv-1506-04725 | Fast Two-Sample Testing with Analytic Representations of Probability Measures | http://arxiv.org/abs/1506.04725 | id:1506.04725 author:Kacper Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, Arthur Gretton category:stat.ML 62G10 G.3  published:2015-06-15 summary:We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics. version:1
arxiv-1506-04721 | Automatic Layer Separation using Light Field Imaging | http://arxiv.org/abs/1506.04721 | id:1506.04721 author:Qiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, Jingyi Yu category:cs.CV  published:2015-06-15 summary:We propose a novel approach that jointly removes reflection or translucent layer from a scene and estimates scene depth. The input data are captured via light field imaging. The problem is couched as minimizing the rank of the transmitted scene layer via Robust Principle Component Analysis (RPCA). We also impose regularization based on piecewise smoothness, gradient sparsity, and layer independence to simultaneously recover 3D geometry of the transmitted layer. Experimental results on synthetic and real data show that our technique is robust and reliable, and can handle a broad range of layer separation problems. version:1
arxiv-1506-04720 | Latent Regression Bayesian Network for Data Representation | http://arxiv.org/abs/1506.04720 | id:1506.04720 author:Siqi Nie, Qiang Ji category:cs.LG  published:2015-06-15 summary:Deep directed generative models have attracted much attention recently due to their expressive representation power and the ability of ancestral sampling. One major difficulty of learning directed models with many latent variables is the intractable inference. To address this problem, most existing algorithms make assumptions to render the latent variables independent of each other, either by designing specific priors, or by approximating the true posterior using a factorized distribution. We believe the correlations among latent variables are crucial for faithful data representation. Driven by this idea, we propose an inference method based on the conditional pseudo-likelihood that preserves the dependencies among the latent variables. For learning, we propose to employ the hard Expectation Maximization (EM) algorithm, which avoids the intractability of the traditional EM by max-out instead of sum-out to compute the data likelihood. Qualitative and quantitative evaluations of our model against state of the art deep models on benchmark datasets demonstrate the effectiveness of the proposed algorithm in data representation and reconstruction. version:1
arxiv-1506-04655 | Leveraging the Power of Gabor Phase for Face Identification: A Block Matching Approach | http://arxiv.org/abs/1506.04655 | id:1506.04655 author:Yang Zhong, Haibo Li category:cs.CV  published:2015-06-15 summary:Different from face verification, face identification is much more demanding. To reach comparable performance, an identifier needs to be roughly N times better than a verifier. To expect a breakthrough in face identification, we need a fresh look at the fundamental building blocks of face recognition. In this paper we focus on the selection of a suitable signal representation and better matching strategy for face identification. We demonstrate how Gabor phase could be leveraged to improve the performance of face identification by using the Block Matching method. Compared to the existing approaches, the proposed method features much lower algorithmic complexity: face images are only filtered by a single-scale Gabor filter pair and the matching is performed between any pairs of face images at hand without involving any training process. Benchmark evaluations show that the proposed approach is totally comparable to and even better than state-of-the-art algorithms, which are typically based on more features extracted from a large set of Gabor faces and/or rely on heavy training processes. version:1
arxiv-1412-8060 | Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity | http://arxiv.org/abs/1412.8060 | id:1412.8060 author:Zheng Qu, Peter Richtárik category:math.OC cs.LG cs.NA math.NA  published:2014-12-27 summary:We study the problem of minimizing the sum of a smooth convex function and a convex block-separable regularizer and propose a new randomized coordinate descent method, which we call ALPHA. Our method at every iteration updates a random subset of coordinates, following an arbitrary distribution. No coordinate descent methods capable to handle an arbitrary sampling have been studied in the literature before for this problem. ALPHA is a remarkably flexible algorithm: in special cases, it reduces to deterministic and randomized methods such as gradient descent, coordinate descent, parallel coordinate descent and distributed coordinate descent -- both in nonaccelerated and accelerated variants. The variants with arbitrary (or importance) sampling are new. We provide a complexity analysis of ALPHA, from which we deduce as a direct corollary complexity bounds for its many variants, all matching or improving best known bounds. version:2
arxiv-1502-06134 | Learning with Square Loss: Localization through Offset Rademacher Complexity | http://arxiv.org/abs/1502.06134 | id:1502.06134 author:Tengyuan Liang, Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG math.ST stat.TH  published:2015-02-21 summary:We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of \citep{RakSriTsy15} for the bounded case while also providing guarantees without the boundedness assumption. version:3
arxiv-1501-07242 | Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions | http://arxiv.org/abs/1501.07242 | id:1501.07242 author:Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin category:cs.NA cs.LG math.OC  published:2015-01-28 summary:We consider the problem of optimizing an approximately convex function over a bounded convex set in $\mathbb{R}^n$ using only function evaluations. The problem is reduced to sampling from an \emph{approximately} log-concave distribution using the Hit-and-Run method, which is shown to have the same $\mathcal{O}^*$ complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an $\epsilon$-minimizer after $\mathcal{O}^*(n^{7.5}\epsilon^{-2})$ noisy function evaluations by inducing a $\mathcal{O}(\epsilon/n)$-approximately log concave distribution. We also consider in detail the case when the "amount of non-convexity" decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning. version:2
arxiv-1409-4127 | Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network | http://arxiv.org/abs/1409.4127 | id:1409.4127 author:Yu-Chuan Su, Tzu-Hsuan Chiu, Chun-Yen Yeh, Hsin-Fu Huang, Winston H. Hsu category:cs.CV cs.LG  published:2014-09-15 summary:Unconstrained video recognition and Deep Convolution Network (DCN) are two active topics in computer vision recently. In this work, we apply DCNs as frame-based recognizers for video recognition. Our preliminary studies, however, show that video corpora with complete ground truth are usually not large and diverse enough to learn a robust model. The networks trained directly on the video data set suffer from significant overfitting and have poor recognition rate on the test set. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from images to videos to utilize the knowledge in the weakly labeled image corpus for video recognition. The image corpus help to learn important visual patterns for natural images, while these patterns are ignored by models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. We show that by means of transfer learning from image to video, we can learn a frame-based recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image data sets required by previous works. The same approach may be applied to other visual recognition tasks where only scarce training data is available, and it improves the applicability of DCNs in various computer vision problems. Our experiments also reveal the correlation between meta-parameters and the performance of DCNs, given the properties of the target problem and data. These results lead to a heuristic for meta-parameter selection for future researches, which does not rely on the time consuming meta-parameter search. version:2
arxiv-1506-04608 | Flow Segmentation in Dense Crowds | http://arxiv.org/abs/1506.04608 | id:1506.04608 author:Javairia Nazir, Mehreen Sirshar category:cs.CV  published:2015-06-15 summary:A framework is proposed in this paper that is used to segment flow of dense crowds. The flow field that is generated by the movement in the crowd is treated just like an aperiodic dynamic system. On this flow field a grid of particles is put over for particle advection by the use of a numerical integration scheme. Then flow maps are generated which associates the initial position of the particles with final position. The gradient of the flow maps gives the amount of divergence of the neighboring particles. For forward integration and analysis forward Finite time Lyapunov Exponent is calculated and backward Finite time Lyapunov Exponent is also calculated it gives the Lagrangian coherent structures of the flow in crowd. Lagrangian Coherent Structures basically divides the flow in crowd into regions and these regions have different dynamics. These regions are then used to get the boundary in the different flow segments by using water shed algorithm. The experiment is conducted on the crowd dataset of UCF (University of central Florida). version:1
arxiv-1405-0042 | Learning with incremental iterative regularization | http://arxiv.org/abs/1405.0042 | id:1405.0042 author:Lorenzo Rosasco, Silvia Villa category:stat.ML cs.LG math.OC math.PR  published:2014-04-30 summary:Within a statistical learning setting, we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method. In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and prove strong universal consistency, i.e. almost sure convergence of the risk, as well as sharp finite sample bounds for the iterates. Our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results. version:2
arxiv-1506-04584 | Re-scale AdaBoost for Attack Detection in Collaborative Filtering Recommender Systems | http://arxiv.org/abs/1506.04584 | id:1506.04584 author:Zhihai Yang, Lin Xu, Zhongmin Cai category:cs.IR cs.CR cs.LG  published:2015-06-15 summary:Collaborative filtering recommender systems (CFRSs) are the key components of successful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks since its openness. However, since attack size is far smaller than that of genuine users, conventional supervised learning based detection methods could be too "dull" to handle such imbalanced classification. In this paper, we improve detection performance from following two aspects. First, we extract well-designed features from user profiles based on the statistical properties of the diverse attack models, making hard classification task becomes easier to perform. Then, refer to the general idea of re-scale Boosting (RBoosting) and AdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost (RAdaBoost) as our detection method based on extracted features. RAdaBoost is comparable to the optimal Boosting-type algorithm and can effectively improve the performance in some hard scenarios. Finally, a series of experiments on the MovieLens-100K data set are conducted to demonstrate the outperformance of RAdaBoost comparing with some classical techniques such as SVM, kNN and AdaBoost. version:1
arxiv-1412-6581 | Variational Recurrent Auto-Encoders | http://arxiv.org/abs/1412.6581 | id:1412.6581 author:Otto Fabius, Joost R. van Amersfoort category:stat.ML cs.LG cs.NE  published:2014-12-20 summary:In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state. version:6
arxiv-1506-04566 | Optimising Spatial and Tonal Data for PDE-based Inpainting | http://arxiv.org/abs/1506.04566 | id:1506.04566 author:Laurent Hoeltgen, Markus Mainberger, Sebastian Hoffmann, Joachim Weickert, Ching Hoo Tang, Simon Setzer, Daniel Johannsen, Frank Neumann, Benjamin Doerr category:cs.CV math.OC 94A08  65Nxx  65Kxx  published:2015-06-15 summary:Some recent methods for lossy signal and image compression store only a few selected pixels and fill in the missing structures by inpainting with a partial differential equation (PDE). Suitable operators include the Laplacian, the biharmonic operator, and edge-enhancing anisotropic diffusion (EED). The quality of such approaches depends substantially on the selection of the data that is kept. Optimising this data in the domain and codomain gives rise to challenging mathematical problems that shall be addressed in our work. In the 1D case, we prove results that provide insights into the difficulty of this problem, and we give evidence that a splitting into spatial and tonal (i.e. function value) optimisation does hardly deteriorate the results. In the 2D setting, we present generic algorithms that achieve a high reconstruction quality even if the specified data is very sparse. To optimise the spatial data, we use a probabilistic sparsification, followed by a nonlocal pixel exchange that avoids getting trapped in bad local optima. After this spatial optimisation we perform a tonal optimisation that modifies the function values in order to reduce the global reconstruction error. For homogeneous diffusion inpainting, this comes down to a least squares problem for which we prove that it has a unique solution. We demonstrate that it can be found efficiently with a gradient descent approach that is accelerated with fast explicit diffusion (FED) cycles. Our framework allows to specify the desired density of the inpainting mask a priori. Moreover, is more generic than other data optimisation approaches for the sparse inpainting problem, since it can also be extended to nonlinear inpainting operators such as EED. This is exploited to achieve reconstructions with state-of-the-art quality. We also give an extensive literature survey on PDE-based image compression methods. version:1
arxiv-1506-05101 | Big Data Analytics in Bioinformatics: A Machine Learning Perspective | http://arxiv.org/abs/1506.05101 | id:1506.05101 author:Hirak Kashyap, Hasin Afzal Ahmed, Nazrul Hoque, Swarup Roy, Dhruba Kumar Bhattacharyya category:cs.CE cs.LG  published:2015-06-15 summary:Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities. version:1
arxiv-1408-1336 | On the Generalization of the C-Bound to Structured Output Ensemble Methods | http://arxiv.org/abs/1408.1336 | id:1408.1336 author:François Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy category:stat.ML  published:2014-08-06 summary:This paper generalizes an important result from the PAC-Bayesian literature for binary classification to the case of ensemble methods for structured outputs. We prove a generic version of the \Cbound, an upper bound over the risk of models expressed as a weighted majority vote that is based on the first and second statistical moments of the vote's margin. This bound may advantageously $(i)$ be applied on more complex outputs such as multiclass labels and multilabel, and $(ii)$ allow to consider margin relaxations. These results open the way to develop new ensemble methods for structured output prediction with PAC-Bayesian guarantees. version:2
arxiv-1506-04513 | Convex Risk Minimization and Conditional Probability Estimation | http://arxiv.org/abs/1506.04513 | id:1506.04513 author:Matus Telgarsky, Miroslav Dudík, Robert Schapire category:cs.LG stat.ML  published:2015-06-15 summary:This paper proves, in very general settings, that convex risk minimization is a procedure to select a unique conditional probability model determined by the classification problem. Unlike most previous work, we give results that are general enough to include cases in which no minimum exists, as occurs typically, for instance, with standard boosting algorithms. Concretely, we first show that any sequence of predictors minimizing convex risk over the source distribution will converge to this unique model when the class of predictors is linear (but potentially of infinite dimension). Secondly, we show the same result holds for \emph{empirical} risk minimization whenever this class of predictors is finite dimensional, where the essential technical contribution is a norm-free generalization bound. version:1
arxiv-1506-02914 | Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy | http://arxiv.org/abs/1506.02914 | id:1506.02914 author:Marylou Gabrié, Eric W. Tramel, Florent Krzakala category:cond-mat.dis-nn cs.LG cs.NE stat.ML  published:2015-06-09 summary:Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units. version:2
arxiv-1506-04091 | On the properties of variational approximations of Gibbs posteriors | http://arxiv.org/abs/1506.04091 | id:1506.04091 author:Pierre Alquier, James Ridgway, Nicolas Chopin category:stat.ML math.ST stat.TH  published:2015-06-12 summary:The PAC-Bayesian approach is a powerful set of techniques to derive non- asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately intractable. One may sample from it using Markov chain Monte Carlo, but this is often too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. We specialise our results to several learning tasks (classification, ranking, matrix completion),discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets. version:2
arxiv-1506-04488 | Distilling Word Embeddings: An Encoding Approach | http://arxiv.org/abs/1506.04488 | id:1506.04488 author:Lili Mou, Ge Li, Yan Xu, Lu Zhang, Zhi Jin category:cs.CL cs.LG  published:2015-06-15 summary:Distilling knowledge from a well-trained cumbersome network to a small one has become a new research topic recently, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from high-dimensional embeddings, which can retain high performance and reduce model complexity to a large extent. Experimental results show our method is better than directly training neural networks with small embeddings. version:1
arxiv-1506-04477 | Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy | http://arxiv.org/abs/1506.04477 | id:1506.04477 author:Sang-Woo Lee, Min-Oh Heo, Jiwon Kim, Jeonghee Kim, Byoung-Tak Zhang category:cs.LG  published:2015-06-15 summary:The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner. version:1
arxiv-1412-6610 | Scoring and Classifying with Gated Auto-encoders | http://arxiv.org/abs/1412.6610 | id:1412.6610 author:Daniel Jiwoong Im, Graham W. Taylor category:cs.LG cs.NE  published:2014-12-20 summary:Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modelling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification. version:5
arxiv-1506-04449 | Compressing Convolutional Neural Networks | http://arxiv.org/abs/1506.04449 | id:1506.04449 author:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen category:cs.LG cs.CV cs.NE  published:2015-06-14 summary:Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to "absorb" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines. version:1
arxiv-1506-08663 | Linguistics and some aspects of its underlying dynamics | http://arxiv.org/abs/1506.08663 | id:1506.08663 author:Massimo Piattelli-Palmarini, Giuseppe Vitiello category:cs.CL quant-ph  published:2015-06-14 summary:In recent years, central components of a new approach to linguistics, the Minimalist Program (MP) have come closer to physics. Features of the Minimalist Program, such as the unconstrained nature of recursive Merge, the operation of the Labeling Algorithm that only operates at the interface of Narrow Syntax with the Conceptual-Intentional and the Sensory-Motor interfaces, the difference between pronounced and un-pronounced copies of elements in a sentence and the build-up of the Fibonacci sequence in the syntactic derivation of sentence structures, are directly accessible to representation in terms of algebraic formalism. Although in our scheme linguistic structures are classical ones, we find that an interesting and productive isomorphism can be established between the MP structure, algebraic structures and many-body field theory opening new avenues of inquiry on the dynamics underlying some central aspects of linguistics. version:1
arxiv-1506-04365 | Leveraging Word Embeddings for Spoken Document Summarization | http://arxiv.org/abs/1506.04365 | id:1506.04365 author:Kuan-Yu Chen, Shih-Hung Liu, Hsin-Min Wang, Berlin Chen, Hsin-Hsi Chen category:cs.CL cs.AI  published:2015-06-14 summary:Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods. version:1
arxiv-1506-04364 | Localized Multiple Kernel Learning---A Convex Approach | http://arxiv.org/abs/1506.04364 | id:1506.04364 author:Yunwen Lei, Alexander Binder, Ürün Dogan, Marius Kloft category:cs.LG  published:2015-06-14 summary:We propose a localized approach to multiple kernel learning that, in contrast to prevalent approaches, can be formulated as a convex optimization problem over a given cluster structure. From which we obtain the first generalization error bounds for localized multiple kernel learning and derive an efficient optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that the convex approach to localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts. version:1
arxiv-1505-04891 | Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph | http://arxiv.org/abs/1505.04891 | id:1505.04891 author:Fei Tian, Bin Gao, Enhong Chen, Tie-Yan Liu category:cs.CL  published:2015-05-19 summary:Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are \emph{many-to-one}, \emph{one-to-many} or even \emph{many-to-many}, rather than simply \emph{one-to-one}; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non \emph{one-to-one} relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks. version:2
arxiv-1506-04359 | Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms | http://arxiv.org/abs/1506.04359 | id:1506.04359 author:Yunwen Lei, Ürün Dogan, Alexander Binder, Marius Kloft category:cs.LG  published:2015-06-14 summary:This paper studies the generalization performance of multi-class classification algorithms, for which we obtain, for the first time, a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on $\ell_p$-norm regularization, where the parameter $p$ controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art. version:1
arxiv-1506-04356 | The Artists who Forged Themselves: Detecting Creativity in Art | http://arxiv.org/abs/1506.04356 | id:1506.04356 author:Milan Rajković, Miloš Milovanović category:cs.CV q-bio.NC  published:2015-06-14 summary:Creativity and the understanding of cognitive processes involved in the creative process are relevant to all of human activities. Comprehension of creativity in the arts is of special interest due to the involvement of many scientific and non scientific disciplines. Using digital representation of paintings, we show that creative process in painting art may be objectively recognized within the mathematical framework of self organization, a process characteristic of nonlinear dynamic systems and occurring in natural and social sciences. Unlike the artist identification process or the recognition of forgery, which presupposes the knowledge of the original work, our method requires no prior knowledge on the originality of the work of art. The original paintings are recognized as realizations of the creative process which, in general, is shown to correspond to self-organization of texture features which determine the aesthetic complexity of the painting. The method consists of the wavelet based statistical digital image processing and the measure of statistical complexity which represents the minimal (average) information necessary for optimal prediction. The statistical complexity is based on the properly defined causal states with optimal predictive properties. Two different time concepts related to the works of art are introduced: the internal time and the artistic time. The internal time of the artwork is determined by the span of causal dependencies between wavelet coefficients while the artistic time refers to the internal time during which complexity increases where complexity refers to compositional, aesthetic and structural arrangement of texture features. The method is illustrated by recognizing the original paintings from the copies made by the artists themselves, including the works of the famous surrealist painter Ren\'{e} Magritte. version:1
arxiv-1506-04340 | Deep Secure Encoding: An Application to Face Recognition | http://arxiv.org/abs/1506.04340 | id:1506.04340 author:Rohit Pandey, Yingbo Zhou, Venu Govindaraju category:cs.CV  published:2015-06-14 summary:In this paper we present Deep Secure Encoding: a framework for secure classification using deep neural networks, and apply it to the task of biometric template protection for faces. Using deep convolutional neural networks (CNNs), we learn a robust mapping of face classes to high entropy secure codes. These secure codes are then hashed using standard hash functions like SHA-256 to generate secure face templates. The efficacy of the approach is shown on two face databases, namely, CMU-PIE and Extended Yale B, where we achieve state of the art matching performance, along with cancelability and high security with no unrealistic assumptions. Furthermore, the scheme can work in both identification and verification modes. version:1
arxiv-1506-04338 | Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis | http://arxiv.org/abs/1506.04338 | id:1506.04338 author:Wei Yang, Haiting Lin, Sing Bing Kang, Jingyi Yu category:cs.CV  published:2015-06-14 summary:In perspective cameras, images of a frontal-parallel 3D object preserve its aspect ratio invariant to its depth. Such an invariance is useful in photography but is unique to perspective projection. In this paper, we show that alternative non-perspective cameras such as the crossed-slit or XSlit cameras exhibit a different depth-dependent aspect ratio (DDAR) property that can be used to 3D recovery. We first conduct a comprehensive analysis to characterize DDAR, infer object depth from its AR, and model recoverable depth range, sensitivity, and error. We show that repeated shape patterns in real Manhattan World scenes can be used for 3D reconstruction using a single XSlit image. We also extend our analysis to model slopes of lines. Specifically, parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which can also be used to infer their depths. We validate our analyses using real XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that DDAR and DDS provide important depth cues and enable effective single-image scene reconstruction. version:1
arxiv-1502-06362 | Contextual Dueling Bandits | http://arxiv.org/abs/1502.06362 | id:1502.06362 author:Miroslav Dudík, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, Masrour Zoghi category:cs.LG  published:2015-02-23 summary:We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context. Roughly, the learner's goal is to find the best policy, or way of behaving, in some space of policies, although "best" is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a von Neumann winner, a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three efficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space. version:2
arxiv-1408-5801 | A General Framework for Fast Stagewise Algorithms | http://arxiv.org/abs/1408.5801 | id:1408.5801 author:Ryan J. Tibshirani category:stat.ML stat.CO  published:2014-08-25 summary:Forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount $\epsilon$) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size $\epsilon$ goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an $\ell_1$ norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient). Even when they do not match their $\ell_1$-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the $\ell_1$ norm and sparsity? The current paper is an attempt to do just this. We present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising, and more. version:2
arxiv-1407-2724 | On the Optimality of Averaging in Distributed Statistical Learning | http://arxiv.org/abs/1407.2724 | id:1407.2724 author:Jonathan Rosenblatt, Boaz Nadler category:stat.ML math.ST stat.TH  published:2014-07-10 summary:A common approach to statistical learning with big-data is to randomly split it among $m$ machines and learn the parameter of interest by averaging the $m$ individual estimates. In this paper, focusing on empirical risk minimization, or equivalently M-estimation, we study the statistical error incurred by this strategy. We consider two large-sample settings: First, a classical setting where the number of parameters $p$ is fixed, and the number of samples per machine $n\to\infty$. Second, a high-dimensional regime where both $p,n\to\infty$ with $p/n \to \kappa \in (0,1)$. For both regimes and under suitable assumptions, we present asymptotically exact expressions for this estimation error. In the fixed-$p$ setting, under suitable assumptions, we prove that to leading order averaging is as accurate as the centralized solution. We also derive the second order error terms, and show that these can be non-negligible, notably for non-linear models. The high-dimensional setting, in contrast, exhibits a qualitatively different behavior: data splitting incurs a first-order accuracy loss, which to leading order increases linearly with the number of machines. The dependence of our error approximations on the number of machines traces an interesting accuracy-complexity tradeoff, allowing the practitioner an informed choice on the number of machines to deploy. Finally, we confirm our theoretical analysis with several simulations. version:2
arxiv-1506-00572 | How much is said in a microblog? A multilingual inquiry based on Weibo and Twitter | http://arxiv.org/abs/1506.00572 | id:1506.00572 author:Han-Teng Liao, King-wa Fu, Scott A. Hale category:cs.SI cs.CL cs.CY H.5.3  H.5.4  published:2015-06-01 summary:This paper presents a multilingual study on, per single post of microblog text, (a) how much can be said, (b) how much is written in terms of characters and bytes, and (c) how much is said in terms of information content in posts by different organizations in different languages. Focusing on three different languages (English, Chinese, and Japanese), this research analyses Weibo and Twitter accounts of major embassies and news agencies. We first establish our criterion for quantifying "how much can be said" in a digital text based on the openly available Universal Declaration of Human Rights and the translated subtitles from TED talks. These parallel corpora allow us to determine the number of characters and bits needed to represent the same content in different languages and character encodings. We then derive the amount of information that is actually contained in microblog posts authored by selected accounts on Weibo and Twitter. Our results confirm that languages with larger character sets such as Chinese and Japanese contain more information per character than English, but the actual information content contained within a microblog text varies depending on both the type of organization and the language of the post. We conclude with a discussion on the design implications of microblog text limits for different languages. version:2
arxiv-1506-05068 | Extract an essential skeleton of a character as a graph from a character image | http://arxiv.org/abs/1506.05068 | id:1506.05068 author:Kazuhisa Fujita category:cs.CV  published:2015-06-13 summary:This paper aims to make a graph representing an essential skeleton of a character from an image that includes a machine printed or a handwritten character using the growing neural gas (GNG) method and the relative neighborhood graph (RNG) algorithm. The visual system in our brain can recognize printed characters and handwritten characters easily, robustly, and precisely. How can our brains robustly recognize characters? In the visual processing in our brain, essential features of an object will be used for recognition. The essential features are crosses, corners, junctions and so on. These features may be useful for character recognition by a computer. However, extraction of the features is difficult. If the skeleton of a character is represented as a graph, the features can be more easily extracted. To extract the skeleton of a character as a graph from a character image, we used the GNG method and the RNG algorithm. We achieved to extract skeleton graphs from images including distorted, noisy, and handwritten characters. version:1
arxiv-1503-06619 | Fusing Continuous-valued Medical Labels using a Bayesian Model | http://arxiv.org/abs/1503.06619 | id:1503.06619 author:Tingting Zhu, Nic Dunkley, Joachim Behar, David A. Clifton, Gari D. Clifford category:cs.LG  published:2015-03-23 summary:With the rapid increase in volume of time series medical data available through wearable devices, there is a need to employ automated algorithms to label data. Examples of labels include interventions, changes in activity (e.g. sleep) and changes in physiology (e.g. arrhythmias). However, automated algorithms tend to be unreliable resulting in lower quality care. Expert annotations are scarce, expensive, and prone to significant inter- and intra-observer variance. To address these problems, a Bayesian Continuous-valued Label Aggregator(BCLA) is proposed to provide a reliable estimation of label aggregation while accurately infer the precision and bias of each algorithm. The BCLA was applied to QT interval (pro-arrhythmic indicator) estimation from the electrocardiogram using labels from the 2006 PhysioNet/Computing in Cardiology Challenge database. It was compared to the mean, median, and a previously proposed Expectation Maximization (EM) label aggregation approaches. While accurately predicting each labelling algorithm's bias and precision, the root-mean-square error of the BCLA was 11.78$\pm$0.63ms, significantly outperforming the best Challenge entry (15.37$\pm$2.13ms) as well as the EM, mean, and median voting strategies (14.76$\pm$0.52ms, 17.61$\pm$0.55ms, and 14.43$\pm$0.57ms respectively with $p<0.0001$). version:2
arxiv-1506-04257 | Contamination Estimation via Convex Relaxations | http://arxiv.org/abs/1506.04257 | id:1506.04257 author:Matthew L. Malloy, Scott Alfeld, Paul Barford category:cs.IT cs.LG math.IT math.OC  published:2015-06-13 summary:Identifying anomalies and contamination in datasets is important in a wide variety of settings. In this paper, we describe a new technique for estimating contamination in large, discrete valued datasets. Our approach considers the normal condition of the data to be specified by a model consisting of a set of distributions. Our key contribution is in our approach to contamination estimation. Specifically, we develop a technique that identifies the minimum number of data points that must be discarded (i.e., the level of contamination) from an empirical data set in order to match the model to within a specified goodness-of-fit, controlled by a p-value. Appealing to results from large deviations theory, we show a lower bound on the level of contamination is obtained by solving a series of convex programs. Theoretical results guarantee the bound converges at a rate of $O(\sqrt{\log(p)/p})$, where p is the size of the empirical data set. version:1
arxiv-1506-04229 | Evaluation of the Accuracy of the BGLemmatizer | http://arxiv.org/abs/1506.04229 | id:1506.04229 author:Elena Karashtranova, Grigor Iliev, Nadezhda Borisova, Yana Chankova, Irena Atanasova category:cs.CL  published:2015-06-13 summary:This paper reveals the results of an analysis of the accuracy of developed software for automatic lemmatization for the Bulgarian language. This lemmatization software is written entirely in Java and is distributed as a GATE plugin. Certain statistical methods are used to define the accuracy of this software. The results of the analysis show 95% lemmatization accuracy. version:1
arxiv-1506-04228 | A Publicly Available Cross-Platform Lemmatizer for Bulgarian | http://arxiv.org/abs/1506.04228 | id:1506.04228 author:Grigor Iliev, Nadezhda Borisova, Elena Karashtranova, Dafina Kostadinova category:cs.CL  published:2015-06-13 summary:Our dictionary-based lemmatizer for the Bulgarian language presented here is distributed as free software, publicly available to download and use under the GPL v3 license. The presented software is written entirely in Java and is distributed as a GATE plugin. To our best knowledge, at the time of writing this article, there are not any other free lemmatization tools specifically targeting the Bulgarian language. The presented lemmatizer is a work in progress and currently yields an accuracy of about 95% in comparison to the manually annotated corpus BulTreeBank-Morph, which contains 273933 tokens. version:1
arxiv-1506-04191 | Deep Structured Models For Group Activity Recognition | http://arxiv.org/abs/1506.04191 | id:1506.04191 author:Zhiwei Deng, Mengyao Zhai, Lei Chen, Yuhao Liu, Srikanth Muralidharan, Mehrsan Javan Roshtkhari, Greg Mori category:cs.CV  published:2015-06-12 summary:This paper presents a deep neural-network-based hierarchical graphical model for individual and group activity recognition in surveillance scenes. Deep networks are used to recognize the actions of individual people in a scene. Next, a neural-network-based hierarchical graphical model refines the predicted labels for each class by considering dependencies between the classes. This refinement step mimics a message-passing step similar to inference in a probabilistic graphical model. We show that this approach can be effective in group activity recognition, with the deep graphical model improving recognition rates over baseline methods. version:1
arxiv-1506-04177 | Search Strategies for Binary Feature Selection for a Naive Bayes Classifier | http://arxiv.org/abs/1506.04177 | id:1506.04177 author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG  published:2015-06-12 summary:We compare in this paper several feature selection methods for the Naive Bayes Classifier (NBC) when the data under study are described by a large number of redundant binary indicators. Wrapper approaches guided by the NBC estimation of the classification error probability out-perform filter approaches while retaining a reasonable computational cost. version:1
arxiv-1506-04176 | Using the Mean Absolute Percentage Error for Regression Models | http://arxiv.org/abs/1506.04176 | id:1506.04176 author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:stat.ML cs.LG  published:2015-06-12 summary:We study in this paper the consequences of using the Mean Absolute Percentage Error (MAPE) as a measure of quality for regression models. We show that finding the best model under the MAPE is equivalent to doing weighted Mean Absolute Error (MAE) regression. We show that universal consistency of Empirical Risk Minimization remains possible using the MAPE instead of the MAE. version:1
arxiv-1506-04138 | Exact ICL maximization in a non-stationary time extension of the latent block model for dynamic networks | http://arxiv.org/abs/1506.04138 | id:1506.04138 author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML  published:2015-06-12 summary:The latent block model (LBM) is a flexible probabilistic tool to describe interactions between node sets in bipartite networks, but it does not account for interactions of time varying intensity between nodes in unknown classes. In this paper we propose a non stationary temporal extension of the LBM that clusters simultaneously the two node sets of a bipartite network and constructs classes of time intervals on which interactions are stationary. The number of clusters as well as the membership to classes are obtained by maximizing the exact complete-data integrated likelihood relying on a greedy search approach. Experiments on simulated and real data are carried out in order to assess the proposed methodology. version:1
arxiv-1506-04135 | Reducing offline evaluation bias of collaborative filtering algorithms | http://arxiv.org/abs/1506.04135 | id:1506.04135 author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:cs.IR cs.LG stat.ML  published:2015-06-12 summary:Recommendation systems have been integrated into the majority of large online systems to filter and rank information according to user profiles. It thus influences the way users interact with the system and, as a consequence, bias the evaluation of the performance of a recommendation algorithm computed using historical data (via offline evaluation). This paper presents a new application of a weighted offline evaluation to reduce this bias for collaborative filtering algorithms. version:1
arxiv-1506-03705 | Random Maxout Features | http://arxiv.org/abs/1506.03705 | id:1506.03705 author:Youssef Mroueh, Steven Rennie, Vaibhava Goel category:cs.LG stat.ML  published:2015-06-11 summary:In this paper, we propose and study random maxout features, which are constructed by first projecting the input data onto sets of randomly generated vectors with Gaussian elements, and then outputing the maximum projection value for each set. We show that the resulting random feature map, when used in conjunction with linear models, allows for the locally linear estimation of the function of interest in classification tasks, and for the locally linear embedding of points when used for dimensionality reduction or data visualization. We derive generalization bounds for learning that assess the error in approximating locally linear functions by linear functions in the maxout feature space, and empirically evaluate the efficacy of the approach on the MNIST and TIMIT classification tasks. version:2
arxiv-1506-03431 | Automatic Variational Inference in Stan | http://arxiv.org/abs/1506.03431 | id:1506.03431 author:Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, David M. Blei category:stat.ML  published:2015-06-10 summary:Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan. version:2
arxiv-1506-04093 | Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle Point Problems | http://arxiv.org/abs/1506.04093 | id:1506.04093 author:Zhanxing Zhu, Amos J. Storkey category:stat.ML cs.LG  published:2015-06-12 summary:We consider a generic convex-concave saddle point problem with separable structure, a form that covers a wide-ranged machine learning applications. Under this problem structure, we follow the framework of primal-dual updates for saddle point problems, and incorporate stochastic block coordinate descent with adaptive stepsize into this framework. We theoretically show that our proposal of adaptive stepsize potentially achieves a sharper linear convergence rate compared with the existing methods. Additionally, since we can select "mini-batch" of block coordinates to update, our method is also amenable to parallel processing for large-scale data. We apply the proposed method to regularized empirical risk minimization and show that it performs comparably or, more often, better than state-of-the-art methods on both synthetic and real-world data sets. version:1
arxiv-1506-04051 | Towards Benchmarking Scene Background Initialization | http://arxiv.org/abs/1506.04051 | id:1506.04051 author:Lucia Maddalena, Alfredo Petrosino category:cs.CV  published:2015-06-12 summary:Given a set of images of a scene taken at different times, the availability of an initial background model that describes the scene without foreground objects is the prerequisite for a wide range of applications, ranging from video surveillance to computational photography. Even though several methods have been proposed for scene background initialization, the lack of a common groundtruthed dataset and of a common set of metrics makes it difficult to compare their performance. To move first steps towards an easy and fair comparison of these methods, we assembled a dataset of sequences frequently adopted for background initialization, selected or created ground truths for quantitative evaluation through a selected suite of metrics, and compared results obtained by some existing methods, making all the material publicly available. version:1
arxiv-1412-3421 | Multi-Atlas Segmentation of Biomedical Images: A Survey | http://arxiv.org/abs/1412.3421 | id:1412.3421 author:Juan Eugenio Iglesias, Mert Rory Sabuncu category:cs.CV  published:2014-12-10 summary:Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of "atlases" (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 - 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation. version:2
arxiv-1503-03535 | On Using Monolingual Corpora in Neural Machine Translation | http://arxiv.org/abs/1503.03535 | id:1503.03535 author:Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, Yoshua Bengio category:cs.CL  published:2015-03-11 summary:Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively. version:2
arxiv-1506-04002 | Knowledge Representation in Learning Classifier Systems: A Review | http://arxiv.org/abs/1506.04002 | id:1506.04002 author:Farzaneh Shoeleh, Mahshid Majd, Ali Hamzeh, Sattar Hashemi category:cs.NE cs.LG  published:2015-06-12 summary:Knowledge representation is a key component to the success of all rule based systems including learning classifier systems (LCSs). This component brings insight into how to partition the problem space what in turn seeks prominent role in generalization capacity of the system as a whole. Recently, knowledge representation component has received great deal of attention within data mining communities due to its impacts on rule based systems in terms of efficiency and efficacy. The current work is an attempt to find a comprehensive and yet elaborate view into the existing knowledge representation techniques in LCS domain in general and XCS in specific. To achieve the objectives, knowledge representation techniques are grouped into different categories based on the classification approach in which they are incorporated. In each category, the underlying rule representation schema and the format of classifier condition to support the corresponding representation are presented. Furthermore, a precise explanation on the way that each technique partitions the problem space along with the extensive experimental results is provided. To have an elaborated view on the functionality of each technique, a comparative analysis of existing techniques on some conventional problems is provided. We expect this survey to be of interest to the LCS researchers and practitioners since it provides a guideline for choosing a proper knowledge representation technique for a given problem and also opens up new streams of research on this topic. version:1
arxiv-1506-04000 | MCMC for Variationally Sparse Gaussian Processes | http://arxiv.org/abs/1506.04000 | id:1506.04000 author:James Hensman, Alexander G. de G. Matthews, Maurizio Filippone, Zoubin Ghahramani category:stat.ML  published:2015-06-12 summary:Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly. version:1
arxiv-1506-03998 | Sparse Multi-layer Image Approximation: Facial Image Compression | http://arxiv.org/abs/1506.03998 | id:1506.03998 author:Sohrab Ferdowsi, Svyatoslav Voloshynovskiy, Dimche Kostadinov category:cs.CV cs.IT math.IT  published:2015-06-12 summary:We propose a scheme for multi-layer representation of images. The problem is first treated from an information-theoretic viewpoint where we analyze the behavior of different sources of information under a multi-layer data compression framework and compare it with a single-stage (shallow) structure. We then consider the image data as the source of information and link the proposed representation scheme to the problem of multi-layer dictionary learning for visual data. For the current work we focus on the problem of image compression for a special class of images where we report a considerable performance boost in terms of PSNR at high compression ratios in comparison with the JPEG2000 codec. version:1
arxiv-1506-03995 | Technical Report: Image Captioning with Semantically Similar Images | http://arxiv.org/abs/1506.03995 | id:1506.03995 author:Martin Kolář, Michal Hradiš, Pavel Zemčík category:cs.CV  published:2015-06-12 summary:This report presents our submission to the MS COCO Captioning Challenge 2015. The method uses Convolutional Neural Network activations as an embedding to find semantically similar images. From these images, the most typical caption is selected based on unigram frequencies. Although the method received low scores with automated evaluation metrics and in human assessed average correctness, it is competitive in the ratio of captions which pass the Turing test and which are assessed as better or equal to human captions. version:1
arxiv-1306-0160 | Phase Retrieval using Alternating Minimization | http://arxiv.org/abs/1306.0160 | id:1306.0160 author:Praneeth Netrapalli, Prateek Jain, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT  published:2013-06-02 summary:Phase retrieval problems involve solving linear equations, but with missing sign (or phase, for complex numbers) information. More than four decades after it was first proposed, the seminal error reduction algorithm of (Gerchberg and Saxton 1972) and (Fienup 1982) is still the popular choice for solving many variants of this problem. The algorithm is based on alternating minimization; i.e. it alternates between estimating the missing phase information, and the candidate solution. Despite its wide usage in practice, no global convergence guarantees for this algorithm are known. In this paper, we show that a (resampling) variant of this approach converges geometrically to the solution of one such problem -- finding a vector $\mathbf{x}$ from $\mathbf{y},\mathbf{A}$, where $\mathbf{y} = \left \mathbf{A}^{\top}\mathbf{x}\right $ and $ \mathbf{z} $ denotes a vector of element-wise magnitudes of $\mathbf{z}$ -- under the assumption that $\mathbf{A}$ is Gaussian. Empirically, we demonstrate that alternating minimization performs similar to recently proposed convex techniques for this problem (which are based on "lifting" to a convex matrix problem) in sample complexity and robustness to noise. However, it is much more efficient and can scale to large problems. Analytically, for a resampling version of alternating minimization, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds. We also establish close to optimal scaling for the case when the unknown vector is sparse. Our work represents the first theoretical guarantee for alternating minimization (albeit with resampling) for any variant of phase retrieval problems in the non-convex setting. version:2
arxiv-1506-03958 | Robust Structured Low-Rank Approximation on the Grassmannian | http://arxiv.org/abs/1506.03958 | id:1506.03958 author:Clemens Hage, Martin Kleinsteuber category:stat.ML  published:2015-06-12 summary:Over the past years Robust PCA has been established as a standard tool for reliable low-rank approximation of matrices in the presence of outliers. Recently, the Robust PCA approach via nuclear norm minimization has been extended to matrices with linear structures which appear in applications such as system identification and data series analysis. At the same time it has been shown how to control the rank of a structured approximation via matrix factorization approaches. The drawbacks of these methods either lie in the lack of robustness against outliers or in their static nature of repeated batch-processing. We present a Robust Structured Low-Rank Approximation method on the Grassmannian that on the one hand allows for fast re-initialization in an online setting due to subspace identification with manifolds, and that is robust against outliers due to a smooth approximation of the $\ell_p$-norm cost function on the other hand. The method is evaluated in online time series forecasting tasks on simulated and real-world data. version:1
arxiv-1506-03942 | Optimal $γ$ and $C$ for $ε$-Support Vector Regression with RBF Kernels | http://arxiv.org/abs/1506.03942 | id:1506.03942 author:Longfei Lu category:cs.LG stat.ML  published:2015-06-12 summary:The objective of this study is to investigate the efficient determination of $C$ and $\gamma$ for Support Vector Regression with RBF or mahalanobis kernel based on numerical and statistician considerations, which indicates the connection between $C$ and kernels and demonstrates that the deviation of geometric distance of neighbour observation in mapped space effects the predict accuracy of $\epsilon$-SVR. We determinate the arrange of $\gamma$ & $C$ and propose our method to choose their best values. version:1
arxiv-1506-03936 | A Novel Hybrid Approach for Cephalometric Landmark Detection | http://arxiv.org/abs/1506.03936 | id:1506.03936 author:Mahshid Majd, Farzaneh Shoeleh category:cs.CV  published:2015-06-12 summary:Cephalometric analysis has an important role in dentistry and especially in orthodontics as a treatment planning tool to gauge the size and special relationships of the teeth, jaws and cranium. The first step of using such analyses is localizing some important landmarks known as cephalometric landmarks on craniofacial in x-ray image. The past decade has seen a growing interest in automating this process. In this paper, a novel hybrid approach is proposed for automatic detection of cephalometric landmarks. Here, the landmarks are categorized into three main sets according to their anatomical characteristics and usage in well-known cephalometric analyses. Consequently, to have a reliable and accurate detection system, three methods named edge tracing, weighted template matching, and analysis based estimation are designed, each of which is consistent and well-suited for one category. Edge tracing method is suggested to predict those landmarks which are located on edges. Weighted template matching method is well-suited for landmarks located in an obvious and specific structure which can be extracted or searchable in a given x-ray image. The last but not the least method is named analysis based estimation. This method is based on the fact that in cephalometric analyses the relations between landmarks are used and the locations of some landmarks are never used individually. Therefore the third suggested method has a novelty in estimating the desired relations directly. The effectiveness of the proposed approach is compared with the state of the art methods and the results were promising especially in real world applications. version:1
arxiv-1503-02427 | Syntax-based Deep Matching of Short Texts | http://arxiv.org/abs/1503.02427 | id:1503.02427 author:Mingxuan Wang, Zhengdong Lu, Hang Li, Qun Liu category:cs.CL cs.LG cs.NE  published:2015-03-09 summary:Many tasks in natural language processing, ranging from machine translation to question answering, can be reduced to the problem of matching two sentences or more generally two short texts. We propose a new approach to the problem, called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The approach consists of two components, 1) a mining algorithm to discover patterns for matching two short-texts, defined in the product space of dependency trees, and 2) a deep neural network for matching short texts using the mined patterns, as well as a learning algorithm to build the network having a sparse structure. We test our algorithm on the problem of matching a tweet and a response in social media, a hard matching problem proposed in [Wang et al., 2013], and show that DeepMatch$_{tree}$ can outperform a number of competitor models including one without using dependency trees and one based on word-embedding, all with large margins version:6
arxiv-1307-3617 | MCMC Learning | http://arxiv.org/abs/1307.3617 | id:1307.3617 author:Varun Kanade, Elchanan Mossel category:cs.LG stat.ML  published:2013-07-13 summary:The theory of learning under the uniform distribution is rich and deep, with connections to cryptography, computational complexity, and the analysis of boolean functions to name a few areas. This theory however is very limited due to the fact that the uniform distribution and the corresponding Fourier basis are rarely encountered as a statistical model. A family of distributions that vastly generalizes the uniform distribution on the Boolean cube is that of distributions represented by Markov Random Fields (MRF). Markov Random Fields are one of the main tools for modeling high dimensional data in many areas of statistics and machine learning. In this paper we initiate the investigation of extending central ideas, methods and algorithms from the theory of learning under the uniform distribution to the setup of learning concepts given examples from MRF distributions. In particular, our results establish a novel connection between properties of MCMC sampling of MRFs and learning under the MRF distribution. version:2
arxiv-1409-0578 | Consistency and fluctuations for stochastic gradient Langevin dynamics | http://arxiv.org/abs/1409.0578 | id:1409.0578 author:Yee Whye Teh, Alexandre Thiéry, Sebastian Vollmer category:stat.ML 60J22  65C40  published:2014-09-01 summary:Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally expensive. Both the calculation of the acceptance probability and the creation of informed proposals usually require an iteration through the whole data set. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem by generating proposals which are only based on a subset of the data, by skipping the accept-reject step and by using decreasing step-sizes sequence $(\delta_m)_{m \geq 0}$. %Under appropriate Lyapunov conditions, We provide in this article a rigorous mathematical framework for analysing this algorithm. We prove that, under verifiable assumptions, the algorithm is consistent, satisfies a central limit theorem (CLT) and its asymptotic bias-variance decomposition can be characterized by an explicit functional of the step-sizes sequence $(\delta_m)_{m \geq 0}$. We leverage this analysis to give practical recommendations for the notoriously difficult tuning of this algorithm: it is asymptotically optimal to use a step-size sequence of the type $\delta_m \asymp m^{-1/3}$, leading to an algorithm whose mean squared error (MSE) decreases at rate $\mathcal{O}(m^{-1/3})$ version:2
arxiv-1506-03899 | Place classification with a graph regularized deep neural network model | http://arxiv.org/abs/1506.03899 | id:1506.03899 author:Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu category:cs.RO cs.CV cs.LG cs.NE  published:2015-06-12 summary:Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. It is a nontrivial classification problem which has attracted many research. In recent years, there is a high exploitation of Artificial Intelligent algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With the deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. Firstly, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Secondly, each layer of data is fed into a deep neural network model for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all the layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effective- ness of our end-to-end place classification framework in which both the multi-layer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information. version:1
arxiv-1506-00815 | Classify Images with Conceptor Network | http://arxiv.org/abs/1506.00815 | id:1506.00815 author:Yuhuang Hu, M. S. Ishwarya, Chu Kiong Loo category:cs.CV  published:2015-06-02 summary:This article demonstrates a new conceptor network based classifier in classifying images. Mathematical descriptions and analysis are presented. Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10 and CIFAR-100. The experiments displayed that conceptor network can offer superior results and flexible configurations than conventional classifiers such as Softmax Regression and Support Vector Machine (SVM). version:4
arxiv-1506-03880 | Causal inference via algebraic geometry: necessary and sufficient conditions for the feasibility of discrete causal models | http://arxiv.org/abs/1506.03880 | id:1506.03880 author:Ciarán M. Lee, Robert W. Spekkens category:stat.ML quant-ph  published:2015-06-12 summary:We provide a scheme for inferring causal relations from uncontrolled statistical data which makes use of all of the information in the joint probability distribution over the observed variables rather than just the conditional independence relations. We focus on causal models containing just two observed variables, each of which is binary. We allow any number of latent variables and we do not impose any restriction on the manner in which the observed variables may depend functionally on the latent ones. In particular, the noise need not be additive. We provide an inductive scheme for classifying causal models into distinct observational equivalence classes. For each observational equivalence class, we provide a procedure for deriving, using techniques from algebraic geometry, necessary and sufficient conditions on the joint distribution for the feasibility of the class. Connections and applications of these results to the emerging field of quantum causal models are also discussed. version:1
arxiv-1503-01212 | Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints | http://arxiv.org/abs/1503.01212 | id:1503.01212 author:Alexander Rakhlin, Karthik Sridharan category:cs.LG cs.DS stat.ML  published:2015-03-04 summary:We study online prediction where regret of the algorithm is measured against a benchmark defined via evolving constraints. This framework captures online prediction on graphs, as well as other prediction problems with combinatorial structure. A key aspect here is that finding the optimal benchmark predictor (even in hindsight, given all the data) might be computationally hard due to the combinatorial nature of the constraints. Despite this, we provide polynomial-time \emph{prediction} algorithms that achieve low regret against combinatorial benchmark sets. We do so by building improper learning algorithms based on two ideas that work together. The first is to alleviate part of the computational burden through random playout, and the second is to employ Lasserre semidefinite hierarchies to approximate the resulting integer program. Interestingly, for our prediction algorithms, we only need to compute the values of the semidefinite programs and not the rounded solutions. However, the integrality gap for Lasserre hierarchy \emph{does} enter the generic regret bound in terms of Rademacher complexity of the benchmark set. This establishes a trade-off between the computation time and the regret bound of the algorithm. version:2
arxiv-1506-03852 | Tree-Cut for Probabilistic Image Segmentation | http://arxiv.org/abs/1506.03852 | id:1506.03852 author:Shell X. Hu, Christopher K. I. Williams, Sinisa Todorovic category:stat.ML cs.CV  published:2015-06-11 summary:This paper presents a new probabilistic generative model for image segmentation, i.e. the task of partitioning an image into homogeneous regions. Our model is grounded on a mid-level image representation, called a region tree, in which regions are recursively split into subregions until superpixels are reached. Given the region tree, image segmentation is formalized as sampling cuts in the tree from the model. Inference for the cuts is exact, and formulated using dynamic programming. Our tree-cut model can be tuned to sample segmentations at a particular scale of interest out of many possible multiscale image segmentations. This generalizes the common notion that there should be only one correct segmentation per image. Also, it allows moving beyond the standard single-scale evaluation, where the segmentation result for an image is averaged against the corresponding set of coarse and fine human annotations, to conduct a scale-specific evaluation. Our quantitative results are comparable to those of the leading gPb-owt-ucm method, with the notable advantage that we additionally produce a distribution over all possible tree-consistent segmentations of the image. version:1
arxiv-1502-06464 | Rectified Factor Networks | http://arxiv.org/abs/1502.06464 | id:1502.06464 author:Djork-Arné Clevert, Andreas Mayr, Thomas Unterthiner, Sepp Hochreiter category:cs.LG cs.CV cs.NE stat.ML  published:2015-02-23 summary:We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods. version:2
arxiv-1506-03799 | Pose-Invariant 3D Face Alignment | http://arxiv.org/abs/1506.03799 | id:1506.03799 author:Amin Jourabloo, Xiaoming Liu category:cs.CV  published:2015-06-11 summary:Face alignment aims to estimate the locations of a set of landmarks for a given image. This problem has received much attention as evidenced by the recent advancement in both the methodology and performance. However, most of the existing works neither explicitly handle face images with arbitrary poses, nor perform large-scale experiments on non-frontal and profile face images. In order to address these limitations, this paper proposes a novel face alignment algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for a face image with an arbitrary pose. By integrating a 3D deformable model, a cascaded coupled-regressor approach is designed to estimate both the camera projection matrix and the 3D landmarks. Furthermore, the 3D model also allows us to automatically estimate the 2D landmark visibilities via surface normals. We gather a substantially larger collection of all-pose face images to evaluate our algorithm and demonstrate superior performances than the state-of-the-art methods. version:1
arxiv-1506-03784 | Parallelizing LDA using Partially Collapsed Gibbs Sampling | http://arxiv.org/abs/1506.03784 | id:1506.03784 author:Måns Magnusson, Leif Jonsson, Mattias Villani, David Broman category:stat.ML stat.ME  published:2015-06-11 summary:Latent dirichlet allocation (LDA) is a model widely used for unsupervised probabilistic modeling of text and images. MCMC sampling from the posterior distribution is typically performed using a collapsed Gibbs sampler that integrates out all model parameters except the topic indicators for each word. The topic indicators are Gibbs sampled iteratively by drawing each topic from its conditional posterior. The popularity of this sampler stems from its balanced combination of simplicity and efficiency, but its inherently sequential nature is an obstacle for parallel implementations. Growing corpus sizes and increasing model complexity are making inference in LDA models computationally infeasible without parallel sampling. We propose a parallel implementation of LDA that only collapses over the topic proportions in each document and therefore allows independent sampling of the topic indicators in different documents. We develop several modifications of the basic algorithm that exploits sparsity and structure to further improve the performance of the partially collapsed sampler. Contrary to other parallel LDA implementations, the partially collapsed sampler guarantees convergence to the true posterior. We show on several well-known corpora that the expected increase in statistical inefficiency from only partial collapsing is smaller than commonly assumed, and can be more than compensated by the speed-up from parallelization for larger corpora. version:1
arxiv-1506-03775 | Entity-Specific Sentiment Classification of Yahoo News Comments | http://arxiv.org/abs/1506.03775 | id:1506.03775 author:Prakhar Biyani, Cornelia Caragea, Narayan Bhamidipati category:cs.CL cs.IR cs.SI  published:2015-06-11 summary:Sentiment classification is widely used for product reviews and in online social media such as forums, Twitter, and blogs. However, the problem of classifying the sentiment of user comments on news sites has not been addressed yet. News sites cover a wide range of domains including politics, sports, technology, and entertainment, in contrast to other online social sites such as forums and review sites, which are specific to a particular domain. A user associated with a news site is likely to post comments on diverse topics (e.g., politics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, or Google). Classifying the sentiment of users tied to various entities may help obtain a holistic view of their personality, which could be useful in applications such as online advertising, content personalization, and political campaign planning. In this paper, we formulate the problem of entity-specific sentiment classification of comments posted on news articles in Yahoo News and propose novel features that are specific to news comments. Experimental results show that our models outperform state-of-the-art baselines. version:1
arxiv-1506-03768 | Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process | http://arxiv.org/abs/1506.03768 | id:1506.03768 author:Ye Wang, David B. Dunson category:stat.ML  published:2015-06-11 summary:Learning of low dimensional structure in multidimensional data is a canonical problem in machine learning. One common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold. There are a rich variety of manifold learning methods available, which allow mapping of data points to the manifold. However, there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data. The best attempt is the Gaussian process latent variable model (GP-LVM), but identifiability issues lead to poor performance. We solve these issues by proposing a novel Coulomb repulsive process (Corp) for locations of points on the manifold, inspired by physical models of electrostatic interactions among particles. Combining this process with a GP prior for the mapping function yields a novel electrostatic GP (electroGP) process. Focusing on the simple case of a one-dimensional manifold, we develop efficient inference algorithms, and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video. version:1
arxiv-1506-03767 | Spectral Representations for Convolutional Neural Networks | http://arxiv.org/abs/1506.03767 | id:1506.03767 author:Oren Rippel, Jasper Snoek, Ryan P. Adams category:stat.ML cs.LG  published:2015-06-11 summary:Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training. version:1
arxiv-1506-03729 | Recovering communities in the general stochastic block model without knowing the parameters | http://arxiv.org/abs/1506.03729 | id:1506.03729 author:Emmanuel Abbe, Colin Sandon category:math.PR cs.IT cs.LG cs.SI math.IT  published:2015-06-11 summary:Most recent developments on the stochastic block model (SBM) rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in [AS15] for linear size communities. The results are three-fold: (i) in the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and detects communities with an optimal accuracy scaling for large degrees; (ii) in the regime where degrees are scaled by $\omega(1)$ (diverging degrees), this is enhanced into a fully agnostic algorithm that only takes the graph in question and simultaneously learns the model parameters (including the number of communities) and detects communities with accuracy $1-o(1)$, with an overall quasi-linear complexity; (iii) in the logarithmic degree regime, an agnostic algorithm is developed that learns the parameters and achieves the optimal CH-limit for exact recovery, in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the general SBM with linear size communities. version:1
arxiv-1412-6632 | Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN) | http://arxiv.org/abs/1412.6632 | id:1412.6632 author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille category:cs.CV cs.CL cs.LG  published:2014-12-20 summary:In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html . version:5
arxiv-1503-02041 | On the Invariance of Dictionary Learning and Sparse Representation to Projecting Data to a Discriminative Space | http://arxiv.org/abs/1503.02041 | id:1503.02041 author:Mehrdad J. Gangeh, Ali Ghodsi category:cs.CV  published:2015-03-06 summary:In this paper, it is proved that dictionary learning and sparse representation is invariant to a linear transformation. It subsumes the special case of transforming/projecting the data into a discriminative space. This is important because recently, supervised dictionary learning algorithms have been proposed, which suggest to include the category information into the learning of dictionary to improve its discriminative power. Among them, there are some approaches that propose to learn the dictionary in a discriminative projected space. To this end, two approaches have been proposed: first, assigning the discriminative basis as the dictionary and second, perform dictionary learning in the projected space. Based on the invariance of dictionary learning to any transformation in general, and to a discriminative space in particular, we advocate the first approach. version:2
arxiv-1506-03626 | Margin-Based Feed-Forward Neural Network Classifiers | http://arxiv.org/abs/1506.03626 | id:1506.03626 author:Han Xiao, Xiaoyan Zhu category:cs.LG  published:2015-06-11 summary:Margin-Based Principle has been proposed for a long time, it has been proved that this principle could reduce the structural risk and improve the performance in both theoretical and practical aspects. Meanwhile, feed-forward neural network is a traditional classifier, which is very hot at present with a deeper architecture. However, the training algorithm of feed-forward neural network is developed and generated from Widrow-Hoff Principle that means to minimize the squared error. In this paper, we propose a new training algorithm for feed-forward neural networks based on Margin-Based Principle, which could effectively promote the accuracy and generalization ability of neural network classifiers with less labelled samples and flexible network. We have conducted experiments on four UCI open datasets and achieved good results as expected. In conclusion, our model could handle more sparse labelled and more high-dimension dataset in a high accuracy while modification from old ANN method to our method is easy and almost free of work. version:1
arxiv-1506-03623 | Max-Entropy Feed-Forward Clustering Neural Network | http://arxiv.org/abs/1506.03623 | id:1506.03623 author:Han Xiao, Xiaoyan Zhu category:cs.LG  published:2015-06-11 summary:The outputs of non-linear feed-forward neural network are positive, which could be treated as probability when they are normalized to one. If we take Entropy-Based Principle into consideration, the outputs for each sample could be represented as the distribution of this sample for different clusters. Entropy-Based Principle is the principle with which we could estimate the unknown distribution under some limited conditions. As this paper defines two processes in Feed-Forward Neural Network, our limited condition is the abstracted features of samples which are worked out in the abstraction process. And the final outputs are the probability distribution for different clusters in the clustering process. As Entropy-Based Principle is considered into the feed-forward neural network, a clustering method is born. We have conducted some experiments on six open UCI datasets, comparing with a few baselines and applied purity as the measurement . The results illustrate that our method outperforms all the other baselines that are most popular clustering methods. version:1
arxiv-1506-03599 | Distributed Recurrent Neural Forward Models with Synaptic Adaptation for Complex Behaviors of Walking Robots | http://arxiv.org/abs/1506.03599 | id:1506.03599 author:Sakyasingha Dasgupta, Dennis Goldschmidt, Florentin Wörgötter, Poramate Manoonpong category:cs.NE cs.RO q-bio.NC  published:2015-06-11 summary:Walking animals, like stick insects, cockroaches or ants, demonstrate a fascinating range of locomotive abilities and complex behaviors. The locomotive behaviors can consist of a variety of walking patterns along with adaptation that allow the animals to deal with changes in environmental conditions, like uneven terrains, gaps, obstacles etc. Biological study has revealed that such complex behaviors are a result of a combination of biome- chanics and neural mechanism thus representing the true nature of embodied interactions. While the biomechanics helps maintain flexibility and sustain a variety of movements, the neural mechanisms generate movements while making appropriate predictions crucial for achieving adaptation. Such predictions or planning ahead can be achieved by way of in- ternal models that are grounded in the overall behavior of the animal. Inspired by these findings, we present here, an artificial bio-inspired walking system which effectively com- bines biomechanics (in terms of the body and leg structures) with the underlying neural mechanisms. The neural mechanisms consist of 1) central pattern generator based control for generating basic rhythmic patterns and coordinated movements, 2) distributed (at each leg) recurrent neural network based adaptive forward models with efference copies as internal models for sensory predictions and instantaneous state estimations, and 3) searching and elevation control for adapting the movement of an individual leg to deal with different environmental conditions. Using simulations we show that this bio-inspired approach with adaptive internal models allows the walking robot to perform complex loco- motive behaviors as observed in insects, including walking on undulated terrains, crossing large gaps as well as climbing over high obstacles... version:1
arxiv-1506-02338 | Modeling Order in Neural Word Embeddings at Scale | http://arxiv.org/abs/1506.02338 | id:1506.02338 author:Andrew Trask, David Gilmore, Matthew Russell category:cs.CL  published:2015-06-08 summary:Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network. version:3
arxiv-1506-03495 | BoWFire: Detection of Fire in Still Images by Integrating Pixel Color and Texture Analysis | http://arxiv.org/abs/1506.03495 | id:1506.03495 author:Daniel Y. T. Chino, Letricia P. S. Avalhais, Jose F. Rodrigues Jr., Agma J. M. Traina category:cs.CV  published:2015-06-10 summary:Emergency events involving fire are potentially harmful, demanding a fast and precise decision making. The use of crowdsourcing image and videos on crisis management systems can aid in these situations by providing more information than verbal/textual descriptions. Due to the usual high volume of data, automatic solutions need to discard non-relevant content without losing relevant information. There are several methods for fire detection on video using color-based models. However, they are not adequate for still image processing, because they can suffer on high false-positive results. These methods also suffer from parameters with little physical meaning, which makes fine tuning a difficult task. In this context, we propose a novel fire detection method for still images that uses classification based on color features combined with texture classification on superpixel regions. Our method uses a reduced number of parameters if compared to previous works, easing the process of fine tuning the method. Results show the effectiveness of our method of reducing false-positives while its precision remains compatible with the state-of-the-art methods. version:1
arxiv-1506-03493 | Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts | http://arxiv.org/abs/1506.03493 | id:1506.03493 author:Aaron Schein, John Paisley, David M. Blei, Hanna Wallach category:stat.ML cs.AI cs.LG cs.SI stat.AP  published:2015-06-10 summary:We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form "country $i$ took action $a$ toward country $j$ at time $t$"---known as dyadic events---in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs. version:1
arxiv-1506-03489 | Truthful Linear Regression | http://arxiv.org/abs/1506.03489 | id:1506.03489 author:Rachel Cummings, Stratis Ioannidis, Katrina Ligett category:cs.GT cs.DS stat.ML  published:2015-06-10 summary:We consider the problem of fitting a linear model to data held by individuals who are concerned about their privacy. Incentivizing most players to truthfully report their data to the analyst constrains our design to mechanisms that provide a privacy guarantee to the participants; we use differential privacy to model individuals' privacy losses. This immediately poses a problem, as differentially private computation of a linear model necessarily produces a biased estimation, and existing approaches to design mechanisms to elicit data from privacy-sensitive individuals do not generalize well to biased estimators. We overcome this challenge through an appropriate design of the computation and payment scheme. version:1
arxiv-1502-00163 | Spectral Detection in the Censored Block Model | http://arxiv.org/abs/1502.00163 | id:1502.00163 author:Alaa Saade, Florent Krzakala, Marc Lelarge, Lenka Zdeborová category:cs.SI cond-mat.dis-nn cs.LG math.PR  published:2015-01-31 summary:We consider the problem of partially recovering hidden binary variables from the observation of (few) censored edge weights, a problem with applications in community detection, correlation clustering and synchronization. We describe two spectral algorithms for this task based on the non-backtracking and the Bethe Hessian operators. These algorithms are shown to be asymptotically optimal for the partial recovery problem, in that they detect the hidden assignment as soon as it is information theoretically possible to do so. version:2
arxiv-1506-03475 | Image Tag Completion and Refinement by Subspace Clustering and Matrix Completion | http://arxiv.org/abs/1506.03475 | id:1506.03475 author:Yuqing Hou, Zhouchen Lin category:cs.CV  published:2015-06-10 summary:Tag-based image retrieval (TBIR) has drawn much attention in recent years due to the explosive amount of digital images and crowdsourcing tags. However, the TBIR applications still suffer from the deficient and inaccurate tags provided by users. Inspired by the subspace clustering methods, we formulate the tag completion problem in a subspace clustering model which assumes that images are sampled from subspaces, and complete the tags using the state-of-the-art Low Rank Representation (LRR) method. And we propose a matrix completion algorithm to further refine the tags. Our empirical results on multiple benchmark datasets for image annotation show that the proposed algorithm outperforms state-of-the-art approaches when handling missing and noisy tags. version:1
arxiv-1506-03425 | Fast Online Clustering with Randomized Skeleton Sets | http://arxiv.org/abs/1506.03425 | id:1506.03425 author:Krzysztof Choromanski, Sanjiv Kumar, Xiaofeng Liu category:cs.AI cs.LG  published:2015-06-10 summary:We present a new fast online clustering algorithm that reliably recovers arbitrary-shaped data clusters in high throughout data streams. Unlike the existing state-of-the-art online clustering methods based on k-means or k-medoid, it does not make any restrictive generative assumptions. In addition, in contrast to existing nonparametric clustering techniques such as DBScan or DenStream, it gives provable theoretical guarantees. To achieve fast clustering, we propose to represent each cluster by a skeleton set which is updated continuously as new data is seen. A skeleton set consists of weighted samples from the data where weights encode local densities. The size of each skeleton set is adapted according to the cluster geometry. The proposed technique automatically detects the number of clusters and is robust to outliers. The algorithm works for the infinite data stream where more than one pass over the data is not feasible. We provide theoretical guarantees on the quality of the clustering and also demonstrate its advantage over the existing state-of-the-art on several datasets. version:1
arxiv-1404-0466 | piCholesky: Polynomial Interpolation of Multiple Cholesky Factors for Efficient Approximate Cross-Validation | http://arxiv.org/abs/1404.0466 | id:1404.0466 author:Da Kuang, Alex Gittens, Raffay Hamid category:cs.LG cs.NA  published:2014-04-02 summary:The dominant cost in solving least-square problems using Newton's method is often that of factorizing the Hessian matrix over multiple values of the regularization parameter ($\lambda$). We propose an efficient way to interpolate the Cholesky factors of the Hessian matrix computed over a small set of $\lambda$ values. This approximation enables us to optimally minimize the hold-out error while incurring only a fraction of the cost compared to exact cross-validation. We provide a formal error bound for our approximation scheme and present solutions to a set of key implementation challenges that allow our approach to maximally exploit the compute power of modern architectures. We present a thorough empirical analysis over multiple datasets to show the effectiveness of our approach. version:2
arxiv-1506-03382 | Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via Thresholded Wirtinger Flow | http://arxiv.org/abs/1506.03382 | id:1506.03382 author:T. Tony Cai, Xiaodong Li, Zongming Ma category:math.ST cs.IT math.IT math.NA stat.ML stat.TH  published:2015-06-10 summary:This paper considers the noisy sparse phase retrieval problem: recovering a sparse signal $x \in \mathbb{R}^p$ from noisy quadratic measurements $y_j = (a_j' x )^2 + \epsilon_j$, $j=1, \ldots, m$, with independent sub-exponential noise $\epsilon_j$. The goals are to understand the effect of the sparsity of $x$ on the estimation precision and to construct a computationally feasible estimator to achieve the optimal rates. Inspired by the Wirtinger Flow [12] proposed for noiseless and non-sparse phase retrieval, a novel thresholded gradient descent algorithm is proposed and it is shown to adaptively achieve the minimax optimal rates of convergence over a wide range of sparsity levels when the $a_j$'s are independent standard Gaussian random vectors, provided that the sample size is sufficiently large compared to the sparsity of $x$. version:1
arxiv-1506-03016 | Accelerated Stochastic Gradient Descent for Minimizing Finite Sums | http://arxiv.org/abs/1506.03016 | id:1506.03016 author:Atsushi Nitanda category:stat.ML cs.LG  published:2015-06-09 summary:We propose an optimization method for minimizing the finite sums of smooth convex functions. Our method incorporates an accelerated gradient descent (AGD) and a stochastic variance reduction gradient (SVRG) in a mini-batch setting. Unlike SVRG, our method can be directly applied to non-strongly and strongly convex problems. We show that our method achieves a lower overall complexity than the recently proposed methods that supports non-strongly convex problems. Moreover, this method has a fast rate of convergence for strongly convex problems. Our experiments show the effectiveness of our method. version:2
arxiv-1506-03378 | On the Prior Sensitivity of Thompson Sampling | http://arxiv.org/abs/1506.03378 | id:1506.03378 author:Che-Yu Liu, Lihong Li category:cs.LG cs.AI stat.ML  published:2015-06-10 summary:The empirically successful Thompson Sampling algorithm for stochastic bandits has drawn much interest in understanding its theoretical properties. One important benefit of the algorithm is that it allows domain knowledge to be conveniently encoded as a prior distribution to balance exploration and exploitation more effectively. While it is generally believed that the algorithm's regret is low (high) when the prior is good (bad), little is known about the exact dependence. In this paper, we fully characterize the algorithm's worst-case dependence of regret on the choice of prior, focusing on a special yet representative case. These results also provide insights into the general sensitivity of the algorithm to the choice of priors. In particular, with $p$ being the prior probability mass of the true reward-generating model, we prove $O(\sqrt{T/p})$ and $O(\sqrt{(1-p)T})$ regret upper bounds for the bad- and good-prior cases, respectively, as well as \emph{matching} lower bounds. Our proofs rely on the discovery of a fundamental property of Thompson Sampling and make heavy use of martingale theory, both of which appear novel in the literature, to the best of our knowledge. version:1
arxiv-1506-03374 | Contextual Bandits with Global Constraints and Objective | http://arxiv.org/abs/1506.03374 | id:1506.03374 author:Shipra Agrawal, Nikhil R. Devanur, Lihong Li category:cs.LG cs.AI stat.ML  published:2015-06-10 summary:We consider the contextual version of a multi-armed bandit problem with global convex constraints and concave objective function. In each round, the outcome of pulling an arm is a context-dependent vector, and the global constraints require the average of these vectors to lie in a certain convex set. The objective is a concave function of this average vector. The learning agent competes with an arbitrary set of context-dependent policies. This problem is a common generalization of problems considered by Badanidiyuru et al. (2014) and Agrawal and Devanur (2014), with important applications. We give computationally efficient algorithms with near-optimal regret, generalizing the approach of Agarwal et al. (2014) for the non-constrained version of the problem. For the special case of budget constraints our regret bounds match those of Badanidiyuru et al. (2014), answering their main open question of obtaining a computationally efficient algorithm. version:1
arxiv-1508-05342 | Genetic Algorithms for multimodal optimization: a review | http://arxiv.org/abs/1508.05342 | id:1508.05342 author:Noe Casas category:cs.NE  published:2015-06-10 summary:In this article we provide a comprehensive review of the different evolutionary algorithm techniques used to address multimodal optimization problems, classifying them according to the nature of their approach. On the one hand there are algorithms that address the issue of the early convergence to a local optimum by differentiating the individuals of the population into groups and limiting their interaction, hence having each group evolve with a high degree of independence. On the other hand other approaches are based on directly addressing the lack of genetic diversity of the population by introducing elements into the evolutionary dynamics that promote new niches of the genotypical space to be explored. Finally, we study multi-objective optimization genetic algorithms, that handle the situations where multiple criteria have to be satisfied with no penalty for any of them. Very rich literature has arised over the years on these topics, and we aim at offering an overview of the most important techniques of each branch of the field. version:1
arxiv-1411-3285 | Amoeba Techniques for Shape and Texture Analysis | http://arxiv.org/abs/1411.3285 | id:1411.3285 author:Martin Welk category:cs.CV  published:2014-11-12 summary:Morphological amoebas are image-adaptive structuring elements for morphological and other local image filters introduced by Lerallut et al. Their construction is based on combining spatial distance with contrast information into an image-dependent metric. Amoeba filters show interesting parallels to image filtering methods based on partial differential equations (PDEs), which can be confirmed by asymptotic equivalence results. In computing amoebas, graph structures are generated that hold information about local image texture. This paper reviews and summarises the work of the author and his coauthors on morphological amoebas, particularly their relations to PDE filters and texture analysis. It presents some extensions and points out directions for future investigation on the subject. version:2
arxiv-1506-03358 | Optical Flow on Evolving Sphere-Like Surfaces | http://arxiv.org/abs/1506.03358 | id:1506.03358 author:Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV  published:2015-06-10 summary:In this work we consider optical flow on evolving Riemannian 2-manifolds which can be parametrised from the 2-sphere. Our main motivation is to estimate cell motion in time-lapse volumetric microscopy images depicting fluorescently labelled cells of a live zebrafish embryo. We exploit the fact that the recorded cells float on the surface of the embryo and allow for the extraction of an image sequence together with a sphere-like surface. We solve the resulting variational problem by means of a Galerkin method based on vector spherical harmonics and present numerical results computed from the aforementioned microscopy data. version:1
arxiv-1511-01047 | Detecting Clusters of Anomalies on Low-Dimensional Feature Subsets with Application to Network Traffic Flow Data | http://arxiv.org/abs/1511.01047 | id:1511.01047 author:Zhicong Qiu, David J. Miller, George Kesidis category:cs.NI cs.CR cs.LG  published:2015-06-10 summary:In a variety of applications, one desires to detect groups of anomalous data samples, with a group potentially manifesting its atypicality (relative to a reference model) on a low-dimensional subset of the full measured set of features. Samples may only be weakly atypical individually, whereas they may be strongly atypical when considered jointly. What makes this group anomaly detection problem quite challenging is that it is a priori unknown which subset of features jointly manifests a particular group of anomalies. Moreover, it is unknown how many anomalous groups are present in a given data batch. In this work, we develop a group anomaly detection (GAD) scheme to identify the subset of samples and subset of features that jointly specify an anomalous cluster. We apply our approach to network intrusion detection to detect BotNet and peer-to-peer flow clusters. Unlike previous studies, our approach captures and exploits statistical dependencies that may exist between the measured features. Experiments on real world network traffic data demonstrate the advantage of our proposed system, and highlight the importance of exploiting feature dependency structure, compared to the feature (or test) independence assumption made in previous studies. version:1
arxiv-1412-7012 | Boltzmann-machine learning of prior distributions of binarized natural images | http://arxiv.org/abs/1412.7012 | id:1412.7012 author:Tomoyuki Obuchi, Hirokazu Koma, Muneki Yasuda category:stat.ML cond-mat.dis-nn cs.CV  published:2014-12-16 summary:Prior distributions of binarized natural images are learned by using Boltzmann machine. We find that there emerges a structure with two sublattices in the interactions, and the nearest-neighbor and next-nearest-neighbor interactions correspondingly take two discriminative values, which reflects individual characteristics of three sets of pictures we treat. On the other hand, in a longer spacial scale, a longer-range (though still rapidly-decaying) ferromagnetic interaction commonly appear in all the cases. The characteristic length scale of the interactions is universally about up to four lattice spacing $\xi \approx 4$. These results are derived by using the mean-field method which effectively reduces the computational time required in Boltzmann machine. An improved mean-field method called the Bethe approximation also gives the same result, which reinforces the validity of our analysis and findings. Relations to criticality, frustration, and simple-cell receptive fields are also discussed. version:2
arxiv-1506-03301 | Wide baseline stereo matching with convex bounded-distortion constraints | http://arxiv.org/abs/1506.03301 | id:1506.03301 author:Meirav Galun, Tal Amir, Tal Hassner, Ronen Basri, Yaron Lipman category:cs.CV  published:2015-06-10 summary:Finding correspondences in wide baseline setups is a challenging problem. Existing approaches have focused largely on developing better feature descriptors for correspondence and on accurate recovery of epipolar line constraints. This paper focuses on the challenging problem of finding correspondences once approximate epipolar constraints are given. We introduce a novel method that integrates a deformation model. Specifically, we formulate the problem as finding the largest number of corresponding points related by a bounded distortion map that obeys the given epipolar constraints. We show that, while the set of bounded distortion maps is not convex, the subset of maps that obey the epipolar line constraints is convex, allowing us to introduce an efficient algorithm for matching. We further utilize a robust cost function for matching and employ majorization-minimization for its optimization. Our experiments indicate that our method finds significantly more accurate maps than existing approaches. version:1
arxiv-1502-06354 | First-order regret bounds for combinatorial semi-bandits | http://arxiv.org/abs/1502.06354 | id:1502.06354 author:Gergely Neu category:cs.LG stat.ML  published:2015-02-23 summary:We consider the problem of online combinatorial optimization under semi-bandit feedback, where a learner has to repeatedly pick actions from a combinatorial decision set in order to minimize the total losses associated with its decisions. After making each decision, the learner observes the losses associated with its action, but not other losses. For this problem, there are several learning algorithms that guarantee that the learner's expected regret grows as $\widetilde{O}(\sqrt{T})$ with the number of rounds $T$. In this paper, we propose an algorithm that improves this scaling to $\widetilde{O}(\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the best action. Our algorithm is among the first to achieve such guarantees in a partial-feedback scheme, and the first one to do so in a combinatorial setting. version:2
arxiv-1506-03264 | Memory and information processing in neuromorphic systems | http://arxiv.org/abs/1506.03264 | id:1506.03264 author:Giacomo Indiveri, Shih-Chii Liu category:cs.NE  published:2015-06-10 summary:A striking difference between brain-inspired neuromorphic processors and current von Neumann processors architectures is the way in which memory and processing is organized. As Information and Communication Technologies continue to address the need for increased computational power through the increase of cores within a digital processor, neuromorphic engineers and scientists can complement this need by building processor architectures where memory is distributed with the processing. In this paper we present a survey of brain-inspired processor architectures that support models of cortical networks and deep neural networks. These architectures range from serial clocked implementations of multi-neuron systems to massively parallel asynchronous ones and from purely digital systems to mixed analog/digital systems which implement more biological-like models of neurons and synapses together with a suite of adaptation and learning mechanisms analogous to the ones found in biological nervous systems. We describe the advantages of the different approaches being pursued and present the challenges that need to be addressed for building artificial neural processing systems that can display the richness of behaviors seen in biological systems. version:1
arxiv-1506-03257 | Combining Temporal Information and Topic Modeling for Cross-Document Event Ordering | http://arxiv.org/abs/1506.03257 | id:1506.03257 author:Borja Navarro-Colorado, Estela Saquete category:cs.CL  published:2015-06-10 summary:Building unified timelines from a collection of written news articles requires cross-document event coreference resolution and temporal relation extraction. In this paper we present an approach event coreference resolution according to: a) similar temporal information, and b) similar semantic arguments. Temporal information is detected using an automatic temporal information system (TIPSem), while semantic information is represented by means of LDA Topic Modeling. The evaluation of our approach shows that it obtains the highest Micro-average F-score results in the SemEval2015 Task 4: TimeLine: Cross-Document Event Ordering (25.36\% for TrackB, 23.15\% for SubtrackB), with an improvement of up to 6\% in comparison to the other systems. However, our experiment also showed some draw-backs in the Topic Modeling approach that degrades performance of the system. version:1
arxiv-1506-03208 | A Scale Mixture Perspective of Multiplicative Noise in Neural Networks | http://arxiv.org/abs/1506.03208 | id:1506.03208 author:Eric Nalisnick, Anima Anandkumar, Padhraic Smyth category:stat.ML  published:2015-06-10 summary:Corrupting the input and hidden layers of deep neural networks (DNNs) with multiplicative noise, often drawn from the Bernoulli distribution (or 'dropout'), provides regularization that has significantly contributed to deep learning's success. However, understanding how multiplicative corruptions prevent overfitting has been difficult due to the complexity of a DNN's functional form. In this paper, we show that when a Gaussian prior is placed on a DNN's weights, applying multiplicative noise induces a Gaussian scale mixture, which can be reparameterized to circumvent the problematic likelihood function. Analysis can then proceed by using a type-II maximum likelihood procedure to derive a closed-form expression revealing how regularization evolves as a function of the network's weights. Results show that multiplicative noise forces weights to become either sparse or invariant to rescaling. We find our analysis has implications for model compression as it naturally reveals a weight pruning rule that starkly contrasts with the commonly used signal-to-noise ratio (SNR). While the SNR prunes weights with large variances, seeing them as noisy, our approach recognizes their robustness and retains them. We empirically demonstrate our approach has a strong advantage over the SNR heuristic and is competitive to retraining with soft targets produced from a teacher model. version:1
arxiv-1506-03184 | ICDAR 2015 Text Reading in the Wild Competition | http://arxiv.org/abs/1506.03184 | id:1506.03184 author:Xinyu Zhou, Shuchang Zhou, Cong Yao, Zhimin Cao, Qi Yin category:cs.CV  published:2015-06-10 summary:Recently, text detection and recognition in natural scenes are becoming increasing popular in the computer vision community as well as the document analysis community. However, majority of the existing ideas, algorithms and systems are specifically designed for English. This technical report presents the final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015) competition, which aims at establishing a benchmark for assessing detection and recognition algorithms devised for both Chinese and English scripts and providing a playground for researchers from the community. In this article, we describe in detail the dataset, tasks, evaluation protocols and participants of this competition, and report the performance of the participating methods. Moreover, promising directions for future research are discussed. version:1
arxiv-1409-2617 | Large-scale randomized-coordinate descent methods with non-separable linear constraints | http://arxiv.org/abs/1409.2617 | id:1409.2617 author:Sashank Reddi, Ahmed Hefny, Carlton Downey, Avinava Dubey, Suvrit Sra category:math.OC stat.ML  published:2014-09-09 summary:We develop randomized (block) coordinate descent (CD) methods for linearly constrained convex optimization. Unlike most CD methods, we do not assume the constraints to be separable, but let them be coupled linearly. To our knowledge, ours is the first CD method that allows linear coupling constraints, without making the global iteration complexity have an exponential dependence on the number of constraints. We present algorithms and analysis for four key problem scenarios: (i) smooth; (ii) smooth + nonsmooth separable; (iii) asynchronous parallel; and (iv) stochastic. We illustrate empirical behavior of our algorithms by simulation experiments. version:5
arxiv-1506-03139 | Robust Subgraph Generation Improves Abstract Meaning Representation Parsing | http://arxiv.org/abs/1506.03139 | id:1506.03139 author:Keenon Werling, Gabor Angeli, Christopher Manning category:cs.CL  published:2015-06-10 summary:The Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and LDC2014T12 datasets. version:1
arxiv-1506-03134 | Pointer Networks | http://arxiv.org/abs/1506.03134 | id:1506.03134 author:Oriol Vinyals, Meire Fortunato, Navdeep Jaitly category:stat.ML cs.CG cs.LG cs.NE  published:2015-06-09 summary:We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems. version:1
arxiv-1506-03124 | Multiscale edge detection and parametric shape modeling for boundary delineation in optoacoustic images | http://arxiv.org/abs/1506.03124 | id:1506.03124 author:Subhamoy Mandal, Viswanath Pamulakanty Sudarshan, Yeshaswini Nagaraj, Xose Luis Dean Ben, Daniel Razansky category:physics.med-ph cs.CV  published:2015-06-09 summary:In this article, we present a novel scheme for segmenting the image boundary (with the background) in optoacoustic small animal in vivo imaging systems. The method utilizes a multiscale edge detection algorithm to generate a binary edge map. A scale dependent morphological operation is employed to clean spurious edges. Thereafter, an ellipse is fitted to the edge map through constrained parametric transformations and iterative goodness of fit calculations. The method delimits the tissue edges through the curve fitting model, which has shown high levels of accuracy. Thus, this method enables segmentation of optoacoutic images with minimal human intervention, by eliminating need of scale selection for multiscale processing and seed point determination for contour mapping. version:1
arxiv-1412-7449 | Grammar as a Foreign Language | http://arxiv.org/abs/1412.7449 | id:1412.7449 author:Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton category:cs.CL cs.LG stat.ML  published:2014-12-23 summary:Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation. version:3
arxiv-1506-03074 | Variational consensus Monte Carlo | http://arxiv.org/abs/1506.03074 | id:1506.03074 author:Maxim Rabinovich, Elaine Angelino, Michael I. Jordan category:stat.ML stat.CO  published:2015-06-09 summary:Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (Scott et al, 2013). A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC, achieving near-ideal speedup in some instances. version:1
arxiv-1506-03072 | Clustering by transitive propagation | http://arxiv.org/abs/1506.03072 | id:1506.03072 author:Vijay Kumar, Dan Levy category:cs.LG cond-mat.stat-mech stat.ML  published:2015-06-09 summary:We present a global optimization algorithm for clustering data given the ratio of likelihoods that each pair of data points is in the same cluster or in different clusters. To define a clustering solution in terms of pairwise relationships, a necessary and sufficient condition is that belonging to the same cluster satisfies transitivity. We define a global objective function based on pairwise likelihood ratios and a transitivity constraint over all triples, assigning an equal prior probability to all clustering solutions. We maximize the objective function by implementing max-sum message passing on the corresponding factor graph to arrive at an O(N^3) algorithm. Lastly, we demonstrate an application inspired by mutational sequencing for decoding random binary words transmitted through a noisy channel. version:1
arxiv-1503-08316 | A Variance Reduced Stochastic Newton Method | http://arxiv.org/abs/1503.08316 | id:1503.08316 author:Aurelien Lucchi, Brian McWilliams, Thomas Hofmann category:cs.LG  published:2015-03-28 summary:Quasi-Newton methods are widely used in practise for convex loss minimization problems. These methods exhibit good empirical performance on a wide variety of tasks and enjoy super-linear convergence to the optimal solution. For large-scale learning problems, stochastic Quasi-Newton methods have been recently proposed. However, these typically only achieve sub-linear convergence rates and have not been shown to consistently perform well in practice since noisy Hessian approximations can exacerbate the effect of high-variance stochastic gradient estimates. In this work we propose Vite, a novel stochastic Quasi-Newton algorithm that uses an existing first-order technique to reduce this variance. Without exploiting the specific form of the approximate Hessian, we show that Vite reaches the optimum at a geometric rate with a constant step-size when dealing with smooth strongly convex functions. Empirically, we demonstrate improvements over existing stochastic Quasi-Newton and variance reduced stochastic gradient methods. version:4
arxiv-1506-03041 | The Wreath Process: A totally generative model of geometric shape based on nested symmetries | http://arxiv.org/abs/1506.03041 | id:1506.03041 author:Diana Borsa, Thore Graepel, Andrew Gordon category:cs.AI stat.ML 20-XX  published:2015-06-09 summary:We consider the problem of modelling noisy but highly symmetric shapes that can be viewed as hierarchies of whole-part relationships in which higher level objects are composed of transformed collections of lower level objects. To this end, we propose the stochastic wreath process, a fully generative probabilistic model of drawings. Following Leyton's "Generative Theory of Shape", we represent shapes as sequences of transformation groups composed through a wreath product. This representation emphasizes the maximization of transfer --- the idea that the most compact and meaningful representation of a given shape is achieved by maximizing the re-use of existing building blocks or parts. The proposed stochastic wreath process extends Leyton's theory by defining a probability distribution over geometric shapes in terms of noise processes that are aligned with the generative group structure of the shape. We propose an inference scheme for recovering the generative history of given images in terms of the wreath process using reversible jump Markov chain Monte Carlo methods and Approximate Bayesian Computation. In the context of sketching we demonstrate the feasibility and limitations of this approach on model-generated and real data. version:1
arxiv-1506-03018 | On the Uniform Convergence of Consistent Confidence Measures | http://arxiv.org/abs/1506.03018 | id:1506.03018 author:Yihan Gao, Aditya Parameswaran category:cs.LG  published:2015-06-09 summary:Many classification algorithms produce confidence measures in the form of conditional probability of labels given the features of the target instance. It is desirable to be make these confidence measures calibrated or consistent, in the sense that they correctly capture the belief of the algorithm in the label output. For instance, if the algorithm outputs a label with confidence measure $p$ for $n$ times, then the output label should be correct approximately $np$ times overall. Calibrated confidence measures lead to higher interpretability by humans and computers and enable downstream analysis or processing. In this paper, we formally characterize the consistency of confidence measures and prove a PAC-style uniform convergence result for the consistency of confidence measures. We show that finite VC-dimension is sufficient for guaranteeing the consistency of confidence measures produced by empirically consistent classifiers. Our result also implies that we can calibrate confidence measures produced by any existing algorithms with monotonic functions, and still get the same generalization guarantee on consistency. version:1
arxiv-1505-00428 | A Linear-Time Particle Gibbs Sampler for Infinite Hidden Markov Models | http://arxiv.org/abs/1505.00428 | id:1505.00428 author:Nilesh Tripuraneni, Shane Gu, Hong Ge, Zoubin Ghahramani category:stat.ML  published:2015-05-03 summary:Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametric generalization of the classical Hidden Markov Model which can automatically infer the number of hidden states in the system. However, due to the infinite-dimensional nature of transition dynamics performing inference in the iHMM is difficult. In this paper, we present an infinite-state Particle Gibbs (PG) algorithm to resample state trajectories for the iHMM. The proposed algorithm uses an efficient proposal optimized for iHMMs and leverages ancestor sampling to suppress degeneracy of the standard PG algorithm. Our algorithm demonstrates significant convergence improvements on synthetic and real world data sets. Additionally, the infinite-state PG algorithm has linear-time complexity in the number of states in the sampler, while competing methods scale quadratically. version:2
arxiv-1506-02923 | Compact Shape Trees: A Contribution to the Forest of Shape Correspondences and Matching Methods | http://arxiv.org/abs/1506.02923 | id:1506.02923 author:Abdulrahman Oladipupo Ibraheem category:cs.CV  published:2015-06-09 summary:We propose a novel technique, termed compact shape trees, for computing correspondences of single-boundary 2-D shapes in O(n2) time. Together with zero or more features defined at each of n sample points on the shape's boundary, the compact shape tree of a shape comprises the O(n) collection of vectors emanating from any of the sample points on the shape's boundary to the rest of the sample points on the boundary. As it turns out, compact shape trees have a number of elegant properties both in the spatial and frequency domains. In particular, via a simple vector-algebraic argument, we show that the O(n) collection of vectors in a compact shape tree possesses at least the same discriminatory power as the O(n2) collection of lines emanating from each sample point to every other sample point on a shape's boundary. In addition, we describe neat approaches for achieving scale and rotation invariance with compact shape trees in the spatial domain; by viewing compact shape trees as aperiodic discrete signals, we also prove scale and rotation invariance properties for them in the Fourier domain. Towards these, along the way, using concepts from differential geometry and the Calculus, we propose a novel theory for sampling 2-D shape boundaries in a scale and rotation invariant manner. Finally, we propose a number of shape recognition experiments to test the efficacy of our concept. version:1
arxiv-1506-02922 | An Ensemble method for Content Selection for Data-to-text Systems | http://arxiv.org/abs/1506.02922 | id:1506.02922 author:Dimitra Gkatzia, Helen Hastie category:cs.CL cs.AI  published:2015-06-09 summary:We present a novel approach for automatic report generation from time-series data, in the context of student feedback generation. Our proposed methodology treats content selection as a multi-label classification (MLC) problem, which takes as input time-series data (students' learning data) and outputs a summary of these data (feedback). Unlike previous work, this method considers all data simultaneously using ensembles of classifiers, and therefore, it achieves higher accuracy and F- score compared to meaningful baselines. version:1
arxiv-1411-4280 | Efficient Object Localization Using Convolutional Networks | http://arxiv.org/abs/1411.4280 | id:1411.4280 author:Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christopher Bregler category:cs.CV  published:2014-11-16 summary:Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset. version:3
arxiv-1505-06897 | Times series averaging from a probabilistic interpretation of time-elastic kernel | http://arxiv.org/abs/1505.06897 | id:1505.06897 author:Pierre-François Marteau category:cs.LG cs.DS  published:2015-05-26 summary:At the light of regularized dynamic time warping kernels, this paper reconsider the concept of time elastic centroid (TEC) for a set of time series. From this perspective, we show first how TEC can easily be addressed as a preimage problem. Unfortunately this preimage problem is ill-posed, may suffer from over-fitting especially for long time series and getting a sub-optimal solution involves heavy computational costs. We then derive two new algorithms based on a probabilistic interpretation of kernel alignment matrices that expresses in terms of probabilistic distributions over sets of alignment paths. The first algorithm is an iterative agglomerative heuristics inspired from the state of the art DTW barycenter averaging (DBA) algorithm proposed specifically for the Dynamic Time Warping measure. The second proposed algorithm achieves a classical averaging of the aligned samples but also implements an averaging of the time of occurrences of the aligned samples. It exploits a straightforward progressive agglomerative heuristics. An experimentation that compares for 45 time series datasets classification error rates obtained by first near neighbors classifiers exploiting a single medoid or centroid estimate to represent each categories show that: i) centroids based approaches significantly outperform medoids based approaches, ii) on the considered experience, the two proposed algorithms outperform the state of the art DBA algorithm, and iii) the second proposed algorithm that implements an averaging jointly in the sample space and along the time axes emerges as the most significantly robust time elastic averaging heuristic with an interesting noise reduction capability. Index Terms-Time series averaging Time elastic kernel Dynamic Time Warping Time series clustering and classification. version:3
arxiv-1502-04843 | Generalized Gradient Learning on Time Series under Elastic Transformations | http://arxiv.org/abs/1502.04843 | id:1502.04843 author:Brijnesh Jain category:cs.LG  published:2015-02-17 summary:The majority of machine learning algorithms assumes that objects are represented as vectors. But often the objects we want to learn on are more naturally represented by other data structures such as sequences and time series. For these representations many standard learning algorithms are unavailable. We generalize gradient-based learning algorithms to time series under dynamic time warping. To this end, we introduce elastic functions, which extend functions on time series to matrix spaces. Necessary conditions are presented under which generalized gradient learning on time series is consistent. We indicate how results carry over to arbitrary elastic distance functions and to sequences consisting of symbolic elements. Specifically, four linear classifiers are extended to time series under dynamic time warping and applied to benchmark datasets. Results indicate that generalized gradient learning via elastic functions have the potential to complement the state-of-the-art in statistical pattern recognition on time series. version:2
arxiv-1503-02551 | Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages | http://arxiv.org/abs/1503.02551 | id:1503.02551 author:Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, S. M. Ali Eslami, Balaji Lakshminarayanan, Dino Sejdinovic, Zoltán Szabó category:stat.ML cs.LG G.3; I.2.6  published:2015-03-09 summary:We propose an efficient nonparametric strategy for learning a message operator in expectation propagation (EP), which takes as input the set of incoming messages to a factor node, and produces an outgoing message as output. This learned operator replaces the multivariate integral required in classical EP, which may not have an analytic expression. We use kernel-based regression, which is trained on a set of probability distributions representing the incoming messages, and the associated outgoing messages. The kernel approach has two main advantages: first, it is fast, as it is implemented using a novel two-layer random feature representation of the input message distributions; second, it has principled uncertainty estimates, and can be cheaply updated online, meaning it can request and incorporate new training data when it encounters inputs on which it is uncertain. In experiments, our approach is able to solve learning problems where a single message operator is required for multiple, substantially different data sets (logistic regression for a variety of classification problems), where it is essential to accurately assess uncertainty and to efficiently and robustly update the message operator. version:2
arxiv-1406-4363 | Distributed Stochastic Optimization of the Regularized Risk | http://arxiv.org/abs/1406.4363 | id:1406.4363 author:Shin Matsushima, Hyokun Yun, Xinhua Zhang, S. V. N. Vishwanathan category:stat.ML cs.LG  published:2014-06-17 summary:Many machine learning algorithms minimize a regularized risk, and stochastic optimization is widely used for this task. When working with massive data, it is desirable to perform stochastic optimization in parallel. Unfortunately, many existing stochastic optimization algorithms cannot be parallelized efficiently. In this paper we show that one can rewrite the regularized risk minimization problem as an equivalent saddle-point problem, and propose an efficient distributed stochastic optimization (DSO) algorithm. We prove the algorithm's rate of convergence; remarkably, our analysis shows that the algorithm scales almost linearly with the number of processors. We also verify with empirical evaluations that the proposed algorithm is competitive with other parallel, general purpose stochastic and batch optimization algorithms for regularized risk minimization. version:2
arxiv-1506-02785 | On the Error of Random Fourier Features | http://arxiv.org/abs/1506.02785 | id:1506.02785 author:Dougal J. Sutherland, Jeff Schneider category:cs.LG stat.ML  published:2015-06-09 summary:Kernel methods give powerful, flexible, and theoretically grounded approaches to solving many problems in machine learning. The standard approach, however, requires pairwise evaluations of a kernel function, which can lead to scalability issues for very large datasets. Rahimi and Recht (2007) suggested a popular approach to handling this problem, known as random Fourier features. The quality of this approximation, however, is not well understood. We improve the uniform error bound of that paper, as well as giving novel understandings of the embedding's variance, approximation error, and use in some machine learning methods. We also point out that surprisingly, of the two main variants of those features, the more widely used is strictly higher-variance for the Gaussian kernel and has worse bounds. version:1
arxiv-1506-02776 | Fast Geometric Fit Algorithm for Sphere Using Exact Solution | http://arxiv.org/abs/1506.02776 | id:1506.02776 author:Sumith YD category:cs.CV  published:2015-06-09 summary:Sphere fitting is a common problem in almost all science and engineering disciplines. Most of methods available are iterative in behavior. This involves fitting of the parameters in a least square sense or in a geometric sense. Here we extend the methods of Thomas Chan and Landau who fitted the 2D data using circle. This work closely resemble their work in redefining the error estimate and solving the sphere fitting problem exactly. The solutions for center and radius of the sphere can be found exactly and the equations can be hard coded for high performance. We have also shown some comparison with other popular methods and how this method behaves. version:1
arxiv-1506-02750 | Self Organizing Maps Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using Conditional Rotations | http://arxiv.org/abs/1506.02750 | id:1506.02750 author:César A. Astudillo, B. John Oommen category:cs.NE cs.AI  published:2015-06-09 summary:Numerous variants of Self-Organizing Maps (SOMs) have been proposed in the literature, including those which also possess an underlying structure, and in some cases, this structure itself can be defined by the user Although the concepts of growing the SOM and updating it have been studied, the whole issue of using a self-organizing Adaptive Data Structure (ADS) to further enhance the properties of the underlying SOM, has been unexplored. In an earlier work, we impose an arbitrary, user-defined, tree-like topology onto the codebooks, which consequently enforced a neighborhood phenomenon and the so-called tree-based Bubble of Activity. In this paper, we consider how the underlying tree itself can be rendered dynamic and adaptively transformed. To do this, we present methods by which a SOM with an underlying Binary Search Tree (BST) structure can be adaptively re-structured using Conditional Rotations (CONROT). These rotations on the nodes of the tree are local, can be done in constant time, and performed so as to decrease the Weighted Path Length (WPL) of the entire tree. In doing this, we introduce the pioneering concept referred to as Neural Promotion, where neurons gain prominence in the Neural Network (NN) as their significance increases. We are not aware of any research which deals with the issue of Neural Promotion. The advantages of such a scheme is that the user need not be aware of any of the topological peculiarities of the stochastic data distribution. Rather, the algorithm, referred to as the TTOSOM with Conditional Rotations (TTOCONROT), converges in such a manner that the neurons are ultimately placed in the input space so as to represent its stochastic distribution, and additionally, the neighborhood properties of the neurons suit the best BST that represents the data. These properties have been confirmed by our experimental results on a variety of data sets. version:1
arxiv-1502-01368 | Sparse Representation Classification Beyond L1 Minimization and the Subspace Assumption | http://arxiv.org/abs/1502.01368 | id:1502.01368 author:Cencheng Shen, Li Chen, Carey E. Priebe category:stat.ML  published:2015-02-04 summary:The sparse representation classifier (SRC) proposed in Wright et al. (2009) has recently gained much attention from the machine learning community. It makes use of L1 minimization, and is known to work well for data satisfying a subspace assumption. In this paper, we use the notion of class dominance as well as a principal angle condition to investigate and validate the classification performance of SRC, without relying on L1 minimization and the subspace assumption. We prove that SRC can still work well using faster subset regression methods such as orthogonal matching pursuit and marginal regression, and its applicability is not limited to data satisfying the subspace assumption. We illustrate our theorems via various real data sets including face images, text features, and network data. version:2
arxiv-1506-02732 | Empirical Studies on Symbolic Aggregation Approximation Under Statistical Perspectives for Knowledge Discovery in Time Series | http://arxiv.org/abs/1506.02732 | id:1506.02732 author:Wei Song, Zhiguang Wang, Yangdong Ye, Ming Fan category:cs.LG cs.IT math.IT  published:2015-06-08 summary:Symbolic Aggregation approXimation (SAX) has been the de facto standard representation methods for knowledge discovery in time series on a number of tasks and applications. So far, very little work has been done in empirically investigating the intrinsic properties and statistical mechanics in SAX words. In this paper, we applied several statistical measurements and proposed a new statistical measurement, i.e. information embedding cost (IEC) to analyze the statistical behaviors of the symbolic dynamics. Our experiments on the benchmark datasets and the clinical signals demonstrate that SAX can always reduce the complexity while preserving the core information embedded in the original time series with significant embedding efficiency. Our proposed IEC score provide a priori to determine if SAX is adequate for specific dataset, which can be generalized to evaluate other symbolic representations. Our work provides an analytical framework with several statistical tools to analyze, evaluate and further improve the symbolic dynamics for knowledge discovery in time series. version:1
arxiv-1506-02719 | Non-parametric Revenue Optimization for Generalized Second Price Auctions | http://arxiv.org/abs/1506.02719 | id:1506.02719 author:Mehryar Mohri, Andres Munoz Medina category:cs.LG cs.GT  published:2015-06-08 summary:We present an extensive analysis of the key problem of learning optimal reserve prices for generalized second price auctions. We describe two algorithms for this task: one based on density estimation, and a novel algorithm benefiting from solid theoretical guarantees and with a very favorable running-time complexity of $O(n S \log (n S))$, where $n$ is the sample size and $S$ the number of slots. Our theoretical guarantees are more favorable than those previously presented in the literature. Additionally, we show that even if bidders do not play at an equilibrium, our second algorithm is still well defined and minimizes a quantity of interest. To our knowledge, this is the first attempt to apply learning algorithms to the problem of reserve price optimization in GSP auctions. Finally, we present the first convergence analysis of empirical equilibrium bidding functions to the unique symmetric Bayesian-Nash equilibrium of a GSP. version:1
arxiv-1411-0292 | Population Empirical Bayes | http://arxiv.org/abs/1411.0292 | id:1411.0292 author:Alp Kucukelbir, David M. Blei category:stat.ML cs.LG  published:2014-11-02 summary:Bayesian predictive inference analyzes a dataset to make predictions about new observations. When a model does not match the data, predictive accuracy suffers. We develop population empirical Bayes (POP-EB), a hierarchical framework that explicitly models the empirical population distribution as part of Bayesian analysis. We introduce a new concept, the latent dataset, as a hierarchical variable and set the empirical population as its prior. This leads to a new predictive density that mitigates model mismatch. We efficiently apply this method to complex models by proposing a stochastic variational inference algorithm, called bumping variational inference (BUMP-VI). We demonstrate improved predictive accuracy over classical Bayesian inference in three models: a linear regression model of health data, a Bayesian mixture model of natural images, and a latent Dirichlet allocation topic model of scientific documents. version:2
arxiv-1506-01069 | A CMOS Spiking Neuron for Dense Memristor-Synapse Connectivity for Brain-Inspired Computing | http://arxiv.org/abs/1506.01069 | id:1506.01069 author:Xinyu Wu, Vishal Saxena, Kehan Zhu category:cs.NE cs.ET  published:2015-06-02 summary:Neuromorphic systems that densely integrate CMOS spiking neurons and nano-scale memristor synapses open a new avenue of brain-inspired computing. Existing silicon neurons have molded neural biophysical dynamics but are incompatible with memristor synapses, or used extra training circuitry thus eliminating much of the density advantages gained by using memristors, or were energy inefficient. Here we describe a novel CMOS spiking leaky integrate-and-fire neuron circuit. Building on a reconfigurable architecture with a single opamp, the described neuron accommodates a large number of memristor synapses, and enables online spike timing dependent plasticity (STDP) learning with optimized power consumption. Simulation results of an 180nm CMOS design showed 97% power efficiency metric when realizing STDP learning in 10,000 memristor synapses with a nominal 1M{\Omega} memristance, and only 13{\mu}A current consumption when integrating input spikes. Therefore, the described CMOS neuron contributes a generalized building block for large-scale brain-inspired neuromorphic systems. version:2
arxiv-1506-01072 | Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition | http://arxiv.org/abs/1506.01072 | id:1506.01072 author:Xinyu Wu, Vishal Saxena, Kehan Zhu category:cs.NE cs.AI cs.CV cs.ET  published:2015-06-02 summary:A neuromorphic chip that combines CMOS analog spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different waveforms for pre and post-synaptic spikes that added undesirable circuit overhead. Here we describe a hardware architecture that can feature a large number of memristor synapses to learn real-world patterns. We present a versatile CMOS neuron that combines integrate-and-fire behavior, drives passive memristors and implements competitive learning in a compact circuit module, and enables in-situ plasticity in the memristor synapses. We demonstrate handwritten-digits recognition using the proposed architecture using transistor-level circuit simulations. As the described neuromorphic architecture is homogeneous, it realizes a fundamental building block for large-scale energy-efficient brain-inspired silicon chips that could lead to next-generation cognitive computing. version:2
arxiv-1506-02686 | The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal Prediction | http://arxiv.org/abs/1506.02686 | id:1506.02686 author:George D. Montanez, Cosma Rohilla Shalizi category:stat.ML cs.LG  published:2015-06-08 summary:For the task of unsupervised spatio-temporal forecasting (e.g., learning to predict video data without labels), we propose two new nonparametric predictive state algorithms, Moonshine and One Hundred Proof. The algorithms are conceptually simple and make few assumptions on the underlying spatio-temporal process yet have strong predictive performance and provide predictive distributions over spatio-temporal data. The latter property allows for likelihood estimation under the models, for classification and other probabilistic inference. version:1
arxiv-1506-02633 | A Topological Approach to Spectral Clustering | http://arxiv.org/abs/1506.02633 | id:1506.02633 author:Antonio Rieser category:cs.LG stat.ML  published:2015-06-08 summary:We propose a clustering algorithm which, for input, takes data assumed to be sampled from a uniform distribution supported on a metric space $X$, and outputs a clustering of the data based on a topological estimate of the connected components of $X$. The algorithm works by choosing a weighted graph on the samples from a natural one-parameter family of graphs using an error based on the heat operator on the graphs. The estimated connected components of $X$ are identified as the support of the eigenfunctions of the heat operator with eigenvalue $1$, which allows the algorithm to work without requiring the number of expected clusters as input. version:1
arxiv-1410-5518 | On Symmetric and Asymmetric LSHs for Inner Product Search | http://arxiv.org/abs/1410.5518 | id:1410.5518 author:Behnam Neyshabur, Nathan Srebro category:stat.ML cs.DS cs.IR cs.LG  published:2014-10-21 summary:We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required. version:3
arxiv-1403-5341 | An Information-Theoretic Analysis of Thompson Sampling | http://arxiv.org/abs/1403.5341 | id:1403.5341 author:Daniel Russo, Benjamin Van Roy category:cs.LG  published:2014-03-21 summary:We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance. version:2
arxiv-1506-02617 | Path-SGD: Path-Normalized Optimization in Deep Neural Networks | http://arxiv.org/abs/1506.02617 | id:1506.02617 author:Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.CV cs.NE stat.ML  published:2015-06-08 summary:We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad. version:1
arxiv-1506-02585 | Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection | http://arxiv.org/abs/1506.02585 | id:1506.02585 author:Zhimin Peng, Prudhvi Gurram, Heesung Kwon, Wotao Yin category:cs.LG  published:2015-06-08 summary:In this paper, a novel framework of sparse kernel learning for Support Vector Data Description (SVDD) based anomaly detection is presented. In this work, optimal sparse feature selection for anomaly detection is first modeled as a Mixed Integer Programming (MIP) problem. Due to the prohibitively high computational complexity of the MIP, it is relaxed into a Quadratically Constrained Linear Programming (QCLP) problem. The QCLP problem can then be practically solved by using an iterative optimization method, in which multiple subsets of features are iteratively found as opposed to a single subset. The QCLP-based iterative optimization problem is solved in a finite space called the \emph{Empirical Kernel Feature Space} (EKFS) instead of in the input space or \emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because of the fact that the geometrical properties of the EKFS and the corresponding RKHS remain the same. Now, an explicit nonlinear exploitation of the data in a finite EKFS is achievable, which results in optimal feature ranking. Experimental results based on a hyperspectral image show that the proposed method can provide improved performance over the current state-of-the-art techniques. version:1
arxiv-1506-03128 | License Plate Recognition System Based on Color Coding Of License Plates | http://arxiv.org/abs/1506.03128 | id:1506.03128 author:Jani Biju Babjan category:cs.CV  published:2015-06-08 summary:License Plate Recognition Systems are used to determine the license plate number of a vehicle. The current system mainly uses Optical Character Recognition to recognize the number plate. There are several problems to this system. Some of them include interchanging of several letters or numbers (letter O with digit 0), difficulty in localizing the license plate, high error rate, use of different fonts in license plates etc. So a new system to recognize the license plate number using color coding of license plates is proposed in this paper. Easier localization of license plate can be done by searching for the start or stop patters of license plates. An eight segment display system along with traditional numbering with the first and last segments left for start or stop patterns is proposed in this paper. Practical applications include several areas under Internet of Things (IoT). version:1
arxiv-1506-02649 | Faster SGD Using Sketched Conditioning | http://arxiv.org/abs/1506.02649 | id:1506.02649 author:Alon Gonen, Shai Shalev-Shwartz category:cs.NA cs.LG  published:2015-06-08 summary:We propose a novel method for speeding up stochastic optimization algorithms via sketching methods, which recently became a powerful tool for accelerating algorithms for numerical linear algebra. We revisit the method of conditioning for accelerating first-order methods and suggest the use of sketching methods for constructing a cheap conditioner that attains a significant speedup with respect to the Stochastic Gradient Descent (SGD) algorithm. While our theoretical guarantees assume convexity, we discuss the applicability of our method to deep neural networks, and experimentally demonstrate its merits. version:1
arxiv-1506-02530 | Linear Convergence of the Randomized Feasible Descent Method Under the Weak Strong Convexity Assumption | http://arxiv.org/abs/1506.02530 | id:1506.02530 author:Chenxin Ma, Rachael Tappenden, Martin Takáč category:cs.LG stat.ML  published:2015-06-08 summary:In this paper we generalize the framework of the feasible descent method (FDM) to a randomized (R-FDM) and a coordinate-wise random feasible descent method (RC-FDM) framework. We show that the famous SDCA algorithm for optimizing the SVM dual problem, or the stochastic coordinate descent method for the LASSO problem, fits into the framework of RC-FDM. We prove linear convergence for both R-FDM and RC-FDM under the weak strong convexity assumption. Moreover, we show that the duality gap converges linearly for RC-FDM, which implies that the duality gap also converges linearly for SDCA applied to the SVM dual problem. version:1
arxiv-1506-02520 | Convex recovery of tensors using nuclear norm penalization | http://arxiv.org/abs/1506.02520 | id:1506.02520 author:Stephane Chretien, Tianwen Wei category:stat.ML  published:2015-06-08 summary:The subdifferential of convex functions of the singular spectrum of real matrices has been widely studied in matrix analysis, optimization and automatic control theory. Convex analysis and optimization over spaces of tensors is now gaining much interest due to its potential applications to signal processing, statistics and engineering. The goal of this paper is to present an applications to the problem of low rank tensor recovery based on linear random measurement by extending the results of Tropp to the tensors setting. version:1
arxiv-1506-02510 | Learning Mixtures of Ising Models using Pseudolikelihood | http://arxiv.org/abs/1506.02510 | id:1506.02510 author:Onur Dikmen category:cs.LG stat.ML  published:2015-06-08 summary:Maximum pseudolikelihood method has been among the most important methods for learning parameters of statistical physics models, such as Ising models. In this paper, we study how pseudolikelihood can be derived for learning parameters of a mixture of Ising models. The performance of the proposed approach is demonstrated for Ising and Potts models on both synthetic and real data. version:1
arxiv-1506-02509 | SVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from ImageNet | http://arxiv.org/abs/1506.02509 | id:1506.02509 author:Lei Zhang, David Zhang category:cs.LG cs.CV  published:2015-06-08 summary:Deep learning with a convolutional neural network (CNN) has been proved to be very effective in feature extraction and representation of images. For image classification problems, this work aim at finding which classifier is more competitive based on high-level deep features of images. In this report, we have discussed the nearest neighbor, support vector machines and extreme learning machines for image classification under deep convolutional activation feature representation. Specifically, we adopt the benchmark object recognition dataset from multiple sources with domain bias for evaluating different classifiers. The deep features of the object dataset are obtained by a well-trained CNN with five convolutional layers and three fully-connected layers on the challenging ImageNet. Experiments demonstrate that the ELMs outperform SVMs in cross-domain recognition tasks. In particular, state-of-the-art results are obtained by kernel ELM which outperforms SVMs with about 4% of the average accuracy. The features and codes are available in http://www.escience.cn/people/lei/index.html version:1
arxiv-1311-4825 | Gaussian Process Optimization with Mutual Information | http://arxiv.org/abs/1311.4825 | id:1311.4825 author:Emile Contal, Vianney Perchet, Nicolas Vayatis category:stat.ML cs.LG  published:2013-11-19 summary:In this paper, we analyze a generic algorithm scheme for sequential global optimization using Gaussian processes. The upper bounds we derive on the cumulative regret for this generic algorithm improve by an exponential factor the previously known bounds for algorithms like GP-UCB. We also introduce the novel Gaussian Process Mutual Information algorithm (GP-MI), which significantly improves further these upper bounds for the cumulative regret. We confirm the efficiency of this algorithm on synthetic and real tasks against the natural competitor, GP-UCB, and also the Expected Improvement heuristic. version:3
arxiv-1506-02432 | Reflection Invariance: an important consideration of image orientation | http://arxiv.org/abs/1506.02432 | id:1506.02432 author:Craig Henderson, Ebroul Izquierdo category:cs.CV  published:2015-06-08 summary:In this position paper, we consider the state of computer vision research with respect to invariance to the horizontal orientation of an image -- what we term reflection invariance. We describe why we consider reflection invariance to be an important property and provide evidence where the absence of this invariance produces surprising inconsistencies in state-of-the-art systems. We demonstrate inconsistencies in methods of object detection and scene classification when they are presented with images and the horizontal mirror of those images. Finally, we examine where some of the invariance is exhibited in feature detection and descriptors, and make a case for future consideration of reflection invariance as a measure of quality in computer vision algorithms. version:1
arxiv-1506-02428 | Robust Regression via Hard Thresholding | http://arxiv.org/abs/1506.02428 | id:1506.02428 author:Kush Bhatia, Prateek Jain, Purushottam Kar category:cs.LG stat.ML  published:2015-06-08 summary:We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X \in R^{p x n} and an underlying model w*, the response vector is generated as y = X'w* + b where b \in R^n is the corruption vector supported over at most C.n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X. In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* \in R^p. Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20x faster than the best L1 solver. version:1
