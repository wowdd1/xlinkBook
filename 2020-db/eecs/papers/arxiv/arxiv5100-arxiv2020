arxiv-1403-1310 | AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based Assignments | http://arxiv.org/abs/1403.1310 | id:1403.1310 author:M. A. C. Jiffriya, M. A. C. Akmal Jahan, R. G. Ragel, S. Deegalla category:cs.IR cs.CL cs.DL  published:2014-03-06 summary:Plagiarism is one of the growing issues in academia and is always a concern in Universities and other academic institutions. The situation is becoming even worse with the availability of ample resources on the web. This paper focuses on creating an effective and fast tool for plagiarism detection for text based electronic assignments. Our plagiarism detection tool named AntiPlag is developed using the tri-gram sequence matching technique. Three sets of text based assignments were tested by AntiPlag and the results were compared against an existing commercial plagiarism detection tool. AntiPlag showed better results in terms of false positives compared to the commercial tool due to the pre-processing steps performed in AntiPlag. In addition, to improve the detection latency, AntiPlag applies a data clustering technique making it four times faster than the commercial tool considered. AntiPlag could be used to isolate plagiarized text based assignments from non-plagiarised assignments easily. Therefore, we present AntiPlag, a fast and effective tool for plagiarism detection on text based electronic assignments. version:1
arxiv-1311-6079 | Local Similarities, Global Coding: An Algorithm for Feature Coding and its Applications | http://arxiv.org/abs/1311.6079 | id:1311.6079 author:Amirreza Shaban, Hamid R. Rabiee, Mahyar Najibi category:cs.CV cs.AI  published:2013-11-24 summary:Data coding as a building block of several image processing algorithms has been received great attention recently. Indeed, the importance of the locality assumption in coding approaches is studied in numerous works and several methods are proposed based on this concept. We probe this assumption and claim that taking the similarity between a data point and a more global set of anchor points does not necessarily weaken the coding method as long as the underlying structure of the anchor points are taken into account. Based on this fact, we propose to capture this underlying structure by assuming a random walker over the anchor points. We show that our method is a fast approximate learning algorithm based on the diffusion map kernel. The experiments on various datasets show that making different state-of-the-art coding algorithms aware of this structure boosts them in different learning tasks. version:2
arxiv-1311-0701 | On Fast Dropout and its Applicability to Recurrent Networks | http://arxiv.org/abs/1311.0701 | id:1311.0701 author:Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, Patrick van der Smagt category:stat.ML cs.LG cs.NE  published:2013-11-04 summary:Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients. The control of overfitting has seen considerably less attention. This paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. The derivatives of that regularizer are exclusively based on the training error signal. One consequence of this is the absense of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets. version:7
arxiv-1310-0865 | Electricity Market Forecasting via Low-Rank Multi-Kernel Learning | http://arxiv.org/abs/1310.0865 | id:1310.0865 author:Vassilis Kekatos, Yu Zhang, Georgios B. Giannakis category:stat.ML cs.LG cs.SY  published:2013-10-02 summary:The smart grid vision entails advanced information technology and data analytics to enhance the efficiency, sustainability, and economics of the power grid infrastructure. Aligned to this end, modern statistical learning tools are leveraged here for electricity market inference. Day-ahead price forecasting is cast as a low-rank kernel learning problem. Uniquely exploiting the market clearing process, congestion patterns are modeled as rank-one components in the matrix of spatio-temporally varying prices. Through a novel nuclear norm-based regularization, kernels across pricing nodes and hours can be systematically selected. Even though market-wide forecasting is beneficial from a learning perspective, it involves processing high-dimensional market data. The latter becomes possible after devising a block-coordinate descent algorithm for solving the non-convex optimization problem involved. The algorithm utilizes results from block-sparse vector recovery and is guaranteed to converge to a stationary point. Numerical tests on real data from the Midwest ISO (MISO) market corroborate the prediction accuracy, computational efficiency, and the interpretative merits of the developed approach over existing alternatives. version:2
arxiv-1403-1194 | Latent Semantic Word Sense Disambiguation Using Global Co-occurrence Information | http://arxiv.org/abs/1403.1194 | id:1403.1194 author:Minoru Sasaki category:cs.CL cs.IR  published:2014-03-05 summary:In this paper, I propose a novel word sense disambiguation method based on the global co-occurrence information using NMF. When I calculate the dependency relation matrix, the existing method tends to produce very sparse co-occurrence matrix from a small training set. Therefore, the NMF algorithm sometimes does not converge to desired solutions. To obtain a large number of co-occurrence relations, I propose to use co-occurrence frequencies of dependency relations between word features in the whole training set. This enables us to solve data sparseness problem and induce more effective latent features. To evaluate the efficiency of the method of word sense disambiguation, I make some experiments to compare with the result of the two baseline methods. The results of the experiments show this method is effective for word sense disambiguation in comparison with the all baseline methods. Moreover, the proposed method is effective for obtaining a stable effect by analyzing the global co-occurrence information. version:1
arxiv-1403-0801 | Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring | http://arxiv.org/abs/1403.0801 | id:1403.0801 author:Derrick Higgins, Chris Brew, Michael Heilman, Ramon Ziai, Lei Chen, Aoife Cahill, Michael Flor, Nitin Madnani, Joel Tetreault, Daniel Blanchard, Diane Napolitano, Chong Min Lee, John Blackmore category:cs.CL  published:2014-03-04 summary:Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach. version:2
arxiv-1403-1073 | Artificial Neuron Modelling Based on Wave Shape | http://arxiv.org/abs/1403.1073 | id:1403.1073 author:Kieran Greer category:cs.NE  published:2014-03-05 summary:This paper describes a new model for an artificial neural network processing unit or neuron. It is slightly different to a traditional feedforward network by the fact that it favours a mechanism of trying to match the wave-like 'shape' of the input with the shape of the output against specific value error corrections. The expectation is then that a best fit shape can be transposed into the desired output values more easily. This allows for notions of reinforcement through resonance and also the construction of synapses. version:1
arxiv-1302-4886 | Fast methods for denoising matrix completion formulations, with applications to robust seismic data interpolation | http://arxiv.org/abs/1302.4886 | id:1302.4886 author:Aleksandr Y. Aravkin, Rajiv Kumar, Hassan Mansour, Ben Recht, Felix J. Herrmann category:stat.ML cs.LG 62F35  65K10  published:2013-02-20 summary:Recent SVD-free matrix factorization formulations have enabled rank minimization for systems with millions of rows and columns, paving the way for matrix completion in extremely large-scale applications, such as seismic data interpolation. In this paper, we consider matrix completion formulations designed to hit a target data-fitting error level provided by the user, and propose an algorithm called LR-BPDN that is able to exploit factorized formulations to solve the corresponding optimization problem. Since practitioners typically have strong prior knowledge about target error level, this innovation makes it easy to apply the algorithm in practice, leaving only the factor rank to be determined. Within the established framework, we propose two extensions that are highly relevant to solving practical challenges of data interpolation. First, we propose a weighted extension that allows known subspace information to improve the results of matrix completion formulations. We show how this weighting can be used in the context of frequency continuation, an essential aspect to seismic data interpolation. Second, we propose matrix completion formulations that are robust to large measurement errors in the available data. We illustrate the advantages of LR-BPDN on the collaborative filtering problem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we use the new method, along with its robust and subspace re-weighted extensions, to obtain high-quality reconstructions for large scale seismic interpolation problems with real data, even in the presence of data contamination. version:3
arxiv-1403-1056 | K-Tangent Spaces on Riemannian Manifolds for Improved Pedestrian Detection | http://arxiv.org/abs/1403.1056 | id:1403.1056 author:Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell category:cs.CV  published:2014-03-05 summary:For covariance-based image descriptors, taking into account the curvature of the corresponding feature space has been shown to improve discrimination performance. This is often done through representing the descriptors as points on Riemannian manifolds, with the discrimination accomplished on a tangent space. However, such treatment is restrictive as distances between arbitrary points on the tangent space do not represent true geodesic distances, and hence do not represent the manifold structure accurately. In this paper we propose a general discriminative model based on the combination of several tangent spaces, in order to preserve more details of the structure. The model can be used as a weak learner in a boosting-based pedestrian detection framework. Experiments on the challenging INRIA and DaimlerChrysler datasets show that the proposed model leads to considerably higher performance than methods based on histograms of oriented gradients as well as previous Riemannian-based techniques. version:1
arxiv-1309-0270 | High-Accuracy Total Variation for Compressed Video Sensing | http://arxiv.org/abs/1309.0270 | id:1309.0270 author:Mahdi S. Hosseini, Konstantinos N. Plataniotis category:math.OC cs.CV  published:2013-09-01 summary:Numerous total variation (TV) regularizers, engaged in image restoration problem, encode the gradients by means of simple $[-1,1]$ FIR filter. Despite its low computational processing, this filter severely deviates signal's high frequency components pertinent to edge/discontinuous information and cause several deficiency issues known as texture and geometric loss. This paper addresses this problem by proposing an alternative model to the TV regularization problem via high order accuracy differential FIR filters to preserve rapid transitions in signal recovery. A numerical encoding scheme is designed to extend the TV model into multidimensional representation (tensorial decomposition). We adopt this design to regulate the spatial and temporal redundancy in compressed video sensing problem to jointly recover frames from under-sampled measurements. We then seek the solution via alternating direction methods of multipliers and find a unique solution to quadratic minimization step with capability of handling different boundary conditions. The resulting algorithm uses much lower sampling rate and highly outperforms alternative state-of-the-art methods. This is evaluated both in terms of restoration accuracy and visual quality of the recovered frames. version:2
arxiv-1402-7025 | Exploiting the Statistics of Learning and Inference | http://arxiv.org/abs/1402.7025 | id:1402.7025 author:Max Welling category:cs.LG  published:2014-02-26 summary:When dealing with datasets containing a billion instances or with simulations that require a supercomputer to execute, computational resources become part of the equation. We can improve the efficiency of learning and inference by exploiting their inherent statistical nature. We propose algorithms that exploit the redundancy of data relative to a model by subsampling data-cases for every update and reasoning about the uncertainty created in this process. In the context of learning we propose to test for the probability that a stochastically estimated gradient points more than 180 degrees in the wrong direction. In the context of MCMC sampling we use stochastic gradients to improve the efficiency of MCMC updates, and hypothesis tests based on adaptive mini-batches to decide whether to accept or reject a proposed parameter update. Finally, we argue that in the context of likelihood free MCMC one needs to store all the information revealed by all simulations, for instance in a Gaussian process. We conclude that Bayesian methods will remain to play a crucial role in the era of big data and big simulations, but only if we overcome a number of computational challenges. version:2
arxiv-1403-0921 | Dynamic stochastic blockmodels for time-evolving social networks | http://arxiv.org/abs/1403.0921 | id:1403.0921 author:Kevin S. Xu, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ME G.3; G.2.2  published:2014-03-04 summary:Significant efforts have gone into the development of statistical models for analyzing data in the form of networks, such as social networks. Most existing work has focused on modeling static networks, which represent either a single time snapshot or an aggregate view over time. There has been recent interest in statistical modeling of dynamic networks, which are observed at multiple points in time and offer a richer representation of many complex phenomena. In this paper, we present a state-space model for dynamic networks that extends the well-known stochastic blockmodel for static networks to the dynamic setting. We fit the model in a near-optimal manner using an extended Kalman filter (EKF) augmented with a local search. We demonstrate that the EKF-based algorithm performs competitively with a state-of-the-art algorithm based on Markov chain Monte Carlo sampling but is significantly less computationally demanding. version:1
arxiv-1312-3005 | One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling | http://arxiv.org/abs/1312.3005 | id:1312.3005 author:Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson category:cs.CL  published:2013-12-11 summary:We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models. version:3
arxiv-1403-0873 | Matroid Regression | http://arxiv.org/abs/1403.0873 | id:1403.0873 author:Franz J Király, Louis Theran category:math.ST cs.DM cs.LG stat.ME stat.ML stat.TH  published:2014-03-04 summary:We propose an algebraic combinatorial method for solving large sparse linear systems of equations locally - that is, a method which can compute single evaluations of the signal without computing the whole signal. The method scales only in the sparsity of the system and not in its size, and allows to provide error estimates for any solution method. At the heart of our approach is the so-called regression matroid, a combinatorial object associated to sparsity patterns, which allows to replace inversion of the large matrix with the inversion of a kernel matrix that is constant size. We show that our method provides the best linear unbiased estimator (BLUE) for this setting and the minimum variance unbiased estimator (MVUE) under Gaussian noise assumptions, and furthermore we show that the size of the kernel matrix which is to be inverted can be traded off with accuracy. version:1
arxiv-1312-4354 | Decomposition of Optical Flow on the Sphere | http://arxiv.org/abs/1312.4354 | id:1312.4354 author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV  published:2013-12-16 summary:We propose a number of variational regularisation methods for the estimation and decomposition of motion fields on the $2$-sphere. While motion estimation is based on the optical flow equation, the presented decomposition models are motivated by recent trends in image analysis. In particular we treat $u+v$ decomposition as well as hierarchical decomposition. Helmholtz decomposition of motion fields is obtained as a natural by-product of the chosen numerical method based on vector spherical harmonics. All models are tested on time-lapse microscopy data depicting fluorescently labelled endodermal cells of a zebrafish embryo. version:2
arxiv-1403-0778 | Dynamic Move Chains -- a Forward Pruning Approach to Tree Search in Computer Chess | http://arxiv.org/abs/1403.0778 | id:1403.0778 author:Kieran Greer category:cs.AI cs.NE  published:2014-03-04 summary:This paper proposes a new mechanism for pruning a search game-tree in computer chess. The algorithm stores and then reuses chains or sequences of moves, built up from previous searches. These move sequences have a built-in forward-pruning mechanism that can radically reduce the search space. A typical search process might retrieve a move from a Transposition Table, where the decision of what move to retrieve would be based on the position itself. This algorithm stores move sequences based on what previous sequences were better, or caused cutoffs. This is therefore position independent and so it could also be useful in games with imperfect information or uncertainty, where the whole situation is not known at any one time. Over a small set of tests, the algorithm was shown to clearly out-perform Transposition Tables, both in terms of search reduction and game-play results. version:1
arxiv-1403-0745 | EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines | http://arxiv.org/abs/1403.0745 | id:1403.0745 author:Marc Claesen, Frank De Smet, Johan Suykens, Bart De Moor category:stat.ML cs.LG G.3; I.2.6; I.5.1  published:2014-03-04 summary:EnsembleSVM is a free software package containing efficient routines to perform ensemble learning with support vector machine (SVM) base models. It currently offers ensemble methods based on binary SVM models. Our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models. Experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy. The EnsembleSVM software package is freely available online at http://esat.kuleuven.be/stadius/ensemblesvm. version:1
arxiv-1403-0408 | On the Intersection Property of Conditional Independence and its Application to Causal Discovery | http://arxiv.org/abs/1403.0408 | id:1403.0408 author:Jonas Peters category:math.PR stat.ML  published:2014-03-03 summary:This work investigates the intersection property of conditional independence. It states that for random variables $A,B,C$ and $X$ we have that $X$ independent of $A$ given $B,C$ and $X$ independent of $B$ given $A,C$ implies $X$ independent of $(A,B)$ given $C$. Under the assumption that the joint distribution has a continuous density, we provide necessary and sufficient conditions under which the intersection property holds. The result has direct applications to causal inference: it leads to strictly weaker conditions under which the graphical structure becomes identifiable from the joint distribution of an additive noise model. version:2
arxiv-1403-0728 | A Novel Method for Vectorization | http://arxiv.org/abs/1403.0728 | id:1403.0728 author:Tolga Birdal, Emrah Bala category:cs.CV cs.CG cs.GR  published:2014-03-04 summary:Vectorization of images is a key concern uniting computer graphics and computer vision communities. In this paper we are presenting a novel idea for efficient, customizable vectorization of raster images, based on Catmull Rom spline fitting. The algorithm maintains a good balance between photo-realism and photo abstraction, and hence is applicable to applications with artistic concerns or applications where less information loss is crucial. The resulting algorithm is fast, parallelizable and can satisfy general soft realtime requirements. Moreover, the smoothness of the vectorized images aesthetically outperforms outputs of many polygon-based methods version:1
arxiv-1403-0700 | Random Projections on Manifolds of Symmetric Positive Definite Matrices for Image Classification | http://arxiv.org/abs/1403.0700 | id:1403.0700 author:Azadeh Alavi, Arnold Wiliem, Kun Zhao, Brian C. Lovell, Conrad Sanderson category:cs.CV stat.ML  published:2014-03-04 summary:Recent advances suggest that encoding images through Symmetric Positive Definite (SPD) matrices and then interpreting such matrices as points on Riemannian manifolds can lead to increased classification performance. Taking into account manifold geometry is typically done via (1) embedding the manifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert Spaces (RKHS). While embedding into tangent spaces allows the use of existing Euclidean-based learning algorithms, manifold shape is only approximated which can cause loss of discriminatory information. The RKHS approach retains more of the manifold structure, but may require non-trivial effort to kernelise Euclidean-based learning algorithms. In contrast to the above approaches, in this paper we offer a novel solution that allows SPD matrices to be used with unmodified Euclidean-based learning algorithms, with the true manifold shape well-preserved. Specifically, we propose to project SPD matrices using a set of random projection hyperplanes over RKHS into a random projection space, which leads to representing each matrix as a vector of projection coefficients. Experiments on face recognition, person re-identification and texture classification show that the proposed approach outperforms several recent methods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian Locality Preserving Projection and Relational Divergence Classification. version:1
arxiv-1403-0699 | Multi-Shot Person Re-Identification via Relational Stein Divergence | http://arxiv.org/abs/1403.0699 | id:1403.0699 author:Azadeh Alavi, Yan Yang, Mehrtash Harandi, Conrad Sanderson category:cs.CV stat.ML  published:2014-03-04 summary:Person re-identification is particularly challenging due to significant appearance changes across separate camera views. In order to re-identify people, a representative human signature should effectively handle differences in illumination, pose and camera parameters. While general appearance-based methods are modelled in Euclidean spaces, it has been argued that some applications in image and video analysis are better modelled via non-Euclidean manifold geometry. To this end, recent approaches represent images as covariance matrices, and interpret such matrices as points on Riemannian manifolds. As direct classification on such manifolds can be difficult, in this paper we propose to represent each manifold point as a vector of similarities to class representers, via a recently introduced form of Bregman matrix divergence known as the Stein divergence. This is followed by using a discriminative mapping of similarity vectors for final classification. The use of similarity vectors is in contrast to the traditional approach of embedding manifolds into tangent spaces, which can suffer from representing the manifold structure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDS datasets for the person re-identification task show that the proposed approach obtains better performance than recent techniques such as Histogram Plus Epitome, Partial Least Squares, and Symmetry-Driven Accumulation of Local Features. version:1
arxiv-1312-4400 | Network In Network | http://arxiv.org/abs/1312.4400 | id:1312.4400 author:Min Lin, Qiang Chen, Shuicheng Yan category:cs.NE cs.CV cs.LG  published:2013-12-16 summary:We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets. version:3
arxiv-1403-5475 | An Efficient Method for Face Recognition System In Various Assorted Conditions | http://arxiv.org/abs/1403.5475 | id:1403.5475 author:V. Karthikeyan, K. Vijayalakshmi, P. Jeyakumar category:cs.CV  published:2014-03-04 summary:In the beginning stage, face verification is done using easy method of geometric algorithm models, but the verification route has now developed into a scientific progress of complicated geometric representation and identical procedure. In recent years the technologies have boosted face recognition system into the healthy focus. Researchers currently undergoing strong research on finding face recognition system for wider area information taken under hysterical elucidation dissimilarity. The proposed face recognition system consists of a narrative expositionindiscreet preprocessing method, a hybrid Fourier-based facial feature extraction and a score fusion scheme. We have verified the face recognition in different lightening conditions (day or night) and at different locations (indoor or outdoor). Preprocessing, Image detection, Feature- extraction and Face recognition are the methods used for face verification system. This paper focuses mainly on the issue of toughness to lighting variations. The proposed system has obtained an average of 88.1% verification rate on Two-Dimensional images under different lightening conditions. version:1
arxiv-1403-0648 | Multi-period Trading Prediction Markets with Connections to Machine Learning | http://arxiv.org/abs/1403.0648 | id:1403.0648 author:Jinli Hu, Amos Storkey category:cs.GT cs.LG q-fin.TR stat.ML  published:2014-03-04 summary:We present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. This specific choice on modelling tools brings us mathematical convenience. The analysis shows that the whole market effectively approaches a global objective, despite that the market is designed such that each agent only cares about its own goal. Additionally, the market dynamics provides a sensible algorithm for optimising the global objective. An intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective, and 2) solve machine learning problems by setting up and running certain markets. version:1
arxiv-1403-0623 | Global solar irradiation prediction using a multi-gene genetic programming approach | http://arxiv.org/abs/1403.0623 | id:1403.0623 author:Indranil Pan, Daya Shankar Pandey, Saptarshi Das category:cs.NE cs.CE stat.AP  published:2014-03-03 summary:In this paper, a nonlinear symbolic regression technique using an evolutionary algorithm known as multi-gene genetic programming (MGGP) is applied for a data-driven modelling between the dependent and the independent variables. The technique is applied for modelling the measured global solar irradiation and validated through numerical simulations. The proposed modelling technique shows improved results over the fuzzy logic and artificial neural network (ANN) based approaches as attempted by contemporary researchers. The method proposed here results in nonlinear analytical expressions, unlike those with neural networks which is essentially a black box modelling approach. This additional flexibility is an advantage from the modelling perspective and helps to discern the important variables which affect the prediction. Due to the evolutionary nature of the algorithm, it is able to get out of local minima and converge to a global optimum unlike the back-propagation (BP) algorithm used for training neural networks. This results in a better percentage fit than the ones obtained using neural networks by contemporary researchers. Also a hold-out cross validation is done on the obtained genetic programming (GP) results which show that the results generalize well to new data and do not over-fit the training samples. The multi-gene GP results are compared with those, obtained using its single-gene version and also the same with four classical regression models in order to show the effectiveness of the adopted approach. version:1
arxiv-1403-0598 | The Structurally Smoothed Graphlet Kernel | http://arxiv.org/abs/1403.0598 | id:1403.0598 author:Pinar Yanardag, S. V. N. Vishwanathan category:cs.LG  published:2014-03-03 summary:A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts. version:1
arxiv-1403-0541 | Representing, reasoning and answering questions about biological pathways - various applications | http://arxiv.org/abs/1403.0541 | id:1403.0541 author:Saadat Anwar category:cs.AI cs.CE cs.CL  published:2014-03-03 summary:Biological organisms are composed of numerous interconnected biochemical processes. Diseases occur when normal functionality of these processes is disrupted. Thus, understanding these biochemical processes and their interrelationships is a primary task in biomedical research and a prerequisite for diagnosing diseases, and drug development. Scientists studying these processes have identified various pathways responsible for drug metabolism, and signal transduction, etc. Newer techniques and speed improvements have resulted in deeper knowledge about these pathways, resulting in refined models that tend to be large and complex, making it difficult for a person to remember all aspects of it. Thus, computer models are needed to analyze them. We want to build such a system that allows modeling of biological systems and pathways in such a way that we can answer questions about them. Many existing models focus on structural and/or factoid questions, using surface-level knowledge that does not require understanding the underlying model. We believe these are not the kind of questions that a biologist may ask someone to test their understanding of the biological processes. We want our system to answer the kind of questions a biologist may ask. Such questions appear in early college level text books. Thus the main goal of our thesis is to develop a system that allows us to encode knowledge about biological pathways and answer such questions about them demonstrating understanding of the pathway. To that end, we develop a language that will allow posing such questions and illustrate the utility of our framework with various applications in the biological domain. We use some existing tools with modifications to accomplish our goal. Finally, we apply our system to real world applications by extracting pathway knowledge from text and answering questions related to drug development. version:1
arxiv-1403-0531 | We Tweet Like We Talk and Other Interesting Observations: An Analysis of English Communication Modalities | http://arxiv.org/abs/1403.0531 | id:1403.0531 author:Josiah P. Zayner category:cs.CL  published:2014-03-03 summary:Modalities of communication for human beings are gradually increasing in number with the advent of new forms of technology. Many human beings can readily transition between these different forms of communication with little or no effort, which brings about the question: How similar are these different communication modalities? To understand technology$\text{'}$s influence on English communication, four different corpora were analyzed and compared: Writing from Books using the 1-grams database from the Google Books project, Twitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices revealed that Talking has the most similarity when compared to the other modes of communication, while 1-grams were the least similar form of communication analyzed. Based on the analysis of word usage, word usage frequency distributions, and word class usage, among other things, Talking is also the most similar to Twitter and IRC Chat. This suggests that communicating using Twitter and IRC Chat evolved from Talking rather than Writing. When we communicate online, even though we are writing, we do not Tweet or Chat how we write books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing were clearly differentiable from our analysis with Twitter and Chat being much more similar to Fiction than Nonfiction writing. These hypotheses were then tested using author and journalists Cory Doctorow. Mr. Doctorow$\text{'}$s Writing, Twitter usage, and Talking were all found to have very similar vocabulary usage patterns as the amalgamized populations, as long as the writing was Fiction. However, Mr. Doctorow$\text{'}$s Nonfiction writing is different from 1-grams and other collected Nonfiction writings. This data could perhaps be used to create more entertaining works of Nonfiction. version:1
arxiv-1403-0515 | A Primal Dual Active Set with Continuation Algorithm for the \ell^0-Regularized Optimization Problem | http://arxiv.org/abs/1403.0515 | id:1403.0515 author:Yuling Jiao, Bangti Jin, Xiliang Lu category:math.OC cs.IT math.IT stat.ML  published:2014-03-03 summary:We develop a primal dual active set with continuation algorithm for solving the \ell^0-regularized least-squares problem that frequently arises in compressed sensing. The algorithm couples the the primal dual active set method with a continuation strategy on the regularization parameter. At each inner iteration, it first identifies the active set from both primal and dual variables, and then updates the primal variable by solving a (typically small) least-squares problem defined on the active set, from which the dual variable can be updated explicitly. Under certain conditions on the sensing matrix, i.e., mutual incoherence property or restricted isometry property, and the noise level, the finite step global convergence of the algorithm is established. Extensive numerical examples are presented to illustrate the efficiency and accuracy of the algorithm and the convergence analysis. version:1
arxiv-1403-0485 | Face Recognition Methods & Applications | http://arxiv.org/abs/1403.0485 | id:1403.0485 author:Divyarajsinh N. Parmar, Brijesh B. Mehta category:cs.CV  published:2014-03-03 summary:Face recognition presents a challenging problem in the field of image analysis and computer vision. The security of information is becoming very significant and difficult. Security cameras are presently common in airports, Offices, University, ATM, Bank and in any locations with a security system. Face recognition is a biometric system used to identify or verify a person from a digital image. Face Recognition system is used in security. Face recognition system should be able to automatically detect a face in an image. This involves extracts its features and then recognize it, regardless of lighting, expression, illumination, ageing, transformations (translate, rotate and scale image) and pose, which is a difficult task. This paper contains three sections. The first section describes the common methods like holistic matching method, feature extraction method and hybrid methods. The second section describes applications with examples and finally third section describes the future research directions of face recognition. version:1
arxiv-1403-0481 | Support Vector Machine Model for Currency Crisis Discrimination | http://arxiv.org/abs/1403.0481 | id:1403.0481 author:Arindam Chaudhuri category:cs.LG stat.ML  published:2014-03-03 summary:Support Vector Machine (SVM) is powerful classification technique based on the idea of structural risk minimization. Use of kernel function enables curse of dimensionality to be addressed. However, proper kernel function for certain problem is dependent on specific dataset and as such there is no good method on choice of kernel function. In this paper, SVM is used to build empirical models of currency crisis in Argentina. An estimation technique is developed by training model on real life data set which provides reasonably accurate model outputs and helps policy makers to identify situations in which currency crisis may happen. The third and fourth order polynomial kernel is generally best choice to achieve high generalization of classifier performance. SVM has high level of maturity with algorithms that are simple, easy to implement, tolerates curse of dimensionality and good empirical performance. The satisfactory results show that currency crisis situation is properly emulated using only small fraction of database and could be used as an evaluation tool as well as an early warning system. To the best of knowledge this is the first work on SVM approach for currency crisis evaluation of Argentina. version:1
arxiv-1403-0320 | Matching Image Sets via Adaptive Multi Convex Hull | http://arxiv.org/abs/1403.0320 | id:1403.0320 author:Shaokang Chen, Arnold Wiliem, Conrad Sanderson, Brian C. Lovell category:cs.CV stat.ML  published:2014-03-03 summary:Traditional nearest points methods use all the samples in an image set to construct a single convex or affine hull model for classification. However, strong artificial features and noisy data may be generated from combinations of training samples when significant intra-class variations and/or noise occur in the image set. Existing multi-model approaches extract local models by clustering each image set individually only once, with fixed clusters used for matching with various image sets. This may not be optimal for discrimination, as undesirable environmental conditions (eg. illumination and pose variations) may result in the two closest clusters representing different characteristics of an object (eg. frontal face being compared to non-frontal face). To address the above problem, we propose a novel approach to enhance nearest points based methods by integrating affine/convex hull classification with an adapted multi-model approach. We first extract multiple local convex hulls from a query image set via maximum margin clustering to diminish the artificial variations and constrain the noise in local convex hulls. We then propose adaptive reference clustering (ARC) to constrain the clustering of each gallery image set by forcing the clusters to have resemblance to the clusters in the query image set. By applying ARC, noisy clusters in the query set can be discarded. Experiments on Honda, MoBo and ETH-80 datasets show that the proposed method outperforms single model approaches and other recent techniques, such as Sparse Approximated Nearest Points, Mutual Subspace Method and Manifold Discriminant Analysis. version:1
arxiv-1403-0316 | Cross-Scale Cost Aggregation for Stereo Matching | http://arxiv.org/abs/1403.0316 | id:1403.0316 author:Kang Zhang, Yuqiang Fang, Dongbo Min, Lifeng Sun, Shiqiang Yang. Shuicheng Yan, Qi Tian category:cs.CV  published:2014-03-03 summary:Human beings process stereoscopic correspondence across multiple scales. However, this bio-inspiration is ignored by state-of-the-art cost aggregation methods for dense stereo correspondence. In this paper, a generic cross-scale cost aggregation framework is proposed to allow multi-scale interaction in cost aggregation. We firstly reformulate cost aggregation from a unified optimization perspective and show that different cost aggregation methods essentially differ in the choices of similarity kernels. Then, an inter-scale regularizer is introduced into optimization and solving this new optimization problem leads to the proposed framework. Since the regularization term is independent of the similarity kernel, various cost aggregation methods can be integrated into the proposed general framework. We show that the cross-scale framework is important as it effectively and efficiently expands state-of-the-art cost aggregation methods and leads to significant improvements, when evaluated on Middlebury, KITTI and New Tsukuba datasets. version:1
arxiv-1403-0315 | Summarisation of Short-Term and Long-Term Videos using Texture and Colour | http://arxiv.org/abs/1403.0315 | id:1403.0315 author:Johanna Carvajal, Chris McCool, Conrad Sanderson category:cs.CV stat.AP  published:2014-03-03 summary:We present a novel approach to video summarisation that makes use of a Bag-of-visual-Textures (BoT) approach. Two systems are proposed, one based solely on the BoT approach and another which exploits both colour information and BoT features. On 50 short-term videos from the Open Video Project we show that our BoT and fusion systems both achieve state-of-the-art performance, obtaining an average F-measure of 0.83 and 0.86 respectively, a relative improvement of 9% and 13% when compared to the previous state-of-the-art. When applied to a new underwater surveillance dataset containing 33 long-term videos, the proposed system reduces the amount of footage by a factor of 27, with only minor degradation in the information content. This order of magnitude reduction in video data represents significant savings in terms of time and potential labour cost when manually reviewing such footage. version:1
arxiv-1403-0309 | Object Tracking via Non-Euclidean Geometry: A Grassmann Approach | http://arxiv.org/abs/1403.0309 | id:1403.0309 author:Sareh Shirazi, Mehrtash T. Harandi, Brian C. Lovell, Conrad Sanderson category:cs.CV math.MG stat.ML  published:2014-03-03 summary:A robust visual tracking system requires an object appearance model that is able to handle occlusion, pose, and illumination variations in the video stream. This can be difficult to accomplish when the model is trained using only a single image. In this paper, we first propose a tracking approach based on affine subspaces (constructed from several images) which are able to accommodate the abovementioned variations. We use affine subspaces not only to represent the object, but also the candidate areas that the object may occupy. We furthermore propose a novel approach to measure affine subspace-to-subspace distance via the use of non-Euclidean geometry of Grassmann manifolds. The tracking problem is then considered as an inference task in a Markov Chain Monte Carlo framework via particle filtering. Quantitative evaluation on challenging video sequences indicates that the proposed approach obtains considerably better performance than several recent state-of-the-art methods such as Tracking-Learning-Detection and MILtrack. version:1
arxiv-1403-0289 | Blind and fully constrained unmixing of hyperspectral images | http://arxiv.org/abs/1403.0289 | id:1403.0289 author:Rita Ammanouil, André Ferrari, Cédric Richard, David Mary category:stat.AP stat.ML  published:2014-03-03 summary:This paper addresses the problem of blind and fully constrained unmixing of hyperspectral images. Unmixing is performed without the use of any dictionary, and assumes that the number of constituent materials in the scene and their spectral signatures are unknown. The estimated abundances satisfy the desired sum-to-one and nonnegativity constraints. Two models with increasing complexity are developed to achieve this challenging task, depending on how noise interacts with hyperspectral data. The first one leads to a convex optimization problem, and is solved with the Alternating Direction Method of Multipliers. The second one accounts for signal-dependent noise, and is addressed with a Reweighted Least Squares algorithm. Experiments on synthetic and real data demonstrate the effectiveness of our approach. version:1
arxiv-1403-0829 | Multiview Hessian regularized logistic regression for action recognition | http://arxiv.org/abs/1403.0829 | id:1403.0829 author:W. Liu, H. Liu, D. Tao, Y. Wang, Ke Lu category:cs.CV cs.LG stat.ML  published:2014-03-03 summary:With the rapid development of social media sharing, people often need to manage the growing volume of multimedia data such as large scale video classification and annotation, especially to organize those videos containing human activities. Recently, manifold regularized semi-supervised learning (SSL), which explores the intrinsic data probability distribution and then improves the generalization ability with only a small number of labeled data, has emerged as a promising paradigm for semiautomatic video classification. In addition, human action videos often have multi-modal content and different representations. To tackle the above problems, in this paper we propose multiview Hessian regularized logistic regression (mHLR) for human action recognition. Compared with existing work, the advantages of mHLR lie in three folds: (1) mHLR combines multiple Hessian regularization, each of which obtained from a particular representation of instance, to leverage the exploring of local geometry; (2) mHLR naturally handle multi-view instances with multiple representations; (3) mHLR employs a smooth loss function and then can be effectively optimized. We carefully conduct extensive experiments on the unstructured social activity attribute (USAA) dataset and the experimental results demonstrate the effectiveness of the proposed multiview Hessian regularized logistic regression for human action recognition. version:1
arxiv-1403-0240 | Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation | http://arxiv.org/abs/1403.0240 | id:1403.0240 author:Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale category:cs.CV cs.CE cs.NA q-bio.QM  published:2014-03-02 summary:Bio-image analysis is challenging due to inhomogeneous intensity distributions and high levels of noise in the images. Bayesian inference provides a principled way for regularizing the problem using prior knowledge. A fundamental choice is how one measures "distances" between shapes in an image. It has been shown that the straightforward geometric L2 distance is degenerate and leads to pathological situations. This is avoided when using Sobolev gradients, rendering the segmentation problem less ill-posed. The high computational cost and implementation overhead of Sobolev gradients, however, have hampered practical applications. We show how particle methods as applied to image segmentation allow for a simple and computationally efficient implementation of Sobolev gradients. We show that the evaluation of Sobolev gradients amounts to particle-particle interactions along the contour in an image. We extend an existing particle-based segmentation algorithm to using Sobolev gradients. Using synthetic and real-world images, we benchmark the results for both 2D and 3D images using piecewise smooth and piecewise constant region models. The present particle approximation of Sobolev gradients is 2.8 to 10 times faster than the previous reference implementation, but retains the known favorable properties of Sobolev gradients. This speedup is achieved by using local particle-particle interactions instead of solving a global Poisson equation at each iteration. The computational time per iteration is higher for Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition the optimization problem, however, a smaller number of overall iterations may be necessary for the algorithm to converge, which can in some cases amortize the higher per-iteration cost. version:1
arxiv-1306-3161 | Learning Using Privileged Information: SVM+ and Weighted SVM | http://arxiv.org/abs/1306.3161 | id:1306.3161 author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.LG  published:2013-06-13 summary:Prior knowledge can be used to improve predictive performance of learning algorithms or reduce the amount of data required for training. The same goal is pursued within the learning using privileged information paradigm which was recently introduced by Vapnik et al. and is aimed at utilizing additional information available only at training time -- a framework implemented by SVM+. We relate the privileged information to importance weighting and show that the prior knowledge expressible with privileged features can also be encoded by weights associated with every training example. We show that a weighted SVM can always replicate an SVM+ solution, while the converse is not true and we construct a counterexample highlighting the limitations of SVM+. Finally, we touch on the problem of choosing weights for weighted SVMs when privileged features are not available. version:2
arxiv-1403-0157 | Network Traffic Decomposition for Anomaly Detection | http://arxiv.org/abs/1403.0157 | id:1403.0157 author:Tahereh Babaie, Sanjay Chawla, Sebastien Ardon category:cs.LG cs.NI  published:2014-03-02 summary:In this paper we focus on the detection of network anomalies like Denial of Service (DoS) attacks and port scans in a unified manner. While there has been an extensive amount of research in network anomaly detection, current state of the art methods are only able to detect one class of anomalies at the cost of others. The key tool we will use is based on the spectral decomposition of a trajectory/hankel matrix which is able to detect deviations from both between and within correlation present in the observed network traffic data. Detailed experiments on synthetic and real network traces shows a significant improvement in detection capability over competing approaches. In the process we also address the issue of robustness of anomaly detection systems in a principled fashion. version:1
arxiv-1403-0156 | Sleep Analytics and Online Selective Anomaly Detection | http://arxiv.org/abs/1403.0156 | id:1403.0156 author:Tahereh Babaie, Sanjay Chawla, Romesh Abeysuriya category:cs.LG  published:2014-03-02 summary:We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to model a specific scenario emerging from research in sleep science. Scientists have segmented sleep into several stages and stage two is characterized by two patterns (or anomalies) in the EEG time series recorded on sleep subjects. These two patterns are sleep spindle (SS) and K-complex. The OSAD problem was introduced to design a residual system, where all anomalies (known and unknown) are detected but the system only triggers an alarm when non-SS anomalies appear. The solution of the OSAD problem required us to combine techniques from both machine learning and control theory. Experiments on data from real subjects attest to the effectiveness of our approach. version:1
arxiv-1312-5799 | Accelerated, Parallel and Proximal Coordinate Descent | http://arxiv.org/abs/1312.5799 | id:1312.5799 author:Olivier Fercoq, Peter Richtárik category:math.OC cs.DC cs.NA math.NA stat.ML  published:2013-12-20 summary:We propose a new stochastic coordinate descent method for minimizing the sum of convex functions each of which depends on a small number of coordinates only. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal; this is the first time such a method is proposed. In the special case when the number of processors is equal to the number of coordinates, the method converges at the rate $2\bar{\omega}\bar{L} R^2/(k+1)^2 $, where $k$ is the iteration counter, $\bar{\omega}$ is an average degree of separability of the loss function, $\bar{L}$ is the average of Lipschitz constants associated with the coordinates and individual functions in the sum, and $R$ is the distance of the initial point from the minimizer. We show that the method can be implemented without the need to perform full-dimensional vector operations, which is the major bottleneck of existing accelerated coordinate descent methods. The fact that the method depends on the average degree of separability, and not on the maximum degree of separability, can be attributed to the use of new safe large stepsizes, leading to improved expected separable overapproximation (ESO). These are of independent interest and can be utilized in all existing parallel stochastic coordinate descent algorithms based on the concept of ESO. version:2
arxiv-1403-0087 | Temporal Image Fusion | http://arxiv.org/abs/1403.0087 | id:1403.0087 author:Francisco J. Estrada category:cs.CV cs.GR  published:2014-03-01 summary:This paper introduces temporal image fusion. The proposed technique builds upon previous research in exposure fusion and expands it to deal with the limited Temporal Dynamic Range of existing sensors and camera technologies. In particular, temporal image fusion enables the rendering of long-exposure effects on full frame-rate video, as well as the generation of arbitrarily long exposures from a sequence of images of the same scene taken over time. We explore the problem of temporal under-exposure, and show how it can be addressed by selectively enhancing dynamic structure. Finally, we show that the use of temporal image fusion together with content-selective image filters can produce a range of striking visual effects on a given input sequence. version:1
arxiv-1403-0052 | TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding Initiative guidelines | http://arxiv.org/abs/1403.0052 | id:1403.0052 author:Laurent Romary category:cs.CL  published:2014-03-01 summary:This paper presents an attempt to customise the TEI (Text Encoding Initiative) guidelines in order to offer the possibility to incorporate TBX (TermBase eXchange) based terminological entries within any kind of TEI documents. After presenting the general historical, conceptual and technical contexts, we describe the various design choices we had to take while creating this customisation, which in turn have led to make various changes in the actual TBX serialisation. Keeping in mind the objective to provide the TEI guidelines with, again, an onomasiological model, we try to identify the best comprise in maintaining both the isomorphism with the existing TBX Basic standard and the characteristics of the TEI framework. version:1
arxiv-1402-7351 | A Machine Learning Model for Stock Market Prediction | http://arxiv.org/abs/1402.7351 | id:1402.7351 author:Osman Hegazy, Omar S. Soliman, Mustafa Abdul Salam category:cs.CE cs.NE  published:2014-02-28 summary:Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on a financial exchange. version:1
arxiv-1402-7265 | Semantics, Modelling, and the Problem of Representation of Meaning -- a Brief Survey of Recent Literature | http://arxiv.org/abs/1402.7265 | id:1402.7265 author:Yarin Gal category:cs.CL  published:2014-02-28 summary:Over the past 50 years many have debated what representation should be used to capture the meaning of natural language utterances. Recently new needs of such representations have been raised in research. Here I survey some of the interesting representations suggested to answer for these new needs. version:1
arxiv-1402-7162 | Visual Saliency Model using SIFT and Comparison of Learning Approaches | http://arxiv.org/abs/1402.7162 | id:1402.7162 author:Hamdi Yalin Yalic category:cs.CV I.2.10; I.5.4  published:2014-02-28 summary:Humans' ability to detect and locate salient objects on images is remarkably fast and successful. Performing this process by using eye tracking equipment is expensive and cannot be easily applied, and computer modeling of this human behavior is still a problem to be solved. In our study, one of the largest public eye-tracking databases which has fixation points of 15 observers on 1003 images is used. In addition to low, medium and high-level features which have been used in previous studies, SIFT features extracted from the images are used to improve the classification accuracy of the models. A second contribution of this paper is the comparison and statistical analysis of different machine learning methods that can be used to train our model. As a result, a best feature set and learning model to predict where humans look at images, is determined. version:1
arxiv-1402-7136 | Neural Network Approach to Railway Stand Lateral Skew Control | http://arxiv.org/abs/1402.7136 | id:1402.7136 author:Peter Mark Benes, Ivo Bukovsky, Matous Cejnek, Jan Kalivoda category:cs.SY cs.NE  published:2014-02-28 summary:The paper presents a study of an adaptive approach to lateral skew control for an experimental railway stand. The preliminary experiments with the real experimental railway stand and simulations with its 3-D mechanical model, indicates difficulties of model-based control of the device. Thus, use of neural networks for identification and control of lateral skew shall be investigated. This paper focuses on real-data based modeling of the railway stand by various neural network models, i.e; linear neural unit and quadratic neural unit architectures. Furthermore, training methods of these neural architectures as such, real-time-recurrent-learning and a variation of back-propagation-through-time are examined, accompanied by a discussion of the produced experimental results. version:1
arxiv-1402-5596 | Exact Post Model Selection Inference for Marginal Screening | http://arxiv.org/abs/1402.5596 | id:1402.5596 author:Jason D Lee, Jonathan E Taylor category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2014-02-23 summary:We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$, conditional on the model being selected (``condition on selection" framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $X$. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso. version:2
arxiv-1311-4025 | Signal Recovery from Pooling Representations | http://arxiv.org/abs/1311.4025 | id:1311.4025 author:Joan Bruna, Arthur Szlam, Yann LeCun category:stat.ML  published:2013-11-16 summary:In this work we compute lower Lipschitz bounds of $\ell_p$ pooling operators for $p=1, 2, \infty$ as well as $\ell_p$ pooling operators preceded by half-rectification layers. These give sufficient conditions for the design of invertible neural network layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression. version:3
arxiv-1310-2959 | Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch | http://arxiv.org/abs/1310.2959 | id:1310.2959 author:Partha Pratim Talukdar, William Cohen category:cs.LG  published:2013-10-10 summary:Graph-based Semi-supervised learning (SSL) algorithms have been successfully used in a large number of applications. These methods classify initially unlabeled nodes by propagating label information over the structure of graph starting from seed nodes. Graph-based SSL algorithms usually scale linearly with the number of distinct labels (m), and require O(m) space on each node. Unfortunately, there exist many applications of practical significance with very large m over large graphs, demanding better space and time complexity. In this paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm which compactly stores label distribution on each node using Count-min Sketch, a randomized data structure. We present theoretical analysis showing that under mild conditions, MAD-SKETCH can reduce space complexity at each node from O(m) to O(log m), and achieve similar savings in time complexity as well. We support our analysis through experiments on multiple real world datasets. We observe that MAD-SKETCH achieves similar performance as existing state-of-the-art graph- based SSL algorithms, while requiring smaller memory footprint and at the same time achieving up to 10x speedup. We find that MAD-SKETCH is able to scale to datasets with one million labels, which is beyond the scope of existing graph- based SSL algorithms. version:2
arxiv-1402-7005 | Bayesian Multi-Scale Optimistic Optimization | http://arxiv.org/abs/1402.7005 | id:1402.7005 author:Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas category:stat.ML cs.LG  published:2014-02-27 summary:Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates. version:1
arxiv-1402-7001 | Marginalizing Corrupted Features | http://arxiv.org/abs/1402.7001 | id:1402.7001 author:Laurens van der Maaten, Minmin Chen, Stephen Tyree, Kilian Weinberger category:cs.LG  published:2014-02-27 summary:The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on an almost infinitely large training data set that captures all variations in the data distribution. In practical learning settings, however, we do not have infinite data and our predictors may overfit. Overfitting may be combatted, for example, by adding a regularizer to the training objective or by defining a prior over the model parameters and performing Bayesian inference. In this paper, we propose a third, alternative approach to combat overfitting: we extend the training set with infinitely many artificial training examples that are obtained by corrupting the original training data. We show that this approach is practical and efficient for a range of predictors and corruption models. Our approach, called marginalized corrupted features (MCF), trains robust predictors by minimizing the expected value of the loss function under the corruption model. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are also more robust to feature deletion at test time. version:1
arxiv-1402-6964 | Scalable methods for nonnegative matrix factorizations of near-separable tall-and-skinny matrices | http://arxiv.org/abs/1402.6964 | id:1402.6964 author:Austin R. Benson, Jason D. Lee, Bartek Rajwa, David F. Gleich category:cs.LG cs.DC cs.NA stat.ML G.1.3; G.1.6  published:2014-02-27 summary:Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called "tall-and-skinny matrices". One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics. version:1
arxiv-1402-6942 | A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with Time Windows | http://arxiv.org/abs/1402.6942 | id:1402.6942 author:Jakub Nalepa, Zbigniew J. Czech category:cs.DC cs.NE  published:2014-02-27 summary:This paper presents a parallel memetic algorithm for solving the vehicle routing problem with time windows (VRPTW). The VRPTW is a well-known NP-hard discrete optimization problem with two objectives. The main objective is to minimize the number of vehicles serving customers scattered on the map, and the second one is to minimize the total distance traveled by the vehicles. Here, the fleet size is minimized in the first phase of the proposed method using the parallel heuristic algorithm (PHA), and the traveled distance is minimized in the second phase by the parallel memetic algorithm (PMA). In both parallel algorithms, the parallel components co-operate periodically in order to exchange the best solutions found so far. An extensive experimental study performed on the Gehring and Homberger's benchmark proves the high convergence capabilities and robustness of both PHA and PMA. Also, we present the speedup analysis of the PMA. version:1
arxiv-1402-6932 | Low-Cost Compressive Sensing for Color Video and Depth | http://arxiv.org/abs/1402.6932 | id:1402.6932 author:Xin Yuan, Patrick Llull, Xuejun Liao, Jianbo Yang, Guillermo Sapiro, David J. Brady, Lawrence Carin category:cs.CV  published:2014-02-27 summary:A simple and inexpensive (low-power and low-bandwidth) modification is made to a conventional off-the-shelf color video camera, from which we recover {multiple} color frames for each of the original measured frames, and each of the recovered frames can be focused at a different depth. The recovery of multiple frames for each measured frame is made possible via high-speed coding, manifested via translation of a single coded aperture; the inexpensive translation is constituted by mounting the binary code on a piezoelectric device. To simultaneously recover depth information, a {liquid} lens is modulated at high speed, via a variable voltage. Consequently, during the aforementioned coding process, the liquid lens allows the camera to sweep the focus through multiple depths. In addition to designing and implementing the camera, fast recovery is achieved by an anytime algorithm exploiting the group-sparsity of wavelet/DCT coefficients. version:1
arxiv-1401-6956 | A continuous-time approach to online optimization | http://arxiv.org/abs/1401.6956 | id:1401.6956 author:Joon Kwon, Panayotis Mertikopoulos category:math.OC cs.LG stat.ML  published:2014-01-27 summary:We consider a family of learning strategies for online optimization problems that evolve in continuous time and we show that they lead to no regret. From a more traditional, discrete-time viewpoint, this continuous-time approach allows us to derive the no-regret properties of a large class of discrete-time algorithms including as special cases the exponential weight algorithm, online mirror descent, smooth fictitious play and vanishingly smooth fictitious play. In so doing, we obtain a unified view of many classical regret bounds, and we show that they can be decomposed into a term stemming from continuous-time considerations and a term which measures the disparity between discrete and continuous time. As a result, we obtain a general class of infinite horizon learning strategies that guarantee an $\mathcal{O}(n^{-1/2})$ regret bound without having to resort to a doubling trick. version:2
arxiv-1402-6888 | CriPS: Critical Dynamics in Particle Swarm Optimization | http://arxiv.org/abs/1402.6888 | id:1402.6888 author:Adam Erskine, J Michael Herrmann category:cs.NE  published:2014-02-27 summary:Particle Swarm Optimisation (PSO) makes use of a dynamical system for solving a search task. Instead of adding search biases in order to improve performance in certain problems, we aim to remove algorithm-induced scales by controlling the swarm with a mechanism that is scale-free except possibly for a suppression of scales beyond the system size. In this way a very promising performance is achieved due to the balance of large-scale exploration and local search. The resulting algorithm shows evidence for self-organised criticality, brought about via the intrinsic dynamics of the swarm as it interacts with the objective function, rather than being explicitly specified. The Critical Particle Swarm (CriPS) can be easily combined with many existing extensions such as chaotic exploration, additional force terms or non-trivial topologies. version:1
arxiv-1402-6880 | It's distributions all the way down!: Second order changes in statistical distributions also occur | http://arxiv.org/abs/1402.6880 | id:1402.6880 author:M. T. Keane, A. Gerow category:cs.CL  published:2014-02-27 summary:The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et als) message on distributions; it largely examines the first-order effects of how a single, signature distribution can predict population behaviour, neglecting second-order effects involving distributional shifts, either between signature distributions or within a given signature distribution. Indeed, Bentley et al. themselves under-emphasise the potential richness of the latter, within-distribution effects. version:1
arxiv-1402-6859 | Outlier Detection using Improved Genetic K-means | http://arxiv.org/abs/1402.6859 | id:1402.6859 author:M. H. Marghny, Ahmed I. Taloba category:cs.LG cs.DB  published:2014-02-27 summary:The outlier detection problem in some cases is similar to the classification problem. For example, the main concern of clustering-based outlier detection algorithms is to find clusters and outliers, which are often regarded as noise that should be removed in order to make more reliable clustering. In this article, we present an algorithm that provides outlier detection and data clustering simultaneously. The algorithmimprovesthe estimation of centroids of the generative distribution during the process of clustering and outlier discovery. The proposed algorithm consists of two stages. The first stage consists of improved genetic k-means algorithm (IGK) process, while the second stage iteratively removes the vectors which are far from their cluster centroids. version:1
arxiv-1405-6173 | An Effective Evolutionary Clustering Algorithm: Hepatitis C Case Study | http://arxiv.org/abs/1405.6173 | id:1405.6173 author:M. H. Marghny, Rasha M. Abd El-Aziz, Ahmed I. Taloba category:cs.NE cs.CE  published:2014-02-27 summary:Clustering analysis plays an important role in scientific research and commercial application. K-means algorithm is a widely used partition method in clustering. However, it is known that the K-means algorithm may get stuck at suboptimal solutions, depending on the choice of the initial cluster centers. In this article, we propose a technique to handle large scale data, which can select initial clustering center purposefully using Genetic algorithms (GAs), reduce the sensitivity to isolated point, avoid dissevering big cluster, and overcome deflexion of data in some degree that caused by the disproportion in data partitioning owing to adoption of multi-sampling. We applied our method to some public datasets these show the advantages of the proposed approach for example Hepatitis C dataset that has been taken from the machine learning warehouse of University of California. Our aim is to evaluate hepatitis dataset. In order to evaluate this dataset we did some preprocessing operation, the reason to preprocessing is to summarize the data in the best and suitable way for our algorithm. Missing values of the instances are adjusted using local mean method. version:1
arxiv-1311-1294 | Delay Learning Architectures for Memory and Classification | http://arxiv.org/abs/1311.1294 | id:1311.1294 author:Shaista Hussain, Arindam Basu, R. Wang, Tara Julia Hamilton category:cs.NE q-bio.NC  published:2013-11-06 summary:We present a neuromorphic spiking neural network, the DELTRON, that can remember and store patterns by changing the delays of every connection as opposed to modifying the weights. The advantage of this architecture over traditional weight based ones is simpler hardware implementation without multipliers or digital-analog converters (DACs) as well as being suited to time-based computing. The name is derived due to similarity in the learning rule with an earlier architecture called Tempotron. The DELTRON can remember more patterns than other delay-based networks by modifying a few delays to remember the most 'salient' or synchronous part of every spike pattern. We present simulations of memory capacity and classification ability of the DELTRON for different random spatio-temporal spike patterns. The memory capacity for noisy spike patterns and missing spikes are also shown. Finally, we present SPICE simulation results of the core circuits involved in a reconfigurable mixed signal implementation of this architecture. version:2
arxiv-1402-6792 | Information Evolution in Social Networks | http://arxiv.org/abs/1402.6792 | id:1402.6792 author:Lada A. Adamic, Thomas M. Lento, Eytan Adar, Pauline C. Ng category:cs.SI cs.CL physics.soc-ph  published:2014-02-27 summary:Social networks readily transmit information, albeit with less than perfect fidelity. We present a large-scale measurement of this imperfect information copying mechanism by examining the dissemination and evolution of thousands of memes, collectively replicated hundreds of millions of times in the online social network Facebook. The information undergoes an evolutionary process that exhibits several regularities. A meme's mutation rate characterizes the population distribution of its variants, in accordance with the Yule process. Variants further apart in the diffusion cascade have greater edit distance, as would be expected in an iterative, imperfect replication process. Some text sequences can confer a replicative advantage; these sequences are abundant and transfer "laterally" between different memes. Subpopulations of the social network can preferentially transmit a specific variant of a meme if the variant matches their beliefs or culture. Understanding the mechanism driving change in diffusing information has important implications for how we interpret and harness the information that reaches us through our social networks. version:1
arxiv-1402-6785 | Synthesis of Parametric Programs using Genetic Programming and Model Checking | http://arxiv.org/abs/1402.6785 | id:1402.6785 author:Gal Katz, Doron Peled category:cs.SE cs.AI cs.NE  published:2014-02-27 summary:Formal methods apply algorithms based on mathematical principles to enhance the reliability of systems. It would only be natural to try to progress from verification, model checking or testing a system against its formal specification into constructing it automatically. Classical algorithmic synthesis theory provides interesting algorithms but also alarming high complexity and undecidability results. The use of genetic programming, in combination with model checking and testing, provides a powerful heuristic to synthesize programs. The method is not completely automatic, as it is fine tuned by a user that sets up the specification and parameters. It also does not guarantee to always succeed and converge towards a solution that satisfies all the required properties. However, we applied it successfully on quite nontrivial examples and managed to find solutions to hard programming challenges, as well as to improve and to correct code. We describe here several versions of our method for synthesizing sequential and concurrent systems. version:1
arxiv-1402-6764 | A method to identify potential ambiguous Malay words through Ambiguity Attributes mapping: An exploratory Study | http://arxiv.org/abs/1402.6764 | id:1402.6764 author:Hazlina Haron, Abdul Azim Abd. Ghani category:cs.SE cs.CL  published:2014-02-27 summary:We describe here a methodology to identify a list of ambiguous Malay words that are commonly being used in Malay documentations such as Requirement Specification. We compiled several relevant and appropriate requirement quality attributes and sentence rules from previous literatures and adopt it to come out with a set of ambiguity attributes that most suit Malay words. The extracted Malay ambiguous words (potential) are then being mapped onto the constructed ambiguity attributes to confirm their vagueness. The list is then verified by Malay linguist experts. This paper aims to identify a list of potential ambiguous words in Malay as an attempt to assist writers to avoid using the vague words while documenting Malay Requirement Specification as well as to any other related Malay documentation. The result of this study is a list of 120 potential ambiguous Malay words that could act as guidelines in writing Malay sentences version:1
arxiv-1402-6744 | Robust Asymmetric Clustering | http://arxiv.org/abs/1402.6744 | id:1402.6744 author:Katherine Morris, Paul D. McNicholas, Antonio Punzo, Ryan P. Browne category:stat.ME stat.CO stat.ML  published:2014-02-26 summary:Contaminated mixture models are developed for model-based clustering of data with asymmetric clusters as well as spurious points, outliers, and/or noise. Specifically, we introduce a contaminated mixture of contaminated shifted asymmetric Laplace distributions and a contaminated mixture of contaminated skew-normal distributions. In each case, mixture components have a parameter controlling the proportion of bad points (i.e., spurious points, outliers, and/or noise) and one specifying the degree of contamination. A very important feature of our approaches is that these parameters do not have to be specified a priori. Expectation-conditional maximization algorithms are outlined for parameter estimation and the number of components is selected using the Bayesian information criterion. The performance of our approaches is illustrated on artificial and real data. version:1
arxiv-1402-6690 | Why Are You More Engaged? Predicting Social Engagement from Word Use | http://arxiv.org/abs/1402.6690 | id:1402.6690 author:Jalal Mahmud, Jilin Chen, Jeffrey Nichols category:cs.SI cs.CL cs.CY  published:2014-02-26 summary:We present a study to analyze how word use can predict social engagement behaviors such as replies and retweets in Twitter. We compute psycholinguistic category scores from word usage, and investigate how people with different scores exhibited different reply and retweet behaviors on Twitter. We also found psycholinguistic categories that show significant correlations with such social engagement behaviors. In addition, we have built predictive models of replies and retweets from such psycholinguistic category based features. Our experiments using a real world dataset collected from Twitter validates that such predictions can be done with reasonable accuracy. version:1
arxiv-1402-6650 | A Novel Method for the Recognition of Isolated Handwritten Arabic Characters | http://arxiv.org/abs/1402.6650 | id:1402.6650 author:Ahmed Sahlol, Cheng Suen category:cs.CV 68T10  published:2014-02-26 summary:There are many difficulties facing a handwritten Arabic recognition system such as unlimited variation in human handwriting, similarities of distinct character shapes, interconnections of neighbouring characters and their position in the word. The typical Optical Character Recognition (OCR) systems are based mainly on three stages, preprocessing, features extraction and recognition. This paper proposes new methods for handwritten Arabic character recognition which is based on novel preprocessing operations including different kinds of noise removal also different kind of features like structural, Statistical and Morphological features from the main body of the character and also from the secondary components. Evaluation of the accuracy of the selected features is made. The system was trained and tested by back propagation neural network with CENPRMI dataset. The proposed algorithm obtained promising results as it is able to recognize 88% of our test set accurately. In Comparable with other related works we find that our result is the highest among other published works. version:1
arxiv-1402-6556 | Evolutionary solving of the debts' clearing problem | http://arxiv.org/abs/1402.6556 | id:1402.6556 author:Csaba Patcas, Attila Bartha category:cs.NE cs.AI 97R40 I.2.8; G.2.3  published:2014-02-26 summary:The debts' clearing problem is about clearing all the debts in a group of n entities (persons, companies etc.) using a minimal number of money transaction operations. The problem is known to be NP-hard in the strong sense. As for many intractable problems, techniques from the field of artificial intelligence are useful in finding solutions close to optimum for large inputs. An evolutionary algorithm for solving the debts' clearing problem is proposed. version:1
arxiv-1402-6552 | Renewable Energy Prediction using Weather Forecasts for Optimal Scheduling in HPC Systems | http://arxiv.org/abs/1402.6552 | id:1402.6552 author:Ankur Sahai category:cs.LG  published:2014-02-26 summary:The objective of the GreenPAD project is to use green energy (wind, solar and biomass) for powering data-centers that are used to run HPC jobs. As a part of this it is important to predict the Renewable (Wind) energy for efficient scheduling (executing jobs that require higher energy when there is more green energy available and vice-versa). For predicting the wind energy we first analyze the historical data to find a statistical model that gives relation between wind energy and weather attributes. Then we use this model based on the weather forecast data to predict the green energy availability in the future. Using the green energy prediction obtained from the statistical model we are able to precompute job schedules for maximizing the green energy utilization in the future. We propose a model which uses live weather data in addition to machine learning techniques (which can predict future deviations in weather conditions based on current deviations from the forecast) to make on-the-fly changes to the precomputed schedule (based on green energy prediction). For this we first analyze the data using histograms and simple statistical tools such as correlation. In addition we build (correlation) regression model for finding the relation between wind energy availability and weather attributes (temperature, cloud cover, air pressure, wind speed / direction, precipitation and sunshine). We also analyze different algorithms and machine learning techniques for optimizing the job schedules for maximizing the green energy utilization. version:1
arxiv-1402-6516 | Modelling the Lexicon in Unsupervised Part of Speech Induction | http://arxiv.org/abs/1402.6516 | id:1402.6516 author:Greg Dubbin, Phil Blunsom category:cs.CL  published:2014-02-26 summary:Automatically inducing the syntactic part-of-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag. This one-tag-per-type heuristic counters the tendency of Hidden Markov Model based taggers to over generate tags for a given word type. However, it is clearly incompatible with basic syntactic theory. In this paper we extend a state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model of the lexicon. In doing so we are able to incorporate a soft bias towards inducing few tags per type. We develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions. version:1
arxiv-1402-6428 | Clustering Multidimensional Data with PSO based Algorithm | http://arxiv.org/abs/1402.6428 | id:1402.6428 author:Jayshree Ghorpade-Aher, Vishakha A. Metre category:cs.NE  published:2014-02-26 summary:Data clustering is a recognized data analysis method in data mining whereas K-Means is the well known partitional clustering method, possessing pleasant features. We observed that, K-Means and other partitional clustering techniques suffer from several limitations such as initial cluster centre selection, preknowledge of number of clusters, dead unit problem, multiple cluster membership and premature convergence to local optima. Several optimization methods are proposed in the literature in order to solve clustering limitations, but Swarm Intelligence (SI) has achieved its remarkable position in the concerned area. Particle Swarm Optimization (PSO) is the most popular SI technique and one of the favorite areas of researchers. In this paper, we present a brief overview of PSO and applicability of its variants to solve clustering challenges. Also, we propose an advanced PSO algorithm named as Subtractive Clustering based Boundary Restricted Adaptive Particle Swarm Optimization (SC-BR-APSO) algorithm for clustering multidimensional data. For comparison purpose, we have studied and analyzed various algorithms such as K-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposed algorithm on nine different datasets. The motivation behind proposing SC-BR-APSO algorithm is to deal with multidimensional data clustering, with minimum error rate and maximum convergence rate. version:1
arxiv-1402-6416 | Deconstruction of compound objects from image sets | http://arxiv.org/abs/1402.6416 | id:1402.6416 author:Anton van den Hengel, John Bastian, Anthony Dick, Lachlan Fleming category:cs.CV  published:2014-02-26 summary:We propose a method to recover the structure of a compound object from multiple silhouettes. Structure is expressed as a collection of 3D primitives chosen from a pre-defined library, each with an associated pose. This has several advantages over a volume or mesh representation both for estimation and the utility of the recovered model. The main challenge in recovering such a model is the combinatorial number of possible arrangements of parts. We address this issue by exploiting the sparse nature of the problem, and show that our method scales to objects constructed from large libraries of parts. version:1
arxiv-1402-6387 | Active spline model: A shape based model-interactive segmentation | http://arxiv.org/abs/1402.6387 | id:1402.6387 author:Jen Hong Tan, U. Rajendra Acharya category:cs.CV  published:2014-02-26 summary:Rarely in literature a method of segmentation cares for the edit after the algorithm delivers. They provide no solution when segmentation goes wrong. We propose to formulate point distribution model in terms of centripetal-parameterized Catmull-Rom spline. Such fusion brings interactivity to model-based segmentation, so that edit is better handled. When the delivered segment is unsatisfactory, user simply shifts points to vary the curve. We ran the method on three disparate imaging modalities and achieved an average overlap of 0.879 for automated lung segmentation on chest radiographs. The edit afterward improved the average overlap to 0.945, with a minimum of 0.925. The source code and the demo video are available at http://wp.me/p3vCKy-2S version:1
arxiv-1402-6383 | Large-margin Learning of Compact Binary Image Encodings | http://arxiv.org/abs/1402.6383 | id:1402.6383 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2014-02-26 summary:The use of high-dimensional features has become a normal practice in many computer vision applications. The large dimension of these features is a limiting factor upon the number of data points which may be effectively stored and processed, however. We address this problem by developing a novel approach to learning a compact binary encoding, which exploits both pair-wise proximity and class-label information on training data set. Exploiting this extra information allows the development of encodings which, although compact, outperform the original high-dimensional features in terms of final classification or retrieval performance. The method is general, in that it is applicable to both non-parametric and parametric learning methods. This generality means that the embedded features are suitable for a wide variety of computer vision tasks, such as image classification and content-based image retrieval. Experimental results demonstrate that the new compact descriptor achieves an accuracy comparable to, and in some cases better than, the visual descriptor in the original space despite being significantly more compact. Moreover, any convex loss function and convex regularization penalty (e.g., $ \ell_p $ norm with $ p \ge 1 $) can be incorporated into the framework, which provides future flexibility. version:1
arxiv-1402-6366 | LSSVM-ABC Algorithm for Stock Price prediction | http://arxiv.org/abs/1402.6366 | id:1402.6366 author:Osman Hegazy, Omar S. Soliman, Mustafa Abdul Salam category:cs.CE cs.NE  published:2014-02-25 summary:In this paper, Artificial Bee Colony (ABC) algorithm which inspired from the behavior of honey bees swarm is presented. ABC is a stochastic population-based evolutionary algorithm for problem solving. ABC algorithm, which is considered one of the most recently swarm intelligent techniques, is proposed to optimize least square support vector machine (LSSVM) to predict the daily stock prices. The proposed model is based on the study of stocks historical data, technical indicators and optimizing LSSVM with ABC algorithm. ABC selects best free parameters combination for LSSVM to avoid over-fitting and local minima problems and improve prediction accuracy. LSSVM optimized by Particle swarm optimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparison with proposed model. Proposed model tested with twenty datasets representing different sectors in S&P 500 stock market. Results presented in this paper show that the proposed model has fast convergence speed, and it also achieves better accuracy than compared techniques in most cases. version:1
arxiv-1402-6361 | Oracle-Based Robust Optimization via Online Learning | http://arxiv.org/abs/1402.6361 | id:1402.6361 author:Aharon Ben-Tal, Elad Hazan, Tomer Koren, Shie Mannor category:math.OC cs.LG  published:2014-02-25 summary:Robust optimization is a common framework in optimization under uncertainty when the problem parameters are not known, but it is rather known that the parameters belong to some given uncertainty set. In the robust optimization framework the problem solved is a min-max problem where a solution is judged according to its performance on the worst possible realization of the parameters. In many cases, a straightforward solution of the robust optimization problem of a certain type requires solving an optimization problem of a more complicated type, and in some cases even NP-hard. For example, solving a robust conic quadratic program, such as those arising in robust SVM, ellipsoidal uncertainty leads in general to a semidefinite program. In this paper we develop a method for approximately solving a robust optimization problem using tools from online convex optimization, where in every stage a standard (non-robust) optimization program is solved. Our algorithms find an approximate robust solution using a number of calls to an oracle that solves the original (non-robust) problem that is inversely proportional to the square of the target accuracy. version:1
arxiv-1312-6055 | Unit Tests for Stochastic Optimization | http://arxiv.org/abs/1312.6055 | id:1312.6055 author:Tom Schaul, Ioannis Antonoglou, David Silver category:cs.LG  published:2013-12-20 summary:Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on numerous established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms. version:3
arxiv-1301-3720 | The IBMAP approach for Markov networks structure learning | http://arxiv.org/abs/1301.3720 | id:1301.3720 author:Federico Schlüter, Facundo Bromberg, Alejandro Edera category:cs.AI cs.LG  published:2013-01-16 summary:In this work we consider the problem of learning the structure of Markov networks from data. We present an approach for tackling this problem called IBMAP, together with an efficient instantiation of the approach: the IBMAP-HC algorithm, designed for avoiding important limitations of existing independence-based algorithms. These algorithms proceed by performing statistical independence tests on data, trusting completely the outcome of each test. In practice tests may be incorrect, resulting in potential cascading errors and the consequent reduction in the quality of the structures learned. IBMAP contemplates this uncertainty in the outcome of the tests through a probabilistic maximum-a-posteriori approach. The approach is instantiated in the IBMAP-HC algorithm, a structure selection strategy that performs a polynomial heuristic local search in the space of possible structures. We present an extensive empirical evaluation on synthetic and real data, showing that our algorithm outperforms significantly the current independence-based algorithms, in terms of data efficiency and quality of learned structures, with equivalent computational complexities. We also show the performance of IBMAP-HC in a real-world application of knowledge discovery: EDAs, which are evolutionary algorithms that use structure learning on each generation for modeling the distribution of populations. The experiments show that when IBMAP-HC is used to learn the structure, EDAs improve the convergence to the optimum. version:2
arxiv-1402-6238 | Improving Collaborative Filtering based Recommenders using Topic Modelling | http://arxiv.org/abs/1402.6238 | id:1402.6238 author:Jobin Wilson, Santanu Chaudhury, Brejesh Lall, Prateek Kapadia category:cs.IR cs.CL cs.LG  published:2014-02-25 summary:Standard Collaborative Filtering (CF) algorithms make use of interactions between users and items in the form of implicit or explicit ratings alone for generating recommendations. Similarity among users or items is calculated purely based on rating overlap in this case,without considering explicit properties of users or items involved, limiting their applicability in domains with very sparse rating spaces. In many domains such as movies, news or electronic commerce recommenders, considerable contextual data in text form describing item properties is available along with the rating data, which could be utilized to improve recommendation quality.In this paper, we propose a novel approach to improve standard CF based recommenders by utilizing latent Dirichlet allocation (LDA) to learn latent properties of items, expressed in terms of topic proportions, derived from their textual description. We infer user's topic preferences or persona in the same latent space,based on her historical ratings. While computing similarity between users, we make use of a combined similarity measure involving rating overlap as well as similarity in the latent topic space. This approach alleviates sparsity problem as it allows calculation of similarity between users even if they have not rated any items in common. Our experiments on multiple public datasets indicate that the proposed hybrid approach significantly outperforms standard user Based and item Based CF recommenders in terms of classification accuracy metrics such as precision, recall and f-measure. version:1
arxiv-1310-7637 | Regularization of $\ell_1$ minimization for dealing with outliers and noise in Statistics and Signal Recovery | http://arxiv.org/abs/1310.7637 | id:1310.7637 author:Salvador Flores, Luis M. Briceno-Arias category:math.OC stat.ML  published:2013-10-28 summary:We study the robustness properties of $\ell_1$ norm minimization for the classical linear regression problem with a given design matrix and contamination restricted to the dependent variable. We perform a fine error analysis of the $\ell_1$ estimator for measurements errors consisting of outliers coupled with noise. We introduce a new estimation technique resulting from a regularization of $\ell_1$ minimization by inf-convolution with the $\ell_2$ norm. Concerning robustness to large outliers, the proposed estimator keeps the breakdown point of the $\ell_1$ estimator, and reduces to least squares when there are not outliers. We present a globally convergent forward-backward algorithm for computing our estimator and some numerical experiments confirming its theoretical properties. version:2
arxiv-1303-6066 | Asymmetric Pruning for Learning Cascade Detectors | http://arxiv.org/abs/1303.6066 | id:1303.6066 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2013-03-25 summary:Cascade classifiers are one of the most important contributions to real-time object detection. Nonetheless, there are many challenging problems arising in training cascade detectors. One common issue is that the node classifier is trained with a symmetric classifier. Having a low misclassification error rate does not guarantee an optimal node learning goal in cascade classifiers, i.e., an extremely high detection rate with a moderate false positive rate. In this work, we present a new approach to train an effective node classifier in a cascade detector. The algorithm is based on two key observations: 1) Redundant weak classifiers can be safely discarded; 2) The final detector should satisfy the asymmetric learning objective of the cascade architecture. To achieve this, we separate the classifier training into two steps: finding a pool of discriminative weak classifiers/features and training the final classifier by pruning weak classifiers which contribute little to the asymmetric learning criterion (asymmetric classifier construction). Our model reduction approach helps accelerate the learning time while achieving the pre-determined learning objective. Experimental results on both face and car data sets verify the effectiveness of the proposed algorithm. On the FDDB face data sets, our approach achieves the state-of-the-art performance, which demonstrates the advantage of our approach. version:2
arxiv-1309-1628 | Topology preserving thinning for cell complexes | http://arxiv.org/abs/1309.1628 | id:1309.1628 author:Paweł Dłotko, Ruben Specogna category:cs.CV  published:2013-09-06 summary:A topology preserving skeleton is a synthetic representation of an object that retains its topology and many of its significant morphological properties. The process of obtaining the skeleton, referred to as skeletonization or thinning, is a very active research area. It plays a central role in reducing the amount of information to be processed during image analysis and visualization, computer-aided diagnosis or by pattern recognition algorithms. This paper introduces a novel topology preserving thinning algorithm which removes \textit{simple cells}---a generalization of simple points---of a given cell complex. The test for simple cells is based on \textit{acyclicity tables} automatically produced in advance with homology computations. Using acyclicity tables render the implementation of thinning algorithms straightforward. Moreover, the fact that tables are automatically filled for all possible configurations allows to rigorously prove the generality of the algorithm and to obtain fool-proof implementations. The novel approach enables, for the first time, according to our knowledge, to thin a general unstructured simplicial complex. Acyclicity tables for cubical and simplicial complexes and an open source implementation of the thinning algorithm are provided as additional material to allow their immediate use in the vast number of practical applications arising in medical imaging and beyond. version:2
arxiv-1402-6133 | Bayesian Sample Size Determination of Vibration Signals in Machine Learning Approach to Fault Diagnosis of Roller Bearings | http://arxiv.org/abs/1402.6133 | id:1402.6133 author:Siddhant Sahu, V. Sugumaran category:stat.ML cs.LG  published:2014-02-25 summary:Sample size determination for a data set is an important statistical process for analyzing the data to an optimum level of accuracy and using minimum computational work. The applications of this process are credible in every domain which deals with large data sets and high computational work. This study uses Bayesian analysis for determination of minimum sample size of vibration signals to be considered for fault diagnosis of a bearing using pre-defined parameters such as the inverse standard probability and the acceptable margin of error. Thus an analytical formula for sample size determination is introduced. The fault diagnosis of the bearing is done using a machine learning approach using an entropy-based J48 algorithm. The following method will help researchers involved in fault diagnosis to determine minimum sample size of data for analysis for a good statistical stability and precision. version:1
arxiv-1402-6077 | Inductive Logic Boosting | http://arxiv.org/abs/1402.6077 | id:1402.6077 author:Wang-Zhou Dai, Zhi-Hua Zhou category:cs.LG cs.AI  published:2014-02-25 summary:Recent years have seen a surge of interest in Probabilistic Logic Programming (PLP) and Statistical Relational Learning (SRL) models that combine logic with probabilities. Structure learning of these systems is an intersection area of Inductive Logic Programming (ILP) and statistical learning (SL). However, ILP cannot deal with probabilities, SL cannot model relational hypothesis. The biggest challenge of integrating these two machine learning frameworks is how to estimate the probability of a logic clause only from the observation of grounded logic atoms. Many current methods models a joint probability by representing clause as graphical model and literals as vertices in it. This model is still too complicate and only can be approximate by pseudo-likelihood. We propose Inductive Logic Boosting framework to transform the relational dataset into a feature-based dataset, induces logic rules by boosting Problog Rule Trees and relaxes the independence constraint of pseudo-likelihood. Experimental evaluation on benchmark datasets demonstrates that the AUC-PR and AUC-ROC value of ILP learned rules are higher than current state-of-the-art SRL methods. version:1
arxiv-1402-6076 | Machine Learning at Scale | http://arxiv.org/abs/1402.6076 | id:1402.6076 author:Sergei Izrailev, Jeremy M. Stanley category:cs.LG cs.MS stat.ML I.5.2  published:2014-02-25 summary:It takes skill to build a meaningful predictive model even with the abundance of implementations of modern machine learning algorithms and readily available computing resources. Building a model becomes challenging if hundreds of terabytes of data need to be processed to produce the training data set. In a digital advertising technology setting, we are faced with the need to build thousands of such models that predict user behavior and power advertising campaigns in a 24/7 chaotic real-time production environment. As data scientists, we also have to convince other internal departments critical to implementation success, our management, and our customers that our machine learning system works. In this paper, we present the details of the design and implementation of an automated, robust machine learning platform that impacts billions of advertising impressions monthly. This platform enables us to continuously optimize thousands of campaigns over hundreds of millions of users, on multiple continents, against varying performance objectives. version:1
arxiv-1402-3344 | Intrinsically Motivated Learning of Visual Motion Perception and Smooth Pursuit | http://arxiv.org/abs/1402.3344 | id:1402.3344 author:Chong Zhang, Yu Zhao, Jochen Triesch, Bertram E. Shi category:cs.CV q-bio.NC  published:2014-02-14 summary:We extend the framework of efficient coding, which has been used to model the development of sensory processing in isolation, to model the development of the perception/action cycle. Our extension combines sparse coding and reinforcement learning so that sensory processing and behavior co-develop to optimize a shared intrinsic motivational signal: the fidelity of the neural encoding of the sensory input under resource constraints. Applying this framework to a model system consisting of an active eye behaving in a time varying environment, we find that this generic principle leads to the simultaneous development of both smooth pursuit behavior and model neurons whose properties are similar to those of primary visual cortical neurons selective for different directions of visual motion. We suggest that this general principle may form the basis for a unified and integrated explanation of many perception/action loops. version:2
arxiv-1402-6034 | A DCT Approximation for Image Compression | http://arxiv.org/abs/1402.6034 | id:1402.6034 author:R. J. Cintra, F. M. Bayer category:cs.MM cs.CV stat.ME  published:2014-02-25 summary:An orthogonal approximation for the 8-point discrete cosine transform (DCT) is introduced. The proposed transformation matrix contains only zeros and ones; multiplications and bit-shift operations are absent. Close spectral behavior relative to the DCT was adopted as design criterion. The proposed algorithm is superior to the signed discrete cosine transform. It could also outperform state-of-the-art algorithms in low and high image compression scenarios, exhibiting at the same time a comparable computational complexity. version:1
arxiv-1402-6028 | Algorithms for multi-armed bandit problems | http://arxiv.org/abs/1402.6028 | id:1402.6028 author:Volodymyr Kuleshov, Doina Precup category:cs.AI cs.LG  published:2014-02-25 summary:Although many algorithms for the multi-armed bandit problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as epsilon-greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. Thirdly, the algorithms' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of application of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50% more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies. version:1
arxiv-1405-2062 | Precision Enhancement of 3D Surfaces from Multiple Compressed Depth Maps | http://arxiv.org/abs/1405.2062 | id:1405.2062 author:Pengfei Wan, Gene Cheung, Philip A. Chou, Dinei Florencio, Cha Zhang, Oscar C. Au category:cs.CV  published:2014-02-25 summary:In texture-plus-depth representation of a 3D scene, depth maps from different camera viewpoints are typically lossily compressed via the classical transform coding / coefficient quantization paradigm. In this paper we propose to reduce distortion of the decoded depth maps due to quantization. The key observation is that depth maps from different viewpoints constitute multiple descriptions (MD) of the same 3D scene. Considering the MD jointly, we perform a POCS-like iterative procedure to project a reconstructed signal from one depth map to the other and back, so that the converged depth maps have higher precision than the original quantized versions. version:1
arxiv-1402-6013 | Open science in machine learning | http://arxiv.org/abs/1402.6013 | id:1402.6013 author:Joaquin Vanschoren, Mikio L. Braun, Cheng Soon Ong category:cs.LG cs.DL  published:2014-02-24 summary:We present OpenML and mldata, open science platforms that provides easy access to machine learning data, software and results to encourage further study and application. They go beyond the more traditional repositories for data sets and software packages in that they allow researchers to also easily share the results they obtained in experiments and to compare their solutions with those of others. version:1
arxiv-1402-5979 | DCT-like Transform for Image and Video Compression Requires 10 Additions Only | http://arxiv.org/abs/1402.5979 | id:1402.5979 author:R. J. Cintra, F. M. Bayer, V. A. Coutinho, S. Kulasekera, A. Madanayake category:cs.MM cs.CV stat.ME  published:2014-02-24 summary:A multiplierless pruned approximate 8-point discrete cosine transform (DCT) requiring only 10 additions is introduced. The proposed algorithm was assessed in image and video compression, showing competitive performance with state-of-the-art methods. Digital implementation in 45 nm CMOS technology up to place-and-route level indicates clock speed of 255 MHz at a 1.1 V supply. The 8x8 block rate is 31.875 MHz.The DCT approximation was embedded into HEVC reference software; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC, presented negligible image degradation. version:1
arxiv-1402-5923 | A Testbed for Cross-Dataset Analysis | http://arxiv.org/abs/1402.5923 | id:1402.5923 author:Tatiana Tommasi, Tinne Tuytelaars, Barbara Caputo category:cs.CV  published:2014-02-24 summary:Since its beginning visual recognition research has tried to capture the huge variability of the visual world in several image collections. The number of available datasets is still progressively growing together with the amount of samples per object category. However, this trend does not correspond directly to an increasing in the generalization capabilities of the developed recognition systems. Each collection tends to have its specific characteristics and to cover just some aspects of the visual world: these biases often narrow the effect of the methods defined and tested separately over each image set. Our work makes a first step towards the analysis of the dataset bias problem on a large scale. We organize twelve existing databases in a unique corpus and we present the visual community with a useful feature repository for future research. version:1
arxiv-1402-5886 | Near Optimal Bayesian Active Learning for Decision Making | http://arxiv.org/abs/1402.5886 | id:1402.5886 author:Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew Bagnell, Siddhartha Srinivasa category:cs.LG cs.AI  published:2014-02-24 summary:How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the intractable optimal policy. Our efficient implementation of the algorithm relies on computing subsets of the complete homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator. version:1
arxiv-1402-5859 | A Novel Face Recognition Method using Nearest Line Projection | http://arxiv.org/abs/1402.5859 | id:1402.5859 author:Huanguo Zhang, Sha Lv, Wei Li, Xun Qu category:cs.CV  published:2014-02-24 summary:Face recognition is a popular application of pat- tern recognition methods, and it faces challenging problems including illumination, expression, and pose. The most popular way is to learn the subspaces of the face images so that it could be project to another discriminant space where images of different persons can be separated. In this paper, a nearest line projection algorithm is developed to represent the face images for face recognition. Instead of projecting an image to its nearest image, we try to project it to its nearest line spanned by two different face images. The subspaces are learned so that each face image to its nearest line is minimized. We evaluated the proposed algorithm on some benchmark face image database, and also compared it to some other image projection algorithms. The experiment results showed that the proposed algorithm outperforms other ones. version:1
arxiv-1307-0366 | Learning directed acyclic graphs based on sparsest permutations | http://arxiv.org/abs/1307.0366 | id:1307.0366 author:Garvesh Raskutti, Caroline Uhler category:math.ST cs.LG stat.TH  published:2013-07-01 summary:We consider the problem of learning a Bayesian network or directed acyclic graph (DAG) model from observational data. A number of constraint-based, score-based and hybrid algorithms have been developed for this purpose. For constraint-based methods, statistical consistency guarantees typically rely on the faithfulness assumption, which has been show to be restrictive especially for graphs with cycles in the skeleton. However, there is only limited work on consistency guarantees for score-based and hybrid algorithms and it has been unclear whether consistency guarantees can be proven under weaker conditions than the faithfulness assumption. In this paper, we propose the sparsest permutation (SP) algorithm. This algorithm is based on finding the causal ordering of the variables that yields the sparsest DAG. We prove that this new score-based method is consistent under strictly weaker conditions than the faithfulness assumption. We also demonstrate through simulations on small DAGs that the SP algorithm compares favorably to the constraint-based PC and SGS algorithms as well as the score-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method. In the Gaussian setting, we prove that our algorithm boils down to finding the permutation of the variables with sparsest Cholesky decomposition for the inverse covariance matrix. Using this connection, we show that in the oracle setting, where the true covariance matrix is known, the SP algorithm is in fact equivalent to $\ell_0$-penalized maximum likelihood estimation. version:3
arxiv-1402-5805 | Automatic Estimation of Live Coffee Leaf Infection based on Image Processing Techniques | http://arxiv.org/abs/1402.5805 | id:1402.5805 author:Eric Hitimana, Oubong Gwun category:cs.CV  published:2014-02-24 summary:Image segmentation is the most challenging issue in computer vision applications. And most difficulties for crops management in agriculture are the lack of appropriate methods for detecting the leaf damage for pests treatment. In this paper we proposed an automatic method for leaf damage detection and severity estimation of coffee leaf by avoiding defoliation. After enhancing the contrast of the original image using LUT based gamma correction, the image is processed to remove the background, and the output leaf is clustered using Fuzzy c-means segmentation in V channel of YUV color space to maximize all leaf damage detection, and finally, the severity of leaf is estimated in terms of ratio for leaf pixel distribution between the normal and the detected leaf damage. The results in each proposed method was compared to the current researches and the accuracy is obvious either in the background removal or damage detection. version:1
arxiv-1402-5803 | Sparse phase retrieval via group-sparse optimization | http://arxiv.org/abs/1402.5803 | id:1402.5803 author:Fabien Lauer, Henrik Ohlsson category:cs.IT cs.LG math.IT  published:2014-02-24 summary:This paper deals with sparse phase retrieval, i.e., the problem of estimating a vector from quadratic measurements under the assumption that few components are nonzero. In particular, we consider the problem of finding the sparsest vector consistent with the measurements and reformulate it as a group-sparse optimization problem with linear constraints. Then, we analyze the convex relaxation of the latter based on the minimization of a block l1-norm and show various exact recovery and stability results in the real and complex cases. Invariance to circular shifts and reflections are also discussed for real vectors measured via complex matrices. version:1
arxiv-1402-5766 | No more meta-parameter tuning in unsupervised sparse feature learning | http://arxiv.org/abs/1402.5766 | id:1402.5766 author:Adriana Romero, Petia Radeva, Carlo Gatta category:cs.LG cs.CV  published:2014-02-24 summary:We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well. version:1
arxiv-1402-5758 | Bandits with concave rewards and convex knapsacks | http://arxiv.org/abs/1402.5758 | id:1402.5758 author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG  published:2014-02-24 summary:In this paper, we consider a very general model for exploration-exploitation tradeoff which allows arbitrary concave rewards and convex constraints on the decisions across time, in addition to the customary limitation on the time horizon. This model subsumes the classic multi-armed bandit (MAB) model, and the Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We also consider an extension of this model to allow linear contexts, similar to the linear contextual extension of the MAB model. We demonstrate that a natural and simple extension of the UCB family of algorithms for MAB provides a polynomial time algorithm that has near-optimal regret guarantees for this substantially more general model, and matches the bounds provided by Badanidiyuru et al.[2013] for the special case of BwK, which is quite surprising. We also provide computationally more efficient algorithms by establishing interesting connections between this problem and other well studied problems/algorithms such as the Blackwell approachability problem, online convex optimization, and the Frank-Wolfe technique for convex optimization. We give examples of several concrete applications, where this more general model of bandits allows for richer and/or more efficient formulations of the problem. version:1
arxiv-1402-5728 | Machine Learning Methods in the Computational Biology of Cancer | http://arxiv.org/abs/1402.5728 | id:1402.5728 author:Mathukumalli Vidyasagar category:q-bio.QM cs.LG stat.ML 62P10  published:2014-02-24 summary:The objectives of this "perspective" paper are to review some recent advances in sparse feature selection for regression and classification, as well as compressed sensing, and to discuss how these might be used to develop tools to advance personalized cancer therapy. As an illustration of the possibilities, a new algorithm for sparse regression is presented, and is applied to predict the time to tumor recurrence in ovarian cancer. A new algorithm for sparse feature selection in classification problems is presented, and its validation in endometrial cancer is briefly discussed. Some open problems are also presented. version:1
arxiv-1312-6229 | OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks | http://arxiv.org/abs/1312.6229 | id:1312.6229 author:Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun category:cs.CV  published:2013-12-21 summary:We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat. version:4
arxiv-1402-5708 | The Cerebellum: New Computational Model that Reveals its Primary Function to Calculate Multibody Dynamics Conform to Lagrange-Euler Formulation | http://arxiv.org/abs/1402.5708 | id:1402.5708 author:Lavdim Kurtaj, Ilir Limani, Vjosa Shatri, Avni Skeja category:cs.NE cs.CE cs.RO q-bio.NC C.1.3; F.1.1; I.2.6  published:2014-02-24 summary:Cerebellum is part of the brain that occupies only 10% of the brain volume, but it contains about 80% of total number of brain neurons. New cerebellar function model is developed that sets cerebellar circuits in context of multibody dynamics model computations, as important step in controlling balance and movement coordination, functions performed by two oldest parts of the cerebellum. Model gives new functional interpretation for granule cells-Golgi cell circuit, including distinct function for upper and lower Golgi cell dendritc trees, and resolves issue of sharing Granule cells between Purkinje cells. Sets new function for basket cells, and for stellate cells according to position in molecular layer. New model enables easily and direct integration of sensory information from vestibular system and cutaneous mechanoreceptors, for balance, movement and interaction with environments. Model gives explanation of Purkinje cells convergence on deep-cerebellar nuclei. version:1
arxiv-1402-3580 | Bayesian Inference for NMR Spectroscopy with Applications to Chemical Quantification | http://arxiv.org/abs/1402.3580 | id:1402.3580 author:Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin, Mick D. Mantle, Lynn F. Gladden, Andrew Blake category:stat.AP stat.ME stat.ML  published:2014-02-14 summary:Nuclear magnetic resonance (NMR) spectroscopy exploits the magnetic properties of atomic nuclei to discover the structure, reaction state and chemical environment of molecules. We propose a probabilistic generative model and inference procedures for NMR spectroscopy. Specifically, we use a weighted sum of trigonometric functions undergoing exponential decay to model free induction decay (FID) signals. We discuss the challenges in estimating the components of this general model -- amplitudes, phase shifts, frequencies, decay rates, and noise variances -- and offer practical solutions. We compare with conventional Fourier transform spectroscopy for estimating the relative concentrations of chemicals in a mixture, using synthetic and experimentally acquired FID signals. We find the proposed model is particularly robust to low signal to noise ratios (SNR), and overlapping peaks in the Fourier transform of the FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) which are not possible with conventional spectroscopy (5% sensitivity). version:2
arxiv-1402-5697 | Exemplar-based Linear Discriminant Analysis for Robust Object Tracking | http://arxiv.org/abs/1402.5697 | id:1402.5697 author:Changxin Gao, Feifei Chen, Jin-Gang Yu, Rui Huang, Nong Sang category:cs.CV  published:2014-02-24 summary:Tracking-by-detection has become an attractive tracking technique, which treats tracking as a category detection problem. However, the task in tracking is to search for a specific object, rather than an object category as in detection. In this paper, we propose a novel tracking framework based on exemplar detector rather than category detector. The proposed tracker is an ensemble of exemplar-based linear discriminant analysis (ELDA) detectors. Each detector is quite specific and discriminative, because it is trained by a single object instance and massive negatives. To improve its adaptivity, we update both object and background models. Experimental results on several challenging video sequences demonstrate the effectiveness and robustness of our tracking algorithm. version:1
arxiv-1401-1436 | Accelerating ABC methods using Gaussian processes | http://arxiv.org/abs/1401.1436 | id:1401.1436 author:Richard D Wilkinson category:stat.CO stat.ML  published:2014-01-07 summary:Approximate Bayesian computation (ABC) methods are used to approximate posterior distributions using simulation rather than likelihood calculations. We introduce Gaussian process (GP) accelerated ABC, which we show can significantly reduce the number of simulations required. As computational resource is usually the main determinant of accuracy in ABC, GP-accelerated methods can thus enable more accurate inference in some models. GP models of the unknown log-likelihood function are used to exploit continuity and smoothness, reducing the required computation. We use a sequence of models that increase in accuracy, using intermediate models to rule out regions of the parameter space as implausible. The methods will not be suitable for all problems, but when they can be used, can result in significant computational savings. For the Ricker model, we are able to achieve accurate approximations to the posterior distribution using a factor of 100 fewer simulator evaluations than comparable Monte Carlo approaches, and for a population genetics model we are able to approximate the exact posterior for the first time. version:2
arxiv-1402-5634 | To go deep or wide in learning? | http://arxiv.org/abs/1402.5634 | id:1402.5634 author:Gaurav Pandey, Ambedkar Dukkipati category:cs.LG  published:2014-02-23 summary:To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets. version:1
arxiv-1402-5623 | Localization of License Plate Using Morphological Operations | http://arxiv.org/abs/1402.5623 | id:1402.5623 author:V. Karthikeyan, V. J. Vijayalakshmi category:cs.CV  published:2014-02-23 summary:It is believed that there are currently millions of vehicles on the roads worldwide. The over speed of vehicles,theft of vehicles, disobeying traffic rules in public, an unauthorized person entering the restricted area are keep on increasing. In order restrict against these criminal activities, we need an automatic public security system. Each vehicle has their own Vehicle Identification Number (VIN) as their primary identifier. The VIN is actually a License Number which states a legal license to participate in the public traffic. The proposed paper is to identify the vehicle with the help of vehicles License Plate (LP).LPRS is one the most important part of the Intelligent Transportation System (ITS) to locate the LP. In this paper certain existing algorithm drawbacks are overcome by the proposed morphological operations for LPRS. Morphological operation is chosen due to its higher efficiency, noise filter capacity, accuracy, exact localization of LP and speed. version:1
arxiv-1402-5619 | A Novel Histogram Based Robust Image Registration Technique | http://arxiv.org/abs/1402.5619 | id:1402.5619 author:V. Karthikeyan category:cs.CV  published:2014-02-23 summary:In this paper, a method for Automatic Image Registration (AIR) through histogram is proposed. Automatic image registration is one of the crucial steps in the analysis of remotely sensed data. A new acquired image must be transformed, using image registration techniques, to match the orientation and scale of previous related images. This new approach combines several segmentations of the pair of images to be registered. A relaxation parameter on the histogram modes delineation is introduced. It is followed by characterization of the extracted objects through the objects area, axis ratio, and perimeter and fractal dimension. The matched objects are used for rotation and translation estimation. It allows for the registration of pairs of images with differences in rotation and translation. This method contributes to subpixel accuracy. version:1
arxiv-1402-5584 | Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse Regression | http://arxiv.org/abs/1402.5584 | id:1402.5584 author:Divyanshu Vats, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH  published:2014-02-23 summary:In this paper, we address the challenging problem of selecting tuning parameters for high-dimensional sparse regression. We propose a simple and computationally efficient method, called path thresholding (PaTh), that transforms any tuning parameter-dependent sparse regression algorithm into an asymptotically tuning-free sparse regression algorithm. More specifically, we prove that, as the problem size becomes large (in the number of variables and in the number of observations), PaTh performs accurate sparse regression, under appropriate conditions, without specifying a tuning parameter. In finite-dimensional settings, we demonstrate that PaTh can alleviate the computational burden of model selection algorithms by significantly reducing the search space of tuning parameters. version:1
arxiv-1402-5565 | Semi-Supervised Nonlinear Distance Metric Learning via Forests of Max-Margin Cluster Hierarchies | http://arxiv.org/abs/1402.5565 | id:1402.5565 author:David M. Johnson, Caiming Xiong, Jason J. Corso category:stat.ML cs.IR cs.LG I.5.3; H.2.8  published:2014-02-23 summary:Metric learning is a key problem for many data mining and machine learning applications, and has long been dominated by Mahalanobis methods. Recent advances in nonlinear metric learning have demonstrated the potential power of non-Mahalanobis distance functions, particularly tree-based functions. We propose a novel nonlinear metric learning method that uses an iterative, hierarchical variant of semi-supervised max-margin clustering to construct a forest of cluster hierarchies, where each individual hierarchy can be interpreted as a weak metric over the data. By introducing randomness during hierarchy training and combining the output of many of the resulting semi-random weak hierarchy metrics, we can obtain a powerful and robust nonlinear metric model. This method has two primary contributions: first, it is semi-supervised, incorporating information from both constrained and unconstrained points. Second, we take a relaxed approach to constraint satisfaction, allowing the method to satisfy different subsets of the constraints at different levels of the hierarchy rather than attempting to simultaneously satisfy all of them. This leads to a more robust learning algorithm. We compare our method to a number of state-of-the-art benchmarks on $k$-nearest neighbor classification, large-scale image retrieval and semi-supervised clustering problems, and find that our algorithm yields results comparable or superior to the state-of-the-art, and is significantly more robust to noise. version:1
arxiv-1312-1706 | Swapping Variables for High-Dimensional Sparse Regression with Correlated Measurements | http://arxiv.org/abs/1312.1706 | id:1312.1706 author:Divyanshu Vats, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH  published:2013-12-05 summary:We consider the high-dimensional sparse linear regression problem of accurately estimating a sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that the standard cadre of computationally tractable sparse regression algorithms---such as the Lasso, Orthogonal Matching Pursuit (OMP), and their extensions---perform poorly when the measurement matrix contains highly correlated columns. To address this shortcoming, we develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until convergence. SWAP is surprisingly effective in handling measurement matrices with high correlations. In fact, we prove that SWAP outputs the true support, the locations of the non-zero entries in the sparse vector, under a relatively mild condition on the measurement matrix. Furthermore, we show that SWAP can be used to boost the performance of any sparse regression algorithm. We empirically demonstrate the advantages of SWAP by comparing it with several state-of-the-art sparse regression algorithms. version:2
arxiv-1401-3579 | A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An Actor-Critic Approach | http://arxiv.org/abs/1401.3579 | id:1401.3579 author:Keyvan Yahya category:cs.GT cs.AI cs.LG  published:2013-12-20 summary:This paper aims to find an algorithmic structure that affords to predict and explain economical choice behaviour particularly under uncertainty(random policies) by manipulating the prevalent Actor-Critic learning method to comply with the requirements we have been entrusted ever since the field of neuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that seem relevant to our discussion, we will try to outline some of the important works which have so far been done to simulate choice making processes. Concerning neurological findings that suggest the existence of two specific functions that are executed through Basal Ganglia all the way up to sub- cortical areas, namely 'rewards' and 'beliefs', we will offer a modified version of actor/critic algorithm to shed a light on the relation between these functions and most importantly resolve what is referred to as a challenge for actor-critic algorithms, that is, the lack of inheritance or hierarchy which avoids the system being evolved in continuous time tasks whence the convergence might not be emerged. version:2
arxiv-1402-5497 | Efficient Semidefinite Spectral Clustering via Lagrange Duality | http://arxiv.org/abs/1402.5497 | id:1402.5497 author:Yan Yan, Chunhua Shen, Hanzi Wang category:cs.LG cs.CV  published:2014-02-22 summary:We propose an efficient approach to semidefinite spectral clustering (SSC), which addresses the Frobenius normalization with the positive semidefinite (p.s.d.) constraint for spectral clustering. Compared with the original Frobenius norm approximation based algorithm, the proposed algorithm can more accurately find the closest doubly stochastic approximation to the affinity matrix by considering the p.s.d. constraint. In this paper, SSC is formulated as a semidefinite programming (SDP) problem. In order to solve the high computational complexity of SDP, we present a dual algorithm based on the Lagrange dual formalization. Two versions of the proposed algorithm are proffered: one with less memory usage and the other with faster convergence rate. The proposed algorithm has much lower time complexity than that of the standard interior-point based SDP solvers. Experimental results on both UCI data sets and real-world image data sets demonstrate that 1) compared with the state-of-the-art spectral clustering methods, the proposed algorithm achieves better clustering performance; and 2) our algorithm is much more efficient and can solve larger-scale SSC problems than those standard interior-point SDP solvers. version:1
arxiv-1306-5056 | Class Proportion Estimation with Application to Multiclass Anomaly Rejection | http://arxiv.org/abs/1306.5056 | id:1306.5056 author:Tyler Sanderson, Clayton Scott category:stat.ML cs.LG  published:2013-06-21 summary:This work addresses two classification problems that fall under the heading of domain adaptation, wherein the distributions of training and testing examples differ. The first problem studied is that of class proportion estimation, which is the problem of estimating the class proportions in an unlabeled testing data set given labeled examples of each class. Compared to previous work on this problem, our approach has the novel feature that it does not require labeled training data from one of the classes. This property allows us to address the second domain adaptation problem, namely, multiclass anomaly rejection. Here, the goal is to design a classifier that has the option of assigning a "reject" label, indicating that the instance did not arise from a class present in the training data. We establish consistent learning strategies for both of these domain adaptation problems, which to our knowledge are the first of their kind. We also implement the class proportion estimation technique and demonstrate its performance on several benchmark data sets. version:3
arxiv-1402-5473 | Scaling Nonparametric Bayesian Inference via Subsample-Annealing | http://arxiv.org/abs/1402.5473 | id:1402.5473 author:Fritz Obermeyer, Jonathan Glidden, Eric Jonas category:stat.ML stat.CO  published:2014-02-22 summary:We describe an adaptation of the simulated annealing algorithm to nonparametric clustering and related probabilistic models. This new algorithm learns nonparametric latent structure over a growing and constantly churning subsample of training data, where the portion of data subsampled can be interpreted as the inverse temperature beta(t) in an annealing schedule. Gibbs sampling at high temperature (i.e., with a very small subsample) can more quickly explore sketches of the final latent state by (a) making longer jumps around latent space (as in block Gibbs) and (b) lowering energy barriers (as in simulated annealing). We prove subsample annealing speeds up mixing time N^2 -> N in a simple clustering model and exp(N) -> N in another class of models, where N is data size. Empirically subsample-annealing outperforms naive Gibbs sampling in accuracy-per-wallclock time, and can scale to larger datasets and deeper hierarchical models. We demonstrate improved inference on million-row subsamples of US Census data and network log data and a 307-row hospital rating dataset, using a Pitman-Yor generalization of the Cross Categorization model. version:1
arxiv-1402-5458 | Information Aggregation in Exponential Family Markets | http://arxiv.org/abs/1402.5458 | id:1402.5458 author:Jacob Abernethy, Sindhu Kutty, Sébastien Lahaie, Rahul Sami category:cs.AI cs.GT stat.ML  published:2014-02-22 summary:We consider the design of prediction market mechanisms known as automated market makers. We show that we can design these mechanisms via the mold of \emph{exponential family distributions}, a popular and well-studied probability distribution template used in statistics. We give a full development of this relationship and explore a range of benefits. We draw connections between the information aggregation of market prices and the belief aggregation of learning agents that rely on exponential family distributions. We develop a very natural analysis of the market behavior as well as the price equilibrium under the assumption that the traders exhibit risk aversion according to exponential utility. We also consider similar aspects under alternative models, such as when traders are budget constrained. version:1
arxiv-1402-4884 | Le Cam meets LeCun: Deficiency and Generic Feature Learning | http://arxiv.org/abs/1402.4884 | id:1402.4884 author:Brendan van Rooyen, Robert C. Williamson category:stat.ML  published:2014-02-20 summary:"Deep Learning" methods attempt to learn generic features in an unsupervised fashion from a large unlabelled data set. These generic features should perform as well as the best hand crafted features for any learning problem that makes use of this data. We provide a definition of generic features, characterize when it is possible to learn them and provide methods closely related to the autoencoder and deep belief network of deep learning. In order to do so we use the notion of deficiency and illustrate its value in studying certain general learning problems. version:2
arxiv-1402-5428 | An Evolutionary approach for solving Shrödinger Equation | http://arxiv.org/abs/1402.5428 | id:1402.5428 author:Khalid jebari, Mohammed Madiafi, Abdelaziz Elmoujahid category:cs.NE  published:2014-02-21 summary:The purpose of this paper is to present a method of solving the Shr\"odinger Equation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms generations of trial solutions expressed in an analytical form. We illustrate the effectiveness of this method providing, for example, the results of its application to a quantum system minimal energy, and we compare these results with those produced by traditional analytical methods version:1
arxiv-1211-3046 | Recovering the Optimal Solution by Dual Random Projection | http://arxiv.org/abs/1211.3046 | id:1211.3046 author:Lijun Zhang, Mehrdad Mahdavi, Rong Jin, Tianbao Yang, Shenghuo Zhu category:cs.LG  published:2012-11-13 summary:Random projection has been widely used in data classification. It maps high-dimensional data into a low-dimensional subspace in order to reduce the computational cost in solving the related optimization problem. While previous studies are focused on analyzing the classification performance of using random projection, in this work, we consider the recovery problem, i.e., how to accurately recover the optimal solution to the original optimization problem in the high-dimensional space based on the solution learned from the subspace spanned by random projections. We present a simple algorithm, termed Dual Random Projection, that uses the dual solution of the low-dimensional optimization problem to recover the optimal solution to the original problem. Our theoretical analysis shows that with a high probability, the proposed algorithm is able to accurately recover the optimal solution to the original problem, provided that the data matrix is of low rank or can be well approximated by a low rank matrix. version:4
arxiv-1402-5360 | Important Molecular Descriptors Selection Using Self Tuned Reweighted Sampling Method for Prediction of Antituberculosis Activity | http://arxiv.org/abs/1402.5360 | id:1402.5360 author:Doreswamy, Chanabasayya M. Vastrad category:cs.LG stat.AP stat.ML  published:2014-02-21 summary:In this paper, a new descriptor selection method for selecting an optimal combination of important descriptors of sulfonamide derivatives data, named self tuned reweighted sampling (STRS), is developed. descriptors are defined as the descriptors with large absolute coefficients in a multivariate linear regression model such as partial least squares(PLS). In this study, the absolute values of regression coefficients of PLS model are used as an index for evaluating the importance of each descriptor Then, based on the importance level of each descriptor, STRS sequentially selects N subsets of descriptors from N Monte Carlo (MC) sampling runs in an iterative and competitive manner. In each sampling run, a fixed ratio (e.g. 80%) of samples is first randomly selected to establish a regresson model. Next, based on the regression coefficients, a two-step procedure including rapidly decreasing function (RDF) based enforced descriptor selection and self tuned sampling (STS) based competitive descriptor selection is adopted to select the important descriptorss. After running the loops, a number of subsets of descriptors are obtained and root mean squared error of cross validation (RMSECV) of PLS models established with subsets of descriptors is computed. The subset of descriptors with the lowest RMSECV is considered as the optimal descriptor subset. The performance of the proposed algorithm is evaluated by sulfanomide derivative dataset. The results reveal an good characteristic of STRS that it can usually locate an optimal combination of some important descriptors which are interpretable to the biologically of interest. Additionally, our study shows that better prediction is obtained by STRS when compared to full descriptor set PLS modeling, Monte Carlo uninformative variable elimination (MC-UVE). version:1
arxiv-1402-5073 | Exploiting Two-Dimensional Group Sparsity in 1-Bit Compressive Sensing | http://arxiv.org/abs/1402.5073 | id:1402.5073 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT  published:2014-02-20 summary:We propose a new approach, {\it two-dimensional fused binary compressive sensing} (2DFBCS) to recover 2D sparse piece-wise signals from 1-bit measurements, exploiting 2D group sparsity for 1-bit compressive sensing recovery. The proposed method is a modified 2D version of the previous {\it binary iterative hard thresholding} (2DBIHT) algorithm, where the objective function includes a 2D one-sided $\ell_1$ (or $\ell_2$) penalty function encouraging agreement with the observed data, an indicator function of $K$-sparsity, and a total variation (TV) or modified TV (MTV) constraint. The subgradient of the 2D one-sided $\ell_1$ (or $\ell_2$) penalty and the projection onto the $K$-sparsity and TV or MTV constraint can be computed efficiently, allowing the appliaction of algorithms of the {\it forward-backward splitting} (a.k.a. {\it iterative shrinkage-thresholding}) family. Experiments on the recovery of 2D sparse piece-wise smooth signals show that the proposed approach is able to take advantage of the piece-wise smoothness of the original signal, achieving more accurate recovery than 2DBIHT. More specifically, 2DFBCS with the MTV and the $\ell_2$ penalty performs best amongst the algorithms tested. version:2
arxiv-1308-6628 | Joint Video and Text Parsing for Understanding Events and Answering Queries | http://arxiv.org/abs/1308.6628 | id:1308.6628 author:Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, Song-Chun Zhu category:cs.CV cs.CL cs.MM  published:2013-08-29 summary:We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results. version:2
arxiv-1402-5176 | Pareto-depth for Multiple-query Image Retrieval | http://arxiv.org/abs/1402.5176 | id:1402.5176 author:Ko-Jen Hsiao, Jeff Calder, Alfred O. Hero III category:cs.IR cs.LG stat.ML  published:2014-02-21 summary:Most content-based image retrieval systems consider either one single query, or multiple queries that include the same object or represent the same semantic information. In this paper we consider the content-based image retrieval problem for multiple query images corresponding to different image semantics. We propose a novel multiple-query information retrieval algorithm that combines the Pareto front method (PFM) with efficient manifold ranking (EMR). We show that our proposed algorithm outperforms state of the art multiple-query retrieval algorithms on real-world image databases. We attribute this performance improvement to concavity properties of the Pareto fronts, and prove a theoretical result that characterizes the asymptotic concavity of the fronts. version:1
arxiv-1402-5164 | Distribution-Independent Reliable Learning | http://arxiv.org/abs/1402.5164 | id:1402.5164 author:Varun Kanade, Justin Thaler category:cs.LG cs.CC cs.DS  published:2014-02-20 summary:We study several questions in the reliable agnostic learning framework of Kalai et al. (2009), which captures learning tasks in which one type of error is costlier than others. A positive reliable classifier is one that makes no false positive errors. The goal in the positive reliable agnostic framework is to output a hypothesis with the following properties: (i) its false positive error rate is at most $\epsilon$, (ii) its false negative error rate is at most $\epsilon$ more than that of the best positive reliable classifier from the class. A closely related notion is fully reliable agnostic learning, which considers partial classifiers that are allowed to predict "unknown" on some inputs. The best fully reliable partial classifier is one that makes no errors and minimizes the probability of predicting "unknown", and the goal in fully reliable learning is to output a hypothesis that is almost as good as the best fully reliable partial classifier from a class. For distribution-independent learning, the best known algorithms for PAC learning typically utilize polynomial threshold representations, while the state of the art agnostic learning algorithms use point-wise polynomial approximations. We show that one-sided polynomial approximations, an intermediate notion between polynomial threshold representations and point-wise polynomial approximations, suffice for learning in the reliable agnostic settings. We then show that majorities can be fully reliably learned and disjunctions of majorities can be positive reliably learned, through constructions of appropriate one-sided polynomial approximations. Our fully reliable algorithm for majorities provides the first evidence that fully reliable learning may be strictly easier than agnostic learning. Our algorithms also satisfy strong attribute-efficiency properties, and provide smooth tradeoffs between sample complexity and running time. version:1
arxiv-1402-5123 | Detecting Opinions in Tweets | http://arxiv.org/abs/1402.5123 | id:1402.5123 author:Abdelmalek Amine, Reda Mohamed Hamou, Michel Simonet category:cs.CL cs.SI  published:2014-02-20 summary:Given the incessant growth of documents describing the opinions of different people circulating on the web, including Web 2.0 has made it possible to give an opinion on any product in the net. In this paper, we examine the various opinions expressed in the tweets and classify them positive, negative or neutral by using the emoticons for the Bayesian method and adjectives and adverbs for the Turney's method version:1
arxiv-1402-5077 | Group-sparse Matrix Recovery | http://arxiv.org/abs/1402.5077 | id:1402.5077 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.LG cs.CV stat.ML  published:2014-02-20 summary:We apply the OSCAR (octagonal selection and clustering algorithms for regression) in recovering group-sparse matrices (two-dimensional---2D---arrays) from compressive measurements. We propose a 2D version of OSCAR (2OSCAR) consisting of the $\ell_1$ norm and the pair-wise $\ell_{\infty}$ norm, which is convex but non-differentiable. We show that the proximity operator of 2OSCAR can be computed based on that of OSCAR. The 2OSCAR problem can thus be efficiently solved by state-of-the-art proximal splitting algorithms. Experiments on group-sparse 2D array recovery show that 2OSCAR regularization solved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm (with debiasing) yields the most accurate results. version:1
arxiv-1402-5074 | Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group Sparsity | http://arxiv.org/abs/1402.5074 | id:1402.5074 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT  published:2014-02-20 summary:We propose a new method, {\it binary fused compressive sensing} (BFCS), to recover sparse piece-wise smooth signals from 1-bit compressive measurements. The proposed algorithm is a modification of the previous {\it binary iterative hard thresholding} (BIHT) algorithm, where, in addition to the sparsity constraint, the total-variation of the recovered signal is upper constrained. As in BIHT, the data term of the objective function is an one-sided $\ell_1$ (or $\ell_2$) norm. Experiments on the recovery of sparse piece-wise smooth signals show that the proposed algorithm is able to take advantage of the piece-wise smoothness of the original signal, achieving more accurate recovery than BIHT. version:1
arxiv-1310-4945 | A novel sparsity and clustering regularization | http://arxiv.org/abs/1310.4945 | id:1310.4945 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.LG cs.CV stat.ML  published:2013-10-18 summary:We propose a novel SPARsity and Clustering (SPARC) regularizer, which is a modified version of the previous octagonal shrinkage and clustering algorithm for regression (OSCAR), where, the proposed regularizer consists of a $K$-sparse constraint and a pair-wise $\ell_{\infty}$ norm restricted on the $K$ largest components in magnitude. The proposed regularizer is able to separably enforce $K$-sparsity and encourage the non-zeros to be equal in magnitude. Moreover, it can accurately group the features without shrinking their magnitude. In fact, SPARC is closely related to OSCAR, so that the proximity operator of the former can be efficiently computed based on that of the latter, allowing using proximal splitting algorithms to solve problems with SPARC regularization. Experiments on synthetic data and with benchmark breast cancer data show that SPARC is a competitive group-sparsity inducing regularizer for regression and classification. version:2
arxiv-1402-5047 | Real-time Automatic Emotion Recognition from Body Gestures | http://arxiv.org/abs/1402.5047 | id:1402.5047 author:Stefano Piana, Alessandra Staglianò, Francesca Odone, Alessandro Verri, Antonio Camurri category:cs.HC cs.CV  published:2014-02-20 summary:Although psychological research indicates that bodily expressions convey important affective information, to date research in emotion recognition focused mainly on facial expression or voice analysis. In this paper we propose an approach to realtime automatic emotion recognition from body movements. A set of postural, kinematic, and geometrical features are extracted from sequences 3D skeletons and fed to a multi-class SVM classifier. The proposed method has been assessed on data acquired through two different systems: a professionalgrade optical motion capture system, and Microsoft Kinect. The system has been assessed on a "six emotions" recognition problem, and using a leave-one-subject-out cross validation strategy, reached an overall recognition rate of 61.3% which is very close to the recognition rate of 61.9% obtained by human observers. To provide further testing of the system, two games were developed, where one or two users have to interact to understand and express emotions with their body. version:1
arxiv-1304-7045 | An Algorithm for Training Polynomial Networks | http://arxiv.org/abs/1304.7045 | id:1304.7045 author:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir category:cs.LG cs.AI stat.ML  published:2013-04-26 summary:We consider deep neural networks, in which the output of each node is a quadratic function of its inputs. Similar to other deep architectures, these networks can compactly represent any function on a finite training set. The main goal of this paper is the derivation of an efficient layer-by-layer algorithm for training such networks, which we denote as the \emph{Basis Learner}. The algorithm is a universal learner in the sense that the training error is guaranteed to decrease at every iteration, and can eventually reach zero under mild conditions. We present practical implementations of this algorithm, as well as preliminary experimental results. We also compare our deep architecture to other shallow architectures for learning polynomials, in particular kernel learning. version:2
arxiv-1402-4936 | Enhanced Secure Algorithm for Fingerprint Recognition | http://arxiv.org/abs/1402.4936 | id:1402.4936 author:Amira Mohammad Abdel-Mawgoud Saleh category:cs.CV  published:2014-02-20 summary:Fingerprint recognition requires a minimal effort from the user, does not capture other information than strictly necessary for the recognition process, and provides relatively good performance. A critical step in fingerprint identification system is thinning of the input fingerprint image. The performance of a minutiae extraction algorithm relies heavily on the quality of the thinning algorithm. So, a fast fingerprint thinning algorithm is proposed. The algorithm works directly on the gray-scale image as binarization of fingerprint causes many spurious minutiae and also removes many important features. The performance of the thinning algorithm is evaluated and experimental results show that the proposed thinning algorithm is both fast and accurate. A new minutiae-based fingerprint matching technique is proposed. The main idea is that each fingerprint is represented by a minutiae table of just two columns in the database. The number of different minutiae types (terminations and bifurcations) found in each track of a certain width around the core point of the fingerprint is recorded in this table. Each row in the table represents a certain track, in the first column, the number of terminations in each track is recorded, in the second column, the number of bifurcations in each track is recorded. The algorithm is rotation and translation invariant, and needs less storage size. Experimental results show that recognition accuracy is 98%, with Equal Error Rate (EER) of 2%. Finally, the integrity of the data transmission via communication channels must be secure all the way from the scanner to the application. After applying Gaussian noise addition, and JPEG compression with high and moderate quality factors on the watermarked fingerprint images, recognition accuracy decreases slightly to reach 96%. version:1
arxiv-1402-4888 | Survey on Sparse Coded Features for Content Based Face Image Retrieval | http://arxiv.org/abs/1402.4888 | id:1402.4888 author:D. Johnvictor, G. Selvavinayagam category:cs.IR cs.CV cs.LG stat.ML  published:2014-02-20 summary:Content based image retrieval, a technique which uses visual contents of image to search images from large scale image databases according to users' interests. This paper provides a comprehensive survey on recent technology used in the area of content based face image retrieval. Nowadays digital devices and photo sharing sites are getting more popularity, large human face photos are available in database. Multiple types of facial features are used to represent discriminality on large scale human facial image database. Searching and mining of facial images are challenging problems and important research issues. Sparse representation on features provides significant improvement in indexing related images to query image. version:1
arxiv-1403-7087 | Conclusions from a NAIVE Bayes Operator Predicting the Medicare 2011 Transaction Data Set | http://arxiv.org/abs/1403.7087 | id:1403.7087 author:Nick Williams category:cs.LG cs.CY physics.data-an 62  91  published:2014-02-20 summary:Introduction: The United States Federal Government operates one of the worlds largest medical insurance programs, Medicare, to ensure payment for clinical services for the elderly, illegal aliens and those without the ability to pay for their care directly. This paper evaluates the Medicare 2011 Transaction Data Set which details the transfer of funds from Medicare to private and public clinical care facilities for specific clinical services for the operational year 2011. Methods: Data mining was conducted to establish the relationships between reported and computed transaction values in the data set to better understand the drivers of Medicare transactions at a programmatic level. Results: The models averaged 88 for average model accuracy and 38 for average Kappa during training. Some reported classes are highly independent from the available data as their predictability remains stable regardless of redaction of supporting and contradictory evidence. DRG or procedure type appears to be unpredictable from the available financial transaction values. Conclusions: Overlay hypotheses such as charges being driven by the volume served or DRG being related to charges or payments is readily false in this analysis despite 28 million Americans being billed through Medicare in 2011 and the program distributing over 70 billion in this transaction set alone. It may be impossible to predict the dependencies and data structures the payer of last resort without data from payers of first and second resort. Political concerns about Medicare would be better served focusing on these first and second order payer systems as what Medicare costs is not dependent on Medicare itself. version:1
arxiv-1306-0225 | Convergence Analysis and Parallel Computing Implementation for the Multiagent Coordination Optimization Algorithm | http://arxiv.org/abs/1306.0225 | id:1306.0225 author:Qing Hui, Haopeng Zhang category:math.OC cs.NE math.DS G.1.0; G.1.6; I.2.8  published:2013-06-02 summary:In this report, a novel variation of Particle Swarm Optimization (PSO) algorithm, called Multiagent Coordination Optimization (MCO), is implemented in a parallel computing way for practical use by introducing MATLAB built-in function "parfor" into MCO. Then we rigorously analyze the global convergence of MCO by means of semistability theory. Besides sharing global optimal solutions with the PSO algorithm, the MCO algorithm integrates cooperative swarm behavior of multiple agents into the update formula by sharing velocity and position information between neighbors to improve its performance. Numerical evaluation of the parallel MCO algorithm is provided in the report by running the proposed algorithm on supercomputers in the High Performance Computing Center at Texas Tech University. In particular, the optimal value and consuming time are compared with PSO and serial MCO by solving several benchmark functions in the literature, respectively. Based on the simulation results, the performance of the parallel MCO is not only superb compared with PSO for solving many nonlinear, noncovex optimization problems, but also is of high efficiency by saving the computational time. version:10
arxiv-1402-4862 | Learning the Parameters of Determinantal Point Process Kernels | http://arxiv.org/abs/1402.4862 | id:1402.4862 author:Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, Ben Taskar category:stat.ML cs.LG  published:2014-02-20 summary:Determinantal point processes (DPPs) are well-suited for modeling repulsion and have proven useful in many applications where diversity is desired. While DPPs have many appealing properties, such as efficient sampling, learning the parameters of a DPP is still considered a difficult problem due to the non-convex nature of the likelihood function. In this paper, we propose using Bayesian methods to learn the DPP kernel parameters. These methods are applicable in large-scale and continuous DPP settings even when the exact form of the eigendecomposition is unknown. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on spatial distribution of nerve fibers, and in studying human perception of diversity in images. version:1
arxiv-1402-4861 | A Quasi-Newton Method for Large Scale Support Vector Machines | http://arxiv.org/abs/1402.4861 | id:1402.4861 author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG  published:2014-02-20 summary:This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors. version:1
arxiv-1204-2069 | Asymptotic Accuracy of Distribution-Based Estimation for Latent Variables | http://arxiv.org/abs/1204.2069 | id:1204.2069 author:Keisuke Yamazaki category:stat.ML cs.LG  published:2012-04-10 summary:Hierarchical statistical models are widely employed in information science and data engineering. The models consist of two types of variables: observable variables that represent the given data and latent variables for the unobservable labels. An asymptotic analysis of the models plays an important role in evaluating the learning process; the result of the analysis is applied not only to theoretical but also to practical situations, such as optimal model selection and active learning. There are many studies of generalization errors, which measure the prediction accuracy of the observable variables. However, the accuracy of estimating the latent variables has not yet been elucidated. For a quantitative evaluation of this, the present paper formulates distribution-based functions for the errors in the estimation of the latent variables. The asymptotic behavior is analyzed for both the maximum likelihood and the Bayes methods. version:4
arxiv-1402-4845 | Diffusion Least Mean Square: Simulations | http://arxiv.org/abs/1402.4845 | id:1402.4845 author:Jonathan Gelati, Sithan Kanna category:cs.LG cs.MA  published:2014-02-19 summary:In this technical report we analyse the performance of diffusion strategies applied to the Least-Mean-Square adaptive filter. We configure a network of cooperative agents running adaptive filters and discuss their behaviour when compared with a non-cooperative agent which represents the average of the network. The analysis provides conditions under which diversity in the filter parameters is beneficial in terms of convergence and stability. Simulations drive and support the analysis. version:1
arxiv-1312-6461 | Nonparametric Weight Initialization of Neural Networks via Integral Representation | http://arxiv.org/abs/1312.6461 | id:1312.6461 author:Sho Sonoda, Noboru Murata category:cs.LG cs.NE  published:2013-12-23 summary:A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases. version:3
arxiv-1402-4746 | Near-optimal-sample estimators for spherical Gaussian mixtures | http://arxiv.org/abs/1402.4746 | id:1402.4746 author:Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha Suresh category:cs.LG cs.DS cs.IT math.IT stat.ML  published:2014-02-19 summary:Statistical and machine-learning algorithms are frequently applied to high-dimensional data. In many of these applications data is scarce, and often much more costly than computation time. We provide the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. For mixtures of any $k$ $d$-dimensional spherical Gaussians, we derive an intuitive spectral-estimator that uses $\mathcal{O}_k\bigl(\frac{d\log^2d}{\epsilon^4}\bigr)$ samples and runs in time $\mathcal{O}_{k,\epsilon}(d^3\log^5 d)$, both significantly lower than previously known. The constant factor $\mathcal{O}_k$ is polynomial for sample complexity and is exponential for the time complexity, again much smaller than what was previously known. We also show that $\Omega_k\bigl(\frac{d}{\epsilon^2}\bigr)$ samples are needed for any algorithm. Hence the sample complexity is near-optimal in the number of dimensions. We also derive a simple estimator for one-dimensional mixtures that uses $\mathcal{O}\bigl(\frac{k \log \frac{k}{\epsilon} }{\epsilon^2} \bigr)$ samples and runs in time $\widetilde{\mathcal{O}}\left(\bigl(\frac{k}{\epsilon}\bigr)^{3k+1}\right)$. Our other technical contributions include a faster algorithm for choosing a density estimate from a set of distributions, that minimizes the $\ell_1$ distance to an unknown underlying distribution. version:1
arxiv-1312-1847 | Understanding Deep Architectures using a Recursive Convolutional Network | http://arxiv.org/abs/1312.1847 | id:1312.1847 author:David Eigen, Jason Rolfe, Rob Fergus, Yann LeCun category:cs.LG  published:2013-12-06 summary:A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead. version:2
arxiv-1312-6120 | Exact solutions to the nonlinear dynamics of learning in deep linear neural networks | http://arxiv.org/abs/1312.6120 | id:1312.6120 author:Andrew M. Saxe, James L. McClelland, Surya Ganguli category:cs.NE cond-mat.dis-nn cs.CV cs.LG q-bio.NC stat.ML  published:2013-12-20 summary:Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. version:3
arxiv-1402-4732 | Efficient Inference of Gaussian Process Modulated Renewal Processes with Application to Medical Event Data | http://arxiv.org/abs/1402.4732 | id:1402.4732 author:Thomas A. Lasko category:stat.ML cs.LG stat.AP  published:2014-02-19 summary:The episodic, irregular and asynchronous nature of medical data render them difficult substrates for standard machine learning algorithms. We would like to abstract away this difficulty for the class of time-stamped categorical variables (or events) by modeling them as a renewal process and inferring a probability density over continuous, longitudinal, nonparametric intensity functions modulating that process. Several methods exist for inferring such a density over intensity functions, but either their constraints and assumptions prevent their use with our potentially bursty event streams, or their time complexity renders their use intractable on our long-duration observations of high-resolution events, or both. In this paper we present a new and efficient method for inferring a distribution over intensity functions that uses direct numeric integration and smooth interpolation over Gaussian processes. We demonstrate that our direct method is up to twice as accurate and two orders of magnitude more efficient than the best existing method (thinning). Importantly, the direct method can infer intensity functions over the full range of bursty to memoryless to regular events, which thinning and many other methods cannot. Finally, we apply the method to clinical event data and demonstrate the face-validity of the abstraction, which is now amenable to standard learning algorithms. version:1
arxiv-1312-6199 | Intriguing properties of neural networks | http://arxiv.org/abs/1312.6199 | id:1312.6199 author:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus category:cs.CV cs.LG cs.NE  published:2013-12-21 summary:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input. version:4
arxiv-1312-5847 | Deep learning for neuroimaging: a validation study | http://arxiv.org/abs/1312.5847 | id:1312.5847 author:Sergey M. Plis, Devon R. Hjelm, Ruslan Salakhutdinov, Vince D. Calhoun category:cs.NE cs.LG stat.ML  published:2013-12-20 summary:Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data. version:3
arxiv-1402-4699 | A Powerful Genetic Algorithm for Traveling Salesman Problem | http://arxiv.org/abs/1402.4699 | id:1402.4699 author:Shujia Liu category:cs.NE cs.AI  published:2014-02-19 summary:This paper presents a powerful genetic algorithm(GA) to solve the traveling salesman problem (TSP). To construct a powerful GA, I use edge swapping(ES) with a local search procedure to determine good combinations of building blocks of parent solutions for generating even better offspring solutions. Experimental results on well studied TSP benchmarks demonstrate that the proposed GA is competitive in finding very high quality solutions on instances with up to 16,862 cities. version:1
arxiv-1402-4653 | Retrieval of Experiments by Efficient Estimation of Marginal Likelihood | http://arxiv.org/abs/1402.4653 | id:1402.4653 author:Sohan Seth, John Shawe-Taylor, Samuel Kaski category:stat.ML cs.IR cs.LG  published:2014-02-19 summary:We study the task of retrieving relevant experiments given a query experiment. By experiment, we mean a collection of measurements from a set of `covariates' and the associated `outcomes'. While similar experiments can be retrieved by comparing available `annotations', this approach ignores the valuable information available in the measurements themselves. To incorporate this information in the retrieval task, we suggest employing a retrieval metric that utilizes probabilistic models learned from the measurements. We argue that such a metric is a sensible measure of similarity between two experiments since it permits inclusion of experiment-specific prior knowledge. However, accurate models are often not analytical, and one must resort to storing posterior samples which demands considerable resources. Therefore, we study strategies to select informative posterior samples to reduce the computational load while maintaining the retrieval performance. We demonstrate the efficacy of our approach on simulated data with simple linear regression as the models, and real world datasets. version:1
arxiv-1402-4645 | A Survey on Semi-Supervised Learning Techniques | http://arxiv.org/abs/1402.4645 | id:1402.4645 author:V. Jothi Prakash, Dr. L. M. Nithya category:cs.LG  published:2014-02-19 summary:Semisupervised learning is a learning standard which deals with the study of how computers and natural systems such as human beings acquire knowledge in the presence of both labeled and unlabeled data. Semisupervised learning based methods are preferred when compared to the supervised and unsupervised learning because of the improved performance shown by the semisupervised approaches in the presence of large volumes of data. Labels are very hard to attain while unlabeled data are surplus, therefore semisupervised learning is a noble indication to shrink human labor and improve accuracy. There has been a large spectrum of ideas on semisupervised learning. In this paper we bring out some of the key approaches for semisupervised learning. version:1
arxiv-1402-4624 | Sparse Quantile Huber Regression for Efficient and Robust Estimation | http://arxiv.org/abs/1402.4624 | id:1402.4624 author:Aleksandr Y. Aravkin, Anju Kambadur, Aurelie C. Lozano, Ronny Luss category:stat.ML cs.DS math.OC stat.ME 62F35  65K10  published:2014-02-19 summary:We consider new formulations and methods for sparse quantile regression in the high-dimensional setting. Quantile regression plays an important role in many applications, including outlier-robust exploratory analysis in gene selection. In addition, the sparsity consideration in quantile regression enables the exploration of the entire conditional distribution of the response variable given the predictors and therefore yields a more comprehensive view of the important predictors. We propose a generalized OMP algorithm for variable selection, taking the misfit loss to be either the traditional quantile loss or a smooth version we call quantile Huber, and compare the resulting greedy approaches with convex sparsity-regularized formulations. We apply a recently proposed interior point methodology to efficiently solve all convex formulations as well as convex subproblems in the generalized OMP setting, pro- vide theoretical guarantees of consistent estimation, and demonstrate the performance of our approach using empirical studies of simulated and genomic datasets. version:1
arxiv-1312-0786 | Image Representation Learning Using Graph Regularized Auto-Encoders | http://arxiv.org/abs/1312.0786 | id:1312.0786 author:Yiyi Liao, Yue Wang, Yong Liu category:cs.LG K.3.2  published:2013-12-03 summary:We consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning. In those learning tasks, the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space. To overcome this problem, the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering. Inspired by the recent research works on deep neural network and representation learning, in this paper, we introduce the multiple-layer auto-encoder into image representation, we also apply the locally invariant ideal to our image representation with auto-encoders and propose a novel method, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact representation which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. Extensive experiments on image clustering show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-word cases. version:2
arxiv-1312-6116 | Improving Deep Neural Networks with Probabilistic Maxout Units | http://arxiv.org/abs/1312.6116 | id:1312.6116 author:Jost Tobias Springenberg, Martin Riedmiller category:stat.ML cs.LG cs.NE  published:2013-12-20 summary:We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN). version:2
arxiv-1402-4306 | Student-t Processes as Alternatives to Gaussian Processes | http://arxiv.org/abs/1402.4306 | id:1402.4306 author:Amar Shah, Andrew Gordon Wilson, Zoubin Ghahramani category:stat.ML cs.AI cs.LG stat.ME  published:2014-02-18 summary:We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes. version:2
arxiv-1402-6636 | Analysis of Multibeam SONAR Data using Dissimilarity Representations | http://arxiv.org/abs/1402.6636 | id:1402.6636 author:Iain Rice, Roger Benton, Les Hart, David Lowe category:cs.CE stat.ML  published:2014-02-19 summary:This paper considers the problem of low-dimensional visualisation of very high dimensional information sources for the purpose of situation awareness in the maritime environment. In response to the requirement for human decision support aids to reduce information overload (and specifically, data amenable to inter-point relative similarity measures) appropriate to the below-water maritime domain, we are investigating a preliminary prototype topographic visualisation model. The focus of the current paper is on the mathematical problem of exploiting a relative dissimilarity representation of signals in a visual informatics mapping model, driven by real-world sonar systems. An independent source model is used to analyse the sonar beams from which a simple probabilistic input model to represent uncertainty is mapped to a latent visualisation space where data uncertainty can be accommodated. The use of euclidean and non-euclidean measures are used and the motivation for future use of non-euclidean measures is made. Concepts are illustrated using a simulated 64 beam weak SNR dataset with realistic sonar targets. version:1
arxiv-1402-4542 | Unsupervised Ranking of Multi-Attribute Objects Based on Principal Curves | http://arxiv.org/abs/1402.4542 | id:1402.4542 author:Chun-Guo Li, Xing Mei, Bao-Gang Hu category:cs.LG cs.AI stat.ML  published:2014-02-19 summary:Unsupervised ranking faces one critical challenge in evaluation applications, that is, no ground truth is available. When PageRank and its variants show a good solution in related subjects, they are applicable only for ranking from link-structure data. In this work, we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks. To overcome the challenge, we propose five essential meta-rules for the design and assessment of unsupervised ranking approaches: scale and translation invariance, strict monotonicity, linear/nonlinear capacities, smoothness, and explicitness of parameter size. These meta-rules are regarded as high level knowledge for unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a ranking principal curve (RPC) model, which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute observations. Furthermore, the RPC is modeled to be a cubic B\'ezier curve with control points restricted in the interior of a hypercube, thereby complying with all the five meta-rules to infer a reasonable ranking list. With control points as the model parameters, one is able to understand the learned manifold and to interpret the ranking list semantically. Numerical experiments of the presented RPC model are conducted on two open datasets of different ranking applications. In comparison with the state-of-the-art approaches, the new model is able to show more reasonable ranking lists. version:1
arxiv-1402-4539 | A Statistical Approach to Set Classification by Feature Selection with Applications to Classification of Histopathology Images | http://arxiv.org/abs/1402.4539 | id:1402.4539 author:Sungkyu Jung, Xingye Qiao category:stat.ME stat.ML  published:2014-02-19 summary:Set classification problems arise when classification tasks are based on sets of observations as opposed to individual observations. In set classification, a classification rule is trained with $N$ sets of observations, where each set is labeled with class information, and the prediction of a class label is performed also with a set of observations. Data sets for set classification appear, for example, in diagnostics of disease based on multiple cell nucleus images from a single tissue. Relevant statistical models for set classification are introduced, which motivate a set classification framework based on context-free feature extraction. By understanding a set of observations as an empirical distribution, we employ a data-driven method to choose those features which contain information on location and major variation. In particular, the method of principal component analysis is used to extract the features of major variation. Multidimensional scaling is used to represent features as vector-valued points on which conventional classifiers can be applied. The proposed set classification approaches achieve better classification results than competing methods in a number of simulated data examples. The benefits of our method are demonstrated in an analysis of histopathology images of cell nuclei related to liver cancer. version:1
arxiv-1402-4507 | High Dimensional Semiparametric Scale-Invariant Principal Component Analysis | http://arxiv.org/abs/1402.4507 | id:1402.4507 author:Fang Han, Han Liu category:stat.ML  published:2014-02-18 summary:We propose a new high dimensional semiparametric principal component analysis (PCA) method, named Copula Component Analysis (COCA). The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. COCA improves upon PCA and sparse PCA in three aspects: (i) It is robust to modeling assumptions; (ii) It is robust to outliers and data contamination; (iii) It is scale-invariant and yields more interpretable results. We prove that the COCA estimators obtain fast estimation rates and are feature selection consistent when the dimension is nearly exponentially large relative to the sample size. Careful experiments confirm that COCA outperforms sparse PCA on both synthetic and real-world datasets. version:1
arxiv-1312-5853 | Multi-GPU Training of ConvNets | http://arxiv.org/abs/1312.5853 | id:1312.5853 author:Omry Yadan, Keith Adams, Yaniv Taigman, Marc'Aurelio Ranzato category:cs.LG cs.NE  published:2013-12-20 summary:In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs. version:4
arxiv-1402-4442 | Artificial Mutation inspired Hyper-heuristic for Runtime Usage of Multi-objective Algorithms | http://arxiv.org/abs/1402.4442 | id:1402.4442 author:Donia El Kateb, François Fouquet, Johann Bourcier, Yves Le Traon category:cs.SE cs.NE  published:2014-02-18 summary:In the last years, multi-objective evolutionary algorithms (MOEA) have been applied to different software engineering problems where many conflicting objectives have to be optimized simultaneously. In theory, evolutionary algorithms feature a nice property for runtime optimization as they can provide a solution in any execution time. In practice, based on a Darwinian inspired natural selection, these evolutionary algorithms produce many deadborn solutions whose computation results in a computational resources wastage: natural selection is naturally slow. In this paper, we reconsider this founding analogy to accelerate convergence of MOEA, by looking at modern biology studies: artificial selection has been used to achieve an anticipated specific purpose instead of only relying on crossover and natural selection (i.e., Muller et al [18] research on artificial mutation of fruits with X-Ray). Putting aside the analogy with natural selection , the present paper proposes an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial selective mutation to improve the convergence speed of MOEA. Sputnik leverages the past history of mutation efficiency to select the most relevant mutations to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives on-demand provisioning while considering conflicting performance and cost objectives. We have conducted experiments to highlight the significant performance improvement of Sputnik in terms of resolution time. version:1
arxiv-1312-5869 | Principled Non-Linear Feature Selection | http://arxiv.org/abs/1312.5869 | id:1312.5869 author:Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes category:cs.LG  published:2013-12-20 summary:Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics. version:2
arxiv-1402-1971 | Direct Processing of Run Length Compressed Document Image for Segmentation and Characterization of a Specified Block | http://arxiv.org/abs/1402.1971 | id:1402.1971 author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV  published:2014-02-09 summary:Extracting a block of interest referred to as segmenting a specified block in an image and studying its characteristics is of general research interest, and could be a challenging if such a segmentation task has to be carried out directly in a compressed image. This is the objective of the present research work. The proposal is to evolve a method which would segment and extract a specified block, and carry out its characterization without decompressing a compressed image, for two major reasons that most of the image archives contain images in compressed format and decompressing an image indents additional computing time and space. Specifically in this research work, the proposal is to work on run-length compressed document images. version:2
arxiv-1402-4388 | Automatic Detection of Font Size Straight from Run Length Compressed Text Documents | http://arxiv.org/abs/1402.4388 | id:1402.4388 author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV  published:2014-02-18 summary:Automatic detection of font size finds many applications in the area of intelligent OCRing and document image analysis, which has been traditionally practiced over uncompressed documents, although in real life the documents exist in compressed form for efficient storage and transmission. It would be novel and intelligent if the task of font size detection could be carried out directly from the compressed data of these documents without decompressing, which would result in saving of considerable amount of processing time and space. Therefore, in this paper we present a novel idea of learning and detecting font size directly from run-length compressed text documents at line level using simple line height features, which paves the way for intelligent OCRing and document analysis directly from compressed documents. In the proposed model, the given mixed-case text documents of different font size are segmented into compressed text lines and the features extracted such as line height and ascender height are used to capture the pattern of font size in the form of a regression line, using which the automatic detection of font size is done during the recognition stage. The method is experimented with a dataset of 50 compressed documents consisting of 780 text lines of single font size and 375 text lines of mixed font size resulting in an overall accuracy of 99.67%. version:1
arxiv-1402-4381 | Fast X-ray CT image reconstruction using the linearized augmented Lagrangian method with ordered subsets | http://arxiv.org/abs/1402.4381 | id:1402.4381 author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML  published:2014-02-18 summary:The augmented Lagrangian (AL) method that solves convex optimization problems with linear constraints has drawn more attention recently in imaging applications due to its decomposable structure for composite cost functions and empirical fast convergence rate under weak conditions. However, for problems such as X-ray computed tomography (CT) image reconstruction and large-scale sparse regression with "big data", where there is no efficient way to solve the inner least-squares problem, the AL method can be slow due to the inevitable iterative inner updates. In this paper, we focus on solving regularized (weighted) least-squares problems using a linearized variant of the AL method that replaces the quadratic AL penalty term in the scaled augmented Lagrangian with its separable quadratic surrogate (SQS) function, thus leading to a much simpler ordered-subsets (OS) accelerable splitting-based algorithm, OS-LALM, for X-ray CT image reconstruction. To further accelerate the proposed algorithm, we use a second-order recursive system analysis to design a deterministic downward continuation approach that avoids tedious parameter tuning and provides fast convergence. Experimental results show that the proposed algorithm significantly accelerates the "convergence" of X-ray CT image reconstruction with negligible overhead and greatly reduces the OS artifacts in the reconstructed image when using many subsets for OS acceleration. version:1
arxiv-1402-4380 | A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification | http://arxiv.org/abs/1402.4380 | id:1402.4380 author:Samuel Danso, Eric Atwell, Owen Johnson category:cs.CL  published:2014-02-18 summary:A Verbal Autopsy is the record of an interview about the circumstances of an uncertified death. In developing countries, if a death occurs away from health facilities, a field-worker interviews a relative of the deceased about the circumstances of the death; this Verbal Autopsy can be reviewed off-site. We report on a comparative study of the processes involved in Text Classification applied to classifying Cause of Death: feature value representation; machine learning classification algorithms; and feature reduction strategies in order to identify the suitable approaches applicable to the classification of Verbal Autopsy text. We demonstrate that normalised term frequency and the standard TFiDF achieve comparable performance across a number of classifiers. The results also show Support Vector Machine is superior to other classification algorithms employed in this research. Finally, we demonstrate the effectiveness of employing a "locally-semi-supervised" feature reduction strategy in order to increase performance accuracy. version:1
arxiv-1312-5985 | Learning Type-Driven Tensor-Based Meaning Representations | http://arxiv.org/abs/1312.5985 | id:1312.5985 author:Tamara Polajnar, Luana Fagarasan, Stephen Clark category:cs.CL cs.LG H.3.1  published:2013-12-20 summary:This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider. version:2
arxiv-1402-4371 | A convergence proof of the split Bregman method for regularized least-squares problems | http://arxiv.org/abs/1402.4371 | id:1402.4371 author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML  published:2014-02-18 summary:The split Bregman (SB) method [T. Goldstein and S. Osher, SIAM J. Imaging Sci., 2 (2009), pp. 323-43] is a fast splitting-based algorithm that solves image reconstruction problems with general l1, e.g., total-variation (TV) and compressed sensing (CS), regularizations by introducing a single variable split to decouple the data-fitting term and the regularization term, yielding simple subproblems that are separable (or partially separable) and easy to minimize. Several convergence proofs have been proposed, and these proofs either impose a "full column rank" assumption to the split or assume exact updates in all subproblems. However, these assumptions are impractical in many applications such as the X-ray computed tomography (CT) image reconstructions, where the inner least-squares problem usually cannot be solved efficiently due to the highly shift-variant Hessian. In this paper, we show that when the data-fitting term is quadratic, the SB method is a convergent alternating direction method of multipliers (ADMM), and a straightforward convergence proof with inexact updates is given using [J. Eckstein and D. P. Bertsekas, Mathematical Programming, 55 (1992), pp. 293-318, Theorem 8]. Furthermore, since the SB method is just a special case of an ADMM algorithm, it seems likely that the ADMM algorithm will be faster than the SB method if the augmented Largangian (AL) penalty parameters are selected appropriately. To have a concrete example, we conduct a convergence rate analysis of the ADMM algorithm using two splits for image restoration problems with quadratic data-fitting term and regularization term. According to our analysis, we can show that the two-split ADMM algorithm can be faster than the SB method if the AL penalty parameter of the SB method is suboptimal. Numerical experiments were conducted to verify our analysis. version:1
arxiv-1402-4354 | Hybrid SRL with Optimization Modulo Theories | http://arxiv.org/abs/1402.4354 | id:1402.4354 author:Stefano Teso, Roberto Sebastiani, Andrea Passerini category:cs.LG stat.ML  published:2014-02-18 summary:Generally speaking, the goal of constructive learning could be seen as, given an example set of structured objects, to generate novel objects with similar properties. From a statistical-relational learning (SRL) viewpoint, the task can be interpreted as a constraint satisfaction problem, i.e. the generated objects must obey a set of soft constraints, whose weights are estimated from the data. Traditional SRL approaches rely on (finite) First-Order Logic (FOL) as a description language, and on MAX-SAT solvers to perform inference. Alas, FOL is unsuited for con- structive problems where the objects contain a mixture of Boolean and numerical variables. It is in fact difficult to implement, e.g. linear arithmetic constraints within the language of FOL. In this paper we propose a novel class of hybrid SRL methods that rely on Satisfiability Modulo Theories, an alternative class of for- mal languages that allow to describe, and reason over, mixed Boolean-numerical objects and constraints. The resulting methods, which we call Learning Mod- ulo Theories, are formulated within the structured output SVM framework, and employ a weighted SMT solver as an optimization oracle to perform efficient in- ference and discriminative max margin weight learning. We also present a few examples of constructive learning applications enabled by our method. version:1
arxiv-1312-5559 | Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds | http://arxiv.org/abs/1312.5559 | id:1312.5559 author:Irina Sergienya, Hinrich Schütze category:cs.CL I.2.6; I.2.7  published:2013-12-19 summary:There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task. version:3
arxiv-1402-4322 | On the properties of $α$-unchaining single linkage hierarchical clustering | http://arxiv.org/abs/1402.4322 | id:1402.4322 author:A. Martínez-Pérez category:cs.LG 62H30  68T10  published:2014-02-18 summary:In the election of a hierarchical clustering method, theoretic properties may give some insight to determine which method is the most suitable to treat a clustering problem. Herein, we study some basic properties of two hierarchical clustering methods: $\alpha$-unchaining single linkage or $SL(\alpha)$ and a modified version of this one, $SL^*(\alpha)$. We compare the results with the properties satisfied by the classical linkage-based hierarchical clustering methods. version:1
arxiv-1312-6168 | Factorial Hidden Markov Models for Learning Representations of Natural Language | http://arxiv.org/abs/1312.6168 | id:1312.6168 author:Anjan Nepal, Alexander Yates category:cs.LG cs.CL  published:2013-12-20 summary:Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods. version:3
arxiv-1402-4293 | The Random Forest Kernel and other kernels for big data from random partitions | http://arxiv.org/abs/1402.4293 | id:1402.4293 author:Alex Davies, Zoubin Ghahramani category:stat.ML cs.LG  published:2014-02-18 summary:We present Random Partition Kernels, a new class of kernels derived by demonstrating a natural connection between random partitions of objects and kernels between those objects. We show how the construction can be used to create kernels from methods that would not normally be viewed as random partitions, such as Random Forest. To demonstrate the potential of this method, we propose two new kernels, the Random Forest Kernel and the Fast Cluster Kernel, and show that these kernels consistently outperform standard kernels on problems involving real-world datasets. Finally, we show how the form of these kernels lend themselves to a natural approximation that is appropriate for certain big data problems, allowing $O(N)$ inference in methods such as Gaussian Processes, Support Vector Machines and Kernel PCA. version:1
arxiv-1402-4283 | Discretization of Temporal Data: A Survey | http://arxiv.org/abs/1402.4283 | id:1402.4283 author:P. Chaudhari, D. P. Rana, R. G. Mehta, N. J. Mistry, M. M. Raghuwanshi category:cs.DB cs.LG  published:2014-02-18 summary:In real world, the huge amount of temporal data is to be processed in many application areas such as scientific, financial, network monitoring, sensor data analysis. Data mining techniques are primarily oriented to handle discrete features. In the case of temporal data the time plays an important role on the characteristics of data. To consider this effect, the data discretization techniques have to consider the time while processing to resolve the issue by finding the intervals of data which are more concise and precise with respect to time. Here, this research is reviewing different data discretization techniques used in temporal data applications according to the inclusion or exclusion of: class label, temporal order of the data and handling of stream data to open the research direction for temporal data discretization to improve the performance of data mining technique. version:1
arxiv-1312-4695 | Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity priors | http://arxiv.org/abs/1312.4695 | id:1312.4695 author:Wiktor Mlynarski category:cs.LG cs.SD q-bio.NC  published:2013-12-17 summary:Complex-valued sparse coding is a data representation which employs a dictionary of two-dimensional subspaces, while imposing a sparse, factorial prior on complex amplitudes. When trained on a dataset of natural image patches, it learns phase invariant features which closely resemble receptive fields of complex cells in the visual cortex. Features trained on natural sounds however, rarely reveal phase invariance and capture other aspects of the data. This observation is a starting point of the present work. As its first contribution, it provides an analysis of natural sound statistics by means of learning sparse, complex representations of short speech intervals. Secondly, it proposes priors over the basis function set, which bias them towards phase-invariant solutions. In this way, a dictionary of complex basis functions can be learned from the data statistics, while preserving the phase invariance property. Finally, representations trained on speech sounds with and without priors are compared. Prior-based basis functions reveal performance comparable to unconstrained sparse coding, while explicitely representing phase as a temporal shift. Such representations can find applications in many perceptual and machine learning tasks. version:3
arxiv-1312-5921 | Group-sparse Embeddings in Collective Matrix Factorization | http://arxiv.org/abs/1312.5921 | id:1312.5921 author:Arto Klami, Guillaume Bouchard, Abhishek Tripathi category:stat.ML cs.LG  published:2013-12-20 summary:CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning. version:2
arxiv-1402-4259 | Extracting Networks of Characters and Places from Written Works with CHAPLIN | http://arxiv.org/abs/1402.4259 | id:1402.4259 author:Roberto Marazzato, Amelia Carolina Sparavigna category:cs.CY cs.CL  published:2014-02-18 summary:We are proposing a tool able to gather information on social networks from narrative texts. Its name is CHAPLIN, CHAracters and PLaces Interaction Network, implemented in VB.NET. Characters and places of the narrative works are extracted in a list of raw words. Aided by the interface, the user selects names out of them. After this choice, the tool allows the user to enter some parameters, and, according to them, creates a network where the nodes are the characters and places, and the edges their interactions. Edges are labelled by performances. The output is a GV file, written in the DOT graph scripting language, which is rendered by means of the free open source software Graphviz. version:1
arxiv-1311-2891 | The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures | http://arxiv.org/abs/1311.2891 | id:1311.2891 author:Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James Voss category:cs.LG cs.DS stat.ML  published:2013-11-12 summary:In this paper we show that very large mixtures of Gaussians are efficiently learnable in high dimension. More precisely, we prove that a mixture with known identical covariance matrices whose number of components is a polynomial of any fixed degree in the dimension n is polynomially learnable as long as a certain non-degeneracy condition on the means is satisfied. It turns out that this condition is generic in the sense of smoothed complexity, as soon as the dimensionality of the space is high enough. Moreover, we prove that no such condition can possibly exist in low dimension and the problem of learning the parameters is generically hard. In contrast, much of the existing work on Gaussian Mixtures relies on low-dimensional projections and thus hits an artificial barrier. Our main result on mixture recovery relies on a new "Poissonization"-based technique, which transforms a mixture of Gaussians to a linear map of a product distribution. The problem of learning this map can be efficiently solved using some recent results on tensor decompositions and Independent Component Analysis (ICA), thus giving an algorithm for recovering the mixture. In addition, we combine our low-dimensional hardness results for Gaussian mixtures with Poissonization to show how to embed difficult instances of low-dimensional Gaussian mixtures into the ICA setting, thus establishing exponential information-theoretic lower bounds for underdetermined ICA in low dimension. To the best of our knowledge, this is the first such result in the literature. In addition to contributing to the problem of Gaussian mixture learning, we believe that this work is among the first steps toward better understanding the rare phenomenon of the "blessing of dimensionality" in the computational aspects of statistical inference. version:3
arxiv-1312-6204 | One-Shot Adaptation of Supervised Deep Convolutional Models | http://arxiv.org/abs/1312.6204 | id:1312.6204 author:Judy Hoffman, Eric Tzeng, Jeff Donahue, Yangqing Jia, Kate Saenko, Trevor Darrell category:cs.CV cs.LG cs.NE  published:2013-12-21 summary:Dataset bias remains a significant barrier towards solving real world computer vision tasks. Though deep convolutional networks have proven to be a competitive approach for image classification, a question remains: have these models have solved the dataset bias problem? In general, training or fine-tuning a state-of-the-art deep model on a new domain requires a significant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as susceptible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (one example per category) or no labeled domain specific data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost. version:2
arxiv-1402-4678 | When Learners Surpass their Sources: Mathematical Modeling of Learning from an Inconsistent Source | http://arxiv.org/abs/1402.4678 | id:1402.4678 author:Yelena Mandelshtam, Natalia Komarova category:cs.CL  published:2014-02-18 summary:We present a new algorithm to model and investigate the learning process of a learner mastering a set of grammatical rules from an inconsistent source. The compelling interest of human language acquisition is that the learning succeeds in virtually every case, despite the fact that the input data are formally inadequate to explain the success of learning. Our model explains how a learner can successfully learn from or even surpass its imperfect source without possessing any additional biases or constraints about the types of patterns that exist in the language. We use the data collected by Singleton and Newport (2004) on the performance of a 7-year boy Simon, who mastered the American Sign Language (ASL) by learning it from his parents, both of whom were imperfect speakers of ASL. We show that the algorithm possesses a frequency-boosting property, whereby the frequency of the most common form of the source is increased by the learner. We also explain several key features of Simon's ASL. version:1
arxiv-1309-1369 | Semistochastic Quadratic Bound Methods | http://arxiv.org/abs/1309.1369 | id:1309.1369 author:Aleksandr Y. Aravkin, Anna Choromanska, Tony Jebara, Dimitri Kanevsky category:stat.ML cs.LG math.NA stat.CO 90C55  90C15  62H30  published:2013-09-05 summary:Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets. version:4
arxiv-1312-5398 | Continuous Learning: Engineering Super Features With Feature Algebras | http://arxiv.org/abs/1312.5398 | id:1312.5398 author:Michael Tetelman category:cs.LG stat.ML  published:2013-12-19 summary:In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on the original input space. After a finite number of iterations N, the non-linear features become 2^N -degree polynomials on the original space. We show that in a limit of an infinite number of iterations derived non-linear features must form an associative algebra: a product of two features is equal to a linear combination of features from the same feature space for any given input point. Because each iteration consists of solving a series of convex problems that contain all previous solutions, the likelihood of the models in the sequence is increasing with each iteration while the dimension of the model parameter space is set to a limited controlled value. version:2
arxiv-1402-4084 | Selective Sampling with Drift | http://arxiv.org/abs/1402.4084 | id:1402.4084 author:Edward Moroshko, Koby Crammer category:cs.LG  published:2014-02-17 summary:Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting. version:1
arxiv-1402-4067 | Statistical Noise Analysis in SENSE Parallel MRI | http://arxiv.org/abs/1402.4067 | id:1402.4067 author:Santiago Aja-Fernandez, Gonzalo Vegas-Sanchez-Ferrero, Antonio Trsitan-Vega category:cs.CV  published:2014-02-17 summary:A complete first and second order statistical characterization of noise in SENSE reconstructed data is proposed. SENSE acquisitions have usually been modeled as Rician distributed, since the data reconstruction takes place into the spatial domain, where Gaussian noise is assumed. However, this model just holds for the first order statistics and obviates other effects induced by coils correlations and the reconstruction interpolation. Those effects are properly taken into account in this study, in order to fully justify a final SENSE noise model. As a result, some interesting features of the reconstructed image arise: (1) There is a strong correlation between adjacent lines. (2) The resulting distribution is non-stationary and therefore the variance of noise will vary from point to point across the image. Closed equations for the calculation of the variance of noise and the correlation coefficient between lines are proposed. The proposed model is totally compatible with g-factor formulations. version:1
arxiv-1402-4053 | The Algebraic Approach to Phase Retrieval and Explicit Inversion at the Identifiability Threshold | http://arxiv.org/abs/1402.4053 | id:1402.4053 author:Franz J Király, Martin Ehler category:math.FA cs.CV cs.IT math.AG math.IT stat.ML  published:2014-02-17 summary:We study phase retrieval from magnitude measurements of an unknown signal as an algebraic estimation problem. Indeed, phase retrieval from rank-one and more general linear measurements can be treated in an algebraic way. It is verified that a certain number of generic rank-one or generic linear measurements are sufficient to enable signal reconstruction for generic signals, and slightly more generic measurements yield reconstructability for all signals. Our results solve a few open problems stated in the recent literature. Furthermore, we show how the algebraic estimation problem can be solved by a closed-form algebraic estimation technique, termed ideal regression, providing non-asymptotic success guarantees. version:1
arxiv-1312-6108 | Modeling correlations in spontaneous activity of visual cortex with centered Gaussian-binary deep Boltzmann machines | http://arxiv.org/abs/1312.6108 | id:1312.6108 author:Nan Wang, Dirk Jancke, Laurenz Wiskott category:cs.NE cs.LG q-bio.NC  published:2013-12-20 summary:Spontaneous cortical activity -- the ongoing cortical activities in absence of intentional sensory input -- is considered to play a vital role in many aspects of both normal brain functions and mental dysfunctions. We present a centered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling the activity in early cortical visual areas and relate the random sampling in GDBMs to the spontaneous cortical activity. After training the proposed model on natural image patches, we show that the samples collected from the model's probability distribution encompass similar activity patterns as found in the spontaneous activity. Specifically, filters having the same orientation preference tend to be active together during random sampling. Our work demonstrates the centered GDBM is a meaningful model approach for basic receptive field properties and the emergence of spontaneous activity patterns in early cortical visual areas. Besides, we show empirically that centered GDBMs do not suffer from the difficulties during training as GDBMs do and can be properly trained without the layer-wise pretraining. version:3
arxiv-1301-3584 | Revisiting Natural Gradient for Deep Networks | http://arxiv.org/abs/1301.3584 | id:1301.3584 author:Razvan Pascanu, Yoshua Bengio category:cs.LG cs.NA  published:2013-01-16 summary:We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it. version:7
arxiv-1402-4036 | Is Spiking Logic the Route to Memristor-Based Computers? | http://arxiv.org/abs/1402.4036 | id:1402.4036 author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.ET cond-mat.mtrl-sci cs.AR cs.NE 94C-06 C.1.3; B.3.1  published:2014-02-17 summary:Memristors have been suggested as a novel route to neuromorphic computing based on the similarity between neurons (synapses and ion pumps) and memristors. The D.C. action of the memristor is a current spike, which we think will be fruitful for building memristor computers. In this paper, we introduce 4 different logical assignations to implement sequential logic in the memristor and introduce the physical rules, summation, `bounce-back', directionality and `diminishing returns', elucidated from our investigations. We then demonstrate how memristor sequential logic works by instantiating a NOT gate, an AND gate and a Full Adder with a single memristor. The Full Adder makes use of the memristor's memory to add three binary values together and outputs the value, the carry digit and even the order they were input in. version:1
arxiv-1402-4029 | Connecting Spiking Neurons to a Spiking Memristor Network Changes the Memristor Dynamics | http://arxiv.org/abs/1402.4029 | id:1402.4029 author:Deborah Gater, Attya Iqbal, Jeffrey Davey, Ella Gale category:cs.ET cs.NE physics.bio-ph 92C-06  94C-06  published:2014-02-17 summary:Memristors have been suggested as neuromorphic computing elements. Spike-time dependent plasticity and the Hodgkin-Huxley model of the neuron have both been modelled effectively by memristor theory. The d.c. response of the memristor is a current spike. Based on these three facts we suggest that memristors are well-placed to interface directly with neurons. In this paper we show that connecting a spiking memristor network to spiking neuronal cells causes a change in the memristor network dynamics by: removing the memristor spikes, which we show is due to the effects of connection to aqueous medium; causing a change in current decay rate consistent with a change in memristor state; presenting more-linear $I-t$ dynamics; and increasing the memristor spiking rate, as a consequence of interaction with the spiking neurons. This demonstrates that neurons are capable of communicating directly with memristors, without the need for computer translation. version:1
arxiv-1402-4007 | Does the D.C. Response of Memristors Allow Robotic Short-Term Memory and a Possible Route to Artificial Time Perception? | http://arxiv.org/abs/1402.4007 | id:1402.4007 author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.RO cs.ET cs.NE 94Cxx  published:2014-02-17 summary:Time perception is essential for task switching, and in the mammalian brain appears alongside other processes. Memristors are electronic components used as synapses and as models for neurons. The d.c. response of memristors can be considered as a type of short-term memory. Interactions of the memristor d.c. response within networks of memristors leads to the emergence of oscillatory dynamics and intermittent spike trains, which are similar to neural dynamics. Based on this data, the structure of a memristor network control for a robot as it undergoes task switching is discussed and it is suggested that these emergent network dynamics could improve the performance of role switching and learning in an artificial intelligence and perhaps create artificial time perception. version:1
arxiv-1402-3973 | Dimensionality reduction with subgaussian matrices: a unified theory | http://arxiv.org/abs/1402.3973 | id:1402.3973 author:Sjoerd Dirksen category:cs.IT cs.DS math.IT stat.ML  published:2014-02-17 summary:We present a theory for Euclidean dimensionality reduction with subgaussian matrices which unifies several restricted isometry property and Johnson-Lindenstrauss type results obtained earlier for specific data sets. In particular, we recover and, in several cases, improve results for sets of sparse and structured sparse vectors, low-rank matrices and tensors, and smooth manifolds. In addition, we establish a new Johnson-Lindenstrauss embedding for data sets taking the form of an infinite union of subspaces of a Hilbert space. version:1
arxiv-1404-0566 | Weyl group orbit functions in image processing | http://arxiv.org/abs/1404.0566 | id:1404.0566 author:Goce Chadzitaskos, Lenka Háková, Ondřej Kajínek category:cs.CV  published:2014-02-17 summary:We deal with the Fourier-like analysis of functions on discrete grids in two-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. For these cases we present the convolution theorem. We provide an example of application of image processing using the $C-$ functions and the convolutions for spatial filtering of the treated image. version:1
arxiv-1402-3926 | Sparse Coding Approach for Multi-Frame Image Super Resolution | http://arxiv.org/abs/1402.3926 | id:1402.3926 author:Toshiyuki Kato, Hideitsu Hino, Noboru Murata category:cs.CV  published:2014-02-17 summary:An image super-resolution method from multiple observation of low-resolution images is proposed. The method is based on sub-pixel accuracy block matching for estimating relative displacements of observed images, and sparse signal representation for estimating the corresponding high-resolution image. Relative displacements of small patches of observed low-resolution images are accurately estimated by a computationally efficient block matching method. Since the estimated displacements are also regarded as a warping component of image degradation process, the matching results are directly utilized to generate low-resolution dictionary for sparse image representation. The matching scores of the block matching are used to select a subset of low-resolution patches for reconstructing a high-resolution patch, that is, an adaptive selection of informative low-resolution images is realized. When there is only one low-resolution image, the proposed method works as a single-frame super-resolution method. The proposed method is shown to perform comparable or superior to conventional single- and multi-frame super-resolution methods through experiments using various real-world datasets. version:1
arxiv-1402-3891 | Performance Evaluation of Machine Learning Classifiers in Sentiment Mining | http://arxiv.org/abs/1402.3891 | id:1402.3891 author:Vinodhini G Chandrasekaran RM category:cs.LG cs.CL cs.IR  published:2014-02-17 summary:In recent years, the use of machine learning classifiers is of great value in solving a variety of problems in text classification. Sentiment mining is a kind of text classification in which, messages are classified according to sentiment orientation such as positive or negative. This paper extends the idea of evaluating the performance of various classifiers to show their effectiveness in sentiment mining of online product reviews. The product reviews are collected from Amazon reviews. To evaluate the performance of classifiers various evaluation methods like random sampling, linear sampling and bootstrap sampling are used. Our results shows that support vector machine with bootstrap sampling method outperforms others classifiers and sampling methods in terms of misclassification rate. version:1
arxiv-1312-7335 | Correlation-based construction of neighborhood and edge features | http://arxiv.org/abs/1312.7335 | id:1312.7335 author:Balázs Kégl category:cs.CV cs.LG stat.ML  published:2013-12-20 summary:Motivated by an abstract notion of low-level edge detector filters, we propose a simple method of unsupervised feature construction based on pairwise statistics of features. In the first step, we construct neighborhoods of features by regrouping features that correlate. Then we use these subsets as filters to produce new neighborhood features. Next, we connect neighborhood features that correlate, and construct edge features by subtracting the correlated neighborhood features of each other. To validate the usefulness of the constructed features, we ran AdaBoost.MH on four multi-class classification problems. Our most significant result is a test error of 0.94% on MNIST with an algorithm which is essentially free of any image-specific priors. On CIFAR-10 our method is suboptimal compared to today's best deep learning techniques, nevertheless, we show that the proposed method outperforms not only boosting on the raw pixels, but also boosting on Haar filters. version:2
arxiv-1402-3849 | Scalable Kernel Clustering: Approximate Kernel k-means | http://arxiv.org/abs/1402.3849 | id:1402.3849 author:Radha Chitta, Rong Jin, Timothy C. Havens, Anil K. Jain category:cs.CV cs.DS cs.LG  published:2014-02-16 summary:Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k-means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that its running time and memory requirements are significantly lower than those of kernel k-means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm. version:1
arxiv-1312-5479 | Sparse similarity-preserving hashing | http://arxiv.org/abs/1312.5479 | id:1312.5479 author:Jonathan Masci, Alex M. Bronstein, Michael M. Bronstein, Pablo Sprechmann, Guillermo Sapiro category:cs.CV cs.DS  published:2013-12-19 summary:In recent years, a lot of attention has been devoted to efficient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-off between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very difficult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting computational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high-dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experimental evaluation involving visual and multi-modal data shows the benefits of the proposed method. version:3
arxiv-1312-5242 | Unsupervised feature learning by augmenting single images | http://arxiv.org/abs/1312.5242 | id:1312.5242 author:Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox category:cs.CV cs.LG cs.NE  published:2013-12-18 summary:When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial 'seed' patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101). version:3
arxiv-1312-6095 | Multi-View Priors for Learning Detectors from Sparse Viewpoint Data | http://arxiv.org/abs/1312.6095 | id:1312.6095 author:Bojan Pepik, Michael Stark, Peter Gehler, Bernt Schiele category:cs.CV  published:2013-12-20 summary:While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes. version:2
arxiv-1401-4660 | On the Resilience of an Ant-based System in Fuzzy Environments. An Empirical Study | http://arxiv.org/abs/1401.4660 | id:1401.4660 author:Gloria Cerasela Crisan, Camelia-M. Pintea, Petrica C. Pop category:cs.NE  published:2014-01-19 summary:The current work describes an empirical study conducted in order to investigate the behavior of an optimization method in a fuzzy environment. MAX-MIN Ant System, an efficient implementation of a heuristic method is used for solving an optimization problem derived from the Traveling Salesman Problem (TSP). Several publicly-available symmetric TSP instances and their fuzzy variants are tested in order to extract some general features. The entry data was adapted by introducing a two-dimensional systematic degree of fuzziness, proportional with the number of nodes, the dimension of the instance and also with the distances between nodes, the scale of the instance. The results show that our proposed method can handle the data uncertainty, showing good resilience and adaptability. version:2
arxiv-1402-3722 | word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method | http://arxiv.org/abs/1402.3722 | id:1402.3722 author:Yoav Goldberg, Omer Levy category:cs.CL cs.LG stat.ML  published:2014-02-15 summary:The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean. version:1
arxiv-1312-6192 | Can recursive neural tensor networks learn logical reasoning? | http://arxiv.org/abs/1312.6192 | id:1312.6192 author:Samuel R. Bowman category:cs.CL cs.LG  published:2013-12-21 summary:Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of "some animal walks" from "some dog walks" or "some cat walks," given that dogs and cats are animals. This model learns representations that generalize well to new types of reasoning pattern in all but a few cases, a result which is promising for the ability of learned representation models to capture logical reasoning. version:4
arxiv-1312-4190 | One-Shot-Learning Gesture Recognition using HOG-HOF Features | http://arxiv.org/abs/1312.4190 | id:1312.4190 author:Jakub Konečný, Michal Hagara category:cs.CV  published:2013-12-15 summary:The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the \textit{ChaLearn Gesture Dataset}. We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos --- to remove all the unimportant frames from videos. We present two methods that use combination of HOG-HOF descriptors together with variants of Dynamic Time Warping technique. Both methods outperform other published methods and help narrow down the gap between human performance and algorithms on this task. The code has been made publicly available in the MLOSS repository. version:2
arxiv-1210-0252 | A Linguistic Model for Terminology Extraction based Conditional Random Fields | http://arxiv.org/abs/1210.0252 | id:1210.0252 author:Fethi Fkih, Mohamed Nazih Omri, Imen Toumia category:cs.CL cs.AI I.2.6; I.2.7  published:2012-09-30 summary:In this paper, we show the possibility of using a linear Conditional Random Fields (CRF) for terminology extraction from a specialized text corpus. version:2
arxiv-1402-3657 | A Narrative Vehicle Protection Representation for Vehicle Speed Regulator Under Driver Exhaustion -- A Study | http://arxiv.org/abs/1402.3657 | id:1402.3657 author:V. Karthikeyan, B. Praveen Kumar, S. Suresh Babu, R. Purusothaman, shijin Thomas category:cs.CV cs.HC  published:2014-02-15 summary:Driver fatigue is one of the important factors that cause traffic accidents, and the ever-increasing number due to diminished drivers vigilance level has become a problem of serious concern to society. Drivers with a diminished vigilance level suffer from a marked decline in their abilities of perception, recognition, and vehicle control, and therefore pose serious danger to their own life and the lives of other people. Exhaustion resulting from sleep deprivation or sleep disorders is an important factor in the creasing number of accidents. In this projected work, we discuss the various methods of the existing and the proposed method based on a real time online safety prototype that controls the vehicle speed under driver fatigue. The purpose of such a model is to advance a system to detect fatigue symptoms in drivers and control the speed of vehicle to avoid accidents. This system was tested adequately with subjects of different technology of various researchers finally the validity of the proposed model for vehicle speed controller based on driver fatigue detection is shown. version:1
arxiv-1402-3648 | Auto Spell Suggestion for High Quality Speech Synthesis in Hindi | http://arxiv.org/abs/1402.3648 | id:1402.3648 author:Shikha Kabra, Ritika Agarwal category:cs.CL cs.SD  published:2014-02-15 summary:The goal of Text-to-Speech (TTS) synthesis in a particular language is to convert arbitrary input text to intelligible and natural sounding speech. However, for a particular language like Hindi, which is a highly confusing language (due to very close spellings), it is not an easy task to identify errors/mistakes in input text and an incorrect text degrade the quality of output speech hence this paper is a contribution to the development of high quality speech synthesis with the involvement of Spellchecker which generates spell suggestions for misspelled words automatically. Involvement of spellchecker would increase the efficiency of speech synthesis by providing spell suggestions for incorrect input text. Furthermore, we have provided the comparative study for evaluating the resultant effect on to phonetic text by adding spellchecker on to input text. version:1
arxiv-1405-6177 | Automated Fabric Defect Inspection: A Survey of Classifiers | http://arxiv.org/abs/1405.6177 | id:1405.6177 author:Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman, Farruk Ahmed category:cs.CV cs.LG  published:2014-02-14 summary:Quality control at each stage of production in textile industry has become a key factor to retaining the existence in the highly competitive global market. Problems of manual fabric defect inspection are lack of accuracy and high time consumption, where early and accurate fabric defect detection is a significant phase of quality control. Computer vision based, i.e. automated fabric defect inspection systems are thought by many researchers of different countries to be very useful to resolve these problems. There are two major challenges to be resolved to attain a successful automated fabric defect inspection system. They are defect detection and defect classification. In this work, we discuss different techniques used for automated fabric defect classification, then show a survey of classifiers used in automated fabric defect inspection systems, and finally, compare these classifiers by using performance metrics. This work is expected to be very useful for the researchers in the area of automated fabric defect inspection to understand and evaluate the many potential options in this field. version:1
arxiv-1402-3557 | Improving Streaming Video Segmentation with Early and Mid-Level Visual Processing | http://arxiv.org/abs/1402.3557 | id:1402.3557 author:Subarna Tripathi, Youngbae Hwang, Serge Belongie, Truong Nguyen category:cs.CV  published:2014-02-14 summary:Despite recent advances in video segmentation, many opportunities remain to improve it using a variety of low and mid-level visual cues. We propose improvements to the leading streaming graph-based hierarchical video segmentation (streamGBH) method based on early and mid level visual processing. The extensive experimental analysis of our approach validates the improvement of hierarchical supervoxel representation by incorporating motion and color with effective filtering. We also pose and illuminate some open questions towards intermediate level video analysis as further extension to streamGBH. We exploit the supervoxels as an initialization towards estimation of dominant affine motion regions, followed by merging of such motion regions in order to hierarchically segment a video in a novel motion-segmentation framework which aims at subsequent applications such as foreground recognition. version:1
arxiv-1307-6134 | Modeling Human Decision-making in Generalized Gaussian Multi-armed Bandits | http://arxiv.org/abs/1307.6134 | id:1307.6134 author:Paul Reverdy, Vaibhav Srivastava, Naomi E. Leonard category:cs.LG math.OC stat.ML  published:2013-07-23 summary:We present a formal model of human decision-making in explore-exploit tasks using the context of multi-armed bandit problems, where the decision-maker must choose among multiple options with uncertain rewards. We address the standard multi-armed bandit problem, the multi-armed bandit problem with transition costs, and the multi-armed bandit problem on graphs. We focus on the case of Gaussian rewards in a setting where the decision-maker uses Bayesian inference to estimate the reward values. We model the decision-maker's prior knowledge with the Bayesian prior on the mean reward. We develop the upper credible limit (UCL) algorithm for the standard multi-armed bandit problem and show that this deterministic algorithm achieves logarithmic cumulative expected regret, which is optimal performance for uninformative priors. We show how good priors and good assumptions on the correlation structure among arms can greatly enhance decision-making performance, even over short time horizons. We extend to the stochastic UCL algorithm and draw several connections to human decision-making behavior. We present empirical data from human experiments and show that human performance is efficiently captured by the stochastic UCL algorithm with appropriate parameters. For the multi-armed bandit problem with transition costs and the multi-armed bandit problem on graphs, we generalize the UCL algorithm to the block UCL algorithm and the graphical block UCL algorithm, respectively. We show that these algorithms also achieve logarithmic cumulative expected regret and require a sub-logarithmic expected number of transitions among arms. We further illustrate the performance of these algorithms with numerical examples. version:3
arxiv-1312-0365 | The Law of Total Odds | http://arxiv.org/abs/1312.0365 | id:1312.0365 author:Dirk Tasche category:math.PR stat.AP stat.ML 60A05  62H30  published:2013-12-02 summary:The law of total probability may be deployed in binary classification exercises to estimate the unconditional class probabilities if the class proportions in the training set are not representative of the population class proportions. We argue that this is not a conceptually sound approach and suggest an alternative based on the new law of total odds. We quantify the bias of the total probability estimator of the unconditional class probabilities and show that the total odds estimator is unbiased. The sample version of the total odds estimator is shown to coincide with a maximum-likelihood estimator known from the literature. The law of total odds can also be used for transforming the conditional class probabilities if independent estimates of the unconditional class probabilities of the population are available. Keywords: Total probability, likelihood ratio, Bayes' formula, binary classification, relative odds, unbiased estimator, supervised learning, dataset shift. version:5
arxiv-1312-6098 | On the number of response regions of deep feed forward networks with piece-wise linear activations | http://arxiv.org/abs/1312.6098 | id:1312.6098 author:Razvan Pascanu, Guido Montufar, Yoshua Bengio category:cs.LG cs.NE  published:2013-12-20 summary:This paper explores the complexity of deep feedforward networks with linear pre-synaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has $kn$ hidden units and $n_0$ inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$ layer model with $n$ hidden units on each layer it is $\Omega(\left\lfloor {n}/{n_0}\right\rfloor^{k-1}n^{n_0})$. The number $\left\lfloor{n}/{n_0}\right\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$ tends to infinity or when $k$ tends to infinity and $n \geq 2n_0$. Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis. version:5
arxiv-1402-3511 | A Clockwork RNN | http://arxiv.org/abs/1402.3511 | id:1402.3511 author:Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber category:cs.NE cs.LG  published:2014-02-14 summary:Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks. version:1
arxiv-1311-0707 | Generative Modelling for Unsupervised Score Calibration | http://arxiv.org/abs/1311.0707 | id:1311.0707 author:Niko Brümmer, Daniel Garcia-Romero category:stat.ML cs.LG  published:2013-11-04 summary:Score calibration enables automatic speaker recognizers to make cost-effective accept / reject decisions. Traditional calibration requires supervised data, which is an expensive resource. We propose a 2-component GMM for unsupervised calibration and demonstrate good performance relative to a supervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstrates that the uncertainty associated with the unsupervised calibration parameter estimates is surprisingly small. version:3
arxiv-1312-6002 | Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence | http://arxiv.org/abs/1312.6002 | id:1312.6002 author:Mathias Berglund, Tapani Raiko category:cs.NE cs.LG stat.ML 62M45 I.2.6  published:2013-12-20 summary:Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different biases and variances for stochastic gradient estimates of individual data points. It is well known that CD yields a biased gradient estimate. In this paper we however show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the mean of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD. version:3
arxiv-1402-3405 | Authorship Analysis based on Data Compression | http://arxiv.org/abs/1402.3405 | id:1402.3405 author:Daniele Cerra, Mihai Datcu, Peter Reinartz category:cs.CL cs.DL cs.IR stat.ML  published:2014-02-14 summary:This paper proposes to perform authorship analysis using the Fast Compression Distance (FCD), a similarity measure based on compression with dictionaries directly extracted from the written texts. The FCD computes a similarity between two documents through an effective binary search on the intersection set between the two related dictionaries. In the reported experiments the proposed method is applied to documents which are heterogeneous in style, written in five different languages and coming from different historical periods. Results are comparable to the state of the art and outperform traditional compression-based methods. version:1
arxiv-1304-5299 | Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget | http://arxiv.org/abs/1304.5299 | id:1304.5299 author:Anoop Korattikara, Yutian Chen, Max Welling category:cs.LG stat.ML  published:2013-04-19 summary:Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time. version:4
arxiv-1402-3382 | Machine Learning of Phonologically Conditioned Noun Declensions For Tamil Morphological Generators | http://arxiv.org/abs/1402.3382 | id:1402.3382 author:K. Rajan, Dr. V. Ramalingam, Dr. M. Ganesan category:cs.CL  published:2014-02-14 summary:This paper presents machine learning solutions to a practical problem of Natural Language Generation (NLG), particularly the word formation in agglutinative languages like Tamil, in a supervised manner. The morphological generator is an important component of Natural Language Processing in Artificial Intelligence. It generates word forms given a root and affixes. The morphophonemic changes like addition, deletion, alternation etc., occur when two or more morphemes or words joined together. The Sandhi rules should be explicitly specified in the rule based morphological analyzers and generators. In machine learning framework, these rules can be learned automatically by the system from the training samples and subsequently be applied for new inputs. In this paper we proposed the machine learning models which learn the morphophonemic rules for noun declensions from the given training data. These models are trained to learn sandhi rules using various learning algorithms and the performance of those algorithms are presented. From this we conclude that machine learning of morphological processing such as word form generation can be successfully learned in a supervised manner, without explicit description of rules. The performance of Decision trees and Bayesian machine learning algorithms on noun declensions are discussed. version:1
arxiv-1402-3371 | An evaluative baseline for geo-semantic relatedness and similarity | http://arxiv.org/abs/1402.3371 | id:1402.3371 author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.CL  published:2014-02-14 summary:In geographic information science and semantics, the computation of semantic similarity is widely recognised as key to supporting a vast number of tasks in information integration and retrieval. By contrast, the role of geo-semantic relatedness has been largely ignored. In natural language processing, semantic relatedness is often confused with the more specific semantic similarity. In this article, we discuss a notion of geo-semantic relatedness based on Lehrer's semantic fields, and we compare it with geo-semantic similarity. We then describe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a new open dataset designed to evaluate computational measures of geo-semantic relatedness and similarity. This dataset is larger than existing datasets of this kind, and includes 97 geographic terms combined into 50 term pairs rated by 203 human subjects. GeReSiD is available online and can be used as an evaluation baseline to determine empirically to what degree a given computational model approximates geo-semantic relatedness and similarity. version:1
arxiv-1402-1783 | Active Clustering with Model-Based Uncertainty Reduction | http://arxiv.org/abs/1402.1783 | id:1402.1783 author:Caiming Xiong, David Johnson, Jason J. Corso category:cs.LG cs.CV stat.ML 62H30  published:2014-02-07 summary:Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are \emph{passive} in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an \emph{active} clustering method---i.e. an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters. version:2
arxiv-1305-0213 | Recovering Graph-Structured Activations using Adaptive Compressive Measurements | http://arxiv.org/abs/1305.0213 | id:1305.0213 author:Akshay Krishnamurthy, James Sharpnack, Aarti Singh category:stat.ML cs.IT math.IT  published:2013-05-01 summary:We study the localization of a cluster of activated vertices in a graph, from adaptively designed compressive measurements. We propose a hierarchical partitioning of the graph that groups the activated vertices into few partitions, so that a top-down sensing procedure can identify these partitions, and hence the activations, using few measurements. By exploiting the cluster structure, we are able to provide localization guarantees at weaker signal to noise ratios than in the unstructured setting. We complement this performance guarantee with an information theoretic lower bound, providing a necessary signal-to-noise ratio for any algorithm to successfully localize the cluster. We verify our analysis with some simulations, demonstrating the practicality of our algorithm. version:3
arxiv-1312-0516 | Grid Topology Identification using Electricity Prices | http://arxiv.org/abs/1312.0516 | id:1312.0516 author:Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick category:cs.LG cs.SY stat.AP stat.ML  published:2013-12-02 summary:The potential of recovering the topology of a grid using solely publicly available market data is explored here. In contemporary whole-sale electricity markets, real-time prices are typically determined by solving the network-constrained economic dispatch problem. Under a linear DC model, locational marginal prices (LMPs) correspond to the Lagrange multipliers of the linear program involved. The interesting observation here is that the matrix of spatiotemporally varying LMPs exhibits the following property: Once premultiplied by the weighted grid Laplacian, it yields a low-rank and sparse matrix. Leveraging this rich structure, a regularized maximum likelihood estimator (MLE) is developed to recover the grid Laplacian from the LMPs. The convex optimization problem formulated includes low rank- and sparsity-promoting regularizers, and it is solved using a scalable algorithm. Numerical tests on prices generated for the IEEE 14-bus benchmark provide encouraging topology recovery results. version:2
arxiv-1402-3261 | Hand-Eye and Robot-World Calibration by Global Polynomial Optimization | http://arxiv.org/abs/1402.3261 | id:1402.3261 author:Jan Heller, Didier Henrion, Tomas Pajdla category:cs.CV math.OC  published:2014-02-13 summary:The need to relate measurements made by a camera to a different known coordinate system arises in many engineering applications. Historically, it appeared for the first time in the connection with cameras mounted on robotic systems. This problem is commonly known as hand-eye calibration. In this paper, we present several formulations of hand-eye calibration that lead to multivariate polynomial optimization problems. We show that the method of convex linear matrix inequality (LMI) relaxations can be used to effectively solve these problems and to obtain globally optimal solutions. Further, we show that the same approach can be used for the simultaneous hand-eye and robot-world calibration. Finally, we validate the proposed solutions using both synthetic and real datasets. version:1
arxiv-1402-3085 | Gaussian Process Volatility Model | http://arxiv.org/abs/1402.3085 | id:1402.3085 author:Yue Wu, Jose Miguel Hernandez Lobato, Zoubin Ghahramani category:stat.ME stat.ML 62P05  published:2014-02-13 summary:The accurate prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the variances. Moreover, function parameters are usually learned using maximum likelihood, which can lead to overfitting. To address these problems we introduce a novel model for time-changing variances using Gaussian Processes. A Gaussian Process (GP) defines a distribution over functions, which allows us to capture highly flexible functional relationships for the variances. In addition, we develop an online algorithm to perform inference. The algorithm has two main advantages. First, it takes a Bayesian approach, thereby avoiding overfitting. Second, it is much quicker than current offline inference procedures. Finally, our new model was evaluated on financial data and showed significant improvement in predictive performance over current standard models. version:1
arxiv-1402-3080 | Software Requirement Specification Using Reverse Speech Technology | http://arxiv.org/abs/1402.3080 | id:1402.3080 author:Santhy Viswam, Sajeer Karattil category:cs.CL cs.SD  published:2014-02-13 summary:Speech analysis had been taken to a new level with the discovery of Reverse Speech (RS). RS is the discovery of hidden messages, referred as reversals, in normal speech. Works are in progress for exploiting the relevance of RS in different real world applications such as investigation, medical field etc. In this paper we represent an innovative method for preparing a reliable Software Requirement Specification (SRS) document with the help of reverse speech. As SRS act as the backbone for the successful completion of any project, a reliable method is needed to overcome the inconsistencies. Using RS such a reliable method for SRS documentation was developed. version:1
arxiv-1402-3070 | Squeezing bottlenecks: exploring the limits of autoencoder semantic representation capabilities | http://arxiv.org/abs/1402.3070 | id:1402.3070 author:Parth Gupta, Rafael E. Banchs, Paolo Rosso category:cs.IR cs.LG stat.ML  published:2014-02-13 summary:We present a comprehensive study on the use of autoencoders for modelling text data, in which (differently from previous studies) we focus our attention on the following issues: i) we explore the suitability of two different models bDA and rsDA for constructing deep autoencoders for text data at the sentence level; ii) we propose and evaluate two novel metrics for better assessing the text-reconstruction capabilities of autoencoders; and iii) we propose an automatic method to find the critical bottleneck dimensionality for text language representations (below which structural information is lost). version:1
arxiv-1402-3040 | Event Structure of Transitive Verb: A MARVS perspective | http://arxiv.org/abs/1402.3040 | id:1402.3040 author:Jia-Fei Hong, Kathleen Ahrens, Chu-Ren Huang category:cs.CL  published:2014-02-13 summary:Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of the representation of verbal semantics that is based on Mandarin Chinese data (Huang et al. 2000). In the MARVS theory, there are two different types of modules: Event Structure Modules and Role Modules. There are also two sets of attributes: Event-Internal Attributes and Role-Internal Attributes, which are linked to the Event Structure Module and the Role Module, respectively. In this study, we focus on four transitive verbs as chi1(eat), wan2(play), huan4(change) and shao1(burn) and explore their event structures by the MARVS theory. version:1
arxiv-1402-3032 | Regularization for Multiple Kernel Learning via Sum-Product Networks | http://arxiv.org/abs/1402.3032 | id:1402.3032 author:Ziming Zhang category:stat.ML cs.LG  published:2014-02-13 summary:In this paper, we are interested in constructing general graph-based regularizers for multiple kernel learning (MKL) given a structure which is used to describe the way of combining basis kernels. Such structures are represented by sum-product networks (SPNs) in our method. Accordingly we propose a new convex regularization method for MLK based on a path-dependent kernel weighting function which encodes the entire SPN structure in our method. Under certain conditions and from the view of probability, this function can be considered to follow multinomial distributions over the weights associated with product nodes in SPNs. We also analyze the convexity of our regularizer and the complexity of our induced classifiers, and further propose an efficient wrapper algorithm to optimize our formulation. In our experiments, we apply our method to ...... version:1
arxiv-1306-0618 | Prediction with Missing Data via Bayesian Additive Regression Trees | http://arxiv.org/abs/1306.0618 | id:1306.0618 author:Adam Kapelner, Justin Bleich category:stat.ML cs.LG  published:2013-06-03 summary:We present a method for incorporating missing data in non-parametric statistical learning without the need for imputation. We focus on a tree-based method, Bayesian Additive Regression Trees (BART), enhanced with "Missingness Incorporated in Attributes," an approach recently proposed incorporating missingness into decision trees (Twala, 2008). This procedure takes advantage of the partitioning mechanisms found in tree-based models. Simulations on generated models and real data indicate that our proposed method can forecast well on complicated missing-at-random and not-missing-at-random models as well as models where missingness itself influences the response. Our procedure has higher predictive performance and is more stable than competitors in many cases. We also illustrate BART's abilities to incorporate missingness into uncertainty intervals and to detect the influence of missingness on the model fit. version:3
arxiv-1402-2959 | Local Optima Networks: A New Model of Combinatorial Fitness Landscapes | http://arxiv.org/abs/1402.2959 | id:1402.2959 author:Gabriela Ochoa, Sébastien Verel, Fabio Daolio, Marco Tomassini category:cs.NE cs.AI  published:2014-02-12 summary:This chapter overviews a recently introduced network-based model of combinatorial landscapes: Local Optima Networks (LON). The model compresses the information given by the whole search space into a smaller mathematical object that is a graph having as vertices the local optima and as edges the possible weighted transitions between them. Two definitions of edges have been proposed: basin-transition and escape-edges, which capture relevant topological features of the underlying search spaces. This network model brings a new set of metrics to characterize the structure of combinatorial landscapes, those associated with the science of complex networks. These metrics are described, and results are presented of local optima network extraction and analysis for two selected combinatorial landscapes: NK landscapes and the quadratic assignment problem. Network features are found to correlate with and even predict the performance of heuristic search algorithms operating on these problems. version:1
arxiv-1205-0953 | Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization | http://arxiv.org/abs/1205.0953 | id:1205.0953 author:Martin Slawski, Matthias Hein category:math.ST stat.ML stat.TH  published:2012-05-04 summary:Least squares fitting is in general not useful for high-dimensional linear models, in which the number of predictors is of the same or even larger order of magnitude than the number of samples. Theory developed in recent years has coined a paradigm according to which sparsity-promoting regularization is regarded as a necessity in such setting. Deviating from this paradigm, we show that non-negativity constraints on the regression coefficients may be similarly effective as explicit regularization if the design matrix has additional properties, which are met in several applications of non-negative least squares (NNLS). We show that for these designs, the performance of NNLS with regard to prediction and estimation is comparable to that of the lasso. We argue further that in specific cases, NNLS may have a better $\ell_{\infty}$-rate in estimation and hence also advantages with respect to support recovery when combined with thresholding. From a practical point of view, NNLS does not depend on a regularization parameter and is hence easier to use. version:2
arxiv-1306-0308 | Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics | http://arxiv.org/abs/1306.0308 | id:1306.0308 author:Philipp Hennig, Søren Hauberg category:stat.ML cs.LG math.NA 65L05  65L10  58D17  published:2013-06-03 summary:We study a probabilistic numerical method for the solution of both boundary and initial value problems that returns a joint Gaussian process posterior over the solution. Such methods have concrete value in the statistics on Riemannian manifolds, where non-analytic ordinary differential equations are involved in virtually all computations. The probabilistic formulation permits marginalising the uncertainty of the numerical solution such that statistics are less sensitive to inaccuracies. This leads to new Riemannian algorithms for mean value computations and principal geodesic analysis. Marginalisation also means results can be less precise than point estimates, enabling a noticeable speed-up over the state of the art. Our approach is an argument for a wider point that uncertainty caused by numerical calculations should be tracked throughout the pipeline of machine learning algorithms. version:2
arxiv-1402-2796 | PR2: A Language Independent Unsupervised Tool for Personality Recognition from Text | http://arxiv.org/abs/1402.2796 | id:1402.2796 author:Fabio Celli, Massimo Poesio category:cs.CL  published:2014-02-12 summary:We present PR2, a personality recognition system available online, that performs instance-based classification of Big5 personality types from unstructured text, using language-independent features. It has been tested on English and Italian, achieving performances up to f=.68. version:1
arxiv-1309-5701 | Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability | http://arxiv.org/abs/1309.5701 | id:1309.5701 author:Tomohiko Mizutani category:stat.ML  published:2013-09-23 summary:We present a numerical algorithm for nonnegative matrix factorization (NMF) problems under noisy separability. An NMF problem under separability can be stated as one of finding all vertices of the convex hull of data points. The research interest of this paper is to find the vectors as close to the vertices as possible in a situation in which noise is added to the data points. Our algorithm is designed to capture the shape of the convex hull of data points by using its enclosing ellipsoid. We show that the algorithm has correctness and robustness properties from theoretical and practical perspectives; correctness here means that if the data points do not contain any noise, the algorithm can find the vertices of their convex hull; robustness means that if the data points contain noise, the algorithm can find the near-vertices. Finally, we apply the algorithm to document clustering, and report the experimental results. version:2
arxiv-1306-3343 | Relaxed Sparse Eigenvalue Conditions for Sparse Estimation via Non-convex Regularized Regression | http://arxiv.org/abs/1306.3343 | id:1306.3343 author:Zheng Pan, Changshui Zhang category:cs.LG cs.NA stat.ML  published:2013-06-14 summary:Non-convex regularizers usually improve the performance of sparse estimation in practice. To prove this fact, we study the conditions of sparse estimations for the sharp concave regularizers which are a general family of non-convex regularizers including many existing regularizers. For the global solutions of the regularized regression, our sparse eigenvalue based conditions are weaker than that of L1-regularization for parameter estimation and sparseness estimation. For the approximate global and approximate stationary (AGAS) solutions, almost the same conditions are also enough. We show that the desired AGAS solutions can be obtained by coordinate descent (CD) based methods. Finally, we perform some experiments to show the performance of CD methods on giving AGAS solutions and the degree of weakness of the estimation conditions required by the sharp concave regularizers. version:3
arxiv-1307-0813 | Multi-Task Policy Search | http://arxiv.org/abs/1307.0813 | id:1307.0813 author:Marc Peter Deisenroth, Peter Englert, Jan Peters, Dieter Fox category:stat.ML cs.AI cs.LG cs.RO  published:2013-07-02 summary:Learning policies that generalize across multiple tasks is an important and challenging research topic in reinforcement learning and robotics. Training individual policies for every single potential task is often impractical, especially for continuous task variations, requiring more principled approaches to share and transfer knowledge among similar tasks. We present a novel approach for learning a nonlinear feedback policy that generalizes across multiple tasks. The key idea is to define a parametrized policy as a function of both the state and the task, which allows learning a single policy that generalizes across multiple known and unknown tasks. Applications of our novel approach to reinforcement and imitation learning in real-robot experiments are shown. version:2
arxiv-1306-6709 | A Survey on Metric Learning for Feature Vectors and Structured Data | http://arxiv.org/abs/1306.6709 | id:1306.6709 author:Aurélien Bellet, Amaury Habrard, Marc Sebban category:cs.LG stat.ML  published:2013-06-28 summary:The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come. version:4
arxiv-1210-1766 | Bayesian Inference with Posterior Regularization and applications to Infinite Latent SVMs | http://arxiv.org/abs/1210.1766 | id:1210.1766 author:Jun Zhu, Ning Chen, Eric P. Xing category:cs.LG cs.AI stat.ME stat.ML  published:2012-10-05 summary:Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors can affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks whose Bayesian formulation results in hybrid chain graph models. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results were not available until now, and contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community. version:3
arxiv-1402-2720 | Noise Analysis for Lensless Compressive Imaging | http://arxiv.org/abs/1402.2720 | id:1402.2720 author:Hong Jiang, Gang Huang, Paul Wilford category:cs.CV  published:2014-02-12 summary:We analyze the signal to noise ratio (SNR) in a recently proposed lensless compressive imaging architecture. The architecture consists of a sensor of a single detector element and an aperture assembly of an array of aperture elements, each of which has a programmable transmittance. This lensless compressive imaging architecture can be used in conjunction with compressive sensing to capture images in a compressed form of compressive measurements. In this paper, we perform noise analysis of this lensless compressive imaging architecture and compare it with pinhole aperture imaging and lens aperture imaging. We will show that the SNR in the lensless compressive imaging is independent of the image resolution, while that in either pinhole aperture imaging or lens aperture imaging decreases as the image resolution increases. Consequently, the SNR in the lensless compressive imaging can be much higher if the image resolution is large enough. version:1
arxiv-1402-2704 | Sex as Gibbs Sampling: a probability model of evolution | http://arxiv.org/abs/1402.2704 | id:1402.2704 author:Chris Watkins, Yvonne Buttkewitz category:q-bio.PE cs.NE  published:2014-02-12 summary:We show that evolutionary computation can be implemented as standard Markov-chain Monte-Carlo (MCMC) sampling. With some care, `genetic algorithms' can be constructed that are reversible Markov chains that satisfy detailed balance; it follows that the stationary distribution of populations is a Gibbs distribution in a simple factorised form. For some standard and popular nonparametric probability models, we exhibit Gibbs-sampling procedures that are plausible genetic algorithms. At mutation-selection equilibrium, a population of genomes is analogous to a sample from a Bayesian posterior, and the genomes are analogous to latent variables. We suggest this is a general, tractable, and insightful formulation of evolutionary computation in terms of standard machine learning concepts and techniques. In addition, we show that evolutionary processes in which selection acts by differences in fecundity are not reversible, and also that it is not possible to construct reversible evolutionary models in which each child is produced by only two parents. version:1
arxiv-1309-7750 | An Extensive Experimental Study on the Cluster-based Reference Set Reduction for speeding-up the k-NN Classifier | http://arxiv.org/abs/1309.7750 | id:1309.7750 author:Stefanos Ougiaroglou, Georgios Evangelidis, Dimitris A. Dervos category:cs.LG  published:2013-09-30 summary:The k-Nearest Neighbor (k-NN) classification algorithm is one of the most widely-used lazy classifiers because of its simplicity and ease of implementation. It is considered to be an effective classifier and has many applications. However, its major drawback is that when sequential search is used to find the neighbors, it involves high computational cost. Speeding-up k-NN search is still an active research field. Hwang and Cho have recently proposed an adaptive cluster-based method for fast Nearest Neighbor searching. The effectiveness of this method is based on the adjustment of three parameters. However, the authors evaluated their method by setting specific parameter values and using only one dataset. In this paper, an extensive experimental study of this method is presented. The results, which are based on five real life datasets, illustrate that if the parameters of the method are carefully defined, one can achieve even better classification performance. version:2
arxiv-1402-2673 | Real-Time Hand Shape Classification | http://arxiv.org/abs/1402.2673 | id:1402.2673 author:Jakub Nalepa, Michal Kawulok category:cs.CV  published:2014-02-11 summary:The problem of hand shape classification is challenging since a hand is characterized by a large number of degrees of freedom. Numerous shape descriptors have been proposed and applied over the years to estimate and classify hand poses in reasonable time. In this paper we discuss our parallel framework for real-time hand shape classification applicable in real-time applications. We show how the number of gallery images influences the classification accuracy and execution time of the parallel algorithm. We present the speedup and efficiency analyses that prove the efficacy of the parallel implementation. Noteworthy, different methods can be used at each step of our parallel framework. Here, we combine the shape contexts with the appearance-based techniques to enhance the robustness of the algorithm and to increase the classification score. An extensive experimental study proves the superiority of the proposed approach over existing state-of-the-art methods. version:1
arxiv-1402-2667 | On Zeroth-Order Stochastic Convex Optimization via Random Walks | http://arxiv.org/abs/1402.2667 | id:1402.2667 author:Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin category:cs.LG stat.ML  published:2014-02-11 summary:We propose a method for zeroth order stochastic convex optimization that attains the suboptimality rate of $\tilde{\mathcal{O}}(n^{7}T^{-1/2})$ after $T$ queries for a convex bounded function $f:{\mathbb R}^n\to{\mathbb R}$. The method is based on a random walk (the \emph{Ball Walk}) on the epigraph of the function. The randomized approach circumvents the problem of gradient estimation, and appears to be less sensitive to noisy function evaluations compared to noiseless zeroth order methods. version:1
arxiv-1402-2606 | A Fast Two Pass Multi-Value Segmentation Algorithm based on Connected Component Analysis | http://arxiv.org/abs/1402.2606 | id:1402.2606 author:Dibyendu Mukherjee category:cs.CV  published:2014-02-11 summary:Connected component analysis (CCA) has been heavily used to label binary images and classify segments. However, it has not been well-exploited to segment multi-valued natural images. This work proposes a novel multi-value segmentation algorithm that utilizes CCA to segment color images. A user defined distance measure is incorporated in the proposed modified CCA to identify and segment similar image regions. The raw output of the algorithm consists of distinctly labelled segmented regions. The proposed algorithm has a unique design architecture that provides several benefits: 1) it can be used to segment any multi-channel multi-valued image; 2) the distance measure/segmentation criteria can be application-specific and 3) an absolute linear-time implementation allows easy extension for real-time video segmentation. Experimental demonstrations of the aforesaid benefits are presented along with the comparison results on multiple datasets with current benchmark algorithms. A number of possible application areas are also identified and results on real-time video segmentation has been presented to show the promise of the proposed method. version:1
arxiv-1402-2594 | Online Nonparametric Regression | http://arxiv.org/abs/1402.2594 | id:1402.2594 author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG math.ST stat.TH  published:2014-02-11 summary:We establish optimal rates for online regression for arbitrary classes of regression functions in terms of the sequential entropy introduced in (Rakhlin, Sridharan, Tewari, 2010). The optimal rates are shown to exhibit a phase transition analogous to the i.i.d./statistical learning case, studied in (Rakhlin, Sridharan, Tsybakov 2013). In the frequently encountered situation when sequential entropy and i.i.d. empirical entropy match, our results point to the interesting phenomenon that the rates for statistical learning with squared loss and online nonparametric regression are the same. In addition to a non-algorithmic study of minimax regret, we exhibit a generic forecaster that enjoys the established optimal rates. We also provide a recipe for designing online regression algorithms that can be computationally efficient. We illustrate the techniques by deriving existing and new forecasters for the case of finite experts and for online linear regression. version:1
arxiv-1312-6594 | Sequentially Generated Instance-Dependent Image Representations for Classification | http://arxiv.org/abs/1312.6594 | id:1312.6594 author:Gabriel Dulac-Arnold, Ludovic Denoyer, Nicolas Thome, Matthieu Cord, Patrick Gallinari category:cs.CV cs.LG  published:2013-12-20 summary:In this paper, we investigate a new framework for image classification that adaptively generates spatial representations. Our strategy is based on a sequential process that learns to explore the different regions of any image in order to infer its category. In particular, the choice of regions is specific to each image, directed by the actual content of previously selected regions.The capacity of the system to handle incomplete image information as well as its adaptive region selection allow the system to perform well in budgeted classification tasks by exploiting a dynamicly generated representation of each image. We demonstrate the system's abilities in a series of image-based exploration and classification tasks that highlight its learned exploration and inference abilities. version:3
arxiv-1402-2826 | Realtime Multilevel Crowd Tracking using Reciprocal Velocity Obstacles | http://arxiv.org/abs/1402.2826 | id:1402.2826 author:Aniket Bera, Dinesh Manocha category:cs.CV  published:2014-02-11 summary:We present a novel, realtime algorithm to compute the trajectory of each pedestrian in moderately dense crowd scenes. Our formulation is based on an adaptive particle filtering scheme that uses a multi-agent motion model based on velocity-obstacles, and takes into account local interactions as well as physical and personal constraints of each pedestrian. Our method dynamically changes the number of particles allocated to each pedestrian based on different confidence metrics. Additionally, we use a new high-definition crowd video dataset, which is used to evaluate the performance of different pedestrian tracking algorithms. This dataset consists of videos of indoor and outdoor scenes, recorded at different locations with 30-80 pedestrians. We highlight the performance benefits of our algorithm over prior techniques using this dataset. In practice, our algorithm can compute trajectories of tens of pedestrians on a multi-core desktop CPU at interactive rates (27-30 frames per second). To the best of our knowledge, our approach is 4-5 times faster than prior methods, which provide similar accuracy. version:1
arxiv-1402-2499 | Justifying Information-Geometric Causal Inference | http://arxiv.org/abs/1402.2499 | id:1402.2499 author:Dominik Janzing, Bastian Steudel, Naji Shajarisales, Bernhard Schölkopf category:stat.ML  published:2014-02-11 summary:Information Geometric Causal Inference (IGCI) is a new approach to distinguish between cause and effect for two variables. It is based on an independence assumption between input distribution and causal mechanism that can be phrased in terms of orthogonality in information space. We describe two intuitive reinterpretations of this approach that makes IGCI more accessible to a broader audience. Moreover, we show that the described independence is related to the hypothesis that unsupervised learning and semi-supervised learning only works for predicting the cause from the effect and not vice versa. version:1
arxiv-1402-2427 | An evaluation of keyword extraction from online communication for the characterisation of social relations | http://arxiv.org/abs/1402.2427 | id:1402.2427 author:Jan Hauffa, Tobias Lichtenberg, Georg Groh category:cs.SI cs.CL cs.IR  published:2014-02-11 summary:The set of interpersonal relationships on a social network service or a similar online community is usually highly heterogenous. The concept of tie strength captures only one aspect of this heterogeneity. Since the unstructured text content of online communication artefacts is a salient source of information about a social relationship, we investigate the utility of keywords extracted from the message body as a representation of the relationship's characteristics as reflected by the conversation topics. Keyword extraction is performed using standard natural language processing methods. Communication data and human assessments of the extracted keywords are obtained from Facebook users via a custom application. The overall positive quality assessment provides evidence that the keywords indeed convey relevant information about the relationship. version:1
arxiv-1402-2426 | Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision | http://arxiv.org/abs/1402.2426 | id:1402.2426 author:Keith Dillon, Yeshaiahu Fainman category:cs.CV  published:2014-02-11 summary:In this paper we broadly consider techniques which utilize projections on rays for data collection, with particular emphasis on optical techniques. We formulate a variety of imaging techniques as either special cases or extensions of tomographic reconstruction. We then consider how the techniques must be extended to describe objects containing occlusion, as with a self-occluding opaque object. We formulate the reconstruction problem as a regularized nonlinear optimization problem to simultaneously solve for object brightness and attenuation, where the attenuation can become infinite. We demonstrate various simulated examples for imaging opaque objects, including sparse point sources, a conventional multiview reconstruction technique, and a super-resolving technique which exploits occlusion to resolve an image. version:1
arxiv-1401-5899 | Kernel Least Mean Square with Adaptive Kernel Size | http://arxiv.org/abs/1401.5899 | id:1401.5899 author:Badong Chen, Junli Liang, Nanning Zheng, Jose C. Principe category:stat.ML cs.LG  published:2014-01-23 summary:Kernel adaptive filters (KAF) are a class of powerful nonlinear filters developed in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel is usually the default kernel in KAF algorithms, but selecting the proper kernel size (bandwidth) is still an open important issue especially for learning with small sample sizes. In previous research, the kernel size was set manually or estimated in advance by Silvermans rule based on the sample distribution. This study aims to develop an online technique for optimizing the kernel size of the kernel least mean square (KLMS) algorithm. A sequential optimization strategy is proposed, and a new algorithm is developed, in which the filter weights and the kernel size are both sequentially updated by stochastic gradient algorithms that minimize the mean square error (MSE). Theoretical results on convergence are also presented. The excellent performance of the new algorithm is confirmed by simulations on static function estimation and short term chaotic time series prediction. version:3
arxiv-1402-2363 | Animation of 3D Human Model Using Markerless Motion Capture Applied To Sports | http://arxiv.org/abs/1402.2363 | id:1402.2363 author:Ashish Shingade, Archana Ghotkar category:cs.GR cs.CV  published:2014-02-11 summary:Markerless motion capture is an active research in 3D virtualization. In proposed work we presented a system for markerless motion capture for 3D human character animation, paper presents a survey on motion and skeleton tracking techniques which are developed or are under development. The paper proposed a method to transform the motion of a performer to a 3D human character (model), the 3D human character performs similar movements as that of a performer in real time. In the proposed work, human model data will be captured by Kinect camera, processed data will be applied on 3D human model for animation. 3D human model is created using open source software (MakeHuman). Anticipated dataset for sport activity is considered as input which can be applied to any HCI application. version:1
arxiv-1402-3578 | Learning-assisted Theorem Proving with Millions of Lemmas | http://arxiv.org/abs/1402.3578 | id:1402.3578 author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO  published:2014-02-11 summary:Large formal mathematical libraries consist of millions of atomic inference steps that give rise to a corresponding number of proved statements (lemmas). Analogously to the informal mathematical practice, only a tiny fraction of such statements is named and re-used in later proofs by formal mathematicians. In this work, we suggest and implement criteria defining the estimated usefulness of the HOL Light lemmas for proving further theorems. We use these criteria to mine the large inference graph of the lemmas in the HOL Light and Flyspeck libraries, adding up to millions of the best lemmas to the pool of statements that can be re-used in later proofs. We show that in combination with learning-based relevance filtering, such methods significantly strengthen automated theorem proving of new conjectures over large formal mathematical libraries such as Flyspeck. version:1
arxiv-1402-2335 | Sparsity averaging for radio-interferometric imaging | http://arxiv.org/abs/1402.2335 | id:1402.2335 author:Rafael E. Carrillo, Jason D. McEwen, Yves Wiaux category:astro-ph.IM cs.CV  published:2014-02-11 summary:We propose a novel regularization method for compressive imaging in the context of the compressed sensing (CS) theory with coherent and redundant dictionaries. Natural images are often complicated and several types of structures can be present at once. It is well known that piecewise smooth images exhibit gradient sparsity, and that images with extended structures are better encapsulated in wavelet frames. Therefore, we here conjecture that promoting average sparsity or compressibility over multiple frames rather than single frames is an extremely powerful regularization prior. version:1
arxiv-1402-2333 | Modeling sequential data using higher-order relational features and predictive training | http://arxiv.org/abs/1402.2333 | id:1402.2333 author:Vincent Michalski, Roland Memisevic, Kishore Konda category:cs.LG cs.CV stat.ML  published:2014-02-10 summary:Bi-linear feature learning models, like the gated autoencoder, were proposed as a way to model relationships between frames in a video. By minimizing reconstruction error of one frame, given the previous frame, these models learn "mapping units" that encode the transformations inherent in a sequence, and thereby learn to encode motion. In this work we extend bi-linear models by introducing "higher-order mapping units" that allow us to encode transformations between frames and transformations between transformations. We show that this makes it possible to encode temporal structure that is more complex and longer-range than the structure captured within standard bi-linear models. We also show that a natural way to train the model is by replacing the commonly used reconstruction objective with a prediction objective which forces the model to correctly predict the evolution of the input multiple steps into the future. Learning can be achieved by back-propagating the multi-step prediction through time. We test the model on various temporal prediction tasks, and show that higher-order mappings and predictive training both yield a significant improvement over bi-linear models in terms of prediction accuracy. version:1
arxiv-1402-2300 | Feature and Variable Selection in Classification | http://arxiv.org/abs/1402.2300 | id:1402.2300 author:Aaron Karper category:cs.LG cs.AI stat.ML  published:2014-02-10 summary:The amount of information in the form of features and variables avail- able to machine learning algorithms is ever increasing. This can lead to classifiers that are prone to overfitting in high dimensions, high di- mensional models do not lend themselves to interpretable results, and the CPU and memory resources necessary to run on high-dimensional datasets severly limit the applications of the approaches. Variable and feature selection aim to remedy this by finding a subset of features that in some way captures the information provided best. In this paper we present the general methodology and highlight some specific approaches. version:1
arxiv-1402-2562 | Étude cognitive des processus de construction d'une requête dans un système de gestion de connaissances médicales | http://arxiv.org/abs/1402.2562 | id:1402.2562 author:Nathalie Chaignaud, Valérie Delavigne, Maryvonne Holzem, Jean-Philippe Kotowicz, Alain Loisel category:cs.IR cs.CL  published:2014-02-10 summary:This article presents the Cogni-CISMeF project, which aims at improving medical information search in the CISMeF system (Catalog and Index of French-language health resources) by including a conversational agent to interact with the user in natural language. To study the cognitive processes involved during the information search, a bottom-up methodology was adopted. Experimentation has been set up to obtain human dialogs between a user (playing the role of patient) dealing with medical information search and a CISMeF expert refining the request. The analysis of these dialogs underlined the use of discursive evidence: vocabulary, reformulation, implicit or explicit expression of user intentions, conversational sequences, etc. A model of artificial agent is proposed. It leads the user in its information search by proposing to him examples, assistance and choices. This model was implemented and integrated in the CISMeF system. ---- Cet article d\'ecrit le projet Cogni-CISMeF qui propose un module de dialogue Homme-Machine \`a int\'egrer dans le syst\`eme d'indexation de connaissances m\'edicales CISMeF (Catalogue et Index des Sites M\'edicaux Francophones). Nous avons adopt\'e une d\'emarche de mod\'elisation cognitive en proc\'edant \`a un recueil de corpus de dialogues entre un utilisateur (jouant le r\^ole d'un patient) d\'esirant une information m\'edicale et un expert CISMeF af inant cette demande pour construire la requ\^ete. Nous avons analys\'e la structure des dialogues ainsi obtenus et avons \'etudi\'e un certain nombre d'indices discursifs : vocabulaire employ\'e, marques de reformulation, commentaires m\'eta et \'epilinguistiques, expression implicite ou explicite des intentions de l'utilisateur, encha\^inement conversationnel, etc. De cette analyse, nous avons construit un mod\`ele d'agent artificiel dot\'e de capacit\'es cognitives capables d'aider l'utilisateur dans sa t\^ache de recherche d'information. Ce mod\`ele a \'et\'e impl\'ement\'e et int\'egr\'e dans le syst\`eme CISMeF. version:1
arxiv-1307-1954 | B-tests: Low Variance Kernel Two-Sample Tests | http://arxiv.org/abs/1307.1954 | id:1307.1954 author:Wojciech Zaremba, Arthur Gretton, Matthew Blaschko category:cs.LG stat.ML  published:2013-07-08 summary:A family of maximum mean discrepancy (MMD) kernel two-sample tests is introduced. Members of the test family are called Block-tests or B-tests, since the test statistic is an average over MMDs computed on subsets of the samples. The choice of block size allows control over the tradeoff between test power and computation time. In this respect, the $B$-test family combines favorable properties of previously proposed MMD two-sample tests: B-tests are more powerful than a linear time test where blocks are just pairs of samples, yet they are more computationally efficient than a quadratic time test where a single large block incorporating all the samples is used to compute a U-statistic. A further important advantage of the B-tests is their asymptotically Normal null distribution: this is by contrast with the U-statistic, which is degenerate under the null hypothesis, and for which estimates of the null distribution are computationally demanding. Recent results on kernel selection for hypothesis testing transfer seamlessly to the B-tests, yielding a means to optimize test power via kernel choice. version:3
arxiv-1302-5186 | Unsupervised edge map scoring: a statistical complexity approach | http://arxiv.org/abs/1302.5186 | id:1302.5186 author:Javier Gimenez, Jorge Martinez, Ana Georgina Flesia category:cs.CV stat.AP  published:2013-02-21 summary:We propose a new Statistical Complexity Measure (SCM) to qualify edge maps without Ground Truth (GT) knowledge. The measure is the product of two indices, an \emph{Equilibrium} index $\mathcal{E}$ obtained by projecting the edge map into a family of edge patterns, and an \emph{Entropy} index $\mathcal{H}$, defined as a function of the Kolmogorov Smirnov (KS) statistic. This new measure can be used for performance characterization which includes: (i)~the specific evaluation of an algorithm (intra-technique process) in order to identify its best parameters, and (ii)~the comparison of different algorithms (inter-technique process) in order to classify them according to their quality. Results made over images of the South Florida and Berkeley databases show that our approach significantly improves over Pratt's Figure of Merit (PFoM) which is the objective reference-based edge map evaluation standard, as it takes into account more features in its evaluation. version:2
arxiv-1402-2232 | Image Search Reranking | http://arxiv.org/abs/1402.2232 | id:1402.2232 author:V Rajakumar, Vipeen V Bopche category:cs.IR cs.CV  published:2014-02-10 summary:The existing methods for image search reranking suffer from the unfaithfulness of the assumptions under which the text-based images search result. The resulting images contain more irrelevant images. Hence the re ranking concept arises to re rank the retrieved images based on the text around the image and data of data of image and visual feature of image. A number of methods are differentiated for this re-ranking. The high ranked images are used as noisy data and a k means algorithm for classification is learned to rectify the ranking further. We are study the affect ability of the cross validation method to this training data. The pre eminent originality of the overall method is in collecting text/metadata of image and visual features in order to achieve an automatic ranking of the images. Supervision is initiated to learn the model weights offline, previous to reranking process. While model learning needs manual labeling of the results for a some limited queries, the resulting model is query autonomous and therefore applicable to any other query .Examples are given for a selection of other classes like vehicles, animals and other classes. version:1
arxiv-1402-2224 | Characterizing the Sample Complexity of Private Learners | http://arxiv.org/abs/1402.2224 | id:1402.2224 author:Amos Beimel, Kobbi Nissim, Uri Stemmer category:cs.CR cs.LG  published:2014-02-10 summary:In 2008, Kasiviswanathan et al. defined private learning as a combination of PAC learning and differential privacy. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. gave a generic construction of private learners for (finite) concept classes, with sample complexity logarithmic in the size of the concept class. This sample complexity is higher than what is needed for non-private learners, hence leaving open the possibility that the sample complexity of private learning may be sometimes significantly higher than that of non-private learning. We give a combinatorial characterization of the sample size sufficient and necessary to privately learn a class of concepts. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure RepDim corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with sample complexity m implies RepDim(C)=O(m), and that there exists a private learning algorithm with sample complexity m=O(RepDim(C)). We further demonstrate that a similar characterization holds for the database size needed for privately computing a large class of optimization problems and also for the well studied problem of private data release. version:1
arxiv-1402-2188 | Handwritten Character Recognition In Malayalam Scripts- A Review | http://arxiv.org/abs/1402.2188 | id:1402.2188 author:Anitha Mary M. O. Chacko, P. M Dhanya category:cs.CV  published:2014-02-10 summary:Handwritten character recognition is one of the most challenging and ongoing areas of research in the field of pattern recognition. HCR research is matured for foreign languages like Chinese and Japanese but the problem is much more complex for Indian languages. The problem becomes even more complicated for South Indian languages due to its large character set and the presence of vowels modifiers and compound characters. This paper provides an overview of important contributions and advances in offline as well as online handwritten character recognition of Malayalam scripts. version:1
arxiv-1402-2148 | An Algorithmic Framework for Computing Validation Performance Bounds by Using Suboptimal Models | http://arxiv.org/abs/1402.2148 | id:1402.2148 author:Yoshiki Suzuki, Kohei Ogawa, Yuki Shinmura, Ichiro Takeuchi category:stat.ML  published:2014-02-10 summary:Practical model building processes are often time-consuming because many different models must be trained and validated. In this paper, we introduce a novel algorithm that can be used for computing the lower and the upper bounds of model validation errors without actually training the model itself. A key idea behind our algorithm is using a side information available from a suboptimal model. If a reasonably good suboptimal model is available, our algorithm can compute lower and upper bounds of many useful quantities for making inferences on the unknown target model. We demonstrate the advantage of our algorithm in the context of model selection for regularized learning problems. version:1
arxiv-1306-3162 | Learning to encode motion using spatio-temporal synchrony | http://arxiv.org/abs/1306.3162 | id:1306.3162 author:Kishore Reddy Konda, Roland Memisevic, Vincent Michalski category:cs.CV cs.LG stat.ML  published:2013-06-13 summary:We consider the task of learning to extract motion from videos. To this end, we show that the detection of spatial transformations can be viewed as the detection of synchrony between the image sequence and a sequence of features undergoing the motion we wish to detect. We show that learning about synchrony is possible using very fast, local learning rules, by introducing multiplicative "gating" interactions between hidden units across frames. This makes it possible to achieve competitive performance in a wide variety of motion estimation tasks, using a small fraction of the time required to learn features, and to outperform hand-crafted spatio-temporal features by a large margin. We also show how learning about synchrony can be viewed as performing greedy parameter estimation in the well-known motion energy model. version:3
arxiv-1402-2088 | Signal Reconstruction Framework Based On Projections Onto Epigraph Set Of A Convex Cost Function (PESC) | http://arxiv.org/abs/1402.2088 | id:1402.2088 author:Mohammad Tofighi, Kivanc Kose, A. Enis Cetin category:math.OC cs.CV  published:2014-02-10 summary:A new signal processing framework based on making orthogonal Projections onto the Epigraph Set of a Convex cost function (PESC) is developed. In this way it is possible to solve convex optimization problems using the well-known Projections onto Convex Set (POCS) approach. In this algorithm, the dimension of the minimization problem is lifted by one and a convex set corresponding to the epigraph of the cost function is defined. If the cost function is a convex function in $R^N$, the corresponding epigraph set is also a convex set in R^{N+1}. The PESC method provides globally optimal solutions for total-variation (TV), filtered variation (FV), L_1, L_2, and entropic cost function based convex optimization problems. In this article, the PESC based denoising and compressive sensing algorithms are developed. Simulation examples are presented. version:1
arxiv-1107-3059 | From Small-World Networks to Comparison-Based Search | http://arxiv.org/abs/1107.3059 | id:1107.3059 author:Amin Karbasi, Stratis Ioannidis, Laurent Massoulie category:cs.LG cs.DS cs.IT cs.SI math.IT stat.ML  published:2011-07-15 summary:The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner: the user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates. This problem is known to be strongly related to the small-world network design problem. However, contrary to prior work, which focuses on cases where objects in the database are equally popular, we consider here the case where the demand for objects may be heterogeneous. We show that, under heterogeneous demand, the small-world network design problem is NP-hard. Given the above negative result, we propose a novel mechanism for small-world design and provide an upper bound on its performance under heterogeneous demand. The above mechanism has a natural equivalent in the context of content search through comparisons, and we establish both an upper bound and a lower bound for the performance of this mechanism. These bounds are intuitively appealing, as they depend on the entropy of the demand as well as its doubling constant, a quantity capturing the topology of the set of target objects. They also illustrate interesting connections between comparison-based search to classic results from information theory. Finally, we propose an adaptive learning algorithm for content search that meets the performance guarantees achieved by the above mechanisms. version:3
arxiv-1402-2044 | A Second-order Bound with Excess Losses | http://arxiv.org/abs/1402.2044 | id:1402.2044 author:Pierre Gaillard, Gilles Stoltz, Tim Van Erven category:stat.ML cs.LG math.ST stat.TH  published:2014-02-10 summary:We study online aggregation of the predictions of experts, and first show new second-order regret bounds in the standard setting, which are obtained via a version of the Prod algorithm (and also a version of the polynomially weighted average algorithm) with multiple learning rates. These bounds are in terms of excess losses, the differences between the instantaneous losses suffered by the algorithm and the ones of a given expert. We then demonstrate the interest of these bounds in the context of experts that report their confidences as a number in the interval [0,1] using a generic reduction to the standard setting. We conclude by two other applications in the standard setting, which improve the known bounds in case of small excess losses and show a bounded regret against i.i.d. sequences of losses. version:1
arxiv-1402-2043 | Approachability in unknown games: Online learning meets multi-objective optimization | http://arxiv.org/abs/1402.2043 | id:1402.2043 author:Shie Mannor, Vianney Perchet, Gilles Stoltz category:stat.ML cs.LG math.ST stat.TH  published:2014-02-10 summary:In the standard setting of approachability there are two players and a target set. The players play repeatedly a known vector-valued game where the first player wants to have the average vector-valued payoff converge to the target set which the other player tries to exclude it from this set. We revisit this setting in the spirit of online learning and do not assume that the first player knows the game structure: she receives an arbitrary vector-valued reward vector at every round. She wishes to approach the smallest ("best") possible set given the observed average payoffs in hindsight. This extension of the standard setting has implications even when the original target set is not approachable and when it is not obvious which expansion of it should be approached instead. We show that it is impossible, in general, to approach the best target set in hindsight and propose achievable though ambitious alternative goals. We further propose a concrete strategy to approach these goals. Our method does not require projection onto a target set and amounts to switching between scalar regret minimization algorithms that are performed in episodes. Applications to global cost minimization and to approachability under sample path constraints are considered. version:2
arxiv-1402-2031 | Deeply Coupled Auto-encoder Networks for Cross-view Classification | http://arxiv.org/abs/1402.2031 | id:1402.2031 author:Wen Wang, Zhen Cui, Hong Chang, Shiguang Shan, Xilin Chen category:cs.CV cs.LG cs.NE  published:2014-02-10 summary:The comparison of heterogeneous samples extensively exists in many applications, especially in the task of image classification. In this paper, we propose a simple but effective coupled neural network, called Deeply Coupled Autoencoder Networks (DCAN), which seeks to build two deep neural networks, coupled with each other in every corresponding layers. In DCAN, each deep structure is developed via stacking multiple discriminative coupled auto-encoders, a denoising auto-encoder trained with maximum margin criterion consisting of intra-class compactness and inter-class penalty. This single layer component makes our model simultaneously preserve the local consistency and enhance its discriminative capability. With increasing number of layers, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-view image classification tasks demonstrate the superiority of our method over state-of-the-art methods. version:1
arxiv-1402-2026 | Genomic Prediction of Quantitative Traits using Sparse and Locally Epistatic Models | http://arxiv.org/abs/1402.2026 | id:1402.2026 author:Deniz Akdemir category:stat.AP stat.ML  published:2014-02-10 summary:In plant and animal breeding studies a distinction is made between the genetic value (additive + epistatic genetic effects) and the breeding value (additive genetic effects) of an individual since it is expected that some of the epistatic genetic effects will be lost due to recombination. In this paper, we argue that the breeder can take advantage of some of the epistatic marker effects in regions of low recombination. The models introduced here aim to estimate local epistatic line heritability by using the genetic map information and combine the local additive and epistatic effects. To this end, we have used semi-parametric mixed models with multiple local genomic relationship matrices with hierarchical designs and lasso post-processing for sparsity in the final model. Our models produce good predictive performance along with good explanatory information. version:1
arxiv-1402-2020 | Binary Stereo Matching | http://arxiv.org/abs/1402.2020 | id:1402.2020 author:Kang Zhang, Jiyang Li, Yijing Li, Weidong Hu, Lifeng Sun, Shiqiang Yang category:cs.CV  published:2014-02-10 summary:In this paper, we propose a novel binary-based cost computation and aggregation approach for stereo matching problem. The cost volume is constructed through bitwise operations on a series of binary strings. Then this approach is combined with traditional winner-take-all strategy, resulting in a new local stereo matching algorithm called binary stereo matching (BSM). Since core algorithm of BSM is based on binary and integer computations, it has a higher computational efficiency than previous methods. Experimental results on Middlebury benchmark show that BSM has comparable performance with state-of-the-art local stereo methods in terms of both quality and speed. Furthermore, experiments on images with radiometric differences demonstrate that BSM is more robust than previous methods under these changes, which is common under real illumination. version:1
arxiv-1402-2013 | Foreground segmentation based on multi-resolution and matting | http://arxiv.org/abs/1402.2013 | id:1402.2013 author:Xintong Yu, Xiaohan Liu, Yisong Chen category:cs.CV  published:2014-02-10 summary:We propose a foreground segmentation algorithm that does foreground extraction under different scales and refines the result by matting. First, the input image is filtered and resampled to 5 different resolutions. Then each of them is segmented by adaptive figure-ground classification and the best segmentation is automatically selected by an evaluation score that maximizes the difference between foreground and background. This segmentation is upsampled to the original size, and a corresponding trimap is built. Closed-form matting is employed to label the boundary region, and the result is refined by a final figure-ground classification. Experiments show the success of our method in treating challenging images with cluttered background and adapting to loose initial bounding-box. version:1
arxiv-1402-1958 | Better Optimism By Bayes: Adaptive Planning with Rich Models | http://arxiv.org/abs/1402.1958 | id:1402.1958 author:Arthur Guez, David Silver, Peter Dayan category:cs.AI cs.LG stat.ML  published:2014-02-09 summary:The computational costs of inference and planning have confined Bayesian model-based reinforcement learning to one of two dismal fates: powerful Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian non-parametric models but using simple, myopic planning strategies such as Thompson sampling. We ask whether it is feasible and truly beneficial to combine rich probabilistic models with a closer approximation to fully Bayesian planning. First, we use a collection of counterexamples to show formal problems with the over-optimism inherent in Thompson sampling. Then we leverage state-of-the-art techniques in efficient Bayes-adaptive planning and non-parametric Bayesian methods to perform qualitatively better than both existing conventional algorithms and Thompson sampling on two contextual bandit-like problems. version:1
arxiv-1402-1947 | Classification Tree Diagrams in Health Informatics Applications | http://arxiv.org/abs/1402.1947 | id:1402.1947 author:Farrukh Arslan category:cs.IR cs.CV cs.LG  published:2014-02-09 summary:Health informatics deal with the methods used to optimize the acquisition, storage and retrieval of medical data, and classify information in healthcare applications. Healthcare analysts are particularly interested in various computer informatics areas such as; knowledge representation from data, anomaly detection, outbreak detection methods and syndromic surveillance applications. Although various parametric and non-parametric approaches are being proposed to classify information from data, classification tree diagrams provide an interactive visualization to analysts as compared to other methods. In this work we discuss application of classification tree diagrams to classify information from medical data in healthcare applications. version:1
arxiv-1402-1931 | MCA Learning Algorithm for Incident Signals Estimation: A Review | http://arxiv.org/abs/1402.1931 | id:1402.1931 author:Rashid Ahmed, John A. Avaritsiotis category:cs.NE  published:2014-02-09 summary:Recently there has been many works on adaptive subspace filtering in the signal processing literature. Most of them are concerned with tracking the signal subspace spanned by the eigenvectors corresponding to the eigenvalues of the covariance matrix of the signal plus noise data. Minor Component Analysis (MCA) is important tool and has a wide application in telecommunications, antenna array processing, statistical parametric estimation, etc. As an important feature extraction technique, MCA is a statistical method of extracting the eigenvector associated with the smallest eigenvalue of the covariance matrix. In this paper, we will present a MCA learning algorithm to extract minor component from input signals, and the learning rate parameter is also presented, which ensures fast convergence of the algorithm, because it has direct effect on the convergence of the weight vector and the error level is affected by this value. MCA is performed to determine the estimated DOA. Simulation results will be furnished to illustrate the theoretical results achieved. version:1
arxiv-1402-1921 | A Hybrid Loss for Multiclass and Structured Prediction | http://arxiv.org/abs/1402.1921 | id:1402.1921 author:Qinfeng Shi, Mark Reid, Tiberio Caetano, Anton van den Hengel, Zhenhua Wang category:cs.LG cs.AI cs.CV  published:2014-02-09 summary:We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of a log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels--specifically, the gap between the probabilities of the best label and the second best label. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs. We demonstrate empirically that the hybrid loss typically performs least as well as--and often better than--both of its constituent losses on a variety of tasks, such as human action recognition. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction. version:1
arxiv-1302-3283 | StructBoost: Boosting Methods for Predicting Structured Output Variables | http://arxiv.org/abs/1302.3283 | id:1302.3283 author:Chunhua Shen, Guosheng Lin, Anton van den Hengel category:cs.LG  published:2013-02-14 summary:Boosting is a method for learning a single accurate predictor by linearly combining a set of less accurate weak learners. Recently, structured learning has found many applications in computer vision. Inspired by structured support vector machines (SSVM), here we propose a new boosting algorithm for structured output prediction, which we refer to as StructBoost. StructBoost supports nonlinear structured learning by combining a set of weak structured learners. As SSVM generalizes SVM, our StructBoost generalizes standard boosting approaches such as AdaBoost, or LPBoost to structured learning. The resulting optimization problem of StructBoost is more challenging than SSVM in the sense that it may involve exponentially many variables and constraints. In contrast, for SSVM one usually has an exponential number of constraints and a cutting-plane method is used. In order to efficiently solve StructBoost, we formulate an equivalent $ 1 $-slack formulation and solve it using a combination of cutting planes and column generation. We show the versatility and usefulness of StructBoost on a range of problems such as optimizing the tree loss for hierarchical multi-class classification, optimizing the Pascal overlap criterion for robust visual tracking and learning conditional random field parameters for image segmentation. version:3
arxiv-1312-7469 | Collaborative Discriminant Locality Preserving Projections With its Application to Face Recognition | http://arxiv.org/abs/1312.7469 | id:1312.7469 author:Sheng Huang, Dan Yang, Dong Yang, Ahmed Elgammal category:cs.CV  published:2013-12-28 summary:We present a novel Discriminant Locality Preserving Projections (DLPP) algorithm named Collaborative Discriminant Locality Preserving Projection (CDLPP). In our algorithm, the discriminating power of DLPP are further exploited from two aspects. On the one hand, the global optimum of class scattering is guaranteed via using the between-class scatter matrix to replace the original denominator of DLPP. On the other hand, motivated by collaborative representation, an $L_2$-norm constraint is imposed to the projections to discover the collaborations of dimensions in the sample space. We apply our algorithm to face recognition. Three popular face databases, namely AR, ORL and LFW-A, are employed for evaluating the performance of CDLPP. Extensive experimental results demonstrate that CDLPP significantly improves the discriminating power of DLPP and outperforms the state-of-the-arts. version:2
arxiv-1402-1879 | Sparse Illumination Learning and Transfer for Single-Sample Face Recognition with Image Corruption and Misalignment | http://arxiv.org/abs/1402.1879 | id:1402.1879 author:Liansheng Zhuang, Tsung-Han Chan, Allen Y. Yang, S. Shankar Sastry, Yi Ma category:cs.CV  published:2014-02-08 summary:Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required gallery images to one sample per class. To compensate for the missing illumination information traditionally provided by multiple gallery images, a sparse illumination learning and transfer (SILT) technique is introduced. The illumination in SILT is learned by fitting illumination examples of auxiliary face images from one or more additional subjects with a sparsely-used illumination dictionary. By enforcing a sparse representation of the query image in the illumination dictionary, the SILT can effectively recover and transfer the illumination and pose information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the state of the art in the single-sample regime and with less restrictions. In particular, the single-sample face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple gallery images per class. Furthermore, the face recognition accuracy exceeds those of the SRC and Extended SRC algorithms using hand labeled alignment initialization. version:1
arxiv-1401-4566 | Excess Risk Bounds for Exponentially Concave Losses | http://arxiv.org/abs/1401.4566 | id:1401.4566 author:Mehrdad Mahdavi, Rong Jin category:cs.LG stat.ML  published:2014-01-18 summary:The overarching goal of this paper is to derive excess risk bounds for learning from exp-concave loss functions in passive and sequential learning settings. Exp-concave loss functions encompass several fundamental problems in machine learning such as squared loss in linear regression, logistic loss in classification, and negative logarithm loss in portfolio management. In batch setting, we obtain sharp bounds on the performance of empirical risk minimization performed in a linear hypothesis space and with respect to the exp-concave loss functions. We also extend the results to the online setting where the learner receives the training examples in a sequential manner. We propose an online learning algorithm that is a properly modified version of online Newton method to obtain sharp risk bounds. Under an additional mild assumption on the loss function, we show that in both settings we are able to achieve an excess risk bound of $O(d\log n/n)$ that holds with a high probability. version:2
arxiv-1402-1801 | Efficient Low Dose X-ray CT Reconstruction through Sparsity-Based MAP Modeling | http://arxiv.org/abs/1402.1801 | id:1402.1801 author:SayedMasoud Hashemi, Soosan Beheshti, Patrick R. Gill, Narinder S. Paul, Richard S. C. Cobbold category:stat.AP cs.CV  published:2014-02-08 summary:Ultra low radiation dose in X-ray Computed Tomography (CT) is an important clinical objective in order to minimize the risk of carcinogenesis. Compressed Sensing (CS) enables significant reductions in radiation dose to be achieved by producing diagnostic images from a limited number of CT projections. However, the excessive computation time that conventional CS-based CT reconstruction typically requires has limited clinical implementation. In this paper, we first demonstrate that a thorough analysis of CT reconstruction through a Maximum a Posteriori objective function results in a weighted compressive sensing problem. This analysis enables us to formulate a low dose fan beam and helical cone beam CT reconstruction. Subsequently, we provide an efficient solution to the formulated CS problem based on a Fast Composite Splitting Algorithm-Latent Expected Maximization (FCSA-LEM) algorithm. In the proposed method we use pseudo polar Fourier transform as the measurement matrix in order to decrease the computational complexity; and rebinning of the projections to parallel rays in order to extend its application to fan beam and helical cone beam scans. The weight involved in the proposed weighted CS model, denoted by Error Adaptation Weight (EAW), is calculated based on the statistical characteristics of CT reconstruction and is a function of Poisson measurement noise and rebinning interpolation error. Simulation results show that low computational complexity of the proposed method made the fast recovery of the CT images possible and using EAW reduces the reconstruction error by one order of magnitude. Recovery of a high quality 512$\times$ 512 image was achieved in less than 20 sec on a desktop computer without numerical optimizations. version:1
arxiv-1402-1792 | Binary Excess Risk for Smooth Convex Surrogates | http://arxiv.org/abs/1402.1792 | id:1402.1792 author:Mehrdad Mahdavi, Lijun Zhang, Rong Jin category:cs.LG stat.ML  published:2014-02-07 summary:In statistical learning theory, convex surrogates of the 0-1 loss are highly preferred because of the computational and theoretical virtues that convexity brings in. This is of more importance if we consider smooth surrogates as witnessed by the fact that the smoothness is further beneficial both computationally- by attaining an {\it optimal} convergence rate for optimization, and in a statistical sense- by providing an improved {\it optimistic} rate for generalization bound. In this paper we investigate the smoothness property from the viewpoint of statistical consistency and show how it affects the binary excess risk. We show that in contrast to optimization and generalization errors that favor the choice of smooth surrogate loss, the smoothness of loss function may degrade the binary excess risk. Motivated by this negative result, we provide a unified analysis that integrates optimization error, generalization bound, and the error in translating convex excess risk into a binary excess risk when examining the impact of smoothness on the binary excess risk. We show that under favorable conditions appropriate choice of smooth convex loss will result in a binary excess risk that is better than $O(1/\sqrt{n})$. version:1
arxiv-1312-1858 | How Santa Fe Ants Evolve | http://arxiv.org/abs/1312.1858 | id:1312.1858 author:Dominic Wilson, Devinder Kaur category:cs.NE  published:2013-12-06 summary:The Santa Fe Ant model problem has been extensively used to investigate, test and evaluate Evolutionary Computing systems and methods over the past two decades. There is however no literature on its program structures that are systematically used for fitness improvement, the geometries of those structures and their dynamics during optimization. This paper analyzes the Santa Fe Ant Problem using a new phenotypic schema and landscape analysis based on executed instruction sequences. For the first time we detail systematic structural features that give high fitness and the evolutionary dynamics of such structures. The new schema avoids variances due to introns. We develop a phenotypic variation method that tests the new understanding of the landscape. We also develop a modified function set that tests newly identified synchronization constraints. We obtain favorable computational efforts compared to those in the literature, on testing the new variation and function set on both the Santa Fe Trail, and the more computationally demanding Los Altos Trail. Our findings suggest that for the Santa Fe Ant problem, a perspective of program assembly from repetition of highly fit responses to trail conditions leads to better analysis and performance. version:2
arxiv-1310-3745 | Alternating Minimization for Mixed Linear Regression | http://arxiv.org/abs/1310.3745 | id:1310.3745 author:Xinyang Yi, Constantine Caramanis, Sujay Sanghavi category:stat.ML  published:2013-10-14 summary:Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels). In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EM's performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem. version:2
arxiv-1402-1720 | Performance of Hull-Detection Algorithms For Proton Computed Tomography Reconstruction | http://arxiv.org/abs/1402.1720 | id:1402.1720 author:Blake Schultze, Micah Witt, Yair Censor, Reinhard Schulte, Keith Evan Schubert category:cs.CV physics.med-ph  published:2014-02-07 summary:Proton computed tomography (pCT) is a novel imaging modality developed for patients receiving proton radiation therapy. The purpose of this work was to investigate hull-detection algorithms used for preconditioning of the large and sparse linear system of equations that needs to be solved for pCT image reconstruction. The hull-detection algorithms investigated here included silhouette/space carving (SC), modified silhouette/space carving (MSC), and space modeling (SM). Each was compared to the cone-beam version of filtered backprojection (FBP) used for hull-detection. Data for testing these algorithms included simulated data sets of a digital head phantom and an experimental data set of a pediatric head phantom obtained with a pCT scanner prototype at Loma Linda University Medical Center. SC was the fastest algorithm, exceeding the speed of FBP by more than 100 times. FBP was most sensitive to the presence of noise. Ongoing work will focus on optimizing threshold parameters in order to define a fast and efficient method for hull-detection in pCT image reconstruction. version:1
arxiv-1402-1700 | On the Prediction Performance of the Lasso | http://arxiv.org/abs/1402.1700 | id:1402.1700 author:Arnak S. Dalalyan, Mohamed Hebiri, Johannes Lederer category:math.ST stat.ML stat.TH  published:2014-02-07 summary:Although the Lasso has been extensively studied, the relationship between its prediction performance and the correlations of the covariates is not fully understood. In this paper, we give new insights into this relationship in the context of multiple linear regression. We show, in particular, that the incorporation of a simple correlation measure into the tuning parameter leads to a nearly optimal prediction performance of the Lasso even for highly correlated covariates. However, we also reveal that for moderately correlated covariates, the prediction performance of the Lasso can be mediocre irrespective of the choice of the tuning parameter. For the illustration of our approach with an important application, we deduce nearly optimal rates for the least-squares estimator with total variation penalty. version:1
arxiv-1307-0802 | A Statistical Learning Theory Framework for Supervised Pattern Discovery | http://arxiv.org/abs/1307.0802 | id:1307.0802 author:Jonathan H. Huggins, Cynthia Rudin category:stat.ML cs.AI  published:2013-07-02 summary:This paper formalizes a latent variable inference problem we call {\em supervised pattern discovery}, the goal of which is to find sets of observations that belong to a single ``pattern.'' We discuss two versions of the problem and prove uniform risk bounds for both. In the first version, collections of patterns can be generated in an arbitrary manner and the data consist of multiple labeled collections. In the second version, the patterns are assumed to be generated independently by identically distributed processes. These processes are allowed to take an arbitrary form, so observations within a pattern are not in general independent of each other. The bounds for the second version of the problem are stated in terms of a new complexity measure, the quasi-Rademacher complexity. version:2
arxiv-1402-1668 | Evaluation of YTEX and MetaMap for clinical concept recognition | http://arxiv.org/abs/1402.1668 | id:1402.1668 author:John David Osborne, Binod Gyawali, Thamar Solorio category:cs.IR cs.CL 68  published:2014-02-07 summary:We used MetaMap and YTEX as a basis for the construc- tion of two separate systems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the recognition of clinical concepts. No modifications were directly made to these systems, but output concepts were filtered using stop concepts, stop concept text and UMLS semantic type. Con- cept boundaries were also adjusted using a small collection of rules to increase precision on the strict task. Overall MetaMap had better per- formance than YTEX on the strict task, primarily due to a 20% perfor- mance improvement in precision. In the relaxed task YTEX had better performance in both precision and recall giving it an overall F-Score 4.6% higher than MetaMap on the test data. Our results also indicated a 1.3% higher accuracy for YTEX in UMLS CUI mapping. version:1
arxiv-1401-3607 | A Brief History of Learning Classifier Systems: From CS-1 to XCS | http://arxiv.org/abs/1401.3607 | id:1401.3607 author:Larry Bull category:cs.NE cs.LG  published:2014-01-15 summary:Modern Learning Classifier Systems can be characterized by their use of rule accuracy as the utility metric for the search algorithm(s) discovering useful rules. Such searching typically takes place within the restricted space of co-active rules for efficiency. This paper gives an historical overview of the evolution of such systems up to XCS, and then some of the subsequent developments of XCS to different types of learning. version:2
arxiv-1402-1503 | Tracking via Motion Estimation with Physically Motivated Inter-Region Constraints | http://arxiv.org/abs/1402.1503 | id:1402.1503 author:Omar Arif, Ganesh Sundaramoorthi, Byung-Woo Hong, Anthony Yezzi category:cs.CV  published:2014-02-06 summary:In this paper, we propose a method for tracking structures (e.g., ventricles and myocardium) in cardiac images (e.g., magnetic resonance) by propagating forward in time a previous estimate of the structures via a new deformation estimation scheme that is motivated by physical constraints of fluid motion. The method employs within structure motion estimation (so that differing motions among different structures are not mixed) while simultaneously satisfying the physical constraint in fluid motion that at the interface between a fluid and a medium, the normal component of the fluid's motion must match the normal component of the motion of the medium. We show how to estimate the motion according to the previous considerations in a variational framework, and in particular, show that these conditions lead to PDEs with boundary conditions at the interface that resemble Robin boundary conditions and induce coupling between structures. We illustrate the use of this motion estimation scheme in propagating a segmentation across frames and show that it leads to more accurate segmentation than traditional motion estimation that does not make use of physical constraints. Further, the method is naturally suited to interactive segmentation methods, which are prominently used in practice in commercial applications for cardiac analysis, where typically a segmentation from the previous frame is used to predict a segmentation in the next frame. We show that our propagation scheme reduces the amount of user interaction by predicting more accurate segmentations than commonly used and recent interactive commercial techniques. version:1
arxiv-1402-1473 | Near-Optimal Joint Object Matching via Convex Relaxation | http://arxiv.org/abs/1402.1473 | id:1402.1473 author:Yuxin Chen, Leonidas J. Guibas, Qi-Xing Huang category:cs.LG cs.CV cs.IT math.IT math.OC stat.ML  published:2014-02-06 summary:Joint matching over a collection of objects aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve maps between pairs of them. Given multiple matches computed between a few object pairs in isolation, the goal is to recover an entire collection of maps that are (1) globally consistent, and (2) close to the provided maps --- and under certain conditions provably the ground-truth maps. Despite recent advances on this problem, the best-known recovery guarantees are limited to a small constant barrier --- none of the existing methods find theoretical support when more than $50\%$ of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding to match instances that are only partially similar to each other. In this paper, we develop an algorithm to jointly match multiple objects that exhibit only partial similarities, given a few pairwise matches that are densely corrupted. Specifically, we propose to recover the ground-truth maps via a parameter-free convex program called MatchLift, following a spectral method that pre-estimates the total number of distinct elements to be matched. Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e. in the asymptotic regime it is guaranteed to work even when a dominant fraction $1-\Theta\left(\frac{\log^{2}n}{\sqrt{n}}\right)$ of the input maps behave like random outliers. Furthermore, MatchLift succeeds with minimal input complexity, namely, perfect matching can be achieved as soon as the provided maps form a connected map graph. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability of MatchLift. version:1
arxiv-1311-4803 | Beating the Minimax Rate of Active Learning with Prior Knowledge | http://arxiv.org/abs/1311.4803 | id:1311.4803 author:Lijun Zhang, Mehrdad Mahdavi, Rong Jin category:cs.LG stat.ML  published:2013-11-19 summary:Active learning refers to the learning protocol where the learner is allowed to choose a subset of instances for labeling. Previous studies have shown that, compared with passive learning, active learning is able to reduce the label complexity exponentially if the data are linearly separable or satisfy the Tsybakov noise condition with parameter $\kappa=1$. In this paper, we propose a novel active learning algorithm using a convex surrogate loss, with the goal to broaden the cases for which active learning achieves an exponential improvement. We make use of a convex loss not only because it reduces the computational cost, but more importantly because it leads to a tight bound for the empirical process (i.e., the difference between the empirical estimation and the expectation) when the current solution is close to the optimal one. Under the assumption that the norm of the optimal classifier that minimizes the convex risk is available, our analysis shows that the introduction of the convex surrogate loss yields an exponential reduction in the label complexity even when the parameter $\kappa$ of the Tsybakov noise is larger than $1$. To the best of our knowledge, this is the first work that improves the minimax rate of active learning by utilizing certain priori knowledge. version:2
arxiv-1402-1454 | An Autoencoder Approach to Learning Bilingual Word Representations | http://arxiv.org/abs/1402.1454 | id:1402.1454 author:Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M. Khapra, Balaraman Ravindran, Vikas Raykar, Amrita Saha category:cs.CL cs.LG stat.ML  published:2014-02-06 summary:Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. Since training autoencoders on word observations presents certain computational issues, we propose and compare different variations adapted to this setting. We also propose an explicit correlation maximizing regularizer that leads to significant improvement in the performance. We empirically investigate the success of our approach on the problem of cross-language test classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). These experiments demonstrate that our approaches are competitive with the state-of-the-art, achieving up to 10-14 percentage point improvements over the best reported results on this task. version:1
arxiv-1402-1379 | A Three-Phase Search Approach for the Quadratic Minimum Spanning Tree Problem | http://arxiv.org/abs/1402.1379 | id:1402.1379 author:Zhang-Hua Fu, Jin-Kao Hao category:cs.DS cs.NE  published:2014-02-06 summary:Given an undirected graph with costs associated with each edge as well as each pair of edges, the quadratic minimum spanning tree problem (QMSTP) consists of determining a spanning tree of minimum total cost. This problem can be used to model many real-life network design applications, in which both routing and interference costs should be considered. For this problem, we propose a three-phase search approach named TPS, which integrates 1) a descent-based neighborhood search phase using two different move operators to reach a local optimum from a given starting solution, 2) a local optima exploring phase to discover nearby local optima within a given regional search area, and 3) a perturbation-based diversification phase to jump out of the current regional search area. Additionally, we introduce dedicated techniques to reduce the neighborhood to explore and streamline the neighborhood evaluations. Computational experiments based on hundreds of representative benchmarks show that TPS produces highly competitive results with respect to the best performing approaches in the literature by improving the best known results for 31 instances and matching the best known results for the remaining instances only except two cases. Critical elements of the proposed algorithms are analyzed. version:1
arxiv-1402-1371 | Quantile Representation for Indirect Immunofluorescence Image Classification | http://arxiv.org/abs/1402.1371 | id:1402.1371 author:David M. J. Tax, Veronika Cheplygina, Marco Loog category:cs.CV  published:2014-02-06 summary:In the diagnosis of autoimmune diseases, an important task is to classify images of slides containing several HEp-2 cells. All cells from one slide share the same label, and by classifying cells from one slide independently, some information on the global image quality and intensity is lost. Considering one whole slide as a collection (a bag) of feature vectors, however, poses the problem of how to handle this bag. A simple, and surprisingly effective, approach is to summarize the bag of feature vectors by a few quantile values per feature. This characterizes the full distribution of all instances, thereby assuming that all instances in a bag are informative. This representation is particularly useful when each bag contains many feature vectors, which is the case in the classification of the immunofluorescence images. Experiments on the classification of indirect immunofluorescence images show the usefulness of this approach. version:1
arxiv-1402-1359 | Real-time Pedestrian Surveillance with Top View Cumulative Grids | http://arxiv.org/abs/1402.1359 | id:1402.1359 author:Kai Berger, Jeyarajan Thiyagalingam category:cs.CV  published:2014-02-06 summary:This manuscript presents an efficient approach to map pedestrian surveillance footage to an aerial view for global assessment of features. The analysis of the footages relies on low level computer vision and enable real-time surveillance. While we neglect object tracking, we introduce cumulative grids on top view scene flow visualization to highlight situations of interest in the footage. Our approach is tested on multiview footage both from RGB cameras and, for the first time in the field, on RGB-D-sensors. version:1
arxiv-1402-1349 | Dissimilarity-based Ensembles for Multiple Instance Learning | http://arxiv.org/abs/1402.1349 | id:1402.1349 author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.LG  published:2014-02-06 summary:In multiple instance learning, objects are sets (bags) of feature vectors (instances) rather than individual feature vectors. In this paper we address the problem of how these bags can best be represented. Two standard approaches are to use (dis)similarities between bags and prototype bags, or between bags and prototype instances. The first approach results in a relatively low-dimensional representation determined by the number of training bags, while the second approach results in a relatively high-dimensional representation, determined by the total number of instances in the training set. In this paper a third, intermediate approach is proposed, which links the two approaches and combines their strengths. Our classifier is inspired by a random subspace ensemble, and considers subspaces of the dissimilarity space, defined by subsets of instances, as prototypes. We provide guidelines for using such an ensemble, and show state-of-the-art performances on a range of multiple instance learning problems. version:1
arxiv-1402-1348 | A Cellular Automata based Optimal Edge Detection Technique using Twenty-Five Neighborhood Model | http://arxiv.org/abs/1402.1348 | id:1402.1348 author:Deepak Ranjan Nayak, Sumit Kumar Sahu, Jahangir Mohammed category:cs.CV  published:2014-02-06 summary:Cellular Automata (CA) are common and most simple models of parallel computations. Edge detection is one of the crucial task in image processing, especially in processing biological and medical images. CA can be successfully applied in image processing. This paper presents a new method for edge detection of binary images based on two dimensional twenty five neighborhood cellular automata. The method considers only linear rules of CA for extraction of edges under null boundary condition. The performance of this approach is compared with some existing edge detection techniques. This comparison shows that the proposed method to be very promising for edge detection of binary images. All the algorithms and results used in this paper are prepared in MATLAB. version:1
arxiv-1312-5604 | Learning Transformations for Classification Forests | http://arxiv.org/abs/1312.5604 | id:1312.5604 author:Qiang Qiu, Guillermo Sapiro category:cs.CV cs.LG stat.ML  published:2013-12-19 summary:This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework. version:2
arxiv-1402-1331 | An Estimation Method of Measuring Image Quality for Compressed Images of Human Face | http://arxiv.org/abs/1402.1331 | id:1402.1331 author:Abhishek Bhattacharya, Tanusree Chatterjee category:cs.CV  published:2014-02-06 summary:Nowadays digital image compression and decompression techniques are very much important. So our aim is to calculate the quality of face and other regions of the compressed image with respect to the original image. Image segmentation is typically used to locate objects and boundaries (lines, curves etc.)in images. After segmentation the image is changed into something which is more meaningful to analyze. Using Universal Image Quality Index(Q),Structural Similarity Index(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be shown that face region is less compressed than any other region of the image. version:1
arxiv-1210-6292 | A density-sensitive hierarchical clustering method | http://arxiv.org/abs/1210.6292 | id:1210.6292 author:Álvaro Martínez-Pérez category:cs.LG 62H30  68T10  published:2012-10-23 summary:We define a hierarchical clustering method: $\alpha$-unchaining single linkage or $SL(\alpha)$. The input of this algorithm is a finite metric space and a certain parameter $\alpha$. This method is sensitive to the density of the distribution and offers some solution to the so called chaining effect. We also define a modified version, $SL^*(\alpha)$, to treat the chaining through points or small blocks. We study the theoretical properties of these methods and offer some theoretical background for the treatment of chaining effects. version:2
arxiv-1402-1263 | Localized epidemic detection in networks with overwhelming noise | http://arxiv.org/abs/1402.1263 | id:1402.1263 author:Eli A. Meirom, Chris Milling, Constantine Caramanis, Shie Mannor, Ariel Orda, Sanjay Shakkottai category:cs.SI cs.LG  published:2014-02-06 summary:We consider the problem of detecting an epidemic in a population where individual diagnoses are extremely noisy. The motivation for this problem is the plethora of examples (influenza strains in humans, or computer viruses in smartphones, etc.) where reliable diagnoses are scarce, but noisy data plentiful. In flu/phone-viruses, exceedingly few infected people/phones are professionally diagnosed (only a small fraction go to a doctor) but less reliable secondary signatures (e.g., people staying home, or greater-than-typical upload activity) are more readily available. These secondary data are often plagued by unreliability: many people with the flu do not stay home, and many people that stay home do not have the flu. This paper identifies the precise regime where knowledge of the contact network enables finding the needle in the haystack: we provide a distributed, efficient and robust algorithm that can correctly identify the existence of a spreading epidemic from highly unreliable local data. Our algorithm requires only local-neighbor knowledge of this graph, and in a broad array of settings that we describe, succeeds even when false negatives and false positives make up an overwhelming fraction of the data available. Our results show it succeeds in the presence of partial information about the contact network, and also when there is not a single "patient zero", but rather many (hundreds, in our examples) of initial patient-zeroes, spread across the graph. version:1
arxiv-1402-2941 | Multispectral Palmprint Encoding and Recognition | http://arxiv.org/abs/1402.2941 | id:1402.2941 author:Zohaib Khan, Faisal Shafait, Yiqun Hu, Ajmal Mian category:cs.CV  published:2014-02-06 summary:Palmprints are emerging as a new entity in multi-modal biometrics for human identification and verification. Multispectral palmprint images captured in the visible and infrared spectrum not only contain the wrinkles and ridge structure of a palm, but also the underlying pattern of veins; making them a highly discriminating biometric identifier. In this paper, we propose a feature encoding scheme for robust and highly accurate representation and matching of multispectral palmprints. To facilitate compact storage of the feature, we design a binary hash table structure that allows for efficient matching in large databases. Comprehensive experiments for both identification and verification scenarios are performed on two public datasets -- one captured with a contact-based sensor (PolyU dataset), and the other with a contact-free sensor (CASIA dataset). Recognition results in various experimental setups show that the proposed method consistently outperforms existing state-of-the-art methods. Error rates achieved by our method (0.003% on PolyU and 0.2% on CASIA) are the lowest reported in literature on both dataset and clearly indicate the viability of palmprint as a reliable and promising biometric. All source codes are publicly available. version:1
